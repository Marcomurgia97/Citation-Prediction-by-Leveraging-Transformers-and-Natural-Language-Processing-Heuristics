text
"by application of the output simulation module, patterns of potential microbial sediment respiration in the floodplains could be predicted. it was found that potential microbial respiration increases in aquatic regions of lower connectivity once they become connected during higher main river discharges. in addition, hot spots of potential co 2 emissions were also found in areas characterised by larger water depths, which is credited to an increased carbon accumulation rate in sediments of deeper water bodies compared to shallow water areas. by plotting and analysing the residuals associated with the model output, the uncertainties corresponding to the results could be quantified. regions of high connectivity and large water depths were found to exhibit larger uncertainties than those of low connectivity or water depth. this finding underlined that the transferability of these modelling results was given only to other floodplain systems of similar size, connectivity range and sediment properties, while the residuals and the associated uncertainty were assumed to increase if any of these parameters deviate significantly. while the modelling framework itself is invariant to the underlying hydrologic, geomorphic and sediment properties, a field sampling campaign would have to be conducted if the model was applied for floodplains differing significantly in size, connectivity or sediment quality."
"after a statistical evaluation of frequency and duration of event occurrence, the resulting parameters were obtained: (i) probability of connection (connectivity); (ii) average connection duration; (iii) average disconnection duration. this statistical information was then stored for every computation node, and used for plotting maps of connectivity properties."
", an annual flood discharge of 5300 m 3 s à1 (table 4 ) and a strong seasonality controlled by the regime of its alpine catchment . before the major regulation scheme of 1875, the river stretch was originally an anabranched section consisting of a main stem and a channel-network of numerous small and large branches [cit] . ecological degradation of the river led to constrained side arm connectivity and loss of riverine habitats due to floodplain aggradation. nowadays the river is under major restoration efforts which aim at improving the ecological status of the river [cit] ."
"any hydrodynamic model capable of delivering steady-state water surface elevations for a 2-d domain is suitable for this module, which particularly includes all 2-d and 3-d numerical models. if the number of model boundary conditions is reduced to one e as in some backwaters or flood retention pools e the dynamic component could be dropped from the flow equations and a simple node-propagation technique could be applied to obtain the water surface elevations."
"river systems can be viewed as open ecosystems dynamically linked along the river network and within the riverine landscape at the catchment scale. the water exchange conditions and the linkage to the hydrological regime at the local (i.e. reach) scales determine matter transport, cycling and transformation [cit] . within riverine landscapes, riparian wetlands and floodplains are key areas for biogeochemical cycling, where the hydrological dynamics affect the temporal variability and spatial heterogeneity [cit] . the constant adaptation to changing hydrology and nutrient inputs occurring within in the floodplains create a variety of processes occurring at different scales and interfaces . as a result, these riverine landscape features, where physical sedimentation and biological activities occur, act as biogeochemical hot spots, in particular for carbon and nitrogen cycling [cit] . consequently, this variability at the local scale has been shown to impact global elemental cycles [cit] . especially, the roles of river networks and within rivers, the role of different landscape elements such as floodplains, have been recently discussed for their contribution to global estimates of carbon cycling [cit] ."
"the calculated water surfaces for every node and characteristic discharge were inserted into a database used by the connectivity and output simulation modules. in addition to these results of steadystate simulations, a minimum water surface was calculated for every node, corresponding to the lowest possible water level at this location. this minimum water surface was identical to the terrain elevation for nodes that could actually fall dry, while it corresponded to the horizontal pool water level in all regions within the computational domain that remained wet due to pool conditions in the hypothetical case of the main river falling completely dry. the minimum water surface was determined by computing a stepwise water-level drawdown from a high starting level, either by hydrodynamics or node-propagation: whenever the model predicted a node to be wet at a higher water surface elevation and dry at a lower water surface elevation, even though its elevation allowed it to be wet under these conditions, pool conditions were present for that node. in such a case, the corresponding pool water surface was found iteratively between the two elevations. the calculated minimum water surface elevations directly correspond to the threshold of connectivity for every node: whenever a node encounters a water surface elevation higher than the minimum water surface, it must necessarily be connected to the main river. all connectivity threshold elevations for every node were inserted into the database as well."
the connectivity module outlined here comprised a number of simplifications regarding the actual fluvial processes encountered during the exchange of mass between a river and its riparian zones. these simplifications are therefore only applicable under several premises:
"where c is a system-wide constant which must be calibrated according to gauge readings but is usually small, i.e. in the range of 0.05 to 0.10 m."
substrate induced respiration (sir) [cit] . three subsamples of 10 g of wet sediment were weighed into three 100 ml schott glass flasks and closed with a silicon insert lid and received a 5 g c l à1 (as glucose) amendment.
"the average duration of disconnection was less than 10 consecutive days in the lower part of the lobau, between 15 and 30 days in the middle section and almost 90 days in the upper reach (fig. 8e) . in the orth study area (fig. 8f), this parameter ranged between 20 and 60 days in most of the side arms, with a smaller reach dominated by backwater conditions characterised by up to 90 days of disconnection."
"the output simulation module connected the output of the hydrodynamics and connectivity modules with sediment properties in order to predict the potential biogeochemical output of the entire system. this was performed in a two-stage procedure ( fig. 1 ): (i) assemble the master table, which linked physical, biogeochemical and hydromorphological parameters of all samples taken; (ii) calculate the best fit for properties at an arbitrary mesh node as a function of main river discharge and corresponding water depths."
"within the modelling framework, data processing was performed in three modules ( fig. 1) : (a) a hydrodynamics module, used for computing water surface elevations in the floodplain for characteristic river discharges; (b) a connectivity module, linking the output of the hydrodynamics module with a long-term hydrograph and performing an automatic event classification to yield connectivity properties of every computation node; (c) an output simulation module, calculating the potential microbial respiration for the entire mesh based on the best fit of hydrological and morphological properties at sampling sites."
"if the abovementioned assumptions are not met, the hydrodynamic model must be run in unsteady mode for the entire duration of the input hydrograph instead. in such a case the nodepropagation code cannot be used."
"in the study presented here, the weighting factors were derived from a mantel test on the entire master table, yielding the fig. 3 . diagram representing the event analysis in the connectivity module: events are defined as sections of the hydrograph exceeding the connectivity threshold h 0 as derived from the computation of a minimum surface by numerical modelling of a water surface drawdown; in order to account for retention leading to levelling of comparably small peaks in the hydrograph, events are counted only if exceeding a critical water surface elevation h c . event duration is denoted by t ei, intervals between events by t ii . [cit] . all calculations were done in r 2.9."
"in this study a modelling framework capable of predicting patterns of potential microbial respiration in the aquatic compartment of complex floodplain landscapes has been derived. hydrodynamics, hydrological connectivity and potential microbial respiration patterns were computed in a procedure performed in three modules. based on the modelled water depths for characteristic discharges combined with a long-term hydrograph, flow events within the floodplain could be characterised regarding their frequency and duration at every point, eventually leading to the integrative parameters connectivity, average connection duration and average disconnection duration. these parameters were calculated for the corresponding locations of a statistically relevant number of field samples for which the potential biogeochemical output in terms of carbon had been measured. subsequently a best-fit approach based on the method of least square errors was applied to every point within the floodplain in order to determine the sampling site that most closely resembles the environmental parameters encountered. the corresponding potential biogeochemical output was then assigned to the respective point, finally allowing for upscaling a limited number of samples to the entire floodplain and predicting potential microbial respiration patterns."
"the modelling framework presented here has shown to be a suitable tool to evaluate the consequences of river restoration and management on potential sediment microbial activities estimated by potential respiration. furthermore, this modelling tool could be used to assess the effects of changing hydrology on microbial processing and to elucidate potential effects due to changed flow regimes at regional scales. considerations for the future application of the approach include the availability of data in general, in particular regarding hydrologic and morphologic data sets, such as the length of time series observed at gauging stations or terrain elevations in reasonable accuracy. moreover, a precondition for the applicability of the modelling framework is that hydrological connectivity between a river channel and its floodplain must be a primary driver for ecosystem processes. future improvements of the modelling framework could be aimed towards the inclusion of actual instead of potential rates and the consideration of water temperature as additional variable; the model output could then be further enhanced by the conduction of a sampling campaign in non-saturated soils within the dry areas of the floodplain, thus eventually allowing for the calculation of actual respiration rates in the entire riverine landscape."
"the modelling approach was used to evaluate the potential microbial respiration of the floodplain sediment under different river water discharges in two study sites at the austrian danube east of vienna. potential microbial respiration provided an assessment of the total heterotrophic respiration of the floodplain sediments and was tightly controlled by sediment aeration status, and in turn by water residence time. the spatial variability of the patterns obtained from connectivity calculations showed that a floodplain system strongly dominated by anthropogenic influences through the presence of retention structures (lobau) was governed by geomorphology rather than hydrology, as opposed to a restored side arm system without human interference (orth) which was mainly influenced by hydrology. the comparison of modelled and measured stage hydrographs for validation purposes exhibited good agreement, justifying the assumptions made in the design and implementation of the connectivity module."
two study sites were selected: lobau and orth. both are situated within the alluvial zone national park at the danube east of vienna (fig. 4a) . [cit] m 3 s
"histograms of the parameters connectivity, average connection duration and average disconnection duration, detailing the fractions of the total area occupied by classes of these parameters, are depicted in fig. 9 . due to the distribution of terrain elevations, in both study areas a high proportion of around 90% of the total area were characterised by a low connectivity of less than 10% (fig. 9a) . however, while the orth study area featured a more or less equal distribution of the remaining classes throughout the connectivity spectrum up to 50%, the corresponding values for the lobau were subject to larger fluctuations between classes in that range. these fluctuations resulted from the geomorphic control of the lobau due to its anthropogenic partitioning into several reservoir-like sections. moreover, the orth study site also encompassed areas in the range of 50% up to permanent connectivity (100%), while such areas were virtually nonexistent in the lobau, which is another indicator of its underlying geomorphic control. a similar pattern was visible for the average duration of connection (fig. 9b): around 90% of the total area were connected only for short periods of up to four days; while the lobau exhibited peaks for some of the remaining classes e corresponding to the reservoirlike sections e and did not feature areas of a longer connection duration than 20 days, the study site near orth showed a more uniform distribution of areas spread over a larger class spectrum, which indicated predominantly hydrologic control. when analysing the average duration of disconnection (fig. 9c), a remarkably high fraction of 68.8% of the area within the lobau study site was characterised by statistical average disconnection times longer than 500 days, while only 13.5% were in this class for the side arm system near orth. this histogram revealed also an interesting feature of the orth study site, where low variability of the bank elevations throughout the side arms led to sudden widespread inundations (42.2% of the area) on average every 200 days; while this feature is clearly of geomorphic nature, the general control is hydrologic, as indicated by the class distribution in the remaining spectrum."
"a validation of the modelled stage hydrographs for sites within the lobau study region calculated by the connectivity module was conducted by comparing the corresponding module output to gauge readings. the objective was to verify whether the assumptions on which the connectivity module is based are justified and to confirm validity of its implementation. [cit], starting november 1, 2006 and ending october 31, 2007 . this period was selected as it is the only hydrologic year for which data without major gaps were available at four different gauges. the corresponding sites are located in different reservoirs of the lobau (fig. 5a) . fig. 6 depicts a comparison between modelled and measured hydrographs at all four sites. the diagrams indicate that the calculated minimum water surface e which is found if no flood event has taken place for significant time e represented adequately the measured data. it confirms that the concept of a minimum surface, the node-propagation technique applied to calculate it, as well as the underlying digital elevation model was appropriate. the minimum surface was also not subject to large fluctuations, indicating that evaporation was not a major issue in these study sites to be considered in the modelling framework. in general, the timing of events was properly modelled, even though the magnitude was overestimated for some events at most sites. however, the nature of the gauge readings available, i.e. two manuals (kgt and ew) and two automatic (st and kw) might not represent the reality of the water level during high flood events. indeed, during flood events, gauges are often inaccessible for personnel while automatic gauge recorders may stop operating; therefore the peak of a flood wave may be missed. [cit], when the highest water levels noted at site ew, near the upper end of the lobau, exceeded those recorded at site kw, much closer to the inlet, which was contrary to the pattern retention effects would cause. moreover, the readings showed the correctly modelled peak of the event at site st to decrease by 1.5 m within the distance of 2.5 km to site kw, which cannot be credited to retention alone. however, while the magnitude of the events in terms of water levels was sometimes overestimated, the event duration important for connectivity calculations was generally well predicted. only for very large events, the duration of the falling limb of the hydrograph was underestimated by the model; however, in terms of functional connectivity with the main river, it was unlikely to see any river water input into the system during that period as the flow direction was inverted during that phase."
"the side-arm system of orth is located just downstream of the lobau. it covers approximately 5.5 km 2 (fig. 5b), and features very diverse flow characteristics. some side arms are characterised by a through-flow at runoff just above estival mean flow, while others are connected at much higher flow conditions. most of the historical retention structures present in this river system have been removed in recent years as the inlets of the side-arms have been improved by lowering the bank heights to 1 m above low water level, thus increasing the side-arm discharge significantly as well as the connection duration. there are four main inlets and one outlet connecting this sidearm system to the main river. modelling of the orth study area was conducted using 2-d hydrodynamic modelling on a mesh of approximately 55,000 nodes, including the main river itself. the computation mesh was refined along the side arms and"
"as a result of flood protection engineering, riverine floodplains in urban or agricultural areas are often decoupled from the main river, thus changing their connectivity patterns and nutrient cycling regimes. restoration by reconnection is an attempt to recreate the nutrient processing potential of floodplains [cit] ). however, created and restored sites, due to homogenous sediment characteristics and changing hydrological regimes, may not return to the full potential of undisturbed, heterogeneous systems [cit] ."
"connectivity, average connection duration and average disconnection duration for both study regions are depicted in fig. 8 . in the lobau study area, the region close to the exchange inlet/outlet was characterised by connection probabilities of well above 50% (fig. 8a) . however, the morphology of the floodplain and the presence of the retention structures reduced this parameter to 30% and even 10% in the regions of the study area characterised by higher terrain altitudes. in contrast, the connectivity for the orth study area (fig. 8b) followed a different pattern as this region was dominated by through-flow rather than backwater flow. due to the absence of functional retention structures, only the natural morphological features presented an obstacle to the flow and therefore to connectivity. while the reach close to the outlet was permanently connected, side arms with more frequent through-flow conditions fig. 8 . modelling results of the connectivity module: connectivity map of the lobau (a) and orth (b) study areas; map of average connection duration in the lobau (c) and orth (d) study areas; map of average disconnection duration in the lobau (e) and orth (f) study areas. featured connectivity probabilities of 30%; those dominated by backwater conditions were characterised by a connectivity of around 10%."
"this procedure, referred to as gauge transformation, was applied to a hydrograph covering a statistically relevant period; usually 30 years or longer. as result of this procedure a local, virtual hydrograph of water levels h was obtained for every computation node (fig. 3) . applying the respective connectivity threshold h 0, calculated individually for each node within the hydrodynamics module, this stage hydrograph was then converted into a time series of binary information for each node, corresponding to a connected or disconnected state. subsequently, by analysis of the binary state changes, the time series was broken down into events. this allowed calculating event duration t ei and interval between events t ii . however, while the connectivity threshold was a reasonably good limiting value for the calculation of event durations, smaller flow events may not have reached the respective computation node, due to levelling caused by retention effects during wave propagation which were not covered in the modelling approach. therefore, an event was only counted at an individual node if its peak was sufficiently higher than the connection threshold h 0 for this node. this correction for hydrologic retention was realised by introducing a critical connection threshold h c for each node. this threshold was obtained from:"
"in simulation domains (i.e., study sites) characterised by inflow and outflow through the same cross-section profile, resulting in only one boundary condition, a node-propagation technique was employed using the rsim-gui graphical pre-processor of the rsim-3d hydrodynamic model [cit],b ): similar to a particle tracing approach [cit] ), a given water surface elevation e or flood wave crest height e was propagated from a computation node to its dry neighbours provided sufficiently low bed elevation. this procedure was realised using a binary tree data structure which guaranteed very fast execution time, particularly in comparison to a fully 2-d shallow water model. as only the final steady-state water surface was of interest, such a calculation without dynamic components did not introduce additional error."
"the propagation time of flood waves through the computation domain must be of the same order/scale as the time resolution of the input hydrograph, as otherwise significant errors would be introduced."
"the lobau is a floodplain of the danube river located on the left river bank downstream of vienna. it covers an area of approximately 23 km 2 . except for groundwater-surface water exchange and a small upstream inflow, the primary water exchange with the main river takes place through an artificial breach in the flood levee encompassing the lobau's southern side (fig. 5a ). three major retention structures with culverts prevent the side arms to fall completely dry during low flow periods, resulting in reservoir-like conditions (fig. 4b) . several gauging stations, most of them gauge boards, but some also equipped with automatic recorders, are present in the lobau; however, time series of gauge readings without major data gaps are only available for gauges located in the reservoir-like sections constituting the main stem of the floodplain system. time series of different gauges within each of these sections are highly intercorrelated. hence, out of the gauges available, four have been selected e one per section in the main stem of the river system e to validate the model (fig. 5a) . modelling of the lobau was performed on a mesh comprised of approximately 150,000 nodes using the node-propagation module, based on hydrographs of a gauge in the danube river located 250 m downstream of the inlet."
"the output simulation module was based on the parameters given in table 1, derived from the hydrodynamics module (hy), the connectivity module (co) or field sampling (sa). parameters 1 through 5 were input parameters, whereas parameter 6 was a model output parameter. for each of the input parameters, the minimum and maximum values were determined in order to allow for deriving dimensionless parameters through standardisation. for every computation node i and every row of the master table, corresponding to sampling point j, the residual r i,j was evaluated according to equation (1),"
"following the approach outlined before, the output simulation module was run using the potential microbial respiration data and median sediment grain sizes determined from field samples as well as the modelled connectivity characteristics of the study regions. the modelled potential co 2 emissions for the wetted regions of both floodplain systems are depicted in fig. 10 . the potential output took values of up to 7000 mg co 2 m à2 h"
"all flasks were incubated at room temperature in the dark for 4 h. a 10 ml gas sample was taken from each flask and injected into an evacuated 10 ml headspace glass vial. gas samples were analysed using a gas chromatograph (agilent 6890n coupled with an agilent g1888 headspace sampler) [cit] . initial co 2 concentrations were assumed to be zero, so the concentration after 4 h was the rate of sir. substrate induced respiration (sir) was measured in the laboratory within 2 days after sampling. these potential activities represent the state of enzyme pools present at the sampling time with no other limiting factor, and without de novo enzyme synthesis and cell multiplication. in the framework of our study, these potential activity measurements were more relevant than actual in situ rates (flux measurements) since (i) actual fluxes cannot be accurately measured under in vitro conditions and (ii) variations in actual activities can reflect short-term variations in environmental conditions (e.g. temperature, water content) whereas variations in potential activities reflect deeper modifications of the sediment microbial functioning such as a modification of the level of enzyme synthesis involved in the carbon degradation, a modification of the density and/or diversity of the microbial community responsible for a given biotransformation [cit] )."
"1. negligible hysteresis effects in the study area. water levels were assumed to be equal for the same discharge during the ascending and descending limb of the flood hydrograph. 2. no sharp increase or drop in water levels. the approach was based on quasi-steady state hydrodynamic modelling results. therefore, the temporal change in water levels must be smooth at any time."
"this paper presents such a modelling framework and its underlying modules that aim at predicting spatially detailed system connectivity and related output patterns of potential microbial sediment respiration leading to co 2 emissions in complex floodplain landscapes. the predictive model integrates hydrology and morphology. it is based on morphological input derived from lidar, bathymetric and terrestrial surveys, a long-term hydrograph of the main river, and biogeochemical and physical characteristics of sediment samples taken in the field. in a first step, connectivity patterns and wetting/drying cycles of natural floodplain environments were determined for a computation mesh based on the output of 2-d steady-state hydrodynamic modelling combined with a long-term discharge hydrograph of 30 years using a gauge transformation technique. this allowed for an automatic analysis of flow events, eventually yielding statistically relevant statements regarding connectivity probabilities and the respective duration of connection and disconnection cycles for every mesh node within the computational domain. these properties were assigned to field samples according to the sampling location. in a final step, the model linked the properties of every computation node to those of the field samples employing a best-fit approach based on least square errors. the predictive power of the modelling approach was applied to two study sites located in the alluvial zone national park of the austrian danube east of vienna."
"have been shown to have a major impact on biogeochemical cycling in wetlands by increasing the substrate availability and stimulating bacterial activity [cit] . in general, the change between dry and wet phases was expected to impact the microbial activity and the oxygen availability and thus the processing of matter [cit] . these findings of the temporal and spatial distribution of potential rates underline the importance of connectivity patterns for the microbial processing and the sediment microbial activity. yet, it should be remembered that in situ rates can deviate significantly from the measured potential rates. changes in water temperature due to flooding and seasonality were not considered in this study; rather the relative changes occurring within the floodplain were compared. therefore, the model output can only predict areas of potential hot spot activity in wetted areas under different flooding conditions. since the prediction of potential co 2 emissions was performed using a modelling approach based on least squares residuals, the method allowed for a calculation and interpretation of the uncertainty inherent to the results presented. the square root of the residuals corresponding to the results is plotted in fig.11, normalised to fit the range of 0.0 (zero residuals, exact match with conditions encountered during field sampling) to 1.0 (maximum residuals, basically unobserved natural state). at mean discharge in the main river ( fig. 11a and b), the residuals were generally low for both floodplain systems, with two notable exceptions near the inlet of the lobau and the outlet of the orth side arm system. these states of frequent (lobau) or permanent connection (orth) have not been covered by field sampling due to technical difficulty. therefore, the predictions in these regions were characterised by a higher uncertainty than those in other regions of the floodplain systems. higher residuals were also present in areas of comparable large water depths. again, these situations were rarely sampled due to technical difficulties; however, some data comparable to the conditions encountered were available. this is why the residuals were not as high as for the inlet/outlet sections. with rising discharge ( fig. 11c and d), the issue of unobserved situations of frequent connection was still visible, while the increasing areas of large water depths led to larger regions of higher uncertainties. a significant increase in overall uncertainty was finally encountered during the annual flood stage (fig. 11e and f), as the combination of water depth and connectivity present under these conditions in many regions, oxbows and side arms within both floodplain systems was never observed during the field sampling campaign. however, a large number of areas and particularly those identified as hot spots by the model were still characterised by comparably low residuals and therefore low modelling uncertainty, even under annual flood conditions."
"where w h, w d50, w pc, w tc and w td were weighting factors of the parameters water depth, median sediment grain size, connectivity, average connection duration and average disconnection duration, respectively. the sampling point j featuring the minimal residual r i,j was assumed to be representative for the computation node i, and the corresponding output parameters for potential microbial respiration were set for the mesh node. once the output for all nodes had been derived, output maps of the computation domain could be plotted."
". given that the metavariable association between the physical template and output processes was determined as 0.13, whereas the association between hydrology and output processes was calculated as 0.08 ("
"in the spatial distribution of the parameters connectivity, average connection duration and average disconnection duration the dominant influence of geomorphology e i.e. the conditioning fig. 9 . histograms of connectivity properties for both study sites, given as fraction of the total area: (a) connectivity in percent; (b) average duration of connection in days; (c) average duration of disconnection in days. of spatial parameter variability by morphological and retention structures e was visible for the lobau. in contrast, hydrology was the prevailing factor in the orth side arm system, as indicated by smooth patterns in the parameter distributions without major spatial discontinuities. this finding demonstrated the potential effects of restoration. a side-arm system restored through reconnection by lowering inlet elevations could be expected to exhibit generally higher connectivity values, while patterns of durations of connection or disconnection will be following the natural hydrographs of the main river rather than the discontinuities induced by anthropogenic structures. in general, this will be accompanied by an increase in the average connection duration and a decrease in disconnection duration, particularly for areas distant from the main river."
"the modelling framework relies on five fundamental prerequisites: (i) a high-quality digital elevation model of the study area, derived from lidar and combined with additional bathymetric and terrestrial measurements; (ii) a computation mesh representing the topography reasonably well, i.e. by applying local mesh refinement; (iii) water surface elevations for several characteristic river discharges derived from hydrodynamic modelling; (iv) a long-term discharge hydrograph, covering 30 years or more, for a gauge either within the computational domain or used as boundary condition for the hydrodynamic model; (v) a statistically relevant number of field sediment samples and their biogeochemical properties measured in the lab."
"once implemented these features and having a sufficiently functional system, it was tested with real users [cit] ). these tests were helpful to verify the actual usefulness of the system and its operation. likewise, once the system had been put to production, the authors made improvements in the reporting of fig. 4 . real-time information that students can get in second life about the audit performed. in the spanish text that appears in the figure, the system informs the student about the performance (% of the goals in the day and in all the days of the practice) and suggests the different elements where the student has not completed the audit. users (students in the virtual world, teachers using the web client, etc.)."
"compared with other published dual-band phased arrays antenna [cit], the proposed antenna array has a low profile and a low lost, and can be easily integrated with the phased array system. by judiciously designing geometrical structure and parameters, miniaturized and integrated unit cells are proposed to achieve the maximum scanning angle without the grating lobes phenomenon. metal shielding vias are introduced to suppress the coupling between the antenna elements operating in different bands, meanwhile dgs is introduced to eliminate the scan blindness effect and improve the performance beam scanning effectively. the proposed antenna array also supports a larger operational frequency ratio of 2:1, as shown in table i . in each operational band, the proposed antenna array has broad bandwidth. the proposed antenna array enables great wide-angle scanning ability with less gain decrease, and the side lobe level can keeps below −21.2db in the whole scanning range."
"generally, each element of the phased array antenna has a transmitting and a receiving channel, which makes it can be used to search, identify and track target simultaneously [cit] . to realize multiple targets"
"it is true that in the field of serious games there are already initiatives for the interconnection with systems like moocs, ples, vles, learning analytics platforms, etc [cit], but in the realm of virtual worlds there are not many such initiatives [cit], since a review of the current literature reveals that there is a tendency in the approach to proposals for the integration of different systems like vles, ples, architectures and applications together with virtual worlds [cit], but not with systems that analyze the performance of students within virtual practicals, or analyzing (however basic it may even be considered) the interaction and the evidences collected from student usage of 3d learning scenarios and practices. the literature mainly describes the interconnection of virtual worlds environments like opensim or second life with widespread platforms in the current teaching (moodle, mainly) by using sloodle and other platforms, so that these 3d environments extend their horizons of educational usage based on the support of systems and platforms where teachers already perform constant work creating materials and using them as an aid to teaching."
"in fact, the authors of this paper believe that this system is a true innovation in the field of teaching in virtual worlds (and even more in the field of education in health sciences) because of its features and vision outside the mere connection with systems that provide learning objects or materials created by teachers, but as an aid to the evaluation and understanding of how students perform in such complex and dynamic environments like these [cit] . even in the most recent literature it is possible to find authors [cit], for example) that describe the proposal of this usalpharma architecture as an interesting contribution that can open up new possibilities in the use of external systems completing the educational use of virtual worlds through the evidence collection system, the real-time responses both within the virtual world and outside it, and its possibilities of use for the evaluation of activities. therefore, for the authors it is possible to consider it a real, innovative and viable proposition for help in teaching. the interconnection of this architecture (using the services already available and others to be built) with other applications extends its field of education and elearning, and could help to achieve better results in learning scenarios arranged in 3d environments of this type."
"and requirements this section presents the problem proposed by teachers and technical personnel responsible for the usalpharma lab and the specific objectives to be met by the solution, as well as the software requirements developed to solve the problem."
"then, the proposed objectives for this system (both objectives proposed in the first version of the system, as well as those proposed for successive versions) are as follows:"
"to evaluate the contribution of term t to document d, we combine the frequency of t in d and the sentiment score smi (t). thus the weight v (t, d) for term t in document d is defined as:"
"this article shows how a software architecture has been conceived, designed, implemented and tested that helps support an educational activity in a learning environment like a virtual world. the article delves into the problems that such systems can solve as well as the benefits, profit, exploitation thereof, or the improvement achieved from current systems and methodologies."
"to make an intuitive understanding, table 1 shows the top 15 positive unigrams and negative ones for reviews on kitchen appliances and books in our real-world dataset respectively. we can observe that most of these terms reflect correct sentiment inclinations intuitively. here, the term \"not waste\" denotes a tag \"not \" is appended to the term \"waste\", which will be described in subsection 4.2. when someone does not like something, he or she can often say \"don't waste your time on ...\". thus, we should properly process the negatory words. some outliers like \"lodge\" and \"trotsky\" etc. will be discussed further in subsection 4.3."
"to illustrate this particular case, this article is divided into the following sections (in addition to this first introductory section): the second section, usalpharma: educational environment in a virtual world, accurately describes the context of the problem to be solved, introducing the reader to the virtual environment where the teaching of students in the knowledge area of pharmacy is developed, helping to understand the rest of the article. the third section, the problem: proposal, objectives and requirements, poses the challenges faced by the multidisciplinary team, as well as the objectives and requirements that the solution must meet. the fourth section, solution and product developed, specifies the theoretical solution that was proposed after applying an approach of software engineering to the problem, and the translation of this theoretical solution in a tangible software product and which can be used in a real context as presented. the fifth section, exploitation and results, presents the experience gained after applying this software solution for two academic years in the activity carried out within the context of the virtual world second life, as well as a number of indicators of use and usefulness of that solution. the sixth section, discussion, comments on the main aspects of the work that has been carried out, trying to enlighten the reader about the satisfactory and unsatisfactory aspects of the proposed solution and the experiment carried out, always from a standpoint of reflection and awareness of the results and their significance. finally, the seventh section, conclusions and future work, summarizes the results in general and a series of lines that open future work on this project."
"at the same time, the second life client has an even more essential role, and that is to send data to the system, so that it registers every evidence of interaction between the user and the virtual laboratory (data entry to the platform). to do this, all 3d objects to be audited by students in the virtual laboratory have an associated script that throws a http request to the application, which is responsible for collecting information about user interaction to record the action that is occurring at the moment. this request is carried out silently and transparently for the user; the object itself is responsible for collecting the data on the date and the exact moment of interaction (timestamp), with what the user has interacted, where in the virtual lab it is, etc., so that the user is not aware of being transmitting that information to the system, although all students are previously notified of the monitoring carried out in the laboratory."
"this section focuses on commenting the main points shown in the article, highlighting both the lights and the shadows cast by this experience, the tools developed, and the utility that the system acquires in a learning context such as the one shown. to do so, a number of subsections are set, each of which presents a question, key in the author's view, which will be answered as concisely as possible:"
"from a standpoint of software engineering, this paper describes how a team of engineers has been able to develop an innovative solution to a problem of some complexity, showing what tools and methods they followed to overcome the difficulties, how they planned the development and use of the solution from a multi-platform view."
"after this initial proposal, the project came under development and, based on the first results and observed possibilities, the authors saw the opportunity to expand the system functionality, including a few months after the first enforcement and feedback mechanisms within the 3d laboratory [cit] and opening the way for learning personalization and adaptivity of learning (referred as part of future work, as will be discussed at the end of the paper)."
"after this first stage in the implementation of this type of system to support teaching, the teaching innovation team wanted to go a step further, through implementing a system that would replace the teacher's guide during the students' practical sessions, controlling whether students were doing or not the practice within the virtual world, as well as to include a number of requirements on the feedback that students receive when participating in their practical work. that is, a system that allows taking advantage of the potential of virtual worlds in terms of the ability of autonomous learning, the possibility of not depending on the time and physical space (time, physical location of students) to leverage resources provided by virtual learning environments as well as presenting better immersive experiences to students within the 3d environment [cit] ."
"in this third part of the section are detailed the various major components of the product developed from the theoretical proposal for solving the problem. this product consists of three main parts, architecture and server itself, as well as web clients (for teachers) and client embedded in second life (for students): 1) software architecture: as previously discussed, it is implemented using the web framework django, and consists of a series of applications that are responsible for collecting the http requests that launches the second life client for recording data interaction, cleaning and processing such information, contacting the persistence layer data and database mongodb data, serving requests such as \"display information\" depending on the context / client from which they are requested (the same information is not served when the user launches requests from the web application or from the virtual world), calculating data associated with user interaction: measures of the time spent by each student to complete their practical work [cit], calculating the most relevant key points achieved [cit], reporting for each user or group of users, etc. these applications and the multiple layers composed architecture are responsible for maintaining the logic of the whole system as well as the bulk of the functionality."
"as for the second information vector, about the usefulness of this type of system for monitoring, reporting results and aiding in the evaluation and development of practical work, a series of metrics can be displayed collected by the platform that illustrate the reader about the volume of information that the system has collected in these two academic years (2013 [cit], specificly between 18 [cit] and 20 [cit] ), saving teachers from:"
"the unigram is one of the most commonly used feature type in text classification. the second experiment concerns the effectiveness of tf*mi based on unigrams. figure 1 shows the results of accuracy comparisons on four product domains. the value of α on unigram for each domain is shown in figure 3 . our approach, tf*mi, achieves the best performance in all product domains. on the one hand, both delta tfidf and tf*mi outperform the other three weighting methods invariably. on the other hand, tf*mi is better than delta tfidf in all domains, especially the accuracies of tf*mi is about 2.5% higher than that of delta tfidf in d. thus, both tf*mi and delta tfidf are effective ways for unigram to sentiment classification and the second half, log2(nt/pt), of formula (1) can also capture the sentiment polarities of terms effectively, but our approach is more effective. at the same time, tf*senti performs poorly in total, because a word often express several meanings. when it is considered solely, it is difficult to determine its sentiment polarity in a document. we observe that the feature presence is not always superior to the feature frequency, that is true in d and k but false in b and e. comparatively, it seems the feature frequency is better than feature presence based on the average accuracy in all domains. figure 2 shows the comparisons on the weighting methods based on bigram. the value of α on bigram for each domain is shown in figure 4 . like the previous experiment on unigram, our approach, tf*mi, achieves the best performance in all domains, and the accuracy is improved more significantly comparing whit the results in figure 1 . it is worth noting that delta tfidf is worse than the frequency and pres- further, we observe that the classification accuracies on reviews of electronics and kitchen appliances are higher than those in books and dvds domains clearly. the sentiment expressions on books or dvds are often more subtle than those of general products. for instance, a review of a book is written as following:\"when i read this book, i can't conceal my rage on the leading man, his ugly personality make me sick.\". the terms \"rage\", \"ugly\" and \"sick\" always express intensive negative emotion, while we concern them without context. but the reviewer is praising this book indeed."
"the training that students receive in this virtual scenario is based on learning methodologies, standards and audit processes in laboratories of the pharmaceutical industry in a practical way (and not merely theoretically, as had been usual), through interaction with a 3d scenario that replicates a laboratory that represents a real case. to do this, students act within virtual facilities as true external auditors who are responsible for assessing compliance with the laboratory of these glp measures, generating a report outlining compliance (or not) of the rules at the end of practical sessions, flaws identified in it (there are always a number of deficiencies brought intentionally by teachers), critical nature, etc. [cit] as part of the subject quality assurance in the laboratory analysis in the pharmaceutical industry within the master in drugs evaluation and development of the university of salamanca (spain)."
"2) web client: from the web client, teachers can consult all data concerning students and their work progress. among the metrics that can be known should be available the number of collected interactions, time spent by students performing, the achievement by the students of the inspection of the various elements (review of safety measures laboratory, review of equipment to be audited, documentation, etc.) as well as comprehensive reports for each class of students, so that a teacher can know in a detailed way the performance of each student, or acquire an overview of the group of students in their practicals. this web client is developed to be used from any device (pc, tablets, smartphones, etc.). figure 3 presents some data visualization of various metrics that teachers can know about student performance in usalpharma lab."
"our approach consists of two parts: first, we capture the sentiment tendency of the terms via evaluating their mutual information with polarity labels; second, the contribution of terms to a document is determined, which will be combined with the former to weight the terms in this document."
"based on these results, it is possible to see how students have increased their average mark (and decreasing the standard deviation) in each year, showing no significant changes in the trend between the different years -namely, the disparities observed between those years where the practice was conducted as a single session with teachers' guidance and the other years where the practice was aided using the complementary technological tools developed within the scope of this project. as will be discussed in the discussion section, these data do not have to convey that the use of this architecture and the tools provided improve the results of practical sessions (this may be due to many factors, such as the decline in number of students, the possible prior habituation of students to similar tools or 3d contexts, etc.), but it illustrates how a trend of improving results is maintained even though the method of help, support and evaluation of the practicals varies."
"the first experiment concerns the effect of the sentiment score smi presented in subsection 3.1. recalling the results in table 1, almost all of the top 15 sentiment scores of positive unigrams and negative ones in reviews of books and kitchen appliances reflect strong sentiment direction correctly. now we focus on the analysis of some outliers. the \"lodge\" in column 1 of table 1 always express the neutral sentiment without context. but the \"lodge\" is a famous manufacture, whose products on kitchen appliances including dutch ovens retrain a lot of popular credibility. there are 10 reviews total on the products of lodge in our dataset, and 90 percent of them are positive. that is why the term \"lodge\" builds stronger relationship with positive label. seven of the ten reviews on sharpeners are labeled as positive, and the left three negative. thus, the term \"sharpeners\" is more closely related to the positive label. the terms \"probe\" and \"candy\" are similar to this case, the former is included in 15 negative reviews and 1 positive review, and the later occurs in 6 negative reviews and in 1 positive review. in our dataset the dutch ovens received wide praise, but \"ovens\" does not occur in table 1 due to \"oven\" used by reviewers sometimes and no stemming applied in our experiments. we omit such discussion on dvds and electronics domains, which are similar to the cases discussed above."
3. performing a series of experiments on several kinds of feature presentation methods for sentiment classification in multiple product reviews. the results show the proposed approach is more effective than the traditional ones.
"the usefulness of such systems, from the point of view of the authors, depends primarily on the problem that it helps to solve. in the case of usalpharma laboratory, this proposal provides information that cannot be achieved otherwise within a virtual world like second life, but it is necessary to consider whether this information is vital or not in a process of teaching and evaluation of students. in this case, the authors agree that, in the present case, the information is not vital, since evaluation depends mostly on the report submitted by the students (it represents 75% of the final mark); therefore, this system only provides extra assessment elements. what must be noted is that, in this particular case, those extra elements are also designed to establish a right to assessment or not (the reader will remember the conditions set for the assessment of the practical, of at least 12% of proven elements and a minimum time of interaction with the virtual laboratory of one hour). moreover, the authors also wish to state that the contribution made by the system in the form of information for students and teachers is a faithful overview of what happens in a environment like a virtual world, allowing teachers to measure aspects such as dedication (hours used), effort (number of sessions, number of actions to detect all elements that must be evaluated) or persistence (number of sessions used, evolution of the completion of the activity in each session, etc.), for example. this type of data, although not taken into account in the specific assessment for this practical work (and therefore the architecture is not decisive in the present evaluation) could be considered as catalyst values for the final evaluation. the data were part of the rubric for assessing practice work, giving specific metrics to aspects that are not typically measured in a clear way. in summary, although the system has not a vital utility for the practical work, it is increasingly being consolidated as a helper to the teacher. additionally, the system is aligned with the current trends of tools, applications and utilities that are opening new paths in new learning contexts that will be relevant in technologyenhanced learning in the future."
"where p(x, y) is the joint probability of x and y, p(x) and p(y) are the marginal probability of x and that of y respectively. now we quantify a term's relationship with each label by mutual information. given n labeled samples, a is the number of times term t and label l co-occur, b is the number of times term t occurs without label l, c is the number of samples with label l but not include term t. thus the mutual information m i(t, l) of t and l can be evaluated by:"
"in this section are detailed three fundamental parts of the search process of a proper solution to the proposed problem: previous considerations, the theoretical solution proposed and the final developed product that is currently on production."
"in any case, time will determine whether this system or others like this will significantly and unequivocally improve the students' learning and results, or automation of certain tasks previously performed by the teacher, are not really replaceable, and these systems simply must serve teachers as an aid in the acquisition of knowledge and decision making."
"among the different results that can be shown to illustrate the operation of the entire system, it is possible to distinguish between two vectors of data to help test whether the system works, and to indicate if the system may be accepted for its actual use in teaching:"
"where is the antenna active reflection coefficient. the simulated results are shown in fig. 4, from which we can see that the scan blindness effect can be suppressed effectively in ka-band."
"in the past few decades, phased array antenna has obtained a great development [cit] . the beam scanning performance of phased array antenna is realized by controlling the feeding phase of each antenna unit cell electronically [cit], which can overcome the heavy servomechanism structures of the traditional mechanical scanning antenna. benefit from the performances of fast scanning speed and flexible beam controllability [cit], phased array antenna has been widely applied in mobile communication [cit], satellite navigation systems [cit], radio astronomy [cit] and radar systems [cit] ."
"the rest of this paper is organized as follows. section 2 provides an overview of the related work. our solution is presented in detail in section 3. a series of experiments are conducted to demonstrate the effectiveness of our solution in section 4. finally, we conclude our work in section 5."
"in such practice sessions, these tools presented here have been taken into account for the assessment of the subject, because teachers have decided to give a part of the evaluation of practice to the data provided by the system. specifically, 25 percent of the practice mark directly depends on the percentage of critical points assessed by the student in the laboratory (% checkpoints achieved in figure 3) . the other 75% of the practice mark corresponds to the audit report presented by students at the end of their practice sessions, and where they really show their knowledge about the glp regulations and the elements of the laboratory that meet them or not. other data taken into account when assessing the practicals are: a) if the student has carried out work on the agreed dates, b) if the student has made more than 1 hour of audit (in one or more sessions), c) if they have reached more than 12% of checkpoints revised. the teaching staff sets these three elements as the minimum requirements to show that students have actually audited and evaluated the laboratory."
"from the results in table 1 for book domain, \"trotsky\" and \"crichton\" are two writers whose books gained recognition in our dataset. the most reviews on books referring to the term \"vietnam\" are positive (five positive reviews and only one negative review), the term \"sons\" is similar to it. for the term \"zero\" in books, the negative reviews contain some negative information including \"zero information\", \"zero interesting anecdotes\", while \"zero\" is often regarded as the neutral one considered in isolation. thus, our approach captures term's sentiment tendency correctly."
"engineering is a sort of swiss army knife that gives those who use it (engineers) the necessary tools to solve problems by varying these tools depending on the area where engineers apply their knowledge or the nature and complexity of the problem to be solved. still furthermore, the application of this swiss army knife is not banned to non-engineers: engineers use it and know better than anyone how to use it, but its use can benefit the whole society. even more specifically, it can benefit communities of individual users or work teams regardless of their status, area of knowledge and experience, to solve different kinds of problems."
"this proposal is, even today, a proof of concept on the use and application of such systems and platforms related to elearning. the authors strongly believe that this proposal could have a more intensive usage and increased functionality through integration with other systems and elearning platforms, besides using more advanced algorithms and analysis techniques than those currently applied, so that they could get to perform complex learning analysis, behavioral analysis (which is already done in virtual worlds in other ways [cit] ), determination of the learning path for each student, personalizing learning, development of adaptive systems within virtual worlds, copy-detection among practices performed in 3d environments, etc."
"2 test doesn't work in sentiment polarity classification. consequently, a more sophisticated method is acquired for measuring the sentiment polarity of terms. we enhance the relevance of terms and polarity labels with mutual information in this paper, which is verified to be effective for increasing the classification accuracy in our experiments."
"sentiment classification has seen a great deal of attention in recent years, on which the bag of words of framework is widely applied. in such settings the predefined feature type and weighting method are crucial to classification accuracy. in this paper we introduce information theory into the sentiment polarity classification, and propose an improved feature weighting method for sentiment polarity classification of documents, in which sentiment polarities of terms are identified correctly, which are expressed as sentiment scores evaluated based on mutual information. to measure term's contribution to a document, its frequency in this document is integrated into our solution. in a series of experiments, our approach achieves the best performance in a real-world dataset including multiple product reviews comparing with the traditional weighting methods."
"before designing and developing a viable and optimal solution for this problem, researchers had to take into account a number of specific preliminary considerations of the problem to be solved, among which are highlighted:"
"in probability theory and information theory, the mutual information can capture the difference between the joint distribution on (x, y ) and the marginal distributions on x and y . moreover, it is a quantity that measures the mutual dependence of two random variables. formally, the mutual information of two discrete values is evaluated as follow:"
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. with the prevailing of web 2.0 techniques, more and more users prefer to share their opinions on the web. these usergenerated and sentiment-rich data are valuable to many applications like credibility analysis of news sites on the web [cit], recommendation system [cit], business and government intelligence [cit], etc. at the same time, it brings urgent need for detecting overall sentiment inclinations of documents generated by users, which can be treated as a classification problem. sentiment analysis includes several subtasks [cit] which have seen a great deal of attention in recent years: (1) detecting if a given document is subjective or objective; (2) identifying if a given subjective document express a positive opinion or a negative opinion; (3) determining the sentiment strength of a given subjective document, such as strongly negative, weakly negative, neutral, weakly positive and strongly positive. in this paper we focus on the second subtask."
"3) second life client: this client is somewhat different from the web client because it can be used by any avatar (user in virtual world) that is in the laboratory. the client is used through interaction (click) on an object in the laboratory, so that the student will see a dialog showing in real time the percentage of critical points assessed, both in the day when a query is made and, in general, making suggestions on what the student should re-audit. in the first year of practice the system showed only feedback about the particular day of the request, without giving an overview of the development of the practical on different days given to audit. figure 4 shows this type of activity report."
"for identifying sentiment polarities of words, the sentiment lexicon, such as sentiwordnet [cit], is an intuitive way to determine the sentiment polarities of words. the limitations of adopting this approach include: (1) the sentiment of most words or phrases are topic-dependent or domaindependent, it is possible that one and the same word or phrase appearing in different domains can indicate different polarities. for example, \"simple\" is negative in book domain while being positive in electronics domain. (2) many words always have multiple meanings, it is difficult to determine which one should be chosen in current text without context. thus, more sophisticated approaches are required for evaluating the sentiment polarities of words."
"1. tf*senti: sentiwordnet 3.0 is used to determine the sentiment score of a term, the term is weighted by the product of its frequency and its sentiment score."
"in a virtual world usalpharma lab is a virtual laboratory built by the teaching innovation group usalpharma and the department of pharmacy and pharmaceutical technology of the university of salamanca, which is within the virtual world second life [cit] . the laboratory simulates the facilities, equipment, documentation and tools afforded by a real laboratory of the pharmaceutical industry that complies with glp (good laboratory practices) [cit] to train graduate students in subjects related to quality in the pharmaceutical industry [cit] . the reason for building a virtual scenario of this type of laboratory is the economic cost and physical space that must be used, nonasumible for a university or any other institution that would not get any direct economic profit from it."
"this solution is based on an architecture that could be deployed in a cloud environment so that the architecture, based on the typical client-server schemes, consisted of a number of layers (data collection, data persistence of analysis, presentation, etc.) connected to each other in the same way (client-server depending on which requires the services of other interactions), enabling the deployment of each layer even in different clouds (with the potential to scale only those resources required, apply different technologies to each layer, etc.) due to the fact that the layers simply interact among them using services (figure 1 )."
"this article, entitled \"usalpharma: a software architecture to support learning in virtual worlds,\" presents how engineering has been used to solve a real problem in an educational context. it describes how a multidisciplinary team composed of profiles in the world of computer engineering and the area of health sciences (specifically the area of pharmacy) have been able to solve a complex problem, a priori, such as monitoring student activity within the context of a virtual world to assist in the evaluation of educational activities worldwide, through the application of technical and engineering tools (in this case software engineering) [cit] . related to the application of software engineering in a multidisciplinary context and in educational settings still in development, it should be noted that this project is neither unique nor pioneer in this type of collaboration and expansion of the application spectrum of technology solutions to service education, but it brings its vision and specific solutions within the set of the current generation of education systems enhanced by technology and current trends in technology to support processes of acquisition of knowledge and skills in heterogeneous learning environments."
"from a standpoint of elearning, this work shows how a multidisciplinary team has been able to design, build and put in use a solution to a real and specific learning problem of graduate students from the branch of knowledge of health sciences and pharmacy. in addition, from this point of view, this work shows the results that have been achieved in the first two years that the proposal has been used, and how these results have been satisfactory and provide real value to the activity of teachers and students."
"where a l is the number of times t and l co-occur, b l is the number of times the t occurs without l, c l is the number of times l occurs without t, d l is the number of times neither l nor t occurs, and n is the total number of documents. in two-category (such as l1, l2) setting, the value χ 2 (t, l1) of term t and label l1 is equal to the χ"
"in this sense, it is difficult to provide a clear answer. by the data in table 1, it is possible to say that the shift of paradigm and support in this case from classroom guidance to a system that provides feedback to students automatically, has not broken the trend of improved results, even when the results were improved significantly in the last year. the paradigm shift provided by this system is helping to improve teaching in a context such as a virtual world, allowing students to experience completely immersive environment like second life and avoiding the guidance of the teacher in real time. however, venturing this conclusion would be going too far as it is possible that other factors affect results. among the factors that could be relevant for the improvement of the results, can be highlighted the smaller number of participant students in the last years, the students' better predisposition towards such tools, the previous experience of some students with this kind of audit simulation, etc."
"for better understanding the influence of neglecting the non-resonant modes, in fig. 4 we have plotted the modal energies of each subsystem with and without considering them. the modal energies of subsystem 12 are not affected by the non-resonant modes (fig. 4a), which is logical because subsystem 12 is directly excited with the external force."
"on fitness function calculation in the every iteration. exploration as well as exploitation can be balanced to achieve global maxima with the help of sco algorithm, which has rapid convergence velocity. mathematically, the updated position for searched particle can be expressed as"
"the extra clock cycle needed per input for a systolic priority queue of length n decrees the sorting latency the size and latency requirements for sorting networks and systolic sorters are an impediment for implementation. large data sets demand o(n 2 ) swap comparators in sorting networks and o(n) cells in systolic sorters, putting a strain on the system's area requirements. however, systolic sorters can be implemented as components of sorting architectures than can handle large or infinite streaming data sets and provide constant-time sorting to fixed-size sorting systems for arbitrary size data [cit] . furthermore, they can be employed to manipulate cost-performance tradeoffs in hybrid sorting systems that decompose sorting into sequential and parallel parts [cit] ."
"the ils system needs to accumulate values before its sorted output becomes relevant. therefore, there must be some logic in control of output streaming. a simple choice is to start the output when one of the linear sorters is detected as full or almost full, ensuring a continuous flow of sorted values. another option would be to test the top sorted tag value before streaming. if we were to ensure the order for predefined tags, then the output logic must test each tag in a round-robin fashion before deletion from each linear sorter (an extension of the system presented in figure 6 ). because w tag values need to be inspected in a single clock cycle, this method's logic and routing generally limits the maximum frequency of the system. an alternative approach for sorting the ils's output is to use a pipelined sorting network. this sorting network can resort the top values from each of the w linear sorters. since w is generally small, the necessary processing elements, latency, and area are also kept small, thus overcoming the hindrances of a sorting network implementation."
"the dmf outlined in section 2.2 is next extended to the four-plate benchmark structure considering the subsystem partitioning presented above. hereafter the following notation applying the dmf and the conditions of conservation of slope and bending moment at the junction, the modal equations for the 3 coupled subsystems can be deduced."
"three scenarios were setup with the ils system doing hardware-based sorting. in each scenario, the unsorted data was saved in four blockrams, with each blockram output connected to the corresponding interleaved linear sorter input. the ils output logic was set to delete tags and values in strictly contiguous increments, acting as a sorter rather than a priority queue. this means the output will halt until the next tag in the sequence is sorted to the top of the queue, as was shown in figure 6, ensuring that we will get the same end results as the microblaze test. setting the depth of the four linear sorters to 16 nodes prevents each individual linear sorter from filling up while waiting for the input arrival of contiguous tags, even in the worst case scenario. this depth property further ensures that the input streaming remains not stalled due to lack of space in any of the linear sorters, preventing the system from going into a deadlock situation and allowing continuous output streaming."
"this example has been chosen because it is widely met in practical applications of sea (e.g., in the naval and building industries). however, to keep the problem amenable for analytical description we have only considered the flexural motion of the panels."
"in all the interleaving cases in table 7, the ils system experiences less delay than that of the register renaming process in the superscalar processor. on average, the ils delay was 34% of the register renaming delay, with 26% of that delay due to logic and 74% due to routing."
"the dmf must be discarded because only one subsystem could be described by its uncoupled blocked modes, the other ones having to be characterized by uncoupled free"
"to achieve world's power requirement, integration of renewable energy sources to the electrical grid is very important and for this inverter control design is required. the inverter controllers are needed to synchronize and injected sine wave inverter current to utility grid. predictive controller [cit], sine pulse width modulation (spwm) [cit], hysteresis based inverter controller [cit], space vector pulse width modulation (svpwm) [cit], fuzzy space vector pulse width modulation (fsvpwm) [cit] have been discussed to generate gate signal for firing inverter switches. in case of predictive controller, the output depends on loading conditions that has design complexities and has slow system responses. moreover, varying switched frequency and constant bands are the major issues of hysteresis based inverter control strategies. the svpwm is better-mapped method for inverter control and has lower switched losses with low harmonic contents as well. however, fixed dc-link control with low ripple contents and non-linearity are the major issues of this classical svpwm method. the fuzzy space vector pulse width modulation (fsvpwm) delivers fast dynamic dc-link utilization with reduced harmonic contents and manages non-linearity of the pv power system. however, practical implementation of fsvpwm using dspace hardware interface is complex."
"consider two continuous elastic subsystems 1 and 2, coupled at a junction γ, subsystem 1 being excited by a mechanical force. the dmf [cit] allows one to calculate the vibration response of the two coupled subsystems from the uncoupled subsystem modes and the reduced modal equations. in accordance with the formulation (see [cit] and appendix a), the stiffer subsystem is described by its displacement field and its uncoupled-free modes (i.e. assuming null stresses at the connection), whereas the softer one should be characterized by a stress field (e.g., acoustic pressure) and by their uncoupled-blocked modes (i.e. contemplating null displacements at the connection)."
"due to the finite nature of our linear sorter depth n, by default, only the top n results are kept. if sorting greater than n values is required, the replacement policy can be augmented with external logic that checks for full conditions. when the sorter has only one free node available and an insertion occurs, an output signal bit is set. this enables the external input feeding logic to start buffering rather than discarding new input data."
"one of the key attributes of a linear sorter is its regular structure. because of this regularity, a linear sorter of depth n can be implemented with little effect over the system's logic and routing delays. therefore, regardless of the linear sorter depth, its implementation maintains timing performance. this is an asset, since ideally the linear sorter depth n needs to be large enough to prevent full conditions. in simple cases, full linear sorters stall the input flow until one of its elements is removed. with more complex systems, there is a possibility that the logic controlling the sorted output streaming is input dependent (e.g., the system starts streaming after the tag number 1 is identified as the top sorted value). if the ils needs to validate a condition before streaming its output, it will accumulate values until the condition is met. it is then possible that one of the queues becomes full and not able to service inputs or outputs, that is, a deadlock condition. because only a single value can be inserted and deleted in each linear sorter, the depth n must be large enough to allow a sliding window of values. that is, at any given time, every input value received can be dependent only on the inputs received less than n clock cycles before. fortunately, n can be easily changed to accommodate this restriction."
"the zeta converter in continuous conduction mode is shown in fig. 3 . the converter works in two states; one when switch is turned on and another when switch is turned off. the power circuit of zeta converter comprises diode, switch, two inductors (l a, l b ), two capacitors (c a, c b ), internal resistances (r a, r b ) with r load . when switch s is turned on, inductor l a and l b are charged and capacitor c a is discharged. the equivalent circuitry for on state is shown in fig. 4 (a) and the mathematical equation is obtained as follows,"
"however, an important question remains, namely, which could be the modal formulation amenable with the partition scheme in fig. 7b that also satisfies the mcas?"
"merging networks are more efficient than insertion or bubble networks, which are impractical due to their large depth. these networks keep two lists of ordered data and produce a single ordered list after merging. batcher was the first to propose sorting networks, by introducing the oddeven mergesort and bitonic sort networks [cit] . the odd-even international journal of reconfigurable computing 3 mergesort network sorts all odd and even data in separate lists before merging, while bitonic networks first sort values in monotonically increasing and decreasing lists and then merge the lists into a single sorted sequence. these types of sorting networks are common in network crossbars for asynchronous transfer mode (atm) switching, usually in the form of a batcher-banyan switch [cit] ."
"numerical tests will be presented for different impedance mismatches between subsystems, to determine under which conditions the resonant modes can represent the global behavior of the system, and therefore fulfill with the mcas."
"in sea, the latter seems to be a theoretical problem rather than a practical one. this is so because there is a way out to the situation: one can resort to the wave approach to sea instead of the modal one. travelling waves are considered and one can deduce the expressions for the coupling loss factors at a junction [cit] from wave transmission coefficients [cit] . finite elements [cit], spectral methods [cit] and hybrid finite element (fem)-sea approaches [cit] can also be employed to compute them. in smeda, however, the problem is not only of theoretical but also of practical importance. this is so because in smeda the subsystem modes are required to compute the coupling loss factors, and there is no alternative akin to the wave approach in sea."
"this paper deals the msco based mppt controller for pv grid integration. in this research work, zeta converter of 4 th order buck-boost converter is employed for mppt functioning and acts as an interface between pv module and inverter. the proposed afsmc based inverter strategy is the combination of flc and smc methods in which lyapunov stability criteria has been employed to obtain high precise and robust response of the pv system which is responsible for unity power factor operation. fig.1 demonstrates the msco and afsmc employed control system for pv grid integration."
"in this paper, we present an alternative approach for linear sorters that solves the previously identified problems by (i) expanding the linear sorter implementation and making it versatile, reconfigurable and better suited for streaming input and output (section 3), (ii) parallelizing the linear sorter for increased throughput for scenarios with random and uniform distribution of streaming data (section 4), (iii) minimizing latency costs by buffering of contending parallel sorting requests (section 5), (iv) implementing the high-throughput linear sorter and outmatching the performance of current linear sorter approaches (section 6)."
"in this work, the dmf is extended to multiple subsystems sharing a junction. still, the method requires one subsystem to be clearly stiffer than the others, the stiffest subsystem being characterized by its free modes and the remaining ones by blocked modes."
"several optimized mppt method based on soft computing techniques, such as jaya, ant colony optimization (aco) etc. [cit] have implementation complexities because of large size of population as well as crowd based searching regulation. jaya based mppt algorithm [cit] provides best solution by neglecting worst previous parameter in every iteration which results slow tracking velocity as well as lacking to search global victory position. grey wolf optimization (gwo) [cit] works on prey tracking encircle and attacked process for global searching. however, under sharp global maximum power point, the gwo algorithm gets confused and local maximum power point arises."
"the mean modal energy,  p e, in response to a white noise force of unit power spectral density in the octave band of central frequency c f, is obtained by integrating (using a simple quadrature rule),"
"because the leftmost value is deleted, the node's functionality needs to be enhanced to include a left shift, in addition to the default operations shift right, hold, and insert. axis indicating clock cycles. during each clock cycle, a subset of the active nodes either shift left, shift right, hold, or insert. the output logic for this sample linear sorter deletes contiguous tags (and their associated data). once the first tag is received, the output logic will keep deleting as it retrieves data with incrementally contiguous tags. when it stops deleting, the linear sorter continues to receive input, and deletion resumes when the next contiguous tag is in the top of the queue. the gap in the sorted output column of figure 6 indicates a pause in delete requests from the output logic. the worst case scenario, where the first tag is inserted last, does not affect the sorting rate, but it does increase the latency of results."
"a conventional sorting system involves data acquisition and collection, processing, dynamic and long-term storage, and sorting and dispatch. many hardware approaches use linear sorting to keep a sorted list with in-order insertions but fail to optimize throughput, the rate at which data elements are processed. the system's sorting throughput is limited by the weakest link, which in many cases is the sorting stage. even though parallelism can speed up hardware-based sorting, sorted results are generally attained only one at a time. thus, a hardware-based linear sorter would only achieve a maximum throughput of one sorted output per clock cycle, regardless of the system's available and exploitable parallelism. additionally, specialized hardware like priority schedulers might have heterogeneous data processing and arrival times, which need to be considered for the system to work in a pipelined fashion. pipelined systems are not able to start the next stage before completing the current one, so stalling on a stage potentially halts the pipeline's progress until data becomes ready. this property essentially invalidates other hardware sorting approaches like bitonic sorting networks [cit], which are able to achieve high-throughput only when acting on fully available 2 international journal of reconfigurable computing data sets. to achieve low-latency pipelining and increase throughput through parallelism in a sorting system, a novel sorter implementation is needed."
"we chose the cases in which the contention buffer input size cw was twice the interleaved linear sorter system's width w. this buffer size is practical, because it avoids overflows when multiple values get sent to a contention buffer that is almost full. table 4 presents the revised latency of the ils system with respect to its width. the additional latency for large values of w is not as dramatic as was presented previously on table 2 and, therefore, counters the reduced clock frequencies experienced in implemented systems, as section 6 will describe in detail."
"the capacitor c a is charged from inductor l a and at the same time inductor l b is discharged through load r load and diode d 1 . the equivalent circuitry for off state is shown in fig. 4 (b) and the mathematical equation is obtained as follows, combining on, off state equation, and following, state space equation is obtained as follows (10) and (11), as shown at the bottom of this page."
"the coupling between two subsystems in smeda and sea takes place according to a set of conditions that the resonant modes in the subsystems must fulfill. we may term such conditions as the modal coupling assumptions (mcas), to be detailed below. the mcas permit characterizing the coupling between subsystems (groups of resonant modes) in sea, or between individual modes in smeda, through coupling loss factor coefficients."
"the implementation of a 4-way interleaving ils was compared against quicksort running in a microblaze processor, both in a virtex-2 pro ml310 device. the clock frequencies for software and hardware implementations matched the bus frequency of 100 mhz."
"on the other hand, we shall notice that only flexural waves have been considered in this work. future developments may address the influence of longitudinal and shear waves on the modal coupling. also, it would be interesting to derive coupling loss factors in smeda and sea from the herein reported results."
"one option could be that in fig. 7a . it consists in choosing, for instance, panel p1 as a subsystem and then grouping all remaining panels into another subsystem. in such situation, there would be a significant mechanical impedance mismatch between the two subsystems and the dmf could be applied. panel p1 (subsystem 1) could be well characterized by blocked modes, whereas the three others panels (subsystem 2) could be represented by free modes. however, this alternative sub-structuring would not be very feasible for complex built-up structures in practice, so it will be not pursued hereafter. a more manageable sub-structuring alternative for case #2 could rely on the weak coupling definition by fahy and james [cit], already cited in section 3.1. given that all panels have the same dynamic stiffness, each one perceives the other three at the junction."
"as opposed, non-resonant modes somewhat influence the less energetic modes of next, let us focus on test case #2 described in section 3.1. the same type of computations than for test #1 have been carried out. the results are summarized in table 1, where the overall subsystem energies are presented. as observed, significant discrepancies with the global modes reference solution are found for subsystems 3 and 4, even if both, resonant and non-resonant modes are included in the computations. this means that the 200 -resonant or non-resonant -modes are not sufficient to ensure the convergence of the modal expansion and a larger number of them should be needed to 14 achieve a better result. in fact, this is not unexpected since the considered subsystem modes do not resemble those of the system global modes as in test case #1."
"the microblaze version ran a c program for quicksort. the unsorted data resided as an array of values in a single blockram, and values were retrieved and written through the on-chip peripheral bus (opb) bram interface controller. for a small dataset of 64 values, microblaze took 49,982 clock cycles, which include bus arbitration and read and write requests over the opb. the end result is a sorted set saved in blockram."
"moreover, the three translations remain blocked for the 134 nodes belonging to the coupling edge because, as said, only flexural motion has been taken into account. the meshes of the subsystems have been built from the global mesh. appropriate boundary conditions at the nodes of the junction have been imposed in agreement with the requirements of the dmf method. the subsystem modes are then extracted using the implicitly restarted lanczos method (eigs command in sdtool, [cit] ). all subsequent dmf computations have been carried with matlab."
"to better understand this point we may compare the subsystem modal parameters considered in the dmf with those of the global modes. these parameters are the modal the maximum angular rotations of subsystem 12 are systematically greater than those of the global system, whilst the modal frequencies become lower. consequently, it is apparent that for the second test case the dmf seems not to be the appropriate approach to get a modal coupling scheme according to the mcas."
"careful consideration must be used with the external delete signal to avoid deadlock problems. the linear sorter should be configured with a depth n large enough to allow a sliding window for contiguous values. if, for example, the system can guarantee that no two contiguous data values can arrive more than m clock cycles away, then the minimum value of n should be m + 1. this also specifies the maximum latency of sorted results."
"few works seem to exist analyzing the particular fulfilment of the mcas in mechanical systems. crandall and lotz [cit], for instance, established the modal equations for the specific case of two flexural beams coupled by a torsion spring. the subsystem modes were chosen as the modes of the uncoupled-free subsystems and displacements were used to describe the vibration of the two beams. however, the resulting modal coupling scheme was not in accordance with the mcas; some terms arise in the formulation which express direct coupling between modes belonging to the same subsystem. nonetheless, if the torsion stiffness of the spring is small in comparison with the beam flexural stiffness, it was shown that the inner mode coupling can be neglected leading to fulfillment of the mcas. scharton and lyon had previously obtained similar results for two weakly coupled identical beams [cit] ."
"the structure of the systolic sorter is a linear array of processing cells. figure 4 demonstrates the regular structure of a priority queue with five cells, with the forward path heading right, the reverse path pointing left, and the rightmost cell reversing the traversal paths. registered outputs, rather than combinatorial logic, are a key factor of the cell and the priority queue functionality. serial data is sent to the systolic sorter's input every other clock cycle, allowing comparisons between adjacent forward and reverse queue values, which would otherwise bypass each other."
"two typical cases of building structures are contemplated. in the first one (test case #1), the floor is 20 cm in thickness and made of concrete, while the walls are 5 cm in thickness and made of brickwork. therefore, there is a clear impedance mismatch at the junction, the concrete floor being much stiffer than the brickwork walls. in the second situation (test case #2), both, the floor and the walls will be made of concrete with a thickness of 20 cm. consequently, all individual panels will have the same impedance."
"once defined the test cases, the next step is that of partitioning the four-plate system into subsystems. this sub-structuring [cit] should comply with the weak coupling assumption of sea. different definitions for the latter can be found in literature, which have always been a matter of discussion. the following ones could be useful in the present context."
"the startup behavior of the proposed pv system has been evaluated and depicted using fig. 11(a) and fig. 11(b) with conventional p&o and proposed msco based mppt methods, respectively. the msco based employed mppt has fast mpp achievement period, accurate performance and zero oscillations nearer to mpp region and hence high pv tracking efficiency because the losses due to oscillations have been neglected. under varying sun insolation level, the performance of the proposed versus classical p&o mppt methods have also been validated using practical responses in fig. 12(a) and fig. 12(b), respectively. experimental responses reveal that the proposed sine-cosine algorithm detects variation in environmental condition and detain next assessment up until the transition is dissolved. the working of the proposed mppt method has been evaluated by comparing existing pso and abc mppt techniques under same operating conditions. the comparison of pso, abc and proposed msco based mppt has been presented by fig. 13, which reveals that pv power extraction using msco method, has rapid tracking speed, fast convergence to achieve mpp with zero oscillations around this operating point. table 2 presents the pv tracking efficiency by employment of pso, abc, and msco mppt methods; which demonstrates msco has better pv tracked efficiency compared to pso and abc algorithms. practical tracking efficiency presented in table 2 strongly supports the ability of the msco based mppt algorithm under fluctuating weather conditions compared to pso and abc based mppt methods. the performance of the proposed pv power system has been tested under decreasing and increasing solar insolation conditions using obtained practical results and respectively shown in fig. 14(a)-(b) ."
"the modal coupling assumptions (mcas) characterizing the interaction between modes, or groups of modes, belonging to two different subsystems, can be summarized as follows (see e.g., [cit]"
"an improved sine-cosine optimized algorithm proposed and realized using dspace platform in the paper work. volume 7, 2019 complete hardware testing condition performed under uniform and non-uniform weather conditions for pv grid integration, and results compared with to recent pso and abc based algorithms. obtained results confirms that the developed msco algorithm provides simple implementation strategy, better tracking ability, high convergence rate and execute in digital controller platform using dspace. the mppt operation maintained under different perturbation conditions by selecting optimal duty ratio of zeta dc-dc converter. the employed inverter controller provides the unity power factor regulated by synchronizing utility grid through the inverter. he has vast experience in the field of biorefinery concepts and biogas production-anaerobic digestion, implementation projects of bio-energy systems in denmark with provinces and european states. he has served as a technical advisory for many industries in this field. he has executed many large-scale european union and united nation projects in research aspects of bioenergy, bio refinery processes, the full chain of biogas, and green engineering. he has authored over 100 scientific papers. he was a member on invitation in various capacities in the committees of over 200 various international conferences and was an organizer of international conferences, workshops, and training programmes in europe, central asia, and china. his focus areas are renewable energy, sustainability, and green jobs for all. he has been working in the area of distributed power systems and renewable energy integration for the last 10 years, and has published a number of research papers and posters in this field. [cit], he has been an assistant professor with the department of electrical engineering and renewable energy, oregon tech, where is currently involved with several research projects on renewable energy and grid tied microgrid systems. his research interests include the modeling, analysis, design, and control of power electronic devices; energy storage systems; renewable energy sources; integration of distributed generation systems; microgrid and smart grid applications; robotics; and advanced control systems. he, with his dedicated research team, is looking forward to explore methods to make the electric power systems more sustainable, cost-effective, and secure through extensive research and analysis on energy storage, microgrid systems, and renewable energy sources. he is also a registered professional engineer in the state of oregon, usa. he is currently serving as an associate editor for the ieee access. volume 7, 2019"
"the maximum throughput results in figure 10 do not consider two important factors. first, interleaving contention for the same linear sorter results in an average latency that increases with w, as previously shown in tables 2 and 4 . for the average case, the maximum frequency needs to be normalized by one of these factors. second, the input logic for a small ils width w will result in fairly simple logic and minimal delay. it is unlikely that this ils delay would limit a nontrivial system. instead, it is most likely that logic delays elsewhere in the same system determine the critical path and consequently sets the maximum frequency. as such, we assumed a maximum frequency of 300 mhz, which was the highest frequency obtained for 16-bit comparisons. this eliminates some of the artificially high frequencies an isolated ils system achieves."
"another modal approach is the statistical modal energy distribution analysis (smeda) method [cit], which establishes power balance equations between individual connected subsystem modes, rather than between the subsystems themselves. this circumvents the sea requirements of modal energy equipartition and enables applying smeda to cases of low modal overlap, locally excited subsystems, and to deal with complex heterogeneous subsystems. sea is then recovered as a limiting case from smeda [cit] ."
"because of the depletion of conventional energy sources, the demands of renewable energy sources are increasing day by day [cit] . among all renewable sources, photovoltaic generation (pvg) has widely considered renewable technology to produce electrical power [cit] . as per latest global solar demand monitor, the global solar installed capacity has been expressed 104 [cit] . pv modules have non-linear i-v nature and its output depends on sun insolation, ambient temperature and loading conditions which results necessity to control maximum power point (mpp) operating region for peak power extraction from pv system [cit] ."
"the basic reason for above behavior and failure of the dmf in case #2 is that the floor and the panels have the same thicknesses and material properties, and thus exhibit the same dynamic stiffness. hence, a more pertinent partitioning scheme for #2 compliant with the dmf is needed."
"where, ωω (t ) is inertia weight, ωω max and ωω min is maximum and minimum inertia weight, the values of ωω (t ) and s 1 (t ) increases in initial stage of iteration which results exploration and ωω (t ) and s 1 (t ) decreases at the end iteration and which results local development of msco algorithm."
"the decision logic to shift, insert, or hold a node value is a function of these three sets of inputs, specifically, the tags and validity bits. shifting decisions are summarized on table 1 . isolated insertions (row 1) right-shift all tags smaller than the new tag. an isolated deletion (row 2) always left-shifts every node. a left-shift can also occur while both inserting and deleting if the new tag is greater than the node's right neighbor (row 3); otherwise, the node inserts the new tag if that tag is also greater than its own (row 4). for isolated insertions, nodes will also insert the new tag if it falls between its left neighbor and its own tag (row 5) or if the new tag is added at the end of the queue (row 6). otherwise, the nodes keep their current tag and data values."
"the area overhead comes from the extra implementation logic for interleaving among multiple linear sorters. likewise, it also includes the adder/subtractor and counter necessary for detecting full conditions on the linear sorters. storage for the data to be sorted utilizes the fpga board's blockrams."
"modes. an appealing alternative would be to resort to the well-known craig-bampton (cb) method [cit] . given that the normal modes in the cb are defined with fixed-interfaces, this popular approach may be well adapted to our objective. checking whether the cb, or some variation of it, is suitable to characterize multiple subsystems connected at a junction, with low impedance matching and according to the mcas, will be the topic of companion paper [cit] ."
"several popular sorting algorithms (e.g., quicksort, mergesort, and heapsort) use divide-and-conquer techniques to achieve efficiency [cit] . intuitively, one would assume they are suitable for a parallel hardware implementation. regrettably, upon breakdown to a register-transfer level representation, these algorithms are plagued with data movement, synchronization, bookkeeping, and memory access overhead. the sorting speed is highly dependent on a fast and robust computing platform, the type of platform that is inadequate for mobile, embedded, real-time, low-power, or reconfigurable systems."
"one may therefore expect each panel to be blocked at the junction by the other three. the global modes could be more localized in that picture and resemble those of uncoupled panels with clamped boundary conditions at the junction. this would result in the substructuring scheme depicted in fig. 7b, in which each panel is identified with a subsystem."
"let us consider two elastic continuous mechanic systems rigidly coupled at the surface  (see fig. a.1 ). 1 v and 2 v represent the volumes occupied by subsystems 1 and 2, and the unit vectors 1 n and 2 n represent the outer normal to these volumes."
"in this paper we have studied the possibility of obtaining a modal coupling scheme for multiple flexural waveguides sharing a junction, in accordance with the required modal coupling assumptions (mcas) of some energy-based methods, like sea and smeda. in particular, the dual modal formulation (dmf) has been extended to deal with multiple connected subsystems. it has been shown that the latter can satisfy the mcas, the modes of the different subsystems being coupled through gyroscopic elements. however, the dmf suffers from one drawback: it only works well if one of the subsystems is clearly stiffer than the others. therefore, the approach fails when applied to subsystems with similar dynamic stiffness. this has motivated to address the problem of connected subsystems with low impedance mismatch in a companion paper [cit] ."
"data to be sorted resided in blockrams. to create the tags, a pseudorandom scheme was used. the size of our sliding window for tag generation was set at 64, meaning that two contiguous tags would not be more than 16 address spaces apart within the four blockrams. unsorted sets of 64 tags and data were written in random order to the blockrams while ensuring the sliding window property. the same data was used for both the microblaze and the interleaved linear sorter tests."
"-the interaction concerns the modes of the uncoupled subsystems, -the dynamic behavior of a subsystem mode can be associated to the dynamic behavior of an oscillator (mass-spring-damper system), -the coupling between the modes in different subsystems is conservative and takes place through mass, stiffness and/or gyroscopic elements, -modes within a subsystem are uncoupled (orthogonality of modes),"
"there is no evidence, however, indicating that the mcas should generalize to multiple subsystems connected at a junction. it is precisely the purpose of this work to make some first steps towards this goal, by considering the case of several flexural waveguides sharing a common joint. as far as the authors know, this problem has not been tackled before."
"linear sorters overcome the latency disadvantages of sorting networks and systolic sorters. their regular structure makes them highly configurable and a fitting solution for streaming data. nevertheless, their single-output nature limits their throughput. we have presented an implementation using interleaving of linear sorters that alleviates this limitation, using a delayed contention buffering scheme to maintain a low sorting latency. an ils system of width 4 showed, on average, a 1.8 speedup over a regular linear sorter and a speedup of 68 against an embedded microblaze processor. in an all hardware-implementation without the need of bus requests, like our superscalar processor implementation, this speedup became 1666. the versatility of the ils system also allows designers to easily configure the number of sorting nodes per linear sorter and the number of linear sorters in the system to best match system bandwidth and area requirements. this configurability makes ils systems easily adaptable to a variety of applications, particularly those that require high throughput for sorting streaming data."
"the paper is organized as follows. the mcas are detailed in section 2, together with a summary of the dmf applied to a pair of subsystems. as a by-product of the paper, we also present a new derivation of the dmf equations in the appendix, stemming from hamilton's principle and its complementary form [cit], rather than from reissner's principle. section 3 introduces a benchmark problem consisting of four panels sharing a junction, which will be used to facilitate discussion through the remaining of the paper."
practical responses reveal that the proposed mppt controller works accurately with high pv tracking efficiency and consisting fast convergence speed under decreasing and increasing solar irradiance profile. fig. 15 explains the performance of the proposed msco based mppt for pv system under uniform and partial shade conditions. practical responses reveal that the global power point is achieved under partial shading condition and mppt controller is able to differentiate uniform and shaded situations accurately.
"in this research work, zeta converter of 4 th order buckboost converter is selected for mppt functioning [cit] . it works as a power factor correction device, which works in continuous conducting modes. this proposed converter has high stepped voltage design with lower voltage stress. compared to sepic (single ended primary inductance converter) and cuk converter, the zeta converter comprises continuous output current with lower ripple in output [cit] . the proposed zeta converter produced lower ripple in output and minimizes the design complexities and rental of buck/boost switched converter with simpler compensation. for minimizing pv modules efficiency, the pv panels should be associated with switched mode power converter. [cit] has discussed hybrid cauchy and sine-cosine optimization algorithm for pv battery charging applications. in this method sine-cosine, based optimization technique produces population, which is followed by mpp achievement region with employed cauchy algorithms and has high convergence velocity. this hybrid algorithm provides effective pv battery charging under uniform and non-uniform implementation using dspace platform. this algorithm has design complexities and has high computational burden interfaced to microcontroller based system. sahu and londhe [cit] has implemented sine-cosine optimization based method for reduction of harmonics in 5-level inverter system. this method provides high convergence velocity with reduced harmonic contents. however, the mppt operation has not been discussed in this paper and has total harmonic distortion (thd) of 17.1%, which does not satisfy the ieee 519 standard. only these two papers have been discussed by any researchers for application of sine-cosine optimization based algorithm for renewable energy and power electronics based system."
"modified systolic sorters that support large data sets nonetheless suffer from large sorting latencies, since incoming values must still traverse the length of the systolic sorter twice. while appropriate for offline sorting, these large latencies create obstacles for streaming data, an encumbrance that is compounded with the extra multicycle input rate of systolic sorters. by contrast, linear sorters, the third approach to hardware sorting, allow for single-clock insertion and single-clock sorting latency. hardware one can iteratively traverse every node in the list when inserting values [cit], a more appropriate method is to send incoming values to all nodes in parallel. nearneighbor interconnections exist among nodes and their left and right neighbors, which allow each sorting node to make an autonomous decision about its new value and positioning in the list. this decision process involves each node acquiring a new value by comparing the incoming value against the node's current value and that of its neighbors. although the underlying hardware structure of the linear sorter is unchanged, each node and its corresponding value can be thought of as shifting right, shifting left or holding its current value in the context of the sorted list. figure 5 shows a linear sorter and the interconnections between its nodes. the data input (with a value of 4) is propagated to all nodes, forcing the third one (with a value of 5) to \"shift\" its value to the right, allowing the reception of the newly inserted value on the sorted list. linear sorters have small logic and control footprints, regular structures, and relatively straightforward hardware implementations using shift registers. additionally, these sorters are particularly appropriate for streaming data, where low sorting latency and continual sorting is crucial. the main drawback of linear sorters is the serial nature of both their inputs and outputs. even though the output of a linear sorter can be accessed in parallel (as depicted in figure 5 ), at most, one value can be erased from the queue during continuous operation of streaming data. the fixed size of a linear sorter adds extra restrictions as it limits the size of growing lists before depleting node availability. when the sorter is full, new inserted values must replace old ones. this can be done using a fifo that outputs the top n results of a list [cit] . alternative approaches augments the incoming data with an associated tag that indicates either an insertion or a deletion [cit] . such a linear sorter must additionally allow nodes to shift values left in addition to shifting right, holding, and inserting. furthermore, it is also possible to sort on tags rather than on the data itself [cit] . this approach is useful when implementing priority schedulers, or for preserving the order of data with identical tags. a final approach is to use a linear merge sorter, where two fifo sorted queues are merged into a single sorted queue through linear sorters [cit] . regardless of the approach, with only a serial output, linear sorters are confined to an output rate of one value per clock cycle, limiting the overall throughput of the system. both sorting networks and linear sorters can capitalize on increased throughput to improve their performance. sorting networks can be pipelined to increase sorting throughput but at a high area cost due to their depth. regardless, sorting networks still suffer from a latency of o(log 2 n) and are still unable to handle data streams [cit] . linear sorters, on the other hand, have a single clock cycle of latency and reduced area but by default are unable to produce increased throughput. we propose an extension to linear sorters which effectively increases their throughput by using parallel linear sorters and interleaving logic."
"the greatest benefit of our interleaved linear sorter system is achieved on a hardware-only computational platform. even though it is difficult to find applications that can fit this sorting scheme, the speedups attained in these systems make our approach an attractive option. therefore an ideal target application is one that receives an unsorted streaming input of values, and transforms them into a streaming output of sorted data for other hardware components. one such application is a reconfigurable superscalar processor. the configurable interleaved linear sorter was implemented as a component in an fpga-based configurable superscalar processor [cit], which required resorting of processed results to allow precise interrupts. the superscalar processor utilizes register renaming and out-of-order execution to increase performance by exploiting instruction-level parallelism. one of the key components is its heterogeneous collection of functional units, which provide specialized processing for different instruction types at reduced execution latencies. within the realm of reconfigurable computing, these functional units can also provide custom execution of instructions. the chronological sequence of events for an instruction execution starts with instruction retrieval, decoding, elimination of false data dependencies through register renaming, and storage in the reservation stations. these waiting instructions contain only true data dependencies, and once their operands are ready from the execution of previous instructions, they can be immediately dispatched in out-of-order fashion. performance is thus increased when executing multiple instructions simultaneously in the parallel functional units."
"in this msco method, the optimized search is carried out with varying sine/cosine trigonometric parameters. initially, sco method generates small size of population and movement is possible to achieve best outcome with the application of sine-cosine trigonometric function. mirjalili has invented sco algorithm for solving optimization-based problems [cit] . this stochastic optimized algorithm provides population based random search to obtain optimal solution based"
"there is potential for improving the linear sorter contention scheme. the buffered input signal accumulates all contending values from the current tag set and dispenses them immediately. the average latency cost associated with this procedure was previously shown in table 2 . we instead accumulate contending tags over multiple sets of inputs, dispensing them when the buffered input signal is full. table 3 shows the average performance increase when using this delayed buffered contention resolution scheme against the previously described immediate contention resolution scheme."
"systolic arrays, which are a matrix arrangement of processing cells, can be structured to sort serial input. this approach was first proposed by leiserson [cit] in the form of a systolic priority queue. queue data is fed serially into the systolic system. it traverses the length of the priority queue going forward then traverses it again in reverse direction towards the output. when two data elements, traversing in opposite directions, meet in a queue cell, the one with the maximum value keeps traversing in reverse (towards the output), while the minimum is sent back to the forward path. a special case of the systolic priority queue occurs if all inputs are loaded before starting queue extraction. the priority queue then becomes a systolic sorter [cit], but the system requires a systolic array length equal to the number of all input elements to be sorted."
"architecture. to achieve the linear sorter functionality we described, each sorting node implements the appropriate interface and logic. the sorter node interface has three sets of inputs and a set of outputs, each set containing signals for validity, tag, and data, as shown in figure 7 . the widths of the data (d) and tags (t) can be specified as a parameter during synthesis. the three input sets come from the left neighbor, the right neighbor, and the linear sorter insertions being forwarded to all nodes. the node's output set is sent to both its left and right neighbors, so that they too can make sorting decisions autonomously. this architecture coincides with those of single linear sorters [cit], which stores (key, data) pairs and compares them against neighbors and incoming values."
"where, r t i (j) is j th searched particle's position in the i th dimension, s 1 iscontrolling parameters, t is no. of iteration, s 2, s 3, s 4 random parameter, q t i is i th dimension destination position. in addition, s 1 controlling parameter explains the mobility orientation under inside or outside location between target and solution that is calculated mathematically as:"
"the proposed sine-cosine based mppt for pv integrated system has been implemented using dspace real time board. during hardware design la-25p (hall sensors) irfp460 (mosfet), mur1520 (fast recovery diode), ic sn74hc73ap (buffer integrated circuit) and hcpl-3120 (driver integrated circuit) have been employed as major components. the simulink model is interfaced with hardware circuitry using dspace control board, which is completely programmable. the dspace real time board comprises 36 analog to digital (adc) and 8 digital to analog (dac) channels. control desk is employed to monitor sensed (vpv, ipv) signals which are passed through analog grid integrated system and laboratory set-up has shown in fig 7 . fig. 8 depicts the pv voltage, current and grid voltage/current under sun insolation level 1000 w/m 2 . the experimental response demonstrates the unity power factor achievement using proposed mppt and inverter controller with dspace platform. fig. 9(a) demonstrates the grid voltage/current with unit power factor achievement under varying insolation level. fig. 9(b) shows the inverter voltage, current and grid voltage obtained practically by employment of proposed inverter controller. experimental response reveals that inverter voltage and utility grid voltage are synchronized to each other."
"the output of the linear sorter can be accessed in parallel by retrieving multiple node values at once. this is useful for systems which process data in batches, since the linear sorter depth n can be set to match the batch size. once an input batch is received and processed, the linear sorter can be reset to start the process again for a new input batch. for continuous operation, the output must be serial, the top value must be erased, and the queue must be informed of this action. to accomplish these properties, an additional external signal makes a request to delete the top value while retrieving it, thus freeing up nodes. this operation is akin to the pop() operation used in standard fifo queues and stacks."
"in the case of large models, localizing global modes for sub-structuring is not very practical because it implies computing all system modes. however, it provides a helpful guideline for our small four panel benchmark problem. for instance, in fig. 2 altogether, this indicates that the dmf is well adapted to describe the vibration behavior of this system. the floor, which is the stiffest component, should constitute a single subsystem described in terms of displacements and by its uncoupled-free modes (null stress at the coupling junction). the vertical walls should be two additional independent subsystems characterized by their stresses and uncoupled-blocked modes (null displacement at the coupling junction). therefore, in what follows subsystem 12 will designate the floor (panels p1 plus p2), whereas subsystems 3 and 4 will respectively correspond to the vertical walls p3 and p4."
"in the first scenario, a microblaze processor writes the unsorted data to the four opb-based blockrams and then sets a signal that starts the input streaming into the ils. a simple hardware counter was used to drive the addresses of the brams to cycle through all the unsorted values. the ils output was then connected to the input ports of four secondary opb brams that hold the sorted values. finally, the microblaze reads back the sorted values through an on-chip peripheral bus (opb) bram interface controller. sorting with an ils takes 2272 clock cycles, achieving a speedup of 22 over the microblaze-only option."
"the latter strongly facilitate solving sea and smeda systems, and resolving problems of practical industrial interest, like the computation of energy transmission paths [cit] ."
"to service interrupts from external sources like i/o and network, the processor saves its state, services the interrupt, and then restores state to continue execution. the superscalar's in-flight instructions, variable number of functional units, and out-of-order execution make restoration particularly difficult, so a reorder buffer is utilized to track in-order completion of instructions. the reorder buffer was an ideal target application for an interleaved linear sorter, as it had streaming input of instructions, streaming output of executed results, and a sliding window that set a maximum latency between contiguous tags depending on the number of in-flight instructions. additionally, the processor's reconfigurable and superscalar nature also demanded increased throughput and performance while requiring flexibility for different number of instruction streams. the interleaved linear sorter system met all these throughput, streaming, and adaptability requirements. when used as the reorder buffer for the reconfigurable superscalar processor, the interleaved linear sorter system surpassed the throughput requirements of the system. the amount of interleaving was determined by a reconfigurable parameter of the processor, which controlled the memory banks available for parallel instruction retrieval. table 7 shows the implementation delay and performance for a superscalar processor with an instruction issue width of four and four parallel functional units."
"for ease of exposition, the viability of the dmf approach when applied to multiple subsystems is analyzed for a specific structure composed of four flexural panels coupled at a junction, at right angle (see fig. 1 ). panels p1 and p2 in the figure represent the floor whereas panels p3 and p4 stand for vertical walls. the plates have clamped boundary conditions on the outer edges."
"the dmf calculations to be performed in subsequent sections need the subsystem modes as inputs, with appropriate boundary conditions. these modes have been computed 8 using the finite element method (fem) and the sdtool code [cit] in matlab. the computational mesh for test case #1 has been defined with a criterion of six elements per flexural wavelength at 2 khz. the global fem mesh is composed of 52126 nodes and 51604 quadrilateral shell elements, with 134 nodes belonging to the coupling junction. the same mesh has been considered for the second test case, which presents stiffer walls."
"the second scenario is set up in the same fashion as the first, but the results do not need to be read back into the microblaze over the arbitrated bus since final storage happens in the ils itself, whose linear sorters are deep enough to hold all the data. however, the microblaze checks the ils output to ensure all values have been retrieved before streaming the sorted results into the four opb blockrams, which takes a total of 732 clock cycles. again, the end result is the sorted set saved in blockram, but the speedup is magnified to 68 from the initial setup. the third and final scenario involves a hardware-only approach with no microblaze involvement. the output from the ils is consumed by other hardware components as soon as it is ready (but still keeping the contiguous sorted output limitation). under these circumstances, the interleaved linear sorting system takes only 30 clock cycles, a speedup of 1666 over the microblaze quicksort."
", is applied on panel p1, at the 1 khz central frequency octave band. the point of excitation is located at coordinates (0m, 1.6m, 0.87m). the damping loss factor of the floor and walls has been fixed to 0.02 in both configurations."
"despite of the mcas being at the very core of sea and smeda, as said in the introduction it is unclear whether they could also apply to the connection of more than two subsystems. before extending the dmf to multiple subsystems to check that issue, let us first give a summary of the method when applied to the easier situation of two coupled subsystems."
n in eq. (3) represents the normal vector pointing outwards to the boundary of subsystem 2. note that viscous damping could be also easily introduced in the modal equations (2).
"therefore, the coupling among the plates only takes place through rotations and moments at the junction. the coupling between the out of plane motions (flexural) and the in-plane motions (longitudinal/shear motions) has been discarded in the analysis."
"the strict hypotheses demanded by sea essentially confine it to the high frequency range. to address the so-called mid-frequency problem and extend energy-based methods to lower frequencies than those in the range of sea, several proposals have been made. in the framework of modal approaches, the energy distribution analysis (eda) [cit], for instance, expresses the energy influence coefficients [cit] of a built-up structure in terms of the modes of the whole structure. in the asymptotical scaled modal analysis (asma) method [cit], the physical size of the solution domain is reduced based on a scaling law, while the damping loss factor is artificially increased. this leaves a scaled model that can represent the mean response of the original system, with the advantage of only needing a reduced number of modes."
"sorting is an essential function for many scientific and data processing applications. extensive research has optimized multiple software sorting algorithms for general-purpose computing, thereby increasing application performance. the need for higher performance has also motivated the migration of sorting algorithms into specialized hardware to exploit spatial parallelism. nonetheless, many of the assumptions made to increase performance on a generalpurpose processor do not hold for custom hardware implementations. thus, reconfigurable applications typically do not enjoy the benefits of software-based sorting algorithms. when directly translated into hardware, software algorithms can quickly degrade into a series of data retrievals, comparisons, swaps, and writes; all problems that can be magnified in systems with low processor speeds, limited storage, disabled caches, and high-latency memory access times."
"hardware sorting makes extensive use of concurrent data comparisons and swaps each clock cycle, rather than relying on the sequential execution of multiple assembly operations like its software counterpart. due to its parallelism, a hardware implementation can speed up sorting applications, even at lower clock frequencies. hardware comparisons can occur simultaneously on multiple pairs of elements. a first approach involves making multiple concurrent small operations in comparators that cascade into a well-structured network and is called a sorting network. inserting serial input to a systolic array of sorter cells provides a second approach to hardware sorting. the third and final approach involves a single large parallel computation over multiple independent nodes and is called a linear sorter."
"the sorter node has registers of length d, t, and 1 for the data, tag and validity bit, respectively. three less than comparators test the inserted tag value against both neighbors' and the node's tags. finally, additional boolean gates are used to assess the input validity and the operations being requested."
"to overcome aforesaid drawbacks, in this manuscript a novel stochastic modified sine-cosine optimized (msco) based mppt method with lyapunov stability based adaptive fuzzy sliding mode control (afsmc). inverter control strategy has been implemented practically using dspace interface which works under every weather condition. the recentness of msco mppt and afsmc as inverter control is that these algorithms have neither implemented nor been demonstrated in any past research and under same operating conditions. additionally, the msco based proposed mppt technique has been equated with pso and abc methods and performance has been validated using hardware implementation with dspace real time board. the proposed afsmc based inverter strategy is the combination of flc and sliding mode controller (smc) methods in which lyapunov stability criteria has been employed to obtain high precise and robust response of the pv system. moreover, the proposed controls strategy is implemented in practically which has simple hardware requirement and adapts easily under changing environmental conditions."
"some considerations on subsystem substructuring are also outlined, followed by the derivation of the dmf for the four panel test case. numerical simulations and a discussion on the results are provided in section 4. the conclusions and future perspectives close the paper in section 5."
"in this research work, fuzzy logic control is combined with sliding mode controller which works as hybrid control for generation of gating pulses of inverter and adaptive rules based lyapunov stability controller has been proposed which where, η p is the positive constant,"
"certain popular content can be placed at a small subset of venues through traditional channels (e.g., the internet and postal services) to facilitate further delivery. when that is possible, the selection of these seed venues has significant impact on the performance of subsequent distribution. we use the following heuristics for placing the seeds. note that our system does not depend on seed selection and can work without it (e.g., in case of user generated videos). we choose to do it for popular content as it improves performance when we can strategically bootstrap content to a few venues."
"to address the second challenge, we study a range of algorithms to determine initial bootstrap kiosk locations (section iii-a). the algorithms use information such as venue popularity and the number of people traveling between different venues, which is readily available from the mobility traces."
"for partial content verification, we use segmented downloading methods [cit] . each video is divided into segments and a secure hash value is computed for each segment and signed by the content server. a segment is the smallest unit in data transfer. the integrity of a segment is verified by the hash value and authenticated by verifying the signature. copyright management: some videos may have copyright. in this case, we need digital copyright management to ensure users can only view the content up to a limited number of times or a given amount of duration (e.g., 24 hours) and cannot share copies with others. there are many existing drm (digital rights management) solutions [cit] that solve similar problems (e.g., protecting the copyright of songs, movies, and e-books). they either use software solutions to provide drm on general computing platforms (e.g., used by digiboo [cit] ), or tie the key to the identity of the hardware to prevent it from transferring to other devices (e.g., used by amazon kindle). we can leverage the existing works to manage copyright."
"we also compare practical routing schemes against oracle routing strategy described in section iii-b. we use 50 flows with utility placement and 5% seeds. we found that the utility based routing gives reasonable performance compared to oracle routing. for example in seoul, oracle routing performs between 86% and 90% in terms of traffic delivery rate including partial deliveries, where multi-hop traffic achieves 69%. for paris, oracle routing achieves 81% -87%, where the number for multi-hop traffic is 60%. for london, oracle routing performs 84% -85%, where multi-hop traffic achieves 69%. impact of number of flows: the number of flows determines how congested the network is and thus affects the performance. figure 6 shows the performance as we vary the number of flows from 10 to 200. we find that as the number of flows increases, the delivery rate reduces for all the schemes. flooding suffers the most from network congestion, with the average delivery rate going from 87% to 21% as the number of flows increases from 10 to 100. in comparison, the delivery rate of multi-hop traffic only drops from 88% to 54% with the same increase in the system load. impact of video size: video size also impacts the system performance. figure 7 shows the delivery rate as we increase the video size from 100 mb to 5 gb. as expected, the delivery rate decreases as the file size increases. the results show that 500 mb videos can be delivered with good delivery rates (77% -88%) using utility based replication. the delivery rate is around 70% even with larger 1 gb videos. this means that most mobile videos can be supported in our system."
"methodology: suppose a user u i visits a venue v j and we want to predict the probability of visiting a venue v k right after current venue v j . we consider three cases: (i) the user has previously visited v j and then v k right after that, (ii) the user has previous check-in history (at v k and other venues) but visits v j for the first time, and (iii) the user does not have any check-in history. for the case (i), we can naturally estimate his next check-in probability at v k as the ratio between the number of times that the user visits v k right after visiting v j and the total number of times that the user visits v j . for the case (ii), we estimate his next check-in probability at v k as the ratio between the number of times that the user visits v k and the total number of his check-ins at all venues. for case (iii), since we do not have per user information, we estimate user's next check-in using all other users' check-ins. the probability of next check-in at v k is the ratio between the total number of times any user visits v k right after visiting v j and the total number of times any user visits v j . evaluation: our evaluation focuses on the check-ins at the popular venues, i.e., venues that have at least 25 check-ins in the first two weeks in the analysis period. we use a one month trace for each dataset: [cit] for gowalla. table iv shows the prediction accuracy. we find that predicting the next check-ins as the ones with the highest probability of visit is more accurate than predicting based on the weighted average of their visit probabilities (e.g., if a user visits venue v k and we predict the probability of visiting v k as 0.8, then the prediction error is 0.2). so we predict based on the highest probability of visit in the remaining evaluation."
"providing incentive: carrying and transferring content consumes storage and power of users' mobile devices. therefore we should incentivize users to participate in forwarding. in videofountain, users are rewarded after every successful upload. based on the amount of the uploaded traffic and the importance of the venue that the content is uploaded to, certain amount of reward points are issued to the user. the reward points can be used to consume video content in the future, or get discounts in participating businesses. to further reduce users' concern about the energy consumption, we can have charging stations co-located with the venues. content verification: content uploaded by users may be corrupted or even modified maliciously. thus it needs to be verified, even if it is only part of a video. content verification also ensures only valid upload will be rewarded."
"in videofountain, we leverage user mobility to distribute the content from the seeds to the requesting venues. it is similar to routing in dtns, where mobile nodes serve as relays and communication happens during the contacts. different from traditional dtns focused on routing between mobile users, routing in videofountain involves two types of nodes: mobile users and static venues, where venues serve as rendezvous points. therefore an effective routing design should leverage the following unique characteristics of the venues: (i) stable popularity with zipf-like distribution, (ii) stable human traffic between venues, which decays rapidly with rankings, and (iii) the predictability of users' next check-in venues."
"our traces also have limitations: (i) check-in information does not come with the check-out time. so we assume that the dwell time follows an exponential distribution according to the observations from ( [cit] . (ii) as check-ins are voluntary, we may miss opportunities of video transfer. but we avoid over-estimation of the network connectivity."
"in this section, we describe algorithms to determine the initial placement of content and routing schemes to distribute the content from the initial locations to the desired destinations through human mobility."
"we evaluate the performance of videofountain with trace-driven simulations. we first get the check-in time stamps from the traces. check-out time stamps are generated from an exponential distribution with 60 minutes as the mean, which is the average wi-fi session time in real traces [cit] . each pair of check-in and check-out corresponds to one contact between a user and a venue."
"we also compare different content replacement strategies in user storage. and we find that utility based replacement strategy always performs better than fifo and random strategies. we omit the results for brevity. user mobility prediction: lastly, we compare the mobility prediction scheme against the oracle knowledge of next check-in in figure 11 . for all the utility based schemes, the performance difference between oracle and our prediction scheme is between 18% and 38%. given the fact that we use a very simple prediction scheme, the results are still promising."
other studies use the exact gps tracking [cit] . but most of these traces involve a smaller number of user movements and only a handful of studies examine movements of over 1000 users [cit] .
"we assume that a user device has 10 gb available space (which high-end smartphones already have and other phones can achieve via external microsd cards), and a venue has 1 tb. each video consumes 1 gb space. each venue has 50 mbps wireless capacity. for users' next check-ins, we use the individual mobility prediction from section ii-f. we generate 50 flows in a run and report the average of three experiments for each configuration. all results presented are from foursquare, as gowalla results show similar trend."
"we assume a control channel through which all venues share the distribution status such as the utility values of different flows, which venues have what content, and whether the destinations already have the content so that they can be removed from the utility vectors. similar assumption has been used in previous work [cit] . we can realize this assumption using a thin control channel (e.g., by leveraging mobile users' cellular connectivity)."
"in this paper, we propose videofountain. we study the feasibility of such a system by first analyzing large-scale location-based social network traces. these traces show that the popularity of venues exhibits zipf-like distribution and is stable over time; the degree of separation between the venues is small; the human traffic between venues is zipf-like distributed and temporally stable; inter-venue link capacity is comparable to internet bandwidth; we can predict users' next check-ins with a simple method. based on the insights from the traces, we develop schemes to place initial content and routing algorithms to distribute the content from the initial seed venues to other destination venues. our trace-driven simulation shows that the initial placement is important to the success of the system; with an appropriate placement, a simple utility-based routing scheme can perform very well due to the small degree of separation between the venues. these results suggest videofountain is a promising way of disseminating mobile videos in the near future."
"to support video watching on the go, we deploy kiosks equipped with wi-fi and a large storage at popular venues, such as restaurants, bus stops, railway stations and gas stations, where users can upload and download videos at a high speed and watch them on the go. to eliminate the need of significant effort of service personnel copying videos to every kiosk and to avoid the need for internet deployment, we propose videofountain, which replicates videos through mobile users who travel between kiosks and can carry the content on their way. the kiosks will then serve the videos to interested customers. in order to make this system a reality, a number of significant challenges need to be addressed:"
"flow generation: sources are generated based on the placement strategies in section iii-a. for each flow, destinations are randomly chosen among the venues that are not seeds. we generate the number of destinations for each flow using a uniform distribution, which can be up to 10% of the venues. we inject all the flows at the beginning of the simulation."
"there has been significant amount of work on mobility analysis using various location datasets, ranging from coarse grained locations from cellular networks [cit] to finer grained locations from wi-fi networks [cit] . however, locations in these studies are approximated based on the locations of cell towers or wi-fi hotspots and therefore have large estimation errors."
"mobility based content distribution: opportunistic networks leverage human mobility to distribute various content [cit] . instead of relying on the infrastructure, this approach emphasizes communications among mobile devices. in this context, various properties of mobile nodes have been studied such as node degree [cit], contact frequency [cit], contact time [cit], inter-contact time [cit], temporal-spatial connectivity [cit], social community [cit], to improve the content dissemination in user-to-user communications."
"first, the graph is constructed using the first two week data for venues with at least 25 check-ins in foursquare. we use daily snapshots (i.e., t is a day). figure 3 (a) and (b) show the degree of separation in the optimistic and conservative cases, respectively. the percentage number in parenthesis indicates the fraction of reachable pairs. as expected, in the optimistic case the degree of separation is lower and connectivity is higher (85%-100% connected pairs in optimistic case vs. 61%-90% in conservative case). in the optimistic case, 28%-63% pairs are within two hops and 77%-93% pairs are within three hops. even in conservative case 22%-62% pairs are within two hops and 62%-84% pairs are within three hops. since most venue pairs are within 2-3 hops, routing in our system is relatively easy."
"(ii) check-ins are voluntary and thus users only check-in when they are 'idle' to use their smartphones, similar to what videofountain targets. (iii) the traces naturally capture different properties of different venues. in some venues, users tend to be busy and they are less likely to check-in, while in other venues users have more free time to check-in. when videofountain is successful, users may have more incentives to use their smartphones, which will help to further improve the performance of video distribution."
"once we have the marginal utility value of all potential uploads and downloads, we sort them based on the marginal utility divided by the flow size and start replicating from the ones with the highest marginal per-packet utility."
"we then study the impact of the user storage as we vary the numbers from 1 gb to 10 tb while assuming the venue has 50 mbps capacity ( figure 10 ). we find that when the user storage is small (e.g. 1 gb), the delivery rate can be low. for example, we get only 30% traffic delivered with 1 gb and 76% with 20 gb when we use multihop traffic in austin. however once the user storage is as large as 10 gb, further increasing the storage size does not significantly improve the delivery rate. that is because in that case the wireless link becomes the bottleneck, i.e., although we still have available space, the amount of data that can be downloaded or uploaded is limited by the contact time and the wireless capacity."
"user movements closely follow a line in the log-log scale with the slope α ranging from 2.59 to 3.07. such a close fit suggests the inter-venue traffic closely follows a zipf-like distribution (i.e., the volume of users moving between venues decreases exponentially with the exponent of α), which shows that popular venues have much more traffic. temporal stability: next we analyze the temporal stability of tms, i.e., how much human traffic changes over time. we quantify the temporal stability using the following metric: normalized change in traffic (nct), defined as follows:"
"we plot the complementary cdf of tm elements in log-log scale in figure 4, where y-axis is the probability of having more than x users moving from one location to another. we further fit the curve using the maximum likelihood fitting as done in section ii-b."
"finally, we address a series of practical issues, such as incentivizing users by offering discounts, applying message authentication to ensure the integrity of the videos, leveraging digital right management tool to manage the copyright of the videos, and performing service discovery (section vi)."
"our work complements them as we go beyond the individual mobility and analyze aggregated movements between venues. moreover, to the best of our knowledge, this paper is the first attempt to apply the properties learned from lbsn data to the human mobility based content dissemination domain."
"places check-ins austin, tx 13,718 836 51,638 london, uk 16,426 621 36,679 manhattan, ny 115,605 5,824 481,976 paris, france 11,197 504 25,341 seoul, korea 9,158 457 20,255 (section iii-b)."
"to address the first challenge, we analyze traces of location-based social networks (lbsns), which have experienced an explosive growth in popularity (section ii). foursquare [cit], the most popular lbsn, has over 30 million users with 3 [cit] . major social networking sites like facebook, twitter, and google+ have also added location-based features into their services. lbsn data is particularly useful to study the potential of our system for the following reasons. first, all the check-ins are performed by mobile devices and they represent when and where people use their devices. these devices will be the targets for our video download. second, users stay at these locations for a while (at least long enough to check-in manually), which is likely to be sufficient for downloading videos. third, the check-in trace is massive: it contains mobility patterns of millions of users spanning across the world."
"service discovery: if users are interested in the content themselves, then they can walk to a kiosk to select the videos to download. but if they are merely carrying the video, they may not have the motivation to voluntarily look for video kiosk to start the upload. we leverage existing service discovery protocols e.g., upnp [cit], apple bonjour [cit], to discover when to start the upload."
"there have been many routing schemes proposed in various dtn scenarios [cit] . under lightly loaded cases, flooding can be used to achieve high delivery rate under the excessive resource usage. epidemic routing [cit] has been proposed to exchange content between mobile nodes while restricting the hop count and the number of replications. the notion of utility based replication was first proposed by rapid [cit] . the concept is intuitive: every packet is associated with a utility and a node replicates packets from its buffer in a descending order of marginal utility. the effectiveness of such a routing scheme depends on the utility function."
recently lbsn data has received attention as a good source for mobility analysis because of its scale. interesting properties of human mobility are discovered such as periodic patterns [cit] and close relations between mobility and social interactions [cit] .
"at a microscopic level, predicting an individual user's next check-in is essential to the design of content distribution, because content should be downloaded to users who are likely to visit the destination venue or at least make progress towards the destination. below we quantify the prediction accuracy."
"definition: to understand aggregated user movements at a macroscopic level, we define a human matrix t (a, b, t) as the number of users moving from a venue a to another venue b during a day t. if a user has visited a then b, we consider it as a one unit movement from a to b. human tms are derived by aggregating all user movements in a given day."
", where l is the hop count, t (l) is the total volume of traffic with the hop count of l, and β is a decay factor to penalize longer paths."
"people across the world spend significant time commuting. for example, americans spend over 100 hours a year commuting [cit] . the average trip is 38 minutes for the european union [cit] . one way to put the commute time into good use is for the passengers to download and watch interesting videos, such as popular movie clips, tv shows, and user generated videos, on-the-go. the current practice is to download videos on demand through cellular or wi-fi networks while watching them on the road. however, despite significant advances in cellular technologies, cellular providers have trouble keeping up with the rapidly increasing user demands and warn of crisis in mobile spectrum [cit] . on the other hand, wifi with internet connectivity is still very sparse due to its limited range and its coverage increases slowly due to high deployment cost."
"we first study how different placement schemes affect the routing performance. we compare random placement (random), popularity based placement (popularity), and utility based placement (utility). we vary the number of seeds from 1% to 5% of all the venues. for utility placement, we use multi-hop traffic as the utility function, i.e., we maximize the multi-hop traffic from the seeds to the venues. we also tried other utility functions (e.g., delay, geo, single-hop traffic) and found multi-hop traffic to be the best. figure 5 shows the evaluation result. we can see that placement is very important for all routing schemes. random placement results in much lower delivery rate than popularity placement and utility placement. in london with 1% seeds, random placement achieves up to 23% delivery rate while popularity placement and utility placement achieve 77% and 64%, respectively. using 1% seeds, we find the performance of popularity placement is slightly better than utility placement, (e.g., within 20%). however, utility placement outperforms popularity based scheme within 10% using 5% seeds. comparing 1% seeds and 5% seeds, we find that with same placement scheme, having more seeds consistently improves the delivery rate as we would expect. for example, with utility placement, 67% delivery rate is achieved in seoul when 5% venues are seeds, while the number is 56% when only 1% seeds are used. figure 5 also shows that utility based replication performs the best. for example, we achieve 54% delivery rate in paris with the utility based replication, while flooding only delivers 27% and epidemic only delivers 20%. we will use 5% seeds with utility placement as the default setting in all following evaluation as it gives good performance with a reasonably small number of seed nodes."
"we apply the utility-based routing framework in videofountain. the framework works as follows: given a contact between a venue and a user, we first identify all potential downloads and uploads. we then compute the marginal utility of each potential replication, which is the improvement in the utility function if we complete this replication. the utility value of a flow i is computed based on the existing replicas of i at all venues and the utility of the current user or venue."
"in multi-destination case, for each destination there is a utility value, thus we maintain a utility vector for each flow, where one vector element corresponds to one destination in the flow. we consider the marginal utility for this flow as the sum of the (non-negative) marginal utilities of all destinations and make the routing decisions in the same way."
"we run the evaluation on the foursquare trace from january 17 to january 30, 2012. for the utility functions that need training data, we use data from january 2 to january 16, 2012 as the learning period. we compare the delivery rate (of complete videos) of different routing schemes. note that while the delivery ratios with respect to the venues are sometimes low, the system is still useful since users can search for the venues where the deliveries are successful and only download videos from those venues."
spatial distribution: figure 2 shows the cdf of pair-wise distances between top 20 and 100 popular venues. only 6%-14% of top 20 venues are within 1 km from each other and only 3%-11% of top 100 venues are within 1 km distance. hence the popular venues are spread across the city.
"we collect and analyze data from foursquare (4sq) and gowalla (gwl), two of the most popular lbsns. both services provide access to the data through open apis ( [cit], which we use to collect the data. traces we collected contain: (i) user information, (ii) detailed venue information (e.g., venue name, latitude and longitude, venue categories), (iii) check-in information (e.g., user identifier, venue identifier, time stamp). tables i and ii show reasons for using lbsn traces: these traces are large scale and contain millions of users over an extended period of time. in addition to their sheer sizes, they have several important characteristics that make them appropriate for understanding the potential of videofountain: (i) the lbsn traces only consist of smartphone users, who are the potential customers of videofountain."
"we compare the top k% venues in one snapshot against those in the next snapshot, where each snapshot lasts two weeks. table iii summarizes the results and shows the top few venues are relatively stable over time."
there is a set of videos to be distributed. let i denote the index of the video. our goal is to maximize the number of videos going from the initial seeds s i to the destination venues d i . so this is routing for multisource to multi-destination multi-commodity flows.
"to address the third challenge, we examine a variety of routing schemes for dtns. we develop several new routing metrics to quantify the utility of the next hop and direct traffic towards the next hops that have high marginal utility city users"
"we then measure the impact of the contact time in figure 8 by varying the mean contact time from 10 to 90 minutes, while using exponential distribution. with 30-minute contact time, we can achieve 65% -70% delivery. increasing the contact time to 60 minutes (our default), the delivery rate increases to 67% -78%. impact of wireless capacity: figure 9 shows the impact of the wireless capacity, while varying the capacity from 10 to 200 mbps. we find that an increase in wireless capacity significantly improves the delivery rate. for example, delivery rate with 200 mbps is 2.5 times the delivery rate when the capacity is 10 mbps. this indicates that with wider deployment of the latest wireless technology, such as 802.11n in the future, the performance of videofountain can greatly improve. the utility based schemes consistently outperform other schemes under different wireless capacities. specifically, the utility based schemes achieve about 28% higher delivery rate than other schemes when the wireless capacity is 100 mbps and 23% higher when the capacity is 10 mbps."
"for both traces, we only consider the tm elements between active venues with more than 25 check-ins during the first two weeks of the analysis period. we observe that n ct is 0 for 87.72%-92.9% cases in the daily tms for the five cities in foursquare traces. for gowalla, we analyze tm snapshots from may 1, 2010 to june 4, 2010. we observe n ct is 0 for 92.6% and 89.7% cases in austin and manhattan, respectively. these numbers indicate that the tms exhibit high temporal stability. thus we can exploit such stability for predicting venue-to-venue traffic. this is critical for routing design."
"we first describe our data collection. then we analyze venue popularity distribution and stability, the number of people traveling between venues, link capacity between venues, and the ability of predicting users' future check-ins. all of these metrics are important to the design of a mobile content distribution system."
"we then simulate the content distribution by playing the trace. for the utility-based schemes, upon every check-in at a venue, we update the potential uploads and downloads at the venue and compute their marginal utilities. upon every check-out of a user, we compute what content is downloaded or uploaded by the user and update the utility values for all the involved flows. for flooding, we always replicate the packets sequentially. epidemic is similar to flooding with limited hop count (two hops in our implementation)."
"the algorithm thus requires three parameters: the number of stages (s), the number of characters of emitted hash (c), and the number of letters used to produce the hash (l). these have an impact on the quality of the graph, on the quantity of data that has to be shuffled, on the parallelism of the algorithm, on the quantity of ram required by the reducers, and on the number of similarities to compute."
"let be the original sc and be an unknown sequence of the same length. sequence v is considered as a sc if only the number of different bits between and, when compared bit by bit, is less or equal than to a predefined threshold [cit] ."
"and ii) use only a subset of letters in the hash, instead of the 64 original letters. moreover, if we only emit each string once, we will end up with a series of unconnected subgraphs, as each string is binned into a single bucket, and no edges are created between the strings of different buckets. to reconnect the graph, in the map phase, the algorithm creates a longer hash (using a coefficient we call stages) and emits the input string once for each subpart of the hash."
"rlsr [cit] : this paper proposes a semi-supervised feature selection algorithm by finding the global and sparse solutions of the projection matrix. the main contribution of the algorithm is to propose a regular term rfs [cit] : this paper presents a new robust feature selection algorithm through sparse learning. specifically, it applies 2,1 l  norm to both the loss function and the regular item. sparse restriction on the regular term can effectively remove the redundant features. the 2,1 l  norm of the loss function can effectively remove the noise samples, thus achieving a robust effect."
"for watermark extraction, host signal is splitted into frames and emd is performed on each one as in embedding. we extract binary data using rule given by (3). we then search for scs in the extracted data. this procedure is repeated by shifting the selected segment (window) one sample at time until a sc is found. with the position of sc determined, we can then extract the hidden information bits, which follows the sc. let denote the binary data to be extracted and denote the original sc. to locate the embedded watermark we search the scs in the sequence bit by bit. the extraction is performed without using the original audio signal. basic steps involved in the watermarking extraction, shown in fig. 5, are given as follows:"
"theorem 1 let t α be the sequence generated by algorithm 1, then for 1 t , (34) holds: through the above inequality and theorem 1, we can easily see that our algorithm is convergent."
"a graph is a mathematical structure made up of nodes (or vertices) connected with edges. in some cases, the edges have a weight, resulting in a weighted graph. instead of physical relationships like \"is a friend of\", \"likes\" or \"has an hyperlink to\", edges may also represent the similarity between nodes. this results in a nearest neighbors graph, of which two flavors exist. the most commonly used is the k-nearest neighbors graph (k-nn graph), where each node is connected to (has an edge to) its k nearest neighbors, according to a given similarity metric."
"in this section, we first introduce the symbols used in this article and then explain our proposed unsupervised feature selection algorithm via local structure learning and kernel function(abbreviated as: lsk fs), in sections 3.1 and 3.2, respectively, and then optimize the proposed optimization method in section 3.3. finally, we analysis the convergence of the objective function in section 3.4."
"to show the effectiveness of our scheme, simulations are performed on audio signals including pop, jazz, rock and classic sampled at 44.1 khz. the embedded watermark, w, is a binary logo image of size bits (fig. 8) . we convert this 2d binary image into 1d sequence in order to embed it into the audio signal. the sc used is a 16 bit barker sequence 1111100110101110. each audio signal is divided into frames of size 64 samples and the threshold is set to 4. the value is fixed to 0.98. these parameters have been chosen to have a good compromise between imperceptibility of the watermarked signal, payload and robustness. fig. 9 shows a portion of the pop signal and its watermarked version. this figure shows that the watermarked signal is visually indistinguishable from the original one."
"since the number of imfs and then their number of extrema depend on the amount of data of each frame, the number of bits to be embedded varies from last-imf of one frame to the following. watermark and scs are not all embedded in extrema of last imf of only one frame. in general the number of extrema per last-imf (one frame) is very small compared to length of the binary sequence to be embedded. this also depends on the length of the frame. if we design by and the numbers of bits of sc and watermark respectively, the length of binary sequence to be embedded is equal to . thus, these bits are spread out on several last-imfs (extrema) of the consecutive frames. further, this sequence of bits is embedded times. finally, inverse transformation is applied to the modified extrema to recover the watermarked audio signal by superposition of the imfs of each frame followed by the concatenation of the frames (fig. 3) . for data extraction, the watermarked audio signal is split into frames and emd applied to each frame (fig. 4) . binary data sequences are extracted from each last imf by searching for scs (fig. 5) . we show in fig. 6 the last imf before and after watermarking. this figure shows that there is little difference in terms of amplitudes between the two modes. emd being fully data adaptive, thus it is important to guarantee that the number of imfs will be same before and after embedding the watermark (figs. 1, 4) . in fact, if the numbers of imfs are different, there is no guarantee that the last imf always contains the watermark information to be extracted. to overcome this problem, the sifting of the watermarked signal is forced to extract the same number of imfs as before watermarking. the proposed watermarking scheme is blind, that is, the host signal is not required for watermark extraction. overview of the proposed method is detailed as follows:"
"for example, to run the algorithm with 100 buckets and two stages, the custom ctph function produces a hash of three characters, using ten letters. if the hash of an input string is abc, the original string will be emitted twice by the mapper: once for ab, and once for bc. in this way, we can expect that the reduce task for bucket bc will produce edges to strings located outside bucket ab, hence reconnecting the graph."
", where () obj t represents the value of the objective function at the t-th iteration. from figure 3 we can see that: 1. the proposed optimization algorithm is convergent, which makes the value of the objective function gradually decrease in each iteration, and finally converges. 2. the proposed algorithm converges on most datasets within 20 iterations, which indicates that the proposed algorithm converges quickly."
"in order to eliminate the interference of the outliers, the noise samples are removed at the same time [cit] . this paper adds a low rank constraint [cit] to the matrix w, namely:"
"we now study the influence of the number of buckets on processing time and on the quality of produced graph. we have two ways to modify the number of buckets: varying the length of the hash, and varying the number of letters used to produce the hash. therefore we run three series of tests. for each series, we use nnctph to build a 10-nn graph from our spam dataset. given the results of our previous section, we use two stages as this offers the best trade-off between running speed and recall. in the first and second series, we use respectively one and two characters, and we let the number of possible letters used to compute the hash vary. in the third series, we use a fixed number of possible letters (two), and we let the number of characters of the hash vary. these values are summarized below. the resulting running time and recall of each series of experiments are displayed on figure 2 . as we can observe, the running time and recall both tend to decrease when the number of buckets increases, but the impact on recall is quite limited. we can also see that, for the same number of buckets, shorter hashes (with less characters, but created using more possible letters, like s1) produce slightly better graphs but run slower."
"since the objective function is not co-convex, the closed solution cannot be directly obtained. therefore, this paper proposes an alternate iterative optimization method to solve the problem, which is divided into the following four steps:"
"feature selection is an important way of data pre-processing. it mainly looks for a subset of features that represent the original data. due to the popularity of sparse learning, most of the existing feature selection algorithms use sparse learning. however, from the perspective of machine learning, the existing feature selection algorithms are mainly divided into unsupervised feature selection algorithm, semi-supervised feature selection algorithm and supervised feature selection algorithm."
"we use nnctph to build a 10-nn graph from our dataset. we use hashes of two characters, with 32 possible letters. we thus create 1024 buckets, and we let the number of stages vary between one and ten. the quantity of data that has to be shuffled and transmitted over the network is directly proportional to the number of stages, which is confirmed by our experiments. using a higher number of stages will thus slightly slow the algorithm down. at the same time, this will distribute the same input string into more buckets, thus increasing the probability to find correct edges."
"we evaluate the performance of our method in terms of data payload, error probability of sc, signal to noise ratio (snr) between original and the watermarked audio signals, bit error rate and normalized cross-correlation . according to international federation of the photographic industry (ifpi) recommendations, a watermark audio signal should maintain more than 20 db snr. to evaluate the watermark detection accuracy after attacks, we used the and the defined as follows [cit] : (4) where is the xor operator and are the binary watermark image sizes. and are the original and the recovered watermark respectively."
"eufs [cit] : this paper proposes a new unsupervised feature selection algorithm, which is different from other unsupervised feature selection algorithms to generate tags through clustering algorithms. this method directly embeds feature selection into the clustering algorithm through sparse learning theory. the most prominent contribution of this method is that other unsupervised feature selection algorithms can be applied to this framework."
"we now test how the algorithm behaves when the size of the dataset increases. therefore we use other datasets with up to 800.000 spams. based on previous experiments, we use two stages, hashes of two characters, and we tune the number of letters used so that each buckets receives an average of 200 spams, as summarized below. we also compare nnctph with our mr implementation of nn-descent and with our sequential implementation of nn-descent. for both algorithms, we chose parameters that deliver approximately the same recall. the resulting running times and recalls are displayed on figure 4 . as we can see, the recall achieved by nnctph with these parameters is very stable, and the running time rises very slowly. as a result, the bigger the dataset is, the higher the speedup with respect to mr nn-descent. with our dataset of 800,000 spams, we reach a speedup of nearly an order of magnitude for the same quality of the final graph. clearly here mr nn-descent suffers from its iterative structure, which is not well suited for the mr framework, and requires a lot of slow disk i/o operations."
"values are all above 0.9964 and values are all below 3%, demonstrating the good performance robustness of our method on these audio files. this is robustness is due to the fact that even the perceptual characteristics of individual audio files vary, the emd decomposition adapts to each one. table iv shows comparison results in terms of payload and robustness to mp3 compression attack of our method to nine recent watermarking schemes [cit] . due to diversity of these embedding approaches, the comparison is sorted by attempted data payload. it can be seen that our method achieves the highest payload for the three audio files. also, for these signals our scheme has a good performance against mp3 (32 kb/s) compression, where the maximum of ber against this last is of 1%. fig. 10 plots the versus . we see that tends to 0 when . so, this confirms the choice of sc length. fig. 11 shows that the is dependent on the length of watermark bits. so, we note that for the embedding bits length, the tends to 0. since the watermark size in bits used is o f 1632, the obtained is very low. in this paper a new adaptive watermarking scheme based on the emd is proposed. watermark is embedded in very low frequency mode (last imf), thus achieving good performance against various attacks. watermark is associated with synchronization codes and thus the synchronized watermark has the ability to resist shifting and cropping. data bits of the synchronized watermark are embedded in the extrema of the last imf of the audio signal based on qim. extensive simulations over different audio signals indicate that the proposed watermarking scheme has greater robustness against common attacks than nine recently proposed algorithms. this scheme has higher payload and better performance against mp3 compression compared to these earlier audio watermarking methods. in all audio test signals, the watermark introduced no audible distortion. experiments demonstrate that the watermarked audio signals are indistinguishable from original ones. these performances take advantage of the self-adaptive decomposition of the audio signal provided by the emd. the proposed scheme achieves very low false positive and false negative error probability rates. our watermarking method involves easy calculations and does not use the original audio signal. in the conducted experiments the embedding strength is kept constant for all audio files. to further improve the performance of the method, the parameter should be adapted to the type and magnitudes of the original audio signal. our future works include the design of a solution method for adaptive embedding problem. also as future research we plan to include the characteristics of the human auditory and psychoacoustic model in our watermarking scheme for much more improvement of the performance of the watermarking method. finally, it should be interesting to investigate if the proposed method supports various sampling rates with the same payload and robustness and also if in real applications the method can handle d/a-a/d conversion problems."
we first test the influence of the number of stages used to run the algorithm. the number of stages is the number of times each input string will be emitted by the mapper. we use this coefficient to reconnect the different subgraphs produced by the reducers.
"-cropping: segments of 512 samples are removed from the watermarked signal at thirteen positions and subsequently replaced by segments of the watermarked signal contaminated with wgn. values are all below 3%. the extracted watermark are visually similar to the original watermark. these results shows the robustness of watermarking method for pop audio signal. even in the case of wgn attack with snr of 20 db, our approach does not detects any error. this is mainly due to the insertion of the watermark into extrema. in fact low frequency subband has high robustness against noise addition [cit] . table iii reports similar results for classic, jazz and rock audio files."
"1.14 ). if we choose l and c such that the number of strings per bucket (n/l c ) is constant (with a number of buckets proportional to the size of the dataset), this means that the computational cost of our algorithm is proportional to the number of buckets, and thus proportional to the size of the dataset. this is a lower bound. if the input data is skewed, which is the case of our test dataset, the total number of similarities to compute is higher."
"as shown in figure 2, the proposed algorithm is sensitive to adjusting parameters. specifically, it is not very sensitive on most data sets, but it also has subtle changes. this is because the algorithm proposed by us has better robustness. 1  is used to control the value of"
"different approaches exist to build a k-nn graph. some of them tolerate incorrect edges to speedup the building process and produce an approximate graph, while others produce an exact graph. in both cases, these building algorithms are closely related to nearest neighbor search algorithms. but when it comes to building a k-nn graph from a big unstructured text dataset, where each node consists of a string, none of these offer an efficient solution."
"the number of buckets produced by the hashing function is l c . if we assume the input strings are uniformly distributed over the buckets (if data is not skewed), the number of strings per bucket is n l c . [cit] experimentally found that the computational cost of nn-descent is around o(n 1.14 ). as we use nn-descent inside the buckets to build the subgraphs, the number of similarities to compute is:"
"netfs [cit] : this method is also an unsupervised feature selection algorithm, but the algorithm is mainly for networked data. due to the large amount of noise in networked data, the algorithm combines latent representation learning for feature selection. through sparse learning and latent representation learning, the algorithm can remove the noise interference well and finally achieve good robustness."
"to compute the similarity between spam subjects, we use the jaro-winkler distance [cit] . this measure of string similarity is normalized such that zero equates to no similarity and one is an exact match."
"before embedding, scs are combined with watermark bits to form a binary sequence denoted by -th bit of watermark (fig. 2) . basics of our watermark embedding are shown in fig. 3 and detailed as follows:"
"the rest of this paper is organized as follows. in section 2 we present existing algorithms to build a k-nn graph, and algorithms that perform nearest neighbor search in general. in section 3 we present the implementation details of our algorithm. in section 4 we show experimental results, and compare our algorithm with a sequential and a mapreduce implementation of nn-descent. finally, in section 5 we present our conclusion and directions for future work."
"to asses the robustness of our approach, different attacks are performed: -noise: white gaussian noise (wgn) is added to the watermarked signal until the resulting signal has an snr of 20 db. -filtering: filter the watermarked audio signal using wiener filter."
"step 6: evaluate the similarity between the extracted segment and bit by bit. if the similarity value is, then is taken as the sc and go to step 8. otherwise proceed to the next step."
"note that 1 α is convex but not smooth. so using approximate gradient to optimize α, we can update iterations α by the following rules."
"k_ofsd [cit] : the algorithm is an online feature selection algorithm, which mainly selects relevant features through neighbor learning. through this method, the class imbalance problem can be effectively solved. at the same time, the dimensionality of the high-dimensional data is effectively reduced by the dependency between the conditional features and the decision-making class."
"we present here the design of an enhanced version of nnctph. the algorithm, presented in algorithm 1, requires a single mr job. in the map phase, the algorithm uses a modified ctph function to produce a hash of each input string. this hash value is then used to bin the string into a bucket. each reduce task builds a k-nn graph of the strings in the bucket. we experimentally found that, for small datasets, the naive method requires less computations and processing time than sequential nn-descent. therefore, if the number of strings in the bucket is smaller than a given threshold θ, the reduce task uses the naive method, otherwise it uses nn-descent."
"ndfs [cit] : the algorithm is a new unsupervised feature selection algorithm that mines discerning information. specifically, it imposes a non-negative constraint on the class indication to learn clustering tags more accurately. at the same time, 2, 1 l  norm is used to remove redundant features. it is an algorithm that performs clustering and feature selection simultaneously."
"in this paper we presented our implementation of nnctph, a mapreduce algorithm that builds an approximate k-nn graph from large text datasets. we used datasets containing the subject of spam emails to experimentally test the influence of the different parameters of the algorithm on the quality on processing time and on the quality of the final graph. we also compared the algorithm with a sequential and a mapreduce implementation of nn-descent. for our datasets, the algorithm proved to be up to ten times faster than the mapreduce implementation of nn-descent, for the same quality of produced graph. moreover, the speedup increased with the size of the dataset, making nnctph a perfect choice for very large text datasets."
"we also implement a complete mapreduce (mr) version of nn-descent, that we compare to our algorithm. we run nn-descent on our dataset, and for each iteration we measure the total running time and recall. the results are shown on figure 3 . on the same figure, we also present the results of previous experiments. as we can see, in some cases nnctph runs 6 times faster than nn-descent for the same quality of produced graph, but the attainable recall is limited. this is mainly due to the principle of binning itself. at some point, the hashing function has to produce different hashes for different input strings. this means that two similar strings, that differ by only one letter, may receive different hashes. therefore they will be binned into different buckets, which makes the creation of an edge between them impossible. we mitigate this effect using multiple stages, but the resulting attainable recall is still limited to roughly 50%, as shown on figure 1 . we can also reduce this effect by using less buckets, and more strings per bucket, but this increases the computational cost of the algorithm, as shown on"
"to control the number of buckets, and hence the number of strings per bucket, we modified the original ctph function to: i) produce a hash of variable size;"
"we tested our proposed unsupervised feature selection algorithm on four binary data sets and eight multi-class data sets. they are yale, colon, lung_discrete, glass, spectf, sonar, clean, arrhythmia, movements, ecoli, urban_land and forest, where the first three data sets are from feature selection data, and the last nine data sets are from the uci data set. the details of the data set are shown in at the same time, we found eight representative feature selection comparison algorithms to compare with our proposed algorithm. the main introduction of the algorithm is as follows:"
"in figure 3, we show the objective function values for each iteration of the proposed algorithm, on 12 data sets. we set the stop criteria of the proposed algorithm to 4 ( 1) ( ) ( ) 10 obj t obj t obj t"
", we added an orthogonal limit to a, in order to fully consider the correlation between the output variables. we add a 1 l  norm of α for sparse learning and feature selection [cit] . finally we get our final objective function as follows:"
"fsasl [cit] : the intrinsic structure of the data is rarely considered for existing feature selection algorithms. by placing structural learning and feature selection in a framework at the same time, the algorithm can effectively select representative features while maintaining the structure of the data (i.e., structural learning and feature selection complement each other)."
"in the future we plan to further study the quality of the produced graphs: until now we have an idea of how many edges were correctly discovered by our algorithm, but we would like to know which edges are correctly detected, and which ones are not. we also want to study the influence of graph quality on the post-processing algorithms that use an approximate knn graph (e.g. connected components). we will have to tackle the problem of skewed data, and study the possibility of using multiple hashing functions in parallel to improve recall. finally, we plan to compare our algorithm with algorithms that build a k-nn graph using the bag-of-words (bow) model."
"in table 3, we can see the average standard deviation of each algorithm on 12 data sets. the standard deviation of the proposed lsk_fs algorithm is the smallest, indicating that our algorithm has the best stability. the lsk_fs algorithm can achieve such good results, mainly for the following two reasons: 1. consider the similarity between data features. 2. fully consider the nonlinear relationship between data features."
the idea of the proposed watermarking method is to hide into the original audio signal a watermark together with a synchronized code (sc) in the time domain. the input signal is first segmented into frames and emd is conducted on every frame to extract the associated imfs ( fig. 1) . then a binary data sequence consisted of scs and informative watermark bits ( fig. 2) is embedded in the extrema of a set of consecutive last-imfs. a bit (0 or 1) is inserted per extrema.
"rsr [cit] : the algorithm is a new unsupervised feature selection algorithm that learns by regularized selfcharacterization. specifically, if a feature is particularly important, it can be represented linearly by most other features (i.e., a feature can be represented by a combination of other features). in addition, the algorithm shows good performance on both manual and real data sets."
"step 3: embed times the binary sequence into extrema of the last imf by qim [cit] : (2) where and are the extrema of of the host audio signal and the watermarked signal respectively. sgn function is equal to \" \" if is a maxima, and \" \" if it is a minima."
"where x represents the total number of samples and correct x represents the correct number of samples for classification. at the same time, we define the standard deviation to measure the stability of our algorithm, as follows: (32) where n represents the number of experiments, i acc represents the classification accuracy of the i-th experiment, μ represents the average classification accuracy, and the smaller the std, the more stable the representative algorithm."
"is the probability that a sc is detected in false location while is the probability that a watermarked signal is declared as unwatermarked by the decoder. we also use as performance measure the payload which quantifies the amount of information to be hidden. more precisely, the data payload refers to the number of bits that are embedded into that audio signal within a unit of time and is measured in unit of bits per second (b/s)."
"the naive method, also called linear search, consists in computing the distance between the query point and every other point in the set, keeping track of the \"best node so far\" (or k \"best nodes so far\"). similarly, the most naive way to build a complete k-nn graph is is to use brute force to compute all pairwise similarities. then, for each node, the algorithm keeps only the k edges with the highest similarity. this method has a computational cost of o(n 2 ) and is thus very slow, even implemented in parallel."
perceptual quality assessment can be performed using subjective listening tests by human acoustic perception or using objective evaluation tests by measuring the snr and objective difference grade (odg). in this work we use the second approach. odg and snr values of the four watermarked signals are reported in table i . the snr values are above 20 db showing the good choice of value and confirming to ifpi standard. all odg values of the watermarked audio signals are between and 0 which demonstrates their good quality.
to locate the embedding position of the hidden watermark bits in the host signal a sc is used. this code is unaffected by cropping and shifting attacks [cit] .
"step 10: extract the watermarks and make comparison bit by bit between these marks, for correction, and finally extract the desired watermark. watermarking embedding and extraction processes are summarized in fig. 7 ."
"this article has proposed a new feature selection algorithm to remove redundant features. specifically, the local structure learning is applied to the features to take into account the local structure and similarity of the features. moreover, the kernel function is applied to map all the features in the high-dimensional space, so that the nonlinear relationship of the features is linearly separable in the high-dimensional space. in addition, low rank constraints are used to remove noise samples, maintain the global structure of the data and achieve robust results. finally, the experimental results also show the superiority of our proposed algorithm. in future work, we plan to improve our algorithms through robust statistical learning."
"in this paper, we enhance the original design of the algorithm by using nn-descent inside the buckets to reduce the computational cost of the algorithm. we also experimentally test the algorithm on different datasets consisting of the subject of spam emails. we test the influence of the different parameters of the algorithm on the number of computed similarities, on processing time, and on the quality of the final graph. we also compare the algorithm with a sequential and a mapreduce implementation of nn-descent, and show that nnctph largely outperforms state of the art approaches in term of run-time."
"for the data matrix nd   xr, the i-th row and the j-th column are denoted as i x and j x respectively, and the elements of the i-th row and the j-th column are denoted as, ij x . the trace of the matrix x is denoted by () tr x, t x denotes the transpose of the matrix x, and 1  x represents the inverse of the matrix x. we also denote the norm and norm of x respectively as"
"in figure 1, we can clearly see the classification accuracy of the 10 experiments. the algorithm we proposed is not the highest every time, but most of the cases are the highest. in table 2, we can see the average classification accuracy of each algorithm on 12 data sets. the algorithm proposed by us is obviously superior to other comparison algorithms. specifically, it is 4.78% higher than eufs in average classification accuracy and 5.05% higher than fsasl, which indicates that our algorithm is better than the general feature selection algorithm. compared with k_ofsd, ndfs, netfs, rlsr, rfs, and rsr. lsk_fs increased 13.63%, 8.55%, 6.69%, 7.88%, 6.68%, and 3.59%, respectively. in particular, our algorithm is particularly effective on the dataset spectf."
"the resulting running time and recall are shown on to correctly discover 50% of edges in the dataset. increasing the number of stages increases the running time, as expected, but has only a limited effect on recall."
"to analyze the performance of our algorithm, we implement it using hadoop mapreduce and test it on datasets containing the subject of 200.000 spam emails. [cit] . it is mainly used to improve spams signature definitions, and to analyze trends in spam campaigns. we also compare it against our hadoop mapreduce implementations of nn-descent and of the brute-force method. all algorithms are executed on a cluster of 20 worker nodes, each equipped with a four-core processor, 8gb of ram, and four 1gb ethernet cards."
"the unsupervised feature selection algorithm mainly mines more representative features in the data. in the absence of the class label y, using the data matrix x as a response matrix, the internal structure of the original features of the data can be better preserved [cit] . in order to fully exploit the nonlinear relationship of data features. get the following expression: is the kernel matrix."
"the above equation represents the case where y i is the reference distribution and the local score is denoted as s kl . kl-divergence being an asymmetric measure, there are other possible ways to estimate the kl-divergence:"
"the implantation procedure was similar to that reported previously [cit] . briefly, all surgical tools and the microdrive were sterilized by ultraviolet radiation for more than 20 min before implantation. one cranial window of 0.5 by 1 mm was made in either hemisphere centered at a. p. +1.42 mm and m.l. 2.5 mm, and electrodes were lowered at d.v. 3 mm to target the apc. tissue gel (1469 sb, 3m) and dental cement were carefully applied to cover the exposed brain tissue and to fix the microdrive. antibiotics (ampicillin sodium, 20 mg/ml, 160 mg/kg b.w.) was injected for three consecutive days after surgery. behavior training started 7 to 14 days after that. recordings of behaving mice were made with electrodes lowered for approximately 50 mm each day after the last day of shaping."
"in this process it was also required to define the best position to attach the smartphone during the execution of exercises, taking into account comfort and the quality of recorded sensor data. based on exercises characteristics, few possible positions would be admitted, for example, waist, arms, legs or hands. to facilitate the set up of each exercise and their sequence, the less number of locations are considered to attach the smartphone, the better. as the fall prevention program chosen to support this application is mainly focused on arms and legs movements, they could be divided into two groups. with this division it was possible to have only two locations for the smartphone, one on the thigh and the other on the wrist (fig. 2) . in both positions the smartphone is strap to the user with an armband."
"based on the results of computational experiments, the best results of abstract clustering by containing and not containing medical intervention were obtained using the k-means ++ algorithm together with lsa, choosing the fi rst 210 facts. the quality of classifi cation abstracts by subtypes of medical interventions value for existing ones [cit] has been improved using non linear svm algorithm, with \"bag of words\" model and the removal of stop words. the results of clustering obtained in this study will help in grouping abstracts by levels of evidence, using the classifi cation by subtypes of medical interventions and it will be possible to extract information from the abstracts on specifi c types of interventions."
"implantation of optical fiber was carried out in a manner similar to that described previously [cit] . briefly, mice were anaesthetized and placed in a stereotaxic instrument (stoelting co. wood dale, il). after removal of the scalp, periosteum and other associated soft tissues, a custom designed steel plate was fixed onto the skull using tissue adhesive (1469 sb, 3m, maplewood, mn) and dental cement. craniotomies of roughly 1 mm in diameter were made bilaterally above the anterior piriform cortex (apc). for optical fiber implantation, two optic fibers (200 mm in diameter, leizhao biotech., shanghai, china) with ceramic ferrule were implanted at a.p. 1.78 mm, m.l. 2.60 mm and d.v. 3.40 mm. a thin layer of silicone elastomer (kwik-sil, wpi, sarasota, fl) was applied to protect the brain tissue, then dental cement was applied to connect the skull, plate and optical fibers for structural support. antibiotic drug (ampicillin sodium) was injected i.p. for three consecutive days after surgery."
"optogenetic experiments were performed as described previously [cit] . briefly, an optical patch cable (200 mm in diameter, n.a. 0.37) was used to connect the implanted optic fiber (through a ceramic sleeve) to a laser source (bl473t3-50fc, sloc, shanghai, china). laser power was programmatically controlled by a pic microcontroller through analog voltage input. output laser power was measured with a laser power meter (lp1, sanwa electric instrument co., tokyo, japan) and compensated for optical power loss in fiber implant and coupling attenuation (determined before surgery). in optogenetic experiments with laser power ramp, the dynamics were further calibrated by an oscilloscope."
"state-of-the-art automatic speech recognition (asr) systems are based on hidden markov models (hmms). the development of an hmm-based asr system is often decomposed into two problems [cit] . first, the relationship between subword units or \"lexical units\" and acoustic feature observations is modeled. second, the syntactic constraints of the language are modeled."
"where v (i) denotes the set of acoustic unit probability vectors assigned to state l i and m (i) is the cardinality of v (i). sp-hmm is a special case of the tied-hmm approach with the optimal state distribution,"
"we then examined the apc neuronal activity in the dual task. even after the distracting task during the delay period, apc population activity still coded the dpa samples and gng distractors, as shown in the mi analysis of one example neuron in figure 7a . in the apc neuronal population, approximately 30% of neurons can code for both the dpa samples and the gng odors (figure 7b )."
"due to some restrictions related to the size of the smartphone and the required positioning of the device (i.e. to enable the sensors to read meaningful data about the movement performed), not all of the exercises included in the programme were appropriate to be adapted to the smartphone. therefore, those related to neck retractions and rotations were not considered for evaluation using the smartphone. exercises focused on the thoracic spine, upper members, ankle and lower limbs were all selected to be included in the fall prevention tool. the extension of the thoracic spine is one of those exercises. it is performed with the elderly sitting on a chair and with arms abducted and external rotated, holding this position for a while, which will improve flexibility and muscle endurance."
"in this work, we compared probabilistic lexical model based systems with deterministic lexical model based systems. in deterministic lexical model based systems either the acoustic model is adapted on target language data or both acoustic and lexical models are trained on target language data. in our studies we observed that, with increase in target language acoustic data, the gap between kl-hmm and acoustic model adaptation based systems reduces. this suggests that there may be benefits in combining acoustic model adaptation and probabilistic lexical modeling, especially when more training data is available."
"each exercise begins in a different initial position that must be matched to a set of pre-defined values so that the person is always forced to begin the exercise correctly, in the pre-defined position. to do this, the smartphone orientation is perceived and, when it matches the desired orientation (by means of predefined values), the exercise can begin. at this moment, the application starts to collect all the metrics associated with the corresponding exercise. these metrics are summarized in the table 1. table i. exercise metrics"
"previously we showed that the delay-period activity of the mpfc was important during the learning but not during the well-trained phase in the olfactory wm task [cit] . therefore, it is pertinent to examine the roles of different brain regions in learning and well-trained mice. the results of our optogenetic and recording experiments demonstrated that the apc delay activity was important in the well-trained phase. thus, the functional role of memory maintenance may be partially transferred from the mpfc to the apc through learning. the underlying mechanisms of the transfer and the potential involvement of other brain regions remain to be determined."
"hmm-based asr is a statistical asr approach, where given an acoustic likelihood estimator, a lexicon and a language model, the most likely word sequence w * is achieved by finding the most likely state sequence q *,"
"as mentioned in section 5.1.2, it is possible to directly decode the hiwire test set using language-independent acoustic and lexical models without any adaptation. the performance on the hiwire task for the kl-hmm, sp-hmm, tied-hmm and the language-independent hmm/gmm systems is given in table 5. the lexical model for the kl-hmm, sp-hmm and tied-hmm systems is trained on the speechdat(ii) english data. it can be observed that, for both phone and grapheme subword units the kl-hmm system performs better than the sp-hmm, tied-hmm and li hmm/gmm systems. also, it is interesting to note that irrespective of the subword units used, the performances of all the probabilistic lexical model based systems (that use context-independent phones as acoustic units) are better than that of the li hmm/gmm system (that uses context-dependent phones as acoustic units)."
"in order to quantify the specific contribution of apc delay-period activity to performance, when compared to other explaining task parameters, we designed seven candidate generalized linear models (glms) by systematically varying the combinations of task parameters in fitting the dnms and control task performance ( figure 3g ). a good model should exhibit a high coefficient of determination in explaining performance (r-squared) and a low akaike information criterion (aic, see materials and methods), which punishes a higher number of free parameters. such formal model comparison can quantitatively reveal the relative contributions of different behavioral parameters in determining performance [cit] ."
"it can be observed from eqn. (20) that the first quantity, the entropy of probability distribution z t, is independent of the lexical unit. the matching only takes place with the second quantity which is the cross entropy between distributions z t and y i, with z t as the reference. the local score s skl (y i, z t ) gives equal importance to the acoustic and lexical models. another difference between the kl-hmm and tied-hmm/sp-hmm approaches is that the kl-divergence based local scores can be linked to hypothesis testing [cit] . the acoustic model evidence and lexical model evidence is matched discriminatively irrespective of the local score used. we use these distinctions to better explain our findings in section 6."
"in the kl-hmm approach, the local score is based on kl-divergence. however, two posterior probability distributions can be compared with different cost functions such as scalar product or bhattacharya distance [cit] . it is possible to envisage an hmm where the local score is based on the scalar product, i.e.,"
the hypothesis is validated by training a single language-independent multilingual acoustic model and conducting asr studies on the following three different resource-constrained tasks where only a lexical model is trained:
"in practice, phone-based asr system development can be seen as a two stage process: development of pronunciation lexicon followed by asr system training. pronunciation lexicon development is a semi-automatic process. usually, given an existing manually developed or verified lexicon, a grapheme-tophoneme (g2p) converter is trained to extract pronunciations for new words or to add pronunciation variants [cit] . the augmented lexicon is then used to build an asr system. however, for some languages, a seed lexicon may not be available to train a g2p convertor. therefore, alternate subword units like graphemes, which make lexicon development easy, have been explored in the literature [cit] ."
"the performance on the test set of the scottish gaelic corpus for the kl-hmm, sp-hmm, tied-hmm, tandem and hmm/gmm systems for the orthography-based and knowledge-based grapheme lexica is given in table 6. the map system was not investigated for the knowledge-based lexicon due to the mismatch between the acoustic unit set and the lexical unit set. it can be observed that the systems using the knowledge-based grapheme lexicon perform better than the systems using the orthography-based grapheme lexicon. this shows that integrating orthographic knowledge specific to the language in a grapheme lexicon can help in improving the performance of grapheme-based asr systems. the kl-hmm systems perform better than all the other systems. the tandem system performs better than the hmm/gmm system. furthermore, the map, sp-hmm and tied-hmm systems perform worse than the tandem and hmm/gmm systems. finally, in the case of the orthography-based lexicon, the map system is not able to capitalize on the language-independent data."
"finally, the last two recommended exercises are the voluntary stepping (in forward, lateral and backward direction), which reduce the sensation of balance loss, and walking every day, which improves gait speed and helps the user to keep active. to be effective, these exercises need to be performed regularly at home, which sometimes creates motivational problems on the senior and an interesting problem to be solved in order to keep prevention exercises effective."
"to temporally dissociate information maintenance from perception and decision making, we first trained head-fixed mice to perform an olfactory delayed non-match to sample (dnms) task [cit], using an automatic training system [cit] . in this task, two odors (s1, n-butanol; or s2, propyl formate) were presented to mice, separated by a delay period (5-40 s, figure 1a,b, video 1). the first odor residue was briskly cleared (figure 1-figure supplement 1, the delay-period residual concentration is lower than the sensory threshold for this task [cit] ) at the beginning of the delay period. licking following the non-match trials (s1-s2, or s2-s1) led to water reward. no explicit punishment was applied for the error trials. mice readily performed the task with little licking during the delay period ( figure 1c, video 1, video 2). a hallmark of wm is a progressive decay of performance with increasing delay duration [cit], which was indeed observed in the task (figure 1d )."
"we observed a small effect of optogenetic suppression on behavioral performance with shorter duration of optogenetic suppression. for example, the effect size was à0.8 and à0.68 for 5 s and 8 s delay duration, respectively, which were statistically smaller than the effect size of à1.62 observed for the 12 s delay (figure 1h-j) . also, there was no significant drop in performance when the apc was suppressed in early delay period (figure 2b and d) . furthermore, in the fixed optogenetic suppression varied delays task, the effects of the optogenetic suppression of apc were also quite variable (figure 2g) . one possibility may be that our optogenetic suppression was quite focal and spared large areas of apc, as shown by the reduction in c-fos stained active neurons (figure 1-figure supplement 2c) . therefore the simpler tasks with shorter-delay duration imposed less demand on apc delay-period activity, and the remaining unaffected areas within apc might be sufficient to maintain the information. another possibility is that there could be a loop of activity involving apc and other regions (see below for more discussion). thus, suppression of activity in other parts of the brain might contribute to our observations. moreover, the partial optogenetic suppression could act like a damper on recurrent activity, with a slow decay rate. this would produce the observed larger effect of the longer illumination (figures 1h-j and 2b) . the observation of a smaller optogenetic effect for a short delay duration is consistent with other studies. [cit] showed that optogenetic silencing of the medial prefrontal cortex (mpfc) impaired spatial wm performance in a task with 60 s but not 10 s delay duration. the optogenetic silencing impairment in dpa performance is smaller than that in the dnms task (comparing figure 4 with figure 1 ). the designs of the two tasks are quite different, for example, the dpa task requires the association between sample and test odors, which is not required for the dnms task. therefore, the neural circuit for the dpa task might be different from that for the dnms task, which remains to be determined in future studies."
"in the literature, the lack of acoustic resources has been typically addressed through approaches that exploit multilingual or crosslingual acoustic and lexical resources [cit] . the first step in most of these approaches is the definition of a common or universal phone set across all out-of-domain languages and the target language. this step ensures that the phone sets match across languages, thus addressing the third constraint mentioned above. the common or universal phone set can be defined either in a knowledge-based manner [cit] or in a data-driven manner [cit] . multilingual acoustic models are first trained on the language-independent data and then adapted on the target language data."
"in this study, we also investigate a grapheme lexicon that does not use any knowledge, such as broad and slender consonants. we refer to it as the orthography-based lexicon. this lexicon is prepared in the traditional way from the orthography of words. the orthography-based lexicon consists of 32 gaelic graphemes comprising 25 graphemes, 5 accents and silence. table 2 summarizes the information about the different corpora used."
"it has been shown that the number of stimuli used for training can bias the strategies of the animals [cit], and this limits the generality of the results when using just two odorants as the sample odors. we therefore trained the mice with a dpa task using six odorants as samples (s1-s6) and two odorants as test (t1 and t2). in this task, the s1, s2 and s3 odors were paired with the t1 odor, whereas the s4, s5 and s6 odors were paired with the t2 odor (multiple-sample dpa, figure 6a ). despite an initial relearning each day, mice performed this task well ( figure 6b) . as for the dnms task with two sample odors, we observed many neurons with sample-odor information during the delay period, as revealed by mutual information (mi, figure 6c, see materials and methods), which measures the degree to which responses are informative about the identity of the stimulus. a similar result was also obtained with svm decoding analysis (figure 6d )."
"some of the following methods are similar to those previously published [cit] . we utilized the dnms task, dpa task and dual-task as described previously [cit] or as described below. in brief, the olfactometry apparatus was enclosed in sound-proof training boxes. an embedded system, custom built around a pic digital signal controller (dspic30f6010a, microchip, chandler, az), was used to control the olfactory cue and water delivery by switching solenoid valves and to detect lick responses with an infra-red beam break detector or a capacitance sensor. the air-and-odorant-mixture nozzle/lick-port/beam-breaker assembly were 3d-printed and placed in front of the mouse. the air flow rate was controlled at 1.35 l/min during and between odor deliveries. propyl formate (245852, sigma-aldrich, st. louis, mo), butyl formate (261521, sigma-aldrich), methyl butyrate (246093, sigma-aldrich), 3-methyl-2-buten-1-ol (w364703, sigma-aldrich), 1-butanol (b7906, sigma-aldrich), 1-pentanol (398268, sigmaaldrich), ethyl propionate (112305, sigma-aldrich) and propyl acetate (133108, sigma-aldrich) were used at 1:500 concentration in mineral oil (v/v; o1224, fisher scientific, pittsburgh, pa), or stock odorants are kept and evaporated in an air-tight bottle. during odor delivery, the odorant vapor was exposed to individually controlled air flow, then mixed with air at 1:10 concentration (v/v). the concentration of odors was measured using a photoionization detector (200b minipid, aurora scientific inc, st., aurora, canada), and the concentration during the delay fell to the baseline level within 1 s after valve shut-off. behavior events and timings were simultaneously sent to and recorded by a computer using customized software written in java (oracle, redwood shores, ca)."
"the aim of this project is therefore to develop a smartphone-based solution capable of guiding an elderly person throughout a set of exercises in a safe and easy way, as well as evaluate and provide feedback about the user performance. this set of exercises is presented to the user as a serious game, increasing the motivation to perform them frequently at seniors' home."
"it also has a lot of built-in services out of the box like the gps, gyroscope, accelerometer and magnetic sensor. in addition to it android offers portability for current and future hardware. this portability is only possible because all of the programs that run in android are written in java and are executed by a virtual machine called dalvik [cit] . this virtual machine is optimized to compile the java class files taking into account the device limitations such as memory, processor speed and power [cit] . this design makes possible to write java code to be run on the device and therefore it is easy to develop to android-based programs."
"in addition to these sensors, android has some built-in sensor fusion techniques available by software and hardware. sensor fusion is a process of combining multiple sensors data in order to obtain more accurate results [cit] . with these techniques it is possible to smooth the noise produced by sensors like the accelerometer, or compute device's orientation relative to the earth. in this sense, accelerations due to gravity can be discriminated from those due to movement, therefore improving the accuracy of measurements. on android the result of sensor fusion is used as a sensor, e.g. the gravity sensor that gives the value of the gravity after being processed according to the accelerometer readings. modern devices include virtual or hardware sensors capable of measuring data with respect to the global coordinate system."
"after recording the exercises with android it was necessary to process that information. a program in python was used to read files and process the recorded signals. to obtain a visual analysis of the results it was used a python 2d plotting library, the matplotlib. with this approach it was possible to build a plot for each sensor, exercise and experimental positioning of the smartphone."
"for the permutation test, the absolute difference between the means of two groups of samples was calculated, then the two groups were pooled together. for a number (usually 1000) of repeated permutations, the pooled data were regrouped into two subsets that matched the original two groups in size, then the absolute differences between the means of the subsets were calculated and stored in a vector. the p value is the probability that the permuted difference is equal to or larger than the test group difference. the paired permutation test was similar to the permutation test, but the test data were mean differences between data pairs, and the sign (positive or negative) was randomly flipped in each repeat of the permutation."
"working memory (wm) is a function of the brain that supports the active maintenance and manipulation of information over a delay period of several seconds [cit] . a buffer between recent external inputs and immediate behavioral outputs, wm is a critical component of cognition [cit] . originally the prefrontal cortex was thought to be the specific site for wm [cit] . however, later results suggested that parallel distributed circuits are responsible for wm [cit] . previously, we have shown that the delay-period activity of the medial prefrontal cortex (mpfc) of mice is only important during the learning but not the well-trained phase in an olfactory wm task [cit] . here, we seek to elucidate the contribution of a sensory cortex to wm after mice are welltrained."
"in the framework of hmm/gmm systems, multilingual acoustic models or the gmms serve as the seed models to be adapted on the target language data using techniques such as maximum a posteriori adaptation (map), maximum likelihood linear regression (mllr) and subspace gaussian mixture models (sgmm). the out-of-domain lexical model or the decision trees are either retained [cit] or redefined using target language data [cit] . in the framework of hybrid hmm/ann systems, the multilingual ann can be used for the target language local emission score estimation after phone set mapping [cit] . other possibilities are training a hierarchical neural network, adapting the multilingual ann or the last layer of the multilingual ann on the target language data [cit] etc."
"the potential and efficacy of the proposed approach is demonstrated with experiments and comparisons with other standard approaches on three asr tasks. the standard asr approaches considered for comparison are the acoustic model adaptation and tandem approaches that exploit language-independent resources, and the hmm/gmm approach that uses only the target language data. the paper is organized as follows: section 2 provides a background on standard hmm-based asr systems and elucidates the deterministic lexical model aspect in theory and practice. section 3 presents implications of deterministic lexical modeling. section 4 presents three different probabilistic lexical modeling approaches, their potential implications and the proposed approach. sections 5 and 6 present the experimental setup and the results, respectively. finally, in section 7 we provide a discussion followed by a conclusion."
"it is worth mentioning that in hmm-based asr literature, due to this deterministic relationship, typically no distinction is made between the acoustic and lexical units, or the acoustic and lexical models. our main reason to refer to the lexical and acoustic units, or the acoustic and lexical models distinctly here is to bring out the contributions of the present paper clearly."
"not have any phonetic lexical resources. the g2p relationship of gaelic is regular, and many-to-one as the number of graphemes in a word is significantly higher than the number of phones ."
"the two theories make opposite predictions in perturbation and recording experiments. if a sensory cortex is important, behavioral performance should be impaired following suppression of delayperiod activity in a sensory region in a temporally precise manner. moreover, in a task with distractors during the delay, the neuronal activity of sensory regions should be able to maintain information even following a distractor. if a sensory cortex is not important, both experiments should produce negative results. in the current study, we specifically tested these predictions in the olfactory domain in behaving mice, using optogenetic perturbation [cit] and electrophysiological recordings with the necessary temporal specificity."
"where the x i is the value read from the sensor, and a is the smoothing value. in this case the value of a is 0,1, which is a decent choice for a smooth plot. in addition to the filter it was also necessary to amplify the signal. the original signal is squared in order to maintain the signal curvature unchanged. in order to have only positive values before squaring the signal, a safe value is added. therefore the x i is:"
"hmm in the three probabilistic lexical modeling approaches discussed, the local score estimation at time frame t can be seen as a match between \"bottom-up\" acoustic information z t or v t and \"top-down\" lexical information y i, as shown in figure 1 . yet another similarity between the three approaches is that they reduce to the standard hybrid hmm/ann system described in section 2 when the lexical model is deterministic, i.e., y i is a kronecker delta function. despite these similarities, the kl-hmm approach has additional advantages compared to the tied-hmm and sp-hmm approaches. we discuss them briefly in this section."
"a part of the speechdat(ii) corpus, specifically, british english, italian, spanish, swiss french and swiss german, is used as the language-independent dataset. each language has approximately 12 hours of speech data, in total amounting to 63 hours. all the speechdat(ii) lexica use sampa symbols. a multilingual phone set of 117 units obtained by merging phones that share the same symbols across the above mentioned five languages serves as the acoustic or the subword unit set."
the hardware design and software source code for the automatic training system [cit] and the custom-written computer codes that support the findings of this study can be obtained at github [cit] (https://github.com/wwweagle/; copies archived at https:// github.com/elifesciences-publications/behaviorparser; https://github.com/elifesciences-publications/ zmat; https://github.com/elifesciences-publications/ephysparser; https://github.com/elifesciencespublications/spk2fr; https://github.com/elifesciences-publications/pic20odor and https://github. com/elifesciences-publications/serialj).
"1. in the acoustic model, the relationship between latent variables and acoustic features is modeled. 2. in the lexical model, a probabilistic relationship between latent variables and lexical units is modeled."
"from the experiments presented earlier in this section, it can be observed that despite using exactly the same acoustic model, the performance trends of the various probabilistic lexical modeling approaches are different. the kl-hmm system performs better than the deterministic lexical model based systems in under-resourced conditions and performs similar to the deterministic lexical model based system in well-resourced conditions. while, the sp-hmm and tied-hmm systems show gains over the deterministic lexical model based systems mainly in under-resourced conditions (see tables 3 and 4 ). we attribute the superiority of the kl-hmm system to its abilities discussed in section 4.4. in order to ascertain the reason for difference in performance trends among the various probabilistic lexical modeling approaches, we conducted the following study. on the hiwire task, with the 150 minute target data condition, the lexical model trained using the kl-hmm rkl approach is decoded with a viterbi decoder using various local scores, namely, s kl (y i, z t ), s skl (y i, z t ), s tied (y i, v t ) and s sp (y i, z t ). the study was conducted for both graphemebased and phone-based systems. the results of this study are given in table 7 . it can be observed that decoding with kl-divergence based local scores s rkl (y i, z t ), s skl (y i, z t ) and s kl (y i, z t ) results in better performance compared to decoding with s sp (y i, z t ) and s tied (y i, v t ) local score. this result indicates that kl-divergence based local scores are better than scalar product based local scores. furthermore, decoding with s kl (y i, z t ), s sp (y i, z t ) and s tied (y i, v t ) yields lower performance than decoding with s rkl (y i, z t ). however, decoding with s skl (y i, z t ) that gives equal importance to the acoustic and lexical model yields performance similar to s rkl (y i, z t ). it can also be noted that the lexical model trained using the kl-hmm approach and decoded with s sp (y i, z t ) and s tied (y i, v t ) local scores results in better performance compared to the lexical model trained using the sp-hmm and tied-hmm approaches and decoded with s sp (y i, z t ) and s tied (y i, v t ) local scores. this indicates that the kl-hmm approach with the local score s rkl yields a better lexical model compared to the sp-hmm or tied-hmm approaches. deeper investigations on these aspects are out of the scope of the present paper."
"the kl-hmm approach using the local score s kl (y i, z t ) where y i is the reference distribution reflects the hmm-based asr. more specifically,"
"in the dnms task optogenetic sessions (figures 1h-1n and 2b-2d, and figure 1 -figure supplement 5), the optogenetic suppression was arranged in an interleaved one-trial-on, one-trial-off fashion; each session started with a laser-off trial. in the fixed delay, varied optogenetic stimulation tasks, various optogenetic stimulation designs, including laser-off trials, were pseudo-randomly carried out on a trial-by-trial basis. in the dpa task and dual-task optogenetic sessions (figure 4b,g,h and figure 4 -figure supplement 1), the optogenetic suppression was arranged in an interleaved one-block-on, one-block-off fashion, and each block consisted of 24 trials; each session started with a laser-off block."
"each context-dependent subword unit was modeled using three-hmm states and each hmm state was modeled using a mixture of 16 gaussians. then, map or mllr adaptation 3 was performed using speech data from the target language or domain. for mllr adaptation, we a use regression class tree to group the gaussians in the model set into regression classes and we use up to 32 regression classes."
"the population decoding analysis was performed with the libsvm library (https://www.csie.ntu.edu. tw/~cjlin/libsvm) according to the library's documented instructions. briefly, the spike counts for each neuron during well-trained correct task trials were binned into 500 ms windows and grouped by sample odors. only neurons with more than 30 well-trained trials for each sample odor were included to minimize overfitting. the firing rates for all neurons were normalized to the [cit] range. for each repeat, the trials were first randomly partitioned to a training set (no fewer than 30 trials) and a testing set (one trial for each sample odor, unless stated otherwise), with no overlap between the training and testing sets. from the training set, the firing rates of all neurons in 30 bootstrap trials for each sample odor were selected to train the support vector machine (svm) with the radial basis function (rbf) kernel. from the testing set, one trial for each sample odor was used to test the classification accuracy. all trials were then randomly re-partitioned and the training and testing repeated. averaged decoding accuracy was obtained with 500 repeats. because the training and testing sets never overlapped and all trials were likely to be used both in training set and testing set due to the large repetition number, the average decoding accuracy equals the cross validation accuracy in a resampled leave-one-trial-out form. a good combination of the two parameters for the rbf kernel, c and g, were grid-searched using exponentially growing sequences in the ranges 2^ [à5, 5] and 2^[à10, 0], respectively. for the shuffled control, the procedure was repeated 1000 times with the nominal sample odor for each trial randomly redistributed. for the cross temporal decoding analysis, the c and g values were kept at the previous obtained value, and the decoding accuracy was calculated using template and test activity vectors from different time windows across the entire length of a trial. for the study of correlation of behavioral performance and population neuronal decoding (figure 7f ), the decoding accuracy was obtained using averaged firing rates from 1 s before the test onset to the test offset, to reflect the neuronal activity that is important to decision making in the dpa task."
"all the exercises are evaluated in real-time therefore realtime processing is required. real-time analysis is made by comparison of sensors' readings with fixed threshold values able to detect and confirm the current orientation of the smartphone, which may vary in line with the movement. once again, the ankle movements' exercise is an exception, since threshold values used may vary according to the initial orientation of the smartphone. to overcome this problem, each time the exercise starts the initial position is processed, and then the signal amplitude is evaluated taking into account the initial orientation of the device."
we then elucidate that in standard hmm-based asr systems the lexical model is deterministic. the deterministic lexical model imposes constraints such as: the latent variables and the lexical units have to be of the same kind; the acoustic resources from target language or domain are required to train or adapt both the acoustic model and the lexical model.
"given these observations, we hypothesize that the proposed grapheme-based asr approach can address both acoustic and lexical resource constraints better than acoustic model adaptation based approaches developed in the framework of deterministic lexical modeling."
"to engage active wm maintenance, we trained mice to perform several wm tasks, including a dual-task paradigm. in this task, we inserted a distracting task into the delay period of an ongoing wm task. to perform the dual task successfully, mice need to maintain the sample information of the outer wm task while performing the distracting task. such a dual-task design explicitly challenges the central executive control ability of a subject [cit] ) . a hallmark of the dual task is that performance in either of the component tasks is reduced relative to that trained in isolation, termed dualtask interference. previously, the neural correlates of dual-task interference have been reported in the neuronal activity of monkey prefrontal cortex [cit] and in results from human functional imaging [cit] . patients with a frontal-cortex lesion are also impaired in this type of task [cit], but the functional role and neural correlates of sensory cortices in dual-task performance remain unknown."
"in previous study on scottish gaelic asr, a knowledge-based grapheme lexicon that tagged word beginning and end graphemes was used and wordinternal context-dependent graphemes were modeled . the kl-hmm and hmm/gmm systems achieved a word accuracy of 72.8% and 64.8%, respectively. in this work, the same knowledge-based grapheme lexicon was used but without any word begin and end tags. as a result, the total number of grapheme subword units is smaller. furthermore, in this paper we modeled cross-word context-dependent subword units. as can be seen from table 6, the knowledge-based hmm/gmm system yields an absolute improvement of 3.2% word accuracy compared to the previous work and the grapheme kl-hmm system achieves performance comparable to that of the previous study."
"all recorded neurons were single units. wide band signals (0.5-8000 hz) from all tetrodes were amplified (â20,000) and digitized at 40 khz with the multichannel acquisition processor (plexon inc, dallas, texas) and all data were saved to hard-disks. detection and sorting of spike events were performed offline as described in a previous paper [cit] . briefly, offline spike detection was performed with offline sorter (plexon inc). raw signals were filtered in 250~8000 hz to remove field potentials. signals larger than five times the standard deviation recorded on any recording site of the tetrode were considered to be spike events. principle component analysis (pca) was performed for tetrode-waveforms to extract the first three principle components (pcs) explaining the largest variance. then, the contour or valley-seek clustering techniques provided by offline sorter were performed in 2d or 3d feature space (including principle components) of waveforms. single units were included only if no more than 0.15% of the spikes occurred within a 2 ms refractory period (false-alarm rate) and the averaged firing rate was higher than 2 hz. all of the neurons that we sorted exhibited large signal-noise ratios and small false-alarm rates, demonstrating the high quality of single-unit sorting (figure 1-figure supplement 3) . all recording sites were further confirmed by passing current (50ma, 100 ms, 1 hz x five pulses) through the electrodes, and verified with dapi staining and immunochemistry 1 day after the lesion ( figure 5-figure supplement 1c )."
"the two main approaches used in the literature to model the acoustic units are gaussian mixture models (gmms) and artificial neural networks (anns). the resulting asr systems are usually referred to as hmm/gmm [cit] and hybrid hmm/ann [cit] systems, respectively."
"finally, it is relevant to mention the biggest handicap of this prototype, which is the single member evaluation. the exercises must be performed using both limbs (upper or lower), but the smartphone is only capable of evaluating one limb (as it is attached solely to one of our limbs). this problem can be overcome using external sensors. new sensors could be placed on the user's leg or arm reducing the need of changing the mobile phone position according to the exercise performed and at the same time evaluate both limbs. the main drawback of this approach is the addition of an extra device that also adds an extra cost to the system."
"ageing is one of the biggest problems faced by our society. [cit] 18,7% of european population will have more than 65 years old, against 15,6% of young people. furthermore, this gap tends to increase through the upcoming years with the index of ageing growing up from 120 [cit] to near 200 [cit] ."
"depending on the subword context modeled, there are two types of asr systems: (1) context-independent subword unit based asr systems, where lexical units are context-independent subword units, and (2) context-dependent subword unit based asr systems, where the lexical units are context-dependent subword units."
"alternatively, in the case of tandem approaches, the multilingual ann is used to generate data-driven bottleneck or tandem features for the target language. these data-driven features are used to train an hmm/gmm system for the target language [cit] . to fit the target language better, the multilingual ann is sometimes adapted on the target language data with [cit] or without [cit] phoneset mapping. however, in the tandem approach, as the acoustic and lexical models are trained on the target language data, minimal resources from the target language are necessary to robustly estimate the parameters."
"most of these constraints can be overcome with today's technologies. modern video game consoles, for example, are able to identify users' movements; therefore they can be used as a strategy to help users perform exercises at home. however, it will restrict the exercises execution to the place where the console is. moreover a console can be quite expensive to be used as a facilitator of fall prevention exercises. another technology available is the computer, or, preferably, due to its portability, the laptop [cit] . the main disadvantage is that they require the connection of external devices to detect user movements and an additional configuration step, which might reduce the usability of the whole system."
"1. multilingual phone-based acoustic models are sharable across languages. as discussed in section 3.1, many acoustic model adaptation approaches addressing acoustic resource constraints in asr system development exploit this aspect. 2. as mentioned in section 4.5, when the acoustic units are based on phones and the lexical units are based on graphemes, probabilistic lexical modeling techniques such as kl-hmm are capable of learning a probabilistic g2p relationship. [cit] ) . these grapheme-based asr systems performed similarly to phone-based asr systems, where the target domain phone lexicon is built by training a g2p converter on a cross-domain phone lexicon. this suggests that probabilistic lexical modeling approaches with lexical units based on graphemes and acoustic units based on phones could address lexical resource constraints by integrating lexicon learning as a phase in training the asr system. 3. the probabilistic g2p relationship could be learned on a relatively small amount of target-domain transcribed speech [cit] . further, such a grapheme-based asr system performed better than conventional phone-based acoustic model adaptation systems."
"another set of exercises can be evaluated always in the same position, but in this case on the leg. active ankle dorsiflexion and plantar flexion, performed in a seated position, will develop better gait mobility and therefore improve daily actions like stair climbing. the exercise of sitting down and standing up from a chair will be able to improve lower limb strength. routine activities like dressing can also be improved with the single leg stance exercise, which will enhance equilibrium and therefore, avoid falls."
"in order to reduce the number of falls, it is important to identify the most common fall risk factors and apply specific fall prevention strategies to reduce those risks. reduced muscle strength, slowed pace of movements, poor balance and an obvious decrease of physical activity are some examples of fall risk factors [cit] . an effective strategy to prevent falls is the practice of physical exercise, preferably targeting specific risks."
"other brain regions may contribute to the maintenance of sensory information in a wm task (as discussed in the previous section). the hypothetical existence of these regions does not conflict with the current results showing that a sensory region plays important roles in wm maintenance. the exact mechanism underlying the distributed neural circuit that underlies wm remains to be determined in the future. furthermore, the functional role of other sensory regions [cit] and the potential interaction between the apc and the mpfc [cit] in the learning and well-trained phases of wm tasks remain to be determined."
"so far, several concerns were presented and need to be taken into account so that platform requirements are fulfilled. the platform must incorporate several sensors capable of detecting movement, acceleration, and relative position. in addition it must be programmable and allow the communication with the sensors. nevertheless it must be affordable to the masses and have an easy and interactive interface specially adapted to elderly people."
"the optogenetic manipulation during the delay period might impair sensory perception during the test-odor delivery period. to exclude this possibility, we suppressed apc activity before odor delivery in three tasks. the logic of the experiments was to recruit perception in similar tasks that do not require delay-period wm. if no behavioral deficit was observed by silencing apc activity, any impaired performance in the dnms task should not be due to processes unrelated to delay-period wm maintenance. in all three tasks, we observed intact behavioral performance (figure 3a-f, figure 3-source data 1-2; effect size, figure 3-figure supplement 1b) . first, we optogenetically suppressed apc activity before the sample-delivery period (baseline) for 3 s (delay duration 5 s), 6 s and 10 s (delay duration 12 s) in the dnms task, matching the delay-period optogenetic suppression (figure 3a-c) . second, mice were trained to perform a sensory-discrimination go/no-go (gng) task (figure 3d) . the apc activity before the test odor was optogenetically suppressed (figure 3d and e). third, mice were trained to perform a non-match-to-sample without delay (nms-wod) task (figure 3d ). the essence of the design of this task is that the decision is based on matching the relationship between sample and test odors, as in the dnms task. the third task takes longer to train (~300 trials) than the go/no-go task, which can be learned within 100 trials in one day, suggesting a higher level of attention and effort. however, there is a minimal delay between odors, and therefore no requirement for information maintenance. apc activity before and after test-odor delivery was suppressed in the task (figure 3d and f). the lack of behavioral impairment in all three control experiments excluded the perception-impairment hypothesis ( figure 3 -source data 1-2). therefore, the behavioral deficits following optogenetic perturbation of apc delay-period activity reflected the contribution of this activity to information maintenance in the dnms task."
"we started with the null-hypothesis that no task parameter affects performance (#1, figure 3g,h). adding the variables of both the sensory cues (#2) and the time constant for memory decay (#3, from figure 1d ) improved the performance of the models, consistent with the obvious importance of sensory cues and delay duration in this task. the genotypes, laser on/off, and the perturbation in delay/baseline period did not further improve the model if added individually (#4). however, the interaction among all these terms improved the model (#5), consistent with impaired performance by optogenetic suppression during the delay-period for the vgat-chr2 mice, but not during the baseline period or for the control mice. by eliminating less-predictive variables, we obtained the optimal model (#6, aic value significantly smaller than all other models; figure 3g -i, figure 3 source data 3 and 4), which contained only four key parameters and the interactions among them: source data 1. the optogenetic suppression effects were not due to impaired sensory perception. optogenetic suppression or not during the delay period, match or non-match relationship, and the delay duration (coupled with laser-on duration). this association between the wm delay perturbation and the delay duration is consistent with the important role of apc delay activity in information maintenance (figure 3j) . quantitatively, the impact of optogenetic perturbation out-weighted that of delay duration (figure 3j) . the above analysis further demonstrated the importance of apc delay-period activity for wm in the olfactory dnms task."
"in context-independent subword unit based asr systems, the deterministic relationship between lexical and acoustic units is knowledge driven. therefore, lexical model training is not involved, and the deterministic map between lexical and acoustic units is the lexical model. the gmms in the case of the hmm/gmm approach or the ann in the case of the hybrid hmm/ann approach is the acoustic model."
"the accelerometer on android is a set of tiny masses on tiny springs. these springs bend whenever an inertial force acts, being able to measure it. it can measure the forces applied to the device, either the earth's gravity or the forces resulting from shaking the device [cit] ."
"1. irrespective of the subword units used, kl-hmm systems perform better than deterministic lexical model based systems when there is limited training data and comparable to deterministic lexical model based systems as the training data is increased. 2. on both tasks, the difference in performance between phone and grapheme-based systems is minimal for the kl-hmm approach compared to all other approaches. 3. on both the hiwire (where the g2p relationship is irregular) and greek (where the g2p relationship is regular) tasks it can be observed that deterministic lexical model based systems are more suitable for phones than graphemes."
"1. the availability of sufficient and well developed acoustic data in the target language or domain to effectively train both an acoustic model and a lexical model. 2. the availability of a well developed phonetic lexicon, as most of the asr systems use phones as lexical units. 3. the asr system trained with one phone set cannot be directly ported to or used as it is for a new domain which has a lexicon based on a different phone set. for a language, it can happen that there are different phonetic lexicons based on different phone sets. for instance, in english there are phonetic lexicons based on arpabet, cmubet, sampa etc."
"the linear acceleration sensor is used to detect the number of times a person forces to maintain a certain position, for example, the number the forcing movements that occurs on the upper limb elevation. as this movement produces low accelerations, the influence of gravity could not be considered, in order to obtain more accurate readings specifically related to movement."
"in this phase it was necessary to find a way to collect and analyze the data gathered from the inertial sensors. to record this data, was built a simple android application was built. this application has access to the data read by the sensors and saves the information in an external file. acquisition of signals can be performed using different sampling rates. in android, it is possible to define four different default rates: the fastest, the game, the ui and, finally, the normal [cit] ."
"it is worth mentioning that kl-hmm was originally developed as an alternative acoustic modeling technique to the tandem approach [cit] . however, as shown recently and briefly explained in this section, kl-hmm is a probabilistic modeling approach (rasipuram and magimai.- [cit] b,a) . in this paper, we explain and interpret all the literature on kl-hmm in terms of probabilistic lexical modeling."
"such requirements suggest the use of the android platform which provides a wide set of tools that enable a full exploration of the smartphone capabilities. android is an open source and free development platform based on linux. the linux kernel is the base of the whole platform and it is responsible for memory and process management, as well as networking and others system services on android [cit] ."
"at this point of the project there are already some reflections regarding the use of the sensors available on android to evaluate the exercises. in fact, several physical differences exist between each individual; also, for the execution of tests, each individual can use different chairs with different heights. in addition to it, between repetitions the user might place his legs in different initial positions during the same exercise set. this required the adjustment of thresholds for each situation, which requires some time in a static position to get calibration data able to adjust the upcoming sensor readings according to the initial smartphone positioning. however users may not be willing to wait for the calibration process, therefore these adjustments must be done in background during the exercise execution. the main drawback of doing this evaluation on the background is the small number of samples that can be collected and processed during the exercise execution."
"all experiments were performed in compliance with the animal care standards set by the u.s. national institutes of health and have been approved by the institutional animal care and use committee of the institute of neuroscience, chinese academy of sciences (shanghai, china). b6.cg-tg (slc32a1-cop4*h134r/eyfp)8gfng/j mice, commonly referred to as vgat-chr2 mice, were used in optogenetic experiments, and littermates of the same sex were used as controls. all mice were healthy male, group housed, of age 8-12 weeks and 20-30 g in weight at the start of training. for all experiments, individual animals are presented as individual data points, otherwise the sample sizes (n) are shown in the corresponding figures. in each experiment condition, individual data points represent individual animals (biological replicates). data from the same animal across multiple conditions are indicated by color or joint line segments and treated accordingly (e.g., statistical tests with repeated measures). when bootstrap resampling methods were used (technical replicates), this is clearly stated along with the number of resampling repeats. we ensured that each group in the behavior studies included at least 10 mice, which had been shown to be sufficient to detect the effects of optogenetic manipulations in comparable working memory tasks [cit] ."
"the hiwire corpus contains english utterances spoken by natives of france (31 speakers), greece (20 speakers), italy (20 speakers) and spain (10 speak- ers) [cit] . the utterances contain spoken pilot orders made of 133 words. the database provides a grammar with a perplexity of 14.9. the hiwire task does not have training data. it only contains adaptation data of 50 utterances per speaker, approximately 150 minutes and test data of 50 utterances per speaker, approximately 150 minutes. to simulate limited resources the amount of adaptation data is reduced from 150 to 3 minutes (specifically, 150, 120, 90, 64, 32, 16, 10 and 3 minutes respectively) by picking various subsets of utterances [cit] . the grapheme-based lexicon was transcribed using 27 graphemes comprising 26 english graphemes and silence. [cit] and this paper is the following: in the previous work a phone-lexicon based on the arpa-bet phone set supplied with the hiwire corpus was used, whereas in this work we use a phone-lexicon based on the sampa phone set. the phone-lexicon based on the sampa phone set was created by borrowing pronunciations of 102 words that are in common from the speechdat(ii) english lexicon. for the remaining 31 words, we obtained pronunciations by mapping arpabet phones to sampa phones. the main reason to use the sampa phone set based lexicon in this work is to have a shared subword unit set between the out-of-domain lexicon and the target-domain lexicon. this allowed the evaluation of acoustic model adaptation based systems (map and mllr) discussed in section 5.2.2. also, native english is present in out-of-domain resources. therefore, in the case of the kl-hmm, sp-hmm and tied-hmm approaches, the lexical model parameters trained on speechdat(ii) english are adapted using the hiwire adaptation data. additionally, we could also investigate the case where no lexical model or acoustic model adaptation is performed."
"as an acoustic model, we use a standard three-layer multilingual multilayer perceptron (mlp) trained on the language-independent dataset to classify 117 context-independent multilingual phones. more recently, mlps with deep architectures classifying context-dependent clustered phone units have gained lot of attention [cit] . in the present work, we use the three-layer mlp for the following reasons:"
"as shown in table 8, the phone-based kl-hmm system performs better than the approaches proposed in the literature. the grapheme-based kl-hmm system performs comparable to the phone-based systems reported in the literature. it can also be observed from tables 8 and 5 that the phone-based li hmm/gmm system performs similarly to the systems from the literature, whereas the grapheme-based li hmm/gmm system performs worse."
"despite being a work in progress this application can be a promising contribution to fall prevention. a full validation of the exercise's algorithms performance, detection, evaluation and feedback are still required to ensure that proper, useful and reliable metrics are being collected in the exercises."
"the multiple sample dpa (ms-dpa) task is like the dpa task described previously, except that the number of candidate sample odors was increased to six; three of the samples and one test formed rewarded pairs (e.g., s1, s2, s3 and t1), and the other three samples and the other test formed the remaining rewarded pairs (e.g., s4, s5, s6 and t2). the delay duration in the ms-dpa task is 5 s."
in overall this is a new approach to guide and evaluate fall prevention exercises that in combination with a serious game tries to fulfill the lack of motivation that these exercises may generate on the elderly. keep the user motivated through time is essential otherwise the effectiveness of the programme will be compromised. nevertheless it is necessary to ensure that these exercises are totally safe to be performed alone without any risk to the patient.
"can be estimated using the embedded viterbi training algorithm, and the decoding can be performed by replacing the log-likelihood based score in the standard viterbi decoder with s sp (y i, v t )."
"we present asr systems based on standard map and mllr adaptation techniques. for this purpose, multilingual context-dependent phonebased and grapheme-based hmm/gmm systems were trained on the languageindependent data set. the phone-based hmm/gmm system used multilingual phones as subword units."
"also, to improve the range of motion of the shoulder, an upper limb exercise is included. this exercise consists in a simple arm elevation until the maximum range is achieved, also holding this position for a certain amount of time. these two exercises together with the reaching forward exercise (that intends to amend balance) can be evaluated at the level of the arm; therefore, the smartphone can be maintained in the same position during the execution of this set of exercises."
1. it can be seen as a particular case of the tied-hmm approach where the priors in the scaled-likelihood estimation are dropped or assumed to be equal. 2. sp-hmm and kl-hmm differ only in terms of the cost function used for parameter estimation and the local score used for decoding.
"the dual-task design (figure 4c ) explicitly challenges the central executive control ability of a subject [cit] . it is an important tool to study the distributed nature of wm maintenance, which could depend on task complexity [cit] . a simpler sensory memory task could be more dependent on a sensory cortex than a more complex task. thus, if suppressing the delay activity of a sensory cortex in the dual task could impair wm performance, it will certainly implicate this sensory region in active information maintenance in wm. most of the previous studies have focused on prefrontal cortex [cit] . here, we show that the delay-period activity of a sensory cortex can maintain sample information after the distracting task, and that suppressing this activity resulted in performance impairments (e.g. significantly increased false-alarm rate). therefore the delay-period activity of this sensory cortex is involved in dual-task performance."
"the present section is organized as follows. first, we present results on the rapid development of asr systems with both acoustic and lexical resource constraints on the hiwire and greek asr tasks. later, we present results on minority language speech recognition using the scottish gaelic task. the performance of all the systems is reported in terms of word accuracy. tables 3 and 4 summarize the performance in terms of word accuracy on the hiwire and greek tasks for various amounts of language-dependent training data for the kl-hmm, sp-hmm, tied-hmm, tandem, map, mllr and hmm/gmm systems. the results are analysed using figures 3 and 4 along two aspects, namely, comparison of different probabilistic lexical model based systems (section 6.1.1), and comparison of probabilistic lexical model based systems against acoustic model adaptation based systems and standard hmm/gmm systems (section 6.1.2). figure 3 (a) plots the performances of the phone-and grapheme-based kl-hmm, sp-hmm and tied-hmm systems with increasing amounts of training data on the hiwire task. similarly, figure 3(b) plots the performances on the greek asr task with increasing amounts of training data. the figures show that the kl-hmm system consistently performs better compared to the sp-hmm and tied-hmm systems for both phone and grapheme subword units. furthermore, on the hiwire task, the difference between the kl-hmm system and the sp-hmm or tied-hmm systems is more for grapheme-based systems than for phone-based systems."
"this occurs due to the positioning of the smartphone on the thigh. [cit] ieee 15th international conference on e-health networking, applications and services [cit] down movement of the thigh, produced as a consequence of the ankle flexion movement when the user is sitting on a chair. therefore it was necessary to use the fastest rate, to detect and process those variations. in this case, in order to obtain better results it was necessary to smooth the values recorded from the sensors. using a low-pass filtering technique, in this case weighted smoothing it was possible to obtain a more uniform set of readings. this technique involves weighting the newest value against the old mean, such that [cit] :"
"our work demonstrates that the apc is important for active maintenance in olfactory working memory. using optogenetic manipulation in a series of behavioral tasks that temporally isolated the retention of sensory information during the delay period from decision making, we demonstrated that wm performance can be impaired in wm tasks with or without distraction. the control experiments suggested that the behavioral defects of optogenetic perturbation during the delay period may be due to impaired information maintenance. the apc population activity exhibited statistically significant mutual information and decoding power for the odor samples during the odordelivery and delay periods. we observed that only about 10% of apc neurons carried wm information during the late delay period (figure 5g and i) . the small percentage of apc neurons that are involved in coding the maintained information is consistent with the sparse coding previously observed in piriform cortex [cit] [cit] . therefore, although the apc activity is constantly updated by ongoing sensory inputs, this sensory cortex could resist overwriting and could maintain information through population activity in the wm tasks, even when a distracting task is performed during the delay period."
"in a kl-hmm, as both the feature observations and the state distributions are probability vectors, the local score or the match between acoustic and lexical model evidence at each hmm state can be the kullback-leibler (kl) divergence between the feature observation z d t and the categorical distribution y i,"
"to further assess whether apc activity could stably maintain wm during the delay period, we performed a population cross-temporal discrimination (ctd) analysis [cit] . the essence of this analysis is to use the activity at a given time to decode the maintained information (the sample odor) at other time points systematically (figure 5l, see materials and methods). a higher decoding power in the off-diagonal space of a ctd plot suggests more stable information maintenance [cit] . in the ctd analysis of apc neuronal activity, significant decoding was observed in the off-diagonal space in tasks with both 4-s and 8 s delay periods, suggesting stable maintenance of wm information by apc neuronal activity throughout the delay period."
"the performance of deterministic lexical model based asr systems is dependent on the accuracy of the deterministic mapping which is in turn determined by the availability of well-developed resources. more specifically, deterministic lexical modeling imposes the following three constraints:"
"in the second set of experiments, the optogenetic suppression was kept at 3 s in duration and within the late delay period, while the delay duration was varied between 5 s and 20 s, in a trial-bytrial fashion (figure 2f) . we observed significant effect of delay duration on dnms task performance, independent of genotype ( figure 2g performance impairment induced by delay-period optogenetic suppression is not due to impaired perception during the test-odor delivery period"
"on exercises like the upper limb elevation or reaching forward, the readings from the gravity sensor range from negative to positive values, according to the smartphone's orientation relative to the vertical of the earth, being easy to detect when and how the movement is performed. another sensor used in this application is the rotation vector, which is capable of detecting the device rotation during the upper limb elevation."
"the olfactory pathway downstream from the olfactory bulb is organized in a highly parallel manner, as mitral/tufted neurons in the olfactory bulb project to multiple brain regions, including the apc, the anterior olfactory nucleus, the cortical amygdala, the olfactory tubercle, and the lateral entorhinal cortex [cit] . therefore, it was surprising that optogenetic perturbation of just one out of these five brain regions can impair olfactory wm performance, suggesting the importance for wm of the apc among the parallel olfactory pathways."
"at this moment the algorithms to measure and collect metrics from the exercises only have a very preliminary and sketchy evaluation that was performed during the program development. but it is possible to say that the set of exercises performed with the smartphone on the leg are less propitious to involuntary errors. on the other hand the set of the exercises with focus on the arm are much more unstable, as much more random movements can be created by arms and therefore introduce misleading information that can be processed inaccurately."
"systems figure 4 (a) plots the performances of the phone-and grapheme-based kl-hmm, map, mllr, tandem and hmm/gmm systems with increasing amounts of training data on the hiwire task. similarly, figure 4(b) plots the performances on the greek asr task with increasing amounts of training data. we can draw the following inferences from the figures:"
"in context-dependent subword unit based asr systems, lexical units are context-dependent subword units whereas acoustic units are clustered contextdependent subword units. as mentioned in section 2.3.1, the decision trees and the phonetic question set are used to deterministically relate a lexical unit to an acoustic unit. therefore, in context-dependent subword unit based hmm/gmm systems, the decision trees are the lexical model and the gmms are the acoustic model. similarly, in the case of hybrid hmm/ann systems, decision trees are the lexical model and the ann is the acoustic model [cit] ."
"the prototype construction involves two distinct phases. the first one, more exploratory, involves recording sensor data, analyze the data gathered and explore different positions for the smartphone. the second phase is more technical, involving the design of the algorithms to process the movements and provide the correct feedback to the users."
"standard hmm-based asr systems, for various reasons as elucidated shortly in the following subsections, implicitly model the dependency between acoustic feature observation x t and a lexical unit l i through the latent variable or the"
"as this study focusses on grapheme-based asr systems, a grapheme lexicon was developed using 25 graphemes comprising 24 greek graphemes and silence. the acoustic model adaptation systems impose the constraint that subword unit sets of the language-independent data and the target language data match. as greek graphemes are different from roman graphemes, graphemebased acoustic model adaptation systems described in section 5.2.2 were not directly applicable to the greek asr task. this necessitated transliteration of greek graphemes in terms of english or roman graphemes, as given by [ table 1 ]."
"the dnms task was carried out as described previously [cit] . briefly, a sample olfactory stimulus was presented for 1 s at the start of a trial, followed by a delay duration of 4-40 s (mice need to retain the information of the first stimulus (sample) during the delay duration), then a test olfactory stimulus for 1 s, identical to (in match trials) or different from (in non-matched trials) the sample. after a 1-s pre-response-delay, mice were trained to lick in the 0.5 s response window only in non-matched trials. hit or false alarm was defined as detection of lick events in the response window in a non-match or match trial, respectively. similarly, miss or correct rejection were defined as the absence of a lick event in the response window in a match or non-match trial, respectively. a reward of 5 ml water was triggered immediately only after hit; mice were neither rewarded nor punished following other responses. mice were allowed to perform up to 300 trials each day, a consecutive combination of 10 miss and correct rejection trials also triggered the end of the session. only the trials within well-trained performance windows (no less than 80% correct rate within consecutive 40 unperturbed trials) were included in the data analysis, unless stated otherwise. in the fixed delay, varied optogenetic stimulation tasks, the well-trained window was reduced to 20 consecutive unperturbed trials because of the reduced number of unperturbed trials in each block. in the fixed optogenetic stimulation, varied delay task, the well-trained window is defined as no fewer than six correct trials in eight unperturbed trials in a 32-trial block. in the increasing delay duration experiment, the mice were trained to perform the dnms task with 5 s delay to the well-trained criterion, then the delay duration was increased every day; the first 100 trials were included in the the analysis to accomplish direct parallel comparison. an inter-trial interval twice as long as the delay duration separated consecutive trials. in the dpa task, one of two sample odors was presented for 1 s at the start of a trial, followed by a delay duration of 13 s, then one of two different test odors was presented for 1 s. one of the sample odors and one of the test odors formed a rewarded pair, while the other two odors formed another rewarded pair. after a 1-s pre-response-delay, mice were trained to lick in the 0.5-s response window only in paired trials. hit or false alarm was defined as the detection of lick events in the response window in a paired or non-paired trial, respectively. similarly, miss or correct rejection were defined as absence of a lick event in the response window in a paired or non-paired trial, respectively. a reward of 5 ml water was triggered immediately only after hit responses; mice were neither rewarded nor punished following other responses. mice were allowed to perform up to 300 trials each day, a consecutive combination of 10 miss and correct rejection trials also triggered the end of the session. only the trials within well-trained performance windows (no fewer than 80% correct rate within consecutive 40 unperturbed trials) were included in the data analysis, unless stated otherwise. an inter-trial interval as long as the delay durations separated consecutive trials."
"averaged firing rates of individual neurons in well-trained correct trials (between 33 and 160 trials, depending on the specific session) for different olfactory sample were binned into 200 ms periods ( figure 5e and figure 5-figure supplement 3a) . the baseline period was defined as the 1 s before the onset of the sample odor delivery. firing rates from baseline of each trial were averaged to form the baseline activity vectors for each neuron. the mean and standard deviation of this baseline activity vector were used to convert averaged firing rates into z-scores. auroc of all the neurons were calculated with the normalized firing rates using the perfcurve function in matlab. the activities of all neurons following different sample odors were sorted by the differences between sample odors during the delay period, and plotted as a heat map using the 'jet' color-map defined in matlab. firing rate selectivity (figure 5d and figure 5 -figure supplement 3b) was defined as the firing rate following sample 1 minus that following sample 2, divided by the sum of these firing rates, for example (fr s1 -fr s2 )/(fr s1 +fr s2 ), which resulted in a [à1,1] range. the results were plotted as a heat map using the 'jet' color-map defined in matlab."
"all the five considered european languages use the roman alphabet. therefore, a multilingual grapheme set of 47 units was formed by merging graphemes that are common across all languages in the language-independent data set. accents and diacritics are treated as separate graphemes. the grapheme-based hmm/gmm system used multilingual graphemes as subword units."
"for the c-fos activity labeling, three mice of the same vgat-chr2 genotype were optogenetically stimulated in 3-sec-on, 17-sec-off cycles for 50 trials, mimicking the dnms task optogenetic stimulation, while no olfactory cues were presented to reduce background noise."
"android devices define two coordinate systems: one to represent the global coordinate system (i.e. the earth) and one to represent the device coordinate system. inertial sensors are designed to measure motion. accelerometer, gyroscope and magnetometer are some examples of sensors usually available in android devices [cit] ."
"the scottish gaelic speech corpus 1 was collected by cstr 2, university of edinburgh. the experimental setup is similar to that of . the gaelic corpus consists of speech from 46 speakers. the training set consists of 22 speakers and 2389 utterances amounting to 3 hours of speech; the development set consists of 12 speakers and 1112 utterances amounting to 1 hour of speech; and the test set consists of 12 speakers and 1317 utterances amounting to 1 hour of speech. the speakers in the training, development and test sets are different. the vocabulary size is 5000 unique words. the database does not contain a phone pronunciation lexicon. the grapheme-based lexicon contains 83 graphemes comprising 5 vowels, 5 long vowels, 23 broad consonants, 23 slender consonants, 26 consonants and silence. this grapheme lexicon is obtained by considering broad and slender gaelic consonants as separate graphemes. we refer to this lexicon as the knowledge-based grapheme lexicon."
"to investigate the neural correlates of the apc in wm, we recorded single-unit activity using custom-made tetrodes while mice were performing wm tasks (tetrodes, figure 5 -figure supplement 1a-c; example neurons, figure 5a and b; spike-sorting quality in figure 1 -figure supplement 3) [cit] ). in the dnms task, mice were trained with a 4 s delay period. recording was started from the first day of the training. recording electrodes were advanced daily (approximately 50 mm/day after recording sessions). after 2-4 days into the training, the delay duration was increased to 8 s. we recorded 204 neurons from 19 mice while they performed the task with 4 s delay duration and 156 neurons from 20 mice with 8 s delay duration. a subset of apc neurons showed selective activity following exposure to different sample odors during the delay period in correct trials (for examples, see figure 5a -c, figure 5 -figure supplement 2a-i). some of these neurons were able to code for the sample information in single trials (figure 5c and d, figure 5 figure supplement 2c,f and i, as measured by the area under receiver operating characteristic, auroc). the distribution of the auroc for all neurons is plotted in figure 5 -figure supplement 2j and k. in error trials, the coding ability of these neurons was reduced (as measured by auroc, figure 5c and d, figure 5 -figure supplement 2b-i). thus, apc neuronal activity encodes wm information and is correlated with task performance. significant coding for the maintained information in apc neuronal activity was further revealed in neuronal-activity heat maps, separately plotted for different sample odors (8 s and 4 s delay duration in figure 5e and figure 5-figure supplement 3a, respectively), or for the firing-rate selectivity index (figure 5f and figure 5 -figure supplement 3b, see materials and methods). more than 25% (figure 5j and k) . because the electrodes were generally positioned at different depths during the 4-s and 8-s recording sessions (estimated 200 mm apart), the difference in decoding power for the different delay durations could be due to different layers or learning experience. it is nonetheless interesting to see that the fraction of sample selective neuron in the apc was greater in the 8s-delay task than in the 4s-delay task ( figure 5-figure supplement 3f), both in the total fraction of selective neurons (sample-selective for more than 0.5 s) and in the subpopulation that showed longer persistent selectivity (sample-selective for more than 5 s). furthermore, the dynamics of the decoding accuracy in the 8s-delay-tasks was not a passive continuation of the decoding accuracy in the 4 s task, as the decoding accuracy at 5 s since sample onset was much higher in the 8 s task (~85%, figure 5k ) than in the 4 s task (~70%, figure 5j ), consistent with the active recruitment of neurons for the maintenance of information in accordance with task requirements."
"neuronal activity in the apc is associated with olfactory perception, but it also varies depending on brain states, task design, or learning experience [cit] . for this reason, the apc has long been suggested to be an associative sensory region [cit] . consistent with this notion, our results demonstrated the importance of the apc activity beyond sensory perception. in summary, our results underscore the importance of the olfactory sensory cortex in memory maintenance beyond immediate sensory processing."
"the tandem features were extracted by transforming the 117-dimensional outputs of the multilingual mlp described in section 5.2.1, with log transformation followed by principal component analysis. the dimensionality of the output features is either kept the same or reduced to 39."
"after defining the smartphone position based on the recorded data and with a set of valid values from the sensors it was necessary to start creating the algorithms for real-time analysis. the first step was to define the most appropriated sensor rate. the normal rate was chosen for sensor data sampling, as this frequency is able to capture all the information that is present in the movements that constitute the exercises. also, this will save battery on the device. this rate works perfectly for most of the exercises, except for the ankle movements exercise, where the delay of the normal rate is too big to detect small variations."
"a hallmark of active maintenance in wm is resistance against distractors during the delay period [cit] . to test this ability in mice, we added a distracting gng task during the delay period of the dpa task (figure 4d ). this paradigm belongs to the dual-task designs that have been used to study central executive control [cit], because mice are required to split attention in the middle of the delay period in order to perform the gng task, while simultaneously maintaining the sample information of the dpa task (figure 4c ). the design of this task also ensured that any lingering residual sample odor would be flushed by the concentrated olfactory cues of the inner task, therefore the wm but not residual odor is required for the outertask performance. mice were trained to perform the dpa task, then the dual-task (figure 4d,e; red and blue curves). after the initial drop in dpa performance, mice learned to perform dual-task well (figure 4d,e) . dpa performance in the dual-task paradigm was worse than that in the simple dpa task even in the well-trained phase (figure 4d,f), consistent with the dual-task interference observed in humans [cit] and monkeys [cit] . moreover, interference was dependent on the trial types inserted in the dpa delay, with the worst performance for the go-distractor trials (figure 4f) . we then optogenetically suppressed the delay-period activity of apc pyramidal neurons after the distracting gng task. the dpa false-alarm rate within the dual task increased significantly in the laser-on trials in the vgat-chr2 group (figure 4h) . the overall"
"the hmm/gmm systems used 39-dimensional plp cepstral feature vectors as acoustic features. all the phone subword based systems use a phonetic question set and grapheme subword based systems use a singleton question set for the decision tree state tying procedure. the number of mixture components for each of the tasks and the training conditions were tuned on the development set. additionally, for tandem systems, the dimensionality of the feature observations (either 117 or 39 dimensions) was tuned on the development set. the htk toolkit was used to build all the hmm/gmm systems [cit] ."
"in this paper a method is presented to be used as a fallprevention solution, using the smartphone. fall-prevention exercises detailed on argel de melo, c. programme [cit] were adapted to the smartphone, offering the means to guide and evaluate the exercises at home with increased motivation to the user. this study provides the first steps in this adaptation."
"in the literature, there are studies that have been reported on the hiwire task [cit] . despite using the same adaptation and test sets, the studies reported in this paper and the literature differ in terms of the sampling frequency of speech data, type and amount of the out-of-domain data used. first, we compare with studies in which no kind of adaptation was performed."
"it was also necessary to define the most appropriated axis or axes to process data. to understand this need we can look to the fig. 1 where it is possible to see clearly the variation produced in each axis of the accelerometer during the execution of the upper limb exercise. in this movement a variation is produced by the gravity on the y-axis that goes from near -10 m/s 2 at the initial position to 10 m/s 2 when the exercise position is reached. at this stage, the arms are forced to stay in this position with small forcing movements, which are more easily readable on the z-axis of gravity."
"in this work, we showed that asr systems can be rapidly built using a language-independent acoustic model and training only the lexical model on a small amount of target language data. in recent work, it has been shown that the lexical model can be completely knowledge driven and asr systems could be developed for new languages without using any acoustic and lexical resources from the language, i.e., near zero resource asr systems . further, it was also shown that if untranscribed speech data from the target language is available then the lexical model parameters can be adapted in an unsupervised manner to improve the performance of the asr system."
"each point in the figures represents the averaged result from one mouse. therefore the data from one mouse were never tested multiple times in one experiment. the genotype of each mouse would only be revealed after the experiments and statistical analysis of individual mice had been finished. then the behavior performance in trials without and with optogenetic suppression were calculated separately for the experiment group and the control group. for the optogenetic tasks, mixedbetween-within-anova were performed, in which 'between-group' factors were defined by genotype and 'within-group' factors were defined as laser-on and -off trials [cit] ) (ranova function in matlab). statistical significance was defined by the genotype and laser on-off interaction p-value. one-way anova (matlab function anovan) of correct rate change (d correct rate) was performed for chr2 mice across all delay durations."
"in the case of the greek task, as previously mentioned phone-based kl-hmm, mllr, map and hmm/ [cit] [figure 4.3 in page 59 and figure 4 .4 in page 60] have been used as reference. [cit] and this chapter differ mainly in terms of the dimensionality of the tandem features used. [cit] always used 117-dimensional tandem features. in this work, the dimension of features i.e., either 117 or 39 was tuned on the development set for each of the training conditions. we found dimensionality reduction to be beneficial, especially in the low acoustic resource conditions. for example, on the 5 [cit] was 30.2% word accuracy with 117-dimensional tandem features. in this paper, with 39-dimensional tandem features we achieved a performance of 66.9% word accuracy."
the above differences among different kl-divergence based local scores are from the decoding perspective. the details on the role of different cost functions from the training perspective were presented by .
"the present paper focuses on the first problem. to model the relationship between lexical units and acoustic features, transcribed speech data and a phonetic lexicon are required. while this is not an issue for resource rich languages, it is challenging for under-resourced languages and domains that may not have such resources [cit] . in the literature, the lack of transcribed speech data has been typically addressed through multilingual and crosslingual approaches [cit] . in these approaches, first the relationship between lexical units and acoustic feature observations is learned on domainor language-independent data and later adapted on target language or domain data. if the phonetic lexicon in the target language is not available, then the use of alternate subword units such as graphemes has been explored [cit] . however, the lack of both acoustic and lexical resources has rarely been studied in the past (stüker, 2008b,a) . the focus of this paper is on building asr systems for languages and domains that lack both a phonetic lexicon and transcribed speech data."
"unfortunately, many languages do not have well-developed acoustic and lexical resources [cit] . in the following subsections, we provide a literature survey on how the resource constraints have been addressed in the framework of deterministic lexical modeling."
"associated to an aging population, new social and economic problems arise, which constitutes a challenge to our society. particularly, falls represent a serious and common problem affecting older persons, as this age group is particularly prone to falls and injuries. it is known that around the world one out of three seniors experience falls each year [cit] and this number rises to nearly 50% when we are talking about institutionalized people [cit] . the economic impact of falls is associated to health care costs that are particularly relevant when a fall leads into a serious injury, for example, a fracture, and the person tends to stay at the hospital for an extended period of time [cit] . additionally, a fall may create a fear of falling again, which will cause loss of independence and a reduced quality of life of the patient [cit] . as elderly people are broadly affected by the occurrence of falls, it becomes extremely important to find ways to reduce the number of falls and consequently to improve their quality of life."
"for all experiments with blind design, rq hou, hm fan and zq chen labeled the mice with unique numbers without revealing the genotype; they did not participate in the behavior or optogenetic experiments. the genotype of mice would only be revealed after the experiments and statistical analysis of individual mice had been finished. furthermore, all of the mice participated in the same behavior studies in identical sequences in the task design, so there is no need for further randomization of samples in this study."
"there is an ongoing debate concerning the necessity of delay-period activity of sensory cortices in wm. the 'essential' [cit] and 'unessential\" [cit] theories argue for and against the importance of sensory cortices, respectively. perturbation during the delay period specifically is a key experiment that can disentangle the debate. [cit], temporal suppression of the delay-period activity of an early sensory region should produce behavioral defects if sensory regions are essential for wm maintenance. our experimental data are consistent with this latter prediction, suggesting that a sensory region can be important for olfactory wm."
"in this paper, we first show that the modeling of the relationship between lexical units and acoustic feature observations can be factored into two parts or models, namely, the acoustic model and the lexical model through a latent variable."
"eqn (10) can be seen as a match between the acoustic and lexical model evidence, which in this case turns out to be the scalar product of y i and v t ."
"it is expected that this prototype will soon evolve into a serious game, offering an attractive design and a scoring system that may motivate the user to play the game design, that is being thought, has two very distinctive approaches. one requires a set-top box (for example a google tv) connected to the smartphone where the user can move objects shown in the television using the smartphone movements performed during the exercise. the other alternative is to use the mobile phone screen as interface but this has some limitations, mainly because during the exercise execution the user is not able to correctly see the screen. to overcome this problem the interaction may be done through audio, playing specific sounds at key positions of the exercise execution."
"this signal processing produces a much linear plot, as it is visible on the fig. 3 . in this case, it is possible to see clearly the four local maximums of the y-axis gravity corresponding to four plantar flexion movements. the main problem associated to this approach is that the values on the graphic cannot be easily associated to the normal value of gravitational acceleration, around 9,8 m/s 2 . fig. 3 . plantar flexion recorded with gravity sensor."
"these are hmm/gmm asr systems where both the acoustic model and the lexical model are trained on the language-dependent data. we investigate two systems: the hmm/gmm system that uses standard cepstral features as feature observations, and the tandem system that uses tandem features as feature observations [cit] . as indicated in table 1, the tandem system exploits both language-dependent and language-independent resources similarly to probabilistic lexical model based systems and acoustic model adaptation based systems."
"physical exercises are known to improve older person's muscle mass, increase muscle strength, and more importantly, improve balance. currently, several exercise programs exist and are suitable to be performed by the older person alone or in group, without compromising their safety [cit] . unfortunately, fall prevention exercises are rarely applied. this occurs due to the high costs of transportation to clinical environments where group exercises are performed in a daily or weekly basis [cit] . furthermore, when the senior is alone at home, he/she does not have the motivation required to perform these tiresome exercises. having these problems in mind it is necessary to find solutions to increase the adherence to the fall prevention exercises, and also increase the awareness about the benefits these exercises represent to the older person, therefore increasing their motivation and the \"treatment\" compliance."
"in the svm decoding analysis based on apc neuronal activity during the later delay period after the distracting task, one can decode the dpa sample odors in the dual-task design in the trials without distractor, with the no-go distractor, or with the go distractor (figure 7c-e) . as mentioned in the previous optogenetic results, the dpa performance was related to the distractor type used in the recording sessions (figures 4e and 7f) . interestingly, we observed that the svm decoding accuracy during the delay period after the distractor task was also related to the distractor type and to performance in the distractor task (figure 7f), suggesting that apc selectivity is important for performance in the dpa task in the dual-task design."
"when we think about new and emerging technologies we are thinking on tablets and smartphones, devices that are extremely powerful with high levels of mobility and usability. both devices have built-in sensors that can be used to evaluate movements performed by the user. in particular, the smartphone, due to its reduced dimensions, can be attached to certain part of the body, being able to evaluate specific movements of the user."
grapheme table 5 : performance in terms of word accuracy on the hiwire test set using systems trained on the speechdat(ii) data. the li hmm/gmm system refers to the multilingual hmm/gmm system trained on the language-independent (li) data.
"as mentioned in section 3, in the deterministic lexical modeling framework, acoustic model adaptation and lexical model adaptation can be combined in different ways. for instance, (a) by combining acoustic model adaptation with polyphone decision tree state tying [cit] or (b) using the sgmm approach [cit] . comparing probabilistic lexical modeling and deterministic lexical modeling along these lines with graphemes as subword units is part of our future work. our studies, in addition to showing the efficacy of the proposed approach, also explicated that it is the constraints imposed by the deterministic lexical model that demand the availability of well-developed acoustic resources and phonetic lexical resources from the target language. furthermore, our investigations also showed that the deterministic lexical model based asr approach is more suitable for phone-based asr than grapheme-based asr, while the probabilistic lexical model based asr approach is suitable for both."
"the smartphone approach isn´t too expensive and nowadays almost everyone has a smartphone that can be used for fall prevention purposes. exercise's evaluation will enable the monitoring of several parameters, which can be used to monitor the patient's evolution. therefore, it will enable collecting, processing and measure fall prevention exercises using just a single device, the smartphone."
"after the adaptation to a serious game it is important to test and validate the game design with the target audience, in this case with elderly people. in this validation, the most important aspect that requires to be validated is the motivation that the game will produce on the elderly."
"the role of sensory cortices in working memory is debated. on the one hand, the 'essential theory' stressed the importance of sensory cortices in wm [cit], arguing that it is more parsimonious to receive, process, and maintain information in the same regions. some recording and functional imaging experiments supported the hypothesis that sensory cortices can exhibit information-selective delay-period activity in wm tasks [cit] .furthermore, perturbation of neural activity in sensory areas can impair wm performance [cit] . on the other hand, the 'unessential theory' stressed that sustained activity emerges as a property of association cortices, but is not present in early sensory cortices, at least in the visual, somatosensory, and auditory domains [cit] . more importantly, a distractor presented during the delay period diminished delay-period sustained activity in sensory cortices without changing performance [cit], arguing that sensory cortices may not be important for wm tasks [cit] ."
"as described in the previous section, in standard hmm-based asr systems the lexical model is deterministic and the pronunciation lexicon (θ pr ) determines the lexical unit set l and the acoustic unit set a. as a consequence:"
"while micro-computed tomography (micro-ct) is the current standard for obtaining high-resolution images of bone and other tissues, the large amount of radiation involved prevents its clinical use, limiting its application to isolated samples or small animals [cit] . by contrast, mtexture is designed to be implemented clinically for diagnosis and monitoring of disease. mri does not involve ionizing radiation, but existing mr methods cannot achieve the resolution of micro-ct. in traditional mri, measurements are made in the spatial frequency domain; the raw data matrix is referred to as k-space, which is fourier transformed to obtain the final image. rather than acquiring a two-dimensional (2d) image, mtexture finely samples one point of k-space at a time to obtain high-resolution data in the spatial frequency domain, at frequencies relevant to the texture of the targeted tissue. hence, mtexture is not limited by patient motion as in traditional mri, and can probe smaller length scales than existing mr methods."
"in this work, we introduce and perform an initial validation of the mtexture technique and the diagnostic ratio metric, using simulated measurements on a relatively small sample of four human vertebrae. although these four bones provide a large set of vois for analysis and the results show promising classification performance, future work will examine the characteristics of trabecular bone across a larger dataset from a wider demographic range of individuals, in order to determine the performance of the proposed diagnostic across the population in clinical settings."
"histomorphometric analysis of micro-ct images is accomplished using a bruker ct-analyser, or ctan [cit] . all images must be thresholded before histomorphometric analysis can be performed. the images are thresholded with 2d otsu thresholding [cit], followed by a 'despeckling' process in which black and white speckles, which are artefacts of image noise, below a specified threshold size are removed. analysis performed in the ctan gives average tb.th and tb.sp values for dataset ae12l2 of 0.15 mm and 0.71 mm, respectively. f60l3 has slightly higher tb.th and tb.sp of 0.19 mm and 0.81 mm, respectively. while the sample is non-osteoporotic, it is taken from a considerably older patient. the tb.th values fall within ranges reported in the literature for human vertebral bone, though the tb.sp is slightly low [cit] ."
"while this initial study focuses on trabecular bone, the methods described can be generalized to other biological tissues. mtexture can be used to investigate textural changes at scales down to approximately 40 mm (and smaller, with the use of machine learning techniques) in a variety of tissues, including the development of fibrosis in the lungs, liver, heart or kidney; the degradation of neuronal architecture with alzheimer's and other neurodegenerative diseases; and the formation of tumours marked by angiogenesis, thereby informing diagnosis at early stages of disease. furthermore, mtexture can be implemented clinically as a short, non-invasive procedure that can be repeated over time to monitor disease progression."
"patient motion severely affects traditional mri at the resolution required to image the fine texture of biological tissues. even when the patient holds their breath during imaging, cardiac pulsatile motion and twitching can cause blurring. imaging at higher resolution lengthens the data acquisition time and worsens the motion-induced blurring. furthermore, on top of the longer times required to image higher k-values (shorter wavelengths), signal strength weakens as k increases. because mtexture trades acquisition of a full 2d image for a high-resolution profile at a few chosen k-space values, the acquisition time required to obtain relevant frequency-domain information about the tissue is vastly reduced. mtexture acquires measurements from one k-value on the time scale of milliseconds, small enough such that blurring due to patient motion is negligible."
"we simulate mtexture measurements on micro-ct images of ex vivo human vertebral specimens obtained from a local organ bank and scanned at the cartilage tissue engineering lab (cte) at the university of california, san diego (ucsd). there are two image sets generated from two specimens from two non-osteoporotic patients, and two from an osteoporotic patient. each set comprises image slices of one vertebral body. the specimens were obtained from different vertebrae; the non-osteoporotic image sets were taken from l2 and l3, and the osteoporotic image sets from th10 and th11. the nonosteoporotic vertebral bodies were obtained from a 75-year-old female (f60l3) and a 32-year-old male (ae12l2); neither patient had any bone-related diseases at the time of death. the two osteoporotic vertebral bodies were obtained from a 52-year-old male who died of chronic obstructive pulmonary disease and labelled ae15th10 and ae15th11. all images have a voxel size of 9 mm isotropic."
"we also note that characterization of bone strength depends not solely on the geometry and histomorphometry at this approximately 100 mm mesoscale, as can be probed with mtexture, but also on the micromechanics of bone constituents, such as mineralized collagen fibrils, at smaller scales [cit] . mtexture is unable to probe these scales, but a combination of mesoscale textural measurements and microscale mechanical modelling can provide a more complete characterization of bone strength."
"all samples used in this study were cadaveric; thus, this study is not classified as human subjects research by the institutional review boards of uc santa barbara or uc san diego, and no ethical assessment was required. animal ethics: an animal ethics assessment does not apply to this study."
"trabecular architecture is quantified with histomorphometry, the study of the shape and form of tissue, typically from analysis of high-resolution images. commonly used histomorphometric parameters include trabecular thickness (tb.th), which quantifies the average thickness of the trabeculae; trabecular spacing (tb.sp), which quantifies the average width of the gaps between trabeculae; and trabecular number (tb.n), which measures the average number of trabeculae per unit length [cit] . trabeculae erode and perforate with age and the onset of disease, resulting in a decreased tb.th; tb.n decreases as well, resulting in an increased tb.sp [cit] . histomorphometric parameters can thus serve as informative diagnostic markers for the health and strength of trabecular bone. values for tb.th and tb.sp are typically reported as an average value over a region; however, the thickness and spacing can be highly variable throughout a volume of bone. measures of variability in tb.th and tb.sp, e.g. moments or other characteristic quantities of their distributions, may provide further diagnostic information. the anisotropy of the trabecular structure has also been shown to be predictive of bone mechanics [cit] ."
"note that at higher erosion radii, such as in figure 2e, the trabecular elements can be eroded to the point of splitting in two, uniting gaps on either side of the elements. this can also result in isolated trabecular elements artificially created from the erosion process, though these are typically small enough to be identified and removed through the despeckling procedure. we also find that trabecular number decreases with increased erosion radius (figure 3)."
"while in this paper we focus on probing trabecular bone through isolated vertebral samples that have been washed to remove soft tissue, mtexture can be used to measure multiple chemical species in a tissue that may have differing spatial compositions. unlike typical mri, mtexture can probe large enough regions with signal averaging to map chemical species as a function of wavelength. with volume selection and no k-encoding, mtexture can be used to measure the nmr spectrum in order to correlate chemical species with the measured textures. one potential application is in characterizing inflammation, as the water signal of healthy tissue may be relatively organized compared to inflamed tissue, in which the water may have migrated, resulting in a more disordered composition."
"we verify that a bone voi can be correctly labelled as healthy or osteopenic/osteoporotic (eroded) based on the ratio metric. that is, we use the ratio metric as the sole input feature for two-class classification. we determine the decision boundary using a support vector machine (svm) with a linear kernel function, implemented using matlab (the mathworks inc., natick, ma, usa). we perform fivefold cross-validation and calculate the average sensitivity (fraction of eroded bone correctly classified) and average specificity (fraction of healthy bone correctly classified) to assess the classifier; we repeat this process a total of 50 times to minimize the effect of the partitioning of the data on the classification accuracy. overall, we find that the sensitivity and specificity can vary significantly depending on the chosen analysis direction. for a classifier trained and tested on vois taken from dataset ae12l2, choosing the anterior -posterior analysis direction gave the highest for a classifier trained and tested on vois from both healthy vertebral datasets combined, the mediallateral direction gave the highest sensitivity (0.920 + 0.003 for the 4-voxel case, 0.847 + 0.004 for the 2-voxel case) and specificity (0.946 + 0.003 for the 4-voxel case, 0.873 + 0.006 for the 2-voxel case). the corresponding data and decision boundary are shown in figure 6e,f. sensitivities and specificities, as well as the average ratio metric values, for each analysis direction and dataset are tabulated in the electronic supplementary material, tables s1 (for the 2-voxel case) and s2 (for the 4-voxel case). moreover, for a diagnostic application, the decision boundary could be moved in order to prioritize minimizing false negatives, for example, at the expense of increasing the number of false positives."
"we introduce an mr technique called mtexture, which can be used to rapidly acquire high-resolution information, at scales approximately 40 mm, about the complex architecture of biological tissues. focusing on the specific case of osteoporosis in trabecular bone, we identify a diagnostic marker figure 8 . comparing ratio metric distributions for healthy, eroded and osteoporotic bone. each plot compares the ratio metric histograms for baseline (blue) and eroded (orange) vois from datasets ae12l2 and f60l3, and the ratio metric histogram for the thresholded vois from each of the osteoporotic datasets (ae15th10, green, a; ae15th11, yellow, b). as in figure 6, the ratio metric was calculated using the medial -lateral analysis direction. owing to the much smaller number of osteoporotic vois, counts are normalized such that the total number of counts in each histogram equals 1. rsos.royalsocietypublishing.org r. soc. open sci. 5: 180563 called the ratio metric that is predictive of deterioration in both osteoporotic and artificially eroded bone samples. importantly, we demonstrate in silico that the ratio metric can be determined from only a few kspace values, which can be acquired rapidly with mtexture in small targeted regions within a bone. this procedure provides diagnostic information without the need to acquire an entire 2d mr image or even a 1d spectrum, thus avoiding the motion limitations that have previously limited the ability to probe complex bone architecture in vivo. by enabling the acquisition of predictive structural information in a short and non-invasive clinical procedure, mtexture has the potential to supplement traditional bonedensity measurements and significantly improve the detection and monitoring of osteoporosis."
"trabeculae erode and perforate with age and disease, leading to wider spaces between them. the accumulation of microcracks and breakage in ageing bone contributes to its fragility. at the same time, the bone is constantly remodelling itself; old bone is resorbed and replaced with new bone, but an imbalance between resorption and formation results in osteoporotic bone loss, characterized partly by diminished bmd [cit] . however, while bmd can explain the variance in the mechanical strength of trabecular bone only up to about 70%, a combination of bmd and structural properties such as anisotropy can explain up to 90% [4 -6,13,14] ."
"while mtexture is an mr technique, it is not a procedure that is applied to existing mr images. rather, it is a technique for obtaining frequency-space data using clinical mr equipment without acquiring full 2d or 3d images. in this paper, we first introduce and detail the mtexture technique for probing biological tissues by measuring the mr signal at specific spatial frequencies relevant to the tissue texture. we then conduct an in silico validation of mtexture by simulating mtexture measurements, using micro-ct data as ground truth, to obtain spatial frequency information associated with trabecular structure. we start by transforming high-resolution micro-ct images into spatial frequency data, and extract a subset of this data at frequencies specifically chosen to be relevant to the structure of trabecular bone. we use the simulated mtexture measurements to calculate a ratio metric, which we then use to train a classifier to distinguish between healthy bone and bone that has been artificially eroded to simulate osteopenia and osteoporosis. we apply this classifier to bone with osteoporotic characteristics to show that the ratio metric can be used to identify diseased bone, indicating that a full 2d image is not required to yield diagnostic information derived from bone architecture."
"osteoporosis, a metabolic bone disease which leads to increased bone fragility and fracture propensity, affects millions worldwide and results in a significant economic toll. in the usa, osteoporosis affects approximately 10.2 million adults over age & 2018 the authors. published by the royal society under the terms of the creative commons attribution license http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited. heart disease and greater than that of cancer [cit] . the annual costs of osteoporotic fractures have been estimated at 20 billion usd in the us and 30 billion usd in the european union [cit] . osteoporosis is diagnosed by estimating bone mineral density (bmd), typically accomplished using dual-energy x-ray absorptiometry (dxa) or quantitative computed tomography (qct). however, bmd is poorly correlated with fracture likelihood; there is considerable overlap in bmd between healthy individuals and fracture patients [cit], and it has also been shown that bmd cannot fully explain the variance in strength-related properties of bone [cit] ."
"mtexture uses a custom pulse sequence (figure 1) to isolate a small, targeted region, which is typically a prism with one dimension (1d) designated as the 'analysis' dimension and the other two the 'cross-section' dimensions. within one mtexture excitation, the prism is excited and phaseencoded for the desired k-value or values (hereinafter referred to as a k-encode), and the signal is measured. up to approximately 10, k-values can be measured in one repetition time (tr; the time rsos.royalsocietypublishing.org r. soc. open sci. 5: 180563 interval between excitations), though figure 1 describes an example procedure in which one k-value is measured in each tr. these steps can be repeated several times within the same analysis volume and the magnitude of the signals can be averaged to improve the signal-to-noise ratio. the signals from several different non-overlapping prisms, in a technique called interleaved acquisition, can also be acquired in one tr. additional wavelengths can be probed by repeating the encoding of other k-values over subsequent trs, thereby building up a sampling of k-space pertinent to the pathogenesis of a disease."
"in order to obtain a fourier spectrum that contains information regarding the texture of the trabecular bone, the length of the analysis dimension should be long enough to contain several repeats of the 'pattern' of trabecular bone and spacing. furthermore, the prism should be relatively narrow in the cross-section dimensions, such that averaging over these dimensions does not result in excessive washing-out of structure. in practice, however, narrowing the cross-section size, while helpful in delineating structure, will also reduce the signal-to-noise ratio."
"to computationally validate the effectiveness of mtexture, and to identify optimal cross-section sizes and other measurement parameters for diagnostic power, we simulate mtexture data acquisition using test datasets constructed from micro-ct scans of ex vivo vertebral bone samples. we transform the micro-ct scans into frequency space and extract the signal intensities at frequencies relevant to trabecular bone texture. we first simulate mtexture measurements on the trabecular bone within two healthy vertebral bodies, labelled ae12l2 and f60l3. an example of a micro-ct image slice from vertebral body f60l3 is shown in figure 2a. we also simulate osteoporotic bone by artificially eroding the healthy bone images; examples of eroded regions at different levels of erosion are compared with a baseline thresholded image in figure 2c-e. furthermore, we compare our results with those from two osteoporotic vertebral bodies, labelled ae15th10 and ae15th11; an example image slice from ae15th11 is shown in figure 2b."
"ae15th10 and ae15th11, in comparison, have lower tb.th values, but also lower tb.sp values. the average tb.th for ae15th10 and ae15th11 is 0.072 and 0.073 mm, respectively, while the average tb.sp is 0.45 and 0.44 mm, respectively. however, these two datasets exhibit lower bone volume fractions of 10.7% and 10.0%, compared to 15.7% and 16.5% for ae12l2 and f60l3, respectively."
"as measurements from one k-value are done in a single tr, they are inherently immune to motion during signal recording. the protons in the volume of interest (voi) are independent, without coherence or interference effects, and the proton spin direction is decoupled from the molecular orientation. the encoded spins move with the tissue regardless of translation, rotation or distortion of the tissue; as long as the voi stays within the receiver and the homogeneous magnetic field, the signal is not affected. furthermore, because mtexture probes texture, rather than acquiring an image, there is no need for precise spatial coherence between subsequent excited volumes. hence, in a series of k-encodes over several trs, each measurement is independent. thus, mtexture is tolerant to motion across excitations, and this motion immunity is not tied to the fast (milliseconds-long) acquisition but to the fact that data within a chosen k-value are acquired within a single tr."
"the frequency bands used for calculating the ratio metric contain 5 frequency points (for the low-frequency band) and 19 frequency points (for the high-frequency band). we investigate whether narrower bands, which would correspond to fewer mtexture measurements, result in a significant figure 5 . comparing frequency-space intensities of healthy and simulated diseased bone. the fourier spectra of a baseline thresholded voi and the same voi eroded to two different extents (2-voxel radius, simulating osteopenia; and 4-voxel radius, simulating osteoporosis) are shown. the baseline fourier spectrum is the same as that shown in figure 4e. each individual dotted line is generated by isolating a 1 â 1 â 5 mm prism generated from stacking micro-ct images, collapsing it in the cross-section (1 â 1 mm) dimensions, then taking the fourier transform of the 5-mm-long 1d spatial signal. the thick lines are generated by averaging together the dotted lines of corresponding colour, such that each thick line represents a (5 mm) 3 voi. the pink shaded areas correspond to the low-and high-frequency bands used in calculating the ratio metric ( figure 4 ). rsos.royalsocietypublishing.org r. soc. open sci. 5: 180563 decrease in classification accuracy. we keep the same low-frequency band, but use a narrower high-frequency band of [3.8, 5.6] mm 21, which contains 10 points, to calculate the ratio metric, with a medial-lateral analysis direction. we train the svm classifier on these values of the ratio metric for figure 7 illustrates the change in classification accuracy as a function of total sample cross-sectional area."
"a question arises as to whether a smaller region of bone can provide sufficient diagnostic information. we determine the svm classification accuracy when varying the number of sub-samples within each (5 mm) 3 voi, i.e. varying the size of the cross-sectional area of the prism targeted by mtexture. we systematically increase the number of sub-samples between one (a 1 â 1 â 5 mm prism, and thus the smallest possible resolvable cross-section) and 25 (constituting the entire voi). for classifying healthy and 4-voxel eroded bone, we found that the svm accuracy is significantly lower when the cross-sectional area is less than 5 mm 2 ( figure 7) . however, for larger areas, the accuracy exhibits no significant trend, and any small variation in the accuracy could be attributed to small variation in the bone itself."
"architectural parameters can be readily calculated from 2d images of trabecular bone with histomorphometry software. however, extracting structural information from frequency-domain data within a small subset of k-space is more subtle, particularly due to the variability in tb.th and tb.sp. we identify a quantity that can be extracted from a small number of k-values, as determined from simulated mtexture measurements, which can give insight into trabecular structure and serve as a diagnostic marker of bone disease."
"to focus the analysis on trabecular structure, we select vois from the interior trabecular region of the bone images, excluding portions of the images that contain the cortical shell or areas outside the bone. we subdivide this interior trabecular region into non-overlapping contiguous rectangular vois that encompass as much of the region as possible. from vertebral body f60l3, we generate a total of 106 5 â 5 â 5 mm vois from a usable region of trabecular bone spanning roughly 25 â 30 â 36 mm, and from vertebral body ae12l2, we generate a total of 166 (5 mm) 3 vois from a region spanning roughly 30 â 30 â 35 mm. the size of the vois was chosen such that the superior-inferior, anterior-posterior and medial-lateral directions could be used as analysis directions, and where the voi would be long enough in the analysis direction to contain several repeats of the trabecular pattern in order to achieve high signal-to-noise ratio. in calculating the ratio metric, we further subdivide the vois into 25 1 â 1 â 5 mm prisms, the signals from which are averaged together, as integrated power within our chosen frequency bands increases for narrower cross-sections (electronic supplementary material, figures s1 and s2). the same procedure is followed for the osteoporotic samples, yielding 13 vois for ae15th10 and 15 vois for ae15th11."
"we have developed an mr technique known as mtexture [cit], which allows for fast acquisition of mr data from in vivo biological tissues while overcoming most of the motion limitations of other commonly used diagnostic mri techniques [cit] . mtexture is able to resolve the texture of biological tissues at wavelengths down to less than 40 mm, or even smaller in conjunction with machine learning techniques, compared with the approximately 80 mm resolution of hr-pqct or approximately 140 mm resolution of micro-mri available for in vivo human clinical use. in contrast with typical mri, which acquires data from all or most of k-space and takes the fourier transform to obtain an image, mtexture probes k-space one point (or small region) at a time, acquiring a measure of signal magnitude versus k-value (frequency) at the desired points or regions in k-space for a selected volume of tissue. that is, mtexture focuses on obtaining frequency-domain data at specific frequencies relevant to the texture of the targeted tissue."
"we demonstrate the feasibility of our proposed procedure through simulations of the mtexture measurement on high-resolution micro-ct bone data. we calculate the ratio metric from simulated measurements on healthy bone and artificially eroded versions of healthy bone, and find that an svm classifier can distinguish the healthy and eroded bone using the ratio metric, with high sensitivity and specificity. we apply our classifier to simulated measurements of the ratio metric using micro-ct images of osteoporotic bone, and find that the metric is able to accurately classify healthy and diseased bone. we also show that a ratio metric measured with narrower frequency bands (i.e. fewer k-space measurements) can be used to classify healthy and eroded bone with only a minor sacrifice in accuracy, suggesting that mtexture measurements within only a few trs could be sufficient to measure a diagnostic predictive of osteoporosis."
"we note that a potential limitation of mtexture involves frequencies relevant to bone beyond which mtexture can probe. while the average trabecular thickness is on the order of 100 mm in healthy humans, tb.th for osteoporotic patients is much lower, and some trabeculae can be thinner than the approximately 40 mm limiting wavelength of mtexture. despite this, our results in this paper show that mtexture is a promising tool for rapidly, non-invasively and effectively supplementing current methods of diagnosing and monitoring bone disease. providing information about the complex architecture of bone, which is known to be a crucial factor in determining bone strength and fragility, this procedure has the potential to substantially improve osteoporosis detection."
"finally, in this initial validation, we choose several parameters that may affect the classification outcome, including the sizes of the vois that mtexture samples from the bone. as described above, the ideal cross-sectional voi size, across which the signal is averaged, will give a good trade-off between a higher signal-to-noise ratio and a finer structural resolution. for this analysis, we chose these sizes guided by both the practical limits on voi size imposed by mtexture, and a study of which sizes produce the largest signal in the frequency bands of interest. future work, however, will work to optimize this and other parameters through more in-depth investigations of larger datasets, in order to enable the best classification performance in clinical applications."
"simulated mtexture measurements on human vertebral trabecular bone consist of intensities at specified frequency values within chosen vois. first, we select rectangular prisms from several stacked micro-ct image slices and collapse each prism in two chosen cross-section dimensions (i.e. averaging the 3d spatial signal in the two cross-section dimensions) to obtain a 1d spatial signal the length of the rsos.royalsocietypublishing.org r. soc. open sci. 5: 180563 analysis dimension. we then compute the discrete fourier transform of the 1d signal. however, a mtexture measurement examines one spatial frequency in one tr, though measurements at different spatial frequencies (as many as approx. 10 in a single excitation) are possible. thus, to simulate a suite of mtexture measurements, we extract from the full spectrum the intensities of a selected subset of kspace values to represent a direct acquisition of signal intensities."
"it is known that osteoporosis risk and bone architecture depend on several demographic characteristics. for example, women are more likely to develop osteoporosis than men, and the condition affects white, hispanic and asian women more than black women [cit] . future studies will characterize distributions of the ratio metric across a representative sample of the population, and determine how classification boundary depends upon factors such as age, ethnicity or sex."
"bone is a hierarchical material that exhibits mechanisms of fracture resistance across multiple scales. at the macroscale, human bone consists of two types, the dense, shell-like cortical bone, and the web-like trabecular (or cancellous) bone. trabecular bone is found mostly in the vertebrae and at the ends of long bones, encased by a cortical shell. at the mesoscale, the structure of trabecular bone resembles a highly porous network of struts and rods (trabeculae) that are individually on the order of tens of microns in thickness. this structure results in a lightweight material with high stiffness and strength which can tolerate large deformations [cit] . at the sub-microscale, individual trabeculae are made up of mineralized collagen fibrils, the 'building blocks' of bone, which are made up of hydroxyapatite crystals embedded in a collagen matrix [cit] . the micromechanics of these components have been shown to be predictive of overall bone stiffness [cit] ."
"we introduce a magnetic resonance (mr) technology called mtexture for probing the texture of various biological tissues. mtexture overcomes the motion limitations of existing mr imaging (mri) methods to acquire high-resolution data that can inform the detection and monitoring of disease. a vast number of diseases, such as hepatitis c, non-alcoholic fatty liver disease and pulmonary fibrosis, are linked to changes in tissue texture in the heart, liver and other organs [cit] . in this paper, we focus on the case of trabecular bone, a porous bone tissue resembling a network of interconnected spindles chiefly found in the interior of the vertebrae and the ends of long bones such as the femur. trabecular bone is known to exhibit structural damage and changes in anisotropy with the onset and progression of osteoporosis and its less severe form, osteopenia [cit] ."
"for all samples, the koh is neutralized with the addition of glacial acetic acid at 0.052 times the volume of koh. the samples are then washed with milli-q water, sonicated for 15 min at room temperature, washed again with milli-q water, then stored in 70% ethanol at room temperature before imaging. the samples are imaged in the sagittal plane using a skyscan 1076 (bruker, kontich, belgium) micro-ct scanner at a 9 mm voxel size."
"in this paper, we simulate mtexture measurements on the trabecular bone tissue to determine acquisition parameters that will provide valuable diagnostic information related to the structure of the probed tissue. we furthermore computationally validate the diagnostic ability of mtexture in the case of osteoporosis by developing a 'ratio metric' for classification of healthy and diseased bone."
"the vertebral specimens are kept frozen before digestion with koh. specimens are thawed, and the vertebral bodies are dissected from the spinal column with a bone saw. each vertebral body is placed in a beaker, to which 300 ml 1m koh is added. the healthy samples are incubated at 568c for 5 h, with the koh replaced after the first 1.5 h of incubation. the healthy samples are washed with milli-q water several times to remove soft tissue, then incubated for another hour in koh, for a total of 6 h of incubation. the osteoporotic samples are incubated for a total of only 4.5 h."
"we now apply our classifier trained on artificially eroded bone to images of bone with osteoporotic characteristics. the osteoporotic vertebral bodies ae15th10 and ae15th11 both contain a smaller volume of trabecular bone than the healthy bodies and thus we generate much fewer vois from the respective ct images, obtaining 13 (5 mm) 3 vois from ae15th10 and 15 vois from ae15th11. we determine the ratio metric for each of the vois, and use the svm classifier trained on the baseline and artificially 4-voxel eroded data (using vois from both healthy vertebral bodies) to classify the osteoporotic vois. we find that classification accuracy is higher for the medial-lateral analysis direction than the anterior -posterior analysis direction. figure 8 compares the ratio metric distributions for ae15th10 and ae15th11 with the distributions from the healthy baseline and eroded data (from ae12l2 and f60l3, using the medial-lateral analysis direction) used to train the classifier. the osteoporotic ratio metric distributions coincide with the eroded ratio metric distribution, but also partially overlap with the baseline distribution. applying the svm classifier, vois from ae15th10 are classified as osteoporotic with a sensitivity of 0.92, while vois from ae15th11 are classified with a sensitivity of 0.80. sensitivities and specificities for other analysis directions are tabulated in electronic supplementary material, table s3."
"to simulate diseased bone, we artificially erode the micro-ct images of the healthy samples at various degrees to produce thinner trabeculae and wider spacings (figure 2c-e). the erosion process is performed by initially thresholding the images, following the otsu and despeckling procedures described above, and then eroding the thresholded image with a kernel (or structuring element) of a chosen erosion radius. that is, a cubical (as the erosion is performed in 3d) kernel twice the erosion radius in length is used to remove voxels from the surfaces of each bone element in the voi. the higher the erosion radius, the more voxels are removed (the thinner the bone elements). image erosion is not performed on images of the osteoporotic samples."
"in this analysis, we use artificial erosion of healthy bone samples as a model of bone disease, in addition to testing our methods on osteoporotic bone. this choice erodes all bone elements uniformly. however, this is not necessarily the case in actual osteoporotic bone tissue, especially due to preferential resorption of unloaded trabeculae. indeed, we observe that variability and anisotropy are rsos.royalsocietypublishing.org r. soc. open sci. 5: 180563 fundamental characteristics of trabecular bone architecture across the samples in this study. previous studies have emphasized the relationship between the anisotropy of trabecular bone and its mechanical properties, though additional measures are needed to fully predict bone fracture [cit] . the method proposed in this work determines the ratio metric through measurements on several small vois within the larger bone sample, without considering spatial variability in structure explicitly. importantly, the method classifies healthy and diseased bone successfully even with this limitation. however, future work will extend the analysis of the variability in trabecular architecture in healthy and diseased bone. this variability in itself may provide important diagnostic information about the health and strength of trabecular bone, which could be leveraged to enhance the predictive capacity of the metric that we introduce here."
"merge: to merge the codeword k, p k de-registers itself as a codeword node, thus removing k from the global codebook similar to the file removing process. after that, p k will transfer the descriptors that associate with k to their corresponding new codeword nodes."
"except for that the probeset 216381_x_at is ranked as 9, 9 and 1 by trank, wrank and rocrank, respectively, all the other features selected by mctwo are ranked lower than 25, as shown in table 4 . for example, the dataset gastric1 even has a 831-ranking feature selected into the classification model with 95.35 % in overall accuracy. the dataset t1d has 54,675 features, and the mctwo-based nn classification model outperforms all the other models in the overall accuracy, as shown in fig. 6b . but this best model uses a feature ranked 52,455 out of the 54,675 features by rocrank. a widely-used feature selection strategy based on the filter algorithms is to choose the top-k ranked features where k is usually determined by trial and error. so such low-ranked features will normally be removed by any filter algorithms."
"and p m is the probability density of y in the m th expert network, l g is the summation of the terms h we apply a kernel parameter vector η to each expert and gate network to kernelize the input space and simulate adversarial attacks. the adversary's objective is to find the kernel parameter vectors that minimize the likelihood of the malicious data given a learned hme model. by introducing a kernel parameter vector to the input at each node in the hierarchy, the adversary can influence the basis function φ by weakening the set of discriminating features of the malicious data. for simplicity, we work out the adversarial learning model of a single level of expert network."
"with respect to emerging travel demand data collection technologies, location based social network data shows unique advantages over the gps, cell phone, and bluetooth data in term of the four major limitation in data collection. the influence of social impact from users' lbsn group or monetary discount such as a coupon for the visiting venue provide the great incentives for the data collection work. additionally, with the rapid development of smartphone, the lbsn application can be easily built in personal mobile and tablet without concerning the maintenance and updates issue in the traditional traffic monitor infrastructure. the sample size can be much larger than other methods due to the penetration rate of social networking service growing at a rapid pace. when it comes to the major concerns of travel demand data collection technologies, for the privacy concern of lbsn data, the user-side data contain the detailed demographic information of each user and their detailed checkin log at every venue is only available after signing the datasharing agreements between the user and foursqaure application provides. thirdly, to take full advantage of the social networking service provider such as foursquare, the position accuracy is based on the venue-side data which the type and location of a venue is pre-defined and well-maintained. such high spatial and temporal resolution enabled researchers to perform fine-grained analysis of users' mobility patterns and their impact on social interactions. for the last key consideration of the trip purpose measurement, through the \"venue\" type it is clearly to identify the location of trip activity such as food, entertainment, work, shop and other trip purpose. moreover, compared to the tradition alternative source, analysis of lbsn data gives the researchers the latent characteristic of human activity and mobility patterns, such as trip repetition and temporal clustering."
"in terms of information maximization, we aim to find a partitioning of the feature space such that partitions/codewords are correlated to the collected relevance information."
"that is, we seek a quantization method that provides maximum amount of information about the relevance information over all queries. with our quantizer codebook k which partitions the feature space into k codewords, we express the mutual information by the kullback-leibler divergence [cit] :"
"we use the percentage of the predicted trip arrivals pattern in the various tod regimes from proposed model to describe the temporal distribution of trip attraction estimation. figure 5 shows the calibration results from genetic algorithm. in this paper, we discuss the condition of all trip purposes. while the temporal distribution of trip attraction may become various in term of different trip purposes, the lbsn dataset performs the ability to verify the categories of venue type of check-ins data which can indicates the trip purposes of individual check-ins. as shown by figure 5, due to the consideration of time of day variation, the predicted temporal characteristic of the human trip arrivals pattern shares the similarity of the percentage of trip attraction with the ground truth data. meanwhile, in certain section of tod regimes including the beginning of am peak and mid-day period, the proposed model introduced a relative higher difference due to the high compensation rate for lowest check-in activities arrival periods."
"the pixel values of an image are the result of a non linear function of the exposure. this function is linked to the characteristic curve (i.e. the response to the variations in exposure) of the sensor. so, creating an hdr image from multiple ldr images, i.e producing a radiance map requires two major stages:"
"split: to split the codeword k into n codewords, p k randomly selects n à 1 neighboring nodes as new codeword nodes and sends the centroid coordinates to them. once all the new centroids register themselves as codeword nodes, the descriptor associations of selected nearby partitions will be updated respectively similar to the file posting process."
"in this paper, we analyse system performance (and ir model performance) for queries with different levels of complexity to determine if particular ir systems, models, methods, or resources can improve a particular subset of topics (i.e. in a query category). [cit] ehealth task 3 medical ir lab, described next."
"traditionally, in first part of the four-step model, in trip generation, measures of trip frequency are developed providing the propensity to travel. trips are represented as trip ends, productions and attractions, which are estimated separately. trips can be modeled at the zonal, household, or person level, with household level models most common for trip productions and zonal level models most common for trip attractions ( [cit] ) . one limitation of the traditional four-step scheme is the absence of temporal scale that trips are not specified in any time reference. furthermore, as noted by boyce [cit], the feedback loop between the upper and lower levels is often neglected or at best implemented using ad hoc rules since the dynamics of the decision update is not obvious. activity-based demand models are a promising replacement solution [cit] . currently, the most common trip attraction method in practice is the ite trip generation procedures ( [cit] ) . for the parameter optimization during the model simulation, a genetic algorithm was implemented. the genetic algorithm (ga) is a search heuristic that mimics the process of natural selection. such heuristic is routinely used to generate useful solutions to optimization problems. the search strategy was based on the selection of the improved chances of finding a global solution, using the concept that \"individuals\" are randomly selected from the current \"population\" as \"ancestor\" of the \"offspring\" for the next generation. figure 1 describes the modeling flowchart of genetic algorithm as follows. using the dynamic lbsn statistics, the parameters were optimized through the genetic optimization algorithm."
"machine learning and data mining algorithms are increasingly being used in security applications such as intrusion detection, spam filtering and malware analysis. in these applications, statistical learning tools are built to discriminate between malicious data and legitimate data such as spam and legitimate e-mail in spam filtering. the presence of adversaries attempting to defeat these learning tools sets adversarial learning problems apart from traditional learning tasks. the adversarial nature of security applications makes a learning task significantly more complicated and challenging, often resulting an arms race between attacks and defenses."
"agriculture is the primitive industry that mankind started first after born on earth and has the longest history among several industries and is very closely connected to the human development. in the past, agriculture was labor-intensive but the next generation farmers and associated organizations trying to apply new agricultural knowledge, new agricultural technology and agriculture·it convergence technology are increasing sharply in today's agriculture [cit] . they are to realize value creation such as increased productivity and quality improvement by combining several it technologies with agriculture. as the application range using a sensor is expanding in recent years, research has been actively carried out through combination with various sectors [cit] . as a case applying the it convergence technology, the productivity and quality of crops are improved by collecting environmental information inside the facilities such as greenhouse or plant factory by using wsn and creating optimal environment conditions for crops accordingly. for optimal environmental conditions, however, it is necessary to collect precise and accurate information and control based on it but many sensors and consequent costs are required for collecting overall data of facilities. in order to improve such a problem, gartner group recently proposed 10 [cit] and one of them is common protocol with which communication is possible between sensors and various devices [cit] . expecting that various devices will be connected to the internet, ietf, an internet standard group, proposes coap which is protocol for the communication environment limited to low power, small devices etc. in core working group. we are to propose an efficient greenhouse monitoring and control system by applying this coap to communication between the sensor nodes in the greenhouse."
"coap is the standard application layer protocol being made by core working group of ietf to realize the object web. it is used asynchronously based on udp protocol. coap is applied to constrained node, the condition with ram and rom of the low capacity as well as cpu of low performance and is the protocol to support resource discovery, multicast support, asynchronous transaction request and response etc. based on rest architecture targeting this [cit] ."
"we would intuitively expect that longer queries provide more context information as they contain more concepts, compared to possibly ambiguous single concept queries. similarly, single concept queries may be too generic or unspecific so that they will be associated with a high number of relevant documents, whereas longer queries are more specific and would have less relevant documents."
"query cost. the query cost is measured by the average number of retrieved postings for all the queries. the number determines the data volume that a query node will receive upon a query, which contributes most to the retrieval time."
"for a better understanding of our adversarial learning framework discussed in later sections, we briefly review the hierarchical mixtures of experts and sparse bayesian learning in this section."
"a class ii filter algorithm measures the association of each feature or feature subset with the sample labels, and orders all the features or feature subsets based on this measurement. most of the filter algorithms evaluate the individual features. for the feature-based filter algorithms, the user has the option of deciding the number of top-ranked features for further experimental validations, but no information is provided for the feature subset with the optimal modeling performance. a filter algorithm does not consider the inter-feature correlations, but its linear calculation time complexity sometimes makes it the only affordable choice for large datasets [cit] . t-test based filtering (trank) algorithm is the most commonly used method to test for the difference of a feature between two groups. it estimates the difference between the two groups and the variation in the dataset giving a statistical significance measurement [cit] . wilcoxon test based feature filtering (wrank) algorithm calculates a non-parametric score of how discriminative a feature is between two classes of samples, and is known for its robustness for outliers [cit] . roc plot based filtering (rocrank) algorithm evaluates how significant the area under the roc curve (auc) of a feature is for the investigated binary classification performance [cit] . the correlation-based feature selection (cfs) [cit] algorithm is a filter-based subset evaluation heuristic algorithm which assumes that features in a good feature subset should be independent of each other and are highly correlated with the samples' class labels."
"we compare the performance of different codebooks in three scenarios: static environment, dynamic environment, and retrieval with index pruning. the detailed settings for different scenarios are discussed in their corresponding sections."
"the above data demonstrates that mctwo performs better than the three filter algorithms on most of the 17 datasets, and similarly well on the others."
"we compare our adversarial hme learning algorithm to the following algorithms: the standard hierarchical mixtures of experts (hme), relevance vector machine (rvm) and its adversarial learning counterpart (ad-rvm), support vector machine (svm) and its one-class learning counterpart (1-class svm). we use a single level hme with two expert networks in our experiments."
"where α and β denotes a pair of interaction sites on different molecules, r is the site separation, q α is a point charge located at site α. the terms a αβ and c αβ are given by"
"the results are summarized in figs. 5a, 5b, 5c, 5d, 5e, 5f, 5g, and 5h and tables 2a, 2b. in terms of retrieval accuracy, all methods achieved significant improvement in the learning process. with relevance information, pcl consistently offers best accuracy, and km generally performs slightly better than pcl-nr and rs."
"we conducted a series of comprehensive comparative experiments with the other commonly used feature selection algorithms, from both the classification accuracy and selected feature numbers aspects. the comparison was conducted against two wrapper algorithms (class i), i.e. pam [cit] and rrf [cit], and three widely used filter algorithms (class ii), i.e. trank [cit], wrank [cit] and rocrank [cit] . since the filter algorithm cfs automatically generates an optimally selected feature subset, cfs is grouped with the wrapper algorithms in the comparison experiments."
"molecular dynamics (md) simulation is an efficient technique to investigate the microscopic behaviour of liquids, electrolyte solutions and interfacial fluids and to relate the molecular correlation functions to macroscopic behaviour [19 -20] . the principles used for these simulations entirely depend on classical mechanics. the molecules under investigation are supposed to be confined in a cubical box. the initial configurations and velocities of the molecules are specified. in each step of the simulation, configurations and velocities of the particles are stored. the equation of motions for n particle system is solved numerically by using a standard algorithm discussed below."
"in this report we have used the spc/e model of water and the p1 model of dmso. the molecularity of the solvents is kept intact by using molecular dynamics with constraints on intra-atomic distances in the molecules. the details of the solvent parameters are given in table 1 [cit] . the p1 model of dmso which is used in the present work differs considerably from the p2 model in that the charge on s in p1model ( 0.54e ) is nearly four times the charge of the p2 model ( 0.139e) and the methyl groups in p1 are uncharged. the energy parameter of methyl in p1 is also twice that of the p2 model. we have already shown that the potentials of mean force are not significantly influenced by the model differences [cit], while the p1 model gives better diffusion coefficients."
"in this paper we present a bag-of-visual-words model based approach for content based image retrieval in peerto-peer networks. in order to overcome the difficulty in generating and maintaining a global codebook when the bovw model is deployed in p2p networks, we formulate the problem of updating an existing codebook as optimizing the retrieval accuracy and workload balance. as a result, the proposed approach is scalable to the number of images shared within a p2p network and the evolving nature of p2p networks. in order to further improve the retrieval performance of the proposed approach and reduce network cost, indexing pruning techniques are applied. we conduct comprehensive experiments to evaluate various aspects of the proposed approach while demonstrating its promising performance."
"team uog's best run adds pseudo-relevance feedback, using the dfr bo1 model, to their baseline run. this addition to the baseline slightly improves retrieval performance for 1-concept queries, and yields similar performance for 2-and 3-concept queries."
"seventeen binary classification datasets were used for the classification performance evaluation in this study, as shown in table 1 . two widely investigated datasets colon [cit] and leukaemia [cit] were retrieved from the r/bioconductor packages colonca and golubesets, respectively. six publicly available datasets, i.e. dlbcl [cit], prostate [cit], all [cit], cns [cit], lymphoma [cit] and adenoma [cit], were downloaded from the broad institute genome data analysis center, which is available at http://www.broadinstitute.org/cgi-bin/cancer/ datasets.cgi. the dataset all was further processed into four binary classification datasets, i.e. all1, all2, all3 and all4, based on different phenotype annotations as described in table 1 . another five new datasets, i.e. myeloma (accession: gds531) [cit], gastric (accession: gse37023) [cit], gastric1/gastric2 (accession: gse29272) [cit], t1d (accession: gse35725) [cit] and stroke (accession: gse22255) [cit], were downloaded from the ncbi gene expression omnibus (geo) database."
we consider the following adversarial learning problem in which an adversary alters malicious data to evade detection at test time. here the traditional assumption that training data and test data follow identical distributions is violated.
"the hierarchical mixtures-of-experts is a treestructured probabilistic learning model. unlike standard decision trees such as id3, hme provides a soft split of data in the input feature space, allowing data to lie in multiple nested regions. the learning task is therefore divided into a set of overlapping sub-tasks of smaller sizes that are solved by components of the mixtures. the internal nodes are referred to as gating networks that score the competence of the experts located at the terminal nodes, for each input. both internal and terminal nodes are input-sensitive predictors. when the adversary modifies the input vector of a data point, the outputs of both gating networks and expert networks are affected. by corrupting the input, the adversary can either poison the solutions of sub-tasks defined on soft partitions of the input or divert data away from the most probable path it is generated."
"five-fold external cross validation is conducted for comparing mctwo with the other feature selection algorithms. due to the excessive computation requirement of the cfs algorithm, the three largest datasets all1, gas1 and mye are chosen for the comparative study of external cross validations. external cross validations are recommended to evaluate whether a feature selection algorithm has a selection bias for small datasets [cit] . the widely-used feature selection algorithm, i.e. support vector machine based on recursive feature elimination (svm-rfe), may be used as either filter or wrapper model [cit] . these are denoted as rferank and rfe in this comparison, respectively."
"3. ad-hme (exp) and ad-rvm are both sparse bayesian adversarial learning models and their results are comparable; however, ad-rvm consistently demonstrates slightly better error rates. the reason ad-hme (exp) underperforms is that the experts are located at the bottom of the hierarchy. table 1 : error rates of hme, ad-hme (exp, gate, exp+gate), rvm, ad-rvm, svm, and 1-class svm on the artificial data set. the best results are bolded. the gating networks at the higher levels may not always select the most competent expert. therefore, modeling attacks at the lower level of the hierarchy is less effective."
this work is supported in part by the khresmoi project (257528) and sfi (07/ce/i1142) as part of the centre for next generation localisation at dublin city university.
"looking up the owners of an exact file is performed with a dht lookup operation: given a file id h f, a list of nodes that has a copy of the file is returned by get(h f )."
"this paper studied a more efficient greenhouse monitoring system to reduce the burden on farmers with lower power, lower costs by applying coap standardized by ietf core working group. by introducing the greenhouse environment monitoring and control system, the domestic greenhouse industry requires a lot of costs and labor force for operation. in order to enhance the competitiveness of this domestic greenhouse industry, high added value such as production and efficiency improvement in the greenhouse, quality improvement can be created by studying and developing agricultural ict convergence technology. it is also expected that existing sensor nodes will be easily applied to the internet of things."
"also note that the retrieval scope-the number of nodes visited during a query, has the complexity of oðclog nþ. as c is bounded, this is reduced to oðlog nþ, which is also scalable as the network grows. therefore, our proposed retrieval approach is scalable in terms of both query cost and scope."
"mctwo outperforms practically all the three other filter algorithms on 15 out of the 17 datasets, when using the nn classification algorithm. the only two exceptions are that rocrank algorithm performs 0.8 and 0.2 % better than mctwo in acc using nn on the dataset pros and adeno, respectively. the three other table s2 . all2 is the most difficult dataset for all four algorithms and the three wrapper algorithms (figs. 5 and 7) . cfs performs better on macc (0.837) but used 56 features compared to 0.716 for mctwo which selected only two features. in all the other cases the improved macc values of the filter algorithms is no more than 1.1 % better than with mctwo, as in additional file 1: table s2 ."
"for codebook generation and update, each iteration consists of three steps: 1) determine the update operation (split, merge or no change) for each codeword; 2) for split and merge, transfer the postings to/from neighbor nodes; and 3) synchronize the new set of codewords across the network."
"the robust track at trec 2 [cit] focused on queries that are difficult for typical systems, aiming to improve the consistency of retrieval technology. this track has resulted in considering evaluation metrics such as the geometric mean average precision for ir when consistent ir effectiveness across all queries is important."
"robust learning against test-time attacks has been increasingly studied recently. [cit] present an adversarial learning algorithm for support vector machine learning. they propose two attack models in terms of the adversary's capabilities of modifying data. they develop an optimal learning strategy for each attack model and solve a convex optimization problem in each strategy. empirical results demonstrate that their adversarial svm model is more robust against adversarial attacks than svm and one-class svm. more recently, they propose another adversarial learning model for sparse bayesian learning [cit] . they use a sparse relevance vector machine (rvm) ensemble in which the input feature space of each rvm is controlled by a kernel vector. optimal attacks are modeled as feature space transformation that minimizes the likelihood of the malicious samples. learning proceeds as re-estimation of model parameters and kernel parameters. empirical results demonstrate that their rvm ensemble model is more resilient to adversarial attacks. in this paper, we also apply kernel parameters to both terminal and nonterminal nodes in the hierarchical mixtures of experts. instead of solving an ensemble on the entire learning task, we allow the adversary to independently or simultaneously attack sub-tasks of the learning problem."
"workload balance. the workload balance is measured by the gini coefficient [cit] among sizes of codewords, where a coefficient of 0 expresses perfect workload balance (all codewords have the same number of descriptors), and a coefficient of 1 expresses maximum workload imbalance (one codeword has all the descriptors)."
"the aforementioned step two of mctwo uses the output feature subset of mcone as its input, and returns the features filtered by the above procedure."
"the raw data from the ncbi geo database were normalized into the gene expression matrix with the default parameters of the rma algorithm [cit], and all the other datasets were downloaded as the normalized data matrix."
"for the query cost, both pcl (except pcl-0.001 for holidays) and km produce similar query costs, reducing the cost by 44.4-48.5 percent for ukbench, and 7.1-8.5 percent for holidays during the updating process. in contrast, rs produces higher query cost, yielding a cost reduction of 31.0 percent for ukbench, and an increase of 0.9 percent for holidays."
"in order to support various operations of our cbir system, we build a file index and a codeword index over dht, as illustrated in fig. 1 . the file index stores (h f, o f ) entries with file id h f as key, and the file ownership information o f as value. the codeword index, which stores the postings of each codeword, is added to support the storage and retrieval of bovw features. it is essentially an inverted index which stores (h k, w k ) entries with codeword id h k as dht key, and the corresponding postings w k as value. each entry is distributed to a node of the network according to its key. for simplicity we refer to a node responsible for an index entry of file f as file node p f, and the node responsible for an index entry of codeword k as codeword node p k . the separation of the file and codeword indices is logical, since a node may be responsible for any number of file index entries and/or feature index entries."
"in the future, we will investigate dht specific optimizations for cost reduction, more advanced matching refinement and multi-modal fusion techniques in p2p networks, and extensions of this approach to other distributed architectures. in particular, for the can network [cit], we can embed the index into the can overlay. that is, we make the can address space corresponding to our feature space, and replace the can zones with codeword partitions. such an embedding will eliminate the overhead of an additional dht layer, as we can implement the split/merge operations as a can zone split/takeover, instead of adding and removing entries on dht."
"the results are summarized in fig. 6 and table 3 . overall, pcl outperforms other methods in terms of workload balance, updating cost and query cost, and equally best with (if not better than) km on retrieval accuracy. while km achieved good retrieval accuracy, it has the highest updating cost and worst workload balance. rs achieved reasonable performance on 10 percent churn level, but failed to keep up with the rapid changes on 50 percent churn level, as indicated by the dramatic increase of updating cost and query cost."
"for the retrieval process, we are able to leverage the existing research on p2p-based text retrieval systems [cit], as the bovw model is an analogy to the bow model. specifically, we build two logical indices over dht: a file index and a codeword index. the file index manages file publishing and removing, while codeword index serves as an inverted index for feature posting lookup. to reduce the query network cost, we applied index pruning techniques to discard postings that are not likely to contribute to top retrieval results."
"as to be discussed in section 4, the global bovw codebook is updated via splitting and merging codewords. the split/ merge operations are essentially publishing/removing entries of the codeword index. for a codeword k stored on node p k, the split and merge operations are implemented as follows:"
"to achieve this, we model the bovw based cbir process with information theory: given the two sets of descriptors q and x-extracted from query image and candidate images respectively, the objective is to find out the subset of descriptors in x that comes from the images related to q. in other words, given q, for each descriptor x 2 x, one needs to determine the relevance of x, or whether x comes from a relevant image. denote the relevance information as y, the amount of information provided by the descriptors can be represented by the conditional mutual information of x and y under all q 2 q: iðx; y jqþ. note that we use a codebook k to quantize the descriptors, and use the resultant codewords to perform the retrieval, the amount of (11)). in the first iteration, k 1 decides to split into two partitions, while k 2 decides to merge. the results are broadcast throughout the network to form a new global codebook. the iterative updating process gradually optimizes the codeword partitioning to form an improved codebook."
"this paper proposes a greenhouse environment monitoring system that can be operated immediately by using zigbee not existing 6lowpan and udp(user datagram protocol) in applying coap to sensor modes. it is not appropriate to use various protocols in the low power loss network environment. thus, the proposed system applied coap by using zigbee, the non-ip protocol and adding a device playing a role of gateway [cit] . basically, sensor nodes in the greenhouse consist of sensor module playing a role of a sensor and rf module for zigbee communication. each node is connected to the gateway and performs 1: n communication. for greenhouse monitoring, environmental information delivered from the sensor is collected and measured data are stored in the server via the gateway. if a user requests data by using the web, the sensing data are shown to the user via the gateway. at this time, uri is used as the expression for resources (temperature, humidity, etc.) used in the request and response of http in coap rest area. coap is efficient for transmitting data in the greenhouse by using the informal uri internally because data size increases if sending full uri to the data targeting the node (constrained node) using low performance cpu and small memory [cit] ."
"mctwo achieves similar overall accuracies to mcone, using different classification algorithms, as shown in fig. 2 [cit] and nbayes [cit] tend to be sensitive to the feature numbers, while mctwo selects a significantly smaller number of features than mcone, which will be discussed in the following paragraphs."
"representative solvent separated ion pair configurations are shown in fig. 7 . chloride ion is shown as a green sphere and sodium ion is shown as a violet sphere. two dmso molecules are on the periphery. in the ion pair on the left [ fig. 7(a) ], there is only one solvent molecule in the region between the ion pair as the interionic distance is 4.8 å. in the ion pair on the right [ fig. 7(b) ], there are two water molecules in the region between the ion pair as the interionic distance is 7.2 å"
"while p2p networks are well known for their efficiency, scalability and robustness on file sharing, providing extended search functionality such as content-based image retrieval (cbir) faces the following challenges: 1) in contrast to centralized environments, data in p2p networks is distributed among different nodes, thus a cbir algorithm needs to index and search for images in a distributed manner; 2) unlike distributed servers/clouds, nodes in p2p networks have limited network bandwidth and computational power, thus the algorithm should keep the network cost low and the workload among nodes balanced; and 3) as p2p networks are under constant churn, where nodes join/leave and files publish to/remove from the network, the index needs to be updated dynamically to adapt to such changes."
"we compare the proposed p2p codebook learning method (pcl) with codebook re-sampling (rs) [cit] and k-means (km) clustering under different settings. for pcl, we use the ground truth relevance information from the top 10 results for training."
"topics. the topics (extended queries) were manually created by medical experts, based on information contained in hospital discharge reports. the topic set comprises 5 training topics and 50 test topics."
"the first column provides the precision at 10 (p@10), which is one of the official clef ehealth task 3 evaluation measures, for each topic category, allowing the analysis of documents returned at top rank, while the second gives the number of relevant documents. p@10 is 0.55 for 1-concept topics, 0.36 for 2-concept topics, and 0.48 for 3-concept topics. however, the performance for 3-concept topics is skewed by 2 topics with p@10 of 0.9 and 1, the remaining 3 topics had p@10 less than 0.4. thus, as expected, complex multi-concept queries obtain lower performance compared to simpler single concept queries or more precisely, it is more difficult to achieve consistent performance for multi-concept queries (hence the outliers). short queries may be expected to obtain better performance as they are less complex; longer queries involving relationships between two concepts might be more difficult to handle for an ir system based on occurrence counting. at the same time, long queries provide much more context, and an ir system is expected to distinguish relevant documents from somewhat relevant ones with such contextual information."
"many camera sensors suffer from limited dynamic range (i.e. the ratio between the lightest and darkest pixel). the result is that there is a lack of clear details in displayed images and videos. this is specifically true for real scenes that simultaneously include areas of low and high illumination due to transitions between sunlit and shaded areas. when capturing such a scene, the camera is unable to store the full dynamic range resulting in low quality images where details are concealed in shadows or washed out by bright lights."
"all the classification algorithms were evaluated for their overall performance measurements using 5 fold internal cross validations, averaged over 30 runs with different seeds for the random number generators. a binary classification algorithm with the larger acc value performs better. if two models perform similarly well, the simpler model is preferred, since it costs less resource and human labour in its clinical deployment [cit] . also, a simpler model may avoid the over-fitting challenge in the biomedical big data area, caused by the \"large p small n\" paradigm [cit] . external cross validations are also conducted to test whether mctwo generates feature selection bias."
"step 2: when a user submits a query image, the local features are extracted and quantized into c codewords. for the exact nearest neighbor quantization we used in the experiment, the quantization takes oðcjkjþ time."
"as mentioned in section 3.2, when the codebook is ready, for a given query, the retrieval process essentially consists of three steps: extracting visual features and obtaining bovw based representation for the query, retrieving the postings via dht lookup, and measuring the similarity between the query and candidate images."
"for workload balance, we aim to partition the feature space evenly and accommodate the computational capacity of each nodes, so that no nodes would be overloaded or underloaded. while achieving good discriminability is important, a codebook used in p2p networks must also produce fair workload for each node. unfortunately, in real world scenarios where relevance information is incomplete, the information maximization process often produces imbalanced partitions, as it has strong bias towards available training data. as illustrated in fig. 4, the resultant codebook tends to use more fine-grained partitions to represent the areas where relevance information is available, but leaves the rest of the areas coarsely partitioned. this not only compromises the retrieval performance of unknown queries, but also produces imbalanced workload: the nodes managing large partitions will be overloaded and the small partitions will be underloaded."
"the architecture of the hierarchical mixtures of experts (hme) is a probabilistic tree. gating networks are located at the non-terminals, providing soft split of the input feature space. data points are therefore grouped into soft partitions and are allowed to lie in multiple regions simultaneously. this technique effectively ameliorates the variance-increasing problem often encountered in a divide-and-conquer problem [cit] . expert networks located at the terminals solve the subproblems defined by the soft partitions of the input."
the simulation has been performed by treating the interaction between the various components to be composed of pair wise additive potentials. the site-site ion-solvent and solvent -solvent potential is represented as
a comparison between the calibrated formulation for dynamic trip attraction and campo trip attraction matrix was done by examining the spatial and temporal distribution of trip attraction rate.
"we first compare the performance of the proposed pcl method with rs and km under a static environment, where data do not change. the experiments are conducted using the whole dataset of ukbench and holidays. to enforce data independence upon initialization, we use the codebook with 20,000 codewords obtained from flickr60k as the initial codebook. the codebook is then updated with pcl, rs, and km methods for 10 iterations."
"this paper investigate the feasibility of using the location-based social networking (lbsn) data to analyze the urban travel demand pattern using a time-dependent model. given by the check-ins statistics by foursquare, lbsn data was used to provide zonal trip attraction in the city of austin area, which use campo ground truth data to evaluate the performance of the proposed methodology. with respect to traditional and emerging travel demand data collection technologies, lbsn data shows unique potential to investigate better spatial and temporal coverage, overcomes the major limitation of sampling bias, privacy concern and provide trip purpose confirmation by distinguishing the categories of venue types. compared to the previous study of lbsn data, we explore the feasibility of finding the hidden trip attraction rate based on current check-ins rates and tod factors. future research should continue in three areas. firstly, the discussion of various trip purposes should be examined in temporal and spatial patterns. typically, lbsn's data was collected by first identifying the venues with the venue id, venue name, category, latitude, and longitude. an initial analysis of the check-ins should be performed to verify that categories were assigned to the venues such as professional places, food, shops & services, and colleges & universities. with a categorical breakdown of the venues, the trip purposes were assigned to the corresponding check-ins such as homebased work trips, home-based non-work retail trips, and home-based non-work school trips identified with the campo study. secondly, the adaptability of the proposed methodology in the area such as the residential venues whose may contains less lsbn service users should also be examined. moreover, due to the thriving development of various lbsn like foursquare, a leading lbsn data provider, the technology and solution of existing social media data integration for transportation agencies should be reviewed to improve the efficiency of data collection, archival, analysis and dissemination. such examination should be researched to further address the issue of potential lower penetration rate of individual social media platform in case of changing trend in lbsn users' preferences which can be told by app usage and download rate in mobile app stores."
"tao mei (m'07-sm'11) received the be degree in automation and the phd degree in pattern recognition and intelligent systems from the university more importantly, however, is that many of his research results have been translated into solutions to real-life problems and have made tremendous improvements to the quality of life for those concerned. he has served as the chair in the international federation of automatic control (ifac) technical committee on biological and medical systems, has organized/chaired over 100 major international conferences/symposia/workshops, and has been invited to give over 100 keynote presentations in 23 countries and regions. he is a fellow of the ieee and australian academy of technological sciences and engineering."
"m p j (y i ). the learning process is best understood as an arms race between the expert and the adversary: given expert parameters (w, α), the adversary finds an η that minimizes the likelihood of the malicious data points, referred to as positive ('+') data points in the input. note that in the minimization term in equation (4.2) the adversary also attempts to minimize the square loss of the output. this may sound counter intuitive since minimizing training loss is not to the best interest of the adversary. a greedy adversary would attempt to maximize the loss of all malicious points. however, a simple validation on the training set would disclose the adversary's attempts. therefore, the adversary's objective is to minimize the likelihood of malicious data and keep the attacks stealthy by maintaining minimum losses during training."
"as shown in figure 1, coap basically uses udp and is divided into coap transactions area processing its upper transaction asynchronously or synchronously and coap rest area for processing above rest data. with id by transaction, transaction messages define new transaction to create new id for every transaction. this is a method for preventing transmission of duplicate packets, etc. also, as a method to apply coap rest, the basic method of get, post, put, delete, the http method is used [cit] ."
"we now present our adversarial hme learning model ad-hme. the basic idea of our method is to take into consideration optimal malicious data transformation at training time. our ad-hme learning algorithm models attacks ab intra as if the adversary were present at the time of model training. the learning process simulates an arms race between the classifier and the (imaginary) adversary. from the adversary's perspective, every time an optimal parameter set of hme is found, the adversary attacks in response to the new predictive model by transforming the malicious data points to the most likely legitimate ones. this is accomplished through updating a kernel parameter vector that regulates data transformation in the feature space. a malicious data transformation is most effective when it maximally reduces the likelihood of the malicious data points in a dataset. from the classifier's perspective, after every attempt of malicious data transformation a new hme model needs to be re-trained to counter this type of attack. the process repeats until a predefined equilibrium is reached, for example, when the adversary has no further incentive to transform data because of high cost or when the classifier stops further re-training because of high false positive rate."
"our data suggests that best classification models may use some features which are ranked low by filter algorithms. this is plausible as the filter algorithm evaluates the association of each feature with the class labels independently, and a combination of the top p ranked features does not necessarily lead to a classification model with high overall accuracy. for example, the features linearly correlated with the top ranked feature will also be highly ranked. however a combination of these linearly correlated highly-ranked features will not improve the classification model based on the top ranked feature. a lower-ranked feature independent of the top ranked feature may lead to a better classification model."
"it is the first study to investigate scalable cbir with the bovw model in p2p networks. a novel objective function for codebook optimization in a p2p environment is proposed, which considers both the relevance information and the workload balance simultaneously. a distributed codebook updating algorithm based on splitting/merging of individual codewords is proposed, which optimizes the objective function with low updating cost."
"additional datasets are used to facilitate the experiments: flickr100k [cit] . for the distractors for large scale experiments, we use the first 100,000 images from the flickr1m, where images are downloaded from flickr."
", and h m is estimated in the e-step in the bayesian em learning algorithm. we use the gaussian kernel to compute the basis function:"
"learning proceeds as iterative re-estimation of: (1) v that maximizes l g given η, and (2) η that minimizes l + g given v until the algorithm convergences."
"trip attraction estimates the number of incoming trips to a destination or zone based on land use and social-economic data. it is an essential part of the trip generation step in the classic fourstep planning process [cit] ) and has been well-documented in standard planning manuals such as ite trip generation manual [cit] . in activity-based travel demand models, analyzing the trip attracted to a traffic analysis zone or finer spatial units such parcel and census block levels also provides valuable input regarding destinations to assist the micro simulation on individual travel [cit] . one key limitation for the convention trip attraction models is their dependency on static land use and census data which limit their usability in dynamic travel demand estimation and prediction needed for the emerging active traffic and demand management (atdm) solutions."
registered nurses and clinical documentation researchers developed a set of patient queries using the pairs of discharge summary and a disorder (randomly selected among all disorders identified) in order to generate a set of realistic patient queries.
"comparing the results of pcl, we can see that the relative weighting value a is correlated to all performance perspectives. a larger a value puts more emphasis on workload balance, therefore the workload is more balanced, but the updating cost may be higher as we try to balance the partitions vigorously. on the other hand, a smaller a value puts more emphasis on relevance, therefore the retrieval accuracy is higher. in addition, the query cost is usually smaller, since the codewords are more discriminative. generally, in order to obtain best retrieval accuracy and efficiency, we should aim at a small a value, as long as the workload imbalance problem remains manageable."
"this study proposes a novel wrapper feature selection algorithm, mctwo, based on the measurement maximal information coefficient (mic) [cit] between two variables. the first step of mctwo screens all the features for their mic associations with the class labels and each other, and only those with significant discriminative power are kept for further screening. then mctwo employs the best first search strategy to find the feature subset with the optimal classification performance. the experimental data suggests that this algorithm outperforms the other algorithms in most cases, with significantly reduced numbers of features."
"for data dynamics, the data in a p2p network is under constant churn. the codebook in such an environment needs to be updated periodically, rather than kept static. at the same time, during the retrieval process, relevance information can be accumulated explicitly by users providing feedback about their query results; or implicitly from the downloading behavior after a query, which can be utilized to improve retrieval performance [cit] . with various dynamic data, updating the codebook incrementally and continuously to maintain optimal performance is very challenging."
"assume that a p2p network consists of n nodes sharing a total of n images, a codebook k of initial size jkj is utilized for bovw based retrieval, and each image has an average of c codewords. our system completes a query in the following steps: 1) feature extraction; 2) quantization; 3a) sending posting lookup message; 3b) receiving postings; and 4) aggregating postings and producing the rank list. we only discuss steps 2-4 as feature extraction time is not affected by our system configuration."
"1. all three adversarial hme algorithms ad-hme (exp), ad-hme (gate) and ad-hme (exp+gate) the x-axis is the input to the gating functions and the y-axis is the posterior of each expert (approximating \"+\" and \"-\" data respectively)."
"the node of coap acts as a client, server and gateway in the greenhouse in some cases. the definition of a gateway in coap is a node paper preparationthat can respond based on the previously cached information on behalf of the node that must originally respond and the gateway is required in coap because all m2m nodes can enter sleep state (that is, sleep state that cannot give a response) at any time. in this case, if there is a request of a resource event for sleep node, the gateway makes a cashed response instead. also, the gateway plays a role of protocol conversion between nodes of http area in the network of the existing greenhouse (nonconstrained node) and nodes in coap area (constrained node). in order to support udp-based multicast, the feature of coap, the gateway has a function of converting udp-based coap multicast protocol into tcp-based http unicast protocol. through this, event data are transmitted and received without distinguishing networks (http, coap) used in the greenhouse control system [cit] . this paper is a study on protocol which is more efficient than existing greenhouse monitoring and control and clearly applies the characteristics of low power, low cost."
"from the system response curve stored in lut, the radiance value of each pixel can be evaluated by a weighted combination of the pixels from each ldr frame."
"our global pipeline architecture dedicated to hdr video creating is shown in fig. 1 . this architecture relies on a specific fpga daughter board embedding a 1.3 million pixel cmos imager from e2v [cit] . with a dynamic range of about 60db, this sensor can be viewed as a low dynamic range imaging system. it embeds dedicated image pre-processing such as image histogram. each frame is delivered with its own histogram encoded in the image data stream footer. based on these histograms, a double auto exposure algorithm has been implemented to evaluate the optimum integration times. the sensor also includes a bi-frame acquisition mode that is fundamental for hdr imaging. with such a mode, the sensor is able to successively acquire 2 images with different integration times, the second acquisition simultaneously occurring with the first frame readout. the sensor sends alternatively low exposure (le) frame and high-exposure (he) frame. the le frame is first stored in ddr2 memory. simultaneously with the acquisition of the he frame, the le frame is read in order to create the hdr image. the sensor sends full resolution images at 60 frame/s, which means that we have an output rate of 30 frame/s for the hdr image generation. then, the hdr image is tone mapped as described in subsection iii-b. the 8-bit resulting image is displayable on a lcd screen through the dvi controller."
"the generated topics contain a classic trec-style title (text of the query), a description (longer description of what the query means), a narrative (expected content of the relevant documents; and profile of patient), an additional discharge-summary field which links to the associated discharge summary, and a profile field, containing information about the patient's profile (such as age, gender, and condition)."
"we present an adversarial learning framework using the hierarchical mixtures of experts. we interactively search for feature space transformations that minimize the likelihood of malicious data given parameters of the baseline learning model. in each round of learning, we model the adversary's best strategy of attacking the current trained learning model. empirical results demonstrate that our ad-hme learning model is robust against adversarial attacks. compared to ad-rvm, our ad-hme algorithm does not require setting a constant ρ to control overcompensating issues and ad-hme can model attacks more efficiently at the gate levels in the divide-and-conquer fashion. in the future, we plan to investigate influences of attacks against terminals and non-terminals at different levels and use mixtures of different learning algorithms in the expert networks."
"the results are summarized in figs. 5i, 5j, 5k, and 5l and table 2c . in terms of retrieval accuracy, the proposed pcl method beats km and rs by a much larger margin compared to results of smaller scale data. this shows the importance of utilizing relevance information when the dataset becomes bigger and noisier. at the same time, pcl also achieved best workload balance and comparable query cost. the increased codebook updating cost is caused by the vigorous setting of a value to optimize workload balance, which can be reduced by reducing the a value."
"the queries in the collection aim to model those used by laypeople (i.e. patients, their relatives, or other representatives) to find out more about their condition, after they have examined their hospital discharge summary. the discharge summaries used for the task originate from the anonymized clinical free-text notes of the mimic ii database, version 2.5 4 . disorders have been identified within discharge summaries and linked to the matching umls (unified medical language system) [cit] ehealth task 1 [cit] ."
"other related research on improving ir evaluation examined minimizing efforts for relevance assessment by dynamically creating the set of pooled documents [cit], determining the quality of test collections [cit], or investigating how to automatically predict query performance [cit], and exploit this information automatically."
"since the distribution of x and y are both fixed when q is given, iðx; y jqþ is fixed. minimizing eq. (1) is equivalent to:"
"coap protocol is to be applied to sensor node etc. with limited resources and designed by considering a number of limitations because its computing capability including memory is poorer compared to the pc environment. as shown in figure 2, it has a very simple message header and option header."
"adversarial learning problems have been modeled as stackelberg games between two opponents. [cit] solve for a nash equilibrium using simulated annealing and the genetic algorithm to discover an optimal set of attributes. additional results on improved nash strategies can be found in bruckner & scheffer, and liu & chawla's work [cit] . brückner and scheffer [cit] later presented another stackelberg game strategy that does not require a unique equilibrium. cost-sensitive opponents have also been introduced to the game theoretic framework for adversarial learning [cit] . the problem is modeled as a game between two optimal opponents. given a cost function, the adversary transforms an instance for which the cost is minimized. the algorithm predicts according to the class that maximizes the conditional utility."
"we selected the city of austin, texas as the study area. austin is a diverse city that had a july 1, 2013 population of 885,400 people (u.s. census bureau estimate) and encompasses an area of 272 mi 2 . the data used in this paper can be categorized into the calculated tod factors for selected trip purposed will be used as the ground truth dataset in the section of model calibration. the zonal od matrix data from campo will be applied for developing zonal attraction table in the section of model evaluation and model application."
"the classification performances of feature subsets selected by mctwo and three other wrapper algorithms cfs, pam and rrf were compared. best classification performance of the features selected by mctwo is usually achieved by the classification algorithms dtree and nn, as shown in fig. 4 and additional file 1: figure s2 ."
"the rest of this section is organized as follows: sections 4.1 and 4.2 present the two parts of our objective function: mutual information and workload balance, and the two parts are combined in section 4.3. section 4.4 introduces the decision-making process of codeword optimization."
4. ad-hme (expert + gate) cannot outperform ad-hme (gate). for some data points ad-hme (exp) and ad-hme (gate) combined may overcompensate for fears of adversarial attacks.
"the second column shows the average number of relevant documents per topic. the very high number for 3-concepts is biased by a topic having 610 relevant documents (topic 19), the average number of relevant documents being 19. so this value ranges from 28 documents for 1-concept queries to 19 for 3-concepts ones, which can be explained by the fact that 1-concept queries are typically shorter without much context and are often ambiguous. figure 1 shows the distribution of relevant documents per topic category. as can be seen, there does not appear to be any relationship between the volume of relevant documents and topic category, while one could expect 1-concept queries to have many more relevant documents than 2-and 3-concept queries. therefore the low performances of 2-concept topics cannot be explained by the complexity of the topics resulting in few matching relevant documents."
"p is the proportion of pixels located in a part of the histogram. s means the short exposure, l means the long exposure, h is the histogram of the image, h is the category position (1 -64), s is the number of saturated pixels (dark pixel at level 0 or bright pixel at level 1023) and n is the total number of pixels. the better exposure time for each image can be evaluated as follows:"
"all the operations of the cbir system are translated into lookup or modification of the entries of the file and/or codeword index, which are implemented by get and put operations over the dht overlay. details of the operations are listed in the following sections. note that node joining/leaving operations are handled by the underlying dht network and therefore not listed here, and we only need to handle the resultant file publishing/removing operations."
"high dynamic range (hdr) imaging techniques appear as a solution to overcome this major issue by extending the precision of the digital images. hdr imaging encodes images with higher than standard 24-bit rgb format, increasing the range of luminance that can be digitally stored. hdr images can be created in two different ways. the first method requires the development of specific hdr sensors that can capture the entire scene dynamic range. several sensors development have been done with techniques such as well-capacity adjusting, time-to-saturation, or self-reset (see [cit] for a comparative analysis of these sensor architectures). to perform these functions, most of these sensors provide a processing unit at chip level or at column level [cit], and even at pixel level [cit] . the second method relies on conventional low dynamic range (ldr) sensors to capture hdr data by recording multiple exposures of the same scene [cit] . by limiting the exposure time, the image loses low-light detail in exchange for improved details in areas of high illumination. a contrario, by increasing the exposure time, the resulting image contains the details in the dark areas but none of the details in the bright areas due to pixel saturation. complex algorithms build a single hdr image (i.e. radiance map) that covers the full dynamic range by combining the details of the successive acquisitions. however, current display technology has a very limited dynamic range, so that hdr images need to be compressed by tone mapping operators [cit] in such a way that the visual sensation of the real scene is faithfully reproduced."
"(2) ∆t t is the exposure time at time t . ∆t opt depends on the number of saturated pixels. it is not possible to precisely evaluate the best exposure for a real scene because the tone mapped hdr image rendering depends on many factors (number of images, type of the scene, sensor sensitivity, etc.). in order to evaluate as precisely as possible the best values of the integration times, the following assumption was made: 80% of the pixels in the lower part of the histogram are present in the low exposure image, and 80% of the pixels in the upper part of the histogram are present in the high exposure image."
"retrieval accuracy. we follow the recommended evaluation protocols of the datasets to measure the retrieval accuracy. for both datasets, we report the mean average precision (map), which is the average area under the precision-recall curve for all the queries; and r-precision (rp), which is the average precision of top r results, where r is the number of relevant images. for the ukbench dataset, we also report the kentucky score (ks) (the average number of positive images in top 4 results, which is essentially rp * 4) used by the dataset authors."
"to rectify this problem, for each k we define a workload factor to measure the difference between the current workload s k and the target workload s k as: fig. 4 . illustration of the bias towards partial training data. assume we only have relevance information for query a, if we only consider mutual information of available training data, the resultant partitioning will be similar to (a), which divides the area of query a with small partitions, but leaves the rest areas (queries b, c and d) coarsely partitioned. if we consider both mutual information and workload balance, the resultant partitioning will be similar to (b), which is likely to get better results for queries b, c, and d due to its finer-grain partition in these areas."
"results for single concept queries might suffer from missing relevance assessments, following the general observation that the likelihood of finding more (unassessed) relevant documents for queries which already have a high number of relevant documents is high and vice versa. in general, we can observe the same pattern here that for simple queries (1-concept queries), it is easier to obtain a high precision in the top ranks."
"the problems of capturing the complete dynamic range of a real scene and reducing this dynamic range to a viewable range have drawn the attention of many authors. however, the main part of the proposed algorithms have been developed without taking into account the specificities and the difficulties inherent to hardware implementations. unfortunately, these works are not generally suitable for efficient real-time implementation on smart cameras. as a consequence, generating real-time hdr images still remains a interesting challenge."
"as one of above potential new data sources, the emergence of location based social network (lbsn) services make it accessible and affordable to study individuals' mobility patterns in a fine-grained level and to estimate trip attraction for each venue in the future. location based social network refers to special social networking services that use lbs (location-based service [cit], replying on gps to locate users, and allow members of the communities to broadcast their locations and activities through their mobile devices. lbsn does not only mean adding a location to an existing social network so that people in the social structure can share location-embedded information, but also consists of the new social structure made up of individual connected by the interdependence derived from their location in the physical world as well as their locationtagged media content [cit] . as a source of input data, it recorded in \"check-in\" or tweeting activities of massive users at different points of interests (pois) named \"venue\" such as ground transportation center, popular restaurant, bar, club, sports stadium, and even a bus. in lbsn services such as foursquare and twitter has enabled users to share their location or the venues they have visited in the past with their social communities. a user will make one check-in or sending one tweet by using a smartphone or tablet to choose nearby venue which will be recorded in lbsn server with the geographical location (i.e., latitude and longitude coordinates). [cit] . foursquare is the leading lbsn provider in the us, attracting 10 [cit], with about 3 million check-ins per day [cit] ."
we use the sparse bayesian learning method with gaussian kernels [cit] to train the expert networks. for regression the marginal likelihood of the experts is:
"we have calculated the na + cl -ion pair trajectories in three compositions of water-dmso mixtures. in these compositions (the dmso mole fraction ranging of 0.20 to 0.50), deviations from idealities are the largest. the residence times of the ion pair at different distances can be easily used to ascertain the existence of the contact ion pair and the solvent separated ion pair. in general, solvent separated ion pairs are found to be predominant and two distinct solvent separated in pairs can be identified from the regional residence time plots. the solvation shells are not rigid and as the ion pair diffuses out, other solvent molecules get inserted between the ion pair either by rotation of the primary solvation shell or by the insertion of the solvent molecules into the interionic region from the secondary solvation shells. we find examples of both these kinds of insertions in our simulations and the diffusion coefficients of these incoming molecules are generally smaller than the bulk diffusion coefficients of the solvents. the values of the diffusion coefficients of the molecules in the primary solvation shell are less than half of the bulk values. in several biological systems, the ratios of the primary solvation shell diffusion coefficients are even smaller (relative to the bulk). the ion pairs span the distance range of 3 å to 10 å regions in a time span of 10 ps. contact ion pairs (ion pair distance of 3å) are found very infrequently. it is reassuring that the radial distribution functions obtained in the present work with larger box lengths of nearly 30 å with over 400 molecules are quite similar to those obtained earlier with smaller box lengths and around 250 molecules. we are investigating similar details in compositions wherein the mole fraction of one of the components is much larger. the results with 91% dmso would be of special interest as this composition shows a significant presence of the contact ion pair."
"on the other hand, the locality-sensitive hashing based approaches use special hash functions that output the same value for similar objects. in p2p networks, these hash functions are usually combined with the hash table interface of dhts [cit], thus similar feature vectors are stored on the same/neighboring nodes to enable efficient similarity searches. to improve the locality of the hash functions, most works compromise the even distribution of hash buckets [cit], which translates to an imbalanced workload among nodes in a p2p network. some works [cit] do tackle this issue by learning more independent hash functions, but all of them perform one-time learning without considering the changes of data distribution that is common in p2p networks due to network churn. even when one can update the hash functions with changing data, implementing it over the dhts is very challenging. as the data is stored among nodes of corresponding hash id, a 1-bit change of the hash function output will result in large portion of (if not all) data being assigned to a different node, causing heavy network traffic."
"all the datasets used in this study are previously published by the other researchers, and publicly available, as described above. so neither ethics nor informed consent forms are needed from this study."
"as the ion pair separates, solvent molecules get inserted in the region between the ion pair. for charged ions, primary solvent shell (the first solvent layer surrounding the ions) is expected to be long lived and the molecules \"bound\" in this region are expected to be far more sluggish compared to the bulk molecules. this behaviour extends to biomolecules such as proteins wherein there is evidence for the existence of two types of solvents, one bound to the biomolecule and the other, a lot more mobile [cit] . in these biological systems, there have been a large number of studies analyzing the differences in the diffusional behaviour of locally bound solvent molecules and the non local labile solvent molecules. the structural and equilibrium behavior of this na + cl -ion pair has been investigated in sufficient detail in the earlier studies [cit] . in the present paper, we investigate the relative dynamics of the two solvation regions and the insertion of molecules from the more mobile region to the tightly bound region. the methodology is given in section 2. the results are described in section 3 followed by concluding remarks in section 4."
"a number of representative classification algorithms are chosen to build the binary classification models based on the features selected by the aforementioned feature selection algorithms. support vector machine (svm) calculates a hyper-plane between the two classes of samples/ points in the high-dimensional space that maximizes the inter-class distance but minimizes the intra-class distances [cit] . the naive bayes (nbayes) model assumes that the features are independent of each other and picks the class label with the maximal posterior probability as the prediction [cit] . nbayes is known to be competitive with the more advanced and computationally-intensive methods, e.g. svms, in some machine learning problems such as text categorization [cit] . a decision tree (dtree) consists of decision rules on the tree nodes about which route to take for the next decision step [cit] . the simple nearest neighbour (nn) algorithm predicts that a query sample belongs to the same class as its nearest neighbour in a given distance measurement [cit] ."
"although most of the features selected by mctwo are ranked low by the filter algorithms, many have known roles in disease onset and development. for example two of the gastric1 features, 216381_x_at and 218595_s_at, are known to be associated with gastric cancer, as shown in table 4 . probeset 216381_x_at of the gene akr7a3 (aldo-keto reductase family 7, member a3) is involved in the biological processes of cellular aldehyde metabolics and oxidation reduction. an independent study observed its differential transcriptional levels between gastric cancers and control samples [cit] . probeset 218595_s_at of the gene heatr1 (heat repeat containing 1) may prevent apoptosis and induce gastric carcinoma in helicobacter pylori-infected gastric epithelial cells [cit] ."
"the cbir search is essentially an inverted index lookup in the codeword index. as illustrated in fig. 2, a user on node c submits a cbir query with an example image (red bike), which will be answered in three steps: firstly, the bovw codewords will be extracted on node c locally. secondly, the codewords will be looked up in the codeword index with get(k), where k is the codeword id (solid arrows). the postings with their corresponding file ids will be returned by the respective nodes (e and a), which will be used as similarity measurement to produce the ranked results for the user. finally, the owner information of relevant images can be obtained by the file lookup process described before."
"comparing the updating costs of pcl and rs in different environments, we can see a correlation between the updating cost and churn level. such an adaptive nature is essential to minimize the updating cost in the ever-changing p2p networks, as we only update the codewords when necessary. for a real world scenario, it is estimated that the daily node population change rate is about 10 to 15 percent (in kad network [cit] ). therefore, a low iteration frequency (once a few hour) is sufficient to maintain the performance."
"this paper presents a complete hardware system dedicated to hdr imaging from multiple exposures acquisition, through radiance maps and tone mapping, to display. we present a simplified and practical hardware solution to produce real time hdr video. the remainder of the paper is as follows: in section ii, we briefly review the literature on real-time hdr imaging solutions. section iii describes hdr creating and tone mapping algorithms. we propose a dedicated hardware architecture in section iv. our results are presented in section v. conclusions are provided in section vi."
"mctwo performs slightly worse in the best classification models than mcone, as shown in fig. 3 . for a given feature subset, researchers will always choose the classification model with the maximal overall accuracy. so the maximal acc"
"we note that the bovw histogram, which will be discussed later, can also be considered and processed as a highdimensional global feature. however, as the bovw histogram can have a dimensionality of thousands or even millions [cit], it is beyond the capacity of existing highdimensional indexing techniques. while lsh can handle higher dimensionality, it requires longer hash codes to maintain the efficiency [cit], causing high network cost when implemented over a dht."
"we further compare mctwo with the three filter algorithms trank, wrank and rocrank for their classification performances. a filter algorithm only outputs an ordered list of features based on a ranking measurement. so for a fair comparison, this study chooses top p features from the ordered list of features ranked by the filter algorithms, where p is the number of features chosen by mctwo."
"to implement these steps in p2p networks, each node firstly performs feature extraction and quantization locally. therefore, the key challenges lie in the steps performed in a distributed way: the codebook generation/updating and retrieval. as discussed earlier, the algorithms need to have a low and even network cost on all nodes, and adapt to data dynamics. table 1 shows the estimated per-node computation and network cost of different steps in our experimental system, assuming an average bandwidth and a typical dht implementation. as shown in the table, the efficiency of a retrieval system mainly depends on the network cost between nodes, rather than the computation cost within a node. therefore it is essential to minimize the network cost and keep the workload balanced during both codebook updating and retrieval."
"the auto-bracketing system implemented in many cameras is a good way to expand the dynamic range captured by the sensor. traditionally, the camera estimates the good exposure and takes several pictures with many multiples of f-stops up and down. our way is not to set fixed exposures, but rather to make the exposures varying from frame to frame to get the more pixels well exposed. our exposure control algorithm is based on pixels statistics on two successive images. the sensor is able to automatically computes a 64 16-bit categories histogram, the number of dark pixels (s low ), and the number of saturated pixels (s high ) for each acquired frame. our aim is to calculate the number of pixels in the upper and lower part of the histograms of the two images as:"
"a preliminary analysis is conducted on the characteristics of the check-ins occurrence by investigating both the spatial and temporal pattern of the check-ins data created by foursquare lbsn service users. the location of the 124,611 check-ins are represented using a dot in figure 2 (a). as shown in figure 2 (b), a heat map also represents the geographic density of check-ins features on study area by using graduated color areas to represent the quantities of those points. the check-ins are more densely distributed in downtown and north central area of the city. to investigate the temporal characteristics of when people use the foursquare lbsn services, the number of check-ins are aggregated for every one hour of different hours of day. we show, in figure 2 the empirical occurrence rates of check-ins, where the x-axis represents the time since 0:00 to 23:59 while the yaxis stands for the occurrence rate of check-ins per hour. figure 2 clearly indicates that the normal check-ins flow pattern is different with the conventional traffic pattern which usually has am peak, pm peak and lowest trip arrivals during the night time."
"to support content indexing and avoid message flooding, structured overlay networks such as distributed hash tables (dhts) [cit] are often implemented on top of a physical network. by organizing the nodes in a structured way, messages can be efficiently routed between any pair of nodes, and the index integrity can be maintained during network churn. for the cbir functionality, most of the existing systems adopt a global feature approach: an image is represented as a highdimensional feature vector (e.g., color histogram), and the similarity between files is measured using the distance between two feature vectors [cit] . usually, the feature vectors are indexed by a distributed high-dimensional index or locality sensitive hashing (lsh) over the dht overlay. however, due to the limitation known as \"curse of dimensionality\", the majority of these solutions have high network costs or serious workload balance issue among nodes when the dimensionality of feature vectors is high."
"as illustrated in fig. 3, our codebook updating algorithm runs iteratively. during an updating iteration, each codeword node p k decides whether its codeword k should be split/merged/unchanged based on the relevance information collected from past queries, and the current workload. after each iteration, the centroid coordinates and the codeword statistics needed for similarity measurement (e.g., document frequencies) will be broadcasted throughout the network, so that all the nodes in the network can have the same codebook. the iterative process runs continuously in order to maintain an updated codebook during data churn. the frequency of update iterations is determined by the degree of churn. as shown by the experiments in section 6.3, a very low update frequency (once a few hour) is enough to maintain the performance."
"the size of k is large enough for subpartitioning, and it is possible to get a good sub-partitioning based on available data. in this case, a split operation is performed: the new centroids of the sub-partitions of k are published to the codebook, thus splitting k into a few smaller partitions."
"as mentioned earlier, in order to support cbir in p2p networks, structured overlay networks are often implemented on top of a physical network. a popular class of overlay networks is distributed hash table [cit], which builds a hash table globally and stores the entries among the nodes with corresponding hash id. with the overlay network, each node needs to be able to represent the files with features, and store/retrieve the features to/from the global structure efficiently. achieving this is very challenging, as we need to align the feature representation and indexing method with the underlying overlay. generally, existing approaches can be divided into the following two streams."
"where : trip attraction estimation from campo matrix for zone ℎ : ground truth trip attraction from foursquare matrix for zone the nearby position of points to the perfect line represent accurate estimation of zonal trip attraction, and the remote position of points suggest inaccurate estimation. as shown in figure 6, the trip attraction pattern from the proposed model and the campo ground truth trip attraction pattern exhibit more similar characteristics than the baseline model. such similarity is in consistence with the small swap ratio value. some slight inconsistency can be found for trip attraction where the proposed model underestimated or overestimated the trip attraction, especially on some zones most of which are those residential areas may indicate less check-ins activities than the others."
"we optimize e k by finding a suitable partition granularity and good centroid positions. the learning algorithm adjusts the partitioning by splitting/merging the partitions iteratively. since the codeword index and relevance information is managed by codeword nodes p k, the decision to split/ merge a codeword k is made by p k individually based on its own data."
we choose the sparse bayesian learning model to build the expert networks. the weights of the experts have zero-mean gaussian priors controlled by hyper parameters α:
"the two datasets gas1 and t1d are selected from the 17 datasets as representatives of cancers and cardiovascular diseases, respectively. the detailed results of all the other datasets can be found in additional file 1: figure s1 . results of all the 17 datasets will be summarized and discussed in the following text."
"the uni-gram web spam data set is taken from the libsvm website [cit] . we cut down the number of attributes from 254 to 50 using the minimumredundancy-maximum-relevance feature selection technique (mrmr) [cit] . we divide the 350,000 instances evenly into training and test sets. in each run 2% of the samples were randomly selected for training and the results are averaged over 10 random runs. table 3 shows the error rates of the six algorithms as the strength of attacks increases. the ad-hme algorithms were superior to others in all cases. their superiority is also attributed to the baseline hme algorithm that significantly outperformed svm and rvm. nevertheless, the ad-hme algorithms consistently outperformed the baseline hme algorithm in all cases."
"the proposed feature selection algorithm may select features for any binary classification datasets. for the convenience of discussion and dataset availability, this study focuses on the classification performance comparison on the microarray-based gene expression profiling datasets."
"from the empirical results on the artificial data, we can see that adversarial learning algorithms that adapt to optimal attacks during training have a better chance against adversarial attacks. both ad-hme (exp) and ad-rvm simulate attacks against individual experts and have encouraging results. compared to ad-rvm, our ad-hme (exp) algorithm is not as sensitive as the ad-rvm algorithm to the initialization of kernel parameters. more importantly, ad-hme (exp) does not use ad-hoc choice of a constant ρ to control overcompensating issues as in ad-rvm [cit] . our ad-hme (gate) algorithm clearly outperforms ad-rvm especially when attacks are intense."
"the codebook re-sampling (rs) updates the codebook using split/merge operations similar to the proposed method. however, the decisions to split/merge are based on the size of codeword:"
"when a new file is added, besides publishing an entry to the file index with put(h f, o f ), the file owner will also extract and quantize the features to form codewords, then put them to the corresponding entries in the codeword index with put(h k, w k ). when a file is removed from the file index (with no owner), the corresponding codeword postings will be removed from the codeword index."
where θ includes gating parameters and expert parameters and p ij (y) is the density of y given the true value of θ. hme [cit] has wide applications and is known for its excellent convergence time on small to medium sized learning tasks [cit] . recently an efficient solution has also been proposed for large scale input in computer vision [cit] .
"as discussed in section 3, both the cbir search and codebook generation/updating take place on the codeword index. therefore, we evaluate the proposed system with a multi-threaded program that simulates the codeword index, where the updating process of each codeword node is executed in an individual thread. although as shown earlier in table 1, the computation cost is small for individual codeword nodes (e.g., an update iteration takes less than 2 seconds), simulating a large number of codeword nodes still takes considerable amount of cpu time and memory. for the large scale experiment (holidays + flickr100k, with 100,000 codeword nodes), it takes more than 1,000 hours of running time, and 380 gb of memory."
"document collection. the document collection contains around one million documents, i.e. web pages from medical sites. the documents are predominantly health and medicine websites that have been certified by the health on the net (hon) foundation 3, as well as commonly used health and medicine websites such as drugbank, diagnosia, and trip answers. the documents are provided in the dataset in their raw html format along with their uniform resource locators (urls)."
the remainder of the paper is organized as follows. section 2 discusses recent related work on adversarial learning. section 3 reviews hierarchical mixtures of experts and sparse bayesian learning. in section 4 we present our adversarial learning framework with sparse bayesian hierarchical mixtures of experts. we discuss our experimental results in section 5 and conclude our work and discuss future directions in section 6.
"holidays [cit] . a benchmark dataset for image retrieval. it contains 1,491 images with 500 queries and 991 corresponding relevant images. the number of relevant images for each query varies from 1 to 11. the sift descriptors coming with the dataset are used as local descriptors."
2. ad-hme (gate) clearly outperforms all the other five learning algorithms. note that gating functions rank the competence of experts in classifying a data point. ad-hme (gate) adaptively selects the expert that is most likely to generate the data point as shown in figure 1 .
"to address the above challenges, in this paper, we present a novel method to dynamically generate and update a global codebook, which considers both the discriminability and workload balance. while processing queries, each node collects the relevance information and workload data. with the relevance information, we maximize the information provided by the codebook about the retrieval results, thus minimizing the information loss incurred by quantization. with workload data, we aim to achieve a fair workload among nodes, thus avoiding overloading or underloading nodes. based on these two criteria, the codebook partitioning is updated routinely by splitting/merging codewords, thus allowing the codebook to grow/shrink in accordance to the data distribution. to minimize the cost of codebook updating, the decision whether a codeword should be split/merged is taken by its managing node individually. finally, the updates are synchronized across the network at the end of each iteration. as a result, the discriminability and workload balance is optimized continuously with the churn of the p2p network."
"the workload balance is measured by the difference between the current and \"ideal\" workload for each codeword. to make our codebook adaptive to dynamic p2p environments, the codebook partitioning is optimized by splitting/merging codewords, thereby allowing the codebook to grow/shrink in accordance to the data distribution and available resources. in summary, our algorithm is tailored to deal with distributed and highly dynamic p2p environments."
"we employ the best first search strategy to further reduce the feature number. our experimental data shows that mcone selects a subset of features with satisfying classification performances. however, mcone may select dozens, or even more than a hundred features, which may lead to the over-fitting problem for some big data areas with the \"large p small n\" challenge [cit] . the best first search strategy is widely used for the purpose of further reducing the number of selected features in a small scale feature subset. this study uses the version implemented in the fselector package version 0.19 in the software r version 3.0.2."
"where u m and u m are the workload thresholds for split and merge operations, respectively. the k-means clustering (km) updates the codebook by moving the codeword centroids to the cluster means. in the experiments, we implement a distributed version of k-means algorithm. during an iteration, each codeword computes the new centroid from the mean of its descriptors. the new centroids are synchronized across the network to form the codebook for the next iteration."
"for the section of model calibration, given by the daily personal trip tables disaggregated into 10 [cit] tmd, the proposed model was to be calibrated using the genetic algorithm to obtain the parameters for each set of parameters for the two corresponding venue types, and the objective function is to minimize the mae (mean absolute error) between the modeled tod percentages and the ground truth trip attraction tod percentage. for the campo daily traffic survey data, the rescaling work was applied through balancing total trip attraction volume in general map between the predicted trip attraction matrices and ground truth campo trip attraction matrices. the time of day (tod) regime were developed to allocate hourly trips in each taz area in order to explore the temporal and spatial characteristics of the mobility pattern in the study area."
"in this paper, we present a complete hardware architecture dedicated to hdr video that can fulfill drastic real-time constraints while satisfying image quality requirements. we propose a hardware vision system based on a standard image sensor associated with a fpga development board. we obtain good results with the conventional technique of hdr creating and a global tone mapping. we are also planning to develop and benchmark other several state-of-art algorithms both for computing radiance maps, calibrating hdr images, and local/global tone mapping. development of motion correction and alignment of images are also considered."
"as discussed in section 2.1, to facilitate the bovw retrieval process, our system builds inverted indices over the hash table interface of dht. before any further discussion, we briefly review dht. dht is a class of structured p2p overlay networks that provides get(k) and put(k, v) operations similar to a hash table, where k, v are the key and value of a table entry, respectively. while different dht implementations organize node connections with different topologies, most of them guarantee that a message from any node can reach the corresponding node in oðlog nþ hops, where n is the number of nodes. additionally, dht handles most issues in node management, including redundancy and failure recovery mechanisms in cases of nodes joining/leaving/failing, and caching and content mirroring for hot spots. therefore, dht forms an infrastructure that can be used to build more complex applications."
"in this paper, we consider adversarial attacks targeting the gating functions, the experts, and both. we model the adversary's optimal strategy using a set of hyper-parameters in the kernel function. learning proceeds as it searches for model parameters that best counter the optimal attacks. our main contribution is a divide-and-conquer adversarial learning framework inherited from the mixtures-of-experts architecture. more specifically, our contributions include:"
"information dominant criterion: feature f j will be kept, if it has the maximum information relevancy with target variable c in the candidate feature subset mic(f j, c) and not redundant with the features already selected."
"where is local random noise, x + and x − are a positive data point and a random negative data point in the test set. as f attack increases from 0 to 1 the intensity of attacks grows from none to the extreme where a malicious data point can be arbitrarily close to a legitimate data point, within a range of small random local noise. we compare six learning models: ad-hme, hme, ad-rvm, rvm, svm, one-class svm on the three data sets. all results reported are averaged over 10 random runs."
"of our network structure. section 4 discusses our proposed codebook generation and updating algorithms. section 5 discusses the retrieval process. section 6 presents the experimental results and discussions. finally, section 7 concludes this paper."
"the zonal trip attraction obtained from the campo od matrix and the modeled matrix are color coded in figure 7 . larger values were represented by darker color and smaller values by lighter colors. trip attraction with zero trip counts in campo data are identified as blank areas in the trip attraction pattern figure. analysis between the ground truth data and the modeled data suggest a consistency between the campo trip attraction spatial pattern and modeled spatial pattern. the proposed model show similar spatial characteristic toward the campo data. however, some inconsistencies can be found within the attraction heat maps. these inconsistencies can be attributed to the relatively lower number of check-in of foursquare data residences, which may also explain the slight inconsistencies in figure 7 . such characteristic may cause blank color in some area especially in the residential area which contains few lbsn check-in activities. despite the inconsistencies, in general, the model shares significant similarity between the trip attraction matrix generated from the model and the campo od matrix."
"the performance on two datasets exhibit different characteristics. most notably, the initial workload is much more balanced on holidays than ukbench (gini coefficient 0.349 versus 0.702), leaving a smaller margin of improvement for the codebook updating algorithms. this indicates that the initial flickr60k codebook is a better fit for the holidays dataset, which is not surprising, considered they are both collected from flickr. as the hot spots are smoothed out as the workload becomes more balanced, a greater query cost reduction is achieved on ukbench."
"publishing a new file is performed by a dht store operation: the file id h f and the list of owner nodes o f are stored by put(h f, o f ). for performance and fault tolerance considerations, such information needs to be reposted periodically, otherwise it would be removed from the owner list o f . therefore, removing an entry is achieved by stopping reposting."
"current research has found four major limitations in emerging travel demand data collection technologies including sampling bias, privacy concern, positioning accuracy and the lack of trip purpose confirmation. the first problem concerns some emerging travel data survey methods require users to actively participate or use the application for the survey purposes. for example, in gps-based survey, sampling bias varies based on income, education level, age and gender. low-income, lower educated, and minority populations bring low participation rates in travel demand collection [cit] ). secondly, especially in large-scale traffic data collection, legal and political issues about the privacy concern can make it difficult for transportation agencies, since most new technologies will request information to trace the identity of travelers. the third key limitation is positioning accuracy of survey data collection. the quality of data may be yielded due to the reliability of gps signals, the distortion or block of signals in large scale urban area, or the requirement of high level of technical expertise [cit] . the last key problem concerns is that the trip purpose confirmation might not be collected through emerging travel demand data collection technologies. unlike the conventional household interview survey which records characteristics of personal trip ends, new primary and secondary data collection methods cannot passively identify the location confirmation of destination and trip purpose. in order to measure them, many technologies such as gps, bluetooth, and cellphone-based methods need to ask travelers to enter their trip information which creates concern of reducing the sample size, or use data mining models to determine those from repeated route and activity patterns which will have significantly repeating patterns such as commuting trips. the rest of paper will be organized as follows. section 2 provides the literature review of research topic and method. the methodology and procedure will be introduced in section 3. next, section 4 introduces details on the experimental design as well as results from the proposed algorithm. finally, section 5 concludes the paper and provides some areas for the continuation of this research effort."
"lastly, similar to the bow model, statistical distributions of the codewords in a given image is utilized to represent the image. in this paper we utilize the well-studied tf-idf weighting scheme and cosine distance as the similarity measurement."
"as shown in fig. 6, the best mctwo model performs similarly well to or better than the three filter feature selection algorithms, however the features selected are not always the top-ranked ones evaluated by the filter algorithms. table 4 summarizes how each of the 4 features of gastric1 and 6 features of t1d selected by mctwo is ranked by the three filter algorithms."
"the rest of this paper is organized as follows. section 2 introduces the related work. section 3 provides an overview the average time to quantize the sift features into codewords. retrieval-cpu: the average time to compute the similarities of a query image. retrieval-dht: the delay of index lookup in a dht network [cit] . retrieval-avg. traffic: the average network traffic of all the nodes involved in a query, assuming a cost of 20 bytes per posting. retrieval-index transfer: the average time to transfer all the postings to the query node, assuming a bandwidth of 3.3 mbps between nodes (2013 q2 global average [cit] ). codebook updating-cpu: the average time to update the codeword on each node. codebook updating-avg. traffic: the average network traffic of all codeword nodes to update the codebook, assuming a cost of 160 bytes per descriptor."
", which is the same as the feature selection algorithm fcbf [cit] . but the filtering step mcone is implemented to evaluate the mic values between features and class labels, which will usually exclude most of the features. then the evaluation of interfeature mic values will be significantly speeded up. so the actual calculation time will not reach the upper-bound o(p 2 ) in most cases."
"various feature selection algorithms have been published, and they may be roughly grouped into three classes, based on how they determine the chosen features [cit] . a class i wrapper algorithm usually adopts an existing data mining algorithm to evaluate a feature subset, and applies a heuristic feature screening rule for the feature subset with the optimal data mining performance. it tends to consume exponentially increased time to find such a feature subset. class i algorithms usually use heuristic rules to find locally optimal solutions. the prediction analysis for microarrays (pam) [cit] algorithm calculates a centroid for each of the class labels, and selects features to shrink the gene centroids toward the overall class centroid. pam is robust for outlier features. the regularized random forest (rrf) [cit] algorithm uses a greedy rule by evaluating features on a subset of the training data at each random forest node. the choice of a new feature will be penalized if its information gain does not improve that of the chosen features."
"we established formal guidelines for manual topic annotation with category information (the number of concepts the topic title and description contain) and had three researchers annotate the 55 topics, achieving 75% agreement. based on the main disagreement, the annotators discussed and reviewed the guidelines. after a second step annotation, they achieved 98% agreement. specifically, they agreed on the number of concepts for all but one query. the main reason for disagreement was the definition of concept. while it seems rather straightforward to distinguish specific medical entities from general ones, some topics were ambiguous. for example, for the query \"white blood cell and bacteria\", the annotators could not reach any agreement: two annotators considered \"bacteria\" to be a concept while the third one did not. this query has been removed from the dataset for the analysis described in this paper. the topic distribution for the 50 test queries is as follows: 22 queries contain one concept (1-concept); 22 queries contain two concepts (2-concept); 5 queries contain three concepts (3-concept); and 1 query is ambiguous. for the 5 training queries we had four 1-concept queries and one 2-concept query."
"team aehrc's best run adds topic acronym expansion and spelling correction to their baseline. this greatly improved their retrieval performance on 1-concept topics, but 2-concept topics results are similar to their baseline, and 3-concepts are lower."
"in recent years, the thriving development of wireless communication, social media technologies, positioning and computing technologies has presented new opportunities for transportation planners and engineers to develop effective solutions to collect travel demand data with high spatial and temporal resolution for dynamic travel demand analysis."
"information retrieval (ir) evaluations following the trec-style tradition typically focus on comparative evaluation of systems and methods, but often put too little emphasis on post-hoc analysis of the task, the associated data, or the submitted runs. in this paper we investigate results of an ir evaluation initiative at clef (crosslanguage evaluation forum) 2013, the share/clef ehealth evaluation lab 1 (short clef ehealth). specifically, we analyse task 3, which is concerned with improving ir systems supporting laypeople in searching for and understanding their health information [cit] . 1 http:// [cit] .dcu.ie/ permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. sigir'14, july [cit] 2014 the specific use case for the evaluation lab is as follows: before leaving the hospital, a patient receives a discharge summary. this describes the diagnosis and the treatment that they received in the hospital. the first task considered in clef ehealth aims at extracting names of disorders from the discharge summaries, while the second task requires normalisation and expansion of abbreviations and acronyms present in the discharge summaries. the use case then postulates that, given the discharge summaries and the diagnosed disorders, patients often have questions regarding their health condition. the goal of the third task, a medical ir task, is to provide valuable and relevant documents to patients, so as to satisfy their health-related information needs. surprisingly, in this task, no team managed to outperform the strong bm25 baseline provided by the lab [cit] . in this paper we examine the topics provided by the organizers for this task, and define levels of query complexity based on the number of concepts in a query. we manually annotate the topics with their complexity category and analyse the performance of participants' runs (i.e. the baseline and best performing run) on these query categories."
"the contributions of this paper relate to: 1) analysis of the relationship between query complexity and ir effectiveness; 2) analysis of the performance of different ir techniques through categorisation (based on retrieval technique employed), grouping and analysis of teams baseline runs; 3) analysis of the performance of the best runs across topics with different levels of difficulty; 4) analysis of patterns in the official runs to isolate the impact of individual techniques, methods, or external resources on ir effectiveness."
"coap, such a low power network protocol, is suitable for the system proposed in this paper in terms of savings and efficient greenhouse monitoring and control."
"although the same process was used to build each topic in the task (described in 3), we observed differences among topics. these differences may be due to the fact that the topics are generated, from a highlighted disorder in a discharge summary, by a human estimating what the information need might be. therefore, some topics may be directly related to a disease, while others enquire about the relationship between two disorders, or symptoms, for example. we thus categorise queries based on complexity, where complexity corresponds to the number of concepts in a query. we define a concept as a specific medical entity. for example, \"diabetes mellitus\" is a concept, but \"disease\" is not."
"each team participating in the clef ehealth task was required to submit a baseline run which did not use any external resources. figure 2 shows the results of the participating teams baseline runs for each topic category. for comparison, the last group is the clef ehealth task baseline described in the previous section. we first observe a pattern in each group. 1-concept topics always perform the best (apart from uthealth), and 2-concept topics always get the lowest results (apart from mayo). this is similar to what we observed for the task baseline in 4.1. medinfo achieves the highest p@10 on 1-concept topics, mayo the highest performance on 2-concept topics, and both medinfo and mayo obtain the highest performance on 3-concept topics. we found that many participating teams used similar ir techniques for their baseline run. figure 3 shows results of the baselines grouped by ir model. group 1 uses language modelling (lm) retrieval approaches (teams mayo, aehrc, medinfo and kc). group 2 uses the vector space model (vsm) and variants (teams thcib, uthealth and ohsu). group 3 uses divergence from randomness (dfr) for retrieval (team uog). the last team, qut, used their own proprietary ir model, topsig. as observed for the baseline in the previous section, 1-concept queries obtain the highest p@10 and 2-concept queries the lowest. however, as expected, overall the lm approaches perform better than the weaker vsm baseline. further, the lm approaches appear to cope much better than the other techniques with 2-concept topics."
"the task organizers provided a baseline experiment, using the bm25 retrieval model with a standard stop-word list containing the okapi stop-words (222 stop-words) for stop-word removal [cit] . the baseline performs two types of document preprocessing: character normalization (i.e. mapping characters with diacritical marks to the equivalent characters without) and word normalization (e.g. correcting frequent spelling errors). spelling correction is based on a list of 9533 spelling errors from medical documents [cit], which was added to a list of 4192 frequent spelling errors compiled from wikipedia. during indexing, misspelled words are replaced with their corrections from this list. table 1 shows the results of the task baseline for each of the 50 test topic categories."
"generally, there are two strategies to distribute index tuples: document partition (or local indexing), and term partition (or global indexing), both are well exploited in the literature [cit] . with document partition, each node manages an index for a subset of documents. a query will be sent to all index nodes, and be answered by combining the lists of candidate documents returned from them. with term partition, each node manages an index for a subset of terms. a query will only be sent to the nodes managing corresponding terms, and answered by combining the inverted list returned from them. therefore, document partition typically has a higher network cost than term partition, especially when the index has a good term sparsity [cit] . this is not a very big issue in shared-memory or distributed servers, but does pose a challenge in p2p networks, as the nodes in p2p networks are loosely coupled and have much lower bandwidth. as a result, term partition is a more popular choice in p2p networks [cit] ."
"as a side note, we observed that teams which used the same baseline ir approach did not obtain the same results. this task (and other tasks) could benefit from providing stricter guidelines on description of baseline experiments, including all parameter settings and all preprocessing steps."
"standard machine learning and data mining algorithms assume training data and test data follow identical distributions. in adversarial learning this assumption is frequently violated, which undermines the generality of trained learning models. as adversaries become more sophisticated, their abilities of making versatile attacks grow. as a result, learning tools used in security applications are facing increasingly unpredictable and rapidly changing attacks. this calls for more flexible modeling techniques to handle ambiguities in the corrupted input. in this paper, we present an adversarial learning framework using bayesian hierarchical mixtures of experts (hme) as the baseline learning model. our framework implements an optimal attack strategy that minimizes the likelihood of malicious data in each round of learning and a divide-and-conquer learning model that counters this type of adversarial attack. the learning process resembles the two-sided arms race by interactively manipulating data against the classifier."
"the solvent mixtures of water and dmso have a unique chemical and biological importance. water-dmso mixture is widely used as a cryo-preservative. these mixtures show maximum deviation from ideal behavior in the dmso mole fraction range of 0.20 to 0.45 [1 -5] . it has been shown experimentally that the water-dmso hydrogen bond is much stronger than the water-water hydrogen bond, which could be one of the major reasons for these macroscopic deviations [6 -7] . the associative properties of dmso responsible for non-idealities in dmso-water systems are also found in its associations with polarizable non-ionic and ionic substances."
"p eer-to-peer (p2p) networks, which are formed by equally privileged nodes connecting to each other in a self-organizing way, have been one of the most important architectures for data sharing. popular p2p file-sharing networks such as edonkey 1 count millions of users [cit] and tens of millions of files. unlike webpages which mainly consist of textual documents such as news, blog articles or forum posts, multimedia files play a dominant role in most p2p networks [cit] . the ever-growing amount of multimedia data and computational power on p2p networks exposes both the need and potential for large scale multimedia retrieval applications such as content-based image sharing, and copyright infringement detection."
"we have performed extensive md simulations on systems containing sodium and chloride ions in mixtures of water and dmso for the dmso mole fractions of 0.21, 0.35 and 0.48. the simulation box lengths (l) and solvent densities (ρ) and the solvent particle numbers (n 1 and n 2 ) are given in table 2 . over 100 ps simulations were done for each composition using the verlet algorithm and shake algorithm to keep the intra-atomic distances in the solvents fixed. representative results over a 10 ps duration are given in the results section. different initial configurations such as only dmso molecules surrounding the ions and only water molecule surrounding the ions as well as a random configuration of both molecules were used as starting points. the results relevant to the present problem are presented in the next section. table 2 . details of the simulation boxes at different compositions."
"team mayo adds two ranking systems combined to their baseline to obtain their best run. the first is a linear combination of markov random field (mrf) model and a mixture of relevance models (mrm). the second is based on a umls cui-representation of the documents, topics, and discharge summaries. they obtained an improvement for each topic category. it cannot be determined from their runs which part of this system is responsible for the improvement in performance, it is very likely the combination."
"the bag-of-visual-words model represents each image with a bag of quantized codewords derived from local features, and measures the similarity between images with the bovw histogram analogous to a bag-of-words model of text retrieval [cit] . the retrieval process is typically supported by an inverted index. though we are not aware of any bovw based p2p cbir systems, many existing p2p text retrieval systems build a distributed inverted index in a highly efficient manner over dht, using term id as key and document id as value [cit] ."
"the best known analysis of an ir evaluation is the reliable information access (ria) workshop [cit], where retrieval results for different runs and systems were analysed manually to detect weaknesses and system failures in ir systems. one of the main findings is that most systems suffer from the same errors. harman and buckley conclude that \"it may be more important for research to discover what current techniques should be applied to which topics, rather than to come up with new techniques\" [cit] ."
"we compare groups of runs (grouped together by their ir model) rather than individual teams. we identified the three major ir models used in the submissions for task 3 as lm, the vector space model (vsm) and its variants, and divergence from randomness (dfr). we excluded retrieval approaches based on proprietary ir models from the analysis described in this paper. figure 4 shows the results of the participating teams' best runs for each topic category. the baseline run was the best performing run for three participating teams (team medinfo, kc and uthealth). the six other teams obtained an improvement over their baseline using various methods. two teams obtained a significant improvement in their performance on 1-concept queries (teams mayo and aehrc). on 2-concept queries, 4 teams obtained a significant improvement (teams mayo, aehrc, thcib and ohsu). as for the 3-concept queries, two teams obtained bet- the first three teams which obtained better performance than their baselines (mayo, aehrc and uog) used very varied approaches."
"we would like to thank professor fujiang liu for his valuable comments and suggestions which have helped us to improve the quality, organization, and presentation of our manuscript."
 reliability is the quality aspect of a web service that represents the capability of maintaining the service and service quality. the number of successfully used per month represents a measure of reliability of a web service.
 availability is the quality aspect of whether the web service is ready for immediate use. it represents the probability that a service is available. larger values represent that the service is always ready to use while smaller values indicate unpredictability of whether the service will be available at a particular time.
"two general association-based approaches have been used in imaging genetics. the candidate gene association approach is a strategy for studying the effects of a well-defined candidate gene that is a good candidate because of its implication in the biological pathway of the disease on measures of brain biology and circuits. on the other hand, the genome-wide association (gwa) approach seeks to discover novel genetic loci which might be related to brain structure and function through screening the entire genome for potential associations. to date, a number of gwa studies of imaging phenotypes have been conducted and some of these studies reported multiple single nucleotide polymorphisms (snps) related to risk for schizophrenia and alzheimer's disease [cit] ."
"bold fmri was performed on a ge signa 3t scanner. a gradient epi sequence was used to acquire 300 images (26 axial images were preprocessed following the same procedures as nback task (slice-timing correction, motion correction, spatial normalization and smoothing) in spm5. a contrast image for response inhibition (no-go) was generated per subject per session using the correct trials only."
"the nback comt dataset. a third dataset consisted of nback data from 216 ncs (43 val/val individuals, 106 val/met individuals, and 67 met/met individuals). the three groups were matched for age, gender, wrat score, handedness, percent correct in 2-back task, and tsnr (one-way anova p values were all above 0.2; see s3 table) ."
"in terms of statistical power, the roi approach is equivalent or better than the voxel-wise analysis approach in detecting significant group differences. different roi summary measures are more similar when using the smaller, focal spherical rois. the drawback is that this type of roi requires a strong a priori hypothesis to pinpoint the center of the sphere and most of individual subjects' peak of activation were not within the sphere selected based on a priori coordinates from published literature. applying a p threshold can help in selecting activated voxels within an roi, especially for the mean, median, and eigenimage weighted mean measures."
"the users use the web browser to send a http request, including some web form arguments, to the web server container. the web server invokes the mds servlet, which uses the form arguments to determine what plugins to use to retrieve the request gml data and the xslt transform tools to apply to it. then use the standard wsrf resource property query interfaces to retrieve service wsdl information from an index. every gt4 web services container includes a default mds-index service with which any gt4 services running in that container are automatically registered. thus, each grid system has an index that allows one to discover what services are available. gt4 is configured to use mds mechanisms to good effect for discovering and monitoring of gt4 services. every gt4 web services supports a minimal set of qos properties and thus can be registered easily into one or more aggregators for monitoring and discovery. the mds use servlet passes arguments to the transaction tool, which then retrieves the appropriate data and xslt transform. the mds servlet applies the xslt transformation to the xml data and returns the result to the web server, which sends it back to the client's web browser. the discovery model is shown in figure 3 ."
"there were three groups of subjects whose data were used in this project: patients with schizophrenia (pts), their unaffected siblings (sibs), and normal control subjects (ncs). all participants were recruited nationwide as part of an ongoing family study of schizophrenia at the clinical brain disorders branch, nimh, nih (protocol 95-m-0150, dr. weinberger pi). the study was approved by the institutional review board of the intramural program of the national institute of mental health, national institutes of health. all participants were assessed with a structured clinical interview for dsm-iv [cit] . they were prescreened to exclude those with premorbid iq below 70, those with recent drug or alcohol abuse (within 1 year) or more than 5 years of previous abuse, and those with medical or neurological conditions. most pts were on a stable regimen of antipsychotic medication (typical neuroleptics and atypical antipsychotics). sibs and ncs were selected with the additional requirements that they are not on any current psychotropic pharmacological treatment. further, ncs should not have a first-degree relative with a schizophrenia spectrum disorder. after a complete description of the study to the subjects, written informed consent was obtained."
"mds is the information services component of the globus tookit4 and provides information about the available resources on the grid and their status [cit] . it is a suite of web services to monitor and discover resources and services on grids [cit] . mds builds on query, subscription, and notification protocols and interfaces defined by the web services resource framework (wsrf) and web services notification families of specifications and implemented by the gt4 web services core. discovery is the process of finding a suitable resource to perform a task. this process may involve both finding which resource are suitable and choosing a suitable member from that set. the discovery process requires the ability to collect information from multiple, perhaps distributed information sources. to meet this need, mds provides so-called aggregator services that collect recent state information from registered information sources. the mds architecture is shown in figure 1 ."
"web services are frequently used to build a distributed system which can be accessed over internet. it is loosely coupled to enhance productivity, simplify using, get reusability and improve system expansibility. but with the widespread proliferation, the qos becomes an important factor in distinguishing the success of a web service application."
"as described earlier, we tested the roi single-value summary measures using four datasets: 1) the nback task split dataset, 2) the nback task sibling dataset, 3) the nback task comt dataset, and 4) the flanker task dataset."
" regulatory is the aspect of the web service in compliance with the standards such as soap, uddi, wsdl, etc and the established service level agreement. strict adherence to correct versions of standards by service providers is necessary for proper invocation of web services by service consumers."
"because many experimental paradigms in imaging genetics activate focal regions that can be pre-specified based on a priori knowledge, reducing the whole-brain search to single-value summary measure of rois is a viable approach, as evaluated in the current study. two questions arise when performing the roi analysis. the first question is how to select an appropriate roi [cit] for extracting the single-value summary measure. the second, potentially more difficult question, is how to find an roi measure that is robust and sensitive to group differences [cit], which is the primary focus of the current study."
"nback task. the nback working memory task consisted of four 30-sec blocks of the 0-back condition alternating with four blocks of the 2-back condition, in which a random series of numbers (1 to 4) were presented every 2 seconds for 500 milliseconds at set locations in a diamond-shaped box. during the 0-back condition, participants were asked to respond to current number being presented by pressing the corresponding keys on a keypad; in the 2-back condition, participants encoded the number currently being shown and simultaneously recalled and responded to the number presented two stimuli earlier."
"some of the measures we are proposing here selected only a smaller subset of voxels out of a large roi (i.e., measures 4, 5, 6, 7, 8, 9, and 10). thus, the localization of the smaller cluster should be taken into consideration when comparing the extracted roi summaries across groups. if the localizations of the selected cluster of voxels are different across groups, then the group difference in magnitude of roi single value summaries would be confounded by the clusters' locations. since most of the roi summary measures were related to selecting smaller clusters around the peak voxel of the larger roi, the peak coordinates were used to approximately represent the smaller clusters' center for each individual contrast map."
"the nback sibling dataset. the dataset include three groups: ncs, sib, and pts, with 43 subjects in each group. to reduce the confound effect of task performance on the prefrontal activation [cit], we only selected the subjects with relative high performance in the 2-back task with the percentage of accuracy greater than 70%. the three groups were also matched for age, gender, wrat score, handedness, percent correct on the 2-back wm task, and tsnr (oneway anova p values were all above 0.2; see s2 table) ."
"4. voxels above percentile threshold a. we selected voxels whose t values were among top percentiles (25 th %tile, 10 th %tile, or 5 th %tile) of all voxels. the mean of the linear fit coefficient of these voxels was taken as the roi summary measurement."
"5. peak voxel within the roi for this approach, we chose the voxel with the maximum t statistic in an roi from the spm t map, and then from the spm contrast map we extract this voxel's linear fit coefficient (beta), which is essentially the slope value from the first-level multiple regression, as the roi's summary measurement."
"in summary, the roi summary measures always demonstrated a group difference whereas in half of the datasets the voxel-wise approach sometimes failed to show a significant effect. one advantage of extracting roi summary over the voxel-level analysis is that the roi summary is extracted at the individual subject level, partially compensating for inter-subject anatomical and functional variability. even though the image data have been normalized to standard space and spatially smoothed, there is still a significant degree of variability across subject's anatomy [cit] . in addition, even though the voxels across subjects can be aligned perfectly, the activation may not be. by expanding the search domain to the whole roi and extracting individual-specific peak activation, it is more likely to precisely pinpoint the real activity for each individual."
"the effect of genes on cognition and behavior is modulated through a complex series of intermediate steps including changes at the molecular level and alternations in neural circuits [cit] . in the past decade, imaging genetics has emerged as a promising genetic association analysis approach that uses a variety of state-of-the-art anatomical or functional brain imaging measures as phenotypes to evaluate the impact of genetic variation on the susceptibility to neuropsychiatric disorders [cit] . the quantitative measures from so-called structural imaging (e.g., volume, cortical thickness, surface area, etc.) or brain function (physiological response of the brain during information processing or in the so-called \"resting state\") derived from neuroimaging modalities are potential intermediate phenotypes that are both genetically associated with neuropsychiatric disorders and closely related to the underlying neural mechanisms of the disease process [cit] ."
"traditionally, the first step toward dimensionality reduction of fmri data is built upon model-driven approaches such as performing a general linear model to reduce the 4-d time series to a 3-d statistical parametric map (spm) for each subject. and then a second-level voxel-wise statistics is performed to find activated voxels across subjects [cit] . however, correction for multiple comparisons across all the voxels results in much reduced statistical power [cit] . in addition, due to the anatomical and functional variability between brains of different individuals, the peak locations of the regional activation vary significantly across subjects, even with careful spatial normalization and smoothing [cit] . therefore, the sensitivity of the grouplevel analysis may be compromised by this approach [cit] ."
in this work we propose to modify the wsdl file in order to increase qos support to the web service. then we propose a method of web services discovery on gt4 mds.
"in summary, the mean, median, and eigenimage weighted mean measures were greatly affected by the inactive or deactivated voxels in an roi. by applying a threshold and selecting only the top activated voxels, the effect sizes significantly improved. this finding is consistent with [cit], who observed that selecting the top fraction voxels of an roi can increase signal-to-noise ratio and consequently the sensitivity. in comparison, other measures already selected the voxels that are around the peak (sphere, peak-correlated voxels) or above certain threshold (percentile). thus the effect sizes did not improve much by applying an additional p-value threshold."
"testing executions were performed on a intel core 2 duo t9300@ 2.5ghz machine, with ddr 666mhz 3.0g of ram and windows xp sp3 operation system as web server. java as developing language, ide: ecplise5.0, apache tomcat as web services container ie7.0 as web browser, gd4 as grid container and grid system environment figure 4 shows the testing results."
"in this project we examined the roi single-value summary measures using four datasets, with the purpose to test the reliability of each measure across datasets. firstly, it has been consistently observed that patients with schizophrenia (pts) show increased brain activity in dlpfc regions compared with normal control subjects (ncs) during the nback working memory task when performance does not differ between the groups [cit] . we compared the replicability of the roi summary measures by comparing two performance matched groups (pts vs. ncs) using an nback task dataset that was split into two matched subgroups."
"in addition, to validate the robustness of our results and to ensure good replication power, we ran many random samplings (without replacement) on this dataset. that is, from among the 100 pts in the first dataset, we used an automated script to randomly choose 50 of them during each iteration. then among the 100 ncs in the same dataset, we selected 50 ncs that were matched for age, gender, wrat, and 2-back accuracy (t test p values were all above 0.2) with the 50 pts. we repeated this random sampling and matching procedure 1000 times so that each sampled subset contained 50 pts and 50 matched ncs that were matched for age, gender, wrat, and 2-back accuracy."
"imaging phenotype measures in gwas depend on large groups of neurons whose structure and function are affected by multiple genes. while studying the relationship between multigene and multiple intermediate neuroimaging phenotypes can generate exciting new discoveries, it places a big challenge for researchers to choose an overall valid strategy and apply suitable computational tools and statistical methods [cit] . there are two major issues related to the gwas approach in imaging genetics. one is that relating whole genome and whole brain data requires immense computational power [cit] . therefore, prior studies have typically made a significant reduction in imaging space and only examined a few imaging variables, such as using magnetic resonance imaging (mri) measures of hippocampal atrophy [cit], total cerebral brain volume (tcbv), white matter hyperintensity (wmh) volumes, etc. [cit] . secondly a whole brain voxel-wise approach in genome-wide searches would also likely have low power. a stringent multiple comparisons correction is needed for searching over the entire genome and all the voxels in the brain to control for the number of false positives."
"the nback task split dataset. this dataset consisted of 100 pts and 100 ncs. to compare the replicability of the roi summary measures, the dataset was split into two equal subsets. each subset comprised of 50 pts and 50 ncs. the ncs in each of the split subsets were matched (t test p values were all above 0.2; see s1 table) for age, gender, premorbid iq estimated by the scores on wide range achievement test (wrat), handedness, task performance on 2-back blocks (% correct), and the image time course stability as measured by temporal signal-to-noise ratio (tsnr), which is calculated by dividing the mean of the time course by its standard deviation (tsnr; [cit] ). the two nc groups and the two pt groups were also matched, respectively."
"replicability across all the datasets. we averaged the flanker nogo task ncs vs. pts group comparison results with the ones generated by the split dataset. overall the patterns replicated what we observed in dataset 1. for the 10 mm spherical roi, all measures except for the peak-cluster extent measure generated medium level effect sizes; whereas for anatomical and task-activated cluster, the percentile (measure 4), top n contiguous voxels (measure 6), sphere surrounding the peak (measure 7), and peak-correlated voxels (measure 8) consistently generated medium effect sizes."
"while the roi approach can increase statistical power, strong prior hypotheses of the particular roi used are required. these prior hypotheses of roi selection can either be based on published literature of the task of interest [cit], made on an anatomical basis, or focused on the functional activation induced by the task [cit] . despite of more advanced roi selection methodologies based on multivariate pattern analysis (e.g., independent component analysis, ica) [cit], since the primary goal of this work was to assess and compare the ten roi-based single summary measures described above, we chose the three simple, univariate roi definition strategies to detect group differences in an attempt to provide practical guidance for future work for selecting the most optimal imaging phenotype/measure in large scale genome-wide imaging genetics studies."
"qos is the ability to provide different priority to different applications and users to guarantee a certain level of performance to data flow. typical qos properties associates with a web service execution cost and time, availability, successful execution rate, reputation, and usage frequency [cit] . the qos covers a range of non-functional properties of web-service such as response time, performance, security, availability and reliability. the service providers provide differentiated capacity model for different service types to ensure appropriate qos levels for different customers' applications. for example, a complex spatial analysis web service might require good throughput, while a simple searching web service might require a good response performance."
"the unaffected siblings (sibs) of patients with schizophrenia have also been shown to manifest dlpfc hyperactivity compared with control subjects with similar levels of performance [cit], suggesting this phenomenon to be a heritable trait measure related to genetic risk for schizophrenia. therefore we included unaffected siblings in the second dataset to examine which of the roi summary measures tested are more sensitive to uncover genetic risk shared by patients with schizophrenia and their unaffected siblings."
"in order to describe the contract between the service provider, the customer and the grid system, several specifications defined by xml based language have been proposed such as wsdl, which is complement to the service description implemented, ws-agreement specification [cit] which is used within a framework allowing the management of web services and their compositions."
"the effect of interest was the 2-back versus 0-back contrast, using which abnormal right dorsolateral prefrontal cortex (rdlpfc) function had been previously reported in patients with schizophrenia and unaffected siblings [cit] ."
"a. for each individual subject's time series, we extracted the timecourses of all the voxels within the roi and then the first eigenimage of these voxels was calculated by svd. each voxel's linear fit coefficient was weighted by this eigenimage and the weighted mean was taken as the roi summary measurement."
" performance is the quality aspect of web service, which is measured in terms of throughput and latency, and response time. throughout represents the number of web service requests served at a given time period. latency is the round-trip time between sending a request and receiving the response. higher throughput, lower latency and less response time values represent good performance of a gis web service."
"resolving this issue is especially critical for studies using functional magnetic resonance imaging (fmri), a popular neuroimaging tool used in imaging genetics studies. a practical solution is to apply dimensionality reduction on the acquired fmri time series that contains tens of thousands of voxels in a single brain volume."
"this study attempted to provide guidance in defining an optimal single-value summary measure that can be reliably used in genome-wide imaging genetics studies. overall, we found that four roi single-value summary measures were reliably consistent across datasets to show significant group differences and produce relatively higher effect sizes: 1) top percentile (measure 4), 2) the top n contiguous voxels surrounding the peak voxel (measure 6); 3) a sphere surrounding the peak voxel (measure 7); and 4) peak-correlated voxels (measure 8)."
"as described in the method section, we used a permutation test to compare the peak localization between groups. s5 table lists the summary of the permutation results and the percentile of the actual group centroids distance in the permutation distribution for each dataset used in the study. it shows that there is no significant difference between groups in the peak location. therefore, the group differences we observed in the roi summary measures were not confounded by roi localization differences between groups."
"test of small roi localization differences between groups. any differential effects observed across the different summary measures may have been influenced by whether all the voxels in an roi or only a subset of voxels within an roi are chosen to create the summary measure. here we tested whether the localization of the selected cluster of voxels is different across the groups in our study. since many of the roi summary measures were related to selecting voxels surrounding each individual subject's peak, as a result, a between-group difference derived from such a measure may be driven by a difference in peak locations between groups. this issue was further examined by a permutation test."
"web service description language (wsdl) is an xml format for describing network services as a set of endpoints operating on messages containing either documentoriented or procedure-oriented information [cit] . specifically, it defines via its porttype component, the web service's abstract interface, specifying the operations that the service supports and for each operation, the format of the messages that the service sends and receives. the client programs read the wsdl files to connect and communicate with the service. the current wsdl file contains the following information: the number of inputs and outputs, the variable type of each input and output, the order the inputs are given and outputs are returned, and how the web service should be invoked. but from the wsdl file, we can not get the qos information. to solve this problem we propose to extend the wsdl to support the qos information description."
"the aim of the present work was to compare the effect of ten different implementations of roi single value summary measures on the results of group analysis, including both diagnostic groups (patients with schizophrenia vs. normal control subjects) and genetic groups (comt val & met polymorphism, val/val, val/met, met/met). we also compared the three different types of roi selection strategies generally adapted by researchers in the literature: 1) a spherical roi centered around previously published group peak coordinates; 2) an anatomical roi encompassing multiple brodmann areas based on a priori information; and 3) task-activated cluster within an anatomical roi. we found that four roi single-value summary measures were reliably consistent across datasets to show significant group differences and produce relatively higher effect sizes: 1) top percentile (measure 4), 2) the top n contiguous voxels surrounding the peak voxel (measure 6); 3) a sphere surrounding the peak voxel (measure 7); and 4) peak-correlated voxels (measure 8). different roi summary measures are more similar when using the smaller, focal spherical rois."
the work proposed in this paper provides an approach to describe web services qos information in wsdl and a discovery model in which the functional and nonfunctional requirements is to be taken into account for the selection. the test results illustrating the method have high performance in practices.
" security is the quality aspect of the web services of providing confidentiality by authenticating the parties involved, encrypting messages, and providing access control. because distributed gis system is deployed and invoked over the public internet, security becomes more important. the service providers have different approaches and levels of providing security depending on the service requester."
"besides editing wsdl documents. another necessary step is to increase qos support to the web services' xml schema documents, and through the extension of the getmultipleresourcepropertiespolltype which is one of the parameters for query aggregator source in configuration file. the qos properties can be retrieved from a index service as resource properties. the aggregator framework is a software framework used to build services that collect and aggregate data. then query aggregator source will request five qos properties in the grid services. there is no limit on the number of \"resourcepropertyname\" parameters that can be specified."
"in this project we only employed three simple roi definition strategies. more advanced methodologies based on multivariate pattern analysis, such as ica and multi-voxel pattern analysis (mvpa), also can be used to define rois. further studies should be conducted to see how these new roi definition strategies would interact with different roi summary measures."
"the number of voxels in an roi may also affect the effect size. in general, we expect to see greater variability between roi summary measures in rois that contain a larger number of voxels such as the anatomical rois. as for the smaller 10-mm spherical roi, the voxels selected using the different roi summary measures may largely overlap and result in similar effect sizes across the measures."
"in this study, the roi summary measures were based on image data that were spatially normalized and smoothed, and the rois were defined from atlases. the roi analysis may be more precise if the rois are defined based on individual anatomy [cit] . going forward, we plan to examine how roi summary measures can be improved for the unnormalized and unsmoothed fmri image data by defining the rois in native space using anatomical landmarks."
"qos properties provide support for service publishing and selection, but how to select web services that can well meet the requirements of web services' consumers becomes a problem."
"in this paper, the optimization stage is carried out using the modified conjugate gradient method, that is, the mdy method. in order to compute the step-length, the standard armijo line search is used. given an initial step-length"
"for the stopping criteria, we consider the following     the algorithm for solving the discretized optimal control problem is given as follows"
"of synapse-dependent asynchronous releases of neural transmitters (see below) following an independent pois- son process at each efferent synapse. the neural transmitters released by the spike-driven synchronous and calcium-dependent asynchronous events follow a fourstate decaying dynamics based on a modification of the tm model,"
"value units is the excitation domain. in our experiment, the control domain might correspond to implantable cardioverter defibrillator (icd) implanted in the chest of a patient to avoid sudden cardiac death. consequently, the control domain for test case 1 can be interpreted as a heavier (with bigger size) icd while the control domain for test case 2 can be interpreted as a lightly (with smaller size) icd that can be placed into the chest. the initial values for the state variables are given as"
"usually, spontaneous activities can be observed after about a week in vitro and the activities are later synchronized into episodic network bursts [cit] . interesting patterns of these neural bursts have been reported [cit], where the activity level of firing rate in the burst can have repeated peaks of rise-and-falls called reverberations at a time scale of hundreds of milliseconds following the initial spike of activities [cit] . these so-called \"super bursts\" [cit] can last for seconds and, for their similarity in the time scales, are thought to be important to understand cognitive functions such as working memory on the cellular and network levels [cit] ."
"the network activity was acquired at a sampling rate of 7.7 khz for each electrode. each recording data set includes network activity of 5 min. but, the data sets containing unstable activity patterns, long silent periods, or abnormal activities with, e.g., strong noise, were excluded for further processing. the qualified data sets for further processing are listed in table 1 . spontaneous activities can be observed after about 2 weeks in vitro, comprising isolated spikes and short bursts involving many neurons (electrodes), e.g., the one shown in fig. 1 . the isolated spikes produced in neurons are detected by the brainwave software through the precise timing spike detection with threshold values that are 8 times of the standard deviation of spike-free signals."
"on the other hand, the initial value for the control variable is set to be zero in the control domain, i.e. no current is applied at the beginning of the numerical experiments."
"we implement a physiologically realistic model of neuronal systems [cit] ) on a geometricallyconstrained, two-dimensional network and identify sets of parameters that can produce reverberatory bursts qualitatively similar to the experimental observations. with all dynamical variables being available in computer simulations, we clarify the roles played by the neuronal noise as well as the depletion of synaptic resources in the continuation and termination of the reverberatory bursts. we find that the depletion, which is responsible for terminating the burst events [cit], is also important in restoring the synchrony of reverberatory activity during the bursts."
kunisch and wagner [cit] proved that the control-to-state mapping is well-defined for the optimal control problem in (1) . this allows us to rewrite the cost functional in (1) as
"there have been active studies on the initiation of in vitro neural bursts [cit] focusing on both the role of hub neurons [cit] and topological effects [cit] . the development of highdensity mea systems has enabled more detailed investigation of the activity propagation in the neural bursts. notably, the collective dynamics of spiking neurons such as center-of-activity trajectory (cat) allow the identification of a propagation phase and a reverberation phase in the progression of a burst event [cit] )."
"are the voltage dependent dynamic parameters, and the threshold v th of membrane potential defines the spiking events which result in synchronous releases of neural transmitters at the efferent synapses. additionally, fig. 3 a predictability as a function of presumed spreading speed for culture c at 33 div. inset is a similar plot from a simulation. b time-histogram of a typical reverberatory burst with identified activity peaks color coded with corresponding centerof-activity trajectories in the insets. the numbers next to the rate peaks show the fractions of the evoked spikes out of all spikes in the activity peaks. c time-histogram for a simulated reverberatory burst. the fractions of evoked spikes are similarly labeled for each activity peak a residual calcium variable r ca driven by the spiking events,"
"in this paper, we have presented the numerical experiment results for the optimal control problem of monodomain model for two test cases. test case 1 consists of a bigger size of the control domain, while test case 2 consists of a smaller one. our results show that when the size of the control domain has changed to a smaller one, more iteration is required to dampen the excitation wavefront. besides that, longer time is also needed for dampening the excitation wavefront if a small size control domain is used. moreover, by implanting a more lightly icd, higher electrical shock is required to be delivered to the patient in order to restore the normal heart rhythm. alternatively, if a heavier icd is implanted in a patient, lower electrical shock will be delivered to the patient for restoring normal heart rhythm. if we can balance the trade-off between the size (weight of icd) and the current (electrical shock delivered by icd), it will be good news for patients that decide to implant icd."
"the detailed dynamics of different factors can be further analyzed in a simulation. in fig. 8, we plot the residual calcium concentration, depleted neural transmitter fraction (z + q), and the active neural transmitter fraction over the very burst shown in fig. 3c . taken from the computational model, the depletion of neural transmitters to the z and q states is driven by the activated transmitters y from the spiking activity. the spiking activity also increases the level of residual cal- cium, which controls the noisy, asynchronous releases of the neural transmitters leading to the reverberation in a burst. apart from making the role of synchronous releases more important in the activation of neurons, the depletion also leads to longer recovery time of neural transmitters. coupling with the rapid decay of calcium between the reverberation peaks, this leads to the lowing in the mean level of residual calcium towards the end of a burst."
"where   v  denotes the partial derivative with respect to v,   w  denotes the partial derivative with respect to w, and o v denotes the transmembrane potential in the observation domain. next, the state and adjoint systems can be formed using (3) - (6) together with the following initial, terminal and boundary conditions."
"the factors driving the spiking activity of a neuron during a bursting event include the synaptic action spreading from its presynaptic neurons and the spontaneous activation driven by its own neuronal or synaptic noises. to identify the dominating factor contributing to a spike, we use the simple linear-spread diffusive model (1) parametrized with a spreading speed, which can be determined by a maximum likelihood method for each recording as documented in table 1 . while a more sophisticated propagation model might produce a better match to the observed behavior, the added complexity is not expected to change our conclusions qualitatively. using the propagation model (1), we classify spikes into evoked spikes and spontaneous spikes. we then determine their ratio for all rate peaks of a burst. the results of evoked-spike fractions plotted in fig. 6a for 33 div recording of culture c show an increase in the fraction of evoked spikes as the network reverberates. to characterize how synchronous the spikes within an activity peak are, we normalize each rate peak i with its spike count n i and use the normalized height h i /n i to quantify the synchrony. in figure 6b, the synchrony of the activity peaks is plotted against the time of the peaks relative to the start of the bursts. while the synchrony data is more disperse, we can see an upward trend following the time course of the bursts. this demonstrates a correlation between the activity spreading and synchrony of the spikes. the result may not be a surprise considering the activity spreading through synaptic action following presynaptic spikes is how neurons can communicate and should help to orchestrate the synchronous activity."
"note that the above studies focused on developing efficient numerical techniques to solve the optimal control problem of monodomain model. however, the effects of the size of the control domain on the optimal control problem of monodomain model still has not been studied. the purpose of this paper is therefore to study these effects using different sizes of the control domain in our numerical experiments."
"the same burst and peak detections for the experimental measurements are applied to the simulation results with slightly different empirical parameters. comparing to the experiments, the full dynamics of the simulations is readily available as numerical data and can be further analyzed to clarify the physical mechanisms of the bursting behavior. beside recording the time and neuron of each spike for the calculation of a time-histogram and keeping track of activity propagation, we are interested in the information of neuronal noise and the depletion of synaptic resources. the former is represented by the average concentration of residual calcium that governs the asynchronous release while the later is represented by the average fraction of inactive and super inactive neural transmitters which deplete the available neural transmitters in a bursting cycle. both of the values are retained at the start time of each detected peak in the spike-rate histogram and used to correlate with the properties of each peak."
"the synchronized network activities observed in our cultures seem to be similar to the switching between up and down states as observed in other neuronal network preparations [cit] . however, since our measurements are carried out on mea, records of the membrane potentials are not available to verify these states. it is known that activities similar to what we reported here can also be induced in acute slice [cit] when inhibitory interactions are blocked. presumably, there are too many recurrent connections in our cultures which might correspond to the pathological condition during epilepsy [cit] ."
"where   t x p, and   t x q, are lagrange multipliers. the first order optimality system is stated by requiring stationarity of"
"the finding from our analysis of the simulated system suggests an interesting phenomena, which we call depletion-enhanced synchronization, at play in the cultured network with the reverberatory bursts. in such a burst, the initiation activity is a fast sweeping wave of propagating spikes across the network that is well synchronized. this activity produces a significant amount of residual calcium, promoting noisy asynchronous releases, and prompting the spontaneous firing of the neurons that results in the subsequent reverberation of the burst. initially, the spontaneous spikes are more or less independent and the heterogeneity in the neurons and their connectivity makes the spike-rate peaks broad and less synchronous. however, as the neural and synaptic resources are increasingly depleted by the continuing spiking activity of the burst, it becomes harder for the neurons to fire independently and they thus increasingly rely on the synchronous releases triggered by the firing of their presynaptic neurons to help them cross the firing threshold. such mechanism accounts for the observed increase of evoked spikes and the synchrony in fig. 6 and may be a general mode of operation for other complex systems."
"step 4. update the reduced gradient step 5. for 1  k, check the stopping criteria in (14) . if one of them is met, then stop."
"two types of optimization methods have been applied by researchers for solving the optimal control problem of monodomain model in the literature, which are nonlinear conjugate gradient method and newton method. nonlinear conjugate gradient method has low memory requirement but usually requires many iterations to converge to a solution. in contrast, newton method is likely to converge with less iteration but requires higher memory storage."
"after plating, spontaneous activities are observed in about a week in vitro. such activities become synchronized into network bursts around 10 div and show reverberations after 15 div. the number of peaks per burst reaches a maximum around 30 div as shown in fig. 5 and falls back to one without reverberation after 40 div. it has been observed that the reverberatory bursts during the intermediate div can be divided into two phases [cit] : a propagation phase where the channels are activated sequentially and diffusively and a reverberation phase where the firings of the neurons are seemingly random and more decoupled. such division was confirmed with cat observation. as evident from streches of the cats shown in the insets of fig. 3b for a reverberatory burst, the propagation is indeed more prominent for the initiating peak of spike rate (blue trajectory) and reduces to a lingering (green) trajectory soon after. however, as the network reverberates, the cat gradually regains its propagating sweeps until the end of the burst (magenta trajectory)."
"as a result, the first order optimality system consists of the state system in (8), the adjoint system in (9) and the optimality condition in (7) that served as reduced gradient during computation."
"in the current study, we use a similar high-density mea system to investigate reverberatory bursts observed in the development of dissociated cortical cultures. instead of considering reduced dynamics such as principal components or cat, we use a propagation model to predict the location of each occurring spike. the effectiveness of such prediction allows the classification of the spikes into evoked and spontaneous ones, and can be used in reverse for an inference on the spreading speed of the recorded spiking activity. we find a recovering dominance of the evoked spikes over the reverberatory phase of a burst following their reduction after the initial propagating wave."
"to further clarify the synaptic dynamics contributing to the increasing dominance of the evoked spikes over spontaneous ones during a reverberatory burst, we turn to our simulations that produce qualitatively similar, reverberatory bursts with the increasing height of activity peaks in the spike-rate, time histogram over the bursts as shown in fig. 3c . with a simulated system, the full set of dynamic variables are available for analysis. we identify two factors of relevance in determining the peak height or the synchrony of the reverberation from our simulations: firstly, the residual calcium concentration controls the rate of asynchronous release at synapses in the model and represents the strength of an internal noise of the neurons. secondly, the inactive and super-inactive states featured in the model take up the neural transmitters as they are activated and represent the depletion of synaptic resources. we correlate the system average of these two factors with the height of activity peaks in a 3d scatter plot for all peaks of the simulated recording as shown in fig. 7 . from the projection fig. 7b, we see that depletion, which increases during a burst, correlates positively with an increase of the peak height and thus the synchrony of the spikes. on the other hand, the noise factor represented by residual calcium, as shown in fig. 7c, is initially pumped up by the spiking activity of a burst, reaching a maximum about half way through the burst, and decreases afterward due to the lengthening intervals between the reverberation peaks until the end of the burst."
we implemented the computational model in the c++ programming language using the common simulation tools framework. the simulation codes along with the framework are included in the supplementary materials of the paper. the spike data from the mea recordings as well as the computer simulations were processed with the python3 programming language and most of the data plots were produced using the matplotlib library module. a jupyter notebook containing the python3 codes for data processing and plotting is also included in the supplementary materials.
"the propagation of the spiking activity in a burst can be visually observed from the animated replay of sustaining spikes (online resource). to quantify the wavelike propagation of the initial sweep of activity and the subsequent distributed activation of neurons, we introduce a simple linear-spread diffusive model that can be used to predict the electrode for the next spike using spikes that have already been recorded. the probability for the next spike occurring at time t to be on the electrode at r is given by"
"finally, we note that while synchrony is often associated with coherence, it actually reduces the diversity in the possible dynamics of a system. in the reverberatory bursts that we focused on, the synchrony results from the depletion of synaptic resources and precedes the termination of the burst. this parallels the recent findings in epilepsy that increasing synchrony can be observed towards the end of seizures [cit] . our results may suggest a possible mechanism for such phenomena for systems of similar episodic dynamics."
"in the current study, we use a high-density mea system to investigate the physical mechanisms underlying the morphological richness of reverberatory neural bursts. our simple linear-spread diffusive model allows a classification of the spikes as well as an inference of the propagation speed of synaptic activities. the change of the predictability of spikes allows us to detect the change in the propagation behavior during a burst as shown in fig. 6 . however, the traveling-wave-like sweep of activity, especially for the initiation of a burst, is not diffusive. a more sophisticated model will be required if one would like to have a more faithful capture of such dynamics. nonetheless, the method of inference for the model parameters using individual spikes as demonstrated remains applicable. the method is enabled by our use of high-density mea and does not resort to data reduction before inferring the propagation dynamics. that is, each spike has a direct contribution to the resolution of the spreading speed and the method can potentially be used to resolve more complex dynamics of the system."
"this section describes the numerical approach for solving the optimal control problem defined in (1). we adopt the optimize-then-discretize approach, where the infinite dimensional optimality system is derived first and the resulting optimality system is then discretized."
"external data sources and collected cultural knowledge were integrated into a catalogue of typical landscape features of rhineland-palatinate. referring to systematics of comparable catalogues (schmidt et. al. 2008, [cit] ) it was developed in close coordination with the advisory body using proven engineering technologies. similar to the catalogue of thuringia, a hierarchical function-oriented structure was developed which enabled the classification of functional cultural landscapes. twelve such categories were then defined ( table 2) . each category was divided into functional complexes which cluster the unique cultural features. in the lowest level one feature could be part of a feature group or a functional ensemble. this way, the features could be stored as point, line or surface objects in a geographically and thematically precise manner. for example, a stadium could be defined as part of sports facilities in category 'sport, tourism and recreation' (table 3) . table 3 . hierarchy for feature 'stadium' in the catalogue."
"dgas play a significant role in a variety of cyber-attacks, and their seemingly random nature makes machine-learning techniques an essential tool in detecting these attacks. through a thorough investigation of various types of deep-learning models and evaluation criteria, this study identified several supervised learning models that are highly effective at detecting malicious domain names generated by dgas. in particular, capsnet and cnn+lstm performed very well, with cnn+lstm performing better in the randomly assigned data experiment, and capsnet performing better in the novel dga experiment. the capsnet model is also an order of magnitude faster in making predictions. however, the capsnet model took longer to train. all models' performances are only slightly degraded when they encounter novel dgas. these results show that capsnet can be used by an it security team to perform real-time dga detection, as it combines the speed of the cnn models with the accuracy of the cnn+lstm model, even if the capsnet is slightly slower to train. more generally, the capsnet architecture can perform well in 1d applications, not just 2d image classification applications, lending itself to a variety of nlp problems. finally, because cnns have a variety of uses in the cybersecurity space [cit], and capsnets can be applied in any instance cnns are used, this opens a whole new space of potential applications for capsnets in the realm of cybersecurity."
"along with the visual analysis in step 3.7, the origin of the image (i.e., photographic or nonphotographic) was identified. images that were the result of a photograph would be labeled as \"photographic\", while images whose presentation was either text, image, map, graphic, or infographic fell into the category of \"non-photographic\"."
"a cnn is a type of neural network used to process inputs that are stored in arrays with fixed dimensions [cit] . it is a popular method used in deep-learning algorithms and is most frequently applied to images. however, cnns can also be applied to 1d arrays such as signals, text, and sequences and to three-dimensional (3d) arrays such as videos and volumetric images. regardless of dimensionality, cnns are used where there is some spatial or temporal ordering and proximity is significant. a 1d cnn can be combined with an embedding layer to produce better results."
"the internet is a common vector for sharing and locating relevant information [cit], including that related to the impact of a natural disaster. much information shared on the web is visual, as images can generate more engagement [cit] . images can serve as vehicles for sharing information between people, but additionally, images document aspects of events. digital memory banks, for example, are frequently used to preserve past experiences of natural disasters [cit] . however, it seems that there is a lack of digital memory banks related to power outages."
"in another analysis for step 3.8, images were categorized based on the nature of their creation. figure 3 portrays how the sample of images was categorized, per event. images portrayed in each event are mostly photographic in argentina, canada, mexico, and united states. on the other hand, images of australia's heatwave were, for the most part, from non-photographic sources, e.g., infographics, followed by puerto rico. the other findings from the results of the examination of the categories are presented next:"
"the most significant limitation in this study is that the benign domains in the training, validation, and test datasets are only from the alexa top 1 million dataset, which does not necessarily include benign domains from ad networks. these domains might look more similar to some dga domains. however, producing this dataset was outside scope of this work and should such a dataset be made publicly available, it would be useful to evaluate these models against it."
"it may be said that understanding the underlying categories of images found on the web may provide useful information to promote consciousness of the preservation of memories of the effects of natural disasters and the loss of energy, and to target educational efforts, since it is proposed that images associated with power outages could provide new insights in this area. therefore, in this article, we propose to explore how the loss of electricity is represented in digital images found on the web in six different locales."
"labels are an important factor in terms of how image analysis works. google defines a label as a 'broad [set] of objects in your images, from flowers, animals, or transportation to thousands of other object categories commonly found within images' [cit] . in other words, google's api inspects the content of the image and categorizes it. to better exemplify how labels are obtained from an image, in figure 1, an analysis made using google cloud vision api is presented. analyzing the characteristics of digital visual narratives related to the loss of energy on different natural disasters around the world can greatly contribute to increasing awareness about the lack of a more educational rhetoric around this issue, just as has been done in other studies to understand what is found when searching for \"saving energy\" on the web [cit] . we believe that people's analyzing the characteristics of digital visual narratives related to the loss of energy on different natural disasters around the world can greatly contribute to increasing awareness about the lack of a more educational rhetoric around this issue, just as has been done in other studies to understand what is found when searching for \"saving energy\" on the web [cit] . we believe that people's behaviors can be adjusted as the consciousness about an issue increases. that is why the main question of this research is: how is a visual digital narrative constructed from the google's best-ranked images relative to the loss of energy following a natural disaster?"
the test datasets in both the randomly assigned and the novel dga experiments contain both types of dgas. the random assignment dataset contains domain names from all of the real word-based dgas without considering their date of generation. the novel dga experiment contains only one of these dgas: unknowndropper.
"the most commonly used type of rnn is the lstm unit [cit] . although there are other types of advanced rnns, the lstm network is the only one discussed in this paper because of its prominence."
"the results of these usability tests showed that the used wiki application is an intuitive and acceptable tool for cultural asset information documentation. however, it was also found that young people especially wished to use mobile devices for direct information collection, for example using their smartphones. the idea of collecting and administering data only with office desktop computers might be out of date for the younger generation. the research of further data sources for cultural asset information enrichment also showed that there is a variety of open information available on the world wide web which could be used to enhance the articles and profiles of the cultural assets within kulis."
"currently about 63,000 cultural assets (automatically or manually added) are stored in kulis. this large amount of such information, however, becomes a problem to categorise. the biggest challenge in the existing system is the huge amount of automatically retrieved data from government sources, because this data comes without the attributions important for characterisation. therefore the need for a large community to help collect the required information is obvious. the developed platform offers the possibilities for a civic participation where interested users can digitise new features and enhance offered government data with additional information, as outlined in the previous chapter. the usability of the system was tested in the municipal association of eich and through student exercises at the university of applied sciences in mainz. the results led to improvements for both the digitisation process and the interface design of the cultural landscape information system. however, at the moment there is no active community making further tests of the system."
"labels: electricity (96%), transmission tower (95%), electrical supply (94%), overhead power line (93%) labels: snow (99%), winter (97%), nature (96%), tree (92%), atmospheric phenomenon (92%) labels: people (94%), crowd (88%), event (78%), public event (76%), geological phenomenon (62%) labels: transport (90%), vehicle (86%), tree (83%), truck (72%), job (62%), plant (59%), car (56%)"
"all five evaluation metrics are derived from the four metrics found in the confusion matrix, which is based on the calculated prediction versus the ground truth, as shown in table 2 :"
"a total of 4691 images of the six events were downloaded and considered for this study (collection of images was done using google chrome, without being logged into a google account and after cookies and other similar data were deleted. [cit], using the advanced search function https://www.google.com/advanced_search. each of the queries presented for each event was entered with the specific limitations of language, date range, and country. note that the date range was selected after the search was executed, using the submenu \"tools\" in the results page. the images obtained were downloaded using a chrome plugin (getthemall) (url: https://chrome.google.com/webstore/detail/getthemall/fhkjfciooifcflkailbnchdaihccdebf) and saved in a specific folder for the event. the plugin downloads images according to their presented order on the results page. this process was repeated for each of the events."
"our first results are focused to step 3.5: identification of the most representative labels found in the event's images in this study. [cit] labels that described 4691 images were identified. due to a high quantity of labels, the 95th percentile was identified and selected to have the most representative labels. after the analysis, only 90 labels were considered as the most representative, which were then grouped using the constant comparison method to identify categories. the obtained results are as follows:"
"the open-source intelligence (osint) dga feed from bambenek consulting, which provided the malicious domain names [cit] . this data feed was based on 50 dga algorithms that together contained 852,116 malicious domain names. the dataset was downloaded on may 23, 2018 and dgas were generated on that day. also, on april 18, 2019, an additional dataset of 855,197 dga generated domains was downloaded from osint for testing differences in model performance based on time and is regarded as a separate test dataset."
"the main objective of kulis was to provide a citizenorientated and internet-accessible open platform. it would also be built following administrative and scientific regulations. further requirements and work packages which strongly influenced the system design are defined in the requirements led to a web platform which united semantic and spatial information about cultural assets into one system, and therefore needed a combination of different web technologies. because independence and adaptability to further development were fundamental necessities, open source technologies were used to meet these requirements."
"domain generation algorithms (dgas) are a type of malware tool used by attackers to generate a large number of domain names on the fly. by generating a massive quantity of domain names as needed, attackers can hide their command and control (c2) server and evade detection by standard cyber security methods. this scheme, called domain fluxing, is similar to hiding the proverbial needle (the attacker's c2 server) in a haystack [a long list of internet protocol (ip) addresses] [cit] . prior to dgas, malware used a static list of domain names, and cyber defenders neutralized the malware by blacklisting specific domain names. however, with the introduction of dgas, the domains are constantly changing, and it becomes impossible for the cyber defender to block all of the attacker domains via a blacklist. furthermore, reverse engineering a dga is a time-consuming task even if the defender can achieve it. a more effective and faster approach involves the use of machine-learning techniques to identify and flag suspected malicious domains."
"along with the visual analysis in step 3.7, the origin of the image (i.e., photographic or nonphotographic) was identified. images that were the result of a photograph would be labeled as \"photographic\", while images whose presentation was either text, image, map, graphic, or infographic fell into the category of \"non-photographic\"."
"along with the visual analysis in step 3.7, the origin of the image (i.e., photographic or nonphotographic) was identified. images that were the result of a photograph would be labeled as \"photographic\", while images whose presentation was either text, image, map, graphic, or infographic fell into the category of \"non-photographic\"."
"this study considered three different experiments. the first, called random assignment, used the full dataset to create the training, validation, and test data using a 60, 20, and 20% split, respectively. for this experiment, 46.03% of the domain names in the test dataset were malicious."
"using the constant comparison method, words from similar branches were grouped together first, and after observing their nature, purpose, or descriptive behavior, categories were proposed and defined by the research team. the purpose of these categories was to grasp the main plot, idea, or meaning behind a label that was constantly found in a sample using one word. the constant comparison method is a data-analytic process in which each interpretation and finding is compared with existing findings as it emerges from analysis [cit] . this method is used for comparing incidents to other incidents to establish underlying uniformity and the varying conditions of generated concepts (categories) and hypotheses. then, the emerging concepts are compared to more incidents to generate new theoretical properties of the concepts and more hypotheses. at the end, the purpose is the elaboration, saturation, and densification of the concepts [cit] ."
"given the nature of the general question, explorative and qualitative research was proposed. to answer it, the following steps were taken, and the resulting data is hereby presented in the same order as the order of operations: (1) definition of selection criteria for events, (2) design and test the search query, (3) search and download images from the six events, (4) perform an analysis of the downloaded images using google cloud vision api (through memespector), (5) identify the most representative labels (considering a 95th percentile), (6) using the constant comparison method to identify the main categories found in the labels, (7) manually categorize a sample (the first 100 images of each event, selected by an order of more to less relevant according to the search engine) and (8) categorize of the images based on the nature of their creation. figure 2 presents a flowchart, showing each of the steps followed during the method process."
"1. business: groups of labels related to any business standpoint (i.e., product, brand, advertising). 2. hazard related: labels referring to natural phenomena with potential to cause disaster or consequences related to natural hazards (i.e., event, demolition, rubble, earthquake, winter). 3. energy: labels related to energy itself (i.e., energy, electricity, public utility). 4. graphic: labels with the purpose of conveying information in a graphical way (i.e., graphic, infographic). 5. infrastructure: labels that refer to places, buildings, roads, architectural reference or materials to construct (i.e., building, city, facade). 6. nature: labels referring to elements found in nature (i.e., tree, plant, sky). non-hazard elements were considered. 7. people and activities: labels referring to persons and their activities (i.e., crowd, pedestrian, recreation, vacation"
"along with the visual analysis in step 3.7, the origin of the image (i.e., photographic or non-photographic) was identified. images that were the result of a photograph would be labeled as \"photographic\", while images whose presentation was either text, image, map, graphic, or infographic fell into the category of \"non-photographic\"."
"to collect the data, a search query was used in google's image search. the construction of the search query was completed following a series of tests of the search engine, with the intention of having a similar search pattern that could be used for every search. after many attempts to establish a pattern, the following one was settled on. first, the name of the natural hazard, and sometimes a specific location, was given (i.e., ontario storm), because google can limit results of a search to a geographical region. then, 'power outage' was included in quotation marks to search for the specific string. despite the fact that 'blackout' is another way to refer to an event of loss of electricity, this term was not included in the search query, since it brought back many images related to activism movements. as power outages may not be a usual search, the string 'or electricity' was added, considering that this is a more commonly used word, and may be more prevalent in descriptions of the loss of electricity. the search in spanish used translated words for the same purpose. in this instance, power outage was searched for as 'apagón', and electricity was 'electricidad'. bilingual researchers made the selection of the words. however, there are some exceptions to the general rule. for example: (1) the query for mexico includes a-\"nuevo león\" to remove the possibility of images associated with a power outage that occurred (not due to a natural disaster) in the state in mexico during the considered time frame, (2) hazards were considered as specific to areas, instead of locales. for example, hurricane irma may include images found in the usa in different states, in contrast to the situation of ontario's ice storm and victoria's heatwave."
"after the main categories of labels were identified, a researcher performed a manual inspection. a sample of the first 100 images for every event was considered, under the assumption that these were the most relevant to the query. images were examined to identify which of the main categories of labels identified were represented in them, to compare how labels and observer perception would be similar or differ. more than one category could be present in one image (i.e., presence of nature, hazard related, and people would be three categories on one image). a frequency log was used to track how many times each category was represented through all the images. table 2 presents examples of interest images for each category, as well as the labels associated with the image."
"author contributions: g.c. contributed by executing data curation, formal analysis, investigation and writing. g.v. contributed through formal analysis, methodology and supervision. t.i. contributed through conceptualization and resources."
"considering the need for citizen participation, a system had to be designed which permitted user enhancement of information about cultural assets by volunteers both in a thematic and geographic way. the need for an intuitive and widely accepted frontend for information display and management through different editors led to a wiki approach. as a content management system, the open source software mediawiki allowed information creation and updating along with user management for editorial purposes [cit] . furthermore, it was configurable for implementation of additional functionalities required in the project. to map the catalogue structure to the wiki and combine the content with geographical information derived from existing administrative data, a spatial data infrastructure structure was needed to provide an interface for the web mapping application, the spatial database and the server-side services. linking information between mediawiki and a spatial database of the mapping application was realised using a unique feature identifier id for each cultural object. figure 2 illustrates the infrastructure of the implemented cultural landscape information system. a more detailed description of the used components is presented in the sections below. as a result, all features in kulis have a profile as shown in figure 3 . besides general descriptions such as name and an illustration, it shows the classification in the catalogue structure in a first block (\"allgemeines\"). a second block follows, with a textual and graphical description of the location as place coordinates along with its pin on a map (\"lage des objektes\"). the properties of the feature, including the geometry type (point, line or surface) and a chronological classification are presented in a third block of the profile (\"objekteigenschaften\"). finally, the last section presents value attributes related to cultural importance (\"objektbewertung\"). these attributes are grouped into six categories and allow a four-levelled description for states of preservation (very good, good, poor and bad), rarity (unique, rare, common and very common), endangerment (high, middle, low and no endangerment), regional characteristic (very typical, typical, not very typical or atypical for region), importance (international, national, state wide, regional or local) and scenic attraction (highly perceptible, perceptible, barely perceptible and imperceptible). the semantic extension makes it possible to organise and search the features in the hierarchical structure of the feature catalogue. intelligent querying of defined attributes of the profiles allows for the analysis and relation of content regarding cultural relevance. the extension offers further functionality, and brings the application in line with semantic web approaches [cit] . gathering attributes valuing the cultural assets and their importance especially helps to develop categorizations for cultural landscapes. with this implementation, a tool is established for later analyses and investigation of the cultural assets and their semantic relationships. in combination with the spatial information, it enhances the definition of cultural landscapes."
"outlining the implementations in this work shows that kulis offers promising possibilities to fulfill the complex requirements of a state administrative controlled ppgis. the use of open source tools, used and maintained by a wide community, made it possible to adaptively change requests during the system development. whereas common wiki implementations only work with point geometries, the power of spatial analysis and visualization is added to the system through a spatial data infrastructure. the combination of local and semantic attributes makes interlinking with other sources using the semantic web possible, and will be more important as the number of interlinkable sources grow in the future. the presented solution provides a modern platform based on open source technologies, enabling citizen participation. this approach can promote transparence and acceptance of administrative actions within a society."
"the models that were trained the fastest were the cnn models, followed by the lstm models, then the capsnet model, which took nearly twice as long to train as the lstm models. however, a single training epoch took no more than 15 minutes. tables 3 and 4 show the simulation results for the two different experiments. table 5 shows the breakdown of the accuracy in table 3 for each dga and benign data. table 6 shows the performance of the same models on the time split data collected nearly one year later. table 7 shows the breakdown of the accuracy in table 4 for each dga and benign data. the best two methods for each metric are shown in bold. these tables show that although the cnn+lstm and capsnet models provided excellent performance, the other methods' performance was nearly as good. furthermore, the performance for the random assignment experiment was noticeably improved over that for the novel dga experiment; however, the results with the novel dga experiment are still highly accurate. in the random assignment data experiment, both the cnn+lstm and the capsnet models were in the top two best performing models for four of the metrics. the capsnet model had the second best the f1-score and the second worst performance in the partial auc metric. additionally, the capsnet model had the second highest accuracy averaged across all the different types of dgas and benign data. performance was even better in the novel dga experiment, in which the capsnet model achieved the highest accuracy, f1-score, and recall, and the second highest partial auc. it also had the highest accuracy averaged across all the different types of dgas and benign data."
"the remainder of this paper is organized as follows: section 2 provides background information on deep learning. sections 3 and 4, respectively, describe the models that were tested in greater detail and the datasets that were used. section 5 discusses the various metrics that were used to evaluate the models. section 6 presents the models' training and testing results. lastly, section 7 presents the conclusions that can be drawn."
"once categories were identified, a human analysis of the images was done as part of step 3.7. to do this, a sample of the first one hundred images were selected per event. tendencies of each category is represented by percentages in the table 3, and the representation for each of the six cases is presented in table 4 . table 3 . categories created from labels in the 95 percentiles (with frequency and percentage). observing the analysis of the sample, it was found that around the 50% of the images focused on themes like the impact upon infrastructure, nature, and hazard related images (see table 3 ). australia appears as an exception, having graphic, energy, and business as the categories mostly identified in the images; additionally, transportation stands out, as no images were identified in this category. mexico exhibits a high percentage of images related to the people and activities category compared to the other events."
"our first results are focused to step 3.5: identification of the most representative labels found in the event's images in this study. [cit] labels that described 4691 images were identified. due to a high quantity of labels, the 95th percentile was identified and selected to have the most representative labels. after the analysis, only 90 labels were considered as the most representative, which were then grouped using the constant comparison method to identify categories. the obtained results are as follows: the main categories to describe the labels from the memespector analysis."
"prior to the introduction of lstms, rnns were difficult to train because the gradients can easily vanish or explode [cit] . with the introduction of the lstm unit, the rnn is easier to train and it is capable of maintaining a long memory. the lstm approach maintains a \"state vector\" that contains the \"memory\" of past events, and this becomes an additional input for the next time step. figure 3 shows three rnn models, each with an embedded layer, which are considered herein. in this paper, the lstm type of rnn is used. the first is a unidirectional rnn, the second is a bidirectional rnn, and the third is a unidirectional rnn with a cnn layer."
"because this study requires certain conditions to be met, a list of disasters that have occurred in different places in the world was examined to determine the best approach to ensure that the information would be reliable for analysis. the following criteria were used."
"because this study requires certain conditions to be met, a list of disasters that have occurred in different places in the world was examined to determine the best approach to ensure that the information would be reliable for analysis. the following criteria were used."
"the procedure for collecting images continued in the following way. an advanced google search was conducted to obtain only images meeting certain predetermined criteria. the steps taken for the search were as follows. in google's image search, advanced search was selected and the following characteristics were entered: (1) image size: any size; (2) aspect ratio: any aspect ratio; (3) colors in the image: any color; (4) type of image: any type; (5) region: geographical area of the selected event; the time frame selected for the images was discussed between authors, as there are differences in the timebound impact of different natural hazards (consider the duration of a hurricane or heat wave compared to that of an earthquake, which may have aftershocks, while hurricanes may last days or more). consideration of these factors led to the decision that a period of two months from the beginning of the disaster would be a proper time horizon (neither too short or long). table 1 indicates the final characteristics of the search query and specific criteria for the searches."
"using the constant comparison method, words from similar branches were grouped together first, and after observing their nature, purpose, or descriptive behavior, categories were proposed and defined by the research team. the purpose of these categories was to grasp the main plot, idea, or meaning behind a label that was constantly found in a sample using one word. the constant comparison method is a data-analytic process in which each interpretation and finding is compared with existing findings as it emerges from analysis [cit] . this method is used for comparing incidents to other incidents to establish underlying uniformity and the varying conditions of generated concepts (categories) and hypotheses. then, the emerging concepts are compared to more incidents to generate new theoretical properties of the concepts and more hypotheses. at the end, the purpose is the elaboration, saturation, and densification of the concepts [cit] ."
all of the models were optimized with the adam algorithm [cit] and trained with a learning rate of 0.001. the models were trained until the validation accuracy showed no signs of increased improvement in batches of 10 epochs.
the focus of this study is to introduce the following two new real-time dga detection models and to then replicate and compare the performance of the most successful of the supervised deep learning techniques presented in the literature to these models.
"1. the disaster was mentioned by different sources, with governmental authorities being preferable. 2. the region was affected by a natural hazard. 3. a power outage occurred because of the natural hazard. 4. the disaster event occurred no more than three years ago (2015 [cit] ). 5. search information must be available in spanish or english."
"most of australia's images were from non-photographic sources (78%); puerto rico had the second highest rate (54% non-photographic). in all other cases, the most prevalent images were photographs, and argentina had the highest rate of these (76% of its images were photographs). there is room for further study of how types of images differ among events, and whether search engines distinguish between them."
"four papers reported different metrics [cit] . the most commonly used metric was the area under the [receiver operating characteristic (roc)] curve (auc). the auc is the area under a curve of the false positive rate vs true positive rate for various threshold values. however, because the aucs differed by such small values, differences could be a result of statistical variation. therefore, this research considered the partial auc, up to a false positive rate of 0.1%. a maximum false positive rate of 0.1% was selected because any higher would make the model unusable in a real environment. additionally, this study considered five other different metrics: accuracy, recall, precision, false positive rate, and f1-score."
"after the main categories of labels were identified, a researcher performed a manual inspection. a sample of the first 100 images for every event was considered, under the assumption that these were the most relevant to the query. images were examined to identify which of the main categories of labels identified were represented in them, to compare how labels and observer perception would be similar or differ. more than one category could be present in one image (i.e., presence of nature, hazard related, and people would be three categories on one image). a frequency log was used to track how many times each category was represented through all the images. table 2 presents examples of interest images for each category, as well as the labels associated with the image. allowing the most frequent elements to be examined."
"using the constant comparison method, words from similar branches were grouped together first, and after observing their nature, purpose, or descriptive behavior, categories were proposed and defined by the research team. the purpose of these categories was to grasp the main plot, idea, or meaning behind a label that was constantly found in a sample using one word. the constant comparison method is a data-analytic process in which each interpretation and finding is compared with existing findings as it emerges from analysis [cit] . this method is used for comparing incidents to other incidents to establish underlying uniformity and the varying conditions of generated concepts (categories) and hypotheses. then, the emerging concepts are compared to more incidents to generate new theoretical properties of the concepts and more hypotheses. at the end, the purpose is the elaboration, saturation, and densification of the concepts [cit] ."
"the most commonly used type of rnn is the lstm unit [cit] . although there are other types of advanced rnns, the lstm network is the only one discussed in this paper because of its prominence."
"the second experiment takes the april 18, 2019 dataset downloaded from osint and tests the models trained in experiment one. this is done to test the ability of the models to detect real word-based dgas after a change in the words used to generate these domain names."
"capsnet, as shown in figure 2, removes the pooling layers entirely and replaces them with hierarchical capsule layers. each capsule applies a subset of the filters applied in the conventional convolutional layer. for example, if 256 filters are applied in the conventional convolutional layer, and there are 32 capsule layers, each capsule layer would be composed of eight filters, each one called a capsule. this is the primarycaps layer. a non-linear weighting function, called a squashing function, is applied to normalize the data and multiplied by a weight. then, in a process similar to k-means clustering called routing by agreement, the capsules with a similar orientation and magnitude are more heavily weighted in an average across all capsules. this is performed by taking the mean of the capsules, then calculating a weight for each capsule as a function of distance from the mean. a new mean is calculated using those weights, and the weights are recalculated. this is repeated for a set number of times. this is the classcaps layer. the squashing function is applied again, and a prediction is made based on the length of the classcaps. capsnet attempts to address the picasso problem, in which a face with all the correct parts but without the correct spatial correlation are recognized as a face. in addition, [cit] claim that capsule networks are pose invariant and can generalize better to new unlearned viewpoints. lastly, capsnet could have a stronger resilience to certain types of adversarial attacks. capsnet, as shown in figure 2, removes the pooling layers entirely and replaces them with hierarchical capsule layers. each capsule applies a subset of the filters applied in the conventional convolutional layer. for example, if 256 filters are applied in the conventional convolutional layer, and there are 32 capsule layers, each capsule layer would be composed of eight filters, each one called a capsule. this is the primarycaps layer. a non-linear weighting function, called a squashing function, is applied to normalize the data and multiplied by a weight. then, in a process similar to k-means clustering called routing by agreement, the capsules with a similar orientation and magnitude are more heavily weighted in an average across all capsules. this is performed by taking the mean of the capsules, then calculating a weight for each capsule as a function of distance from the mean. a new mean is calculated using those weights, and the weights are recalculated. this is repeated for a set number of times. this is the classcaps layer. the squashing function is applied again, and a prediction is made based on the length of the classcaps."
"hence, the resulting dataset contained 1,852,116 domain names; one million were benign and 852,116 were malicious. this dataset contains two overarching types of dgas: ones that produce random looking domains with high character entropy (e.g., oxufyrqcqopty.net) ones with low character entropy composed of real words (e.g., addressblamescore.com). there were seven dgas that used real words or websites in their generation of domain names: cryptowall, gozi, matsnu, pizd, suppobox, unknowndropper, and volatile cedar/explosive."
"power outages continue to occur around the world. despite that, awareness of the implications of the loss of power over long periods of time due natural disasters remains limited for most people. in our study, we found that the visual digital narrative of the loss of energy due to natural disasters may differ depending on the type of disasters and the locale. [cit] earthquake (with a high impact in a short time) seem to present more images focused on the damage to infrastructure, hazardous contexts, and people, while other hazards of longer duration-such as australia's heatwave-presented more information through graphics which was especially associated with energy issues such as the electricity infrastructure, peaks of production, higher prices, etc. unfortunately, there is a lack images showing the burden of living without electricity during a cold storm in canada, the consequences of being without energy for days or weeks in puerto rico, or the need for a working energy grid after an earthquake in mexico, among others. the differences between and memories of each natural hazard remain unclear, presenting an opportunity to gather these experiences for the future."
"using the constant comparison method, words from similar branches were grouped together first, and after observing their nature, purpose, or descriptive behavior, categories were proposed and defined by the research team. the purpose of these categories was to grasp the main plot, idea, or meaning behind a label that was constantly found in a sample using one word. the constant comparison method is a data-analytic process in which each interpretation and finding is compared with existing findings as it emerges from analysis [cit] . this method is used for comparing incidents to other incidents to establish underlying uniformity and the varying conditions of generated concepts (categories) and hypotheses. then, the emerging concepts are compared to more incidents to generate new theoretical properties of the concepts and more hypotheses. at the end, the purpose is the elaboration, saturation, and densification of the concepts [cit] ."
"in addition to embedding and classification layers, there are two other types of layers that make up a cnn, convolution and pooling, as shown in figure 1 . the convolution layers are the core of the cnn. this layer essentially applies a filter to a subset of the input at a specific instance of time. the application of the filter creates a weighted linear sum of the input's subset to which the filter is applied. then, a non-linear function is applied by implementing a rectified linear unit that is applied across the entirety of the height and width of the input. the end result of the application of a single convolutional layer (i.e., a filtering function) and the application of the non-linearity yields a feature map. the output of the cnn is a stack of feature maps, each capturing multiple convolutions of the input and each using a different filter. the pooling layers apply non-linear down-sampling functions. for example, one would select the maximum value over non-overlapping subsets of the feature map. these pooling layers are generally applied periodically between convolution layers to reduce the size of the feature map as the computation proceeds through the network. this has three main benefits:"
this section provides a brief overview of the various concepts discussed in the introduction. a more in-depth technical description of the various methods is beyond the scope of this paper and is provided in the literature.
"the presented system also has further potential for technical development. the amount of smartphone users worldwide passed the 1 [cit] while at the same time location-based services have become common in various domains of our daily lives [cit] . using a variety of functionalities, including photography through imbedded cameras or global positioning, modern mobile devices offer many possibilities to retrieve data directly in the operator's proximity. regarding these facts, a modern ppgis should also support location-based participation of users. there is an opportunity for designing a location-based service as a mobile application using the introduced cultural landscape information system. this would expand the kulis user base as well as be an opportunity to create an even more intuitive application with a slim interface. the already installed system to create and maintain cultural asset information would still persist in the backend and would be enhanced through further application design. this possibility again demonstrates the advantages of a system design using common open source tools and a distributed architecture."
all of the models were optimized with the adam algorithm [cit] and trained with a learning rate of 0.001. the models were trained until the validation accuracy showed no signs of increased improvement in batches of 10 epochs.
"all seven models were built and implemented in python version 3.6.3 using the libraries tensorflow-gpu version 1.8.0 and keras version 2.1.6 on four geforce gtx 1080 ti graphical processing units (gpus). additionally, the metrics were calculated using the functions in the package scikit-learn version 0.20.3. the following models from the literature were built using the descriptions from their respective papers and the parameters specified: lstm, cnn+lstm, bidirectional lstm, and a basic 1d cnn. if certain parameters were not provided, as was the case for the bidirectional lstm and cnn+lstm models, the parameters that produced the best results were chosen so that the comparison would be fair. table 1 provides the parameters of the seven different deep learning models."
"after the main categories of labels were identified, a researcher performed a manual inspection. a sample of the first 100 images for every event was considered, under the assumption that these were the most relevant to the query. images were examined to identify which of the main categories of labels identified were represented in them, to compare how labels and observer perception would be similar or differ. more than one category could be present in one image (i.e., presence of nature, hazard related, and people would be three categories on one image). a frequency log was used to track how many times each category was represented through all the images. table 2 presents examples of interest images for each category, as well as the labels associated with the image. allowing the most frequent elements to be examined."
"our first results are focused to step 3.5: identification of the most representative labels found in the event's images in this study. [cit] labels that described 4691 images were identified. due to a high quantity of labels, the 95th percentile was identified and selected to have the most representative labels. after the analysis, only 90 labels were considered as the most representative, which were then grouped using the constant comparison method to identify categories. the obtained results are as follows:"
"the un office for disaster risk reduction defines a disaster as 'a serious disruption of the functioning of a community or a society at any scale due to hazardous events interacting with conditions of exposure, vulnerability and capacity, leading to one or more of the following: human, material, economic and environmental losses and impacts' [cit] . a hazard is defined as 'a process, phenomenon or human activity that may cause loss of life, injury or other health impacts, property damage, social and economic disruption or environmental degradation'. technological hazards are included in this definition. technological hazards originate in 'technological or industrial conditions, dangerous procedures, infrastructure failures or specific human activities' that may arise as a direct result of the impact of a natural hazard event [cit] . among other events, power outages may arise due to a disaster."
"after the main categories of labels were identified, a researcher performed a manual inspection. a sample of the first 100 images for every event was considered, under the assumption that these were the most relevant to the query. images were examined to identify which of the main categories of labels identified were represented in them, to compare how labels and observer perception would be similar or differ. more than one category could be present in one image (i.e., presence of nature, hazard related, and people would be three categories on one image). a frequency log was used to track how many times each category was represented through all the images. table 2 presents examples of interest images for each category, as well as the labels associated with the image. allowing the most frequent elements to be examined."
"1. business: groups of labels related to any business standpoint (i.e., product, brand, advertising). 2. hazard related: labels referring to natural phenomena with potential to cause disaster or consequences related to natural hazards (i.e., event, demolition, rubble, earthquake, winter). 3. energy: labels related to energy itself (i.e., energy, electricity, public utility). 4. graphic: labels with the purpose of conveying information in a graphical way (i.e., graphic, infographic). 5. infrastructure: labels that refer to places, buildings, roads, architectural reference or materials to construct (i.e., building, city, facade). 6. nature: labels referring to elements found in nature (i.e., tree, plant, sky). non-hazard elements were considered. 7. people and activities: labels referring to persons and their activities (i.e., crowd, pedestrian, recreation, vacation). 8. transportation: groups of labels referring to means of transportation (i.e., vehicle, mode of 5. infrastructure 6. nature 7. people and activities 8. transportation"
"implemented with geoserver allows for the creation, deletion and updating of the spatial information [cit] . for performance reasons, a mapserver set up provides the data structured in twelve web map services (wms) according to the number of functional categories (see table 2 ). with mapbender, a web gis interface is implemented in the wiki frontend (see figure 4, [cit] ). it enables visualisation and digitisation of the location of cultural assets via the wms and the wfs-t provided by the geospatial servers. the use of a spatial database makes it possible to include information about the boundaries of municipalities, cities and counties. during the location digitisation process, the recorded coordinates can be allocated this way and a precise textual attribution of the site (e.g. county or city) enhances the profile of the cultural asset information automatically."
"1. business: groups of labels related to any business standpoint (i.e., product, brand, advertising). 2. hazard related: labels referring to natural phenomena with potential to cause disaster or consequences related to natural hazards (i.e., event, demolition, rubble, earthquake, winter). 3. energy: labels related to energy itself (i.e., energy, electricity, public utility). 4. graphic: labels with the purpose of conveying information in a graphical way (i.e., graphic, infographic). 5. infrastructure: labels that refer to places, buildings, roads, architectural reference or materials to construct (i.e., building, city, facade). 6. nature: labels referring to elements found in nature (i.e., tree, plant, sky). non-hazard elements were considered. 7. people and activities: labels referring to persons and their activities (i.e., crowd, pedestrian, recreation, vacation"
"differences were found between the results of image coding in percentage between the two studies. as an example, the category 'people' was coded for 21.1% of the images, while in our study, only mexico's earthquake had a similar percentage (22.56%). even when the hazard (hurricane sandy) was the same, us (14.19%) and puerto rico (8.06%) were far from having a similar percentage. in another aspect, 'damage' category was coded in 12.8%, while in our study, the category infrastructure (that included damage) showed a percentage range of 12.50-27.82% between events. these differences may be explained by the sources of data, being that the study based on hurricane sandy gathered tweets from major us cities users, while google may collect these images from different websites (i.e., news websites, blogs, social network sites)."
existing data about cultural features in rhineland-palatinate were examined during a first evaluation. the considered infrastructure and its components allowing the integration of external data are described in the next paragraph.
"using the constant comparison method, words from similar branches were grouped together first, and after observing their nature, purpose, or descriptive behavior, categories were proposed and defined by the research team. the purpose of these categories was to grasp the main plot, idea, or meaning behind a label that was constantly found in a sample using one word. the constant comparison method is a data-analytic process in which each interpretation and finding is compared with existing findings as it emerges from analysis [cit] . this method is used for comparing incidents to other incidents to establish underlying uniformity and the varying conditions of generated concepts (categories) and hypotheses. then, the emerging concepts are compared to more incidents to generate new theoretical properties of the concepts and more hypotheses. at the end, the purpose is the elaboration, saturation, and densification of the concepts [cit] ."
"cultural landscapes consist of an extreme variety of features, with such variability stemming from their geographic as well as semantic characteristics. this fact presents a challenge for new developments using an aggregate system combined from distributed software modules. the presented system as a combination of current web tools combining semantic and spatial information in a public participation geoinformation system meets this challenge. developing such an information system for cultural landscapes using exclusively open source tools is not only possible, but also enables the chance to use state of the art technologies for further development."
"prior to the introduction of lstms, rnns were difficult to train because the gradients can easily vanish or explode [cit] . with the introduction of the lstm unit, the rnn is easier to train and it is capable of maintaining a long memory. the lstm approach maintains a \"state vector\" that contains the \"memory\" of past events, and this becomes an additional input for the next time step. figure 3 shows three rnn models, each with an embedded layer, which are considered herein. in this paper, the lstm type of rnn is used. the first is a unidirectional rnn, the second is a bidirectional rnn, and the third is a unidirectional rnn with a cnn layer."
"stories of disasters and power outages have been collected in digital memory banks, from where they have gathered attention. these data repositories allow information to be gathered that communities may wish to study or remember; such information can be used for educational purposes in disaster awareness. [cit], collected over 25,000 items covering themes from museum collections, photograph collections, photographs tied to specific places, children's artwork, and others [cit] . for this project, memories of power outages occupy a space in the collection 121 days of darkness-gentilly after sunset, which illustrates the 121 days after hurricane katrina during which approximately 40% of new orleans was without power [cit] . still to be considered in digital repositories, a review of remote-sensing imagery suggest that satellite images would be an asset to providing an assessment of the economic impacts of natural hazards, and that nightlight images make it possible to study general information of their impact, [cit] permitting viewers to have a broader view of the local and regional impact of a loss of electricity event."
"over the last several decades, energy requirements have increased around the world. natural hazards often interrupt the transmission and generation of electricity, causing economic and social damage. [cit], hurricane irma left around 64% of all customers without electricity in the state of florida [cit] . in puerto rico, after its landfall, hurricane maría left the entire population without electricity, except for those with private generators [cit] . in ontario, canada, freezing rain from ice storms caused thousands of people to be without electricity for hours in cold weather conditions [cit] . in mexico city, an earthquake left 3.8 million people without electricity [cit] homes and promoting priority energy management in hospitals [cit] . in argentina, around 70,000 electricity users were left without electricity due to the impact of floods, hail, and strong winds [cit] ."
"lastly, it's important to speak about the uncertainties of our research. while these images may provide information about what happens in the narrative of the loss of energy and natural disasters, images change across time, and their arrangement can change depending on the algorithm used by its owner (in this case, google). therefore, replication of this study may show different results in terms of images, while we expect that categories would differ less, and may be useful for future studies."
"1. business: groups of labels related to any business standpoint (i.e., product, brand, advertising). 2. hazard related: labels referring to natural phenomena with potential to cause disaster or consequences related to natural hazards (i.e., event, demolition, rubble, earthquake, winter). 3. energy: labels related to energy itself (i.e., energy, electricity, public utility). 4. graphic: labels with the purpose of conveying information in a graphical way (i.e., graphic, infographic). 5. infrastructure: labels that refer to places, buildings, roads, architectural reference or materials to construct (i.e., building, city, facade). 6. nature: labels referring to elements found in nature (i.e., tree, plant, sky). non-hazard elements were considered. 7. people and activities: labels referring to persons and their activities (i.e., crowd, pedestrian, recreation, vacation"
"to analyze the images, memespector was used to do an automatized analysis of a set of images, resulting in a file containing data on the content of the images in the form of labels. memespector is described as a 'simple script for using google vision api', which \"takes a comma-or tab-separated file containing a column with image urls as input, sends images to the vision api and puts the detected annotations back into the list.\" [cit] originally created by bernhard rieder for php and then developed for python, memespector allows the user to do multiple sequential image analyses. this software is used as a tool for projects related to the digital methods initiative [cit] . the automatized analysis produced a total of 1819 labels from the collected images. more information about how the software works can be found in the readme file at the repository of memespector https://github.com/bernorieder/memespector."
"around one hundred years ago, a group of flat stones were carved as a reminder, a warning for future generations not to build any homes below their location, avoiding the loss of more lives [cit] . however, in the words of masayuki oishi, \"we need a modern version of the tsunami stones\" [cit] . the opportunity remains to use these images as a tool to increase awareness of the lack of energy due to disaster experiences and to provide a historical memory, as occurs with projects such as the hurricane digital memory bank [cit] . even when of public services (such as drainage and potable water, public lights, or medical services) are interrupted due to a power outage, the memory of these experiences may not be preserved. in conclusion, it is relevant to maintain a sense of the importance of identifying and understanding the types of narratives that exist relative to losses of energy following natural disasters; as byrd and matthewman [cit] put it: 'no matter how a \"smart\" a city may be, it becomes \"dumb\" when the power goes out'."
"our first results are focused to step 3.5: identification of the most representative labels found in the event's images in this study. [cit] labels that described 4691 images were identified. due to a high quantity of labels, the 95th percentile was identified and selected to have the most representative labels. after the analysis, only 90 labels were considered as the most representative, which were then grouped using the constant comparison method to identify categories. the obtained results are as follows:"
"the semantic web, which can be considered a next step of the world wide web where data can be processed directly and indirectly by machines [cit] ) promises intelligent interoperability of information among different web applications and sources. with the semantic mediawiki extension, the ability to prepare kulis for the web 3.0 is given. the chosen structure for the cultural asset catalogue makes it already possible to request information inside of the information system using semantic queries. for example, it is possible to search for all features in a functional category with a specific landscape importance. other extensions such as linked wiki allow for the external querying of information from semantic web endpoints, and for the combination of this information with existing articles inside of kulis. this opportunity could support new data supplementation and enhancement by automatic data association with existing linked information. an interesting web endpoint for information enrichment in the present cultural landscape information system is dbpedia, a web project processing structured content from wikipedia into interlinked semantic structures [cit] ). for example, dbpedia makes it possible to retrieve an extensive amount of articles highlighting existing currently relevant cultural assets. by querying for the name of a cultural object or looking for articles in a buffer around a location (stored as a pair of coordinates), it is possible to gain new information. moreover, references to further sources can be guided by intelligent linking. the preparation of knowledge for relations of articles and attributes inside of the system can be provided as a web service endpoint, or directly interlinked through semantic platforms such as dbpedia."
"natural hazards may cause natural disasters around the world. however, the lack of energy increases the impacts of these, disrupting the technological progress made by humankind more than one hundred years ago. for cities inhabited by millions of people, the lack of critical infrastructure such as sewage, drainage and water pumps, telecommunications, public transportation, and financial services may create chaos if these are not quickly repaired. in many natural disasters, societies have overcome these difficulties at a cost. however, there is still work to be done to increase awareness and to prevent further future damage caused by climate change and the rising need to provide energy resources to all the people in the world."
"although capsule networks were recently applied to text classification [cit], to the author's knowledge, this is the first time that methods using capsule networks were applied to the language and text domain within the cybersecurity arena. furthermore, because cnns perform better with more layers, a model with a deeper 1d cnn architecture was developed."
"the third experiment, called novel dga, was constructed to test how well the models can generalize to new previously unforeseen (or novel) dgas. in this experiment, the malicious domain names for the training and validation datasets were created with only 41 of the 50 dgas, whereas the malicious domain names for the testing dataset came from the remaining nine dgas. (the 41 dgas were selected at random.) again, the benign domain names were assigned to the training, validation, and test datasets using a 60, 20, and 20% split, respectively. in this experiment, 27.35% of the domain names in the test dataset were malicious. it is expected that performance on the test set will be lower in experiment two than in experiment one because the test dataset contains domains from dgas the model was never trained on. all seven models that were built were trained and tested on these datasets."
the created feature catalogue and its hierarchical structure provided the foundation for the later implementations of a database structure to store and manage information about cultural assets and their relations inside of the system.
"hazard related. in all, except for one region, more than 20% of images were related to this category. australia was the exception and had a quite low figure, i.e., 2.94%. even omitting any image of the direct impact of the heat wave, images suggesting heat wave hazards (such as wildfires, fires, or news related to the effects of a heat wave on animals or people) were not found."
"to provide results for step 3.6, constructed categories are organized in table 2, with their frequency and the respective percentage of the 90 labels. of a total of 90 labels, around a half of them were categorized as being associated with infrastructure (28.89%), nature (15.56%), or being hazard related (13.33%)."
"all seven models were built and implemented in python version 3.6.3 using the libraries tensorflow-gpu version 1.8.0 and keras version 2.1.6 on four geforce gtx 1080 ti graphical processing units (gpus). additionally, the metrics were calculated using the functions in the package scikit-learn version 0.20.3. the following models from the literature were built using the descriptions from their respective papers and the parameters specified: lstm, cnn+lstm, bidirectional lstm, and a basic 1d cnn. if certain parameters were not provided, as was the case for the bidirectional lstm and cnn+lstm models, the parameters that produced the best results were chosen so that the comparison would be fair. table 1 provides the parameters of the seven different deep learning models."
"our first results are focused to step 3.5: identification of the most representative labels found in the event's images in this study. [cit] labels that described 4691 images were identified. due to a high quantity of labels, the 95th percentile was identified and selected to have the most representative labels. after the analysis, only 90 labels were considered as the most representative, which were then grouped using the constant comparison method to identify categories. the obtained results are as follows:"
"the relevance of preserving ones memory related to energy issues is in comprehending that 'the nature of blackouts is more than just a record of past failures; blackouts are dress rehearsals for the future in which they will appear with greater frequency and severity and, as urban areas become more compact, with greater consequences' [cit] . thus, it is important to remember the disasters of the past and use the memories as a resource for the future. following current digital trends, the information available on the internet will continue to give us further opportunities to understand the memories constructed during past events. as a result of the technological developments that have taken place in the new millennium, and due to the recent popularization of digital devices and web 2.0-style services, users are more than just consumers, and are able to become creators of content. landwehr and carley [cit] asserted that 'a photograph is a richer data source than a 140-character message.' it has been documented how people use social network sites such as instagram, facebook, twitter, and others [cit] to record and share their experiences in natural disasters. much of that content, if available on the web, is accessible to search engines and is indexed as a resource for search results."
"the greatest weakness of all the models tested is their deficiencies in detecting real word-based dgas. in some cases, some of these real word-based dgas use a limited dictionary to generate domain names and change that dictionary after some time. this manifests in three ways. the first is that when the model is trained on data from that dga, time is not taken into account and the model fails to detect the malicious domain names, as is the case for matsnu and gozi. the second is when the model can only detect the malicious domain names when it is trained on data from that dga, regardless of time, but fails to detect it otherwise, as is the case for unknowndropper. finally, there are models that initially perform well but after time passes, performance significantly declines because of a change in the dga generator, as is the case with pizd and suppobox. developing a model capable of detecting malicious domains in all three of these situations is critical, and all models tested here fail to do so. therefore, future work will focus on these real word-based dgas."
"although natural disasters may have an impact on energy infrastructure, it is not widely understood how a lack of electricity can affect the aftermath. many developed countries and some developing countries maintain a supply of energy resources, making it difficult for many customers to remember or to understand what it would mean to be without electricity. until such a resource is lost, most will not realize the role that it plays in their lives. even when we do understand how our energy use impacts our life, the ability to understand topics related to energy remains a work in progress."
"internet and new media citizen participation models are increasing worldwide. [cit], the european interoperability framework was defined [cit] to facilitate the interoperability of services and systems. the federal government of germany launched the initiative egovernment 2.0 with the aim of connecting citizens, businesses, public authorities and other organisations [cit] . e-governance already influences administrative processes tremendously [cit] ). the term public participation geoinformation system (ppgis) can be used for a system with possibilities to gather geographical information [cit] outlines the resulting information as volunteered geographic information (vgi), whereas the process of retrieving vgi can be generally described with the term crowdsourcing [cit] . a citizen participation system can give its contributors a positive feeling of influencing democratic processes [cit] . through a ppgis, the developed cultural landscape information system offers those benefits. it is possible to enrich existing administrative data with widespread public knowledge about cultural heritage."
"due to the above-mentioned variety of legal levels in cultural landscape protection and public participation, it became clear that an interdisciplinary process involving different authorities was inevitable. for this reason, the planning authority at the ministry of the interior and sports of rhineland-palatinate initiated the development of a cultural landscape information system involving administrations, the public, and members from academia providing scientific support. all actors together built the advisory body (figure 1) to launch the development process for the cultural landscape information system [cit] . in collaboration between the firu mbh, the university of trier and the i3mainz, institute for spatial information and surveying technology at the mainz university of applied science, a cultural landscape information system called kulis was developed to provide tools and access for cultural heritage documentation usable by both the state administration and the public. the results of this process, experiences and current developments are outlined in this article."
"however, due to the fact that citizens are creating the data of an official information system, this raises the question of the administrative quality assurance role. it is for both political reasons and data-quality demands (high accuracy and consistency are needed) that an administrative authority must manage and continuously supervise the public's data acquisition. enriching authoritative administrative data through crowdsourcing therefore is a sensitive issue; an e-governance application requires an appropriate quality of gathered information while retaining sovereign rights of state administrations. to bring the data in kulis to this standard, a special information qualification process was required. with the flagged revisions extension of mediawiki, additional groups with new user rights were implemented in the system. a user class 'editor,' which had the right to mark articles as read and sifted, and a user class 'reviewer,' who could validate the correctness of articles in kulis, were created this way. any registered user working with the cultural landscape information system could become an 'editor,' whereas a 'reviewer' would have to be an authorized person from a state institution. figure 5 shows the concept of the quality management of the presented system, and illustrates the different tasks of the public and state institutions. in the illustration, the flow of cultural asset information and a validation process for gathered data is shown. state institutions offer their data in the system and can validate and qualify the correctness of the combined information of public crowdsourcing and government data. the public sector is able to access the validated information. in order to configure and adapt the used open source extension to the needs of the project, flags on top of each article were placed to inform wiki frontend users about the information quality of the cultural assets. three simple levels of flags were used, symbolised by the colours of a traffic light (see figure 6 ). newly created articles were first marked as red, read and sifted articles by authorized users as orange, and quality proofed articles by state authorization as green. by implementing this intuitive and simple highlighting function, the status of the cultural asset information is shown at any time for all features. this way, permanent public access to all information according to the idea of open government data and citizen participation is maintained. at the same time, the state authority requirements concerning clear notification of the quality assurance of the presented information are met. with this clear assignment structure, the system is designed to avoid inappropriate information acquisition in more sensitive areas such as active archaeological excavations, where citizens are not allowed to gather information independently."
"in another analysis for step 3.8, images were categorized based on the nature of their creation. figure 3 portrays how the sample of images was categorized, per event. observing the analysis of the sample, it was found that around the 50% of the images focused on themes like the impact upon infrastructure, nature, and hazard related images (see table 3 ). australia appears as an exception, having graphic, energy, and business as the categories mostly identified in the images; additionally, transportation stands out, as no images were identified in this category. mexico exhibits a high percentage of images related to the people and activities category compared to the other events."
"a cnn is a type of neural network used to process inputs that are stored in arrays with fixed dimensions [cit] . it is a popular method used in deep-learning algorithms and is most frequently applied to images. however, cnns can also be applied to 1d arrays such as signals, text, and sequences and to three-dimensional (3d) arrays such as videos and volumetric images. regardless of dimensionality, cnns are used where there is some spatial or temporal ordering and proximity is significant. a 1d cnn can be combined with an embedding layer to produce better results."
"an rnn is a type of neural network capable of receiving input sequences of variable lengths because it processes the inputs one element at a time. an rnn uses the output of the previous input as an additional input for the next element. as a result, rnns are frequently applied to speech and language problems."
"when talking about the results associated with each category, it is interesting to find that the category \"people and activities\" had mexico, puerto rico, and argentina (both spanish speaking countries) with above average values compared to the other countries. it remains unclear if the interest of the observers (photographer) or the search engine may cause this. in the case of australia's heatwave, its 'transportation' category had 0% of images associated with it-a value far from the 6.60% average in the same category of the sample (table 3) . one reason for this may be that australia's sample images were mostly non-photographic (78%), presenting information in categories related to graphic (22.79% of images) and energy (25.74%) topics, such as prices of electricity, grid information, suggestions to tackle the heatwave, and others."
"in addition to embedding and classification layers, there are two other types of layers that make up a cnn, convolution and pooling, as shown in figure 1 . the convolution layers are the core of the cnn. this layer essentially applies a filter to a subset of the input at a specific instance of time. the application of the filter creates a weighted linear sum of the input's subset to which the filter is applied. then, a non-linear function is applied by implementing a rectified linear unit that is applied across the entirety of the height and width of the input. the end result of the application of a single convolutional layer (i.e., a filtering function) and the application of the non-linearity yields a feature map. the output of the cnn is a stack of feature maps, each capturing multiple convolutions of the input and each using a different filter."
"after the main categories of labels were identified, a researcher performed a manual inspection. a sample of the first 100 images for every event was considered, under the assumption that these were the most relevant to the query. images were examined to identify which of the main categories of labels identified were represented in them, to compare how labels and observer perception would be similar or differ. more than one category could be present in one image (i.e., presence of nature, hazard related, and people would be three categories on one image). a frequency log was used to track how many times each category was represented through all the images. table 2 presents examples of interest images for each category, as well as the labels associated with the image. allowing the most frequent elements to be examined."
"an rnn is a type of neural network capable of receiving input sequences of variable lengths because it processes the inputs one element at a time. an rnn uses the output of the previous input as an additional input for the next element. as a result, rnns are frequently applied to speech and language problems."
"along with the visual analysis in step 3.7, the origin of the image (i.e., photographic or nonphotographic) was identified. images that were the result of a photograph would be labeled as \"photographic\", while images whose presentation was either text, image, map, graphic, or infographic fell into the category of \"non-photographic\"."
"the hardest dgas to detect were the real word-based dgas. in table 5, the dgas with the lowest performance were cryptowall, gozi, matsnu, and virut. three of those are real word-based dgas. detection of volatile cedar/explosive, pizd, suppobox, and unknowndropper varied depending on the model. table 6 shows that the performance of all the models dropped significantly in detecting pizd and suppobox after the year. the reason for this is likely that the words used to create these domain names changed, creating vulnerability. only one of the real word-based dgas was present in the test dataset for the novel dga experiment: unknowndropper, which was entirely undetected. the difficulty in detecting this real word-based dga exemplifies how difficult it is to detect novel real word-based dgas and real word-based dgas when the words used to generate the domain name change. table 8 shows the evaluation times of the various models on a single domain. the times were calculated by dividing the amount of time it took to classify each domain in the random assignment test dataset individually by the number of domains. these results are in line with what would be expected. the fastest methods are the two 1d cnn models, followed by capsnet and lastly the lstm models. the lstm models take an order of magnitude longer to process a single domain, making them the most computationally expensive, with the bidirectional lstm taking the longest because it has to process the domain forward as well as backward."
"step 2: preparation of the three-dimensional model for printing ansys [cit], or other commercial software for structural analysis, allows checking that a free-form shell only exhibits compression stresses when submitted to its deadweight. moreover, the directions of the principal stresses are either parallel or normal to the symmetry axes. taking this into account, the shell model was divided into the lowest number of modules, using symmetry axes whenever possible, and bearing in mind the 3d printer capacity."
"the production process of the scale model was organized according to eight consecutive steps. the transition between shape generation and the execution of the scale model of the shell, using the 3d printing technology, implies the previous production of the three-dimensional model. the selection of the most adequate cad environment in this context is conditioned by the nature of the curved geometry, continuous and smooth, of the object."
"among the graphic computation software available, the one with the commercial name rhinoceros ® produced by robert mcneel & associates [cit] was selected. this software is based on the non-uniform rational basis spline (nurbs) geometry, a mathematical model currently used in graphic computation software to represent both curves and surfaces with accuracy and flexibility. it allows the description of any type of shape, from the most simple (2d curves) to the most organic and complex, such as three-dimensional free shapes [cit] ."
"dimensional analysis is a powerful tool for multiple domains of knowledge. in the study of wind effects on scale models of structures, important aspects of dimensional analysis and scale factors applicable to similar physically systems are presented by pankhurst [cit] for fluid dynamic problems. the geometrically similar systems constitute the basis of the whole field of scale model testing in physically similar systems. the scale model represents a geometrically similar shape to the prototype by using concepts as geometrical scale factor, velocity scale factor, time scale factor, among others, in physically similar systems [cit] ."
"the means of digital production ensured reliable data transfer from the virtual to the physical environment, respecting the formal integrity of this type of non-conventional object. in this scope, it can be stated that the 3d printing technology constitutes the adequate production process since it allows the faithful reproduction of the model. nevertheless, the volumetric limitation of the operational capacity of such equipment makes the subdivision of the model inevitable. the use of complementary technologies and workmanship means allowing rigorous rebuilding of the shape of the shell."
"for the current study, the weighting of the shell dimensions took into account the compatibility of the following conditions: (i) its division into a minimum number of elements transportable on a semi-trail; (ii) coverage of a large span, thus justifying the choice of this type of structures; and (iii) assurance of the largest spatial extensibility. a membrane with a diameter of 25.0 m, in plan view, and maximum height of 8.0 m satisfies those conditions and, compared to other options (e.g., height of 5.0, 7.0, or 7.5 m), is the one that best meets the required spatial extensibility. in fact, this value allows a vertical division of the span into two floors, simultaneously ensuring the largest area with a minimum of 3.0 m height at the upper floor, as an administrative zone. the width of the supports was also considered. initially, three alternatives were tested: 0.8 m, 1.6 m and 2.5 m. the thinner support option prevailed, with a width adjusted to 1.0 m, since it provides the shell with its lightest expression. figure 1 evidences some of the several iterations considered before the final shape was settled. the goal of the prefabrication and division of the shell into parts, and the absence of a predefined occupation program, led the authors to consider the shell as a modular element. the 'shell module' concept allows expanding the covered area (circa 270 m 2 ) using different the goal of the prefabrication and division of the shell into parts, and the absence of a predefined occupation program, led the authors to consider the shell as a modular element. the 'shell module' concept allows expanding the covered area (circa 270 m 2 ) using different combinations aiming at reaching the diversity of uses. thus, prefabrication of the shell and space-functional flexibility converge in the genesis of the pilot-shell. its configuration (an equilateral triangle in plan view) motivated the combinations shown in figure 2 . the shape of the shell, named h800n3, was conditioned by the design options previously mentioned."
"the points where the pressure sensors are applied need to be located in the pre-finishing of the object, requiring both mechanical and manual sanding of the surfaces, and also pre-painting with acrylic coating. the support of the pressure sensors, being only one third of the instrumented shell, was bonded and thus stayed free of obstruction. the interfaces for fastening were executed in aluminum flat bar of 2 mm (figure 9(3) ). the final object was adequately resistant for the desired end ( figure 9(4) ). the points where the pressure sensors are applied need to be located in the pre-finishing of the object, requiring both mechanical and manual sanding of the surfaces, and also pre-painting with acrylic coating. the support of the pressure sensors, being only one third of the instrumented shell, was bonded and thus stayed free of obstruction. the interfaces for fastening were executed in aluminum flat bar of 2 mm (figure 9(3) ). the final object was adequately resistant for the desired end ( figure 9(4) )."
"an innovative method for the automatic shape generation and production of reduced-scale models of ultra-thin concrete shells, built with a geometrically nonlinear finite-element-based software and 3d-printing, was presented herein. guidelines for the method have been defined, aiming to serve as reference in future research studies."
"the adopted scale was defined according to the model's objective. lower reduction scales are obviously favourable to more detailed analyses, but it was decided to take other variables into"
"in a free shape there is no geometrical or mathematical rule allowing its reconstitution. this was the principal challenge of the model's production, namely the control of its shape. in this sense, the cad/cam [cit] technologies allow us to ensure a 'natural' migration of data from the computational generation of the shape, including model analysis in a virtual environment, to scale model production using digital means. the flowchart in figure 3 aims at synthesizing the main stages and corresponding outputs of the global process, which evolved from the shell conception to the production of the corresponding scale model. this sequence ensures that the geometry initially defined (in the shape generation process) is present in the physical model. instant 1 in figure 3 illustrates the transition from conceptual to virtual: through successive iterations-depending on the project's options-the shape was continuously modelled. the most sensitive moment is precisely the transition from the virtual to the physical environment, instant 2 in figure 3, since the integrity of the free shape geometry has to be faithfully ensured in the physical object. normally, this transition is guaranteed by conversion of the cad file (of the 3d model) into a standard triangulation or stereolithography language (stl) file, performed from the cad environment itself [cit] . the stl file defines, by triangulation, the coordinates of the object surfaces whose edges must be perfectly coincident. the software of the machines used in digital production generally processes this file type. although this type of conversion process has possible error associated (a slight deviation between edges is enough to compromise the printing), it is generally adopted since it presents some advantages, such as being a user-friendly method to represent 3d data and, for certain shapes, it allows obtaining small files, thus simplifying the data transfer [cit] . the use of traditional or manual methods to build a shell 3d model would imply a bigger effort, in time and resources (including human ones), to ensure the surface's shape control. the gap between the free-form shell generation phase and the reduced model phase would not be so 'naturally' transposed."
"in the next step, 3d printing technology was applied to build a scale model of the shell from its numerically generated free shape. the accuracy of the process was evaluated using terrestrial photogrammetry. a multi-station approach [cit], aiming at the full reconstruction of the scale model, was applied. the adopted procedure required high precision circular targets to be glued to the upper surface of the scale model (figure 15(1) ). this enabled referencing the 384 high precision targets in all images processed (figure 15(2) ). according to the acquisition protocol, multiple convergent shots of eight images around the object were sequentially obtained. figure 14 . free-form shell generated using a geometrically nonlinear analysis [cit] ."
"as a final conclusion, it can be stated that the innovative method herein presented, developed to generate the shape of shells and produce scale models for wind tunnel testing, is a powerful tool, having as limitation only the current constraints of the 3d printing technology. in addition, with the envisaged developments of this technology, the method is expected to become in the near future the most adequate for the production of scale models as well as for other applications."
"the vertical positioning of the pieces to be printed presents the advantage, compared to horizontal or near-horizontal positioning, of saving the support filament. figure 6 (1) presents, as an example, the printing of the largest piece of the scale model used (see figure 5(2,3) for part 1). the support filaments used are the minimum and necessary to avoid the collapse of the model parts during the printing process."
"ansys [cit], or other commercial software for structural analysis, allows checking that a free-form shell only exhibits compression stresses when submitted to its deadweight. moreover, the directions of the principal stresses are either parallel or normal to the symmetry axes. taking this into account, the shell model was divided into the lowest number of modules, using symmetry axes whenever possible, and bearing in mind the 3d printer capacity."
"in the next step, 3d printing technology was applied to build a scale model of the shell from its numerically generated free shape. the accuracy of the process was evaluated using terrestrial photogrammetry. a multi-station approach [cit], aiming at the full reconstruction of the scale model, was applied. the adopted procedure required high precision circular targets to be glued to the upper surface of the scale model (figure 15(1) ). this enabled referencing the 384 high precision targets in all images processed (figure 15(2) ). according to the acquisition protocol, multiple figure 14 . free-form shell generated using a geometrically nonlinear analysis [cit] ."
"in the present case, the planned aerodynamic analysis in a wind tunnel imposes clear and simple requirements: (1) rigorous reproduction of the geometry of the shape; (2) smoothing surface(s) to be tested; and (3) positioning of the pressure sensors on the surface of the model. the analysis includes testing both (internal and external) surfaces of the shell. the optimization of recourses, short deadlines and the formal complexity of the model led to adopting pressure sensors that could be easily adapted to the surfaces. the specificity of shape generation is directly related to the production procedures of the model for aerodynamic tests, mainly if the scale of the model is subjected to problems associated with the ability to reproduce the curves of the shape in both the soffit and the extrados of the model. the restraints for the production are related to the scale definition, namely, the appropriate thickness and, particularly, the control of the shape in the transition from the virtual to the physical object."
"(corresponding to 0.10 m in real scale). the pieces were print reversed, relative to their normal position, ensuring this way the higher stability of the shape during printing. the supports (brown colour thread) are automatically determined by the printers' software. both printers use the fused deposition modelling (fdm) [cit] printing technique, and use abs threads for model printing. the thermal isolation of the industrial printer ensures the stability of the shape avoiding warp phenomena, which are particularly relevant in the case of long and thin pieces, as in the case of this second model printing (figure 6(2,3) )."
"the origin of the shell design was conditioned by assumptions in order to adapt their formal complexity to the goal of the project. free-form shell structures allow the covering of large spans without intermediate supports. spread over the void, these structures can define wide and continuous spaces. in its basic essence, a shell is just a roof protecting a space. this capacity creates a high functional flexibility, including the criteria of (i) extensibility, i.e., the possibility of expansion; (ii) convertibility, i.e., the capacity of changing the internal organization; and (iii) versatility, i.e., the adaptability of the space [cit], which can accommodate a wide set of programs. this option benefits from a minimum number of supports, releasing the shell as much as possible from the ground, and determining its triangular shape."
"(1) the resolution is an important printing definition. basically, the resolution concept in 3d printing refers to the thickness and density of the extruded filament layers. [cit] have explored the meaning of the term 'resolution' applied to 3d printing, as well as the relation between the representation of the virtual model and the resolution of the physical object. likewise, for 2d printing the resolution and printing time are directly related: the higher the resolution, the higher the printing time. for the replicator 2x, the standard resolution (0.2 mm by layer) was selected in the study described and the same is proposed in these guidelines. for this example, the printing times range between 9 and 12 h, depending on the dimensions of the piece. the total printing time was estimated in 73 h. in the case of the dimension sst760, each piece, printed in the highest resolution mode (0.254 mm by layer), consumed 15 h. the total printing time of the model was approximately 45 h."
"example, the printing of the largest piece of the scale model used (see figure 5(2,3) (figure 6(2,3) ). in this alternative procedure, the model was divided only into three pieces, printed with a thickness of 2 mm (corresponding to 0.10 m in real scale). the pieces were print reversed, relative to their normal position, ensuring this way the higher stability of the shape during printing. the supports (brown colour thread) are automatically determined by the printers' software. both printers use the fused deposition modelling (fdm) [cit] printing technique, and use abs threads for model printing. the thermal isolation of the industrial printer ensures the stability of the shape avoiding warp phenomena, which are particularly relevant in the case of long and thin pieces, as in the case of this second model printing (figure 6(2,3) )."
"for the current study, the weighting of the shell dimensions took into account the compatibility of the following conditions: (i) its division into a minimum number of elements transportable on a semi-trail; (ii) coverage of a large span, thus justifying the choice of this type of structures; and (iii) assurance of the largest spatial extensibility. a membrane with a diameter of 25.0 m, in plan view, and maximum height of 8.0 m satisfies those conditions and, compared to other options (e.g., height of 5.0, 7.0, or 7.5 m), is the one that best meets the required spatial extensibility. in fact, this value allows a vertical division of the span into two floors, simultaneously ensuring the largest area with a minimum of 3.0 m height at the upper floor, as an administrative zone. the width of the supports was also considered. initially, three alternatives were tested: 0.8 m, 1.6 m and 2.5 m. the thinner support option prevailed, with a width adjusted to 1.0 m, since it provides the shell with its lightest expression. figure 1 evidences some of the several iterations considered before the final shape was settled. such as area, fixed points or supports, maximum height, and height of free edges [cit] . these apply the action of their dead weight to reveal the final shape of the shell. the origin of the shell design was conditioned by assumptions in order to adapt their formal complexity to the goal of the project. free-form shell structures allow the covering of large spans without intermediate supports. spread over the void, these structures can define wide and continuous spaces. in its basic essence, a shell is just a roof protecting a space. this capacity creates a high functional flexibility, including the criteria of (i) extensibility, i.e., the possibility of expansion; (ii) convertibility, i.e., the capacity of changing the internal organization; and (iii) versatility, i.e., the adaptability of the space [cit], which can accommodate a wide set of programs. this option benefits from a minimum number of supports, releasing the shell as much as possible from the ground, and determining its triangular shape."
"after checking the shape correction, it is necessary to strengthen the model. the contact area of the joints, due to their reduced thickness, was insufficient to ensure the needed strength. thus, a fibreglass fabric was applied to its soffit (300 gr/m 2 ) providing the object with the required robustness (figure 9(1) ). the pre-accelerated polyester resin, used to apply the fibreglass fabric, also"
"the expression 'shell structures with free shapes' refers to curved structures of reduced thickness that do not result from any mathematical expression, although exhibiting optimized structural efficiency. the name has its origin in the procedure adopted to generate the shape of these shells, which is based on the antifunicular of their deadweight, being therefore independent of geometric concepts [cit] ."
"in the present case, the planned aerodynamic analysis in a wind tunnel imposes clear and simple requirements: (1) rigorous reproduction of the geometry of the shape; (2) smoothing surface(s) to be tested; and (3) positioning of the pressure sensors on the surface of the model. the analysis includes testing both (internal and external) surfaces of the shell. the optimization of recourses, short deadlines and the formal complexity of the model led to adopting pressure sensors that could be easily adapted to the surfaces. the specificity of shape generation is directly related to the production procedures of the model for aerodynamic tests, mainly if the scale of the model is subjected to problems associated with the ability to reproduce the curves of the shape in both the soffit and the extrados of the model. the restraints for the production are related to the scale definition, namely, the appropriate thickness and, particularly, the control of the shape in the transition from the virtual to the physical object."
"an average ambient temperature of approximately 22 °c. the points where the pressure sensors are applied need to be located in the pre-finishing of the object, requiring both mechanical and manual sanding of the surfaces, and also pre-painting with acrylic coating. the support of the pressure sensors, being only one third of the instrumented shell, was bonded and thus stayed free of obstruction. the interfaces for fastening were executed in aluminum flat bar of 2 mm (figure 9(3) ). the final object was adequately resistant for the desired end ( figure 9(4) )."
the data collected from this experiment (and the development of more models) will support a systematic method of production of reduced models of free-form shells for wind tunnel tests.
"the object (figure 4(1) ). using the plug-in resurf (function 'pointcloudtomesh'), a regular mesh is generated, i.e., a surface discretized in triangular faces, creating a surface of curves, which is smoother the higher the number of points (figure 4(2) ). the solid is generated from this mesh. the rhinoceros ® software operates with mesh objects. a solid is interpreted as a closed volume, made of meshes, and is called a 'solid mesh'. the mesh is generated starting from the mid-plan mesh by adding 2 mm layers each time (figure 4(3) )."
the importance of 'free shape' projects and their potential applications results clearly from the creation of the task-group 'free-form design' of the international association for shell and spatial
"in these structures, with reduced thickness compared to the other dimensions, the best shape leads only to internal forces of compression [cit] . in this scope, the 'free shape' can be seen as a 'natural shape' in the sense that it results from the deformation of a membrane (with tension stiffness only), with the same geometry in plan view as the shell, loaded with the reverse deadweight of the latter. thus, the shape results only from the application of that action, without any direct human intervention."
"the phased connection of the pieces allows the easy correction of eventual faults. first, using the sub-mould, all parts of the same third of the object are connected (figure 8(1,2) ); next, using the mould, the connection of the three thirds allows building the complete object (figure 8(3) ). the adopted bonding agent is fundamental in this process. it must allow the reversibility of the linking process if needed. in the study herein presented a transparent, acrylic-urethane-based liquid glue was used. the thin end of the glue container made it possible to control the liquid dosage and apply glue points to the pieces' joints. with the help of an ultra-violet (uv) light-emitting diode (led), the glue is dried easily (figure 8(2) ). the joints obtained are robust enough to immediately check the achieved shape ( figure 8(4) ). if the result is unsatisfactory, it is possible to easily break the glue points, remove the waste, and reinitiate the procedure. in the study, the profiles for the moulds were designed based on the 3d model. a previous mould had been produced for validation, using corrugated cardboard and foam board (figure 7(1) ). the final moulds were produced in medium-density fiberboard (mdf) plates with a 3 mm, laser cut (figure 7(2) ) to ensure the maximum precision in the resulting profiles. the option for lower thicknesses has the advantage of reducing the cutting time. in the study conducted, the average time estimated based on the power of the adopted equipment (1100 w) was circa of 15 min per plate. in the joint zones, where it is advisable to increase the support area of the pieces, 3 mm profiles were added. cutting several profiles also allowed a better adjustment of the mould to the curvilinear shape of the pieces. the profiles were assembled and the connections were further consolidated through screwing and gluing."
"in digital production methods, the mechanical devices are computationally controlled, which is the case for 3d printing [cit] . the 3d printing allows obtaining a facsimile of the virtual model [cit], independently of the complexity of the shape. for this reason, this was the production technology"
"the subdivision of the model implies the definition of procedures for the reconstitution of the global shape. this goal is reachable by conceiving moulds with the purpose of enabling the connection of all the pieces in the right position. the moulds (mould and sub-mould) correspond to the resolution is an important printing definition. basically, the resolution concept in 3d printing refers to the thickness and density of the extruded filament layers. [cit] have explored the meaning of the term 'resolution' applied to 3d printing, as well as the relation between the representation of the virtual model and the resolution of the physical object. likewise, for 2d printing the resolution and printing time are directly related: the higher the resolution, the higher the printing time. for the replicator 2x, the standard resolution (0.2 mm by layer) was selected in the study described and the same is proposed in these guidelines. for this example, the printing times range between 9 and 12 h, depending on the dimensions of the piece. the total printing time was estimated in 73 h. in the case of the dimension sst760, each piece, printed in the highest resolution mode (0.254 mm by layer), consumed 15 h. the total printing time of the model was approximately 45 h."
"the phased connection of the pieces allows the easy correction of eventual faults. first, using the sub-mould, all parts of the same third of the object are connected (figure 8(1,2) ); next, using the mould, the connection of the three thirds allows building the complete object (figure 8(3) ). the adopted bonding agent is fundamental in this process. it must allow the reversibility of the linking process if needed. in the study herein presented a transparent, acrylic-urethane-based liquid glue was used. the thin end of the glue container made it possible to control the liquid dosage and apply glue points to the pieces' joints. with the help of an ultra-violet (uv) light-emitting diode (led), the glue is dried easily (figure 8(2) ). the joints obtained are robust enough to immediately check the achieved shape (figure 8(4) ). if the result is unsatisfactory, it is possible to easily break the glue points, remove the waste, and reinitiate the procedure."
"after checking the shape correction, it is necessary to strengthen the model. the contact area of the joints, due to their reduced thickness, was insufficient to ensure the needed strength. thus, a fibreglass fabric was applied to its soffit (300 gr/m 2 ) providing the object with the required robustness ( figure 9(1) ). the pre-accelerated polyester resin, used to apply the fibreglass fabric, also"
"ansys [cit], or other commercial software for structural analysis, allows checking that a free-form shell only exhibits compression stresses when submitted to its deadweight. moreover, the directions of the principal stresses are either parallel or normal to the symmetry axes. taking this into account, the shell model was divided into the lowest number of modules, using symmetry axes whenever possible, and bearing in mind the 3d printer capacity."
"the software for shape generation provides an output list of the coordinates of an adequate number of points to describe the shell with the required detail for realistic modelling. the .txt file imported to the rhinoceros ® environment allows the visualization of the point cloud that represents the object (figure 4(1) ). using the plug-in resurf (function 'pointcloudtomesh'), a regular mesh is generated, i.e., a surface discretized in triangular faces, creating a surface of curves, which is smoother the higher the number of points (figure 4(2) ). the solid is generated from this mesh. the rhinoceros ® software operates with mesh objects. a solid is interpreted as a closed volume, made of meshes, and is called a 'solid mesh'. the mesh is generated starting from the mid-plan mesh by adding 2 mm layers each time (figure 4(3) )."
"the phased connection of the pieces allows the easy correction of eventual faults. first, using the sub-mould, all parts of the same third of the object are connected (figure 8(1,2) ); next, using the mould, the connection of the three thirds allows building the complete object (figure 8(3) ). the adopted bonding agent is fundamental in this process. it must allow the reversibility of the linking process if needed. in the study herein presented a transparent, acrylic-urethane-based liquid glue was used. the thin end of the glue container made it possible to control the liquid dosage and apply glue points to the pieces' joints. with the help of an ultra-violet (uv) light-emitting diode (led), the glue is dried easily (figure 8(2) ). the joints obtained are robust enough to immediately check the achieved shape (figure 8(4) ). if the result is unsatisfactory, it is possible to easily break the glue points, remove the waste, and reinitiate the procedure."
"for the pressure sensors, poly vinyl chloride (pvc) pipes with a 2 mm exterior diameter were adopted. a length of 10 mm was enough since the assembly to the silicone pipes that connect to the data-logger only demands a free end of 5 mm. drilling was performed (figure 11(1,2) ), making it possible to fix the pvc pipes by pressing (figure 11(3) ) and adjusting them easily to each of the (internal or external) surfaces ( figure 11(4) ). the painting with the coating resulted in a polished final finishing of the surfaces (figure 11(2) ) required for the wind tunnel tests. in figure 12, aspects of the concluded scale model are documented. the pressure sensors were manually numbered at the outer surface of the model (figure 12(2) ). in figure 12 (4) an instant during the preliminary tests in the closed circuit aerodynamic tunnel at lnec is illustrated. for the pressure sensors, poly vinyl chloride (pvc) pipes with a 2 mm exterior diameter were adopted. a length of 10 mm was enough since the assembly to the silicone pipes that connect to the data-logger only demands a free end of 5 mm. drilling was performed (figure 11(1,2) ), making it possible to fix the pvc pipes by pressing (figure 11(3) ) and adjusting them easily to each of the (internal or external) surfaces ( figure 11(4) ). the painting with the coating resulted in a polished final finishing of the surfaces (figure 11(2) ) required for the wind tunnel tests. in figure 12, aspects of the concluded scale model are documented. the pressure sensors were manually numbered at the outer surface of the model (figure 12(2) ). in figure 12 (4) an instant during the preliminary tests in the closed circuit aerodynamic tunnel at lnec is illustrated. besides the characteristics associated with their structural behaviour, free-form shells hold potential values of a plastic nature arising from their 'natural' geometry. the second model referred to ( figure 13 ) tries to explore this dimension of the expression and shape values."
"the thickness of the shell in the scale model can be considered a less demanding requirement for the wind tunnel tests, since it does not interfere with the pressure measurements at the model's surface. in fact, it is the geometry of both the inner and outer surfaces that plays an important role in this scope. it should also be mentioned that, in the real structure, the thickness is supposed to be circa 100 mm, leading to 2 mm thickness for a 1:50 scale model. however, if this value was adopted, the production of the scale model would show several difficulties due to its fragility-a vast but thin surface-having at the same time to exhibit robustness in order to stand the wind tests and all the manipulation typical of laboratorial environments. therefore, to avoid the drawbacks referred to and without compromising the geometry, it was decided to increase the thickness of the scale model to 4 mm, corresponding to 200 mm in real scale."
"the 3d printers for domestic use (or desktop) have generalized the access to this technology. several researchers and computer programmers have developed 3d open-source printers with high socio-economic impact [cit], making it possible to easily print daily use objects. the project 3d hubs [cit] refers to a 3d printers network that connects users of these printers worldwide. the hubs community offers 3d printing services, being the budget function of the stl file. currently, costs are approximately one euro per gram of material. in the case of reduced models of complex forms such as shells, 3d printing allows obtaining geometrically accurate models with a low cost. the only disadvantage is the possible constraint actually still imposed by the printing working volume of the 3d printers. depending on the scale of the models, its printing in several parts could be necessary. in this case, the time and costs of the assembly of the parts must be considered."
"the stages of the project 'design and performance of ultra-thin concrete shells' go from the shape generation to the erection of the shell, including the production of the scale model and prototype needed at different stages. in this scope, shape generation follows a path completely different from traditional design, according to which the shape is fully defined and controlled by the designer. on the contrary, in the case of ultra-thin concrete shells, the shape results from a membrane deformation, being indirectly determined through options assumed on key parameters, such as area, fixed points or supports, maximum height, and height of free edges [cit] . these apply the action of their dead weight to reveal the final shape of the shell."
"for the pressure sensors, poly vinyl chloride (pvc) pipes with a 2 mm exterior diameter were adopted. a length of 10 mm was enough since the assembly to the silicone pipes that connect to the data-logger only demands a free end of 5 mm. drilling was performed (figure 11(1,2) ), making it possible to fix the pvc pipes by pressing (figure 11(3) ) and adjusting them easily to each of the (internal or external) surfaces (figure 11(4) ). the painting with the coating resulted in a polished final finishing of the surfaces (figure 11(2) ) required for the wind tunnel tests. in figure 12, aspects of the concluded scale model are documented. the pressure sensors were manually numbered at the outer surface of the model (figure 12(2) ). in figure 12 (4) an instant during the preliminary tests in the closed circuit aerodynamic tunnel at lnec is illustrated."
"in the study, the profiles for the moulds were designed based on the 3d model. a previous mould had been produced for validation, using corrugated cardboard and foam board (figure 7(1) ). the final moulds were produced in medium-density fiberboard (mdf) plates with a 3 mm, laser cut ( figure 7(2) ) to ensure the maximum precision in the resulting profiles. the option for lower thicknesses has the advantage of reducing the cutting time. in the study conducted, the average time estimated based on the power of the adopted equipment (1100 w) was circa of 15 min per plate. in the joint zones, where it is advisable to increase the support area of the pieces, 3 mm profiles were added."
"the subdivision of the model implies the definition of procedures for the reconstitution of the global shape. this goal is reachable by conceiving moulds with the purpose of enabling the connection of all the pieces in the right position. the moulds (mould and sub-mould) correspond to the extrados of the object, the easiest way of positioning the pieces and rebuilding the profiles of the subdivision lines (figure 7) ."
"dimensional analysis is a powerful tool for multiple domains of knowledge. in the study of wind effects on scale models of structures, important aspects of dimensional analysis and scale factors applicable to similar physically systems are presented by pankhurst [cit] for fluid dynamic problems. the geometrically similar systems constitute the basis of the whole field of scale model testing in physically similar systems. the scale model represents a geometrically similar shape to the prototype by using concepts as geometrical scale factor, velocity scale factor, time scale factor, among others, in physically similar systems [cit] ."
"cutting several profiles also allowed a better adjustment of the mould to the curvilinear shape of the pieces. the profiles were assembled and the connections were further consolidated through screwing and gluing. in the study, the profiles for the moulds were designed based on the 3d model. a previous mould had been produced for validation, using corrugated cardboard and foam board (figure 7(1) ). the final moulds were produced in medium-density fiberboard (mdf) plates with a 3 mm, laser cut ( figure 7(2) ) to ensure the maximum precision in the resulting profiles. the option for lower thicknesses has the advantage of reducing the cutting time. in the study conducted, the average time estimated based on the power of the adopted equipment (1100 w) was circa of 15 min per plate. in the joint zones, where it is advisable to increase the support area of the pieces, 3 mm profiles were added. cutting several profiles also allowed a better adjustment of the mould to the curvilinear shape of the pieces. the profiles were assembled and the connections were further consolidated through screwing and gluing."
"firms are recognizing social media as a prominent indicator of equity value that not only improves short-term performance, but also brings about long-term productivity benefits . the reviewed studies suggest that incorporatin social media in firms increases meta-knowledge (who's who in an organization and who does what), which helps avoid knowledge duplication and promotes new ways of managing work [cit] . active management of social media has been observed to be more effective when those inside rather than outside a firm are engaged ."
"the results in sections 5.3.1 and 5.3.2 further demonstrate the importance of proposed features and temporal information in retweeting sentiment tendency analysis, which correspondingly answers the second question."
"the simulation study has been done using the network simulator qualnet 4.0 for performance comparison of the protocols: dsdv, aodv, fsr, lar, olsr, star and zrp. the all seven routing protocols result in improvements of the performance metrics that include throughput, jitter, end-to-end delay and total packets received. it was observed from the simulation that dsdv and aodv gives nearly same and maximum throughput in small sized networks. throughput of fsr, lar, olsr, and star is increasing as the network size is increasing, but olsr performs well in large sized networks. throughput of zrp is well and it is nearer for small and large networks, but for large sized networks it is decreasing. for end-to-end delay and average jitter, the performance of dsdv and aodv is better than fsr, lar, olsr, star and zrp in case of small sized networks. in medium and large sized networks, the end-to-end delay and average jitter of aodv and dsdv protocols are same."
"as described in table 2, information gain of user's profile-based features is lower than other relationship-and emotion-based features. moreover, emotion-based features have higher information gains than relationship-based features. in addition, being processed based on the count of positive and negative emotional words, recent mood statistics and emotion divergence have larger information gains than positive or negative emotional words count."
"the domain of applications for ad hoc wireless networks is diverse, ranging from small, static networks that are constrained by power sources, to large-scale, mobile, highly dynamic networks [cit] . such networks are frequently viewed as a key communications technology enabler for network-centric warfare and military tactical operations-for fast establishment of military communications and troop deployments in hostile and/or unknown environments, disaster relief operations-for communication in environments where the existing infrastructure is destroyed, search and rescue operations and emergency situations-for communication in areas with no wireless infrastructure support, law enforcement-for secure and fast communication during law enforcement operations, commercial use-for enabling communications in exhibitions, conferences & large gatherings, intelligent transportation systems and fault-tolerant mobile sensor grids. most of these applications demand a secure and reliable communication."
"generally speaking, reactive protocols require fewer amounts of memory, processing power, and energy than that of the proactive protocols. the mobility and traffic pattern of the network must play the key role for choosing an appropriate routing strategy for a particular network. it is quite natural that one particular solution cannot be applied for all sorts of situations and, even if applied, might not be optimal in all cases. often it is more appropriate to apply a hybrid protocol rather than a strictly proactive or reactive protocol as hybrid protocols often possess the advantages of both types of protocols."
"the fact that our search terms yielded over 12,000 abstracts suggests that scholars are investing increased interest on research issues related to social media. while an informed researcher may have a general idea of the nature of research undertaken so far, it is humanly impossible to discern the thematic structure of all scholarly documents available on social media. recent advances in topic modeling have made this task relatively easy. topic modeling relies on algorithms and statistical methods to elicit the topics latent in a large corpus [cit] . the term topic refers to a specific and often recognizable theme defined by a cohesive set of words that have a high probability of belonging to that topic. there are several options available for topic modeling: non-negative matrix factorization (nnmf), latent semantic analysis/indexing (lsa/lsi), and latent dirichlet allocation (lda). in this study, we use lda, arguably the most widely used topic modeling algorithm. in order to perform topic modeling on a corpus, the researcher has to specify the number of topics to be extracted. in this study, we extracted the top 100 topics reflected in the scholarship on social media. lda starts with the assumption that each abstract in our study reflects each of these topics to varying degrees [cit] . thus, each abstract has a distribution of the desired 100 topics. the 100 topics that were extracted from our abstracts are shown in table 2 . the machine learning for language toolkit (mallet) [cit] was used for this purpose."
"given the inconsistencies in the use of keywords in social media research, a manual search, rather than a keywordbased one, was deemed to be more appropriate for identifying the existing literature on social media. furthermore, since keywords in the social media literature tend to overlap with topics and/or theories in other related research areas, a keyword search may yield irrelevant articles. for instance, a keyword search for bsocial network^returns articles related to social network theories, which are not necessarily part of social media. the articles reviewed in this study are from the following eight senior scholars' basket of information systems journals: european journal of information systems (ejis); information systems journal (isj); information systems research (isr); journal of the association for information systems (jais); journal of information technology (jit); journal of management information systems (jmis); journal of strategic information systems (jsis) and management information systems quarterly (misq)). along with these eight journals, we have also analysed relevant articles from information systems frontier (isf) journal. this is because it focuses on examining bnew research and development at the interface of information systems (is) and information technology (it) from analytical, behavioural, and technological perspectives. it provides a common forum for both frontline industrial developments as well as pioneering academic research^."
"− which is defined in section 3.2.1 to stand for the th time slice 's weight, and dynamic interaction frequency between user and user v on is defined as follows:"
the literature search for this analysis was conducted in the following two phases: (1) keyword-based search and analysis to explore the overall evolution of social media literature; and (2) manual search across specific is journals to understand the emerging is perspectives on this topic.
"to study the problem of retweeting sentiment tendency analyzing, we leverage a sina microblogging dataset [cit] which contains time series of users' tweets, retweets, and followings' number from september 28, 2012, to october 29, 2012, to evaluate the validity of the proposed method. moreover, since not all users post opinions when retweeting, we manually label a subset of sina microblogging which contains retweeting contents with sentiment polarity. statistics of the dataset are shown in table 1 . the experimental settings of retweeting sentiment tendency analyzing are described as follows: we randomly divide the dataset into two parts and . possesses 90% of retweets used for training. the left 10% of retweets denoted as is designated for testing. and we use 10-fold crossvalidations to ensure that our results are reliable and report the mean performance via precision, recall, and 1-measure."
multicast routing protocols: multicast is the delivery of information to a group of destinations simultaneously. multicast routing protocols for manet use both multicast and unicast for data transmission. multicast routing protocols for manet can be classified again into two categories: tree-based and mesh-based multicast routing protocols. mesh-based routing protocols use several routes to reach a destination while the tree-based protocols maintain only one path. tree-based protocols ensure less end-to-end delay in comparison with the mesh-based protocols.
"some of the hybrid routing protocols are: distributed spanning tree based routing protocol (dst), coreextraction distributed ad hoc routing protocol (ce-dar), zone routing protocol (zrp), zone-based hierarchical link state routing protocol (zhls), distributed dynamic routing protocol (ddr), scalable location update routing protocol (slurp), hybrid ad hoc routing protocol (harp)."
"in addition, our review of the literature on social media identified dominant research methods employed by scholars. qualitative, quantitative, and mixed methods were used by most of these studies. closer scrutiny of the 132 publications reviewed in this study revealed the multitude of techniques applied for gathering data. quantitative methods employed in these studies mostly adopted analytical techniques and surveys (table 4) . on the other hand, publications using qualitative methods mainly used case studies and interviews to gather the required data (table 4) . as expected, studies employing mixed methods used a combination of analytical and conceptual techniques, alongside surveys and content analysis (table 4) . table 4 summarizes the various research approaches used by publications in our corpus."
"some researchers found that naïve bayes was more suitable for sentiment classification on microblogging [cit] . on the basis of bayes' theorem, naïve bayes model presents uncertainty with probability and realizes process of learning and reasoning via probability. hence, in this paper, we put forward a multilayer naïve bayes model which is a tweak of naïve bayes model to analyze user's retweeting sentiment tendency in a fine granularity. we formally define retweeting sentiment tendency analysis as follows: given a group of retweets with related discrete feature vector and corresponding retweeting sentiment tendency label, we aim to leverage prior knowledge to automatically assign retweeting sentiment tendency labels to unknown retweets. and our proposed method consists of three modules: (1) naïve bayes models in bottom layer; (2) naïve bayes model in middle layer; (3) naïve bayes model in top layer. the process outlined is shown in figure 1, where the bottom layer is naïve bayes model for predicting user's profile and emotion, the middle layer is naïve bayes model for predicting user's relationship, and, finally, in the top layer, through multilayer nested naïve bayes model, user's retweeting sentiment tendency is predicted. the detailed descriptions are shown as follows."
"with information systems now expanding beyond organizational peripheries to become a part of the larger societal context, it is important for strategic information systems research to delve into the competitive setting of dynamic social systems. online communities are introducing extrinsic rewards that do not limit users' intrinsic motivations. research on such communities should expand to study the interplay between extrinisic and intrinsic rewards, particularly in terms of their ability to cultivate and sustain users' intrinsic motivations. from an organizational perspective, research on social media should move past the conventional dyadic view of the relationship between an online community and a firm, and focus on reconceptualising online users as an ecosystem of stakeholders. social media has re-established the dynamics between organizations, employees, and consumers. [cit], future researchers should aim to analyze stakeholders' potential in adopting social media tools to successfully accomplish their work goals. as for the limitations of this collective review, publications reviewed here were limited to only nine journals. this potentially means studies with significant contributions to social media literature published in other journals have been overlooked. future researchers can look to overcome such exclusions and focus on the overall review of literature on social media platforms. future reviews may focus on reviewing articles published in a larger number of is journals related to a specific type of social media (i.e. social networking sites, blogs), or specific issues related to social media use, such as information load, stress, and impact on productivity. despite these limitations, our study provides a comprehensive and robust intellectual framework for social media research that would be of value to adacemics and practitioners alike."
"aodv has bidirectional route from source to destination. to find a path from source to destination, the source broadcasts a route request packet. the neighbors in turn broadcast the packet to their neighbors till it reaches an intermediate node that has recent route information about the destination or till it reaches the destination. a node discards a route request packet that it has already seen. the route request packet uses sequence numbers to ensure that the routes are loop free. when a node forwards a route request packet to its neighbors, it also records in its tables the node from which the first copy of the request came. this information is used to construct the reverse path for the route reply packet. aodv uses only symmetric links because the route reply packet follows the reverse path of route request packet. as the route reply packet traverses back to the source, the nodes along the path enter the forward route into their tables. for route maintenance, when a source node moves, it can reinitiate route discovery to the destination. if one of the intermediate nodes moves, then the moved nodes neighbor realizes the link failure and sends a link failure notification to its upstream neighbors and so on till it reaches the source upon which the source can reinitiate route discovery if needed."
"in reactive routing protocols, also known as on-demand routing protocols, a node creates a route in an on-demand fashion, i.e. it computes a route only when needed. when a source wants to send packets to a destination, it invokes the route discovery mechanisms to find the path to the destination. route discovery usually occurs by flooding a route request packet throughout the network. route reply is sent back if the destination itself or node with route to the destination is reached. the discovery procedure terminates either when a route has been found or no route available after examination for all route permutations. reactive routing does not maintain global topological information and, therefore, substantially reduces energy con-"
"protocol (aodv) the ad hoc on-demand distance vector (aodv) [cit] routing protocol is a reactive unicast routing protocol. the protocol constructs efficient route on demand with minimal control overhead and minimal route acquisition latency. aodv is essentially a combination of both dsr and dsdv algorithms and it borrows the basic on demand mechanism of route discovery and route maintenance from dsr, plus the use of hop-by-hop routing sequence numbers from dsdv. the destination sequence number can be used to ensure loop-free and to identify which route with the greatest sequence number is newer one."
"author co-citation analysis (aca) is a bibliometric technique that has been widely used to explicate the conceptual structure of disciplines [cit] . the underlying assumption in aca is that authors who are frequently cited together tend to work on similar concepts. thus, frequently co-cited authors are likely to cluster together when an aca is performed. vosviewer considers only first authors when it performs aca. only authors who had 50 or more citations were included in the analysis. figure 1 shows the results of aca."
"relevant articles were then identified and downloaded from each of the target journals by going through their archives. [cit] were considered in our analysis. articles, research notes, introductions, research commentaries, and editorial overviews relevant to social media were downloaded and numbered to prepare an apa style reference list. the first literature search resulted in 181 articles that had some relevance to the social media domain. a closer examination of individual abstracts and full articles led to the elimination of 49 irrelevant articles, thus giving us a total of 132 articles pertinent to the domain of interest (i.e., social media)."
"another set of studies investigate the differences between traditional and social media. [cit] compares the big money tactics for political campaigning with social media campaigning to reveal that internet and the blogosphere can majorly influence campaigning and election results. [cit] examine the importance of new and old media within the music industry; they find radio positively and consistently affecting sales of songs and albums, and sales displacement from free online sampling overpowering positive word of mouth on sales. [cit] compare traditional and social media to suggest that there are evils associated with the societal benefits of social media, and mass media has a detrimental effect on public discourse."
"given the relevance of social media to various stakeholders, and the numerous consequences associated with its use, social media has attracted the attention of researchers from various fields, including information systems. this is evidenced by the large number of scholarly articles that have appeared in various outlets. researchers have to expend an enormous amount of time and effort in collating, analysing, and synthesising findings from existing works before they embark on a new research project. given the significant number of studies that have already been published, a comprehensive and systematic review can offer valuable assistance to researchers intending to engage in social medi research. our literature search suggests that there are reviews on social media in the marketing context [cit] . however, there exists no comprehensive review that integrates and synthesises the findings from the articles published in information systems journals. such an endeavour will not only provide a holistic view of the extant research on social media, but will also provide researchers a comprehensive intellectual platform that can be used to pursue fruitful lines of enquiry to help advance research in this rapidly expanding area. to fulfill this goal, this study reviewed relevant articles to elucidate the key thematic areas of research on social media, including its benefits and spillover effects. the resulting review is expected to serve as a one-stop source, offering insight into what has been accomplished so far in terms of research on social media, what is currently being done, and what challenges and opportunities lie ahead. by doing so, this study explores the following aspects of existing research on social media: the next section of this paper gives a brief overview of the method employed for carrying out the literature search. the succeeding section discusses citation and text analyses of social media publications. subsequently, we outline the various ways in which scholars have defined social media. this is followed by a section that focuses on the evolution of social media research from an is perspective. next, we articulate the major themes emerging from prior research and use them as a backdrop for our review of the literature on social media. the ensuing section discusses our findings, followed by key conclusions and limitations of the study."
"social media and its associated risks have captured the attention of many authors. [cit] focuses on the problem of media convergence, whereby a gaming website includes social media features, putting vulnerable young audience at the risk of scamming. an australian study suggests that many users are unaware of the potential risks of disclosing personal information on social media site, or consider themselves as low risk targets [cit] find that the ease of forming and maintaining relationships on an enjoyable social platform motivates users to disclose personal information. their study shows that user trust in a service/network provider, and privacy control options on a networking site greatly dismiss user perceptions of associated risk. [cit] finds that farcing attacks on facebook occur at two levels -victim to phishers with phony profiles and victim to phishers soliciting personal information directly from them."
"model. naïve bayes model, which is based on bayes' theorem, reduces computational overhead via conditional independent assumption to classify unknown samples according to their maximum a posteriori probability. since the calculation of user's profile (denoted as up) conforms to naïve bayes model which is determined by the number of bifollowers (denoted as bi), the number of followers (denoted as fo), the number of followees (denoted as fe), the number of contents that user posts (denoted as cn), the user's province (denoted as pr), the user's city (denoted as ci), the user's gender (denoted as ge), the created time of user's account (denoted as ct), and the verified type of user's account (denoted as vt), up can be seen as root node of naïve bayes model; bi, fo, fe, cn, pr, ci, ge, ct, and vt can be seen as leaf nodes of naïve bayes model."
"given that bi, fo, fe, cn, and ct are continuous attributes, in order to calculate the conditional probability of them, we discretize them by using discrete intervals to represent them before modeling up: according to the above features' values, bi is mapped to three levels (namely, few, medium, and many); fo is mapped to three levels (namely, few, medium, and many); fe is mapped to three levels (namely, few, medium, and many); cn is mapped to three levels (namely, few, medium, and many); ct is mapped to three levels (namely, short, medium, and long). consequently, the probability that user's profile equals a certain discrete value is calculated as follows:"
"in the external access mode, then use a common bus address to visit. as the external access is targeted tm or cpu, on the tm and the cpu access is controlled by tmc and arb."
"the issue of negative posts has received considerable attention in the literature. prior research suggests that, overall, the impact of negative posts or electronic word of mouth is much higher than the positive ones that increase readership ). this problem is also prevalent in organizations. according to the studies reviewed here, organizations either prohibit employees from posting controversial content online, or employees themselves refrain from doing so, fearing negative repercussions. the same employees also share positive posts, and the adverse effect of the few negative posts is offset by positive ones. it is in a firm's interests to encourage free will enterprise blogging to break down knowledge silos and yield higher employee productivity ."
"these reviewed studies showcase that social networking encourages shared language and trust between employees in a workspace. another emerging suggestion highlights that organizations should exercise policy, and use socialization and leadership-based mechanisms to counter any problems resulting from differing workplace values. some of these studies show interest in the cognitive side of social ties that positively nurture social relationships and innovation performance."
"to sum up, retweeting sentiment tendency analyzing in dynamic social networks is in the stage of development; how to depict directivity of relationship, how to fuse multidimensional features reasonably, and how to build model that could adapt to dynamic evolution process of network can be very challenging jobs. to this end, we present a multilayer naïve bayes model for analyzing user's retweeting sentiment tendency which is appropriate for dynamic and directed social networks."
"digital control unit plays a controlling role of the operation of the other. first input first output(fifo) unit is the input data buffer. frame delimiter unit is the role as follows; add to start_bit for frame, start delimiter of master_frame, start delimiter of slave_frame. parallel convert serial unit read data from buffer and convert parallel data to the serial data. manchester encode unit encodes data on the communication. outgoing data crc check sequence generated by crc generation unit. when the data output, multi-selector unit selects the starting delimiter, outgoing data and the crc check code."
"dsdv and gsr uses destination sequence numbers to keep routes up-to-date and loop-free. hsr and zhls are hierarchical routing protocols. fsr reduces the size of tables to be exchanged by maintaining less accurate information about nodes farther away. cgsr and cbrp are cluster-based routing protocol where nodes are grouped into clusters. aodv is an on-demand version of dsdv routing protocol. abr uses the degree of associativity to select routes and a localized broadcast query is initiated when a link goes down. wrp maintains the best-path information to a destination, avoids routing loops during route discovery process, and converges quickly after a link failure. in lar, the route request packets propagate in the request zone only. dsr is a source routing proto-col where the route is in each packet. dsr had higher routing overhead as compared to aodv. zhls and slurp are highly adaptable to changing topology, since only the node id and zone id of the destination is required for routing to occur. they do not use a cluster-head to coordinate data transmission, which means that a single point of failure and performance bottlenecks can be avoided. the zrp routing protocol is designed to increase the scalability of mobile ad hoc networks. the advantage of this protocol is that it maintains strong network connectivity (proactively) within the routing zones while determining remote route (outside the routing zone) quicker than flooding."
routing protocols [cit] often are very vulnerable to node misbehavior. a node dropping all the packets is considered as malicious node or selfish nodes. a malicious node misbehaves because it intends to damage network functioning. a selfish node does so because it wants to save battery life for its own communication by simply not participating in the routing protocol or by not executing the packet forwarding. a malicious node could falsely advertise very attractive routes and thereby convince other nodes to route their messages via that malicious node.
"in hybrid routing protocols, some of the characteristics of proactive protocols and some of the characteristics of reactive protocols are combined into one to get better solution for mobile ad hoc networks. these protocols exploit the hierarchical network architecture and allow the nodes with close proximity to work together to form some sort of backbone, thus increasing scalability and reducing route discovery. nodes within a particular geographical region are said to be within the routing zone of the given node. for routing within this zone, a tabledriven approach is used. for nodes that are located beyond this zone, an on demand approach is used."
"vosviewer identified seven distinct clusters: ellison examined social capital across facebook; kuss studied online/social networking addiction (e.g., gaming addiction), and lenhart focused on teens and technology (e.g., mobile internet use), particularly in the use of social media. other topics include bandura's self-efficacy, use and benefits of twitter by scholars, and personality and social characteristics of facebook users (e.g., ross). 4. cluster 4: prominent social theorists/sociologists who have contributed to social capital theory, structuration theory and modern sociological theory are distinguished members of this cluster. these include bourdieu, coleman, giddens, and habermas. papacharissi has written about a variety of topics including the exploration of factors that predict internet use as well as users' behaviors, identity, sense of community and culture on social media. tufekci has studied privacy and disclosure on social media, as well as other topics, including how social networking sites such as facebook might influence one's decision to participate in protests. 5. cluster 5: in this cluster, there is evidence of the influence of vygotsky's socio cultural learning theory as well as lave and wenger's work on communities of practice. in addition to his work on collaborative learning, kirschner has examined the relationship between facebook and academic performance. likewise, selwyn has explored pedagogical and learning engendered by the use of information and computer technologies (ict). 6. cluster 6: this cluster appears to reflect two broad themes."
"a mobile ad hoc network (manet) is an adaptive, self-configurable, self-organizing, infrastructure-less multi-hop wireless network with unpredictable dynamic topologies [cit] . by adaptive, self-configurable and selforganizing, means an ad hoc network can be formed, merged together or partitioned into separated networks on the fly depending on the networking needs. i.e. a formed network can be deformed on the fly without the need for any system administration. by infrastructureless, means an ad hoc network can be promptly deployed without relying on any existing infrastructure such as base stations for wireless cellular networks. by multihop wireless, means, in an ad hoc network the routes between end users may consists of multi-hop wireless links. in addition, each node in a mobile ad hoc network is capable of moving independently and forwarding packets to other nodes."
"in reviewing the publications gathered for this paper, commonalities have been observed in the myriad aspects of social media chosen for investigation. while many studies focussed their attention on understanding the behaviours of social media users, the others examined entrepreneurial participation and firm behaviour. a number of studies have focussed on the content being posted in online communities, several of which report on the repercussions of some of this content being used as an awareness medium during critical events and tragedies. interesting revelations were made by authors studying the use of social media as a platform to render and/or receive help or support, and its incorporation in the field of healthcare and public administration. value creation and the ill-effects associated with the use of social media at the workplace were also discussed. several studies chose to test previously established hypotheses and models, while others compared traditional media with social media. prior research has also provided insights into how firms have been using social media to market their products and services. these strategies run in parallel with the reviews and recommendations posted by users on social media sites, which have also received considerable attention in the literature. in summary, given that different types of social media platforms are emerging, and different consequences are associated with their use, research in this field will continue to evolve. this is also evidenced by the increased number of publications related to usage and impact over the past five years."
"the routing protocols aim to send messages to some or all of the wireless nodes within a particular geographic region. often the nodes know their exact physical positions in a network, and these protocols use that information for transmitting packets from the source to the destination(s)."
"structure of encoder according to the above design of the task is as follows, digital control unit, parallel convert serial unit, frame delimiter unit, fifo unit, multi-selector unit, crc generation unit, manchester encode unit. fig.2 shows encoder architecture."
"studies within this theme focus on the role of community structure and structural patterns in using social media for marketing purposes. for successful social media implementation, it is important to effectively incorporate social computing with content delivery in the digital content industry with growing user population. most studies identify meaningful conversations with customers as an important attribute of social media marketing. also, identifying specific customer segments across social media site, for instance, members of a forum/group or organization, helps e-marketers to target specific customers based on demographic patterns and similar interests."
"in this context, we first investigate the impact that the proposed retweeting sentiment features have and accordingly answer the second question. because of the space crunch, figures 4(a), 4(b), 4(c), and 4(d) merely illustrate the probability distribution of values of dynamic salton metrics, dynamic interaction frequency, recent mood statistics, and emotion divergence under different retweeting sentiment tendencies (positive, negative, and neutral)."
"packet delivery ratio is the ratio of the number of data packets successfully delivered to the destinations to those generated by the constant bit rate (cbr) sources. routing overhead is the number of control packets produced per mobile node. control packets include route requests, replies and error messages. the routing load specifies the load over communications links for traffic flow."
"the source tree adaptive routing [cit] protocol is based on the link state algorithm. each node maintains a source routing tree, which is a set of links containing the preferred paths to every destinations and broadcasts its source-tree information to its neighbors and builds a partial graph of the topology. when a node has data packets to send to a destination for which no path exists in its source-tree, it originates an update message to all its neighbors indicating the absence of a path .this update message triggers another update message from a neighbor which has a path. after getting this, the node updates its source-tree and then finds path to all nodes in the network. in addition to path breaks, the intermediate nodes are responsible for handling the routing loops."
"the paper proposes a novel mvb design method using sopc technology. the proposed mvb controller was successfully tested at a mixed mvb environment. it combines cpu and mvb controller into one single fpga, so this method can save pcb space and reduce total power consumption. the availability of multi million-gate fpga devices and sopc technology has opened the possibility to create complex single chip systems and can accelerate system performance remarkably."
(3) blend temporal information in user's retweeting sentiment features on the basis of time series of user's contents and network topological information so as to capture dynamic evolution process of information and network structure.
"ad hoc wireless network unicast routing protocols can be further classified into three major categories based on the routing information update mechanism: proactive or table driven, reactive or on-demand, and hybrid routing protocols."
"every node periodically exchange routing table updates to its immediate neighbors. the route updates can be either time-driven or event-driven. two types of route update packets: full dump and incremental packets are used. the full dump packet carries all the available routing information, i.e., the entire routing table to the neighbors and the incremental packet carries only the information changed since the last full dump. for updating the routing information in a node, the update packet with the highest sequence number is used. the incremental update messages are sent more frequently than the full dump packets. the protocol will not scale in large network since a large portion of the network bandwidth is used in the updating procedures."
"in the past two decades, various issues related to social media have been examined in line with the rapid evolution of underlying technologies/applications and their appropriation to enable different types of social media usage. an analysis of 132 [cit] were still examining user-generated content as a new type of online content . however, in the last six years, research in this field has made tremendous progress, not just in terms of its scope, but also in explicating the highs and lows associated with the use of social media. while it is difficult to pinpoint evolution on a yearly basis, it has been possible to identify the major aspects of social media research that have emerged over time. [cit] have been reviewed here. [cit] ) [cit] (2011) (2012), the latter studies [cit] [cit] (2013) (2014) (2015) (2016) . there were 18 [cit] being a popular year for such studies. most of these studies [cit] [cit] (2011) (2012) (2013) (2014) (2015) (2016) . there were 17 [cit] (2012) (2013) (2014) (2015) (2016) evaluating the integration of social media for varied organizational purposes. while some studies investigated the employee side (e.g., innovativeness, retention, and motivation) of social media use [cit], the others discussed the relationship between social enterprise systems and organizational networking [cit] ."
"in this paper, we explored the problem of finding the possible variations and analyzing user's retweeting sentiment tendency in dynamic social networks. firstly, relationship-based features were inferred from users' dynamic salton metrics and dynamic interaction frequency. secondly, along with the number of positive and negative emotional words, we built recent mood statistics and emotion divergence based on time series of users' posts. and then on the basis of naïve bayes theory, we represented models in lower layers from profile-, relationship-, and emotion-based dimension, respectively, followed by designing a multilayer naïve bayes model on constructed models of different dimensions to analyze user's retweeting sentiment tendency. finally, we ran a set of experiments on a real-world dataset to investigate the performance of our model and reported system performances in terms of precision, recall, and 1-measure. in general, the experimental results demonstrate the effectiveness of our proposed framework."
"implementation steps of the controller function as follows, chip power, reset, configure the device address, models and other services, start work. when sending data, mcu reading the corresponding data from traffic memory(tm). write to send buffer, then mcu send instructions of send data to the encoder. when accepting data, the manchester decoder receiving signals from the bus controller, upon decoding, verification and data is written in receive buffer, and notifies the main control unit. mcu read data from the receiving buffer, writes corresponding area of tm. mvbc communicate with the microprocessor through tm which uses dual-port ram."
"the advent of ubiquitous computing and the proliferation of portable computing devices have raised the importance of mobile and wireless networking. wireless networking is an emerging technology that allows users to access information and services electronically, regardless of their geographic position. ad hoc is a latin word, which means \"for this purpose only\". the term \"ad hoc\" tends to imply \"can take different forms\" and \"can be mobile, stand alone, or networked\" [cit] . ad hoc networks have the ability to form \"on the fly\" and dynamically handle the joining or leaving of nodes in the network. mobile nodes are autonomous units that are capable of roaming independently. typical mobile ad hoc wireless nodes are laptops, personal digital assistants, pocket pcs, cellular phones, internet mobile phones, palmtops or any other mobile wireless devices. all of these have the capability and need to exchange information over a wireless medium in a network. mobile ad hoc wireless devices are typically lightweight and battery operated."
"social media allows relationship forming between users from distinct backgrounds, resulting in a tenacious social structure. a prominent output of this structure is the generation of massive amounts of information, offering users exceptional service value proposition. however, a drawback of such information overload is sometimes evident in users' inability to find credible information of use to them at the time of need. social media sites are already so deeply embedded in our daily lives that people rely on them for every need, ranging from daily news and updates on critical events to entertainment, connecting with family and friends, reviews and recommendations on products/ services and places, fulfilment of emotional needs, workplace management, and keeping up with the latest in hashion, to name but a few."
"when we refer to social media, applications such as facebook, whatsapp, twitter, youtube, linkedin, pinterest, and instagram often come to mind. these applications are driven by user-generated content, and are highly influential in a myriad of settings, from purchasing/selling behaviours, entrepreneurship, political issues, to venture capitalism . [cit], facebook enjoys the exalted position of being the market leader of the social media world, with 1.97 billion monthly users [cit] . in addition to posts, social media sites are bombarded with photo and video uploads, and according to the recent numbers, about 400 million snaps a day have been recorded on snapchat, with around 9000 photos being shared every second [cit] . while 50 million businesses are active on facebook business pages, two million businesses are using facebook advertising. apparently, 88% businesses use twitter for marketing purposes [cit] ."
"in future work, we will employ crowd sourcing technology to add more context information to our method to ameliorate its performance as well as increase its online application scope. furthermore, we will speculate on what directions can be undertaken to ameliorate its performance with respect to time complexity."
average jitter measures the packet delay variation. it is calculated as the average of the difference of the inter arrival time between subsequently received packets.
"the tcn bus was standardized in the plenary session of the iec technical committee 9, [cit], which resulted to the iec 61375 standard. the ieee vehicular society also standardized the tcn bus, which resulted to the ieee 1473-t standard. the main goal of the tcn is to set a single train equipment specification that is compliant with the requirements/needs of different railway service providers and to warrant the interconnection of any number of different vehicles/units from possibly different manufacturers in the same train. mvb is one component of tcn. the mvb is the vehicle bus specified to connect standard equipment, located in the same vehicle, or in different vehicles in the tcn. the mvb can address up to 4095 devices, of which 256 are stations capable of participating in message communication. in order to allow these devices to communicate with each other, a common communication interface card is designed which is independent of the chosen physical layer and functions associated with each device. the mvbc is an asic chip which realizes the physical and major parts of the link layer protocol of the mvb."
"this paper discusses the findings of 132 publications contributingtotheliteratureonsocialmedia.multipleemergentthemes in this body of literature have been identified to enhance understanding of the advances in social media research. by building on empirical findings of previous social media research, many new studies have been successful in theorizing the nature of most social media platforms. user-generated content allows collective understanding, which is a massive machine-human knowledge processing function capable of managing chaotic volumes of information. some key conclusions relevant to stakeholders,includingresearchers,havebeenidentifiedhere."
"furthermore, the emergence of microblogging has broken the mode of transmission: once a user posts a microblog, other users can retweet it and add more contents via 140 words text box which makes further development and enrichment of information during forwarding process. thanks to resonance of information, retweeting content, as context information of microblogging, contains users' views and emotions to express approval or opposition towards a certain microblog. besides, exploring on retweeting sentiment tendency can make enterprises and governments better understand users' opinions on products, stocks, current hot issues, hit movies, hate to someone, and so forth. as stated, retweeting sentiment tendency analysis is of great significance for public opinion monitoring. our work on predicting user's retweeting sentiment tendency is motivated by its broad application prospect."
"& social media technologies are no longer perceived just as platforms for socialization and congregation, but are being acknowledged for their ability to encourage aggregation."
"layer. finally, since the calculation of user's retweeting sentiment tendency (denoted as st), user's profile (up), relationship (ur), and emotion (ue) all conforms to naïve bayes model, so we adopt a multilayer naïve bayes model to analyze user's retweeting sentiment tendency. in this paper, determined by user's relationship and emotion, user's retweeting sentiment tendency can be regarded as root node of naïve bayes model; user's relationship and emotion can be regarded as leaf nodes of naïve bayes model. thus, user's retweeting sentiment tendency is calculated as follows:"
"where upn(, ) and unn(, ) represent the number of positive emotional words and the number of negative emotional words user used on which are included in hownet knowledge. thus, user 's recent mood statistics on the flow of time slices [0, ] is calculated with"
"in reviewing the 132 publications on social media and social networking, it was observed that many studies relied primarily on social exchange theory, network theory and organization theory. table 3, shown below, lists other theories that have been used by at least two publications. there were several other theories that were used by at least once, including social role theory, game theory, structural holes theory, management and commitment theories, institutional theory, deterrence and mitigation theories, and self determination and self categorization theories. it is noteworthy that dominant is adoption theories such as unified theory of acceptance and use of technology [cit] b, c; [cit], technology acceptance model [cit] and innovation diffusion theory are less widely utilised."
"the first is a range of topics related to medical internet research, broadly referred to as e-health (eysenbach) or online health (duggan). themes in this category include electronic support groups and health in virtual communities (eysenbach), and policies and healthcare associated with social media, and professionals among medical students and physicians in the use of social media (chretien, greysen). the second main thematic area in this cluster deals with scholarship on social media, scholarly communication, and metrics for evaluating impact of articles on the web (e.g., weller, bormann, priem). 7. cluster 7: the dominant theme here is the nature and content of communication. in particular, scholars in this cluster have focused on communication and response in the face of crises (coombs), including image restoration after a controversy (benoit), analysis and reliability of content (krippendorff), and the use of social media sites such as facebook and twitter by government agencies and nonprofit organizations to engage stakeholders (waters)."
"studies reviewed here discuss a social contagion effect of risks associated with social media use. [cit] (2015) (2016) suggest educating audiences about the threats associated with the extent of personal information being disclosed on social media sites. they recommend government agencies to keep the users informed, and the social media sites to control some of their security features. it is necessary to define and control privacy settings across these many existing social networks."
"zrp is suitable for the networks with large span and diverse mobility patterns. the advantage of this protocol is that it has significantly reduced the amount of communication overhead when compared to pure proactive protocols. it also has reduced the delays associated with pure reactive protocols such as dsr, by allowing routes to be discovered faster. this is because, to determine a route to a node outside the routing zone, the routing only has to travel to a node which lies on the boundaries (edge of the routing zone) of the required destination. since the boundary node would proactively maintain routes to the destination (i.e. the boundary nodes can complete the route from the source to the destination by sending a reply back to the source with the required routing address)."
"the difference between dsr and aodv is that in dsr, each packet carries full routing information, whereas in aodv, the packets carry the destination address. this means that aodv has potentially less routing overheads than dsr. the other difference is that the route replies in dsr carry the address of every node along the route, whereas in aodv, the route replies only carry the destination ip address and the sequence number."
"vosviewer was used to analyze terms (i.e., words) in the titles and abstracts of our corpus to obtain a two-dimensional map showing proximities of words that are likely to be related based on their co-occurrences. specifically, vosviewer relies on the apache opennlp toolkit to identify noun phrases, and then compares their overall co-occurrence distribution with their distribution across other noun phrases to compute a relevance score [cit] . the intuition is that frequently co-occurring noun phrases with high relevance are likely to unravel a topic or theme that is latent in the corpus. the term map from vosviewer is shown in fig. 2 . only terms that occurred 50 times or more were included. furthermore, relevance scores computed by vosviewer for every term were used to select the top 80% that met the threshold. vosviewer identified five clusters here. it is evident from the clusters that research on social media has dealt with a broad range of topics, including but not restricted to diffusion of information and opinions, spread of diseases (e.g., influenza), identification of social and emotional health concerns and attendant interventions to deal with them, social media as an influence, the use of social media for marketing purposes, and the implications of social media as a tool for pedagogy (i.e., teaching and learning) and medical practice. these have been summarized in table 1 ."
where ipn( ) and inn( ) denote the number of positive emotional words and the number of negative emotional words used in microblog which are included in hownet knowledge.
"in this article, the classifications of routing protocols for ad hoc wireless networks were discussed. in proactive protocols, each node maintains network connectivity and up-to-date routing information to all the nodes in the network. in reactive protocols, a node finds the route to a destination when it desires to send packets to the destination. in hybrid routing protocols, some of the characteristics of proactive and some of the characteristics of reactive are combined, by maintaining intra-zone information proactively and inter-zone information reactively, into one to get better solution for mobile ad hoc networks."
the rest of the paper is organized as follows: section 2 describes the related work; dynamic retweeting sentiment features are depicted in section 3; section 4 defines the method we propose; details of the experimental results and dataset which is used in this study are given in section 5. finally conclusion appears in section 6.
"since it cannot make a comprehensive analysis on user's retweeting sentiment tendency based on specific types of features only, consequently, in this paper, we synthesize profile-, relationship-, and emotion-based features so as to achieve higher accuracy in retweeting sentiment tendency analysis."
"with the rapid growth of user-generated data on the web, people usually use microblogging which has become a novel social media [cit] for expressing their opinion. therefore, a necessity of analyzing and understanding these online generated data/opinions has arisen [cit] . mining emotional information in users' contents may contribute to analyzing the relationship between social economy change and emotion change expressed by the public [cit], measuring strength of public happiness [cit], detecting current trend of stock market [cit], predicting results of presidential election [cit], and modeling for opinion mining [cit] . hence, analyzing sentiment tendency of microblogging has gradually become a hot research topic."
"where ue [0, ] ( ) represents user 's latent mood statistics on the flow of time slices [0, ] and ie( ) represents emotion statistics expressed in microblog which can be calculated as"
"businesses looking to monetize online content and social search rely heavily on substantial understanding of consumer behaviour in terms of their interaction and participation in social settings ). as consumers gain access to social platforms that offer free content consumption without an obligatory payment, the relationship between sampling and sales becomes all the more important . there is much research supporting the belief that online word of mouth has a critical role to play in a firm's overall performance, and introducing a pay-wall (for previously free content) can significantly reduce the volume of word of mouth for popular content in comparison to niche content . determining consumers' social influence in an online community is of critical interest to managers, who seek to gain some leverage from the potential of social media . some researchers find it difficult to distinguish social influence from users' self selection preferences. from an analysis point of view, it then becomes necessary to separate factors affecting user membership in a social network from various types of social influence ."
"the disadvantage of zrp is that for large values of routing zone, the protocol can behave like a pure proactive protocol, while for small values it behaves like a reactive protocol."
"encoder engages in manchester encoding of master_frame and slave_frame, and generation and sending crc check sequence. above-mentioned work is controlled by mcu. the specific tasks of the module are as follows: (1) the parallel data are extracted from the buffer, (2) the parallel input data transform into serial data suitable for mvb, (3) add to delimiter for ready to send data, (4) according to the length of ready to send data, add one or more 8-bit crc check_sequence. (5)manchester encode for data to be send in order to improve communication quality."
"in proactive routing protocols, also known as tabledriven routing protocols, each node maintains one or more tables that contain consistent and up-to-date routing information to every other node in the network. the routing information is usually kept in a number of different tables. proactive protocols continuously learn the global topology of the network by exchanging topological information among the network nodes. when the network topology changes, the nodes propagate update messages and the topology change information is distributed across the network. if the network topology changes too frequently, the cost of maintaining the network might be very high. each node continuously evaluates routes to all reachable nodes. the overhead to maintain up-to-date network topology information is high."
"star will scale well in large networks since it has significantly reduced the amount of routing overhead disseminated into the network by using a least overhead routing approach (lora) to exchange routing information. however, this protocol may have significant memory and processing overheads in large and highly mobile networks, because each node is required to maintain a partial topology graph of the network (it is determined from the source tree reported by its neighbors), which changes frequently as the neighbors keep reporting different source trees."
"in the same way, the calculation of user's emotion (denoted as ue) also tallies with naïve bayes model which is determined by the number of positive emotional words (denoted as pen), the number of negative emotional words (denoted as nen), recent mood statistics (denoted as rms), and emotion divergence (denoted as ed); as a consequence, ue can be viewed as root node of naïve bayes model; pen, nen, rms, and ed can be viewed as leaf nodes of naïve bayes model."
"the main objective of ad hoc routing protocols is how to deliver data packets among nodes efficiently without predetermined topology or centralized control. expected properties of manet routing protocols are [cit] : a routing protocol for manet should be distributed in manner in order to increase its reliability, the routing protocol should assume routes as unidirectional links, the routing protocol should be power-efficient, the routing protocol should consider its security, and a routing protocol should be aware of quality of service (qos). based on the method of delivery of data packets from the source to destination, classification of the manet routing protocols could be done as unicast, multicast or geocast routing protocols [cit] . unicast routing protocols: the routing protocols that consider sending information packets to a single destination from a single source."
"in this section, we conduct experiments to assess the effectiveness of the proposed framework mlnbrst. through the experiments, we aim to answer the following two questions:"
"the main functions of the decoding module include manchester decoding and error detection of received data, then decoded without error serial data convert to parallel data which stored in the buffer zone and supply of top applications. decode is the anti-process of encode which will have decoded data encoded, it is also the original appearance of data. the task of error detection mainly includes the error detection of crc check code, error of frame length and error of manchester skipping, etc. decoder includes decoding control unit, the detection of start bit unit, delimiter detection unit, manchester decode unit, serial convert parallel unit, fifo unit, crc check unit and the length check unit. fig.3 shows decoder architecture."
"state machine is responsible for the analysis of the telegram and the corresponding control. timer with the state machines perform a variety of timing tasks, while ensure the successful completion of state machines to switch among various states and the corresponding operation."
"& in reviewing the 132 publications on social media and social networking, it was observed that most studies used social exchange theory, network theory and organization theory to support their studies. & facebook, online communities, and twitter are the three most popular networks targeted by publications in the field of social media research. & [cit] were still reporting user-generated content as a new type of online content. however, the last six years have seen tremendous scholarly progression in discussing the many applications of social networking, highlighting the highs and lows associated with its use. & majority of the publications reviewed in this study are focussed on behavioural side of social media, reviews, and integration of social media for marketing and organizational purposes. capabilities, with a focussed group of studies recognizing its effectiveness during natural disasters and critical events. & almost all publications studying information sharing during natural disasters and critical events focus on twitter data. & [cit] ."
"in recent years, with the popularization of microblogging, sentiment analysis of microblogging has become one of the hot research topics [cit] . existing microblogging sentiment analysis algorithms can be roughly categorized into two groups: emotional dictionary-based methods and machine learning methods."
"as outlined in the previous section, social media research is evolving at a fast pace. in reviewing the shortlisted articles, various themes were identified based on the similarities observed across the issues addressed in social media research."
"tmc is composed of sink time manager、master state machine、word access state machine and selector. master state machine controls all of tm and generates control sequences. each word can be accessed by controlling word access state machine. when word being accessed completely, the word will automatically add an access counter, up access to all word. word access state machine is formed by the five state, idle state, acc_begin state, wait state, w_tmrdy state and data_valid state. transition among states is shown in fig.5 shows transition among states."
total packets received is the number of packets received by the tcp sink at the final destination and the number of packets generated by the traffic sources.
"the simulation study has been done using the network simulator qualnet 4.0 for performance comparison of the protocols: dsdv, aodv, fsr, lar, olsr, star and zrp. the improvements shown in lar are gradually increasing than others. hence, it can be concluded that lar is the best among the studied routing protocols."
"before modeling ue, given that pen, nen, rms, and ed are continuous attributes; in order to calculate conditional probability of them, we discretize them by using discrete intervals to represent them: pen is mapped to three levels (namely, few, medium, and many); nen is mapped to three levels (namely, few, medium, and many); rms is mapped to three levels (namely, low, medium, and high); and ed is mapped to three levels (namely, small, medium, and large). and the probability that user's emotion is equal to a certain discrete value is calculated as below:"
(5) evaluate mlnbrst on real-world sina microblogging dataset and elaborate the importance of different retweeting sentiment features and temporal information on user's retweeting sentiment tendency analysis.
"it must be noted that the topics are broad and don't reveal the nuances of research areas embodied in the abstracts examined in this study. the next sub-section presents the results of topic modeling, which has the potential to unravel more focused themes embodied in the large corpus that we analyzed."
"however, previous sentiment tendency analysis methods [cit], which only focused on emotion of contents rather than users' individual emotion, attributes, social correlations, and 2 computational intelligence and neuroscience dynamic nature of network, cannot make a comprehensive analysis on user's retweeting sentiment tendency. hence, in this paper, we propose a multilayer naïve bayes model for analyzing user's retweeting sentiment tendency towards a microblog (denoted as mlnbrst), and our main contributions are summarized next."
(4) build a multilayer naïve bayes model on account of naïve bayes models from different dimensions to complete user's retweeting sentiment tendency analysis in a more fine-grained perspective.
"advantages and disadvantages of proactive, reactive and hybrid approaches are shown in table 1 and overall comparison of all unicast routing protocols are shown in table 2 ."
"arbitrator is used to solve the conflict when cpu and mvbc access tm at the same time, arbitrator at the same time allows only one access tm."
"from figure 3, it can be found that the proposed approach shows the best results from the point of view of precision, recall, and 1-measure. maximum entropy method achieves the worst results because it strongly relies on corpus. since support vector machine is only applicable to a small-size training dataset, therefore, naïve bayes is more suitable for sentiment classification than support vector machine in terms of microblogging which is in line with [cit] . adaptive recursive neural network method only can achieve better precision via a complete dataset. additionally, given context of retweeting content, our proposed method stratifies different factors according to the correlations between them via a multilayer naïve bayes model; consequently, it performs better than naïve bayes and nbsvm method. moreover, we can obtain a significant improvement on performance (+10.7% in terms of precision, 22.3% in terms of recall, and +16.9% in terms of 1-measure) compared with [cit] which leveraged a naïve bayes classifier to analyze user's sentiment via 95 most frequently used emoticons in chinese tweets. besides, we achieve better results (+5.8% in terms of precision, +12.1% in terms of recall, and +9.9% in terms of 1-measure) compared with [cit] which leveraged svr (support vector regression) to classify emotions in chinese tweets on the basis of common social network characteristics and other carefully generalized linguistic patterns."
"from the above, it can be found that the proposed factors can be used as a good indicator of user's retweeting sentiment tendency. however, although social relationship played an important role in individual emotion which is in keeping with fowler and christakis's work [cit], it can only distinguish between neutral and other sentiment tendencies, while emotion-based features are merely with good discriminability on positive and negative sentiment polarity. hence, comprehensive considering on context information of retweeting content is necessary."
"the metrics that are used to evaluate the performance of the routing protocols are: throughput, average end-to-end delay, average jitter, total packets received, packet delivery ratio and routing overhead. throughput is the average rate of successful message/ packets delivery over a communication channel. i.e. throughput is the measure of how fast we can actually send the packets through network. the throughput is usually measured in bits per second (bit/s or bps), and sometimes in data packets per second or data packets per time slot."
"mobile hoc network (manet) is built on the fly where a number of mobile nodes work in cooperation without the engagement of any centralized access point or any fixed infrastructure. the nodes in the network are free to move independently in any direction. node mobility causes route changes. the nodes themselves are responsible for dynamically discovering other nodes to communicate. when a node wants to communicate with a node outside its transmission range, a multi-hop routing strategy is used which involves some intermediate nodes. the network's wireless topology changes frequently and randomly at unpredictable times. every node in ad hoc wireless network acts as a router that discovers and maintains routes in the network. hence, the primary challenge is to establish a correct and efficient route between a pair of nodes and to ensure the correct and timely delivery of packets. route construction should be done with a minimum of overhead and bandwidth consumption. various protocols-proactive, reactive and hybrid-have been proposed and widely evaluated for efficient routing of packets in the literature [cit] ."
"academics and practitioners have explored and examined the many sides of social media over the past years. organizations engage in social media mostly with the aim of obtaining feedback from stakeholders [cit] . consumer reviews are another big part of social media, bringing issues of information quality, credibility, and authenticity to the forefront. to a large extent, online communities have been successful in bringing together people with similar interests and goals, making the concept of micro blogging very popular. while most messages exchanged on social media sites are personal statuses or updates on current affairs, some posts are support seeking, where people are looking for assistance and help. interestingly, these have been recognized as socially exhausting posts that engender social overload, causing other members to experience negative behavioural and psychological consequences, because they feel compelled to respond ."
"the disadvantage of lar protocol is that each node is required to carry a gps. another disadvantage is, especially for the first method, that protocols may behave similar to flooding protocols (e.g., dsr and aodv) in highly mobile networks."
"open access this article is distributed under the terms of the creative comm ons attribution 4.0 international license (http:// creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the creative commons license, and indicate if changes were made."
"the improvements shown in lar are gradually increasing than others. hence, it can be concluded that lar is the best among the studied routing protocols."
"the advantage of fsr is that even though a node does not have accurate information about a destination, as the packet moves closer to the destination, more correct information about the route to the destination becomes available. fsr exhibits a better scalability concerning the network size compared to others as the overhead is controlled in this scheme."
"publications have also shown interest in investigating the effects of user-generated content on entrepreneurial behaviour. [cit] find that discourse in both traditional and user-generated media has a notable influence on it firm founding rates. [cit] reveal that higher usage of twitter, alongside follower numbers and retweets result in higher levels of under pricing for initial public offerings (ipo). [cit] find that online organizational networking has many unbalanced multiplex relationships, mostly comprising of weak ties and temporal change. they attribute the uneven user contribution in social networking sites to discourse drivers and information retrievers. [cit] identify collaboration, broadcast, dialogue, sociability, and knowledge management as the social media types that serve varied organizational purposes. [cit] study facebook to conclude that social media networks can exercise management not only by excluding participants, but also by driving softer changes in incentive/reward systems."
"mcu is a core part of mvbc almost control all the other modules work, play a role in overall control. mcu is not directly involved in concrete action, but by controlling the other modules to complete the appropriate action treatment. mcu is composed of the distribution of master frame, port pretreatment, port post-processing, transport data unit, command processing unit. fig.6 shows mcu architecture. fig.7, f_code is 0000 which means 16 bit process data. the corresponding response frame is 16-bit. the last 12 bit of master frame is logical address of process data. data_out is main frame of process data which includes a frame header, frame end, and crc checksum. send_end is high that means sending master frame of process data is over. the delimiter and checkout value is successfully added. the simulation result of salve frame is shown in fig.8 ."
"therefore, the protocol works based on the mechanisms of: neighbors-sensing based on periodic exchange of hello messages, efficient flooding of control traffic using the concept of multipoint relays, and computation of an optimal route using the shortest-path algorithm. this protocol does not notify the source immediately after detecting a broken link and source node comes to know that route is broken when the intermediate node broadcasts its next packet. the proposed protocol is best suitable for large and dense ad hoc networks."
"average end-to-end delay is the delay experienced by a packet from the time it was sent by a source till the time it was received successfully at the destination. i.e., the end-to-end delay is the time a data packet is received by the destination minus the time the data packet is generated by the source. average end-to-end delay includes all possible delays caused by buffering during route discovery latency, queuing at the interface queue, retransmission delays at the mac, and propagation and transfer times of data packets."
"as depicted in figure 4 (a), if there is a bigger dynamic salton metrics between two users, it is more easier to retweet casually which may lead to either positive emotion or negative emotion being included in retweeting contents. in figure 4(b), users who interact frequently may be less possible to retweet in neutral emotion tendency with each other; instead, there may be more likely to express support or opposition to each other. and in figure 4 (c), users will have higher probability to retweet with positive emotion tendency if they are in high spirits latently, and vice versa."
"with the sopc technology, we have created a novel design and implementation method of the mvb controller. this method can great reduce the chip numbers and card size and power consumption relative to the traditional method. and the design is based on a soft ip core, so it can be easily integrated into other applications."
"in order to gain a deeper understanding of social media, we analyzed relevant abstracts that were downloaded from the web of science (wos) database. our search terms 1 yielded a total of 13,177 records, out of which 12,597 unique abstracts were obtained. the analysis of these records was undertaken in two steps. first, we used vosviewer [cit] to perform a co-citation analysis of first authors in the downloaded corpus. vosviewer allows visualization of similarities in publications and authors through an examination of bibliometric networks. furthermore, we used vosviewer to analyze words derived from titles and abstracts. second, we used latent dirichlet allocation (lda) [cit] to extract key thematic areas latent in the literature on social media. further details about these analyses and results are presented in section 3."
"in summary, the results in sections 5.2.1 and 5.2.2 suggest that all improvement is significant. with the help of multilayer naïve bayes model based on integration of multidimensional features, the proposed framework, mlnbrst, gains significant improvement over representative different feature-based methods and baseline methods, which answers the first question."
"the reviewed publications were also analyzed to determine the nature of the social network that were studied. precisely 46 websites emerged, with facebook, online communities, twitter, blogs and youtube being most frequently targeted. networks analysed by at least two or more studies have been identified in table 5 . the other networks that received attention from the reviewed publications include ebay, flickr, flixster, gtalk, microsoft, msn space, patientslikeme, new york times, tripadvisor.com, and boxofficemojo.com. studies also focussed on websites related to online news, q&a websites, discussion groups and forums, online radio and television, and medical sites such as webmd.com."
"limited number of studies has been recorded for this theme. these studies are fairly recent suggesting a new emerging trend, where health/support based communities are being formed. the expanse of such communities seems to be largely dependent on the information processing capacity and the range of social ties that the members of such networks can handle. using social media to bring together people with similar health conditions suggests that informational and social support can have varying influence on patient health."
except one study [cit] (2016) . these studies suggest the use of social media for increasing public engagement and transparency. most of these studies used technical frameworks and modelling techniques to identify communication clusters and structures to derive insights relevant to open government and political campaigns.
"the fsr protocol is an improvement of global state routing (gsr). the large size of update messages in gsr wastes a considerable amount of network bandwidth. in fsr, each update message does not contain information about all nodes. instead, it reduces the size of the update messages by exchanging information about closer nodes more frequently than it does about farther nodes, which lie outside the fisheye scope. the scope is defined in terms of the nodes that can be reached in a certain number of hops. so, each node gets accurate information about neighbors and accuracy of information decreases as the distance from node increases."
"the advantage of aodv is that it is adaptable to highly dynamic networks. however, node may experience large delays during route construction, and link failure may initiate another route discovery, which introduces extra delays and consumes more bandwidth as the size of the network increases."
"since people tend to post microblogs which express their experiences or views in order to realize desire of self-expression, consequently, there will be quite a lot of emotional words or expressions in their contents. thus, along with the number of emotional words, we employ recent mood statistics and emotion divergence as emotionbased features."
"some studies suggest that there is a negative stigma associated with the use of social media in the workplace. [cit] analyze three employee layers in an organization to find that new hires (users of social media sites) showed improved morale and employee engagement, some middle managers (non users) were frustrated and experienced isolation, while the senior execs were wary of social media use. [cit] suggest that social media has the potential to build employees' social capital to positively influence their knowledge integration. [cit] find that conflicts can stem between workplace values and the values these employees ascribe to social media."
"the findings on the use of social media in emergencies suggests that a general user response in an online community is very different from that during a crisis, as those responses then become more reflexive. it has been observed that in times of crisis, lack of information sources coupled with too many situation reports being shared by the users of a social media platform can precipitate a rumour mill. it thus becomes incumbent on emergency responders to release reliable information, whilst trying to control collective anxiety in the community, to suppress the rumour threads . furthermore, security concerns are increasingly common with involuntary online exposure on social media, and research on this subject suggests that information dissemination with network commonality affects privacy invasion and user bonding [cit] . it has been learnt that an individual's or firm's decision to withhold information in the interest of privacy can have both positive and negative effects on their utility ."
"the location aided routing (lar) [cit] protocol is a reactive unicast routing scheme. in this, a source node estimates the current location range of the destination based on information of the last reported location and the mobility pattern of the destination. in this, an expected zone is defined as a region that is expected to hold the current location of the destination node. during route discovery procedure, the route request flooding is limited to a request zone, which contains the expected zone and location of the sender node. lar decreases overhead of the route discovery by using the location information. it limits the search to a smaller request zone, causing significant reduction of the number of routing messages."
"2 isf enjoys the reputation of a high quality journal across continents. for example, a journal quality ranking by chartered association of business schools, uk, has given it a three star (high ranking) quality rating, while journal ranking by the australian business deans council (abdc) has rated it as an 'a' class journal (the second highest quality journal category after a*, which is reserved for premier publications). in light of these observations, it was deemed appropriate to consider articles from isf along with the aforementioned eight journals."
"common neighbors metrics assumed that similarity between users was proportional to the number of their common neighbors [cit] . salton metrics introduces users' degree compared to common neighbors metrics to measure user's network topological structure. since nonreciprocal friendships, which may reflect moderately valued friendship ties [cit], are more important than reciprocal friends, hence, we make an improvement on traditional salton metrics according to directivity of link. besides, understanding dynamic structure of online social networks plays an important role in the development of retweeting sentiment tendency analyzing algorithms. thus, given the dynamic nature of social network, we consider network as a dynamic flow of time slices (one time slice stands for one day and the older the time slice is, the lower is its importance, so does its weight), and dynamic salton metrics between user and user v on the th time slice is defined as follows:"
"where (, ) and (v, ) stand for the number of posts of user and user v on, respectively, and (, v, ) stands for the number of interactions between user and user v on, where an interaction is defined as user retweeting a microblog of user v or user v retweeting a microblog of user . thus, dynamic interaction frequency between user and user v on the flow of time slices [0, ] is calculated as"
"we draw the following observations: removing either user's relationship-based or emotion-based features may lower the model's prediction abilities obviously. additionally, emotion-based features contribute more to analyzing user's retweeting sentiment polarity than profile-and relationshipbased features since a plenty of positive or negative emotional words in retweeting contents provide a good guarantee for accuracy of emotion predicting. besides, the system improves its performance by shifting the emphasis towards merging profile-, relationship-, and content-based features from which we found that it is difficult to predict user's retweeting emotion tendency with only its specific types of features and it is important to fuse multidimensional features reasonably."
"social media platforms have essentially redefined the ways in which people choose to communicate and collaborate. an online community is a socio-technological space where a sense of communal identity drives engagement, which, in turn, enhances satisfaction . intriguingly, social media are facilitating the emergence of virtual knowledge communities and self help networks. these web-based arrangements allow medical practice and research to access patient experience on a daily basis, which was not possible earlier. however, since research in this area is still in its early stages, it is difficult to assess the social complexity involved (e.g., stability of a networking platform that brings together patients with medical experts) in the process [cit] ."
"a specific line of research focuses on consumers, who substantially rely on online reviews before making any purchase decision. the research papers reviewed in this study exhibit diversity in studying authenticity of reviews for travel sites, social bookmarking and review sites, movie ratings, car manufacturing, and social media check-ins. studies concur that there has been an exponential increase in the number of fake reviews, which is severely damaging the credibility of online reviews and putting business values at risk . some studies have also empirically identified consumers' social media participation as a key metric contributing to the profitability of a business . there evidently exists a direct correlation between consumer engagement on social media sites and their shopping intentions, which makes the issue of legitimate reviews all the more important for businesses and consumers. although some studies have proposed models and algorithms that claim to filter authentic reviews from the rest, there is no single and straightforward solution reported yet that can fully combat this problem."
"the mvb controller contains the encoders/decoders and the logic to control the traffic memory, which decodes the incoming frames and addresses the traffic memory accordingly. this paper describes the design of mvbc which is divided into seven modules that is encoder, decoder, telegram analysis unit(tau), configuration memory, traffic memory controller(tmc), arbitrator and microprocessor control unit(mcu). fig.1 shows mvbc architecture."
"tau is mainly responsible for the analysis of telegram. the received packet is passed to the control unit results mcu, then according to the feedback control unit to the corresponding treatment. there are eight kinds of situations. tau is composed of timer and state machine. fig. 4 shows tau architecture."
"the important characteristics of ad hoc wireless networks [cit] are: dynamic topologies, low bandwidth, limited battery power, decentralized control, weak physical protection, etc. dynamic topologies: the nodes in ad hoc wireless networks are free to move independently in any direction. the network topology changes randomly at unpredictable times and primarily consists of bidirectional links. low bandwidth: these networks have lower capacity and shorter transmission range than fixed infrastructure networks. the throughput of wireless communication is lesser than wired communication because of the effect of the multiple access, fading, noise, and interference conditions. limited battery power: the nodes or hosts operate on small batteries and other exhaustible means of energy. so, energy conservation is the most important design optimization criteria. decentralized control: due to unreliable links, the working of ad hoc wireless network depends upon cooperation of participating nodes. thus, implementation of any protocol that involves a centralized authority or administrator becomes difficult. weak physical protection: nodes in ad hoc wireless networks are usually compact, soft, and hand-held in nature. today, portable devices like mobile phones or personal digital assistants (pdas) are getting smaller and smaller. they could get damaged or lost or stolen easily and misused by an adversary."
"multiple heterogeneous features were extracted and concatenated as a visual feature vector for each image. taking each type of homogeneous features as a group, we sequentially numbered the feature groups in the following sections. details of features, dimensionality, and also the sequence numbers for each of the three datasets are listed as follows:"
"the interaction or switching of local pollination and global pollination can be controlled by a switch probability p [cit], with a slight bias toward local pollination. in order to formulate updating formulas, we have to convert the aforementioned rules into updating equations. for example, in the global pollination step, flower pollen gametes are carried by pollinators such as insects, and pollen can travel over a long distance because insects can often fly and move in a much longer range [cit] .therefore, rule 1 and flower constancy can be represented mathematically as:"
"our future work: (1) in order to improve the accuracy of classification, asrs or ocrs ability of correction will be improved; (2) using model in video scene classification, extracting semantic information from the dialogue and background; (3) with more other features, we derive novel lda model to process two different type features parallelly."
"in order to uncover the different mechanism of heterogeneous feature selection for group lasso, lasso and mtbgs, we output the coefficient vectors respectively from the three algorithms after 10-round repetition, and investigate the results of group selection for each round. the results of heterogeneous feature selection for 2 labels in figure 3 are depicted in figure 5 and figure 6 . we observe that though group lasso and mtbgs can both select groups of features, the coefficient values of within groups are obviously different. mtbgs successfully induces sparsity of coefficient values, i.e., shrink to zeros, within groups, which is like lasso. on the contrary, group lasso intends to include all the coefficients into the model. comparing the group selection results between group lasso and mtbgs, mtbgs produces more consistent group selection with respect to the 10-round repetition training process. moreover, considering the heterogeneous feature selection for the label \"bird\", mt-bgs adaptively includes more groups of heterogeneous features into the model. since the low-level features in images with label \"bird\" are more variant."
"where    , i i f x g x, are supposed to be continuous functions, s is compact. fractional programming problem sof the form (1) arise reality whenever rates such as the ratios (profit/revenue), (profit/time), (-waste of raw material/quantity of used raw material), are to be maximized often these problems are linear concave-convex fractional programming. fractional programmingproblems is a nonlinear programming method that has known increasing exposure recently and its importance, in solving concrete problems, is steadily increasing. furthermore, nonlinear optimization models describe practical problems much better than the linear optimization, with many assumptions, does. the fractional programming problems are particularly useful in the solution of economic problems in which different activities use certain resources in different proportions."
"rule 1: biotic and cross-pollination can be considered as a process of global pollination process, and pollen-carrying pollinators move in a way that obeys le'vy flights. rule 2: for local pollination, a biotic and self-pollination are used. rule 3: pollinators such as insects can develop flower constancy, which is equivalent to a reproduction probability that is proportional to the similarity of two flowers involved."
"to give background, web based training is a computer-based educational service that uses internet to support distance learning. this technology is becoming popular for providing university courses and business training so that it allows students to learn wherever they are. therefore, how to mine the related knowledge and corresponding multimedia from the internet is a key for online-learning. in this paper, we propose a system classifying videos into categories, so users can efficiently find their interesting multimedia sources from the internet. furthermore, users can publish their notes or comments for some lectures and share with others through the network. this makes a community for online-learning, which is interactive and interesting. therefore, the contentbased multimedia information retrieval is a critical issue for our system."
"at random-based optimization algorithms, the methods using chaotic variables instead of random variables are called chaotic optimization algorithms (coa) [cit] . in these algorithms, due to the non-repetition and ergodicity of chaos, it can carry out overall searches at higher speeds than stochastic searches that depend on probabilities [cit] . to achieve this issue, herein onedimensional, noninvertible maps are utilized to generate chaotic sets. we will illustrate some of well-known one-dimensional maps as:"
"in tab. 4, we can find that the words in topic 1 mostly mean describing probability distribution, the words in topic 2 focus on the feature of model or characterizing the objects, and topic 3 mainly involves in analyzing experiment results and performance of models in science field. then, we can prove multimodal lda model, processing with two variables with gibbs sampler, is useful. based on these topics and through analyzing the meaning of every topic, we can define topics and may be one topic can be given two or three words for indexing. users can browse videos through searching these words, and scanning lecture videos directly."
"the optimization problem is expressed as: the constraint ensures that the error between obtained gear ratio and the desired gear ratio is not more than the 50% of the desired gear ratio. the comparison resulted obtained by the cfpa, ca, fa and pso algorithms are given in table ( 3), the comparison in terms of the best, error, mean, standard deviation values, these values where obtained out of 20 independent runs. the result indicates a better achievement for cfpa, with an objective function value of 2.7 e-012. the comparison result of minimum-weight and the statistical values of the best solution obtained by cfpa, cs,fa and pso are given in table ( 2). accuracy performance is measured in terms of the best, mean, and standard deviation values of the solutions obtained by 20 independent runs. the best minimumweight in this study is 7.008391 with thickness 1.05 cm."
"in our experiment, we choose 100 lecture videos from http://videolectures.net to have an experiment, and these 100 videos have different titles which focus on beyesian process or dirichlet process in machine learning, which means the words relating to probability knowledge, characterizing method and describing models are involved. then, we segment them into many shots based on time, for example, we segment a 20-mins video into 20 shots that each shot has 1 minute. therefore, we totally get 1640 shots and 11000 key frames from these 100 videos."
"since there are high-dimensional heterogeneous features in images, it is very natural to perform feature selection at group level (inter heterogeneous feature sets) first and then identify subgroup within a homogeneous feature set. the motivation of structural grouping penalty in mtbgs is to set most of coefficients in vectors to zero and only keep in the model limited number of coefficients, whose corresponding groups of features are discriminative to the th label. that is to say, only discriminative subgroups of homogeneous features are selected out."
"ocr extracts semantic information on slides through scanning and translating image of handwritten, typewritten or printed text into machine-encoded text. an ocr system recognizes the fixed static shape of the characters and uses a dictionary to match recognized words, so that it can increase recognition rates. through this technology, the semantic information on slides can be extracted."
"this leads to an algorithm that cycles through the groups, which is a blockwise coordinate descent procedure [cit] . the criterion (3) is convex and separable so that the blockwise coordinate descent at group level and piecewise coordinate descent within group for individual features can be used for optimization."
"we now derive the algorithm for solving the step 6 in algorithm 1 to identify subgroup. we know that formula (11) is a convex function, therefore a global optimized result can be calculated."
"as shown in fig. 1, our system first segments each video into shots, and obtains two kinds of information from each shot: the speaking content and the presentation slides. regards to the speaking content, we extract what the speaker has said by automatic speech recognition (asr). for the content of slides, we first extract key frames of the shot, which is defined as the frame that contains the slides of lecture. since the lecture video usually containing two kinds of frames: one is an image of speaker, the other is an image of slides, so it is obviously that the frames which contain slides are defined as key frame. after key frame extraction, we extract what the slide shown by optical character recognition (ocr). therefore, we obtain two sets of texts from the shot and they are treated as evidences for semantic content analysis."
"unlike group lasso, our structural grouping penalty in (3) not only selects the groups of heterogeneous features, but also identifies the subgroup of homogeneous features within each selected group."
"for each label and its corresponding indicator vector, the regression model of mtbgs is defined as follows: (2) and is called the structural grouping penalty."
"this paper proposes a framework of multi-label learning for image annotation, called the mtbgs. the mtbgs method is attractive due to its subgroup feature identification by structural grouping penalty in heterogeneous fea-"
"before we classifying videos by semantic information, we need to extract it by the methods. in lecture video, speakers teach knowledge to students or internet users through speaking and showing slides, so semantic information can be extracted from lectures oral presentation and slides. asr extracts semantic information from the speaking through capturing spoken words and then connects words to form a sentence. it is a computer-driven method which transcribes spoken language into text, so the semantic information can be read in real time. to ensure asr working normal, it must follows three steps. the system will capture the words that are recording by any storage device, and then converts the digital signals of the speaking into syllables or even phoneme directly. this is referred as feature analysis. next, the system will match the spoken syllables to a phoneme sequence that is kept in an acoustic model database. this is called pattern classification. finally, the system tries to make sense what is being said by comparing the word phonemes from a language model database. based on asr, speech information can be extracted."
the optimization problem is expressed as: the constraint ensures that the error between obtained gear ratio and the desired gear ratio is not more than the 50% of the desired gear ratio.
"furthermore, we explore the results of heterogeneous feature selection by mtbgs for different size of training data. let's see figure 7, we depict the results by mtbgs for images of of label \"mountain\" in miml dataset with different size of training data. note that the coefficient values are plotted from one round, and the group selection are plotted from 10-round repetition. as can be seen, the heterogeneous feature selection is more consistent and interpretable when the size of training data is increasing. for example, the most discriminative features for label \"mountain\" (see sample images in figure 4 figure 7, noisy feature groups, i.e., the texture feature groups, are almost excluded from the models when the number of training data reaches 900. in particular, the coefficient vector output from the 900 training samples is more sparse."
"the below figure (5) shows the gear train problem [cit] . a gear ratio between the driver and driven shafts must be achieved when designing a compound gear train. the gear ratio for gear train is defined as the ratio of the angular velocity of the output shaft to that of the input shaft. it is desirable to produce a gear ratio as close as possible to 1/6.931. for each gear, the number of teeth must be between 12 and 60. the design variables t a, t b, t d, and t f are the numbers of teeth of the gears a, b, d and f, respectively, which must be integers."
"different from general multimedia content on the internet, lecture videos contain many information sources. specially, a lecture video contains the speaking content and the presentation slides for the lecture, as shown in fig. 1 . furthermore, there are some scripts, user's notes or powerpoint for the lecture on the internet. the fusion of all these information is important for content-based multimedia retrieval. therefore, we propose multi-modal lda model to achieve the goal."
"where    , i i f x g x, are supposed to be continuous functions, s is compact. fractional programming problem sof the form (1) arise reality whenever rates such as the ratios (profit/revenue), (profit/time), (-waste of raw material/quantity of used raw material), are to be maximized often these problems are linear concave-convex fractional programming. fractional programmingproblems is a nonlinear programming method that has known increasing exposure recently and its importance, in solving concrete problems, is steadily increasing. furthermore, nonlinear optimization models describe practical problems much better than the linear optimization, with many assumptions, does. the fractional programming problems are particularly useful in the solution of economic problems in which different activities use certain resources in different proportions."
"chaos is a deterministic, random-like process found in nonlinear, dynamical system, which is non-period, non-converging and bounded. moreover, it depends on its initial condition and parameters [cit] . applications of chaos in several disciplines including operations research, physics, engineering, economics, biology, philosophy and computer science [cit] . recently chaos has been extended to various optimization areas because it can more easily escape from local minima and improve global convergence in comparison with other stochastic optimization algorithms [cit] . using chaotic sequences in flower pollination algorithm can be helpful to improve the reliability of the global optimality, and enhance the quality of the results."
"where    , i i f x g x, are supposed to be continuous functions, s is compact. fractional programming problem sof the form (1) arise reality whenever rates such as the ratios (profit/revenue), (profit/time), (-waste of raw material/quantity of used raw material), are to be maximized often these problems are linear concave-convex fractional programming. fractional programmingproblems is a nonlinear programming method that has known increasing exposure recently and its importance, in solving concrete problems, is steadily increasing. furthermore, nonlinear optimization models describe practical problems much better than the linear optimization, with many assumptions, does. the fractional programming problems are particularly useful in the solution of economic problems in which different activities use certain resources in different proportions."
"automatic annotating images with suitable multiple tags is a very active research field. from a machine learning point of view, the approaches of multi-label image annotation can be roughly classified into the generative model and the discriminative model. the generative model learns a joint distribution over image features and annotation tags. to annotate a new image, the learned generative model computes the conditional probability over tags given the visual features [cit] . on the other hand, the discriminative model trains a separate classifier from visual features for each tag. these classifiers are used to predict particular tags for test image samples [cit] ."
"lecture videos are knowledge sources, intellectual properties of university and material for multimedia course ware and teaching evaluation. compared with text in the book, lecture videos have many unique advantages: it is more salient and attractive, so it grabs users attention instantly; it carries more visual information that can be comprehended more quickly. data, speech and digital tv broadcast are regarded as the most considerable contents of online learning or e-learning, which are rapidly emerging in the world, and separate education to students distributed around the world. according to the booming of internet and digital technology, internet based distance learning has many advantages such as high degree of interactivity, a variety of courses are available at any time, uses less bandwidth."
"corrugated bulkhead design [cit] are often used in chemical tankers and product tankers in order to help facilities cargo tank washing effectively. this problem is as an example of minimumweight design of the corrugated bulkheads for a tanker. four design variables of the problem are width (b), depth (h), length (l), and plate thickness (t) for minimum-weight design of the corrugated bulkheads for a tanker, the mathematical formula for the optimization problem as follows: the comparison result of minimum-weight and the statistical values of the best solution obtained by cfpa, cs,fa and pso are given in table ( 2). accuracy performance is measured in terms of the best, mean, and standard deviation values of the solutions obtained by 20 independent runs. the best minimumweight in this study is 7.008391 with thickness 1.05 cm."
"while the objective is to optimize a certain indicator, usually the most favorable return on allocation ratio subject to the constraint imposed on the availability of resources. it also has a number of important practical applications in manufacturing, administration, transportation, data mining, etc. the solution methods to solve fractional programming problems can be broadly classified into classified into exact and inexact approaches. some examples of traditional approaches is that of ( [cit] who introduced the parametric approach, [cit] solved the linear fractional programming problems by converting fractional programmingproblems fpp into an equivalent linear programming problem and solved it using already existing standard algorithms for linear programming problemlpp, [cit] . a few studies in recent years used heuristics approaches to solve fractional programmingproblems. [cit] presented a genetic algorithm based method to solve the linear fractional programming problems. a set of solution point are generated using random numbers, feasibility of each solution point is verified, and the fitness value for all the feasible solution points are obtained. among the feasible solution points, the best solution point is found out and then replaces the worst solution point. a pairwise crossover method is used for crossover and a new set of solution points is obtained. these steps are repeated"
"while the objective is to optimize a certain indicator, usually the most favorable return on allocation ratio subject to the constraint imposed on the availability of resources. it also has a number of important practical applications in manufacturing, administration, transportation, data mining, etc. the solution methods to solve fractional programming problems can be broadly classified into classified into exact and inexact approaches. some examples of traditional approaches is that of ( [cit] who introduced the parametric approach, [cit] solved the linear fractional programming problems by converting fractional programmingproblems fpp into an equivalent linear programming problem and solved it using already existing standard algorithms for linear programming problemlpp, [cit] reviewed some of the methods that treated solving the fpp as the primal and dual simplex algorithm. the crisscross, which is based on pivoting, within an infinite number of iterations, either solves the problem or indicates that the problem is infeasible or unbounded. the interior point method, as well as dinkelbach algorithms both reduces the solution of the linear fractional programming lfp problem to the solution of a sequence of linear programming lp problems. isbell marlow method,martos' algorithm, cambini-martein's algorithm, bitran and novae's method, swarup's method. moreover, there are many recent approaches employing traditional mathematical methods for solving the fractional programmingproblem fpps such as: [cit] . a few studies in recent years used heuristics approaches to solve fractional programmingproblems. [cit] presented a genetic algorithm based method to solve the linear fractional programming problems. a set of solution point are generated using random numbers, feasibility of each solution point is verified, and the fitness value for all the feasible solution points are obtained. among the feasible solution points, the best solution point is found out and then replaces the worst solution point. a pairwise crossover method is used for crossover and a new set of solution points is obtained. these steps are repeated"
all the experiments were performed on a windows 7 ultimate 64-bit operating system; processor intel core i5 760 running at 2.81 ghz; 4 gb of ramand code was implemented in matlab.
"the parameters 1 and 2 in (3) need to be tuned. at the first training/test partition we choose those parameters by a 5-fold cross validation on the training dataset. these chosen parameters were then fixed to train the mtbgs model for all the 10 partitions. note that, different features play different roles in our mtbgs framework for different labels. therefore, the parameter tuning process is performing separately for each tag. we depict 3 examples of parameter tuning by the 5-fold cross validation with respect to micro-auc in"
"since most existed ocr tools focus on recognize characters instead of words in ocr, a word will be recognized incorrectly even if there is a character in the word is recognized incorrectly. thus, we need do some spelling check and correction operation after ocr extraction."
"in particular, for group selection, we employ a (not squared) ℓ2-norm on the group-level coefficient vector. while for within group sparsity, the ℓ1-norm of the within group coefficient vector is imposed. both the group-level ℓ2-norm and ℓ1-norm penalties are integrated to form the structural grouping penalty. similar as sparse group lasso (group and piecewise penalty) [cit] and group pursuit (pairwise penalty) [cit], the structural grouping sparsity in this paper not only selects the groups of heterogeneous features, but also discriminates the subgroups within homogeneous features, which is most responsible for outcomes of a label. as a whole, our primary objective is to achieve an interpretable and accurate model for multi-label prediction through a computationally efficient method."
"where n i j,(·) is the counting of word i has been assigned to topic j in the vector of assignments z, n i (·),r indicates the counting of word i has been assigned to vocabulary r, and γ (·) is the standard gamma function. then, we get the gibbs sampling equation as follows"
"in order to evaluate the stability of the c&w method, we compare two configurations of the sample selection in the first step of algorithm 3. the first one is the current setting in algorithm 3. the second configuration is to randomly resample some samples from test dataset (different part from training data x and label indicators y) and perform cca for the first step of algorithm 3. to investigate the stability we explore three different re-sampling methods, i.e., cross validation, jackknife [cit], and bootstrap [cit] . details are reported in the experiments."
"in order to achieve the goal of structural sparse feature selection and label correlation learning for multi-label image annotation, this paper proposes a framework of multi-label boosting by the selection of heterogeneous features with structural grouping sparsity (mtbgs)."
"the subgradient equations of (3) with respect to are to be zero, which means the th group is dropped out of the model, is that the system of equation"
"we explored the differences of heterogeneous feature selection between group lasso, lasso, and mtbgs on msrc and miml dataset for each label respectively. sample images for 2 instance labels from msrc and miml are listed in figure 3 and figure 4, respectively. as can be seen, images with different labels (semantics) have different heterogeneous low-level features, such as color and texture etc. therefore, training a model for heterogeneous feature selection is crucial for understanding the semantic content in these images."
"in asr, some speakers teach their lectures with their assent, relating to different countries or different regions, for example, someone speak \"dream\" as \"doraemon\"' and \"very\" as \"vely\". besides that, speakers say some prepositions such as \"is\", \"and\", \"or\", so we also need to delete these words and the similar ones after extraction step."
"curds and whey sets up the connection between multiple response regression and canonical correlation analysis. therefore, the c&w method can be used to boost the performance of multi-label prediction given the prediction results from the individual regression of each label, and hence it can be easily integrated into our mtbgs framework."
"in this model, we have two evidence variables, w and l, corresponding to speaking content and the presentation slides in the video. they are generated from topic z, which indicates the semantic topic of the shot. the document corresponds to a video."
"corrugated bulkhead design [cit] are often used in chemical tankers and product tankers in order to help facilities cargo tank washing effectively. this problem is as an example of minimumweight design of the corrugated bulkheads for a tanker. four design variables of the problem are width (b), depth (h), length (l), and plate thickness (t) for minimum-weight design of the corrugated bulkheads for a tanker, the mathematical formula for the optimization problem as follows: the comparison result of minimum-weight and the statistical values of the best solution obtained by cfpa, cs,fa and pso are given in table ( 2). accuracy performance is measured in terms of the best, mean, and standard deviation values of the solutions obtained by 20 independent runs. the best minimumweight in this study is 7.008391 with thickness 1.05 cm."
"the below figure (5) shows the gear train problem [cit] . a gear ratio between the driver and driven shafts must be achieved when designing a compound gear train. the gear ratio for gear train is defined as the ratio of the angular velocity of the output shaft to that of the input shaft. it is desirable to produce a gear ratio as close as possible to 1/6.931. for each gear, the number of teeth must be between 12 and 60. the design variables t a, t b, t d, and t f are the numbers of teeth of the gears a, b, d and f, respectively, which must be integers."
the usual procedure of performing individual regression of each label on the common set of features ignores the correlations between labels. we propose to take advantage of correlations between labels to improve predictive accuracy. we call this method the multi-label boosting by curds and whey (c&w) [cit] .
formula (11) is the sum of a convex differentiable function (first two terms) and a separable penalty. in next section we develop the gauss-seidel coordinate descent (gscd) algorithm [cit] to minimize (11) by a one-dimensional search over . we summarize the algorithm for solving the regularized regression with structural grouping penalty in algorithm 1.
"three benchmark image datasets are used in our experiments: microsoft research cambridge (msrc), miml [cit], and nus-wide [cit] . 23 and 5 class labels (tags) are respectively associated with images in msrc and miml, which are multi-tagged and can be used as annotation ground truth. we randomly sampled 10, 000 images from nus-wide in our experiments. for the ground truth of nus-wide we chose two indicator matrices from the selected data samples to form two datasets -nus-6 and nus-16. in these two datasets, the top 6 and 16 tags which label the maximum numbers of positive instances were selected respectively."
the remainder of this paper is organized as follows. we first introduce the framework of multi-label boosting by structural grouping sparsity and its computational issues in section 2 and 3 respectively. the experimental analysis and conclusion are given in section 4 and section 5.
"after that, we propose a model to discover semantic content from the obtained evidences. our model is derived from topic model, which is originally proposed for text processing and generates the topics to describe a distribution of words, for example, probability latent semantic analysis (plsa) [cit] and latent dirichlet allocation (lda) [cit] . specifically, lda is a generative prob-abilistic model introduced, and it is a three-level hierarchical bayesian probabilistic model that a mixture of a latent set of distributions of discrete semantic data to set topics. our multi-modal lda model is derived from lda model."
"the optimization problem is expressed as: the constraint ensures that the error between obtained gear ratio and the desired gear ratio is not more than the 50% of the desired gear ratio. the comparison resulted obtained by the cfpa, ca, fa and pso algorithms are given in table ( 3), the comparison in terms of the best, error, mean, standard deviation values, these values where obtained out of 20 independent runs. the result indicates a better achievement for cfpa, with an objective function value of 2.7 e-012."
"in real world images, we can extract high dimensional heterogenous features from one given image, such as global features (color, shape and texture) or local features(sbn [cit], sift, shape context and gloh (gradient location and orientation histogram)). different subsets of heterogenous features have different intrinsic discriminative power to characterize one image label. that is to say, only limited groups of heterogenous features distinguish each label from others. therefore, the selected visual features to be used for the prediction of certain image semantics are usually sparse."
"the paper presents a new approach to solve fpps based on flower pollination algorithm with chaos. ten-benchmark problem weresolvedusingthe proposed algorithm and many other previous approaches.the results employing the proposed algorithmwerecompared with the other exact and metaheuristic approachpreviously used for handling fpps. the algorithm proved their effectiveness, reliability and competences insolving different fpps. the proposed algorithm managed tosuccessfullysolve large-scale fpps with an optimal solution at a finite point and an unbounded constraint set. the features and capabilitiesof the proposed algorithm was more evident when dealing with large-scale problems and a solution is a regular space.thecomputational results proved that cfpa turned out to besuperior to other algorithms for all the accomplished testsyielding a higher and much faster growing mean fitness atless computational time. the proposed algorithm can extend to handle multiobjective fractional optimization."
"in particular, for the training data, we randomly sampled 300, 500, and 1000 samples from msrc, miml, and nus datasets respectively. the remaining data was used as the corresponding test data. for each dataset, this process was repeated ten times to generate 10 random training/test partitions. the average performance in terms of auc and f1 score and standard deviation are evaluated."
"is the th column of indicator matrix y and encodes the label information for the th label, ( ) is the regularizer which can impose certain structural priors of input data. for example, the ridge regression uses the ℓ2-norm to avoid overfitting and lasso produces sparsity on by the ℓ1-norm. basically, mtbgs comprises two steps, namely regression with structural grouping penalty and multi-label boosting by curds and whey."
"here we have to point out that the proposed sparsitybased feature selection is different from other approaches such as visual diversity modeling, in which mixture of image kernels were integrated to characterize the diverse visual similarity relationships between images [cit] ."
"to address these problems, we build up a stop-word dictionary as follows: firstly, we find a stop-word list on the network as initial dictionary; and then through analyzing the extracting information, the words such as \"enough\" and \"local\", can be added to dictionary. repeating this program one by one, we can get dictionary and use it as filter to shield words that have no meaning for discovery knowledge to classify shots. now we take experiment with extracting semantic information to discovery knowledge and classify these shots into different topics. the dataset of clustering is too large, so we show a part of them."
"the below figure (5) shows the gear train problem [cit] . a gear ratio between the driver and driven shafts must be achieved when designing a compound gear train. the gear ratio for gear train is defined as the ratio of the angular velocity of the output shaft to that of the input shaft. it is desirable to produce a gear ratio as close as possible to 1/6.931. for each gear, the number of teeth must be between 12 and 60. the design variables t a, t b, t d, and t f are the numbers of teeth of the gears a, b, d and f, respectively, which must be integers."
"in this paper, we formulate mtbgs as a multi-response least square regression with structural grouping penalty. the basic motivation of imposing structural grouping penalty in mtbgs is to perform heterogeneous feature group selection and subgroup identification within homogeneous features simultaneously. as we know, some subgroups of features in high-dimensional heterogenous features have more discriminative power for predicting certain labels of a given image. furthermore, the correlations between labels are utilized by a curds and whey procedure [cit] to boost the performance of image annotation in multi-label setting."
"after that, we use asr and ocr to extract semantic information from these key frames. the words extracted by asr and ocr are treated as two kinds of evidences."
"in tab. 2 and tab. 3, we can find some words like \"cftp\" is not a word that has some mean. therefore, we state that \"cftp\" means \"coupling from the past\" and it occurs in the sentence we borrow methods for inference ugms: cftp in ising and potts models. thus, we need to note that this type of word means abbreviation."
"in order to take advantage of correlations between the labels to boost multi-label annotation, we propose to utilize the curds and whey (c&w) [cit] method and integrate it into our mtbgs framework."
"to further investigate the stability of c&w(test) method, we repeat the c&w(test) process by re-sampling image subsets from test data using three different re-sampling methods: 100-fold cross validation, jackknife [cit], and 100-round bootstrap [cit] . for three re-sampling methods, the best linear matrix b * for curds and whey is chosen by the training repetition on the re-sampled subsets. the b * that outputs the best multi-label boosting performance with respect to micro-auc is chosen. this process is repeated 10 times, and the average performance and standard deviations are reported in table 1 . from the results we can draw the conclusion that the c&w(test) method is stable."
"we have shown in this paper that multi-modal lda model is able to discovery semantic information from lecture videos. for the purpose that helping users searching and browsing videos directly, we presented an advised model based on lda and added another input, so that system classifies lecture videos into few same titles efficient. the main contributed idea is adding one more variable into lda model based on gibbs sampler. through the model, we can use the semantic information from speaking and slides, but also based on the effect of different extracting technologies, lecture videos can be classified properly. even with the professional words, this model can achieve the goal, so it will be more effective used in other video like sports, news or movies."
"the markov chain monte carlo (mcmc) algorithm is constructed to converge to the target distribution [cit] . as a sampling method deriving from mcmc algorithm, thomas reaches the target distribution by using gibbs sampler, which is a heat bath algorithm in statistical physics and the next state is reached by sequentially sampling all variables from their distribution when conditioned on the current values of all other variables and the data. therefore, thomas proposed the equation based on this thinking as follows,"
"a large field of view achieved through wide-angle lenses furthermore allows detailed analysis of flipper movements, which to date could only be achieved through elaborate modelling of accelerometer data [cit] or use of complicated magnetic logger setups [cit] . neither of these setups provided information about exhalation, which appears to play a much more important role during diving than previously thought. when comparing video data recorded here with videos from previously published studies (e.g., [cit], https://vimeo.com/268905870) it becomes clear that greater visual fidelity of full hd cameras comes along with a much wider range of quantifiable data. this creates a new opportunity for a more holistic approach to study the diving behaviour of marine animals that integrates behaviour, physiology and their environment."
"we collected data using two surveys administered during the cspcc study to specifically investigate student beliefs and attitudes about mathematics among the calculus i student population. [cit] semester and a follow-up survey 2 weeks before the end of the semester. extra credit for completion of the surveys was given to the participating students, and each of the calculus courses had a course coordinator who determined the course's grading scheme how they could best award extra credit for survey completion to both incentivize the process. the coordinator for e added 10 points of extra credit (worth 1% of a letter grade) to students' final grade calculation if a student completed all surveys offered during the study. the coordinator for 1a added 2 points of extra credit to students' final grades for completion of each survey offered during the study. the coordinator for ne added 1 point of extra credit (worth less than 0.5% of a letter grade) to students' total quiz scores (a component of their final grade) for completing each one of the surveys."
"an alternative representation of the cascade system (1)-(3), which incorporates a boundary-value-dependent propagation speed is offered in this section. we recast the original problem into a delay system framework by solving the transport pde (2), (3) with the method of characteristics. the resulting system is a delayed-input-dependent input delay system, which can be explained by the fact that the propagation speed of the pde, namely,"
"in conclusion, to answer our research question, results indicate that student experiences in three versions of calculus i at our institution have an effect on both their beliefs and attitudes towards mathematics. moreover, the impact of the course structures is different, and we were able to isolate the impacts within course structures from the student demographic backgrounds within those structures, implying that some aspects of the course experiences themselves are responsible for these differences. for example, we hypothesize that the strongly traditional lecture-based format of the e course led to a stronger negative impact on students' confidence and enjoyment in mathematics though there may be other properties of the structure responsible for this effect. further work needs to be done to determine what role the way in which engineering majors are concentrated in e and other science majors in ne might be responsible for some of these differences or the way in which student entry into the \"stretch calculus\" via placement impacts student experience and attitudes compared to the more mainstream courses. in all, a more precise characterization of the course formats that would allow for a quantitative comparison of the presence of active learning or the use of traditional lecturing would help the analysis of these impacts and shed further light on how such practices affect student attitudes."
"for the model described in eqs. (117), (123) with the boundary condition (118), the control objective is to stabilize the queue q at the desired equilibrium value q ⋆ ."
"dive data recorded by the tdr at 1 s intervals and depth resolution of ∼0.1 [cit] . dives were classified as pelagic or benthic dives using dive profile characteristics, where near horizontal bottom phases with little vertical variance as well as consistent maximum dive depths on consecutive dives were used as cues for diving along the seafloor. this approach was validated by recorded video data. the tdr also recorded tri-axial accelerometer data which have yet to be analysed."
"depending on which behaviours are quantified, the manual analysis of video data can be quite time-consuming. for example, flipper beats and angles require a frameby-frame analysis; an average dive duration of 3 min translated to 5,400 frames per dive. however, the higher the resolution and quality of the video footage, the greater the potential to develop machine learning algorithms (such as google cloud video intelligence; https://cloud.google.com/video-intelligence/) that may be used to automate the analysis process. for more basic analyses such as prey composition and encounter rates, but also determination of environmental parameters, there already exist software solutions that offer an enhanced workflow, for example the video annotation software boris (http://www.boris.unito.it/)."
"the dynamics of the queue buffer occupancy q (t), the boundary control law f (t) and the nominal compensated ''bang-bang '' control law b(p(d, t) ) are presented. moreover, the evolution in time and space of the part density ρ(x, t) and the predictor state p(x, t) are simulated using a finite volume setup. as shown in figs. 2 and 3, the uncompensated input leads to oscillatory response and the compensated controller allows faster convergence than the open loop control. [cit] extensively discuss the implementation issue of predictor-feedback and offer various numerical schemes for computation of predictor-feedback laws."
it is clear that such t always exists (and is unique for a given t 0 ) as φ u (t) is strictly increasing and from (7) (see assumption 1)
"in this section, we present the dynamical model of the queue in front of the production line. we represent the queue buffer occupancy by defining the load of goods, q, stored in the queue and imposing the conservation of mass [cit] . hence,"
"obviously, there are still limitations to the use of camera loggers. restrictions arise from the battery life as well as the memory to store high definition video data. in our case, 15 min of footage resulted in video file sizes of 1.5 gigabytes. moreover, the deployment with the camera set-up we used requires a certain amount of predictability, particularly knowledge about how soon after departure the bird is likely to engage in behaviours that are of interest (e.g., prey pursuit). for all these reasons, the technology currently available is best suited for short-term deployments on central place foragers. although video data recorded on animals performing long-term foraging trips (e.g., [cit] ) might still deliver valuable data, this has to be weighed against the fact that external devices inevitably have an effect on the animal's foraging ability [cit] . this could be alleviated by incorporating further mechanisms to control camera recording (e.g., duty-cycling of recording function, pressure control). while the use of animal-borne cameras for scientific research is still in its early days, the enormous potential of this technology will doubtlessly result in devices incorporating more elaborate functionality in the future."
". in order to prove the well-posedness of the closed-loop system consisting of (1)-(3) with the controller (5), (6), we first show the well-posedness of the target system (12)- (14). consider first the following systeṁ"
"the non-engineering, one-semester version (ne) serves students primarily from science-related disciplines such as biology, chemistry, and physics. the format of the course includes highly student-focused classroom meetings with an instructor three times per week that incorporates group learning and other activities to develop strong conceptual understanding. these activities incorporate active learning approaches where students develop concepts through guided activities. summative assessments focus on these concepts and de-emphasize complex numerical processes that would require a calculator. students meet with graduate teaching assistants (gtas) twice a week for additional work on problem solving and homework. during the time period for this study, there were 10 sections of this course taught with 34 students in each section. the instructors for these courses were full-time lecturers and graduate student instructors. this course used a common syllabus, common tests, and a common final exam. instructors were allowed to modify the homework policy for their own section of the course."
"using the equivalence between the signals p(x, t) and π(x, t) stated in (28), it can be deduced from (18) that the π -system satisfies the following pde"
"during the video logger's operating time, the penguin spent 29.9 min foraging along the seafloor. the majority of the penguin's bottom time (90%) was spent over coarse sand, whereas time spent over fine sand (7%) and gravel (0.9%) was negligible (figs. 1 & 3) . two thirds of the bottom time (65.9%) was spent over sand ripples, the remaining time (34.1%) the bird foraged over flat ground. brittle stars and anthozoans were present in most areas visited by the penguin with the former being present in 22.5 min (75%) of the benthic video footage while the latter occur for a total of 17.9 min (60%). horse mussels were present for a total of 9.3 min (31%) of the bottom time."
"as stated previously, we seek to answer the following research question: do different learning experiences in calculus i influence students' attitudes and beliefs? the data revealed that a large number of students in all three versions tend to understand that trying to make sense of the materials is a better method of studying calculus i instead of trying to memorize them (table 5, statement 1). however, a pre-and post-survey comparison indicated a decrease in this tendency, especially in ne where there was a massive shift. in the pre-survey, we saw large differences occurred among the versions, and these differences are statistically significant according to anova test. in the post-survey, the differences among three versions were smaller and not found to be statistically significant. students in all versions indicated a low desire to continue studying mathematics (table 5, statement 2) unless required to do so at both the beginning and the end of semester, and we observed a decline in all versions, with a large decline in ne and an especially steep decline in e. pairwise, the differences between 1a and e and between ne and e were large in the pre-survey but very small between 1a and ne. results from the post-survey revealed a different picture. the differences between 1a and ne became large, but the difference between 1a and e diminished. the difference between ne and e became much smaller. anova tests showed that the differences among three versions in both pre-and post-surveys were statistically significant."
"as noted earlier in this work, research has consistently indicated that the affective aspects of student non-cognitive factors such as attitudes, beliefs, confidence, enjoyment, desires, and other underlying beliefs have an impact on stem persistence [cit] . in the current work, we observe in the 1a format small decreases in confidence and enjoyment and a small increase in desire for more mathematics that were not significant. students in ne also showed a decrease in desire for more mathematics, but this change was not statistically significant. the levels of these responses were not as high as was observed in the national study, and students in 1a demonstrated lower levels of agreement in confidence, enjoyment, and desire for more mathematics compared to the e and ne populations. our main result shows that after engaging in the e course structure, students stated a larger, statistically significant decrease in confidence, enjoyment, and desire to continue in mathematics, and in the ne course structure, they showed similarly high statistically significant decreases in confidence and enjoyment when compared to the national population. the strong difference in the response of the 1a group compared to the other two suggests that this population has different levels of enjoyment in mathematics and confidence in their mathematical abilities from the larger aggregated local calculus ne and e populations as well as from the national one-semester calculus population."
"in line with previous descriptions of yellow-eyed penguins as primarily benthic foragers [cit], the penguin's prey pursuit and captures recorded during the camera operation indeed all occurred at the sea floor. swimming very close to the seafloor could serve several purposes. it could be a strategy to flush out benthic prey that blends in with the substrate, but it could also mean the penguin has a greater chance to see its prey from the side, and thus reduce the effect of prey camouflage. opalfish, for example, are very well camouflaged and very difficult to make out from above [cit] . this species seems to principally rely on its camouflage as means of predator avoidance since none of the opalfish captures involved a chase. in contrast, during both successful blue cod encounters, extended high-speed chases ensued before the fish was ultimately captured. blue cod and opalfish differ significantly in their anatomy with the small, slender opalfish presumably lacking the physical prowess for prolonged swimming when compared to muscular blue cod [cit] . when facing an air breathing predator, the latter strategy is likely advantageous as the predator's increased energy requirements for pursuit make escape a more likely outcome for the prey. the penguin's hasty ascent and subsequent failure to consume a blue cod it captured after a 22-second-long chase demonstrates the efficacy of this evasion strategy. both opalfish and blue cod have previously been found to be among the most important prey items in the yellow-eyed penguin's diet [cit] . while both fish species have comparable energetic values (∼20 kj g −1, [cit] ), the body mass of opalfish is considerably lower when compared to blue cod [cit] b) . it is possible that the energy gain from catching blue cod justifies the expenditure to catch it, while the easier-to-catch opalfish might need to be caught in larger quantities. however, recent studies suggest that blue cod might be suboptimal prey for chick-rearing yellow-eyed penguins due to their size [cit] so that the penguin's ability to locate prey such as opalfish might be a decisive factor with regards to reproductive success."
the explicit solution that satisfies the pde (74) and the boundary condition (75) (which can be viewed as a delay line with d time units delay and zero input) is given bȳ
"deploying video loggers on penguins could enable detailed mapping of the benthic habitat within the species' home ranges. yellow-eyed penguins are known to have preferred individual foraging areas often with little overlap between birds [cit] . moreover, the birds tend to often dive along the seafloor when swimming towards their foraging grounds [cit] so that camera logger data in combination with gps information can be used to establish spatial biodiversity indices and benthic habitat maps."
"for statements 4 through 7, differences and similarities were observed across the three course versions in preand post-surveys, but the anova test of the differences and similarities found no statistical significance."
"to assess the extent to which penguin behaviour and foraging success correlate with the composition of the benthic habitat, we developed a camera logger that records full high-definition (hd) videos through wide-angle lenses. the main focus of our study was to assess the suitability of the device for the visual analysis of penguin prey pursuit behaviour and characteristics of the benthic ecosystem. however, the deployment revealed far more information than was anticipated. the video data provided novel insights into physiological aspects of the penguin's diving activities and allowed us to draw conclusions about prey capture techniques. in this paper, we summarise our findings, demonstrate analytical approaches to evaluate animal-borne video data, and highlight the multi-disciplinary potential of wide-angle, full hd video loggers."
"attaching external recording devices to diving animals always comes at the cost of compromising their streamlined body shape (e.g., [cit] ), a problem that can be mitigated via device shape, size and attachment position [cit] . at the surface there were no indications that the penguin was negatively affected by the device; the bird did not exhibit balancing problems which externally attached devices can cause in smaller species [cit], nor did it peck at the device frequently which suggests aberrant behaviour [cit] . moreover, the number of successful prey captures further suggests that the bird's foraging capabilities were not drastically affected by the video logger. with the exception of two unsuccessful blue cod encounters, all events classified as prey pursuit were merely accelerations that did not end in any obvious prey encounter. the bird was one of the few breeders that raised two chicks to fledging in an otherwise poor breeding season."
"here the spatial variable x is an artificial continuous index of the suppliers. denoting ρ(x, t) the density of parts at stage x and time t and ω(x, t) [cit]"
"u (x + φ u (t)) )) ), (a.11) with the help of (102) taking the derivative of both sides of (a.13) with respect to x and using (a.1) we deduce the following relation, t) ) ."
"thus by direct verification, (26) is the inverse of (11). clearly, taking the derivative of (27) with respect to x and t, it can be shown that relation (28) hold given that p and π satisfy the identical class of transport pde (see (21))."
"where p(x, t) is defined in (6), combined with the control law defined in (5), (6), maps the system (1), (2) with the boundary condition (3) into the following target systeṁ"
"since the logger stores video data as a series of full frame images ('progressive scan'), it was possible to conduct a frame-by-frame analysis to accurately time components of the bird's behaviour-i.e., breathing intervals, flipper beat frequencies and amplitudes-as well as time spent over certain benthic habitats. video analysis was conducted in professional editing software (adobe premiere pro cs 6, adobe systems inc., san jose, ca, usa) which allows the quick and precise backward and forward navigation of the video material using the keyboard (''scrubbing'') and provides the option to display frame number in the preview timer."
"taking the derivative of (a.6) with respect to x and t we get where the partial derivative of p(x, t) with respect to time is written as"
"the nonlocal term in (117) suggests that the wip depends nonlinearly upon only the output density at the current time and indicates the full impact of the overall loading of the system on the motion at the beginning of the production line. the density ρ(x, t)"
"from the non-negativeness of the density function ρ(x, t) and the propagation speed v(ρ(0, t)), we deduce that the setpoint control of the queuing model is a non-negative quantity imposing the following restriction"
"we defined the beginning of a prey pursuit as the moment when the penguin markedly accelerated while swimming along the seafloor; the end was reached when the penguin decelerated again to its previous cruise speed (if no prey was caught), or when the prey item was swallowed completely. acceleration and deceleration were associated with temporary blurring of the video footage due to irregular body movement, allowing for exact timing of prey pursuits. where possible, prey species were identified from frames providing a clear view of the prey item."
"differences were also observed between student populations across the three versions when specifically examining confidence, enjoyment, and desire to continue in mathematics in each version and comparing responses to these pre-and post-surveys. an anova was performed on the pre-survey and post-survey results independently using course type as a factor. this approach showed that the mean of students' confidence across three versions was statistically significantly different in the post-survey but not in pre-survey. the mean of students' enjoyment and desire to continue in mathematics among three versions was statistically significantly different across course type in both pre-and post-surveys."
"students in all versions believe exam scores measure the amount of material they understand, and there was an increase in the belief among students in e and ne that exam scores are measuring how well they can do things the way the teacher wants (table 5, statement 3). in the pre-survey, we noticed the differences among the three versions are very small. in the post-survey, the differences across three versions become large and statistically significant according to an anova test."
"our current findings focus on how student beliefs and attitudes change based on their experiences in our courses. a natural extension will be to analyze whether and how students experienced their calculus i courses differently and to attempt to align that with the more granular differences in instruction in the three formats. in addition, it is unclear whether and how the data can be used to actually implement instructional change in the courses. the data do, however, encourage us to further investigate the reasons for explaining how these differences occurred. looking specifically at subsets of these populations, such as only stem-intending subgroups, sex subgroups, or stem-persisting subgroups, may yield insights into what aspects of these courses are effective. at the very least, we hope to provide baseline data needed to document and analyze change in these factors as the courses pursue interventions to retain talented stem majors. our findings represent an important first step in understanding the way in which the national results of the cspcc study can be used to analyze the effects of a local implementation of calculus with a large population involving varied goals and backgrounds. for both the external comparison to the national data and internal comparisons within the three versions of the course offered at our institution, the small size does raise questions about the robustness of our data; if the sample size for the 1a and ne were much larger, the comparisons could be much more convincing. however, at a minimum, these findings highlight several other questions for future work: whether students do indeed benefit more long term by taking the two-semester slow-paced calculus and whether it is better to group stem-intending and non-stem-intending students for this coursework. this work also provides a basis for making informed decisions about changes in courses, specifically in e to address the significant decrease in confidence, enjoyment, and desire to continue in mathematics."
"a third format is offered as a two-semester calculus i equivalent (1a/b). student success and placement data are used to identify a distinct cohort of students who either would have previously not been able to directly enroll in calculus i or are at the highest risk of failing or withdrawing from a one-semester course. as a result, a primary difference for this course is the overall student population. students learn the content covered in the standard calculus i over a two-semester time period allowing for more in-depth coverage of core but troublesome calculus concepts and for time to review precalculus content as needed. students meet with their instructor three times per week in a traditional lecture format with 80 students and once per week in a \"laboratory\" setting with a gta and their instructor to work on activities in groups designed to support the development of concepts. the activities that students complete are a combination of paper-based and computer-supported projects. during the time period for this study, there were 4 sections of this course with 80 students in each section. the instructors for these courses were full-time lecturers. this course used a common syllabus, common tests, and a common final exam, and no modifications to policies or grading were allowed."
"since the transport velocity v is assumed to be strictly positive, the function φ u (t) is a monotonically increasing function and defines a bijective mapping between time and space. the subscript u denotes the delayed-input dependence of the function φ u (t). by combining (96) and (97) we derive the following relation"
"the deployment of a full hd video logger on a yellow-eyed penguin resulted in a versatile visual data set that provided a variety of information well beyond what was initially intended. enhanced video quality allows detailed analysis of the benthic environment as well as prey encounter rates and prey composition. in combination with gps data, the potential for a comprehensive survey of benthic ecosystems is substantial highlighting the multi-disciplinary potential of such data."
"examining the at-sea behaviour of marine animals has long been a challenging endeavour. direct visual observations of behaviour are almost impossible, especially when most of it happens under the ocean's surface. in recent decades, advances in telemetry technologies and the emergence of bio-logging hardware have provided the means to track marine animals and reveal their foraging behaviour in great detail. [cit] s with rather crude location estimates and limited data quality recorded by unwieldy devices that could only be used on large animals, advancements in micro-electronics have resulted in ever smaller and more accurate loggers to pinpoint an animal's position to within a few metres and record their diving depths with oceanography-grade precision [cit] . new technologies such as accelerometers and gyroscopes further refined methods to study marine habitat use (e.g., [cit] ). yet placing dive metrics into a complex behavioural and environmental context can be difficult; ideally, a reference framework based on direct observations is used to match up dive metrics and actual behaviours (e.g., [cit] . so, the original dilemma of having to make direct observations of marine animal behaviours still persists. animal-borne video recorders offer the means to overcome this problem."
"the yellow-eyed penguin (megadyptes antipodes) in new zealand is known to be a benthic forager [cit] that feeds primarily on demersal fish species [cit] . it has been suggested that this strategy might come at the expense of reduced behavioural flexibility, with subsequent vulnerability to changes in the marine environment [cit] . in particular, degradation of seafloor ecosystems in the wake of commercial bottom fisheries are suspected to influence yellow-eyed penguin foraging success and population developments [cit] . while the species' at-sea movement and diving behaviour have been subject to a number of studies in the past decades [cit], information about their benthic habitat is very limited."
"however, the problem of design of predictor-feedback controllers for compensation of input delays that depend on the control input itself is left out in most of the existing contributions. [cit], the design of delaycompensating control laws for such systems of transport pde/ode cascades with input-dependent transport coefficient (that appear for example when describing the dynamics of crushing-mill processes [cit], recycling cstr [cit], and single-phase marine cooling systems [cit] ) remains an open problem. [cit], which is motivated by the dynamical model of fuel to air ratio (far) in gasoline engines, is perhaps the only contribution that covers this particular subject on delay compensation. due to the dependency of the prediction horizon on the future input values a design that completely compensate the input delay does not seem possible."
"the engineering, one-semester version (e) is built around the use of engineering-based application problems to motivate calculus concepts, and the course focuses more on technical skill development and computational precision than on deeper conceptual understanding. three days per week, students attend a lecture meeting with the instructor. students meet with gtas twice a week to work on activities that often align with content they are also learning in their introductory engineering courses and that maintain a high level of computational complexity. many of these classes are offered on the engineering school's campus, instead of near the department's other classes. during the time period for this study, there were 12 sections of this course and up to 42 students in each section. the instructors for these courses were full-time lecturers and graduate student instructors. this course used a common syllabus, common tests, and a common final exam, and no modifications to policies or grading were allowed."
"the high-quality video footage provided a substantial amount of new insights into the foraging behaviour of yellow-eyed penguins and their benthic habitat. while it is impossible to draw far-ranging conclusions from only a single deployment, it nevertheless highlights that high-definition cameras provide a new tool facilitating the examination of various aspects of the foraging ecology of marine predators through direct observation. it can be particularly useful to verify and calibrate behaviours measured with other types of devices such as tdr and accelerometers."
"beyond investigations of behaviour in a wider environmental context, our study also shows the potential application of camera loggers for the investigation of physiological aspects of marine animals."
"in this paper, we develop an infinite-dimensional predictorfeedback control law which enables one to compensate, for nonlinear systems, actuator dynamics governed by a transport pde with boundary-value-dependent propagation velocity. in particular, the actuator dynamics depend on the boundary value of the actuator state anti-collocated to the actuation or sensing mechanism introducing a delay that depends on the delayed input signal. our predictor-feedback controller guarantees a global asymptotic stability of the coupled system. the feasibility of the proposed controller is demonstrated on a model of a production system with finite buffer, using the recently developed ''bang-bang'' nominal controller. u ) and (p, u) define the infinite-dimensional representation of the actuator state as"
"in this report we compare students' beliefs and attitudes towards mathematics across three different offerings of calculus i at a single institution, and a number of differences were observed in student responses to the courses. we draw the following conclusions from this study as described below."
"the uniqueness of this solution follows from the well-known uniqueness of solutions to the transport pde with constant transport speed (74) with boundary condition (75), see, e.g., [cit] . moreover, from the definition for p in (6) we get that (73), (78) thaṫ"
"the transport equation (115) is obtained expressing the mass conservation laws into the system if the yield loss on stages is neglected (no production or loss of material during the production process). [cit], we assume that the speed ω is a function of parts density ρ and define the following relation"
"upon device recovery, it became apparent that the gps logger did not record any data after the camera had started operating. it has since become evident that the mobius action-cam generates significant electromagnetic interference which prevented the gps logger from functioning properly. this can be rectified by wrapping the camera with electrical shielding tape; however, in our case the lack of shielding resulted in failure to record gps data."
we surveyed students' beliefs and attitudes at the beginning (pre-survey) and end (post-survey) of the semester to collect data that might reveal differences in and changes in these beliefs and attitudes during the term as well as across the course populations.
"each version of the course has a coordinator who supervises the instructors in that course and whose philosophy about teaching and goals for the course drive their curricular decisions independent from other courses. these courses will be referred to as versions 1a (the first half of the 1a/b sequence was the focus of our study), e, and ne for the remainder of this paper. in summary, the courses differ primarily in class size, lecture format, and recitation methods. 1a has the largest class size, followed by e, then ne. the lecture format in e and 1a is more traditional but the meeting format in ne is more active learning-driven with group discussions. finally, the recitation activities employed in ne focus on conceptual development while those in e focus on computation and applications. the recitation activities used in 1a focus on understanding concepts using computer-supported projects. the summarized course formats for each version are shown in table 1 ."
"the camera operated continuously from 11:00:22 hrs to 13:01:43 hrs. due to frame loss representing a mean 1.6 seconds of footage when video data were written to the file every 3 min, total length of the recorded footage amounted to 2 h 8 s. forty-six complete dives were video recorded which corresponds to 16% of all dive events; of these 32 dives were benthic dives. however, dives were longer during the middle of the day so that camera footage covered 25% of the trip's cumulative dive time. the video quality proved to be significantly better than that recorded with other animal-borne cameras deployed on penguins to date (https://vimeo.com/268905870). the light sensitivity of the camera was adequate to record clear images at dive depths close to 70 m and, combined with the large field of view, facilitated detailed frame-by-frame analysis."
"data for e began with confidence and enjoyment levels a point lower than the national results, and the effect size we observe in the two variables, ranging from − 0.33 to − 0.70, are all larger than observed nationally. students in ne also began with lower levels, but the effect sizes were similar to the national population. interestingly, students in e exhibit similar levels of desire to continue as the national cohort, but the decrease in that desire is more than twice the size as that observed nationally. students in the other two course structures enter with lower desire to continue, but their desire remains more constant; ne population's decreases with an effect size of − 0.153 while the 1a population's actually increased. it is reasonable to conclude that our population of students is in some way different enough from that of the national study and that an accurate determination of these differences might shed light on what aspects of our course structures resonate with our populations and which do not. these distinctions seen from the point of view of the different course structures then suggest a similar comparison with outcomes in student behaviors. [cit] has identified a number of areas that impact student persistence in their educational track. these can in part be characterized as facets of either academic or social integration. examples of academic integration [cit] include grade outcomes, a student's value of the learning process and what they learn, their enjoyment of a subject, their enjoyment or appreciation of the learning process, the level to which they identify with existing academic norms, and the level to which they identify with the role of \"student.\" in addition, student attitudes and beliefs impact their enjoyment and are related to their confidence in their abilities [cit] . in the data from this study, response rates to the two items concerning confidence and enjoyment show differences across course structures for \"i am confident in my mathematical abilities\" and \"i enjoy doing mathematics.\" of these, the question regarding confidence shows that 7% more of the students in e respond as confident than in ne and 13% more than 1a. we expect then higher persistence of enjoyment and other beliefs for this group compared to others. surprisingly, we see much higher negative effect sizes for e than for either ne or 1a on enjoyment, confidence, and desire for more mathematics coursework."
"on the other hand, thinking of self-efficacy as our perception of our ability to deal with a situation [cit], attitudes and the underlying beliefs that support them tend to move towards negative or unsupportive actions when our self-efficacy is lower and so we would expect students with lower indicators of self-efficacy to exhibit larger negative changes in beliefs. that is, if we perceive ourselves as being incapable of impacting a situation such as an outcome on a mathematics exam, we tend to move away from attitudes or beliefs that support positive action such as a belief that homework/practice is valuable. with this in mind, we can look for this within our data and outcomes, and we find that, indeed, the higher negative effect size for e suggests some underlying issue with self-efficacy interacting with the course structure. students in that course structure exhibit higher levels of self-efficacy and confidence, as expected, but these beliefs are less robust during that course than those of others again implying that indeed, the course structure itself has an impact on the students that was negative regardless of their academic background on entering it."
"focusing more on specific student beliefs and attitudes, we found dominant beliefs in the role of the instructor, the process of problem solving, and the goal of learning calculus across all three versions. these outcomes are again similar to those from the national study. in this work, however, we are more interested in any observed contrasts since our research questions focus on the differences in students' beliefs and attitudes towards mathematics among students in the different versions of the course. as noted above, students in 1a have a lower level of confidence, beliefs, enjoyment, and desire for more mathematics, while in contrast, students in e and ne possess greater self-confidence for overcoming complications (tables 5 and 6 )."
"parameterized by some variable τ, the distributed state of the pde (2), namely, u(x, t), can be described by u(x(τ ), t(τ )) whose total derivative is written as"
"for all dives, the benthic habitat was classified according to sediment type (fine sand, coarse sand with shell fish fragments, gravel), sediment structure (flat, sediment ripples) and composition of the epibenthic communities. for the latter, we used a presence/absence approach with easy-to-identify epibenthic species brittlestars (ophiuroidea), anthozoans (anemones and soft corals), and horse mussels (atrina zelandica), within a 30-frame time window. figure 1 provides a photographic overview of the different habitat characteristics used. future deployments with a functional gps logger can be used for more elaborate analysis of the benthic habitat, e.g., the creation of biodiversity indices."
"the nonlinear predictor-feedback concept, which enables one to design efficient feedback laws that compensate constant input delays arising in nonlinear systems was originally introduced ✩ [cit] american control conference, may 24-26, 2017, seattle, wa, usa. this paper was recommended for publication in revised form by associate editor hiroshi ito under the direction of editor daniel liberzon."
"we surveyed a total of 1019 students, and 715 students completed either the pre-or post-survey or both. we report here on the 471 respondents (120 for 1a, 246 for e, 105 for ne) who completed both the pre-and post-surveys. the response rates of the pre-survey for 1a, e, and ne are 59%, 71%, and 83%, respectively; the response rates of the post-survey for 1a, e, and ne are 42%, 42%, and 83%, respectively."
"and α is the connectivity coefficient, which denotes the fraction of materials flux flowing from the final stage to the buffer when the rate of losses is assumed to be known and equal to 1−α (see fig. 1 ). the queue is characterized by a maximum processing capacity q max and a maximum service rate µ. therefore, we consider that the flux at the output is given as"
"the solution to (91)-(93) corresponds to the constant solution along the characteristics lines (x(τ ), t(τ )). integration of the odes (91) and (92) yields the characteristic curves of the pde (2) given as"
"the majority of prey pursuits occurred in areas that featured anthozoans, principally sea anemones (figs 1 & 3) . anemones are known to play an important role as refugia and feeding habitats for small fish [cit] and could therefore be another indicator for locally increased biodiversity. brittle stars on the other hand, although equally abundant, seemed to be of lesser relevance with regards to prey encounters. so, it appears that examining the composition of the benthic habitat alone might enable assessment of which prey types penguins are foraging for, though more data are required before conclusions can be drawn. however, this already hints at the potential for wide-ranging habitat analysis of at-sea movements in benthic predators, provided that spatial distribution of the different benthic habitats can be obtained. while in our specific case, no such habitat maps exist, planned further deployments of video loggers are expected to provide the necessary environmental information."
"within these frameworks of cognition, beliefs and attitudes towards the learning process and material being learned impact the process of building understanding. as such, we must examine the role that students' attitudes and beliefs towards mathematics, including enjoyment and confidence, play in student success in calculus. [cit], there is no specific or common definition of \"belief\" or \"attitude\" since these terms \"are not directly observable and have to be inferred, and because of their overlapping nature\" (p.96). other researchers hold that it is neither possible nor necessary to unify these different concepts of attitude and belief since different research problems can require different definitions [cit] . [cit] and the definition of attitude and belief, specifically that they include enjoyment and confidence as components, in their work. they formulated the definition of an attitude towards mathematics as the positive or negative emotional disposition towards mathematics and the definition of a belief towards mathematics as one's level of psychological acceptance of the truth and value of mathematics and learning of mathematics including the usefulness, relevance, and worth of mathematics in one's life now and in the future. with these definitions, enjoyment refers to the degree to which students enjoy working in mathematics and mathematics classes, and confidence refers to students' confidence and self-concept of their performance in mathematics. structurally, we note that within psychological studies [cit], the notion of beliefs and values is considered to be precursors to attitude and that it is the latter that then constitutes a predisposition to action. the definitions of fenneman and sherman align with this structure in the sense that a student's beliefs about mathematics will inform their attitudes by contributing to the positive or negative emotional framework for engaging in mathematical practice."
"the present work deals with the problem of compensation of transport pde actuator dynamics with boundary-value-dependent propagation speed in nonlinear systems. equivalently, the nonlinear ode's actuator dynamics are described as a delayed-inputdependent input delay. [cit], but is dependent on the delayed rather than the current input."
"the survey questions in the instruments are mostly likert scale prompts in multiple formats. for the 4-option likert scale questions, the response options ranged from level \"1\" to level \"4\" and were coded with numbers from 1 to 4. for the 6-option likert scale questions, the response options ranged from \"strongly disagree\" to \"strongly agree\" and were coded with numbers from 0 to 5. we analyzed data using factors identified and validated by the cspcc study: beliefs, attitudes, confidence, enjoyment, and desire to continue studying mathematics (table 2) . we ran anova tests to compare sample means of students' responses on each factor in order to identify significant differences and similarities in survey responses across the three different instructional settings, instead of building a model relationship. the pre-and post-surveys provided identical statements regarding student attributes including attitudes, beliefs, mathematical confidence, enjoyment, and desire to continue to calculus ii. we compared responses to questions that appeared on both the pre-and post-surveys (tables 5 and 6 ) for their total change within each course structure cohort. for each statement, we compared course population means to identify the presence of statistically significant differences in the pre-and post-survey responses."
"the paper is organized as follows. in section 2 the general problem is described and the main result together with a global stability proof, based on a pde representation of the predictor-feedback control law, is presented in section 3. an alternative representation of the actuator dynamics as an implicitly defined delayed-inputdependent input delay and the associated delay compensator are given in section 4. section 5 is dedicated to the application of the designed control law to a pde model of production systems enabling a delay-compensating ''bang-bang'' feedback law. concluding remarks are stated in section 7."
"therefore, pde representations [cit] are more efficient computationally, more robust and more accurate for predicting the wip as well as the outflux behaviors in production systems. in this section, we present a pde model of the dynamics of a production system consisting of a chain of outgoing suppliers connected to a queue in front of it. the supply chain is described by a pde with boundaryvalue-dependent propagation speed [cit] and the load of goods stored in the queue is modeled by a nonlinear ode whose input is given by the value of the pde at the exit of the production line [cit] ."
"consequently, using (99) and (101), the original cascade system (1)- (3) is reduced to a nonlinear system with an implicitly defined delayed-input-dependent input delay, which is written aṡ"
the video logger was deployed on a breeding male yellow-eyed penguin tending two chicks on 17 [cit] . deployment occurred at the penguin's nest on the evening of 17 december. the bird was removed from the nest and placed in a cloth bag to reduce stress. the instrumentation procedure lasted around 20 min after which the penguin was released back on its nest. the bird left on a single foraging trip on 18 december before the device was recovered on 19 december; the penguin continued to breed normally after the deployment.
"specifically, we find that students in 1a have lower levels of confidence and enjoyment compared to ne and e. furthermore, between e and ne, students in e show a higher level of confidence and enjoyment than those in ne. there is a small decrease in these three attributes in 1a from the pre-to the post-survey. on the other hand, we observed dramatic decreases in students' confidence and enjoyment in e and ne. we observe a decrease in students' desire to continue in mathematics in e, but not in ne, and the negative effects on these three attributes in e are much greater than in ne. student beliefs and attitudes towards mathematics in 1a were observed to remain almost constant. these differences suggest that there is some impact of the course structure on these student characteristics over time, so we turn our attention to this in the next section."
"our observations of flipper movements, i.e., strong flipper movements at the beginning of a dive that decrease with depth, and cessation of flipper movements during ascent, align with findings reported in other penguins. [cit] found that king penguins showed vigorous flipper beating at the beginning of a dive to counter positive buoyancy. with increasing depth, air volume in the penguin's body becomes compressed, reducing its buoyancy so that fewer flipper beats are required. that this also applies to flipper amplitude (fig. 4) was not detectable by using body acceleration as the only measure. a more elaborate system of sensors and magnets attached to flippers was used on magellanic penguins which allowed the recording of both flipper amplitudes and beat frequencies [cit] . however, the system is known to be prone to failure, rendering the use of back-mounted wide-angle cameras a much more reliable alternative. flipper beat frequencies and amplitudes are directly related to energy expenditure [cit] . they provide the means for the quantification of energy budgets [cit] and subsequently can be used to assess individual fitness in relation to foraging success and subsequent reproductive performance [cit] ."
"on the whole, prior to taking calculus i at our institution, our students had academic backgrounds that suggested that they would be successful in our courses and reported high levels of confidence, beliefs, enjoyment, and desire for more mathematics, even though the levels of these responses were not as high as those observed in the national study. also, students in the 1a \"stretch calculus\" demonstrated lower levels of agreement in these areas compared to the e and ne populations."
"defining the characteristic curves parameterized by some variable τ and expressing the total derivative of π(x(τ ), t(τ )), the solution of the transport pde (87) compatible with the boundary condition (88) is written as"
"judging from the total time the bird spent over a benthic environment dominated by coarse sand and sediment ripples (65.9% of total bottom time) as well as almost exclusive encounters of opalfish over such habitat (figs 1 & 3), it can be assumed that the penguin focussed principally on this species. blue cod encounters were associated with the presence of horse mussels. these large bivalves protrude from the seafloor and provide hard substrate for other epibenthic taxa, thereby increasing local benthic biodiversity [cit] . benthic habitat with increased benthic biodiversity is generally more attractive to a variety of benthic fish species, most likely due to enhanced feeding conditions [cit] . our video data also suggests that the fish use the bivalves and associated cavities as shelter to avoid capture (https://vimeo.com/179414777)."
"given the calculus i impact on student experience in stem programs, many large-scale efforts across the usa have focused on various aspects of calculus instruction and their impact on student persistence. researchers have considered several aspects of persistence in a number of contexts including general educational pursuits or towards the completion of coursework [cit] tinto, 1997 tinto, 2004 . in this paper, we characterize student persistence in the calculus sequence as the primary indicator of continuing in a stem major [cit] . [cit] framework of persistence, satisfaction in the integration of social and academic life in a community has a significant impact on persistence, and later, he asserted that this model also can be employed in the analysis of students' learning and persistence in classrooms as communities [cit] . he highlights that this satisfaction is of critical importance to students during their freshman year because it is a time when their \"membership in the communities of … campus is so tenuous\" [cit] . most students in the usa, especially those planning to major in a stem field, take calculus i during their first year in college. we hypothesize that the calculus i experiences of students in various versions of the course at our institution differ significantly and have the potential to affect their attitudes and beliefs towards mathematics as well as decisions about continuing to pursue a stem major in different ways during this critical time."
the study described in this work takes place at a large research university in the usa where students can enroll in one of three different versions of calculus i depending on their planned major and placement performance.
statistical analysis was carried out in r 3.4.2 [cit] . correlations were examined as linear models (pearson's correlation). comparisons were conducted as simple t-tests accounting for unequal variances [cit]
"we assume that individual sensor-nodes are untrusted. an adversary may introduce its own nodes in a network, or it may capture, compromise and re-introduce existing nodes in the network, as sensor nodes are generally not tamper-resistant. with those capabilities, the adversary attempts to launch pollution attacks to reduce the performance of the network and to consume the limited resources (e.g., battery power, memory) on sensor nodes."
"in this paper, we presented an approach that is able to defend pollution attack against reprogramming protocols based on network coding. this approach employs a homomorphic hashing function and an identity-based aggregate signature to allow sensor nodes to check packets on-the-fly before they accept incoming encoded packets, and introduces an efficient mechanism to reduce the computation overhead at each node and to eliminate bad packets quickly."
"a pi regulator is then employed for each area, aiming at bringing the steady state frequency deviation to zero. the output signal is first weighted by a participation factor yt (different for every generator and established within the balancing market), then fed into the governor summing point along with the droop signal as shown in fig. 5 . note that, the sum of all participation factors must be 1."
"two scenarios have been performed, in order to emphasize the importance of the energy storage systems in low inertia power systems. the model and the parameter settings have been developed to identify qualitative solutions."
"and i 2l (p) is the effective interference on the link l. we note that n and x s, respectively represent the primary outage status and the ingress rate before performing power update. hence, in this regard, they are fixed. since the objective of (33) is concave in p, any feasible transmit power vectorp with minimum total power satisfied the set of constraints (34) is the optimal solution of (33). in this sense, we propose a power control algorithm to solve the problem (33) via the link power updates"
"proposition 5: given (x, n), the iterative algorithm (37) always converges to a unique limit point p while keeping the interference on pu-rx below the tolerable limit via the pu outage price n (t) . † the congestion price update requires only the value of queue backlog on each link. † the pu outage price update is based on the ack/nack feedback information of licensed channel overheard by secondary nodes."
"the two-input boost converter without voltage multiplier cell is shown in figure 1 . the circuit is derived from the conventional boost converter. two converters are connected in parallel for the two inputs. two power switches s1, s2 in the converter structure are the main controllable elements that control the power flow of the hybrid system. the circuit topology enables the switches to be independently controlled through four independent duty ratios d1, d2 respectively. the diodes d1 and d2 conduct in complementary manner with switches s1 and s2. in hybrid power systems applications, the main aim is to achieve an acceptable current ripple in order to fix the output power on desired value. so, the current ripple of the input sources is reduced to make power balance among the input and load."
"more specifically, fig. 4 shows that the trajectories of probabilities converge to the outage thresholds 10 and 15% for both algorithms. for the performance comparison between two algorithms, sop-jrpc has a larger deviation during a transient time before converging to the fixed point. this deviation sometimes makes an excess of desired outage threshold. on the contrary, op-jrpc algorithm always keeps the primary outage probabilities below 10 and 15%. however, it takes a longer time for op-jrpc to reach the optimal point. fig. 5 also shows the trajectory of aggregated throughput. although sop-jrpc has a high variation during transient time, its convergence speed is much faster than those of op-jrpc and high-sir-based algorithm. we can realise that the aggregated throughput of both op-jrpc and sop-jrpc is almost indistinguishable and approaches to the global optimum at an error tolerance of 10"
"in this paper, we focus on the jrpc in crns with primary link protection. the main objective is to maximise the aggregate throughput of all sus without affecting the performance of pus in a distributed manner, that is, neither cooperation with the pus nor requirement of central controllers. therefore sus need to flexibly perform adaptation according to the time-varying nature of fading environments in order to keep pus' qos stable during their update interval. to cope with the severe fading effect, the outage probability [cit], which is defined as the fraction of time a pair of transmitter/receiver experiences an outage over fading blocks, is chosen to best evaluate pu's quality of spectrum-sharing services. consequently, the optimal values are not necessary to be updated whenever the fading channel changes their states. we propose a cross-layer optimisation framework by investigating the jrpc problem via network utility maximisation [cit] . our proposed model allows the sus to periodically perform their rate and power updates without depending on dynamic fading channel. our major contributions to address resource allocation in crns, which also demonstrate the difference from the existing studies are summarised as follows: † a cross-layer framework is developed in multi-hop crns with the outage constraint as a non-linear non-convex optimisation problem. by using the successive convex approximation method, the resultant solution has been proved to converge to the global optimum. † we propose a novel heuristic method to develop a practical sub-optimal distributed algorithm without explicit message passing. we have proved that the distributed algorithm is able to converge to the point near the global optimum. † another solution is also derived through high-sir approximation to show the inherent trade-off between throughput and convergence rate for the proposed algorithms."
"in the last years, the electricity gained the role of multipurpose vector, due to its possible penetration in mobility (especially with electric vehicle [cit] ) and heating production (e.g., with the use of heat pumps [cit] ). on the other hand, the constant increase of the share of renewable energy sources (res) can lead to decarbonization of the entire electricity sector, thus allowing a substantial decrease of greenhouse gases worldwide. however, to reach this point, the electric system will have to face several important challenges."
"the numerical results are displayed in figs. 2 and 3 to highlight the mutual relationships of rate control and power allocation via pricing. as depicted in fig. 2 that there is a difference in power allocation strategies of op-jrpc and sop-jrpc. it can be observed from (18) in op-jrpc algorithm that the transmission power per each link depends on not only its congestion level but also interference impact on the other links (including primary link). as a result, the congestion problem is simultaneously solved by both links (adjust their powers as shown in fig. 2a ) and sources (regulate their rates as shown in fig. 3a ) through the reflection of congestion prices subject to pu outage constraint. hence, the variation between two consecutive adjustments is very small. on the contrary, we can see from (37) in sop-jrpc algorithm that all links always conform their powers according to the changes of ingress rate demand (regulated by sources through congestion prices) subject to pu outage constraint. consequently, the variations between two consecutive adjustments for both rate control and power allocation higher as shown in figs. 1 -3b."
"a two-area power system is considered and modelled for frequency control studies. the two areas, generically denoted by i and k, are interconnected through a tie-line. both primary and secondary frequency controls are considered, as illustrated in the next sections."
"the principle of operation and analysis of different converters with modified dc-dc converter are presented in this paper. the high voltage gain with low duty cycle is achieved for the modified topology. the modified converter significantly minimizes the voltage stress in the two active diodes and switches with high voltage gain. this approach will lead to reduce switching and conduction losses and also to select mosfet and diodes with lower voltage rating respectively. furthermore, the topology explores the way to eliminate the usage of additional circuitry, complex control and also guide to uniform current sharing. from the analysis and comparison of results, the high step-up voltage gain with low duty cycle and reduced switch voltage stress modified converter is ap- propriate one for multi-input source applications."
"however, the network coding-based reprogramming protocols have a potential problem in hostile environments, where an adversary may attack the sensor nodes by launching famous pollution attack [cit], in which a malicious node can send bad encoded packets that consist of bogus data, which leads to erroneous decoding of a large part of the original data upon retrieval. for reprogramming protocols that are not based on network coding, several security approaches [cit] have been proposed. all these approaches are extensions to deluge. the technique common to them is the use of a single digital signature and off-line hash chains [cit] . this technique is not sufficient for network coding-based reprogramming protocols because each sensor node produces unique encoded packets which cannot be signed by a base station."
all simulations have been performed assuming a reference step power imbalance in area k of 0.01 p.u. occurring at 5 seconds from the simulation start.
"by considering frequency regulation, the indication of 100% res-based electricity system is not useful for understanding which type of control will be implemented in the future. in fact, the most important aspect to be considered for frequency is the existence (or not) of system inertia. the inertia is strictly correlated to the installation of rotating machines, e.g., turbines and generators, which are the basic components of the frequency regulation as it is now."
"we have built a simulator by castalia [cit] that allows us to study the damage that malicious nodes can cause in reprogramming system in wsns under different settings (e.g. new generation programming protocol based on network coding, classic reprogramming protocol, or noncooperative and cooperative environments)."
"from table ii we can see that the classic reprogramming system like deluge that do not use encoding only suffer from minor damage in the network and is independent of the probability of checking. however, this is not the case for network coding since malicious packets get quickly re-encoded in the network. from table ii we can see that with network coding and no cooperation, the damage of the system decreases linearly with the checking probability, which requires nodes to check almost every incoming encoded packet to have acceptable levels of efficiency. if cooperation is introduced, then the performance of network coding improves. actually, a cooperative mechanism improves the efficiency of the secure reprogramming system almost ten-fold for a checking probability of 2% (see table ii ). thus, with cooperation the system is able to limit the propagation of bad packets. moreover, observe that the percentage of bad packets is very close to the minimum, which in this simulation is 5% table ii shows the relationship between bad encoded packets and the probability of checking for a network of 100 nodes in which 5% of them are attackers. we study the case of network coding, and no coding, with a cooperative and a non-cooperative system. fig. 3 shows the efficiency of the system for a network of 50 sensor nodes where each node uses network coding and checks with 1% probability as a function of bad packets' rate. we observe that the efficiency of the system largely depends on whether a cooperative mechanism is in place, dropping to less than 20% if this is not the case. we also observe that the efficiency of the system increases linearly as the rate of infection of a malicious node decreases, achieving 90% efficiency for a bad packet rate of 10%. in recent years, solutions for securing deluge have been proposed and improved upon [cit] . they are mainly distinguished through their structure, granularity and strength of hashing. [cit] derive a hash tree only for the first page. this improvement drastically reduces the amount of overhead. there exists a scheme that uses a different one-way key chain for each hop from the base station [cit], with the disadvantage of transmission overhead increasing with hop count. merkle's one-time signature has been proposed as a rom-friendly alternative to digital signature [cit] . the trade-off is that the nodes need to be stateful."
"in the tth iteration, the most violated constraint is condensed at the optimum (x * (t), p * (t) ) by slo. this result is used to form the (t + 1)th successive convex problem p2 by updating the weight vector w l(t+1) (p * (t) ). through this procedure, the global optimum will be achieved when all constraints in the original problem p1 are satisfied according to the certain convergence criteria, for example,"
"besss are very well suited for fast frequency control thanks to their speed and precision in regulating their active power in comparison to traditional generation (tg) units. in fact, tg plant governors have some difficulties to follow fast frequency swings, and power changes could go against the frequency control signal [cit] . in the case of primary frequency control (pfc) some studies pointed out that pfc is already profitable for besss, while other studies forecasted that the profitability will come in the near future, both alone or in combination with other grid services [cit] . in germany and uk the ancillary service for frequency control has been already opened to storage systems, and batteries represent the major part of new resources collected [cit] . technology and safety reasons make installations expensive and besides severe state of charge (soc) dynamics can affect their useful life [cit] (optimal use and design need to be assured)."
"the previous simulations have been done for time response of the bess of 0.1 seconds. however, different bess technologies may have different time reactions. in the following, the input signal of the bess has been set to 0.2δf+0.8df/dt, and three different time constant for the bess have been analyzed the fast reaction time is needed in order to compensate the low inertia caused by the replacement of the thermal power plants (characterized by large inertia) with power electronic based generation units (characterized by low inertia)."
c. third operation mode: in this mode switches s1 and s2 are turned on and resonant inductor lr and the output diode do current reduce linearly to zero. thus the reverse recovery current of the output diode is also minimized.
"one possible solution to improve computation time is to verify encoded packets in batches, either probabilistically or periodically. in such solution nodes do not check every encoded packet, but they check a window of encoded packets all at once. batching is possible thanks to the homomorphic property of the hashing functions. let's assume we have a window of j encoded packets to verify all together. we can build a advantages of using batching is that nodes only need to check one pktr packet rather than l packets. however, this batching scheme is exposed to a specific byzantine attack, which is called the pairwise byzantine attack [cit] ."
"in this mode s1 and s2 are turned off, when output diode is blocked, dm2 conducts and transfers the energy stored in the capacitor cm1 to cm2. when there is a balance of energy between the multiplier capacitors, the diode dm2 is blocked (t4) also with low di/dt. during the switch turn on the input inductor stores some energy as the classical boost."
the modified two-input boost converter with voltage multiplier cell is shown in figure 4 . the circuit is derived from the conventional boost converter. two converters are connected in parallel for the two inputs and a two stage voltage multiplier cell is inserted as common between the output sides of both converters.
"as far as we know, there is an ibas scheme [cit] which is suitable for secure reprogramming in wsns although it has one too strong assumption, i.e., all signers must use a same unique string when signing. however, this assumption can be satisfied in the wsn secure reprogramming service because a unique imgid or every code image is known to all nodes."
"978-1-5386-3669-5/18/$31.00 ©2018 ieee the paper is organized as follows. section ii presents the rationale behind the scenarios definition. section iii shows the two-area power system and all the components used in the simulations. section iv presents the simulation results, and finally section v reports the final remarks."
", ∀l [ l denote the capacity constraint for the approximated problem p2 and original problem p1, respectively. the following three propositions will be the theoretical factual foundations for us to strongly conclude that op-jrpc algorithm converges to the global optimum."
"in most power systems, the hydraulic power plants are the most suitable for the secondary frequency control because they have the best performances in terms of reserve deployment. however, as shown in fig. 7b, their reaction time in transient conditions is higher than those of the thermoelectric power plants. this inconvenience can be overcome by using a battery energy storage."
"we further assume that the channel is block fading, that is, f lk are constant within the duration of power update interval, but vary independently over time scale of interests. as a result, the instantaneous shannon capacity of link l is given by"
"a. first operation mode: in mode 1, the switch s1 & s2 are turned on. so the input voltage vin1 charges the input inductor l1 and input voltage vin2 charges the input inductor l2. due to this, there is no conduction in diodes d1, d2.so that the load is isolated from the input."
c. third operation mode: in this mode switches s1 and s2 are turned on and the resonant inductor lr and the output diode do current reduces linearly to zero. thus the reverse recovery current of the output diode is also minimized.
"v. conclusions this paper proposed a control model for the participation of besss to the frequency control. in order to evaluate the proposed model, a sensitivity analysis have been conducted. the simulations have revealed the following aspects:"
"in this paper, we have formulated the optimisation problem for a cross-layer design of multi-hop crns under primary user outage constraint. as the link capacity constraints are neither convex nor concave, the formed optimisation problem is known to be n p-hard [cit] . our first solution (i.e. op-jrpc) is to solve a sequence of approximated convex problems so as to achieve the global optimal source rates and link powers for the original problem. however, its much slower convergence speed and explicit message passing requirements force us to propose a heuristic solution (i.e. sop-jrpc), which circumvents the aforementioned drawbacks. finally, we also revisited a high-sir-based method to back-substitute the optimality of the proposed algorithms."
"the rest of paper is organised as follows. section 2 presents the system model and problem formulation. section 3 introduces the optimally distributed algorithm based on the successive convex approximations method. in section 4, we develop a new and practical rate and power control algorithm in order to obtain fast convergence speed with no explicit message passing. high-sir approximation problem is derived in section 5. numerical results and conclusions are illustrated in sections 6 and 7."
"we consider the vector w l (l [ l) in problem (12) . it is obtained from any initial power vector p(p [ p) to keep c l (p, w l ) in solving p2. however, link capacity c l (p) in p1 is continuously adjusted towards optimality with power control policy. therefore the optimal solution achieved from slo for p2 may violate some certain constraints in p1. in this regard, the solution can be further improved towards the global optimum by solving a sequence of approximated convex problems formulated from p2 via the following op-jrpc algorithm."
"owing to the coexistence of both pus and sus on the same frequency band, the total interference caused by sus can make the primary link outage. to guarantee the pu's qos, this interference at pu-rx should be maintained at an acceptable level. in a rayleigh-fading environment, sus need to perform power adaptation according to the timevarying nature of fading environments in order to satisfy pu's qos requirement during their power adaption interval."
"we assume that nodes check blocks with probability pr. encoded packets that pass the check are marked as safe, while encoded packets that have not yet been checked are kept in an insecure window. encoded packets are checked in batches. the batch window is equal to the insecure window. whenever a node verifies its insecure window, valid encoded packets are marked as safe and the insecure window is reset."
"we assume the source of the code images, i.e., the base station, is a powerful node (e.g., a laptop) with sufficient energy supply and the only trusted entity in the network. we assume each sensor node has unique identification information (e.g., mac address or production number). we assume while each sensor node is resource constrained, it has sufficient memory to store the security mechanisms our approach adds. we assume the packet size is large enough to hold a signature and other information required by a signature packet. this can be satisfied on sensor platforms with ieee 802.15.4 compliant radios [cit], where the maximum payload size is 102 bytes."
we also note that slo is implemented in a distributed manner: † source algorithm can preserve the existing tcp congestion mechanism in which source s adjusts its rate using (17) via the aggregate price l s for all links in the path s.
"where i e and k e are the voltage magnitudes at the two terminals of the tie-line, l x is the tie-line reactance, and δ i and k δ are the voltage angles at the two terminals. note that, if ik p δ (power flowing between areas i and k) is positive for one area, it will be negative for the other area."
"to reduce the cryptographic work at each sensor node while preventing malicious packets from infecting large portions of the network, we introduce cooperative mechanism where nodes cooperate in checking malicious incoming packets. by having a number nodes checking at every point in time and making them cooperate, expensive homomorphic hashing can be applied less frequently, hence, improving the performance of the network. next we describe the details of how such efficient mechanism works."
"the dual problem (24) can be solved using subgradient projection method [cit], where the congestion prices l l (l [ l) and pu outage price n are adjusted in the descent direction of subgradients ∇ l l(x, p, l, n) and ∇ n l(x, p, l, n)."
"in this mode s1 is turned on and s2 is turned on, when output diode is blocked, dm2 conducts transferring part of the energy stored in the capacitor cm1 to the capacitor cm2 in resonant way. when there is a balance of energy between the multiplier capacitors, the diode dm2 is blocked (t4) also with low di/dt. during the switch turn on the input inductor stores some energy as the classical boost."
"the simulation also shows that the thermoelectric power plants (reheat and non-reheat) are capable to provide better response than the hydroelectric units for frequency stabilization, in droop control (fig. 7b) . then, the powers are controlled to reach a steady state value within the agc level, equal to the participation factor set by the balancing market."
"the primary control is simulated through the droop control signal, as illustrated in fig. 5 (in red) . using the droop equation, we achieve the contribution of a generation unit t for frequency stability, that is:"
the two-input boost converter with and without voltage multiplier cell is shown in figure 3 . the circuit is derived from the conventional boost converter. two converters are connected in parallel for the two inputs and voltage multiplier cell is inserted in the output side of one converter.
"the preceding solutions only consider the authentication of data packets. a μtesla-like scheme has been proposed for authenticating the 'inject', 'reboot', 'erase' commands [cit] . another scheme [cit] has been proposed for authenticating advertisement (adv) and request (req) messages. despite the progress in secure reprogramming, none of the aforementioned approaches can be used for network coding based reprogramming protocols because in these approaches, the packets of a page are transmitted as is (i.e., not encoded). alternative approaches are necessary."
"to solve this problem, we create a batched encoded in our efficient mechanism, sensor nodes do not rely on other nodes to mark encoded packets as safe. however, they actively cooperate with other nodes to detect malicious encoded packets. whenever a sensor node detects a malicious incoming encoded packet, it sends an alert message to all its neighbors. to prevent sensor nodes that have not been infected from processing the alert message, a given node keeps an insecure-activity table with the id of i) those nodes that downloaded incoming packets encoded with insecure window encoded packets, and ii) those nodes that delivered the encoded packets inside the insecure window. alert messages are propagated from one node to another until all infected nodes are informed. if the insecure window is empty, alert messages are not processed. alert messages are processed as soon as they are received. however, alert messages are only propagated after the node is convinced that a malicious encoded packet exists. duplicated alert messages can be received for the same malicious packet since overlays often contain loops. however, such duplicated messages will be discarded when i) the insecure window is empty, or ii) the duplicate message comes from a sensor node that is not in the insecure activity table. in addition to alerting its neighbors, a sensor node takes the following actions: i) it puts encoded packets in the insecure window in isolation to be checked and cleaned in the background, ii) it stops using encoded packets in the insecure window for network coding, and iii) it starts checking encoded packets with probability one until the insecure window is secured and cleaned, thus, preventing new malicious incoming packets from infecting the reprogramming system."
"boost converter with high gain finds applications in various renewable energy sources such as solar energy systems, fuel cell systems and also in electric vehicles. mostly two types of dc-dc converters are present and they are: transformer less (isolated dc-dc converter) and with transformer (non-isolated dc-dc converter) [cit] ."
"the size and efficiency of the power transformer is a major constraint of the isolated dc-dc converters. some non-isolated dc-dc converters such as boost can provide increased voltage gain but the voltage, current stress and duty cycle are high [cit] . during the operation of the converter at high current and voltage levels, the diode reverse recovery current reduces the efficiency of the converter. some of the non-isolated topologies can work with high voltage gain as the quadratic boost and with auxiliary circuits can obtain soft-switching, but the switch voltage stress and the losses are high. the inclusion of voltage multiplier in low frequency rectifiers is a classical solution to increase the dc output voltage. this technique is also used in high-frequency isolated dc-dc converters, mainly for high output voltage (kv) applications as in travelling wave tube amplifiers (twta), reducing the problems presented by high-frequency and high-voltage power transformer [cit] . the voltage multiplier method can be combined with non-isolated dc-dc converters to attain new operation characteristics. the main advantages obtained are increased voltage gain, reduced switch voltage stress, soft switching and reduced reverse recovery current problems. the vmc works as a regenerative clamping circuit and it has low emi and reduced size. these options permit work with high voltage gain, less losses and compact circuit for applications wherever the isolation isn't essential [cit] . the boost and buck-boost converters are able to achieve high voltage gain with a large duty cycle. however, the voltage gain is limited by the size of power devices, inductance and capacitance parameters [cit] . also, the switch voltage stress is high, which makes the lower-voltage and high performance devices inappropriate. moreover, the large duty cycle induces high-current ripples, which in turn increase the conduction losses and induce reverse-recovery problem which in turn reduce the efficiency and limit the power level [cit] . cascade boost converters or cascade three-level boost converters are attractive solutions [cit] . but they are very complex because they need two sets of power devices, magnetic cores and control circuits. and the stability of the cascade structure is another concern [cit] . a severe rectifier reverse-recovery problem occurs in the second stage because the diode used in the second stage needs to sustain high voltage level. so the efficiency is low and the emi noise is serious."
"in this framework, this paper defines more precisely the values to be used in scenario a. the implemented scenario a is compared with a base case, which also considers thermoelectric power plants in operation."
"† from link algorithm, the su-rx of link k locally measures sir k (p) and broadcasts its control message rxctrlmsg containing m in the dual problem (14), the objective function is differential for all l and n. therefore we can also apply the projected gradient-descent method [cit] to solve the minimisation problem (15) via link congestion price updates (19) and pu outage price updates (20) . from propositions 1 and 2, we conclude that slo solves p2. for any initial values of primal and dual variables and the step-size satisfying"
"then, the power allocation subproblem (30) is equivalent to seeking a feasible power vector p to minimise the total interference impact on pu-rx while meeting the incoming rate demand constraints (32)"
"b. second operation mode: in mode 2 operation, the switch s1 & s2 are turned off. so the diodes come to conduction and the input inductors current flows through diode d1 & d2 and capacitance c01 & c02 to load so the capacitors c01 & c02 gets charging. when the switches are off, current in the diodes is reduced to zero. so the diode d1 & d2 is blocked. it will minimize diode reverse recovery current also with low di/dt. so that capacitors are discharges the energy to load."
"in fig. 6, we shows the achievable sir of all links against pu outage thresholds. we assume that pu-rx can successfully decode the received signal at sir threshold 0.41 db. with its fixed transmission power 20 dbm, primary link achieves the maximum sir of 15 db on the rayleigh-fading channel. hence, the primary sir budget remains larger enough to tolerate more interference from su's access. the primary outage probability requirement indicates how much the primary link's sir budget should decrease in order to increase secondary system's throughput. hence, the primary outage probability can be considered as a controllable valve via cross-layer rate and power design to decide the total throughput of secondary system at the risk of primary outage. in this regard, we confirm that secondary system can optimally exploit the primary spectrum if the pu outage probability still below acceptable threshold in an underlay manner."
"inertia-less system operation is one of the critical challenges that for the power system operators. this is due to the very high share of power electronic-based renewable energy sources, such as photovoltaic (pv) and wind power plants [cit] . in such systems, any power disturbance will lead to a significant frequency deterioration and eventually instability problems. hence, call for new technological advancements is required to preserve a stable frequency profile and enhance system dynamic performance. for this aim, the battery energy storage systems (besss) represents a promising solution to compensate system shrunken inertia by providing virtual (or synthetic) inertia. this in turn, contributes with system frequency regulation."
"in this framework, the eu project re-serve [cit] aims to study the possible operation of the electricity system in terms of voltage and frequency control, in case of 100% res electricity production."
"in the physical layer, all sus can perform simultaneous twoway information transfer over a common radio channel known as division-free duplex [cit] . each transceiver is equipped with an omnidirectional dual-antenna, an adaptive rf isolator and an echo canceller in order to significantly mitigate the self-interference for full-duplex transmission. let h l denote the thermal noise power under bandwidth w at each receiver of link l. the instantaneous sir at receiver of link l is defined as"
a. first operation mode 1: in this mode both switches s1 and s2 are on. both the inductors are charged from their input sources vin1 and vin2. the current in both the inductors rise linearly. the diodes in different vm stages are reverse biased and do not conduct. the vm capacitor voltages remain unchanged and the output diode do is reverse biased. thus the load is supplied by the output capacitor co.
"since p2 is a convex optimisation problem [theorem 2], there exists a feasible point satisfying the slater's constraint qualification [cit] in its domain. from the strong duality theorem [cit], there is no duality gap. hence, the optimal solution can be obtained by solving the dual problem (14) via the following iterative optimiser."
"it is clear that the approximated optimisation problem p3 is convex. therefore the optimal solution can be obtained by using standard convex optimisation techniques. this result is used to compare with the performance of our proposals, that is, op-jrpc and sop-jrpc in section 6."
"the goal of this paper is to propose a new security approach that is able to defend pollution attack against reprogramming protocols based on network coding. to achieve this goal, we employ a homomorphic hashing function and an identity-based aggregate signature to allow sensor nodes to check packets on-the-fly before they accept incoming encoded packets, and introduce an efficient mechanism to reduce the computation overhead at each node and to eliminate bad packets quickly. this paper is organized as follows. section ii clarifies our assumptions and the threats which this proposal based. section iii describes our approach in detail. section iv details efficient mechanism. section v presents the simulation results of our schemes. section vi summarizes related work. section vii concludes the paper."
"besss could be used also to mimic the inertial response of synchronous generators. virtual synchronous generators [cit] are converters controlled for reproducing the dynamics of synchronous generators. the storage is needed to provide the energy necessary to control the active power output. various forms of control can be possible with different impact on the grid [cit] . this paper aims to highlight the impact of besss in low inertia power systems, when a proper combination of frequency variation signal and rate of change of frequency (rocof) signal is used as an input to the control of besss. the study is conducted by considering two-area power system dynamically modeled, and shows how besss can contribute to compensate system shrunken inertia and suppress effectively the frequency deviation during power disturbances. furthermore, a sensitivity analysis has been performed, for highlighting the influence of relevant input signals on sizing and the reaction time response of besss."
"the power system as a whole is modelled through the equivalent inertia h, and the load factor d. the purpose of this work is to analyze the behavior of a power system in terms of frequency control capability in the case of reduced equivalent inertia, based on the indications reported in section ii. for this reason, the simulations have been done for inertia values smaller than the traditional ones."
"the component related to δf is similar to the primary frequency control level (droop control), whereas the component related to rocof (df/dt) aims to simulate a virtual inertia. the two input signals are weighted by 1 k and 2 k factors. the battery dynamics is modelled as a first order transfer function [cit] which is suitable for power system stability studies such as the study presented in this paper."
"we let simulator of castalia generate the overlay topology of the network, where each sensor node has a constant number of neighbors k and the population size was fixed to 50-100 nodes. we randomly choose a portion of malicious nodes that generate malicious encoded packets to attack the reprogramming system. the percentage of the malicious nodes varied between 5-20% in our experiments. we also varied the percentage of bad packets injected by a particular malicious sensor node."
"we first make the estimation of pu outage probability such that sus can update pu outage price based on the feedback information on pu channel. let n t denote the number of outage events of the pu-rx observed at secondary nodes during a power update interval [(t 2 1)t, tt]. then, the noisy estimation of the pu outage probability,"
"cognitive radio networks (crns) have recently gained substantial attention for improving the spectrum efficiency and systems co-existence in wireless communications and networks. with the explosive traffic increase and highly dynamic wireless channel, it is very challenging to achieve the bandwidth and network efficiency in crns. efficient resource management is one of the key components in crns to address this challenge. it has been accepted in the community that the throughput in a multi-hop crn is tightly coupled with the lower-layer behaviours [cit] . network utility is dependent on congestion control algorithms at the transport layer, routing protocols at the network layer and channel assignments at the link layer, as well as power allocation at the physical layer. as a consequence, the overall performance of a multi-hop crn calls for a cross-layer design and optimisation approach. the major motivation in this paper is to provide high throughput and resource usage for secondary users (sus) via a cross-layer methodology without degrading the performance of primary users (pus)."
"within the project, two different scenarios have been suggested. the scenario a implies the use of both inertial generators (in particular, hydroelectric power plants) and inverter-based sources (pv and wind in particular), whereas the scenario b is entirely based on inverter-based power plants, (i.e., zero inertia generators). both of them aim to reach 100% res-based electricity generation, and both require the use of storage systems."
"to verify the authenticity of raw hash values at all intermediate sensor nodes in an energy-efficient manner, the best method providing such a property is an identitybased aggregate signature (ibas), in which different raw hash values can be authenticated by one single aggregate signature."
"in this mode s1 is turned off and s2 is turned off. the current in the diode dm1 is zero and this diode is blocked with low di/dt, minimizing the diode reverse recovery current. the resonant inductor current is same as the input inductor current and the energy of the input inductor is transferred to the load through the diode do."
"slo always converges to a unique point [cit] . since p2 is an approximated convex problem of p1 [theorem 2], any optimal point achieved by slo is the local optimum of p1."
"three types of governor-turbine models are used in the model, i.e., for hydraulic, reheat and non-reheat units. additionally, the bess is also considered for inertial control. (resulting from primary and secondary control), whereas the output is the variation of the mechanical power produced by the turbine. fig. 2 shows the governor-turbine generic model for a reheat unit, whereas fig. 3 shows the governor-turbine generic model for a non-reheat unit. block diagram of hydraulic unit [cit] ."
"the subproblem (30) shows that l p (p, l, n) consists of two parts where the second item is separable and convex while the first item is contrary in p. therefore we take into account this inseparable non-convex portion, then equivalently transform the non-convex subproblem (30) into a convex optimisation problem. for the fixed ingress rate, the link powers p must be controlled such that the capacity on each link is equal to bandwidth demand, that is"
"} indicate qos constraints for each source and the hardware restrictions for each node. the optimisation problem (p1) poses that the capacity of each link is a non-linear and neither convex nor concave function with respect to the optimisation variable, that is, the transmit power vector p. the capacity constraint on each link becomes non-linear and non-convex on (x, p) which makes p1 be a non-linear non-convex optimisation problem, and is generally not trivial to solve."
the importance of the mechanical inertia for frequency stability (in a system without storage) has been then analyzed. frequency variation in low inertia power system.
"in this mode switch s1 is off and s2 is on. all the odd numbered diodes are forward biased and the inductor current i lin1 flows through the voltage multiplier capacitors charging the capa- citors (c2, c5) and discharging the capacitors (c1, c4) . if the number of vm stages is odd, then the output diode do is reverse biased and the load is supplied by the output capacitor. however, if the number of vm stages is even, then the output diode is forward biased charging the output capacitor and supplying the load. in the particular case considered here, since there are four vm stages, the output diode is forward biased."
"and n are the lagrange non-negative multipliers. l and n are interpreted as congestion prices and primary outage price, respectively. theorem 2 indicates that the maximisation (14) can be decomposed into two subproblems with respect to primal variables x andp"
"are the functionally partial lagrangians. the congestion control subproblem max x[x l x (x, l) is still the same as the one in op-jrpc algorithm. the power allocation subproblem max p[p l p (p, l, n) is non-convex. therefore we propose a new heuristic method to solve this second subproblem in which local measurements are used together with ack/nack feedback information on pu channel."
p2 can be relaxed by transferring the constraints to the objective function in the form of a weighted sum. its dual problem is then expressed as an unconstraint max -min problem
"the normal sensor nodes cooperate to disseminate a new code image that has a large number of packets. the reprogramming system uses either protocol based on network coding, or classic protocol (e.g. deluge) without coding. the simulator uses a page-by-page dissemination strategy and in each page-round each sensor node can receive and decode at most one page packets. in each page-round, each sensor node with probability p verifies the validity of the encoded packets and, if one or more of them are corrupted, then the node removes them. if cooperative protection is used, then, sensor nodes behave as described in section iv to alert other infected sensor nodes, so that they also check their content. we measure the number of transmissions of correct encoded packets and the number of corrupted encoded packets. fig.1 and fig.2 respectively shows that the percentage of valid encoded packets and the number of infected nodes as a function of the probability of checking. we consider a network of 100 nodes in which 10% of them are attackers and send bad packets at a rate of 10%. fig.1 shows that the benefits of collaboration are achieved even for a minimum probability of checking (e.g. 1% provides 89% efficiency). larger probability of checking results in small increase in the efficiency, hence, not justifying the extra-computational effort. fig.2 shows that the average number of infected sensor nodes per round drops fast as we increase the probability of checking. figure 2 relationship between percentage of valid packets and the probability of checking figure 3 relationship between infected sensor nodes and the probability of checking table ii shows the relationship between bad encoded packets and the probability of checking for a network of 100 nodes in which 5% of them are attackers. we study the case of network coding, and no coding, with a cooperative and a non-cooperative system."
"the scenarios considered in this paper aim to represent a future condition of 100% res-based system: the implementation of this condition leads to consider several aspects of planning and operation of the system, as well as to overcome new challenges [cit] ."
"the proposed modified two-input converter has high step-up voltage gain with reduced component count, reduced voltage stress across the switches and diodes which in turn reduce both switching and conduction losses. the voltage and current stress of the devices are also less because of low duty cycle. this paper is organized as follows. the generation of the new converter family is presented in section 2, and the principle of operation is also given. an extension of the new topology to other topologies is also presented in section 2. the simulation results are presented in section 3. the comparison of different topologies is presented in section 4 and the conclusion is given in section 5."
a. first operation mode: in this mode s1 is turned on and s2 is turned off. the input voltage charges the input inductor lin1 and energy stored in the inductor lin2 is transferred to cm1 through the diode dm1. the resonant inductor current (i lr ) rises linearly from zero until to reach the value of the input inductor current (i lin1 ) and the current in the diode dm1 is reduced at same the quantity. the resonant inductor current which in turn charges the output capacitor co through the diode d0.
"while ctat provides us with significant aid for developing a tutor, notably tools for authoring reasoning rules for the system and behavior graphs for the exercises as well as a database-type tool for managing learners and their profiles (tutorshop), we will extend ctat's core architecture with other components to better meet the needs of our project. for instance, in order to adequately represent the domain knowledge taught in sti-dico, we will expand ctat's authoring tools with an owl ontology of concepts and skills which will enable us to represent the hierarchical nature and inheritance of the knowledge to be acquired by learners."
1. move idle virtual machines to the park server. 2. move a virtual machine from the park server to a computing node when an idle virtual machine becomes active. 3. pack virtual machines within the cluster to minimum number of servers.
"we can also see that the energy efficiency increases with higher load values: if we triple the submission rate, energy consumption increases by only 12 %. this is naturally explained by the relative high idle power of the computing servers. improved energy efficiency usually comes with a cost, as it is the case here as well. we can see this cost in the form of slightly higher processing times of physics jobs. in fig. 4, we have the run times of two different jobs, joba and jobb, with different submission rates. we can see that the static placement gives about 13 % shorter times than the two active management cases. as we can see in fig. 4, the active consolidation of vm instances (the amp method) does not have a significant effect compared to the normal am method."
"a host power management system was not implemented into this cluster, but we have previously shown that it is possible and beneficial to a batch cluster [cit] . instead of making a similar solution, we have measured the idle times of the test servers to show the benefits of active management of virtual machines. to work, the controller of the openstack cluster needs to be active all the time, thus we can only control the power states of the computing nodes. in fig. 11, we show the times that the computing nodes run without any virtual machines, i.e., idle and ready for energy- state change, e.g., shutdown. in these values, we have already taken into account the time it takes to switch from a state to another; boot up and shutdown. in the case of the test environment, the reboot time is approximately 76 s. therefore, this has been deducted from the idle times as many times as there are state changes. to improve the performance and energy efficiency in our test environment, the boot times could be optimized with operating system and bios tuning or with faster hard drives. since the idle time does not necessarily indicate the potential energy savings alone, we estimated energy consumption based on the measured idle times, as shown in fig. 11 . with these times and the idle energy consumption of the servers, 38w, we can calculated the energy-saving estimates, as shown in table 2 . the energy-saving potential, energy while idle, estimates how much energy could be saved by turning off the unnecessary servers."
"our plans for the near future include several components: 1) completing the interface of sti-dico in html5 and openedx; 2) converting the existing exercises and course modules in lexicology [cit] ) to populate our learning object database; 3) think-out-loud experimentation with experts in lexicology in order to validate the skills and activities chosen; 4) evaluation of sti-dico with future primary school french teachers completing their diploma at our university, 5) analysis of the results of the evaluation and improvement of the system."
"dynamic placement of virtual machines has received a lot of attention among cloudcomputing researchers and practitioners during recent years (for example, [cit] ). in practice, this means using a system that reacts to changes in virtual machine workloads in near real time by moving vms between physical machines. the basic idea is the same as with static virtual machine management: to fit the virtual machines in physical resources in a way that is either power efficient or energy efficient, or otherwise allows the virtual resources to run in an optimal way."
"the mapping instance of the os optimization problem is established as follows. the bid of each task is mapping to the value of each item, the offloading amount is mapping to the size of each item, and the deadline of each task is mapping to the capacity. since the capacity is fixed in the knapsack problem, we reduce the deadline of each task into the same value. it is easy to get that the mapping process can be done by taking o(n) time complexity."
"after tasks are scheduled by algorithm 2, we design a payment determination (pd) algorithm, summarized in algorithm 3 payment determination algorithm input: accepted offloading tasks in current time slot. output: payment for each accepted task. 1: for t a in t accept do 2:"
"in our tests, we measure the energy efficiency of high-energy physics (hep) analysis software in a dynamically managed cloud environment and compare it to the standard static cloud. hep computing used in the lhc experiment is both cpu and data intensive. it comprises mainly the analysis of large amounts of both generated and measured data. the generated data are produced with simulations and compared against the real data measured by the experiments at cern. a single high-energy physics analysis can go through millions of events [cit] . this work can be parallelized easily, as the analysis of a single event does not depend on the analysis of another event. normally, the analysis starts from 600 to 1000 separate processes and each of these analyze a subset of input events. physics events are stored in a database, such as container, root files [cit] . input file sizes in the root format are about 100-300 mb, and each file contains 700 [cit] events. an average analysis job would pass through 10-25 million events that are stored in 30,000-40,000 files."
"virtual machines are considered to be either active or idle. idle would be a state, where the virtual machine is waiting for work, and in the active state, it is processing a given workload. depending on the activity of a virtual machine, it is placed either on an energy-efficient park server or on a computing node. idle virtual machines have minimal need for physical resources and it is possible to over-commit physical resources to them without affecting their quality of service. they can be stored into park servers, where they can wait for a more active state. this way the virtual machines that require more processing capacity can be served with more dedicated hardware, and the virtual machines that are waiting for more work take less space, as they can be packed more densely. active virtual machines need more resources, and the less they need to contend for physical resources the better the service level."
"furthermore, it is assumed that each task can only be offloaded to one selected mec server and each mec server can operate only one task at the same time due to limited computation resources. in addition, all task executions are atomic operations, where tasks run completely independently of any other processes. the computation resource of mec server is divided into different time slots with equal length, which is determined by applications. for simplicity, we first focus on the current time slot for the offloading schedule, then we can easily expand it to the consequential time slots. table 1 lists the frequently used variables and their notations in this paper."
the aim of the openstack test was to show that the load balancing method works in practice and migrations do not cause a significant increase for energy consumption or processing time. the energy-saving potential of the method will be demonstrated using simulations.
"in our cluster, we have two types of servers; park servers and computing nodes. the park server is used to store idle virtual machines and its over-commit ratio is high, i.e., there can be multiple virtual machines per cpu core. instead, computing nodes are used by active virtual machines and have one to one mapping of virtual and physical resources, i.e., no over-commitment is used."
"as assumed, active management of virtual machines does not come without a cost, but the cost is reasonable. figures 3 and 4 both show that total energy consumption is higher and job durations slightly longer when the virtual machines are moved in the cluster. figure 3 shows the energy consumption of three different test cases; sp test case, where no active management is used, am, where the first two functions of our load balance manager are used, and then amp, where the third, packing, function is added. we can see that the migrations do not introduce much overhead to the energy consumption or processing times. the energy consumption measurements do not take into account the benefit gained by shutting down the unnecessary servers. this benefit is presented in table 2 at the end of this section."
"we have structured the rest of the paper as follows. first, in sect. 2, we cover basics of cloud computing and existing cloud management systems. in sect. 3, we introduce our load-balancing algorithm. next, in sect. 4, we describe the test software and test environment used in this study, which is followed by the results given in sect. 5. finally, we end with conclusion in sect. 6."
"for the back-end intelligent components of our its, two authoring tools were examined: gift (generalized intelligent framework for tutoring) and ctat (cognitive tutor authoring tools). from an architectural point of view these two systems are different and therefore offer different means of integration with our lms interface. in the current section, we will discuss the advantages and disadvantages of each and justify our final choice of authoring tool."
"all of the energy sources stated in the beginning have small energy density values compared to more classic energy sources, such as batteries. in the past, the use of radio transceivers often implied large amounts of power consumption. this is no longer the case today, as recent advances in the design of low-power electronics and energy storage have made wireless sensor networks a prime candidate for the successful integration of energy harvesting techniques. by analyzing the data from table 2 we can see that solar cells offer the best efficiency while at the same time being an environmentally-friendly power source. in overall, photovoltaic energy conversion is a well-known integrated circuit compatible technology that offers the highest power output levels, when we compare it with the other energy harvesting mechanisms. however, its power output is strongly dependent on environmental conditions, that is, varying light intensity due to the placement (indoor/outdoor). this can be a suitable energy source for locations in which the availability of light to network nodes can be guaranteed to a sufficient degree and for which battery supply is impractical. for indoor lighting, the main source of energy is the lighting system itself. this may seem strange, but a solar energy harvesting system can collect the light and store the energy in a battery for use when it is dark, keeping a system running for year without available."
we evaluated and tested the functionality of our load balancing algorithm in a small openstack test cluster and the scalability of the solution by using the cloudsim simulator software. cloudsim was complemented with our load balancing algorithm and modified to support heterogeneous hardware.
"from an architectural point of view the main challenge will be the scalability of the solution. with very large numbers of users, a server-based tutor engine can be faced with severe server load. to tackle this issue in their ctat-edx experiment, aleven and colleagues developed a javascript version of the tutor's inner loop in order to distribute a large part of the computational workload to the student's computer. a long-term solution for the scalability problem is to distribute the workload of the ctat/tutorshop server from one unique server to many identical servers behind a load balancer, which will allow horizontal scalability. while this solution would require some significant software re-engineering, it is feasible and stable. we have also developed a contingency plan involving the usage of the js input mechanism if ever lti integration becomes an issue."
"in table 4, the detail comparison of our proposed solution with relevant works in wireless home automation system is shown. our proposed system enables home users to check the home automation device status and control them remotely from the globe. in addition, the use of 6lowpan communications technology also helps lower expense of the system and decreases complexity of the home automation architecture. and then, by implementing solar energy harvesting we have 6lowpan-based home automation systems which have long periods of life without the need for periodically replacing their batteries."
"computing clusters, in general, consist of a powerful front-end, special hardware for storage, and a homogeneous group of computing nodes. in the case of physics computing at cern, the computing nodes are equipped with 2 gb of memory per computing core. processors are usually from the mid-range of the current processor family, because they are powerful enough but not overly expensive. high-energy physics computing at cern is performed using batch systems in a distributed way. the significance of a single node is minimal, and in the case of node failure, its load can be re-run on another node."
"the improved energy efficiency has a drawback as the total amount of work and service level decreases. figure 15 shows how the total amount of work, measured as the instruction count, is affected by the change of the park server threshold and the number of park servers. similarly, fig. 16 shows how the service level decreases when the utilization level of park server increases. in this case, the sla (service level agreements) describes the percentage of completed instructions, i.e., the service level."
"we would like to reproduce and extend the ctat-edx experiment using different, more complex domain knowledge and a more complex its architecture. our first prototype of sti-dico will be built using ctat, hosted on the cmu servers, which will be called upon by the course web pages hosted on local open edx servers. the tutor will therefore be able to seamlessly run in open edx course pages. a simplified version of the proposed tutor architecture is shown in figure 2 ."
"3) utility of all offloaded users: we record the utility of offloaded users to verify the property of individual rationality. 4) execution time: the execution time of ato auction is the time to schedule offloading tasks plus the time consumed on payment determination, which represents the computational efficiency of mechanisms. 5) approximation ratio: this metric mainly shows the performance of aos algorithm. it illustrates how both greedy offloading schedule algorithms approach to the optimal solution (symboled as opt), respectively."
"when faced with such a text and a sufficiently vague guidelines (\"improve the quality of this text\"), the users must not only recognize the presence of redundant words, and therefore the need to find synonyms, but they must also be able to find the corresponding dictionary section that presents a word's synonyms, and finally replace the redundant words with their synonyms."
"in this section, we give an overview of the ato, system model for the task offloading in edge computing scenario and formalize the offloading schedule problem."
"the nova scheduler handles resource balancing in openstack. the nova scheduler comes with three different algorithms: a simple, random, and filter scheduler [cit], but it also provides a possibility to add new schedulers as plugins. openstack does not include a dynamic load-based virtual machine manager that would migrate virtual machines between physical hosts. our load balancer extends nova schedulers functionality and provides an active load-based manager that commands nova with openstack api."
"the input of the virtual machines is periodical. constant idle periods are separated with higher load periods that resemble real physics workload, a physics job. some randomness was added to the beginning of the input, so that the first high load period of a virtual machine would be in different spots on every virtual machine. three input sets were created with different idle periods, such that we would have 50, 100, and 150 physics jobs of 8.5 min in one simulation. in every input set, the simulation duration was the same 24 h, 2880 scheduling intervals. we created five variations of every test set to test the effect of the different random seeds. the input set randomness had little effect, and the deviation between results with different random seeds was in the magnitude of 1000."
"another disadvantage of gift is that it currently only supports plug-ins with a small number of interface types, notably those created using microsoft powerpoint and visual basic, and is not up to par in terms of interoperability and authoring services such as knowledge tracing [cit] ) . that being said, in recent years, gift is being re-engineered in order to integrate web services and support lti. however, this re-engineering is not complete at the moment of writing this article (personal communication from k. [cit] ). since an integration of an its tool with open edx would require it to have full web capabilities and lti capacity, it is impossible to consider gift as adequate to develop the intelligent back-end of our its."
"hence, it is obvious that q is a solution of one knapsack instance, if and only if it is a solution of the mapping one of os optimization problem. moreover, the reduction from knapsack instance to os instance is in polynomial time."
"the basic function of the cloud-computing framework is to enable the users to create and destruct virtual machines (vm) in a cluster of physical machines. these virtual machines can be created in different sizes and placed on physical machines, hypervisors, having the required resources; cpu, memory, network, etc. using a virtualized system for consolidating it services, such as the cloud framework, is already more energy-efficient than having just a single service on each physical server, as it has traditionally been done in computing centers. the consolidation of vms alone would be enough, if the workload of the virtual machines were constant, but this hardly is the case. workload changes as a function of time, and so the initially optimal placement of vms may no longer be optimal. thus, the location of virtual machine in a physical server needs to be changed."
"the adaptation of virtualization has resulted in a plethora of both commercial and open-source virtual machine management tools. at first, these tools were mainly made to control single hypervisor systems or to help system administrators to manage their hypervisor clusters. cloud management frameworks making infrastructure as a service (iaas)-type services possible were the next step in this evolution. they provide, on top of the basic hypervisor management, a large collection of user management and monitoring tools."
"ctat supports the creation of two types of tutors: example-tracing tutors and rule-based cognitive tutors, the key difference between them being that example-tracing tutors can be applied in problems that have a limited-branching solution space, and can be created without programming but using problemspecific authoring ( however, there are still issues regarding ctat. for instance, it is not an open-source software (although it may be used freely for research purposes), which prevents modification or personalization of its code to better correspond to the needs of its developers. furthermore, the stable versions of ctat are only compatible with flash and java technologies to build the tutor interface. using flash raises a host of issues (security, mobile device support, compatibility with browser plug-in technologies), whereas using java is complicated for non-programmers, which many its developers are. fortunately, ctat is now transitioning from flash to html5 for the tutor front-end, which will simplify the process of integrating it with open edx (personal communication from v. [cit] )."
"a work-in-progress research project of an intelligent tutoring system has been described, using a service-oriented architecture and integrating an lms platform with activities developed in html5. this system, sti-dico, enabling the learning of dictionary skills, will eventually be offered as an online course module for primary school french tutors studying at our university. based on existing components, such as an ontology of linguistic concepts (polguère, 2010), course modules in lexical didactics [cit] ) and empirical research on dictionary use [cit], the resulting its will be a both fundamentally sound and fully functional in terms of learner cognitive diagnosis and learner interaction."
"at the same time, we also measure total average current consumption measurements of our 6lowpan home automation system during transmission of packets by using the oscilloscope. summary results of total energy consumption for the data transmission without ack, with application ack, data polling, and combining data packets are shown in figure 8 . our result proves that, given enough storage capacity and enough incident radiation, solar energy harvesting can power a 6lowpan home node for an indefinite amount of life time. taking into account the fact that nodes employ power management in the software stack, alternating between long periods of sleep, and only short intervals when they are active, there is actually energy independence with excess energy produced. this additional energy is stored in the battery pack to be consumed during the night or on clouded days."
"as shown in figure 8, the frugality ratio increases with the expanded standard deviation of bids. the reason is that the price difference, of the task with maximum total price or unit price and the one with the absence of the selected task, is enlarged with the expanding fluctuation of users' bids."
"a third type of exercise proposed in sti-dico are those that aim to develop learners' dictionary-specific skills, for instance the usage of wildcards to search in electronic dictionaries (e.g. table* for all words that start with table), or interpreting abbreviations within a dictionary entry (e.g. 'n.f.' for nom féminin, feminine noun in french). these skills, while extremely important for successful dictionary usage, do not depend on any fundamental notions and are often dictionary-specific. we chose the most popular electronic dictionary in quebec, antidote, and limit the activities to its usage, while ensuring that the students acquire an in-depth knowledge of its functioning after having completed sti-dico's activities ( figure 5 )."
"after the tests in the physical test cluster, the load balancing algorithm was tested in a simulator. the simulator allowed us to test with a larger server count and study the use of heterogeneous hardware."
"to test our load balancer in a larger environment and to evaluate whether the results from our physical environment are also relevant in production-scale systems, a simulator was needed. because of different aims of the tests, the simulated results cannot be directly compared to the test with real hardware. we decided to use an existing java-based simulator, cloudsim, that has been widely used for simulating cloud environments [cit] . cloudsim provided most of the necessary features to simulate dynamic management of cloud infrastructure and collect energy consumption information [cit] . however, adding support for heterogenous hardware was needed to be able to simulate our test case. the load balancer that was described earlier in sect. 3 was also implemented into the simulator; it uses the existing algorithms of the simulator [cit] . in addition, the separation between normal computing nodes and park servers was implemented."
"we have presented in this paper our work to answer the issues mentioned at the beginning of this paper. we proposed the simple but reliable system that can control home international journal of distributed sensor networks 7 the important features of our 6lowpan-based home automation system are long periods of life and energy efficient, without the need for periodically/every 53 days replacing its batteries. we designed the 6lowpan home automation device with total energy independence by using 3 v, 70 ma small polycrystalline silicon solar cell for energy harvesting. given enough storage capacity and enough incident radiation, solar energy harvesting can power a 6low-pan home automation node for an indefinite amount of life time."
"difficulties and limits of our project include integrating the various components of the system and assuring their communication as well as the addition of functional dictionary excerpts to permit users to consult them during their learning process. furthermore, while we will save all learning data collected via our experimentation, we have not yet explored the potential of the project in terms of learning analytics and educational data mining. we plan to integrate this dimension in further work on sti-dico."
"the skyrocketing applications of icps, such as product inspection by deep-learning-based image recognition technology have placed severe demands on low-latency, high reliability and stability [cit], while cloud infrastructure fails to meet the above demands due to the network uncertainty. moreover, the above demands also urge burdens on communication and computation techniques of smart devices, which becomes the obstacles in terms of data processing, data latency and data access. these requirements involve the need for highly localized services at the network edge in local proximity to smart devices. in light of this, the mobile edge computing (mec) technology has emerged, volume 7, 2019 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/ with the goal of offloading the cloud services to the network edge. in addition, mec servers typically belong to different authorities and are profit-driven. hence, smart devices need to take the offloading cost, such as the payment to mec server for offloading service, into account when they decide whether to offload tasks or not. in this paper, we consider the general scenario of icps deployed with multiple mec servers in one smart factory, as shown in figure 1 where many assembly lines are on operation at the same time. smart devices (mobile robots, automatic guided vehicles-agvs) equipped within each assembly line generate multiple computation-sensitive tasks with heterogenous constraints on their response time. in order to offload the computation burden from the cloud to mec servers, the system needs the ability to deal with task offloading scheduling, such that mec servers can complete tasks within diverse deadline constraints. meanwhile, production lines require that the inspection system can handle online task flows. furthermore, there exists the limitation of access bandwidth for each mec server, and it needs to be taken into consideration when smart devices select mec servers for computation offloading. the current research on task offloading either ignores the security requirement of offloading tasks [cit], or fails to give considerations to both the heterogeneity of deadlines and the incentive of resource sharing for mec servers [cit] . the intelligent applications in icps have rigid diverse requirements on task offloading, including security of service, heterogeneity of demands and incentives of resource provision. for instance, the industrial data usually have security levels and offloading tasks have diverse constraints on latency. in addition, the mec server provided by the third parties need sufficient motivations to share their computing resource. therefore, it is imperative to design a new incentive mechanism for task offloading combined with security and heterogeneity in icps."
"proof: we prove the np-hardness of the os optimization problem by giving a polynomial time reduction to the knapsack problem, which is one of the classic np-complete problems."
"for this study, we set up a cloud test environment with monitoring system and measured the effect of active management of virtual machines. we used well-known open-source tools for the environment and realistic physics load. our tests show that we can save 9-48 % of energy when the virtual machines are managed actively based on the load of the system. the same algorithm was implemented into a simulator. simulation results also show that the energy efficiency is improved by up to 40 % with active management and with the use of heterogeneous hardware. the use of energy-efficient hardware in park server improves the energy efficiency about 4.5-10 %. the results also show that the energy savings of the active management depend greatly on the workload."
"after tasks are scheduled following the schedule list, we compute the running time of pd algorithm. in each iteration of offloading schedule in the set of t sort without the task under pricing (lines 3-4), the process is similar to algorithm 2 (lines 6-7). thus, the time complexity of finding the first task in t reject to replace the task under pricing is o(mn). moreover, the number of outside for-loop is at most m, since each for-loop completes the pricing process for eachxxxs accepted task. therefore, the payment determination algorithm takes o(m 2 n), which dominates the whole ato auction. it is obtained that the running time of ato auction mechanism is bounded by o(m 2 n)."
"we have previously found that virtual machines have very small impact on total load and power consumption when they are idle [cit] . the less load a single virtual machine has, a single physical server that can handle the more of them. in this paper, we introduce a load-balancing system that tries to maximize the load on computing nodes and minimize the amount of active-computing nodes. we achieve this with dedicated hosts for storing idle virtual machines, active monitoring of resources, and load-based active management of virtual machines in the physical cluster."
"physical servers were installed with ubuntu 13.10 and devstack, a stateless openstack installation. 5 virtual machines were installed with scientific linux at cern 5 (slc5), cern virtual machine file system (cmvfs) [cit] version 2.1.19 and oge 6.2u5. cvmfs is a new way to bring cms software (cmssw) [cit] to the computing nodes. it mounts the software to a local filesystem from cern servers and caches some of the used software locally as they are used. in our tests, a version 6.2.8 of cmssw was used."
"in our earlier work, we have found that virtual machines have very small impact on total load and power consumption when they are idle [cit] . as it is in the case of high-energy physics analysis, when a computing job is waiting for data and thus using very little cpu or memory resources on the physical host. similarly, the resources are unutilized when a virtual machine is waiting for the next job from a batch system. this waiting time could be spent on an energy-efficient server, a server dedicated for parking idle virtual machines, but still capable of network transfers. the less load a single virtual machine has, a single physical server can handle the more of them."
"as we can see, a lot of research has focused on amidst the dynamic management of a virtual computing cluster. all algorithms aim at minimizing the need for physical servers. some emphasize the optimal placement of virtual machines in the cluster and do this by performing complex optimization calculations. some take into account migration cost at a very high level and loose accuracy. head-to-head comparison of these algorithms is difficult. the choice of an algorithm depends on its application. even the heaviest algorithms might turn to be useful in an environment, where load does not vary very much, e.g., high-energy physics computing, where single analysis tasks can last days and their resource usage does not fluctuate during this time (see sect. 4.1.1). on the other hand, outside supercomputing, we have web services that have very fluctuating resource requirements. running these optimally in virtual machine cluster requires fast response from the management system."
"we believe in the success of our project due to the sound cognitive foundation of our system, the positive results regarding the impact of teaching of dictionary skills explicitly [cit] ., 2015) and using service-oriented architectures and web-based interfaces [cit] and will enable us to improve our system and its functionalities and assure its pertinence for the target audience."
"yuzhou wen received the b.e. [cit] . he is currently pursuing the master's degree with the school of information science and technology, zhejiang sci-tech university. his research interests include mobile edge computing, scheduling principle, and incentive mechanism."
"in order to interconnect home automation system, based on 802.15.4 and 6lowpan, with existing ipv6 network, based on ethernet/wi-fi, the home gateway can act as a bridge or as a router. in router mode, this home gateway acts as a full-fledged ipv6 router, interconnecting two ipv6 subnets. the home automation subnet is managed by the rpl protocol, and the ethernet subnet is managed by ipv6 ndp. in this mode, home gateway provides a virtual second interface to filter the packet. the router mode allows us to isolate 6lowpan mesh into its own subnet, therefore clearly identifying the home automation nodes. in bridge mode, this home gateway provides switching capabilities, allowing to interconnect a standard ipv6 based network with an rpl based 6lowpan mesh in one subnet. all incoming packets targeting an 802.15.4 interface or incoming multicast packets on the ethernet interface are forwarded to the home automation segment. conversely, all incoming packets targeting an ethernet interface or incoming multicast packets on the lowpan interface are forwarded to the ethernet segment. home gateway is acting as an ndp proxy on the ethernet side and is using ndp parameters to configure the 6lowpan mesh. source and destination mac addresses are translated, and addresses in icmpv6 packets are also translated. this mode allow us to seamlessly integrate a 6lowpan mesh into an existing ndp based ipv6 network and aggregate several 6lowpan meshes into one virtual ipv6 subnet."
"where is time for which device consumes average current, is total time period for which average consumption is measured, sleep is current consumption while in sleep mode, and avg is average current consumption over period . knowing, sleep, and we can find avg based on the period of active sequences. as a final step, calculate the total life time of our 6lowpan home device, and know that"
"to solve the problem of high latency in cloud computing, ato offloads the computation tasks from smart devices to mec servers. figure 1 illustrates one of the realistic scenarios of using our ato framework, i.e. the smart devices equipped within the assemble lines in one factory want to offload computation tasks to its neighboring mec servers. the user (one smart device) chooses which mec server to offload tasks based on the security levels and the access capability of mec servers. the mec server decides how to schedule the offloading tasks with various deadline constraints, such that its utility is maximized. in the ato auction, the mec server acts as the seller to share its computation resource and each user plays the role of buyer who offers the monetary payment to its offloaded mec server."
"in order to ensure that we maintain the essential link between theory and practice and to advance both components synchronously, we have chosen the design based research (dbr) methodology [cit] . dbr has the advantage to be situated in a concrete application context, which enhances partnership and collaboration between researchers and practitioners, while focusing on the concrete results of the innovation tested. dbr works via an iterative process, each iteration representing both a progression in the complexity of the system tested and of the proposed theoretical knowledge."
"the remainder of the paper is organized as follows. the related work is discussed in section ii. section iii presents the overview of ato framework and system model. in section iv, we present the design of the ato auction mechanism and prove its desirable properties. our theoretical analysis and numerical evaluations are depicted in section v. we conclude in section vi."
the remaining energy by the home automation node at any time t is given by sum of its total power output ( ) minus its total energy being consumed ( ) and can be estimated by the following formula:
"in this paper, we presented and analyzed implementation of energy harvesting in the 6lowpan-based home automation infrastructure. the rest of this paper is divided into five sections. section 2 presents our 6lowpan-based home automation system. section 3 discusses setting environment of our system design for 6lowpan-based home automation energy harvesting. finally, section 4 will evaluate and section 5 will conclude our research in this system."
"gift is a java-based message-oriented middleware (mom) software with a messaging api compatible with jms (java message service) protocol. although jms can be more robust and reliable than web services, jms middleware must control all the software components that it communicates with. this means that gift cannot communicate with external servers on which it has no control, but may however communicate with various rich clients interfaces developed with the java swing library."
"cluster management systems are often based on heuristics that use data gathered from both physical and virtual machines. in the simplest form, these systems make decisions based on the load of physical cpus, while the complex ones take into consideration additional aspects, such as memory usage, network traffic, service level agreements (sla), server energy consumption, server thermal state, virtual machine intercommunication, etc. even the simplest cluster management algorithms improve energy efficiency of a cluster that has a varying workload [cit] ."
"gift is an open-source, modular, service-oriented architecture whose goal is to make automated authoring, instruction and effect analysis easy and cost-effective [cit] . it provides a series of tools for an expert system developer, from course authoring tools to create activities, lessons and guidance, to survey authoring tools for questionnaires which appear during the learning process."
"as discussed, the proposed system architecture implements 6lowpan home automation network. the use of 6lowpan offers certain advantages and provides a comprehensive home automation solution. this technology is designed to be used on applications that required low data rate, low cost, low-power consumptions, and two-way wireless communications [cit] . the wireless nature of 6lowpan helps overcome the intrusive installation problem with the existing home automation systems identified earlier. the 6lowpan devices based on ieee 802.15.4 standard theoretically provides 250 kbps data rate and 127 bytes frame size; this would need a considerable amount of compression for controlling most home automation devices using ipv6. the automatic installation and ipv6 addressing of 6lowpan provide novel solution to end-to-end connectivity and ubiquitous internet-based home automation system and help tackle the expensive and complex architecture problems with existing home automation systems, as identified earlier. figure 2, is our edge route [cit], and it is charged by providing interoperability between different connecting networks. the home gateway provides data translation services between internet based on ethernet/wi-fi and 6lowpan networks based on 802.15.4. one way to integrate 6lowpan into home gateway is to provide basic layer 1-3 functionality using a 6lowpan network processor, which used 802.15.4 as low-power wireless interface. in order to use 6lowpan wireless interface with a standard ipv6 protocol stack, our home gateway functionality implemented 6lowpan adaption layer, 6lowpan-nd, and ipv6 rpl routing, ipv6 interconnection."
"node. the 6lowpan node for this test bed is based on ti cc2530 application board [cit] . the cc2530, depicted in figure 3, is true system-on-chip (soc) solution for 802.15.4 application based on smartrf05 evaluation board. it combines the 2.4 ghz rf transceiver with 8051 mcu, in system 256 kb programmable flash memory, 8 kb ram, batteries, and ambient/environment power source. in this environment, the application boards run contiki, an open source operating system for memory efficient networked embedded system and wireless sensor networks. contiki provides ip communication, both for ipv4 and ipv6, thanks to the embedded uipv6 subsystem. the latter is an implementation of an ipv6/6lowpan stack, able to transmit ipv6 packets using the ieee 802.15.4 radio of cc2530 chip. in our home automation system, this node has connections for digital ambient light sensors isl 29023, an integrated ambient and infrared light-to-digital converter i 2 c interface. in normal operation, typical current consumption of this sensor is 70 a, and the power consumption can be reduced to less than 0.3 a when powered down."
"thresholds for the load balancing functions described in sect. 3 are based on the average load values of linux. in the first function of our balancer, the migration decision is based on the 1-min average load of the virtual machine. if the 1-min average load of virtual machine goes under the given threshold, the virtual machine is migrated to the park server. the second function does the opposite and moves loaded virtual machines from the park server to computing nodes. it takes action if the 1-min load average of the park server surpasses its threshold. in our tests, the thresholds of load were 0.5, minimum and 1, maximum."
"in this paper, we will design a holistic solution for joint computation offloading and resource allocation in the multi-server mec network, such that as many as tasks can be offloaded to mec servers and can obtain computation results before their deadlines, in the meanwhile, mec servers can gain the approximate maximum profits. specifically, we consider a large ultra-dense network of icps where multiple mec servers provide computation offloading services for smart devices. in order to make the inspection system of icps satisfy the requirements of multiple productions and assembly lines, there are some major challenges that need to be addressed. first, the complexity of task offloading decision-making should be handled to satisfy various deadline constraints. second, how to incentive mec servers to share computation resource is critical for task offloading. third, account for various offloading amounts, the incentive mechanism should adapt to the amount pattern of offloading tasks. last but not least, the scheduling methods should process the online tasks for the real-time inspection system."
"many of the proposed cloud management systems use various bin-packing algorithms and are centrally managed [cit] introduce pmapper cluster management system. such as many other centrally managed system, pmapper has a separate monitoring service to get the latest hypervisor resource usage and the load of different virtual machines. based on this knowledge of the state and service constraints, a new virtual machine placement is formed. prerequisite for the functioning of pmapper is forming of power models for different hardwares. pmapper optimizes energy efficiency of a computing cluster by organizing virtual machines by their power model. virtual machines are placed using the first fit decreasing (ffd) bin-packing algorithm. the algorithm places virtual machines on servers, where their energy increase is minimal. migration cost is only calculated from the memory allocation of the virtual machine at the beginning of the migration ignoring the overhead introduced by the workload memory dirtying. this can be quite inaccurate if the virtual machine is active. [cit], with their entropy consolidation manager, divide virtual machine management in a computing cluster into two sub-problems: (1) virtual machine packing problem and (2) virtual machine replacement problem. in the first phase, a minimalist configuration is determined as a constraint satisfaction problem (csp), where vm resource requirements (cpu and memory) are mapped to physical resources. they use a dynamic programming approach to solve this multi knapsack problem. as the solution space is vast, the algorithm is given a reasonable lower bound, the amount of physical nodes, and an upper bound with ffd. in this way, they can get a more optimized configuration in less time. in the second phase, the transition from the previous configuration to this new minimized configuration is also considered as a csp. a reconfiguration plan takes into account migration cost and also possible linked migrations and temporary migrations, e.g., vms, can be moved temporarily to another host or moved to their destinations through intermediate hosts. the algorithm takes a long time to complete, but compared to ffd, it provides some improvement to vm packing efficiency. again, migration cost is calculated inaccurately directly from vm memory allocation."
"outsourcing of computation has become more appealing, and organizations are optimizing their it costs by adopting cloud technologies. besides the commercial cloud providers, many academical cloud services have appeared, too. amazon being the driving force in this development, and its de facto standard api has been used in many other cloud frameworks [cit] . today, there exists several open-source cloudcomputing projects that implement this de facto standard. the appearance of this new model of computing has made companies reconsider their it processes, and it also has enabled new companies starting internet-based services without the initial hardware costs. as several companies with different load patterns purchase their it services from the same cloud provider and the cloud resources are located in the same data center, there is an obvious possibility to optimize the computing systems and increase their energy efficiency."
"definition 2: task pattern is defined as the distribution of the above two classes, which is explored from the historical task requests within a learning window w, such as 100 time slots. in order to explore the impact of task patterns, we generate synthetic task flows with various task patterns. figure 3 demonstrates the performance of both simple greedy schemes with different γ . we can see that sup has better performance than stp when tasks with large amount dominate. through this simulation result, we find that the performances of both scheduling algorithms are significantly influenced by task patterns. for the sake of matching various task patterns, we therefore design an adaptive offloading scheduling (aos) algorithm."
"in this section, we give the details of the mec server selection algorithm (sasa) for each user by taking the security and access constraints into consideration. first, each user generates its authorized list of mec servers, according to the matching result of security levels. next, each user selects the targeted mec server with a maximum achievable transmission rate. the sasa algorithm is illustrated in algorithm 1."
"ato adopts the forward auction which contains multiple buyers and a single seller, where the buyers (smart devices) send bids to compete for the offered computational resource of mec servers. the one who gives the highest bid will win the competition. we use figure 2 to illustrate the auction process between users and mec servers."
"input: security levels of mec servers l, users security levels l, locations of mec servers and users. output: the selected mec server s * for user i."
"function 1 moves idle virtual machines from computing nodes to the park server. here, the migration decision is based on the load of a virtual machine. if the load of virtual machine goes under a given threshold, the virtual machine is migrated. function 2 does the opposite and moves loaded virtual machines from park server to computing nodes. it takes action if the load average of a park server surpasses its threshold. the virtual machine to move is chosen among the active virtual machines on the park server. finally, function 3 moves active virtual machines from underutilized computing nodes to better utilize computing nodes. it attempts to optimize the load on computing nodes and thus improve their energy efficiency (fig. 1)."
"in this paper, we investigate how to incentive mec server with limited access and computation capability to offload tasks efficiently for the qos requirements on task latency and security of intelligent icps applications. specifically, on the one hand, we present a mec server selection method under the access capability and security constraint for each user. on the other hand, an adaptive task offloading (ato) auction is presented to determine task schedules and the payment for each allocated task with deadline constraint, which has the ability to match various task patterns in icps applications. in addition, we propose a task pattern learning scheme to identify the current task pattern taking advantage of the historical information. through theoretical analysis, we demonstrate that the proposed auction mechanism satisfies the properties of computational efficiency, individual rationality and truthfulness. moreover, extensive simulations have been conducted to evaluate the performance of ato auction has better superiority and efficiency than both classic greedy algorithms. shuyun luo received the b.s. degree in electronic information engineering and the ph.d. [cit], respectively. [cit], she was a visiting student with the computer science and engineering department, pennsylvania state university. she is currently an assistant professor with the college of computer science and technology, zhejiang sci-tech university, china. her research interests include mobile edge computing, ad hoc and sensor networks, and network economics."
"due to the diverse computation capability of heterogeneous devices, the offloading benefit of each user depends on task urgency. hence, the user's payoff is related to the saving time by offloading. it is called users whose offloading requests are accepted as winners. the utility of each winner is defined as"
"in a batch system, such as in cern, physical resources are divided into slots. every node has a fixed amount of slots, and usually, this number corresponds to the number of cpu cores. workload, consisting of high-energy physics (hep) analysis or simulation jobs, is distributed in a round robin way evenly into the cluster independent of what kind of load the other slots are running. this approach could have limitations related to memory management in numa architectures, but in the case of the hep computing, it generally works well, since memory requirements of individual jobs are still quite small. workload runs a long time and has phases, where it uses less cpu, e.g., waiting for the data to be uploaded from the remote storage to the local storage. by remote storage, we mean a data storage in a different datacenter and by local storage, a local storage system in the same datacenter. since the job can analyse millions of events possibly stored in different files, we cannot always transfer all required data before the job starts. this causes the physical node or at least the current slot to rest idle. this idle waiting could be done in a server with less computing power. the load is analogous to, for example, server-based computing systems, where desktop sessions may run idle for a long time, or web servers, where the query rate fluctuates as a function of time of day. although the load fluctuates, the servers need to be online and ready to answer."
"the cloudsim simulator was set up to run 60 virtual machines in 15 physical hosts. the physical hosts were divided into park servers and computing nodes. the number of park servers was varied in the tests, but the total server count remained constant. every virtual machine had a separate input file that described their cpu load for every scheduling interval. in our tests, the scheduling interval was set to 30 s and the duration of one simulation to 24 h."
the aim of this study was to show that the energy efficiency of high-energy physics computing could be improved in a cloud environment using an active load-based management of virtual machines. we introduced an idea of storing idle virtual machines on a dedicated hardware and load-based over-committing of resources of idle virtual machines.
"as seen in theorem 2 that the os problem is np-hard, we design feasible greedy algorithms to solve this problem. the naive scheduling scheme is to schedule tasks based on the unit price (sup) or the total price (stp). the highlight of sup is that mec server schedules tasks by the unit price of each task, i.e., the bid value per time unit, which equals to the bid divides the computation time. for stp algorithm, the scheduling baseline is according to the total price, i.e., the bid value of each task. in light of the computation consumption, we divide all tasks into two classes, including 1) the ones with the large offloading amount and 2) the ones with the small offloading amount. in this paper, we use the ratio of the number of tasks in the first class and the total tasks as the metric γ to represent the task pattern, which is defined as follows."
"6lowpan-based home nodes have specific hardware characteristics and limitations. most of these nodes have limited available energy. in our case, although aa batteries that provide the power to the 6lowpan-based home nodes are rechargeable, but, to save long periods of life without the need for periodically replacing its batteries, we need to design the 6lowpan home device with total energy independence. to solve this, additional components for power management and energy harvesting are needed. thus, our 6lowpan-based home automation devices arepresented in the diagram as depicted in figure 4 . the voltage input from the energy harvester is used to charge the aa battery packs by the first stage dc-dc converter. then battery voltage is supplied at a stable level to the 6lowpan home device main circuit. for power management purposes, the node also needs to continuously monitor the voltage and the current drawn from the battery pack, which is achieved by the energy measurement module."
"our test cloud consisted of three servers with two different hardware compositions. as a park server and as an openstack front-end, we used dell 210 with intel x3430 processor and 32gb of memory. as computing nodes, we used the same servers with 12gb of memory. openstack servers were connected to an nfs share that contained virtual machine images. servers were connected using two gigabit networks. the first network was used for openstack communication, and the second was dedicated for nfs. power usage data of the servers were collected using a watts up? pro meter via a usb cable. power usage values were recorded every second. a separate pc was used to control the tests and collect electricity measurements. the test setup is illustrated in fig. 2 ."
"our architectural analysis has led us to choose the integration of open edx with ctat for the architecture of our its. not only is the integration with ctat smoother than that with gift, but ctat also implements the provider side of lti, compatible with the lti consumer capacity of open edx. furthermore, a recent project has demonstrated the technical feasibility of the integration of ctat with open edx [cit] b) using the lti standard. in the architecture proposed by aleven and colleagues in their ctat-edx experiment [cit] b), the tutor's inner loop functionality, meaning its personalized guidance within a problem, and its outer loop functionality, meaning the selection of problems based on the student model [cit], were handled separately. moreover, the inner loop of the tutor was moved into the student's browser by re-implementing it in javascript, which had a positive effect on the scalability of this approach."
"we have already carried out several steps of our methodology, notably the literature review and needs analysis. another step of the project is underway, consisting in the development of the content to be modeled in our its, both in terms of dictionary skills and lexicological knowledge as well as the learning activities. we have currently developed a referential of dictionary skills consisting of 125 skills in 4 categories (conceptual knowledge, linguistic competency, dictionary competency and practical skill) linked with an ontology of 25 linguistic concepts to ensure adequate theoretical grounding. the referential will be transformed into a series of databases used by sti-dico and linked to the learning activities to model the skills acquired by the learner. the data collection of this first iteration is underway, consisting in an evaluation of the referential being performed by three experts in linguistics."
"in this part, we first present two simple greedy algorithms for task offloading scheduling, then analyze the impact of task patterns (defined in def. 2) on their performance. in order to match various task patterns, we design a novel adaptive task offloading scheduling algorithm. theorem 2: the os optimization problem is an np-hard problem."
"cyber-physical systems(cps) is an emerging approach that concentrates on the integration of computational functions, such as data and information processing, with physical devices [cit] . nowadays cps are widely applied in the industrial domain. the industrial cyber-physical systems (icps) as the key innovative actions, have been launched in various programs all over the world, e.g. initiative ''industrie 4.0'' in german [cit], ''smart manufacturing'' in usa [cit], '' [cit] '' [cit], ivi (industrial value chain initiative) in japan [cit], ''industrial national plan 4.0'' in italy [cit], etc. cps in industrial infrastructures requires a high level of decision-making capabilities in terms of the associate editor coordinating the review of this manuscript and approving it for publication was muhammad maaz rehan ."
"discussion: in the current time slot, the rejected users try to send offloading request again with a reduced offloading amount and its corresponding bid. in order to make sure that the submission times bounded by m j, where m j is determined by mec server j, the offloading amount reduces o i /m j in each time. the rejected users will keep trying to compete the opportunity of offloading service by reducing the offloading amount until any of the following three conditions is satisfied. 1) the offloading task is accepted. 2) the corresponding bid becomes non-positive. 3) the current computation resource is fully occupied before its deadline."
"furthermore, to enable learners to be aware of their strengths and weaknesses, we will implement an open learner model [cit] that will visually represent the concepts and skills that a learner has already acquired and link them with their corresponding activities. since the learners we are targeting are sufficiently advanced in the domain and possess adequate contiguous knowledge, we believe that it is important for them to have access to their student model and follow their own learning path in real time."
"so far, research on heterogeneous hardware in a cloud environment has mainly focused on non-homogeneous hardware in big cloud environments, such as amazon cloud [cit], or to the use of specialized hardware, such as gpus in clouds [cit] . cloud environments can have different types and generations of processors [cit] . this can cause the performance of virtual cpu in a virtual machine be different among virtual machines of the same type. besides being unfair to clients, the difference in hardware can also hurt performance. for example, [cit] have found that the heterogeneous hardware of a cloud environment can be harmful for hpc load when it is stalled by its synchronous nature. as a solution, they suggest a method that picks homogeneous hardware from the pool of vms."
"since mec servers are provided by different third parties, they need to obtain the profit as much as possible from a practical business perspective. in the meantime, mec servers can obtain the payment from users only when the offloaded tasks are completed before their deadlines. hence, the definition of the offloading schedule problem is given in the following."
"the contribution of our work lies at the intersection of three critical cutting-edge research topics. (1) multiple mec servers selection; (2) offloading scheduling; (3) incentive mechanisms for edge computing. combining the above cases, a fundamentally novel adaptive task offloading auction mechanism is proposed for the icps system with multiple mec servers and multiple users."
"due to all of the characteristics mentioned above, we see open edx as a powerful lms interface component for our project. furthermore, it is also the lms platform used in our educational institution and therefore we have access to a local version of open edx as well as adequate support and logistics."
"in this section, we will demonstrate that our proposed incentive mechanism of ato auction achieves the desired three properties: computational efficiency, individual rationality and truthfulness."
"the work on sti-dico is in full swing, with an ongoing collaboration with the ctat team [cit] in order to seamlessly link the ctat reasoning motor with openedx and the html5 sti-dico interface. we believe that this integration is important because it will permit for sti-dico to be light and platform-independent. in parallel to this, we are working on integrating the gnt ontology (polguère, 2010) with our referential of dictionary skills and knowledge and indexing these two components with the learning activities in sti-dico. this will permit the system to carry out cognitive diagnosis of a learner's skills and knowledge based on their behavior in the system's activities."
"where ( ) is remaining energy of the home automation node, ( ) is total power output from energy harvesting source, and ( ) is total energy consumption of the automation node."
"depending on the application and location, a variety of sources for energy harvesting have been researched [cit] such as a solar power and thermal, vibration, and kinetic energy. photovoltaic (solar) cell has the capability of converting light energy into electrical energy. several research efforts have been conducted and so far have demonstrated that photovoltaic cells can produce sufficient power to maintain a microsystem. the vibrational harvesters use one of three methods: electromagnetic, electrostatic, or piezoelectric. piezoelectric energy harvesting sources alter mechanical energy into electrical energy by straining a piezoelectric material to complete energy harvesting solution optimized for high-output impedance energy. this is well suited for lowpower wsn nodes, since it accumulates energy over a long period of time to enable efficient use of short power bursts. summary of the most common energy harvesting sources is depicted in table 2 ."
"the basic idea of these management systems is to maximize the usage of an active server and minimize the amount of them. one way to achieve this is to pack already loaded servers more efficiently. cloud management systems can roughly be categorized into active and passive systems. as mentioned earlier, openstack, such as many other open-source systems, do passive management, where load balancing is done at the time of creation of new virtual machines. even though these algorithms can balance load between physical hosts, they are limited in reaction to the load changes of instances. active cloud management systems can better react to changing load pat-terns, as they actively move the virtual machines between physical machines to meet the goals of the system. these goals can be energy efficiency, qos, sla, etc. active systems usually react to several events indicating that the system state is not optimal or some thresholds have been breached."
"for instance, in order to help the user learn how to use a monolingual dictionary in order to find the synonyms of a word, we will place them in a situation where this knowledge is directly mobilized: a student text in which the same word is repeated several times (figure 3) ."
"openstack is one of many amazon ec2 compatible open-source cloud-computing software frameworks. openstack is widely used and has a large community behind it. that is why, cern has also chosen it as a future platform for particle physics computing at cern [cit] . such as many other cloud-computing projects, openstack supports the basic features for creating, pausing, and destroying virtual machines. in addition, it provides a way to spread virtual machines across a cluster of computers [cit] . however, such as many other similar systems, it is missing a load-based dynamic virtual machine management system for minimizing the energy consumption."
"we developed a centralized monitoring system for our active management tests. this system stores information both from virtual machines and physical servers. it uses the information that is gathered by the oracle grid engine (oge), and in addition, cpu load, cpu utilization, and memory state are periodically submitted by hypervisors. all the information is stored in a relational database. our load balancing algorithm retrieves the system state from the database through a web server with json."
"our study focuses on the integration of lms with its, to exploit the scalability and ease of use of the first with the adaptive guidance and intelligence of the second. if this integration is carried out successfully, this could provide its with a springboard towards their usage on a larger scale in classrooms and by independent learners. this, in turn, would permit a more vast collection and analysis of the data obtained from interactions with the lms-its hybrid systems and the usage of educational data mining techniques to further improve their efficiency."
"to evaluate the performance of ato auction mechanism, we do extensive simulations to evaluate the impact of main parameters. the performance metrics are demonstrated as follows."
"our operational hypothesis is that a service-oriented architecture integrating an lms interface with its back-end components is the best integration approach given the needs of our project. while there are only a few existing precedents of lms-its integration [cit] b) we believe that it is a promising development path for the next generation of itss, resulting in adaptive technology that can easily be used and modified by teachers according to their needs while providing a rich source of educational data that can be analyzed for the benefit of course authors, teachers and learners."
"in the current section, we consider the different implementation possibilities for our its, and more specifically, the interoperability of the various tools available to us. in this perspective, we will analyze open edx, gift and ctat."
"as seen earlier in figs. 12 and 13, the load of the system greatly affects the energy efficiency and the importance of load balancing to and from park servers. in fig. 17, we have similar results as earlier on the case of the rate 100 (fig. 14), but now with the higher load. the energy efficiency gets worse than it would without the load balancing functions, as migrations introduce some overhead."
"we established energy harvesting system that can harvest sufficient energy for the 6lowpan home device needs. to successfully power our nodes we did the following experiment: we measured the average output power on a fixed 1 kω load when the cell was in full sunlight. the result of our measurement is depicted in figure 6 . at noon, about 10 hour, the voltage condition reaches stable energy level at 2-3 v, so can generate electricity to give enough storage capacity for our energy harvesting system."
"for this purpose, we set up an openstack installation using devstack. 3 workload was distributed to the cluster as a batch of jobs using oge. 4 twelve openstack t1.small virtual machines instances [cit] worked as oge execution nodes. submission intervals of the batch jobs follow a poisson process [cit] and resemble the real distribution of job arrival rate in cern clusters. although the workload was real physics computation, its duration was artificially limited. the durations of analysis jobs were scaled down to allow more reasonable test times. batch jobs consisted of 4 min, joba, and 7 min, jobb, jobs in equal proportions."
"to illustrate movements of virtual machines among the physical servers during different tests scenarios, we picked samples of migration data using the tests without the active vm consolidation feature, i.e., the am tests. figures 5, 7, and 9 illustrate the movements of virtual machines between the three physical servers. in fig. 5, we have the movements of one test with the rate 0.2, fig. 7 with the rate 0.4, and fig. 9 with the rate 0.6. rates 0.2 and 0.4 are very low and two servers they can basically serve them. based on the tests, we can indicate that the am and amp functions are able to increase the utilization of servers compared to the static placement of instances (the sp method). on the right side of these figures, we have the power consumption of the corresponding tests in figs. 6, 8, and 10 ."
"since mec servers belong to various providers, we consider maximizing each mec server's utility rather than the whole servers when mec server decides the offloading schedule and each winner's payment with diverse deadline constraint. both the offloading schedule scheme and the payment computation approach are assumed to be common knowledge among users. meanwhile, each selfish user will exploit this knowledge to choose a bid to maximize its own utility, which is shown in equation (1) . we also assume that users do not collude."
"since the am scheduler without packing does not attempt to pack load, i.e., consolidate vms into smaller number of computing nodes, this results in the physical servers running with low utilization. as we can see in fig. 7, the computing node two runs a long time with just one virtual machine. these under committed resources are corrected with the use of packing function. rate 0.6 is high enough to saturate the used test cluster. when this happens, the park server load is always moved to the most loaded computing node that still has enough space left to support the additional load. computing nodes are allowed to host only four virtual machines to guarantee the one to one mapping of cpu resources. the park server, on the other hand, can host more virtual machines."
"furthermore, the modular architecture underlying open edx facilitates content composition and reuse, especially since it supports the lti (learning tools interoperability) standard, whose main objective is to establish standard means of integration of distance learning applications (providers) with hosted course platforms (consumers). lti allows a wide range of integration scenarios [cit] and its main purpose is efficient user authentication (see fig. 1 ). open edx also allows course authors to insert custom javascript problems and html5 widgets directly into courses, allowing these custom elements to be evaluated in the same manner as exercises created using open edx templates. this gives the course author a higher degree of freedom in the creation and evaluation of their course."
"a 6lowpan home node that has energy harvesting as a supplement to the battery energy can maximize the life time and virtually run for an infinite amount of time without the need for periodically replacing its batteries. however, total power output needed from energy harvesting depends largely on energy consumption of the 6lowpan home node application and the sensor used. in order to maximize home node performance and satisfy energy independence, we used 3 v, 70 ma small polycrystalline silicon solar cell."
"in the second section of this paper, we expose our hypothesis and methodology to develop sti-dico, an adaptive its for learning dictionary-related skills, using an lms as learning interface and deploying a service-oriented architecture (soa). the third section presents the architecture and components of sti-dico. in the fourth section, we present a concrete learning situation in sti-dico and the way in which it will function. the paper ends with some words on ongoing and future work, followed by a conclusion."
"6lowpan approach for home automation system is designed for control and monitoring of household devices. we are setting up a home automation scenario test environment to experiment interconnection between home automation devices, based on 6lowpan over ieee 802.15.4, with an existing ipv6 network, based on ethernet/wi-fi. to test interconnection between 6lowpan node and outside ipv6 network, we develop a light sensor remote and mobile control based on android application. the remote user's communications transverse the internet until they reach our home gateway. after that, the communications are wirelessly transmitted to the 6lowpan-based home nodes. the application of this test bed has implemented ipv6 using android api inet6address. the captures of our application are seen in figure 5 . the figure on the left showed the first screen of our application and the menu to select sensor nodes that will be monitored. the figure on the right showed on/off commands that sended a message to each 6lowpan home node. as we know, to have home automation systems which have long periods of life, the power management is important. in our implementation, the home gateway is always connected to usb port, and no batteries are needed, but as it has been discussed, our 6lowpan home devices based on ti cc2530 need batteries as power source. to measure the current consumption of our devices the voltage of a resistor 10 ω placed in series with the node was measured. it is determined as long as is below 30 ma. however, the current consumption of our 6lowpan device is almost independent of the input voltage. once the current is determined, the average current consumption can be found using the general formula"
"in our test with real hardware, we compare basic openstack installation with passive management to our own active management algorithm. tests compose of three different cases: in case 1, instances are created permanently for the whole duration of the test. in cases 2 and 3, the placement of instances is static and virtual machines that are migrated between physical machines depending on the load. starting configuration in our test cases is slightly different. in the static placement scenario, virtual machines are spread equally into all nodes. in the other two cases, the virtual machines' initial placement is on the park server. all the measurements are repeated ten times to get statistically comparable results."
"to accompany the authentic learning scenarios, sti-dico also proposes explanations of fundamental concepts that are needed for dictionary use. for instance, if a user is not capable of completing the activity described above, it is possible that he is not familiar with the concept of synonymy. if this is the case, sti-dico diagnoses this knowledge gap and proposes an explanation of the concept of synonym (figure 4), accompanied by examples and presents the learner with a series of more theoretical exercises (for instance, drag and drop exercises to match words with their synonyms) in order to explore this concept in more depth. sti-dico will also highlight the links between concepts (e.g. synonymy and antonymy are both types of lexical relations) in order to help the learner build their concept hierarchy."
"we first tested how the virtual machines placement works in the simulator. in fig. 12, we show the server usage from the beginning of the simulation. it shows the number of free park servers and the number of free computing nodes. in the beginning, when the workloads have not started yet, the virtual machines are aggressively migrated to the park servers. as the workload increases, the number of free execution nodes decreases. in the case of lower load, the need for execution nodes greatly varies. in the case of higher load, in fig. 13, the park servers are less important, as the virtual machines are on the computing nodes."
"the basic idea of our load balancer is to make space for active instances on computing nodes, maximize the load on computing nodes, and minimize the amount of active-computing nodes. this is achieved with specialized hosts for storing idle virtual machines, active monitoring of resources, and load-based active management of virtual machines in the physical cluster."
"we are also planning a project in partnership with inria sofia-antipolis, to ensure that the learning objects that we develop are compliant with the eee standard for learning object metadata (p1484.12). we will therefore be able to share our activities with knowledge object pools such as ariadne (http://www.ariadne-eu.org/) and globe (http://www.globeinfo.org/). we believe that this is an important step for the future development of intelligent tutoring systems, which need to follow recent trends in computing, such as serviceoriented architectures, distributed and ubiquitous computing, and the pooling of resources to ease their entrance to mainstream educational contexts [cit] ."
"when the 6lowpan node is up and running in the home automation network, we use the packet sniffer to visualize the packet going over the air. figure 7 shows air packet capture view of our 6lowpan home automation node transmission. the detail of various results and explanation of what is happening in the home automation device during a data transmission is depicted in table 3 ."
"sti-dico not only evaluates the user's final answer, but also the entire sequence of the activity: the user must click the words in the text that are repeated, indicate how it is possible to replace the words with their synonyms, and also carry out the correct search in an instance of electronic dictionary accessible via the interface. our its can therefore evaluate different skills and types of knowledge that the user has acquired or not, and use this knowledge to propose different activities to address missing concepts or skills."
"open edx is an open-source lms platform designed using a stack of effective technologies such as python programming language, nginx, django, mysql and mongodb [cit] . due to its web service-oriented architecture, open edx is able to provide services to and accept services from other software using standard web protocols. as such, open edx aims to provide its services to hundreds of thousands of students while adjusting to an increasing or decreasing demand for computing resources using cloud infrastructure. this type of architecture enables deployment from a laptop or a small cloud server to a multi-server infrastructure to serve tens of thousands of students."
"sti-dico is aimed at future french teachers in primary schools in quebec, in order to guide them in the acquisition of skills and knowledge necessary to teach the use of french dictionaries (more specifically, electronic dictionaries) in the classroom. in order to ensure authentic, situated learning [cit], we have created a series of authentic learning activities that resemble the tasks the teachers carry out in the classroom: correcting and improving students' texts, designing classroom activities, etc. the activities offered in sti-dico put the users in these familiar situations in order to develop their existing skills as well as to teach them how to exploit the dictionary to improve their performance."
"end is the ending time of idle period i . if there exists such an idle period, task i can be offloaded, and joins the accepted set t accept, shown in lines 9-10. otherwise, task i will be added to the rejected set, t reject, shown in lines 11-12. finally, all accepted tasks will be deleted from the waiting list w l ."
"in our work, we take both the system throughput and energy efficiency into consideration utilizing heterogeneous hardware in a dynamically managed virtual machine cluster. we develop a system for virtual machine management for openstack and introduce a new way of packing idle virtual machines into dedicated park servers. by 'idle', we mean that the virtual machine has little or no workload to run at the moment and we define it using the load value of the operating system. we test the system with high-energy physics workload and also verify the algorithm with a simulator. we show that energy efficiency of a cloud-computing cluster can be improved using dynamic management of virtual machines and prioritizing on energy-efficient hardware on the heterogeneous cluster."
"in the last two decades, the web has become an irreplaceable tool in our personal and our professional lives. it has also become a key component in the democratization of education, giving learners all over the world instant access to up-to-date information. the web is especially useful in the field of educational technology (et), since it helps address the issue of increasing classroom sizes by providing the opportunity of using web-based tools in classrooms [cit] . one of the most widely-used applications of et are learning management systems (lms), which are easy to use and can empower teachers to use the web to share content and exercises with their students. while lmss are not adaptive on their own, in the hands of a skilled post-secondary teacher, they can become powerful tools to add content from the web as a complement to textbooks and classroom activities. another powerful tool of et are intelligent tutoring systems (its), machine tutors that apply artificial intelligence techniques to guide the user throughout the learning process [cit] . its are increasingly using web-based technologies and adopting service-oriented architectures in order to reach more students on a global scale, lower development costs, and permit the integration of content from the semantic web [cit] . adaptivity in et tools is important because it enables the personalization of the learning activities to the objectives, strategies, knowledge and even emotions of learners. this changes the relationship between the learner and the content to be learned, traditionally static, to make it interactive, engaging, collaborative, and pertinent to the user's context of learning."
"this section describes our conceptual design of a flexible home automation network using 6lowpan (see figure 1 ). our goal is to develop a home automation system that is robust, flexible, easy to use and has a wide range of capabilities. the system allows home owners to monitor and control connected devices in the home, through any wi-fi enabled device. additionally, users may remotely monitor and control their home devices using any internet enabled device. a home gateway is implemented to facilitate interoperability between heterogeneous 6lowpan and ordinary ipv6 network based on ethernet and wi-fi. it also facilitates local and remote control and monitoring of the home devices and provides a consistent interface, regardless of the accessing device. remote user communications traverse the internet until they reach the home gateway. they are then wirelessly transmitted to the home devices using the 6lowpan."
"then, we tested how energy efficiency of the cluster is affected by different threshold values in the second function of the load balancer, i.e., at which park server load level, the virtual machines are moved from the park server to computing nodes. in fig. 14, we can see the average energy consumption per instruction for different variations of the test. it also shows how the choice of park server hardware affects the energy consumption. workload for these results is the same 100 jobs per hour per virtual machine. we can see how the energy efficiency improves as the load threshold, and the number of park servers is increased. an increased number of park servers allow the system to pack virtual machines more densely when there is less load."
"while the system architecture proposed is generic and can be applied to a variety of knowledge domains and skills, we have chosen to instantiate it within the domain of linguistics, and more specifically that of dictionary usage. this is due to the fact that in quebec, a gap has been observed between the requirements that the ministry of education has of its students and the reality of teaching in the classroom. more specifically, french primary school teachers are expected to teach their students how to use dictionaries, but themselves have not received adequate training in lexicology to foster the learning of the needed skills by their students [cit] . having established the demand for dictionary-specific training in our joint work with tremblay [cit], our aim is to develop an its which can be offered in complement to existing courses in teacher training, and extended to other users and to other languages."
"the ato framework supports icps systems with heterogeneous mec servers and multiple users (smart devices), which can be applied to scenarios where iot smart devices (such as smart cameras, mobile robots and agvs, etc.) need to offload computation tasks to mec servers. ato consists of a set of users (smart devices) and a set of mec servers."
"two different power models were used. for the energy-efficient server, we used a model that mimics the one of intel xeon e3-1260l (2.4 ghz), 6 and for the normal server, we used the one of xeon e31280 (3.4 ghz). 7 these power models define how much energy is consumed with different processor loads. power models were obtained from spec website, and they are the results of measurements with the specpower_ssj 2008 software 8 on real hardware. in table 1, we have the hardware parameters of the simulation setup. as a park server, both hardware types were used in different tests to compare the effect of hardware on energy efficiency."
"in addition to taking into account light sensors with an additional 1 ma at most and implying that no application acknowledgment is implemented, our 6lowpan home device consumed a maximum of 3 ma at 3 v from the power source. the total energy consumption for the 6lowpan home node to run without pause for the duration of a single day will be where is total energy consumed by 6lowpan home node, is power source voltage, is total average current consumption by 6lowpan node, and is total duration of energy energy consumed."
"i is the moving image after registration, j is the fixed image, and ∩ denotes the intersection of two images. the dice score was calculated in two-dimensional, the number of dice scores for each brain region was 50."
"hours, which used 100 threads in 20-node hpc and cost 56 gb memory for each node. in the addition, we evaluated the performance of registration for large volume dataset (si figure 6 and table 2 )."
"here, we also conducted a quantitative assessment of brain-region levels. in the box plot ( figure 5f ), the median dice score for all brain regions was between 0.9 and 0.99."
"in this paper, we study the continuous coverage problem with minimum number of replacement uavs and prove that it is np complete. we design an efficient algorithm to solve it, the cycles with limited energy algorithm. the proposed algorithm covers the n subareas by cycles, in which each cycle needs one additional uav to ensure continuous coverage. we showed that the energy capacity of the uavs, the number of subareas in the affected area, and the uav charging and traveling times will all impact the required number of uavs. our simulation results showed that applying the cycles with limited energy algorithm, can efficiently reduce the number of additional uavs needed relative to the straightforward method. as future work, we will study the continuous coverage problem using uavs with non-uniform energy capacities and the use of green energy."
"proof. the number of constraints is polynomial in terms of the number of subareas, the number of uavs and the number of time slots. given any solution for our problem, we can check the solution's feasibility in polynomial time, then the problem is np. to prove that the problem is np-hard, we reduce the bin packing problem which is np-hard [cit] to a special case of our problem. the special case will be the discrete coverage problem in which each subarea will be visited one time by one uav during the coverage process. in the bin packing problem, we have p items where each item has volume z p . all items must be packed into a finite number of bins (b 1, b,...,b b ), each of volume v in a way that minimizes the number of bins used. the reduction steps are: 1) the b-th bin in the bin packing problem is mapped to the m-th uav in our problem (where the volume v for each bin is mapped to the energy capacity of the uav e)."
"dataset 5 is the specifically selected problematic dataset, which contains streaks caused by uneven staining and illumination and sample tearing phenomenon. this dataset will be used for comparison of later results. the imaging system and staining method are the same as those obtained by dataset 4. the only difference is that the axial resolution is 1 μ m."
"the straightforward method (sm) to continuously cover n subareas is to allocate two uavs for each subarea. at the first time slot, n uavs cover the n subareas. then, any uav wants to return to the charging station to recharge the battery will do the handoff process with one of the additional uavs that are available at the charging station. by applying sm, we need n additional uavs and we have n cycles to cover all the subareas. our proposed algorithm, the cycles with limited energy algorithm (cle), is inspired by the nearest neighbor algorithm, the nearest neighbor algorithm is used to solve the traveling salesman problem [cit], in which the salesman keeps visiting the nearest unvisited vertex until all the vertices are visited. in our algorithm, the uav (salesman) has limited energy capacity and before visiting any new subarea, we must check if the remaining energy is enough to return to the charging station from the new location or not. in the previous subsection, we show how to find the minimum number of additional uavs that are required to guarantee the continuous coverage for a given cycle, we use the theorem 2 to find the minimum number of additional uavs that are required to provide the continuous coverage for a given area, by finding the cycles that need only one additional uav. the pseudo code of this algorithm is shown in algorithm 1."
"we presented a special registration type in this section that was registered to the reference atlas. using direct or inverse registration to the reference brain space, we were able to acquire the spatial information of the original dataset, which is the key to integrating multi-modal image dataset into a common brain space."
"actually, the power consumed by the uav during data transmission and reception is much smaller than the power consumed during the uav hovering or traveling [cit] . in this paper, we assume that the power wasted during data transmission is constant."
"based on the above mapping relationship, we established a transformation method for large volume datasets. raw image sequences were partitioned into many cubes using tdat tools [cit] b) (figure 1 image blocking). the transformed space was precalculated according to the size of fixed images and scaling factor. next, we applied transformation parameters for each block in transformed space separately. we loaded only a small range of data into memory to solve the contradiction of the tb-scale dataset and the limitation of memory. the details are as follows (figure 1 block transformation)."
"(2) nonlinear parameters are represented by deformation fields, which express the displacement for each voxel in three-dimensional space in the form of three channels p(x, y, z) dentes the space coordinate of voxel p before registration, and the coordinates after linear and nonlinear registration are p' (x', y', z') and p' '(x'', y'', z''), respectively. the mapping relationship is shown in equation 2."
"moreover, we presented the enlarged views of local regions of nucleus for each data type. nucleus with distinct boundaries such as aco, dentate gyrus, granule cell layer (dg-sg), and cp were aligned well to the allen reference atlas. for aca, the left and right boundaries were not obvious, but the upper and lower boundaries could be judged by the original image. for mv and spinal nucleus of the trigeminal, interpolar part (spvi) with inconspicuous boundaries, the accuracy could be roughly judged by the brain spatial orientation. all these areas were well aligned."
"we presented the registration results in the form of checkerboards in figure 4 . the global orientation of the mouse brain was corrected after linear registration ( figure 4a ), and the local brain regions and nucleus were adjusted after nonlinear registration ( figure 4b ). compared to linear results ( figure 4c ), nonlinear registration resulted in good local correction of regions such as olfactory areas (olf), cb, hip, and paraflocculus (pfl) ( figure 4d ). the small purple arrows in figure 4cd indicate that the misalignment positions were corrected after nonlinear registration."
"we simulated sample deformation in model1 and added a non-uniform gray assignment to simulate non-uniform signal or weak signals (eyebrows) in model2. model3 was added worse streak noise. on the basis of above steps, we added a sample tear situation to model4 (figure 3 and materials)."
"using the corresponding constructed models according to the complex situations during brain image registration such as sample deformation, weak signal-to-noise ratio (snr), streak noise and sample tear, we demonstrated the robustness of our proposed brainsmapi and other registration methods in response to various complex situations."
"the rest of this paper is organized as follows. section ii presents our system model including the problem formulation and the proof of the np completeness. in section iii, we show how to find the minimum number of additional uavs that are required to guarantee the continuous coverage. then, we present our proposed algorithm. in section iv, we present our experimental results. finally, section v concludes the paper."
"after the process above, we acquired the registered three-dimensional image dataset at original resolution, which was in the tdat format. finally, the registered two-dimensional image sequences of three anatomical sections (coronal, horizontal, sagittal) were generated by re-slicing (figure 1 blocked data re-slicing)."
"in addition, we evaluated the registration accuracy at brain-region levels (see method). in the box plot ( figure 4e ), the median dice score for all brain regions was between 0.9 and 0.99. generally, a dice score is above 0.8 indicates that good registration effects have been achieved [cit] ."
"we employed 6 whole-brain datasets and 1 metadata to validate brainsmapi. all of the animal dataset 2 is the image volumes representing the canonical waxholm space (whs) adult c57bl/6j mouse brain. five datasets are provided, and three of the datasets were acquired by duke center for in-vivo microscopy using three mri imaging models t1, t2, and t2*, one is the nissl stained dataset obtained by drexel university, and one is the manually labeled atlas dataset. the datasets are available at https://www.nitrc.org/. here we chose the t2*-weight mri image dataset. respectively. here we chose the cytoarchitectonic channel dataset. in addition, we identified and reconstructed the barrel cortex neurons of the gfp channel. approximately, a total of 40 neurons were used as metadata 1 [cit] ."
"in equation (6), p (los) is the probability of having line of sight (los) connection at an evaluation angle of θ, p (n los) is the probability of having non los connection and equal (1-p (los)), l los and l n los are the average path loss for los and nlos paths."
"when the crying face turns into a smiling face, it proves that good registration effects have been achieved. as shown in the last column in figure 3, regardless of deformation, weak snr, streak noise and tearing, proposed method could obtain good registration results."
"many projects [cit] have used registration techniques to integrate various datasets even the recent international brain projects [cit] desire to develop a powerful, standardized, industrialized framework to integrate multi-scale, multi-mode and massive datasets for studies on brain function mapping, disease models, and behavioral cognition [cit] . brainsmapi is highly compatible with the requirements of data integration. it can accurately register various image datasets and existing vectorized metadata, and handle high-throughput handle the tb-scale large volume whole-brain datasets, providing a complete and effective pipeline for brain data integration. the three-dimensional rendering of three anatomical projections (horizontal, sagittal and coronal) of the outline and neurons before (c top) and after registration (c bottom)."
"due to the intractability of the problem, we study partitioning the coverage graph into cycles that start at the charging station. we first characterize the minimum number of uavs to cover each cycle based on the charging time, the traveling time, and the number of subareas to be covered by the cycle. our analysis based on the uniform coverage in which the uav covers each subarea in a given cycle for a constant time. based on this analysis, we then develop an efficient algorithm, the cycles with limited energy algorithm, that minimizes the required number of uavs that guarantees a continues coverage."
"by design, we first assessed the accuracy of the brain-region level, a coarse assessment. a total of ten brain regions of interest were chosen throughout the brain: outline, cb, cp, hindbrain (hb), hip, hypothalamus (hy), isocortex (iso), midbrain (mb), pons (p) and thalamus (th). we manually segmented the moving image after registration, and the segmentation results were regarded as a silver standard [cit] ) instead of a golden standard. to make the segmentation results more accurate and reduce the workload, we did not segment each complete brain region slice by slice, because of the extreme indistinguishability of the start and end of the brain regions. the brain regions were middle intercepted and 50 image sequences were selected at equal intervals and segmented slice by slice. all manual segmentation results were referenced to the allen ccfv3. finally, the dice score (equation 3) was selected as the evaluation measure."
"another important aspect of this paper is the high-resolution nonlinear registration method for large volume datasets, which benefited from the low-resolution acquisition parameters, high-resolution transformation strategies, and data partitioning ideas; this method is also highly scalable and very promising for future applications to peta voxel datasets, such as the marmoset and human brain datasets. presently, there appear to be no feasibility problems, and the only cost is more computing time, which can be accelerated by improving hardware performance."
"it is obvious that we need n uavs to cover n subareas at any given time, but the question here is how many additional uavs are needed to guarantee a continuous coverage. in this subsection, we assume that the uav visits the subareas based on a cycle that starts from the charging station and ends at the charging station for charging process. we also assume that a given uav covers the subareas in the cycle uniformly, in which the uav covers each subarea in a given cycle for a constant time. in theorem 2, we find the minimum number of additional uavs that are needed to guarantee a continuous coverage for a cycle, which will help us while developing algorithm 1."
"in figure 4, the slope of the line produced by sm is greater than the curve of cle. when applying sm, the number of additional uavs increases linearly with the grid size. this is because the number of additional uavs equals the grid size. also, when applying the cle, the number of additional uavs increases with the grid size. this is because more cycles are needed to cover more subareas and each cycle will need one additional uav. in figure 5, we study the effect of the charging time on the number of additional uavs needed. changing the charging time will not affect the number of additional uavs needed when applying sm. this is because the coverage time of each uav will cover the time that the uav needs to return to the charging station to recharge the battery and to visit the subarea again. on the other hand, when applying cle, it will be a critical issue (see theorem 2) . actually, charging the battery of the uav takes long time. for this reason, each uav has a replacement battery [cit] . in this paper, we assume the time needed to replace the battery for each uav is 5 minutes."
"in figure 3, we study the effect of the uav energy capacity on the number of the additional uavs needed to cover the subareas. actually, when increasing the energy capacity of the uav and apply sm, the number of additional uavs needed will not change because each subarea is covered by one cycle and two uavs. when increasing the energy capacity of the uavs, only the coverage time of each uav increases. on the other hand, increasing the energy capacity of each uav results in minimizing the number of additional uavs that needed using cle. this is because increasing the energy capacity of each uav gives the uav a chance to visit and to cover more subareas, which minimizes the number of the cycles that are needed to cover the subareas."
"all these results demonstrated that the method proposed in this paper has extremely high accuracy at the brain-region level, and there was no significant difference from manual results at the nuclei level of 10 μ m resolution."
"proof. consider that all n subareas in the cycle are covered by n uavs and the uav that covers the last subarea want to return to the charging station to recharge its battery. the handoff process needs to begin between one of the additional uavs from the charging station and the uav that covers the first subarea in the cycle. the uav that covers the last subarea needs to wait (n − 1) t to do the handoff process, during this time the additional uavs are covering the first subarea. after the handoff process is completed, the uav needs t time units to return to the charging station, t charge to recharge the battery and t to visit the first subarea in the cycle again. then, we have:"
"three-dimensional nonlinear displacement ( figure 2b ) and nonlinear registration results ( figure 2c) were obtained by the nonlinear registration on the linear results ( figure 2a ). the global and local nonlinear deformation effects where be observed from the coronal, sagittal and horizontal sections respectively ( figure 2d ). for comparison, we also show the registration effects with the atlas line superimposed the nonlinear results ( figure 2e )."
"during the process, parallel technologies of process-level message passing interface (mpi) and thread-level openmp were applied in data reading, writing and calculating operations in a multicomputer environment to efficiently achieve the nonlinear registration of the tb-scale whole-brain dataset."
"we demonstrated the robustness of our registration method on both model data and real brain image datasets and presented the nonlinear registration results of a three-dimensional whole-brain fine image dataset at single-neuron resolution. additionally, the labeled and existing vectorized datasets were registered to a standard brain space. finally, we designed an objective multilevel evaluation method to prove the accuracy."
"by aligning metadata 1 to the allen ccfv3, we presented the registration results of vectorized neurons. vertebral neurons were manually traced from unregistered brain 4 ( figure 9a ). using the proposed vectorized registration method, we deformed and completed the spatial localization of the metadata ( figure 9b ). for comparison, we also presented the results of three-dimensional pre-and post-registration in horizontal, sagittal and coronal ( figure 9c )."
"in figure 6, we study the effect of the traveling time on the number of additional uavs. changing the traveling time will not affect the number of additional uavs when applying sm. on the other hand, it will be a critical issue to choose the appropriate traveling time when applying cle. when increasing the traveling time, the wasted energy during traveling will increase and the coverage time will decrease. hence, the chance to visit other subareas will decrease."
"where t h is the fourth of the quadcopter total weight in n, q is the density of the air in kg/m 3, s is the rotor swept area in m 2 and p is the power consumption of electronics in watt."
"now, we formulate the continuous coverage problem. in order to present the problem formulation, we introduce the binary variable x m that takes the value of 1 if the uav m visits any subarea from charging station during the coverage duration t and equals 0 otherwise; the binary variable y t ij,m that takes the value of 1 if the if the uav m moves through edge ij during the time slot t and equals 0 otherwise; the binary variable z t j,m that takes the value 1 if the uav m covers the subarea j at time slot t and equals 0 otherwise. table i provides a list of the major notations used in this paper."
"where m p is the payload mass in kg, m v is the vehicle mass in kg, r is the lift-to-drag ratio (equals 3 for vehicle that is capable of vertical takeoff and landing), η is the power transfer efficiency for motor and propeller, p is the power consumption of electronics in kw and v is the velocity in km/h."
"(3) model2: using the histograms of corpus callosum (cc), hippocampal region (hip) and cerebellum (cb) of the allen ccfv3 nissl stained dataset, randomly assigned the eyebrows, eyes, and mouth of model1, respectively."
"metadata refers to digitized, vectorized information acquired from a raw image dataset, such as vascular structure, neuron projections, and cell distribution. these metadata are scattered in different brain spaces, individual laboratories and projects. using brainsmapi, vectorized dataset could be registered into a standard brain space to complete the integration of these existing and labeled metadata."
"using dataset 4 as a reference brain, we presented an intra-modal registration for poor quality (sample tearing and streaks) images by aligning dataset 5 to the reference brain based on brainsmapi."
"in equations (7), (8) and (9), α and β are constant values which depend on the environment, f c is the carrier frequency, d is the distance between the uav and the user, c is the speed of the light, ξ los and ξ n los are the average additional loss which depend on the environment."
"the complications simulated in the model data also existed in brain images. there are large differences in image quality even with the same modality images. these images are easily disturbed in the processes of sample preparation and imaging, such as obvious sample tearing and streaks caused by uneven illumination (si figure 2) . gray-level or feature based registration methods cannot easily to solve these problem, which prevents us from comparing and analyzing intro-modal datasets."
"brainsmapi. we used anatomically invariant regions during the registration to ensure the objective extraction of a large number of features, making it able to accurately register various datasets."
"however, this strategy will dramatically increase the memory consumption and running time for the tb-scale dataset, simply stacking hardware cannot solve the problem of nonlinear registration of tb-scale dataset. here, a parameter acquisition and high-resolution transformation strategy were proposed to achieve nonlinear registration of the tb scale dataset at single-cell resolution."
"we used the five-pyramid strategy in both linear and nonlinear registration for acceleration, and mutual information was used as the similarity measure. the entire transformation acquisition step was approximately 3 hours. we obtained the linear matrix m, the nonlinear direct displacement field furthermore, the displacement was presented in a grid form to intuitively illustrate the nonlinear deformation effect of the diffeomorphism method based on regional features (figure 2 )."
"(2) for every voxel in a block, we calculated the spatial coordinate of the corresponding voxel in roi blocks by the mapping relationship (equation 2) and used the grayscale value of the coordinate as this voxel."
"neuroscientific analysis with the brain spatial orientation requires matching the dataset to the standard brain space coordinate system to obtain anatomical boundaries. more general and common analysis skills involve combining multi-modality and multi-scale datasets to reveal the structural and functional relationship of the brain, such as mri, optical imaging, even electron microscopy datasets."
we used the low-resolution registration result (10 figure 8g ). we also presented the registration effects by comparing the pre-and post-registration of a coronal image with the allen ccfv3 superimposed ( figure 8dg ). the neuron fiber morphology was corrected and the complete and continuous structure was simultaneously achieved ( figure 8eh ). fine and weak signals also remained consistent after registration.
"we registered four types of datasets (dataset 2-4 and 6) to allen ccfv3 (dataset 1) and presented the registration results. from top to bottom, the information corresponds to the registration results ( figure 6 ) of dataset 2-4 and 6. we selected three coronal sections in the whole-brain with the form of the allen ccfv3 on the left and the yellow dotted line of the allen ccfv3 superimposed on the original image to show our results. the brain outline and big brain regions such as hip and cb were well aligned. brain regions without obvious anatomical boundaries such as th, hy, mb and cb also had a good alignment with the allen reference which could be judged from their spatial orientation."
"(5) model4: on the basis of model3, set a gray level of zero in a triangular area of the mouth to simulate a tearing condition for the samples."
"the whole pipeline of brainsmapi is shown in figure 1 . we proposed a regional feature extraction method that can accurately, objectively and sufficiently extract features. this method would not be affected by factors such as sample defect, image quality and imaging modality that are crucial for ensuring the robustness of registration. based on these, with a parameter acquisition and high-resolution transformation strategy we achieve the whole-brain nonlinear registration of the tb scale dataset at single-cell resolution. we blocked a large volume dataset, and used high-performance compute to efficiently transform each block in parallel. the pipeline included image preprocessing, regional features extractions, accurate transformation acquisition and nonlinear transformation for a large volume dataset. we will describe the technical detail below."
"in equation (5), p t is the transmit power, p r is the required received power to achieve a snr greater than threshold γ th, l(r, h) is the average path loss as a function of the altitude h and coverage radius r."
"in addition to the differences in image quality, the more general differences in the experiment are individuals. here, we expect to show registration results of large different individuals. using a way of measuring the distance of anatomical landmarks [cit], we found that the greatest different individual pair was dataset 2 and dataset 4 between the four dataset types (dataset 2 (mri), dataset 3 (most), dataset 4 (bps) and dataset 6 (stp)) (si figure 4), and we also shown the distance between these two datasets in figure 5a (median 596.9 μ m)."
"according to the above registration results, we first assessed the accuracy at brain-region level, a rough assessment (see method). we evaluated ten brain regions in brain-wide for the five datasets. the results are shown in box plots ( figure 7a ). the median dice score of all brain regions and datasets were above 0.9."
"image preprocessing aims to obtain high quality images for good registration results (figure 1 preprocessing). first, denoising the original image, such as brightness correction and light-field correction to reduce the uneven staining and illumination of optical microscopy imaging [cit] was necessary, next, we performed a preliminary rotation on the dataset to ensure proper orientation with the reference image. then the corrected dataset was sampled isotopically. finally, we used the adaptive threshold method [cit] and morphological operations (hole filing, opening and closing operators) to extract the mouse outline and remove the background."
"the regional features of our method are no limited to the cytoarchitectural image data, reflecting its wide applicability. as long as the anatomical region can be identified in the image, the method can be used for registration. however, the selection of regional features is not absolute, and can be autonomous based on the image characteristics or experimental requirements until the registration results meet expectations. an ideal situation is to select all the brain regions for registration, which will obtain the best results, but the cost is too high. in this paper, we recommended 14 regions based on our experience, aiming to obtain sufficiently accurate results with lower costs."
"using dataset 2 as a reference brain, dataset 4 was registered. by merging dataset 4 (green channel) to dataset 2 (purple channel), we presented the inter-modal registration results of large individual differences with horizontal ( figure 5b ), sagittal ( figure 5c ) and several coronal sections ( figure 5d ). roughly, the brain outline and big brain regions could be well aligned. moreover, the corresponding enlarged views were given in figure 5e, nucleus such as cb, hip, aco, cc, cp, viin could also be well mapped."
"the accuracy of feature selection has a direct relationship with the registration results. here, using the advantages of brain anatomy information, we proposed a method to extract anatomically invariant regional features to register two brain datasets. the extracted regional features with anatomical meanings, that is, brain regions or nuclei that were also conservative to delineate the boundaries of conservative brain regions or nuclei in three-dimensional space. compared to observing a single anatomical point in three-dimensional brain space, extracting the boundaries of anatomical regions is more accurate, objective, and contains information such as shape and size."
"first, the transformation parameters for the low-resolution dataset (10 μ m isotropic) were obtained based on the above transformation strategy. then we acquired the transformation parameters for the high-resolution dataset as follows:"
"in addition to the design of identifiable nuclei level evaluation at 10 μ m resolution, an amap [cit] approach was used as a reference. nine nucleus were selected brain-wide (anterior cingulate area (aca), primary visual area (visp), primary somatosensory area (ssp), reticular nucleus of the thalamus (rt), ventromedial hypothalamic nucleus (vmh), periaqueductal gray (pag), subiculum (sub), entorhinal area, lateral part (ent1), medial vestibular nucleus (mv)), and included both distinguishable and indistinguishable boundaries. twenty-two trained technicians segmented these regions in five registered datasets (four type). first, we selected a representative coronal section in the ccfv3 for each nuclei, and a stack (40 sequences) of corresponding position in the registered datasets was also provided. then, these 22 individuals needed to identify a single coronal section from the stack that they considered most similar to the reference coronal section and segmented it. based on this procedure, the staple algorithm [cit] ) was used to fuse 22 human segmentation results, and obtain the spatle results. the dice scores of each segmentation result and spatle results were calculated as human performance (hp), and the dice scores of the ccfv3 and spatle results were calculated as registration performance (rp). then, correlation analysis was performed."
"in contrast, we also listed the effects of other registration methods on the model data. the results of manually selecting feature points (eg. tps with manually landmark) are randomness due to the subjectivity, limited numbers and inconsistent location of the selected feature points ("
"contrary to the related work above, we integrate the recharging requirements into the coverage problem and examine the minimum number of required uavs for enabling continuous coverage under that setting (see figure 1) . to the best of our knowledge, this is the first study that jointly considers the coverage and recharging problems where multiple subareas are to be covered. our main contributions in this context are: 1) we formulate the problem of minimizing the number of uavs required to provide continuous coverage of a given area, given the recharging requirement. 2) we prove that this problem is np-complete. 3) due to the intractability of the problem, we study partitioning the coverage graph into cycles that start at the charging station. 4) based on this analysis, we develop an efficient algorithm."
"after accurately extracting features brain wide, we needed to map these features to acquire accurate transformation parameters (figure 1 transformation acquisition). the process of transformation acquisition is an optimal problem. the moving image is warped to the fixed image by initial transformation parameters, and the similarity metric of moving and fixed images is used as an energy function. iteratively, the transformation parameters are updated to achieve an optimal solution and obtain the corresponding transformation. the transformation is composed of linear and nonlinear parameters, where the linear parameter can be simply represented by matrix m describing the translation, rotation, and scaling of 12 degrees of freedom, and the displacement field φ can represent the nonlinear parameter. here, we chose a recent nonlinear registration method, symmetric diffeomorphic normalization (syn) [cit] . syn customizes the symmetry deformation based on the standard method of the large deformation diffeomorphic metric matching (lddmm) proposed by beg [cit] . syn can flexibly record the displacement for each pixel with a large deformation and produces a diffeomorphic transformation of symmetric and invertible. this symmetrical method of processing direct and inverse simultaneously is also reflected in its energy [cit], and v1 and v2 are the velocity field in opposite directions. physically, the distance drives each pixel to move, which is determined based on the image potential energy. when the original images are replaced by our extracted regional features, the movement of each pixel is determined by the potential energy of features, which will not be disturbed by large differences in image gray level and will not fall into local minimums caused by image noise. in short, when we replace i and j which represent the original images with the feature images i 0 and j 0, we can obtain accurate transformations."
dataset 6 is a collection of published article [cit] please refer to the si table 1 for the main information of all image datasets (si figure 1 ) and metadata.
"(1) for each block in transformed space, we obtained the corresponding blocks in their original space by transforming all points on six surfaces of the block with the mapping relationship (equation 2). then, the identified blocks (roi blocks) in original space were loaded to memory."
"to visually demonstrate the effectiveness and robustness of brainsmapi, we designed five simple, cartoon models with smiley and crying faces. the process is as follows."
"in figure 2, we uniformly divide the geographical area into 16 subareas and apply the cle algorithm to find the cycles with minimum number of additional uavs. from the figure, we notice that 5 cycles are needed to cover all subareas with 5 additional uavs. also, we note that the paths of the cycles are intersected in many locations. to avoid the collisions between the uavs, we operate the paths (cycles) at different altitudes with small altitude differences."
"where t coverage is the time that the uav allocates to cover all subareas in the cycle, t is the time that the uav needs to travel from subarea i to subarea j and t charge is the time that the uav needs to recharge the battery at the charging station."
"here, an interactive segmentation tool, amira (version 6.1.1; fei, mérignac cedex, france), was chosen to perform the feature extraction procedure (figure 1 extraction of regional features). briefly, the selection of extracted brain regions and nucleus needs to follow these three criteria."
"when performing fixed point computations for systems of industrially interesting sizes, exploring all states in the composed model explicitly can be computationally expensive, in terms of both time and memory, due to the state space explosion problem. we tackle this problem by representing the models and performing the computations symbolically using binary decision diagrams (bdds), powerful data structures for representing boolean functions. for large systems where the number of states grows exponentially, bdds can improve the efficiency of set and boolean operations performed on the state sets [cit] ."
"a backoff delay of 0.1n ms and a denial delay as 2n ms are used. each request is generated within these parameters with random ingress and egress nodes. the number of functions in the functionsequence (chosen randomly) are min( √ n,random (5,...,15) ). each nfvenabled node is provided with a capacity support approximately min( √ n,10) flows on it. each link is given enough capacity to support an average of five requests (chosen randomly). the whole experiment is repeated 10 times to discard results due to anomalous behavior. following parameters are tracked:"
"another next-generation network technology, softwaredefined networking (sdn) [cit], is a paradigm of networking that separates the control and the data planes. in this architecture, the data plane is primarily responsible for forwarding the data packets through the network based on flow rules. a centralized controller (or a set of synchronized centralized controllers) is responsible for computing the flow rules based on the state of the network to be pushed to the data plane. a protocol is required for communication between the centralized controller and the switches that implement the forwarding, with openflow [cit] being a leading example. sdn allows commodity switches in the data plane to be controlled by control software through standard interfaces. moreover, in this architecture, the controllers can be modified and upgraded easily to effect changes to the behavior of the network as a whole [cit] ."
"(ii) the local biological and physical properties of dna may have a certain relationship with the promoters, which plays an important role in identifying them but were utterly ignored. (iii) few web-servers were provided as the predictors, and hence their usage is quite limited [cit] ."
"consists of the initial values of v i . if the set of marked locations, valuations of a variable v i, or a clock c i is empty, then the entire domain is considered as marked:"
sdn and nfv are complementary [cit] in nature as illustrated in fig. 2 . they have similar objectives of replacing proprietary and closed network elements (switches) fig. 2 software-defined networks and network function virtualization. sdn and nfv are complementary in nature. they have similar objectives of replacing proprietary and closed network elements (switches) with commodity hardware that can be controlled by standardized software. a software-defined networking. b sdn ad nfv with commodity hardware that can be controlled by standardized software.
"the rules are added in sequence starting from the destination switch and the last rule is added at the source switch in the path. after this, the incoming flow packets are switched across the data plane. once the time defined in the description packet is elapsed, the flow is said to have completed. once the flow is completed, the request id is removed from all allocations stored by the sdn controller. at this point, the bandwidth capacity is freed in the links and dynamic cost for the given request for all relevant allocations in the sdn controller. this may also lead to some allocations serving no active requests in the nfp system. the controller triggers a teardown for all such allocations-i.e., it tells the mano layer that the vnf container is no longer required and can be vacated to free the resources. after a corresponding teardown delay, for each such null allocation, the instantiation cost for the vnf container is freed at the switch. this sdn-enabled framework harnesses the power of individual heuristics for each of the components and allows a consolidated feedback-based approach for optimal online service of vnf chaining requests."
"where the variable in the formula, 3mer 1 equals aaa,3mer 2 equals aac, …, 3mer 8 equals ccc. therefore, the sample of eq.2 can be expressed as:"
"the results obtained are shown in fig. 8 . even for large 48-pod fattrees containing 30,528 nodes, the time taken for end-to-end requests was around 10 min. for coreto-end requests, the time taken was 2.5 min, which is relatively acceptable."
"it may so happen that there is no possible route or that for the chosen route there is no feasible network function placement found. in such cases, the solution for the request is delayed by a certain amount described as the backoff delay of the system. the solution is reattempted periodically until a threshold upper bound value denoted as the denial cutoff above which the request is denied and the high latency path through the controller is used for the duration of the flow. however, if the solution is found, then the sdn controller removes the bandwidth and resource capacities of the links and nodes respectively according to the allocation in its internal view of the nft. it then invokes the mano layer of the nfv architecture to migrate the required vnf containers to the required nodes and to scale each of the vnfs to the updated bandwidth. thus, the migration delay for a request is defined as the maximum migration delay out of all the migrations of vnfs required for the request. we also define startup delay as the maximum sum of migration and startup delays of the vnfs required to meet the needs of the specified allocations."
"in step 3, the time evolution bdd is constructed, which will synchronously extend the target valuations of the clocks. the characteristic function of the time evolution bdd is, [cit] . in line 7 clocks c 1 and c k are synchronized, yielding a bdd, i.e. b 1, representing all pairs of valuations, where the difference is i − j. in lines 12 and 13 the saturation function ̺ is implemented."
"as in all sdn-enabled technologies, only the centralized controller will have the full view of the nft and fig. 5 sample run of the heuristic algorithm for dynamic nfp. the allocation of vnfs for a given incoming request along a given candidate path chosen by the hybrid dijkstra's algorithm is shown. in this example, it is assumed that vnf is already allocated on node 4 and the request is handled using divide-and-conquer method. a path assuming allocated on 4. b after allocating on 4. c after allocating on 2. d after allocating on 7"
"in this next section, we define the problem of dynamic nfp formally and describe a modular approach which harnesses the power of independent optimal solutions of each of the individual components, namely, routing and allocation. while the problem of placement of virtual network functions (vnfs) has been widely studied, most solutions either look at routing of flows and optimal allocation of vnfs on the route individually or are too slow for practical implementation as online vnf solvers."
"problem statement: find the placement of network functions among the switches in a network given a set of flows, with each flow taking a pre-computed path through the network and requiring a linear partial order of network functions, each of which has an instance cost and a service cost associated with it, such that the overall cost of placing the network functions for all given flows is minimized."
"proposed protocol for solving nfp. a centralized sdn-enabled approach to nfp is illustrated in this figure. an sdn controller uses the optimal path allocation algorithm and leverages the nfv mano layer to accomplish nfp for incoming requests be able to query the remaining bandwidth or resources of its elements. also the controller will be a reactive agent, which will respond based on the packets or requests that it receives. we also assume that the mano layer of the nfv architecture can communicate with the sdn controller, as is prevalent in stateof-the-art sdn-nfv combined approaches. whenever a new flow enters the nft, the first switch receives a description packet, which is forwarded to the sdn controller. this description packet contains the description of the vnf sequence that the incoming flow requires, the destination node, the bandwidth and the time for which the flow of packets will occur. while the controller is finding the optimal route and allocation for the flow, the packets are routed through the controller to the destination via a high latency path. the controller is assumed to have sufficient resources to run all possible vnfs always to service the chaining request of this flow while the dynamic nfp solution is being calculated [cit] ."
"been encapsulated in specialized hardware known as middleboxes. the middleboxes are usually proprietary with a closed architecture. thus, they impede the cycle of innovation and entail expensive upgrade cycles when the network as a whole must be upgraded to a larger scale. in recent times, middleboxes have been virtualized and replaced by virtual network functions (vnfs) that are more flexible and can scale on demand."
"the results have been shown in fig. 10 . as is evident from the results, the path computation for end-to-end nodes for k 2 flows takes on 70-pod fat trees 28 s, the nfp computation time is 11 s and the migration time is 7 s. thus, when a new request comes, it takes about 1 min for the flow to shift to the data plane for a 91,875-node topology. most of the current large-scale data center networks are 48-pod fat-trees (30,528 nodes) for which the combined delay in worst case is just 8 s. this shows that our protocol can easily scale to the largest of data center networks."
"an nfpsystem is an instance of the proposed algorithm that solves the nfp problem or a sub-problem. by the definition of the nfp problem, an nfpsystem takes as input the list of possible network functions, the allocations that were done a priori, the set of tenant requests that need to be serviced, and the network describing currently available resource capacities of the network elements. it returns with the set of allocations for the given inputs that has the minimum cost among all possible sets of allocations."
"in this section, algorithm 1 (dca) is shown to be complete and sound. then, the time complexity of the dca-h algorithm is discussed to justify that it is polynomial when all the subsystems of a given system are run in parallel. further, the solution is empirically studied with various parameters and finally shown to scale to even large stateof-the-art dcn topologies."
"data center networks are some of the most important places where our proposed protocol can be primarily used. in dcns, there are two types of flows:"
"an action function a i that does not update a variable or clock is denoted by ξ. the semantics of an action function can also be represented by a relation,"
"note that the function chains and the network paths get smaller for the newly created instances of the nfpsystem. eventually, the new set of requests will contain requests with a trivial path that has only a single node. at this point, a simple feasibility check is used to check if that node can support all the functions and, if so, they are allocated on that node. otherwise, it is deemed infeasible. another trivial case occurs when a request has an empty set of functions to be serviced along a path. in other words, there are no functions required to be allocated on a given path. these requests are discarded since they are trivially satisfied."
"as there are not enough experimental confirmed negative sequences, negative samples are collected from both coding and non-coding regions. in simple terms, the benchmark dataset s used in this study can be expressed as:"
"in this section, we discuss an approach for the routing aspect of the dynamic nfp problem that also considers the importance of the re-utilizing previously allocated functions which are servicing existing flow requests already. traditionally, link state routing protocols such as the open shortest path first (ospf) protocol are used for (i) discovering and maintaining the link state of the network and (ii) using algorithm such as dijkstra's to compute shortest paths based on administrative weight or path attributes such as latency. we use the overall latency of allocation as the optimization criterion [cit] ."
"1) usually many iterations are needed in the fixed point computations; 2) the intermediate bdds representing the reachable states can be very big that may need more memory than available, i.e., state space explosion. following, we explain how the iterations caused by the \"tick\" event can be eliminated in the bdd implementation to tackle the above-mentioned issues."
"in this section, we describe a centralized sdn-enabled consolidated approach to solve the problem described above with individual components, as illustrated in fig. 6 . in a traditional network, a set of hardware-based middleboxes and other commercial switches are used to implement the necessary network functions. for the purpose of our discussion, we will assume that the network is made of nfv-enabled commodity switches that are capable of being controlled by an sdn controller connected directly to each other. let the topology of such a network be termed as network function topology (nft). this allows the simplification of discussion by avoiding discussion about conventional switches in the network."
"the rest of this paper is organized as follows: first, the previous work done by researchers on this problem and the need for better algorithms are described. next, the proposed algorithm for solving static nfp is described. in the subsequent section, the above solution is extended with a complementary routing algorithm for dynamically servicing sfc requests. later, a framework is proposed to harness the power of sdn to use the above algorithm for real-time service of such requests. simulation results with appropriate inferences and analysis to highlight the major advantages are then discussed. finally, in conclusion, the proposed protocol, empirical results, and future problems to be solved are summarized."
"next, the flow rules for the request are pushed by the controller in the respective switches. at every switch, the rule will be as follows:"
"the static nfp problem deals with the problem of servicing a static set of incoming flows, each with an sfc requirement, where the routes are provided by an oracle. this reduces the nfp problem to that of allocating the vnfs appropriately along the routes to optimize the use of available resources. even this static version of the nfp problem has been proven to be np-complete [cit] . the dynamic nfp problem involves handling dynamic requests for flows with sfc requirements, where even the routes need to be decided by us."
the nfv architecture gives rise to many interesting problems. one such problem is about the number of instances of the various vnfs required and their placement in the network [cit] . each flow in the network requires a chain (either a strict order or a partial order) of functions that must process the flow according to some defined network policy. such a chain is termed as a service function chain (sfc) [cit] . each instance of a function can be applied to several flows and takes up resources in the network that are constrained in their availability. thus the placement of these functions is a complex optimization problem that takes into account the requirements and the constraints and the overall objective of minimizing the consumption of resources [cit] .
the main contributions in this work are the following: (i) a new divide-and-conquer approach for the problem of static nfp which is theoretically proven to be sound and complete; (ii) a heuristic version of the above solution which allows users to trade off accuracy for fast computation times; (iii) an extended version of this heuristic for dynamic nfp along with a complementary and compatible routing algorithm; (iv) a framework for using the above heuristic for real-time online service of sfc requests; (v) a detailed theoretical and empirical analysis of time complexity and the trade-off between the quality of solution and time complexity of the algorithm.
"algorithm 1 searches for optimal allocation of all tenant requests exhaustively. there are many ways the algorithm can be made more agile by sacrificing the completeness and the soundness of the algorithm. in this section, we 3 sample run of divide-and-conquer algorithm. at each step of the divide-and-conquer algorithm, a vnf allocation is made and a new instance of the problem is spawned corresponding to the allocation. the tables shown list the current capacities of the nodes, instance, and service costs of the vnfs and the outstanding requests to be handled at each step. a original set of request. b after allocating on 2. c after allocating on 2. d after allocating on 1. e after allocating on 3 present dca-h, which is a heuristic version of the algorithm and is more customizable and agile. note that this algorithm is called agile since it is exponentially faster than all previous solutions and customizable since there are multiple parameters that network administrators can tweak to improve the performance according to their available compute resources. the first and most effective way to improve agility is to bound the branching at every nfpsystem instance. this can be enforced by only looking at top t of the allocations possible at every step for the optimal cost solution. once the branching is curtailed, the algorithm becomes significantly faster. therefore, to achieve this optimization, only the top t allocations in line 4 of algorithm 2 are considered. this requires sorting the allocations in a particular order beforehand such that the top t allocations will reasonably represent the overall system. some of the possible sorting orders are the decreasing order of number of requests each allocation serves, decreasing order of data rate of the flows that each allocation serves, or the decreasing order of cost to the system due to the allocation."
"although phylogenetic foot printing takes the advantage of relatively conservative of motifs between species [cit], these motifs are short and not complete species [cit] . for example, in table 1, the number of sites contributing to the construction of motif 1 only 47, which may result in a great deal of false positive results. therefore, it would be practicable to turn to the machine learning-based methods and has been proved to be effective in many fields [26, [cit] ."
"as described in previous sections, for the static nfp problem, the paths of the flows are provided by an oracle. the computation of such paths is left for the dynamic nfp solution. given the paths, we attempt to address the problem of placement of network functions, subject to the various constraints such as the sfc requirement of each flow and the resources available at each switch to deploy the functions. we consider the chain of functions to be of linear order. that is, functions that split or merge the flows are not considered. similarly, we restrict our discussion to network functions that can be deployed on the network switches [cit] . we call these nodes as nfvenabled nodes. given the flows through the network and their sfc requirements (sfc k ), we need to place function instances at the switches such that the overall resource cost is minimized."
"since the path is also pre-defined for each tenant request, the only information about the network that is required is the resource availability of the network elements. an instance of a network function placed at a specific switch to service a set of one or more tenant requests is called an allocation. note that this can be a subset of the set of all requests passing through this switch with the given vnf in their network function sequence. this may be caused due to the limited availability of resources on the node. the output of the algorithm is a set of allocations that satisfy as many input tenant requests as can be fully serviced."
"in this paper, we presented a method to efficiently compute a minimally restrictive nonblocking supervisor for a timed des. the system is modeled by tefas, ordinary automata extended with variables and clocks, and the supervisor is symbolically computed based on the tefas' corresponding tick-efas using bdds. however, in the bdd-based fixed-point computations, the \"tick\" event is eliminated by abstracting the tick-efas. this leads to less iterations and smaller intermediate bdds in the fixed-point computations. as a case study, we applied our method to a classical production cell. there are some possible directions for future work that we are currently working on. from an sct perspective, we still does not handle controllability. from a modeling point of view, we desire to be able to model invariants, i.e., deadlines that must be satisfied, by tefas. finally, we also desire to develop efficient algorithms for quantitative analysis such as time optimization, beside the qualitative analysis (supervisor synthesis). the interesting point about optimization on tefas is the existence of uncontrollable events that may lead to several optimal solutions."
"consequently, there are many models and implementations that are suitable for quantitative analysis (such as time optimization), most of them based on continuous time; and there are many that are suitable for the sct framework (qualitative analysis), most of them based on discrete time. yet no work exists considering both aspects. in this paper, we attempt to combine the best of both paradigms by using timed efas (tefas), efas equipped with a finite set of discrete clocks, where the value of each clock is increased implicitly as time progresses. based on tefas, inspired by the \"zone\" concept from the timed automata community, we symbolically compute a minimally restrictive nonblocking supervisor by using bdds. the main feature of our approach, in the context of sct, compared to most of the other approaches with discrete time, is the elimination of the \"tick\" event in the bdd-based fixed-point computations. in most of the cases, this leads to less number of fixed-point iterations and more compact bdd representation yielding a more efficient implementation. furthermore, from a modeling perspective, the advantage of using tefas compared to ttms is that the time constraints are added as guards on the transitions (as in timed automata [cit] ), rather than lower and upper bounds on the events. this could potentially facilitate the modeling for the users. for instance, if the constraints are associated to the events, it will be complicated to model the situation, where the user wants to put different time constraints on an event that appears on different places on the same model. usually the consequence is a larger state space. the mentioned advantages are demonstrated in section v."
"a notation that will be used frequently in this paper, is the sos-notation (structured operational semantics) used to define the transition relations. the notation premise conclusion should be read as follows. if the proposition above the \"solid line\" (premise) holds, then the proposition under the fraction bar (conclusion) holds as well."
"in this section, we formally describe the problem statement for static nfp. then, we present our sound and complete algorithm to solve the problem. a customizable, agile heuristic version of the algorithm that is exponentially faster than the state-of-the-art solutions is proposed. we define completeness of an nfp algorithm as its ability to consider all possible outcomes and generate a solution if one exists and soundness as the property of the output being the optimal solution. also, a solution is considered more agile than another solution if the algorithm has lower time complexity and customizable if there are parameters that network administrators can tweak to improve the performance according to their needs or available resources."
"base case parameters: given a set of nodes n, we generate √ n requests. each node has a resource capacity of n 0.8 and each request has a random number of nodes in its path size, chosen uniformly from the range n. the effect of increasing the path size and having longer function chains per request is shown in fig. 7a which shows that the computation times is of the order of seconds in the scenarios considered. it is seen that having longer function chains affects performance more adversely than having longer paths."
"the network function placement (nfp) problem that requires meeting the service function chaining (sfc) requirements of data flows in software-defined networks is a critical problem in the field of network function virtualization. this paper proposes a solution based on the divide-and-conquer strategy for the nfp problem and shows that it is complete and sound. the solution is applied to both static nfp requests and dynamic nfp requests. it is shown that this model can be customized to become a heuristic algorithm which trades off precision for time complexity using various parameters. it is theoretically shown that there exists a set of parameters for which our algorithm has a complexity that is just a function of the topology and not dependent on the number of requests received. the algorithm was studied for various parameter values to show their influence on the time complexity of the heuristic algorithm. based on the results, the algorithm scales to large state-of-the-art dcn topologies. in the case of dynamic nfp, existing solutions to optimize individual components of this complex problem are too slow for being used as online nfp solvers. this paper combines the divide-and-conquer approach to nfp with a modified dijkstra's algorithm to propose an online nfp solver. moreover, a framework based on sdn controllers and the etsi mano architecture is presented for deploying flows with corresponding vnf chains. the proposed protocol is simulated on non-symmetric as well as dcn topologies and it is shown that even for dcns beyond the scale of current use, the algorithm takes time in the order of seconds. with further work, these heuristic components can mature into fast and more optimal algorithms meeting the constraint of solving the problem in real time. there is also a possibility of adding fault tolerance to the proposed protocol to handle failures of links, nodes, or vnfs."
"nair [cit] came up with electron-ion interaction pseudopotentials (eiip) value of nucleotides a, g, c, t. the eiip value based methods have been shown effective through previous studies, such as the recognition of gene f56f11.4, prediction of the cystic-fibrosis gene [cit], recognition of enhancer [cit], and so on [cit] . the electron-ion interaction pseudopotentials value for the nucleotides [cit] are shown in table 2 . we let eiip a, eiip t, eiip g, and eiip c denote the eiip values of nucleotides a, t, g and c, respectively. then, we employed the mean eiip value of trinucleotides in each sample to construct feature vector, which can be formulated as:"
"in this section, how the algorithm scales to bigger topologies with a large number of nfv-enabled switches/nodes is analyzed. for each n-node topology, following arrival rates of requests are studied:"
"the results have been shown in fig. 9 . the path computation time (fig. 9a ) seems to be a quadratic function of the number of nodes (n) which is aligned to the expectation since the algorithm looks at approximately n candidate paths of approximately n length. although, for 1000 nodes and heavy requests, the path computation takes 1.6 h, if we consider only the nfv enabled nodes on a topology, the number are not expected to exceed 500 nodes even for a 10,000-node topology where the time taken is 5.4 min. also, faster and better routing algorithms can definitely reduce this time overhead. the nfp computation time (fig. 9b) for 1000 nodes and heavy requests is 920 ms at an average and is only a function of the size of the path and the function sequence. since the path size is constrained to 1000 in the worst case and the function sequence to 15, this result is pretty reasonable to characterize real-world situations. the feasibility delay (fig. 9c ) of 2600 ms for 1000 nodes and heavy requests is actually more than the nfp computation delay which suggests that, either better nfp solutions can be considered until the feasibility delay becomes less than the nfp computation delay, or the backoff delay and the limit for denial delay can be reduced. the migration delay (fig. 9d) for 1000 nodes and heavy requests is 1463 ms though real-world values of migration delays will give a better view on this particular factor."
"we recursively create parallel nfpsystem instances to identify the optimal allocation by exploiting the independence of one allocation from the other. once a feasible allocation has been identified, this allocation is added to the current list of incoming set of allocations to be passed to a sub-system to handle the new set of requests."
"to summarize, we proposed a complete and sound algorithm for static nfp in this section and provided a customizable agile heuristic version of the same, which allows tenants to trade off precision for better computation time."
"in this section, the execution time (in ms) of the proposed algorithm is studied with varying parameters. the execution times for randomly generated connected topologies are studied first, followed by dcn topologies of k-pod fat trees."
"although virtualizing the network functions improves flexibility and accelerates innovation, this abstraction introduces new shortcomings that need to be addressed to realize such functions in actual deployments. in this paper, we focus specifically on the problem of network function placement (nfp). for every data flow entering the network, we are required to route them through the appropriate network functions and in a pre-defined sequential order. for example, a firewall function may be followed by a deep packet inspection function in the order. however, we also need to adhere to bandwidth and capacity requirements of the links along the paths between these vnfs while routing the data flow through a chain of such vnfs. further, this problem is complicated by the fact that the allocation of these vnfs at the nodes must be simultaneously solved. therefore, trivial shortest path routes might be inefficient for the purpose of nfp since inefficient routing and function placement solutions may lead to congestion in the network. figure 1 presents a network as an example depicting two flows through the switches and the placement of network functions to service the flows. for instance, flow 1 utilizes three network functions, one each at nodes 1, 2, and 4."
"the jackknife test is regarded as a unique random test that can produce the unique result for a given dataset [cit] . therefore, all these parameters were optimized through jackknife test."
"due to the rapid development of genome sequencing technology, large-scale data has been generated [cit], the stable and accurate identification of promoter is an important problem. because standard laboratory methods are time-consuming and performance overhead costing, bioinformatics technologies with perfect precision represent the ideal alternative for massive fast recognition of promoter."
"ii. timed extended finite automata a timed extended finite automaton (tefa) is an efa augmented with a finite set of digital clocks. intuitively, a clock in a tefa is a discrete variable in the sense of efas, restricted by some rules, mentioned later. the time automatically elapses only at locations, whereas the transitions occur instantaneously with zero delay."
"in this section, a heuristic approach is discussed for finding the optimal allocations of functions to resolve the sfc requirements of a tenant request using the path computed using the abovementioned algorithm. when a request for finding new allocations is handled, there may be allocations already done on the given path. thus, either existing allocated functions can be used to serve this flow or due to resource constraints, new virtual network functions must be spawned. using existing allocated functions has the advantage of not waiting for the migration and setup of the vnf instances to be complete, while instantiating new functions will allow load balancing, which may facilitate better service of future nfv requests."
"briefly, 70propred is a prediction model based on support vector machine (svm), which was built by pstnp ss and pseeiip sequence coding strategies. an outline of the computational framework of 70propred predictor is shown in fig. 1 ."
"soundness of an nfp algorithm is the property of the output solution being of optimum cost and if the algorithm does not find a solution, then there does not exist a solution. if the algorithm produces a solution, it is clear that at each stage it has found the minimum cost subsystem after each allocation. also, at each stage, we find and use the allocation which gives us the minimum cost. after each system and its subsystems return the possible allocations, the minimum cost among all possibilities is returned to the parent system. this guarantees that given a feasible nfpsystem, the algorithm finds the optimal solution for all subsystems that are visited. using this argument recursively, dca is proven to be sound."
"this section presents the divide-and-conquer algorithm (dca) which solves nfp and is shown to be complete and sound. the pseudo-code of this algorithm is described in algorithm 1. we create the first instance of nfpsystem with the complete available set of network functions, an empty set of allocations, the complete set of tenant requests (in the format described earlier), and the network describing the resource capacities of all the network switches. it starts by finding the set of all possible candidate allocations."
"(ii) the feature selection algorithms can be used to delete the redundant features to improve the prediction model. (iii) more species of promoters should be adopted to estimate the performance of 70propred method. in conclusion, our future work is to extend this method to other species promoter region prediction. we suspect that our feature extraction methods is not only suitable for identifying promoter, but also for other bioinformatics sequence classification tasks."
"the consolidated approach in this paper for dynamic nfp is in contrast to the above solutions, since we propose an architecture and a protocol which are consistent with current state-of-the-art nfv and sdn architectures. the paper addresses the interaction of the two technologies with each other and builds on the prior research work on this topic by the authors [cit] ."
"we introduce another heuristic by reducing the backtracking upon discovering the subsystem created by a verified allocation is infeasible to solve. this can be accomplished by making lines 11-17 optional in algorithm 2. note that this will have significant performance impact since this polynomially reduces the upper bound on the depth of the nfpsystem visitor tree. the same is true for removing backtracking unverified allocations (allocations that will not meet the resource constraints), which can be done by making lines 22-28 optional in algorithm 2. this will not drastically improve the time complexity but will significantly reduce the soundness of the algorithm. thus, it is recommended that this part is left unmodified in dca-h."
"the v and µmax c, respectively. if a variable exceeds its domain, the result is not defined, and from an implementation point of view, it is upon the developer to decide how to implement such cases. for instance, the program can give the user a warning. in our implementation, values outside the domain are not reachable. in contrast to variables, it is assumed that if a clock c i reaches its maximum value, it will keep its value until it is reset. for a clock c i, this behavior is modeled by a saturation function"
"however, such path computation is completely oblivious to the already allocated network functions in the network and conceivably, a longer path with network functions which can be reused for the request being serviced, can provide lower latency by avoiding the migration and setup latency of the network functions. algorithm 3 is a hybrid dijkstra's algorithm that addresses this problem by considering the presence of network functions on the commodity switches to compute a path from source to destination. the algorithm proceeds as follows. the shortest path between the source and the destination using the traditional dijkstra's algorithm (line 3) is first identified. then, to provide alternatives to the shortest path, k-shortest paths are computed between the source and destination. these paths are retrieved by excluding individual links of the shortest path. this approach is similar to the ones taken by yen [cit] to find loopless k-shortest paths. thus, the path computed upon removal of each edge is computed and added to the set of paths to be checked (lines 4-6). an illustration of the above algorithm is shown in fig. 4 . the traditional dijkstra's algorithm is used to find the path with the least latency (1-2-5-7) from the source to the destination node (fig. 4b) . next, for each edge e in this path, the shortest path is computed assuming e is unavailable or has ∞ latency. as illustrated, there are three candidate paths, namely, \"1-2-5-7\", \"1-3-6-7\", and \"1-2-4-5-7\", each found after excluding one of the edges in the shortest path."
"dca complexity: a given nfpsystem is started by finding all possible allocations. the total number of such allocations is o(nm). then for each allocation, the allocation is verified and the network is updated, both of which takes o (1) . the divide-and-conquer algorithm is then executed to find a new set of requests, which is o(k). in the worst case, the number of new requests is twice the number of current requests. thus the complexity of each run of nfpsystem is o(nm + k). then, the depth that the search tree can grow to is analyzed, which has an upper bound of the number of possible allocations o(nm) multiplied by the maximum number of times a function can be allocated on the node, which is o(k). thus, the upper bound of the depth is o(nmk). the branching at each step is determined by the possible number of allocations, which is o(nm). thus, the worst case time complexity is o (nm + k) · nm (nmk) which is o e poly log (poly) ."
"if the allocation is left with no requests to serve, then the given request cannot be handled in the current nfpsystem, making the nfpsystem infeasible for any allocation of the function on the specified node. hence, no update of the optimum allocation is done."
"because the physicochemical property indexes of nucleotides affect the recognition of promoter, incorporating the sample's average energy of delocalized electrons (eiip), especially, the eiip value of trinucleotides with pstnp ss might boost the performance of the training model, the prediction results are listed in table 4 ."
"in this case, we assumed that the clock is not reset. otherwise, the term χć (b (8) to construct a partial transition relation with multiple clocks, the clocks will not be synchronized. if we add another clock to the model, then the result will be (8) ∧ χḿ"
"for each allocation, the algorithm verifies the cost of servicing the given request. note that we accrue the instance cost of the network function only if an instance of that function is not already allocated on that node. if an allocation is not feasible, then the tenant request that has the maximum dynamic service cost is removed and the process is repeated. the intuition behind this is to minimize the number of instances of the same type of network function (say, firewall) and, in turn, minimize the instance cost."
"the combination of the routing and allocation aspects of the nfp problem makes it intractable and most of the complete and sound solutions of the above problems are too slow for deployment in real networks. however, there exist fast heuristics which solve both of the above problems that significantly improve the time complexity. in this section, we propose a consolidated solution which combines such heuristic approaches. however, since the solution is heuristic, neither the soundness nor the completeness of such a solution is guaranteed. to summarize, the proposed solution is a consolidated approach for vnf placement, which combines the objectives of optimal routing as well as least-cost placement vnfs, to provide a real-world online solution for nfp."
"precision is traded off for time complexity when a smaller value of t is chosen (table 1) . hence, it must be verified whether the performance with smaller values of t is comparable to the larger values of t. three topologies of 25,"
"the chain of vnfs that are required to process a flow is called its service function chain (sfc). we formalize nfp problem as the problem of finding the optimal placement of vnfs for a set of flows subject to their sfc requirements, bandwidth, and resource constraints with the objective of minimizing the overall use of the resources. the nfp problem can be handled either statically or dynamically."
"furthermore, we constructed a heat map to visually show the distribution of feature pseeiip in positive dataset, as shown in fig. 7 . each hotspot in the heat map corresponds to a unique trinucleotide; for instance, hotspot (1, 1) corresponds to triplet aaa. for more detailed information on the heat map, please see additional file 2: table s1 . red squares are positively associated with recognition ability."
"in the corresponding efa of a tefa the \"tick\" event never becomes restricted due to synchronization between the clock-efas. proof: based on the definition of full synchronous composition on efas and the following facts, the statement can be directly deduced:"
"modern state-of-the-art switches usually have three types of resources-network, computational, and storage. to simplify our discussion, we will capture the capacity of a switch to host network functions as a single unified resource (cap i ). it is easy to extend the model to different types of resources. we formally define our problem statement as follows:"
"even though attempts [cit] have been made to develop fast and optimal solutions for both of the above components individually, they remain impractical for real-world implementation. also, optimality of any one of the above components does not guarantee the optimality of the overall solution since the output of one feeds into the other and thus constraining the overall solution. instead, we aim to solve this problem considering both components simultaneously, for developing real-world online nfp solvers. additionally, we propose a consolidated sdn-enabled architecture and protocol to perform realtime solutions for sfc requests."
"where µ v,c is the updated value for a variable or clock. consequently, in contrast to variables, clocks may only be inspected, and reset to zero."
"is defined similarly. the set of evaluations for all variables and clocks is represented by µ v and µ c, respectively. to denote the \"current\" evaluation of a variable or clock we use the notation η."
"a parallel nfpsystem is created as a separate thread to solve a sub-problem and return the set of optimal allocations satisfying that new nfpsystem. if the sub-system that handles the new requests is infeasible with the current allocation then the tenant request with the highest data rate is removed. we repeat this process until there is one request left in the allocation. if the allocation has only one request, then there are no further possibilities for reducing the allocation. we deem such a nfpsystem infeasible which does not yield a candidate for optimum allocation."
"nfv architecture aims to harness the power of stateof-the-art virtualization technologies to virtualize the compute, network, and storage resources of commodity network elements to host vnfs that perform the middlebox functionality. this has the advantage of avoiding investments in specialized hardware, while allowing elastic scaling of the capacity of the functions [cit] . moreover, switches and routers can use their spare computational resources to implement these vnfs [cit] . the vnfs perform the same functions as the conventional middleboxes except for the fact that these functions are now implemented in software. this architecture accelerates innovation of the capabilities of the network functions and supports real-time scaling of the virtualized resources as the number of data flows through the network and their service requirements change dynamically [cit] ."
"only maximal allocations are considered initially, i.e., when a function is allocated on a node, it services all requests that pass through the node requesting that function. this will be the optimal allocation strategy if there were infinite resources on each switch. however, in reality, the capacity of the switches is constrained."
"given the set of sub-systems, each nfpsystem compares the cost of allocations from each sub-system and keeps track of the one with the least cost. similarly, it compares the cost among all such allocations by waiting for all the threads to join and the overall minimal allocation is returned to the parent system which spawned that system originally. this algorithm, thus, performs exhaustive search and is therefore as complex as the current solutions based on miqcp. thus, it has a time complexity of o e poly log (poly) . we provide the detailed analysis in section 6. therefore, a more agile solution that can act as a dynamic nfp solver is required, which is presented in the next section."
"in this section, details of the experimentation and theoretical and empirical analysis of various approaches are presented and it is shown that these approaches are significantly better than prior solutions to the nfp problem."
"completeness of an nfp algorithm is its ability to consider all possible inputs and generate a solution if one exists. this is automatically taken care of for each sub-problem in our case by the definition of the inputs. this is guaranteed for a given linear function chain and a given path for every request and given resource capacity of a network node. thus, dca is proven to be complete."
"the controller first looks for a set of candidate paths that could possibly be optimal routes for this flow. the main constraining resource for routing is the bandwidth of the links in the nft. using appropriate cost functions, the controller finds the best possible route of the incoming request. once the appropriate route is found, this route along with the sequence of vnfs that need to be serviced for this flow is supplied to the online nfp solver. the nfp solver uses the divide-and-conquer approach to divide the request into smaller requests and find the optimal nfp solution."
"in this section, we define the system and its parameters that define the problem and the approach to model them. then, we describe our divide-and-conquer approach to solving the nfp problem. finally, we present a customizable heuristic version of our algorithm which is more agile."
"accordingly, the proposed modification, eq. (7), to obtain a in-degree dependent μ is one of the simplest approaches, since it only varies a parameter using a topological characteristic such as the relative in-degree."
"this paper is organized as follows. first, we review previous work on default propagation in financial networks. next, in sect. 3, we propose a contagion model adapting the well-known sis model. section 4 provides a complete description of the data used, and its topological analysis. section 5 includes a set of experiments to study the main characteristics of default propagation in each inter-connected sector. later, we examine the implications of using our default propagation model in sect. 6. finally, sect. 7 provides some conclusions and future work."
"naturally, the observed dynamical behavior is the result of an interplay between the mmca framework and the network topology. to understand the reasons behind the different sectors' response to default propagation, we will characterize the dynamical be- which characterizes how a sector affects the system. the relationship between these two dynamical descriptors and the network structure will be explored next. as mentioned before, these are defined at the steady state, but it is also important to understand how dynamics evolve in the transient regime. this analysis will be carried out by synthetically concentrating defaulted companies (in specific proportions) in the different sectors and exploring pair-wise sectoral interactions at the initial steps of the simulation."
"the methodology also allows for another kind of experiments; what we call synthetic default assessment. previously, we have performed a sensitivity analysis using the real default distribution at the initial step. now we are going to explore what happens when the initial default nodes are concentrated in a given sector and repeat the analysis for every sector. previously, we have focused on the stationary state, where dynamic properties such as s and r 0 were calculated without any dependence on the initial conditions. now, we are interested in the transient phase, when the system has not reached the stationary state yet. in this transient phase, we can gain insights on the speed of default propagation among sectors."
"we have proposed a computational model, based on the probabilities of default contagion, to study the default diffusion at individual and aggregated levels. we have performed massive experiments based on this model by varying several parameters such as the initial default rate, the contagion rate β and the recovery rate μ. this methodology allows us to vary the parameters at the individual level to account for a more realistic scenarios. our results show the relationship between dynamical and topological properties for more than 140,000 bbva firms aggregated at a economic sector level, and also allow us to create a ranking of sectors by sensitivity to default, which can be used in potential applications. for future work, we would like to enrich the network adding different types of payments such as national transfers or direct debits, extending in this way our computational model to a multiplex network, finally we want to enrich model parameters considering for example companies' revenue. we acknowledge support of the publication fee by the csic open access publication support initiative through its unit of information resources for research (urici). the funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript."
"independently of the particular economic insights that may arise from these analysis, the methodology proposed in this paper has a greater advantage. it allows to perform experiments in a massive way. these large amount of data has enabled us to study how the time it takes the system to arrive to the steady state (convergence time) depends on a set of parameters such as the initial default rate or the infection rate β. this may have practical applications. for instance, if we can establish a relationship between simulations convergence time and real time, the risk departments could take advantage of this understanding to estimate the speed of default propagation among sectors."
"the original mmca model was designed to cope with the propagation of epidemics [cit], where the states of the agents (nodes) forming the network of contacts where binary, namely, susceptible or infected. in well-mixed populations, the differential equations governing the number of susceptible (s) and infected (i) individuals are"
"a technician who has been extensively trained through in-house and on-site sessions replaces the rop expert in screening infants for rop and uploading images. the technician analyzes the images using a three-response triage format: red, orange, green. the cost of the rop specialist is reduced by this initial screening of noncritical cases. nn partners with i2i telesolutions to develop a system that enables images to be uploaded using mobile connections to a customized tele-pacsserver and then read in real time by remote experts (doctors) on their pcs, iphones, and ipads. doctors then transmit their reports to the secure data server, complete with a digital signature, using a cell phone or a pc, and these are accessed by the technician in the rural hospitals before a child is handed over to the mother. this saves the infant's parents another trip to the partner hospital for the report, thus improving geographical access. kidrop team experts and locally trained ophthalmologists also provide treatment at the rural center. the iphone application allows images to be viewed similarly to how they were viewed on the pc and even allows comparisons with previous visits. the expert can record findings and create a report using a customized template that is submitted to the server using the gprs cellular network. this reduces dependence on the variable speeds of the internet and allows the expert to view and report \"on the go,\" thereby improving geographical access to the expert. the low cost realized through economies of scale and scope further helps enroll additional partner centers and hospitals. differential pricing for poor patients further increases numbers and geographical access. the project leader also has the discretion to waive a patient's fee, which helps provide access for many more patients."
"since economic crisis often start in a given sector and later expand to others [cit] financial crisis), we are also interested in exploring the dependence of default propagation on attributes related to each sector, such as sectoral default probability density and sectoral inter-connectivity. in particular, to study the dependence on the sectoral default probability density we rewrite eq. (6) for a each sector (c) as:"
"customer-supplier relationships highly depend on economical sectors and the financial situation of the companies involved. to properly model this situation with real data we gathered anonymized quarterly data from the official customer-supplier third party payment declarations collected by the bbva risk management department. this declaration is used as a mechanism to avoid fraud in company vat declarations. here, spanish firms (our nodes) inform about their supplier payments and customer earnings. for each available company, we extracted its operating revenue and financial statement attributes: sector and default status. [cit] . default labels at the initial step were set to 0 or 1 depending on whether a company was in default or not at the beginning of this period. by using customer-supplier relationships, and after removing self-loops, a directed and weighted network with 142,477 nodes and 255,509 edges was obtained. direction of edges follows the path of money injection (from the customer to the supplier). all edge weights (total money transferred) were aggregated annually and normalized by its source node out-strength. note that, both bbva customers and non-customers were included in a percentage of 63% and 37%, respectively. therefore, the network contains an important percentage of missing values."
"interconnected financial networks are the fabric where economic agents from different sectors operate. one of the main challenges we face nowadays on financial networks is assessing systemic risk [cit] . in the literature, systemic risk is defined as the probability of having large cascades of entangled economic events. such cascades are triggered by causes that range from exogenous shocks to endogenous defaults. besides, the succession of several defaults can jeopardize the full system because network financial inter-dependencies act as an economic sounding board. the interplay between the topology of the underlying interaction network and the easiness with which events propagate have proven to be essential to understand the proportion of the financial system affected by default avalanches and to assess the systemic risk [cit] ."
"as illustrated in fig. 1, most of the companies included in the network are micro-sme's and small companies, with an annual revenue smaller than 5 million euros (more than 80% of the informed values). besides, the most common categories are retail (shops), followed by construction & industrial companies. it is important to mention that leisure and consumer & healthcare are also important sectors in the network. although nodes belonging to energy and financial institutions are only a few, they account for 50% of the network's out-strength, whereas other sectors such as retail are relatively abundant in the network (30% of the nodes) but only account for 3% of the total out-strength. to model the probability of contagion, we normalized the edge weights by the out-strength of the source node (customer).as a result, energy and financial institutions account for the 11% of the normalized out-strength in the network, and retail for the 5% (20% and 7% of the corresponding global in-strength). see table 1 for more details."
"note that this discrete equation can be mapped to networks, and in a microscopic approximation, the density of infected individuals will correspond to the individual probability of being infected."
"we also propose a metric of in-sectoral inter-connectivity i in that measures how well, on average, a sector is connected to other sectors by incoming links:"
"we observe that most of the business sectors follow the tendency that the larger the number of sectors they affect, the fewer the number of sectors that at the same time are affecting them. an example can be found in utilities; for a 25% perturbation all other sectors except one are affected, while utilities itself is only affected by another sector (unknown sector). note that this is also the case for transportation, and seems to be a constant throughout the other sectors. this is opposite to what happens to the sectors energy and financial institutions, which is affected rapidly by all the other sectors. knowing the dependence of the default speed contagion on the business sectors may allow risk assessment models to understand the conditions to react to a sudden perturbation of a sector, or to an event that may indicate the initial stages of a sector crisis."
"we will study next three different default propagation scenarios. the first one corresponds to the classical mmca model where all nodes share the same recovery rate. in the second one we use the heterogeneous recovery rate measure introduced in sect. 3.2. besides, to increase our knowledge about the role of each sector in the default propagation process, we synthetically simulate default problems in each sector to analyze the different spreading speed in the transient state. finally, we validate our findings comparing achieved results with a null model built by rewiring the edges of our network."
"the first point we address is the degree distribution of the network. the degree distributions of the real network analyzed (total, in-and out-degree) presents a heavy-tail but does not fit to a power-law. this heavy-tail is relevant for the sake of the analysis given that it clearly indicates the existence of hubs. the complementary cumulative distribution functions are displayed in fig. 2 . table 1 also sorts business sectors according to their average hub and authority score [cit] . the main hub in the network is the transportation sector, followed by telecoms, technology & media and construction & infrastructure sectors. the main authority is related to financial institutions, followed by energy and financial services. hubs and authorities agree with the expected economic behavior. hub sectors such as transportation, construction & infrastructure and telecoms, technology & media, which require energy resources to produce goods, transport them, or even to run it services, therefore those sectors have important connections to authorities such as energy. note also sectors such as financial institutions and services (credit cards, insurances) arising as important authorities. this is a direct result of the financial needs that many the companies have to operate in spain. this need is mostly because the average number of days required to collect invoiced amounts from customers is quite high in spain, around 90 days."
"now we describe the sectoral financial network used in the experiments carried out in this work. to do so, we first provide all the details about network construction. second, we report commonly used network statistical descriptors."
"in this paper we provide a mechanistic model to assess the impact of a particular diffusion process of default on financial networks. to this end, we take as basis a probabilistic computational framework named microscopic markov chain approach (mmca) to compute the probability of the states of individual agents in contagion processes in complex networks [cit], and adapt its formulation to the understanding of the default propagation in financial networks. [cit], covering around 140,000 public and private spanish firms. we set default labels to 0 or 1 based on this data, depending on whether a given company was or not in default at the beginning of the considered period. by means of this data we have access to the real network of interactions and to the initial condition for the dynamics of the default endogenous propagation."
"however, to make this model more realistic we introduce a variation to this setting using default recovery probability μ dependent on the relative in-degree of each company i. a particular μ i for a company i is therefore defined as"
"to perform this experiment, we have randomly set different initial default values, specifically, 2%, 4%, 8% and 25% in each sector and no default in the remaining nodes (in the real data there is an initial default ratio equal to 1.703%, distributed across the sectors). although, several experiments were performed, here we only include two of them, the ones described in figs. 6 and 7. simulations show that the moment when the system arrives to the stationary state depends on the initial default values, the infection rate β and the recovery rate μ. in particular, the larger the recovery rate μ, the faster the system arrives to the stationary state. note that for initial default values of 25% ( fig. 7) there are no horizontal lines at iteration 20, indicating that the affected sector is still in the transient state, since it depends on the perturbed sector. we also observe that there are difference among sectors. energy is the sector which first reaches the stationary for all values of the parameters."
"one of the most commonly used contagion propagation models corresponds to the celebrated susceptible-infected-susceptible (sis). in a sis model, individuals that are cured do not develop permanent immunity, but are again susceptible to the \"disease\". similarly, companies that manage to escape default by overcoming high economic stress can fall into trouble again later on. additionally, sis model provides valuable insights to understand how different situations may affect the outcome of the contagion process, e.g. what the most efficient technique is for isolating a limited number of companies in a given financial network to minimize the risk of observing an avalanche. epidemic modeling is still the main application of sis-like approaches, and the main driver behind the development and refinement of this framework through time. however, the contagion analogy has been applied in different contexts and in particular in those where it is important to consider the spatial and social structure of systems. some examples are adoption of fads and innovations [cit], propagation of news and rumors [cit] and information diffusion [cit] . these are phenomena for which the state of the agent is affected by the interaction with its neighbors. in the financial context there is a strong causal relation between the financial and economical state of a company's clients and how this influences its economical wellbeing [cit] . this dynamics resembles a hawkes stochastic process [cit], where one event, under certain circumstances, is able to generate a new set events allowing the diffusion of a given phenomena [cit] . under this hypothesis, epidemic modeling can shed light on how systemic risk propagates through financial networks. besides the contagion analogy, there are other similarities between the transmission of diseases and the transmission of financial distress in financial networks. for example, both are branching processes where one event produces others. but we can also observe some differences. for instance, disease transmission is usually studied as a continuous phenomena whereas financial distress is studied in a discrete time scale. also, there are different levels of homogeneity in both cases, usually financial networks are more heterogeneous than the population networks used in disease spreading research."
"according to the european central bank definition for risk classification [cit], the susceptible state would correspond to a company which is in step 3 (default). in this step, the credit quality of the company is considered equivalent to a probability of default of between 0.10% and 0.40% over a one-year horizon. therefore, after a given period of time (12-18 months), which depends on its revenue, it can go through the step 2 (cure) and finally come back to step 1 (normal) if it proves to have a good payment behavior. by using this model, we computationally analyze the behavior of default contagion processes in a real topology created by the interactions among different companies. additionally, we want to understand the main properties of default propagation and the emergent clusters containing defaulted companies in the full system. moreover, having a welldefined sector distribution and their annual revenue, see fig. 1, we can elucidate if default figure 1 sector and annual revenue distribution of the customer-supplier network. distribution of the customer-supplier network by sector and annual revenue. firms can be classified in 17 sectors according to their nace code (nace (nomenclature of economic activities) code is the european statistical classification of economic activities) and main economic activity. besides, firms can be also classified according to their annual revenue: micro-sme's (less than 1mm eur, small (1-5mm eur), sme's (5-50mm eur) and large (more than 50mm eur) cascades depend on sectors and their economical states. our objective here is not to build an accurate description of the default process, since for this purpose our model should be more complex and realistic, but to explore the role that the network structure plays on the default propagation process with varying economic scenarios."
"inspired by the microscopic markov chain approach (mmca) designed for epidemic spreading, first we propose an adaptation of the framework for modeling the default cascades observed in the transactions between different companies in real financial networks. then we introduce some measures to dynamically analyze the default contagion process and its functional relations with any sectoral financial network."
"the methodology presented in this work applied to a real customer-supplier network of companies in spain has allowed to gain new insights on financial data. we confirm quantitatively that business sector is a key component in default contagion, both in the strength and in the velocity of the contagion. specifically we have found three differentiated blocks of behaviors depending on the business sectors. the ability to quantitatively estimate the size of this effect via topological analysis of the customer-supplier network demonstrated in this work may allow current risk assessment models to include them for future default prediction in a systematic way. nowadays, this effect is only included from macro-financial data perspective. therefore, there is a lack of relational information at micro-level. customer-supplier network relations would cover this gap. we have proven that this dependence happens in at least two different scenarios: when recovery rate is homogeneous and when it is heterogeneous for every company. however, when the recovery rate depends on a topological property such as customer diversification, default contagion increases. this finding may be only a consequence of the structure of the customer-supplier network in spain. further studies could be carried out by using other country customer-supplier networks, and by using other dependence hypothesis for the recovery rate. this is where a major advantage of the methodology presented in this work comes: the ability to vary infection and recovery parameters at the micro-scale and study the effects on the dynamical properties of the network. for instance, we can base the model on more economically-wise hypotheses for the recovery rate (or infectious rate) that account for more realistic scenarios, such as the companies' revenue, and answer why some sectors do not default although they should, provided the structure of the network. this allows risk experts to study many possible hypothesis. after carrying out a detailed analysis of how different parameters affects default contagion, we have seen that the number of hub companies a business sector includes is essential to estimate its sensitivity to default contagion. although this is not the only variable to take into account, and results can not be generalized to other customer-supplier networks outside of spain, we believe this is a first step towards the study of how the topological properties of the companies in the network affect default contagion. from a topological perspective, the results of our model applied to the real data reveal which sectors are more at risk in the propagation of default, which sectors are more resilient to the default avalanches, and what are the expectations for the cascades of default under different stochastic conditions. for that purpose, we have carried out two type of experiments:"
"where p i, ρ r, ρ o stand for the ith percentile, node ρ values of the rewired network and ρ values of the original network respectively. besides, n r,s and n o,s are the number of nodes within percentiles p 5 and p 95 of the rewired and original network respectively. ρ values of fig. 8 show significant differences between customer-supplier network and the rewire one. consequently, this confirms that the customer-supplier network structure plays an important role in all the aforementioned results."
"bbva, banco bilbao vizcaya argentaria, name of the second largest bank in spain; glm, generalized linear models; mmca, microscopic markov chain approach; sis, susceptible-infected-susceptible epidemic model; sme, small and medium-sized enterprise; vat, value-added tax."
"we still lack an explanation of the observed behavior given the topological characteristics of each sector. to do so, we computed the slope coefficient of the inverse cumulative probability distribution of the companies in-strength for each sector (γ ). the in-strength is defined as the sum of the incoming normalized weights for each company, meaning that the higher the value for a company the more probable that the default dynamics will affect it. as in most complex systems, this probability distribution is heavy-tailed signaling a pareto like distribution where the slope coefficient can be computed. a smaller γ value signals that the sector is more probable to contain well connected companies (sector hubs). we observe that energy and financial institutions are the most susceptible (s rank ) sectors, and coincide with lower values of γ (higher probability of sector hubs). the contrary happens to institutions, and unknown sector reference. this highlights the fact that hub structures play an important role in the dynamics, and in the extent that a sector is affected by it. clearly, leisure does not follow this explanation because it has a middlerange γ value. this could be due to the fact that it has 76% of its companies with zero in-strength. however, it is naive to think that only this structural property can explain the sector response to default dynamics. in table 2, we can observe how less susceptible sectors are more equally inter-connected to other sectors (larger value of i in ). in practice, this causes the default spreading to be less likely to find a high probability path to these sectors because their incoming weights are less concentrated. as before, economical sectors can be grouped in three blocks given its capacity to interact and affect other sectors. on one hand, energy, financial institutions, financial services, transportation, telecommunications and retailers sectors are largely affected by the others due to their large sensitivity. besides, these sectors are highly inter-connected with all the other sectors since their activity is traversal to all sectors and firms, therefore this high degree connectivity allows default to infect them easily. on the other hand, when consider leisure and unknown sectors, we observe that these are not affected by other sectors. for leisure sector, this is a consequence that most of its companies have zero in-strength. similarly, companies belonging to the unknown sector are not bbva clients, so their information is quite limited and most of their connections are not included in the network having, both, low in and out degree. in-between, there are sectors, such as (public) institutions and retail, that are stable independent of the perturbed sector. the main characteristic of these sectors is their low in-strength due to their customers not being companies or not having customers at all because they are public entities. in any case, default contagion does not reach these sectors and they kept healthy in all parameters setting."
"where n is the number of companies and p ss i stands for the default probability of company i at the model's stationary state. note that since we are considering a sis modeling framework, by construction, the dynamics will always reach a steady state. as shown in sect. 3.1, the default probability density depends on the default infection rate β and the recovery rate μ. for obtaining ρ, we monitor p i (t) as a function of time in a discrete manner, until the contagion model reaches the steady state. note that time represents the iterations of the mmca recursive model. to do so, we set p i (0) to the real company default label. for understanding the system dynamics, without loss of generality, we can apply the classic mmca framework where the parameter, β i, the contagion infectivity per node, is constant and equal for all companies in the network."
"the reason why these networks attract so much attention is that, besides economic interchanges, financial risk also propagates through them [24, [cit] . their stability becomes thus an important question [cit] . furthermore, risk and economic distress, and even default in a second stage, can occur in cascades leading to serious systemic instabilities [cit] . therefore, the resilience of the networks to contagion, as well as the circumstances under which it becomes systemic has been analyzed in many works [cit] . following this research line, a method called debtrank was introduced to find nodes in financial networks that can induce large cascades when perturbed [cit] . this method allows to search for measures to mitigate risk propagation [cit] . in the special case of networks where the nodes are banks and the links represent holding of different types of obligations, the complexity of the products traded such as derivatives [cit] and the feedback-loops between solvency perception and stock and obligation values [cit] can play an important role in economic distress propagation. many of these previous works have been focused on banking [cit], where the risk propagation is related to the stress tests performed by central banks. these kind of models resorts on ad-hoc mathematical models for financial institutions. however, as it has been seen in the last crisis, the risk can spill out of the banking system to enter other economic sectors. this is why it is of high relevance to consider risk propagation in more general economic networks, including different sectors and different types of nodes, ranging from large holdings to small companies or even the final individual consumers. this is precisely the direction that we take in the present work where we use general contagion model to evaluate the default spreading in a highly heterogeneous network."
we have also shown in this paper how to derive an soc channel simulator from a reference channel model. two parameter computation methods-the gmea and the brsm have been applied to simulate indoor fading channels. it has been shown that the pdf of the envelope and the temporal acf of the soc channel simulator match perfectly the ones of the reference channel model.
"by taking (10) into consideration, we can express the temporal acf r μρμρ (τ ) [see (18)] in terms of the temporal acf r μμ (τ ) of μ(t) as follows"
"we compare the polar descriptors to two other methods namely, virtual camera planes(vcp) [cit], and local spherical sift descriptors(lsd) [cit] . the vcp method forms image patches by projecting the spherical image to a plane tangent at the feature point and computes sift descriptor on the image patch. the lsd scheme computes the sift directly on the spherical image by forming a rectangular support region for the descriptor."
"applications such as camera calibration, object detection, recognition or tracking generally rely on the localization and matching of salient visual features in multiple images. to provide scaleinvariance, these features are generally computed by a scale-space analysis framework. this is a very important property of these features which provides robustness to resolution changes and camera translations. the most popular scale invariant feature detection algorithm is certainly the sift framework [cit] for perspective camera images. many other methods have been proposed with different feature detection methods and descriptors [cit] for classical cameras."
"assuming that the random variables x and y are independent, the joint pdf p x y (x, y ) of x and y can be expressed as"
"in this section, we present a stochastic soc channel simulator, which can be obtained from the reference model by applying the soc principle [cit] . the idea is to model the diffuse component μ(t) of a flat fading channel by a sum of n cisoids, i.e.,μ"
"we denote the polar descriptor as psd and non-oriented polar descriptor as noorpsd. figure 3 shows the recall vs 1-precision graph for the rotation. in case of rotation, polar descriptors provide the best performance. non-oriented descriptors perform as good as vcp and lsd with less computation cost than the oriented descriptor on the detection phase. it takes around 15% less time when the figure 4 shows the performance of different descriptors for translation of the cameras. polar descriptors perform better than vcp and lsd methods. note that translation causes only a slight change in orientation of the feature. this favors the non-oriented descriptors as there is no orientation computation. on the other hand, extra orientations computed for the oriented descriptors increases ambiguity which causes mismatches. both graphs show the potential of the non-oriented descriptors together with rotation-invariant matching for accurate matching with reduced computational complexity on the detection phase. the increase in performance for the proposed polar descriptors are due to better handling of the sampling without any extra interpolation and geometry adaptiveness. it is also shown that histogram based matching is more precise, similarly to what has been reported in the planar case."
"here, c n, f n, and θ n are called the gain, the doppler frequency, and the phase of the nth propagation path, respectively. for a stochastic channel simulator, it is often assumed that the gains c n and the doppler frequencies f n are constant, which can be determined by a parameter computation method in such a way that the statistical properties of the stochastic channel simulator are as close as possible to those of the reference channel model. the phases θ n are independent, identically distributed random variables, which are uniformly distributed over (0, 2π]. thus, we can model the stochastic soc channel simulator by a random process of the form"
"here, z max describes the distance from the origin to the boundary of the room, which is a function of a, b, and the aoa α. using the geometrical relationships, we derive an expression for z max in form of a piecewise function depending on the aoa ranges separated by the dashed lines in fig. 2 . for brevity, we only present here the final expression for the pdf of the aoa, which can be found at the bottom of this page [see (9) ]."
"in this section, we will illustrate the main theoretical results by evaluating the doppler psd, the pdf of the envelope, and the temporal acf of the reference model and the simulation model."
"we have proposed two scale invariant polar descriptors inspired from log-polar descriptors on the plane. we have built the descriptor so that it considers non-uniform sampling density on the sphere. the proposed descriptor is not limited to equiangular grid and can be applied to any sampling scheme on the sphere. we have also proposed a matching method that can successfully match non-oriented polar descriptors through the kl-divergence criteria. the complexity on the descriptor computation phase is reduced in this case, which enable its use for mobile applications."
we consider the geometrical indoor scattering model as illustrated in fig. 1 . the rectangle in fig. 1 represents a room with length a and width b. the bs and the ms are arbitrarily placed in the horizontal plane of the room. we assume that the transmitter is the bs and the receiver is the ms. the ms moves along the x direction.
"where the time-variant deterministic process m ρ (t) equals the los component in (2) . the stochastic processμ ρ (t) can be interpreted as a family of sample functions depending on the phases θ n . if we fix θ n, e.g., by considering them as the outcomes of a random generator with a uniform distribution over (0, 2π], then the stochastic channel simulator becomes a deterministic one, which can be used for system simulations."
"in this paper, we have proposed a reference channel model for indoor propagation scenarios, where the los component has been taken into account. analytical expressions have been derived for the pdf of the aoa, the even part of the pdf of the aoa, the doppler psd, and the temporal acf. the obtained analytical results are not only important for theoretically studying the performance of indoor communication systems, but also indispensable for deriving efficient channel simulators."
"whereî(t) is the spectrum of the smoothed image at smoothing level t andî (0) is the spectrum of the original image. the spectrum of a spherical signal is computed by a spherical fourier transform, which is defined as is the spherical harmonics of degree l and order m [cit] . using (3) and (2), one can compute the smoothed spherical image at the smoothing level t by first applying the forward spherical fourier transform, and then applying eq. (2). finally an inverse spherical fourier transform is performed."
"suppose that the location of all scatterers is described by (x, y). the position of the ms is denoted by (a, b). as shown in fig. 1, the aoa α can be expressed as"
"descriptor computation stays an important step after definition and localization of the visual features. the distinctiveness or character of the feature is defined by the descriptor and plays an important role in matching of the features. the conventional image patch correlation approach is often replaced by more sophisticated descriptors that can deal with scale, rotation and affine transformations as well as illumination changes. descriptors based on histogram computation provide the best robustness to these transformations. the sift descriptor is based on gradient orientation histograms computed in the region around feature points. the histograms are computed for each spatial bin in the support region. descriptors with different histograms such as surf [cit] and chog [cit] have recently been proposed to improve the performance. in addition, the gloh framework [cit] provides a log-polar descriptor that forms spatial bins by radial division of the support region. all these algorithms are however designed for planar images and they assume that the sampling is uniform along the image. for omnidirectional images, the sampling density differs from region to region and it should be taken into account in the computation of the descriptors."
"to guarantee that the derived indoor channel model can be applied to various indoor propagation scenarios, we first concentrate on a general geometrical indoor scattering model by comprehensively considering different possible scenarios. therefore, we assume that there are an infinite number of local scatterers, which are uniformly distributed across the 2d horizontal plane of the room. it should be mentioned that such an indoor model acts only as a nonrealizable reference model, from which we can obtain different channel simulators with scatterers located in different positions."
"first the approximate relative orientation is computed from the central spatial bins. circular shift is applied to one of the central bin histograms, and the l1 distance between the histograms is computed. the amount of shift giving the smaller distance is assigned as the relative orientation. formally, the shift α is expressed as"
"where k is a scale multiplication factor. the sift features are computed on the difference images by first finding the local extrema compared to 26 neighboring pixels in the current and adjacent difference images. the low contrast extremum points that are below a pre-defined threshold are discarded. edge responses that give unstable feature points are removed by checking the maximum ratio between maximum and minimum principal curvature of the difference images at the pixel position. finally, the position and scale of the feature are refined by fitting a 3d quadratic."
"where d 11 denotes the central spatial bin. after the shift α has been computed, the l2 distance is computed between the descriptors by shifting the indexes of the bins. in other words, we have"
"o ms the quantities ρ, f ρ, and θ ρ in (2) are constant, which denote the gain, the doppler frequency, and the phase of the los component, respectively."
"the most common method to compare descriptors is the l2-norm between the two descriptor vectors. it is a simple and fast method but it is shown [cit] that it does not necessarily provide the best matching scores. histogram-comparison based methods such as kullbackleibler (kl) divergence [cit] or earth mover's distance (emd) [cit] are shown to provide better matching performance at the price of more computation. among these methods, kl-divergence has a good trade-off between accuracy and computation cost. the symmetric kl-divergence is computed as"
"it has been shown that the doppler psd of the diffuse component is symmetrical if the abscissa of the ms location equals zero, while it becomes asymmetrical if the ordinate of the ms location is equal to zero."
"we propose to use the kl-divergence for matching the new polar descriptors on the sphere. if we denote the spatial bin of the polar descriptor d for i th radial division and k th orientation division by d ik, the kl-divergence for descriptors dp and dq is [cit], after the distances are computed, the pair with the minimum distance that is one factor smaller than the second minimum distance is selected as a matching pair. the comparison factor that is called the ambiguity factor is a parameter typically set to 1.5."
"in the following, we apply two parameter computation methods, namely the gmea [cit] and the brsm [cit], to compute the soc parameters c n and f n . 1) the gmea: according to the gmea, the gains c n are defined as"
"due to the infinite realization complexity of the reference model, this model cannot be used directly in system simulations. for the design and performance evaluation of indoor radio communication systems, fading channel simulators with low realization expenditure are desirable, by which the overall production cost and the simulation time can be reduced considerably."
"where the process μ(t) represents the sum of the diffuse part and m ρ (t) denotes the los component. here, the los component can be described by a time-variant deterministic process in the following form [cit]"
"in the reference channel model, we assume that the number of scatterers is infinite. therefore, the doppler psd s μμ (f ) of the diffuse part μ(t) is continuous. the average power within an infinitesimal frequency interval df can be represented by s μμ (f )df . on the other hand, the incoming power between f and f + df is proportional to p f (f )df . thus, the following relation holds"
"to cope with the problems faced by empirical models, several geometrical scattering models have been proposed in the literature, e.g., the one-ring scattering model [cit], the two-ring scattering model [cit], and the elliptical scattering model [cit] . the application of geometrical scattering models has even been extended to characterize mobile fading channels for three-dimensional scattering environments [cit] . most of the aforementioned geometrical models have been widely used to characterize fading channels for outdoor environments. but few applications are found for the indoor channel modeling."
"we now exploit an interesting characteristic of our new descriptors to derive an alternative descriptor that necessitates smaller computational complexity. as the central bin of the descriptor is not divided into orientation bins, the gradient histogram in this bin captures the orientation of the feature. in addition, for a polar descriptor, the rotation of the descriptor is just the shift of the spatial orientation bin indexes and gradient histogram bin indexes. hence, the descriptor can capture the distinctive characteristics of the feature even without orienting the descriptor. non-oriented version of the polar descriptor is computed as if the orientation of the feature is zero. the correct orientation is then computed by the matching algorithm. we propose a matching algorithm to match these non-oriented descriptors."
"in this paper, we propose a new geometrical-based channel model for indoor propagation environments. our starting point is a geometrical indoor scattering model, where we assume that an infinite number of scatterers are uniformly distributed in the 2d horizontal space of a room. in our channel model, we will consider the general case, where the base station (bs) and the mobile station (ms) are arbitrarily located in the room. moreover, the los component between the bs and the ms will be taken into consideration, which allows us to include the nlos scattering scenarios as a special case. analytical expressions are derived for the pdf of the aoa, the doppler psd, and the temporal acf. moreover, we will describe how to derive an soc channel simulator from the reference model. to compute the main parameters of the soc channel simulator, we apply the gmea [cit] and the brsm [cit] . the pdf of the envelope and the temporal acf will be visualized assuming nlos and los propagation conditions. it will be shown that the most important statistical properties of the soc channel simulator match very closely the ones of the reference channel model. the excellent fitting demonstrates that the reference channel model can be approximated by an soc channel simulator using a finite number of cisoids. it also indicates that the gmea and the brsm are efficient parameter computation methods for designing indoor soc channel simulators. when comparing the two methods, we find that the brsm is better regarding the approximation of the temporal acf."
"the growing interest in wireless indoor communication systems and applications of local area networks has resulted in many investigations on the characteristics of indoor radio propagation channels. in the last few years, several empirical channel models [cit] have been developed based on indoor channel measurements. the obtained experimental results, such as the pdf of the aoa and the angular spread, are important statistical quantities describing the characteristics of fading channels. however, the empirical models are only useful and accurate for environments having the same specific characteristics as those where the measurements were made."
"omnidirectional vision sensors with their large fields of view present many advantages for applications such as scene analysis, representation and detection and thus omnidirectional vision has been an active research field in the related research areas. the omnidirectional cameras typically consist of either a fisheye lens or a lens and a mirror (catadioptric) system such as parabolic or hyperbolic mirrors. the structure of the resulting images is highly dependent on the geometry of the mirror, which should be taken into account for appropriate processing of the light information."
"in this section, we first give an introductory description of the reference channel model. then, we analyze the pdf of the aoa and the doppler psd for the proposed indoor reference channel model."
"the rest of the paper is organized as follows. section ii describes the geometrical scattering model for indoor propagation environments. section iii presents a new indoor reference channel model and its statistical characterization in form of the pdf of the aoa, the doppler psd, and the temporal acf. section iv shows how to derive an soc channel simulator from the reference model. numerical results are presented in section v under the assumption of los and nlos propagation conditions. finally, our conclusions are given in section vi."
"the remainder of this paper is organized as follows. in section ii, a detailed explanation of the wsn model and the basic assumptions are presented. section iii investigates the ideal mmse detectors for a wsn environment. it also highlights the issues of implementing an ideal mmse detector for wsns. section iv discusses the implementation of the preprocessing matrix. the design of the preprocessing matrix with the assistance of the cs-based technique is discussed in section v, which is followed by the design of the preprocessing matrix with the assistance of reduced-rank techniques in section vi. the complexity of all these algorithms and the ideal mmse detector is derived and compared in section vii. simulation results are presented in section viii. finally, section ix concludes this paper with summarizing comments."
"the rigor with which researchers actually comply with funding agency mandates is now a subject of much debate in the research data community. neylon has proposed that the focus shifts from development of data sharing policies to culture change. 16 his argument is that although policies are important, they focus on changing individual behavior. given the barriers to data sharing, such as lack of understanding of how to go about sharing data and the need to commit extra human and funding resources to sharing data, a change in culture to promote data sharing would help to motivate researchers to change their individual behavior."
"considering the difficulty in designing a memory system and verifying that it satisfies its consistency model, we propose a system model, called the fractally consistent model (fcm), that enables easier pre-silicon verification. the key is that, for systems that adhere to the fcm, verification of memory consistency is factored into three verification problems that are small and scalable. instead of trying to verify memory consistency in one fell swoop, fcm enables verification to be done in three parts, each of which is self-contained and scalable. fcm simplifies verification by explicitly considering verification early in the development process."
"to solve this problem, a design of the preprocessing matrix with the assistance of reduced-rank techniques is proposed in this paper. reduced-rank techniques have been widely applied to array processing [cit], radar signal processing, directsequence code-division multiple-access [cit], space-time coded space-division [cit], and ultrawide band systems [cit] . specifically, in this paper, three types of reduced-rank techniques are considered, which derive their detection subspaces based on the concepts of a principal component (pc) [cit], cross-spectral metric (csm) [cit], and taylor polynomial approximation (tpa) [cit] ."
"to minimize the likelihood of memory consistency bugs escaping into shipped processors, industrial development teams spend a vast amount of time and effort in the process of pre-silicon (static) design verification (also known as design validation). the current state of the art is to design the system and then verify it using a combination of testing and formal verification. the combination is due to the fact that both testing and formal verification have their own advantages and unavoidable problems. traditional testing methods are intuitive, but unlikely to cover every possible scenario in non-trivial, scalable systems. formal verification of memory consistency, which is complete and good at uncovering subtle bugs, is extremely difficult. previous work [cit] in this area is either too abstract to be realistic, non-scalable (i.e., using model checking), or requires huge amounts of manual effort (i.e., using theorem proving). thus, while formal methods are useful tools, their limitations have restricted the extent to which they can be used to verify memory consistency. thus, whatever verification methodology is used, the current situation is that memory consistency cannot be completely verified for complex, modern processors."
"the uniprocessor ordering invariant requires us to design the core such that it is logically in-order. fortunately, all cores are architecturally in-order, regardless of whether the microarchitecture is pipelined, superscalar, speculative, or out-of-order. all existing cores appear to execute instructions sequentially in program order."
an example of early use of mpublish deposition in a published article 18 illustrates how the article can formally cite the data (as references 35 35 and 36 18 in this case) and in turn how the article can itself be cited via the metadata describing the data thus establishing bidirectional linking.
"in the pc-based technique, the autocorrelation matrix r is decomposed in terms of eigenvalues and eigenvectors [cit] . a number of principal eigenvectors are chosen to form a detection subspace [cit] . the decomposed autocorrelation matrix r can be given as"
"as the desired signal b arrives at the destination with the assistance of l sensors, l copies of the desired signal need to be collected. the vector form of the received signal can be represented by"
"architects must design a multiprocessor's memory system such that it satisfies the specified memory consistency model. however, the process of designing the memory system and then verifying that the design satisfies the specified consistency model is complicated and error-prone, even for seemingly simple consistency models like sequential consistency (sc). the complexity is due to both complicated hardware designs that seek to optimize performance and the need for architects and verification teams to reason about concurrency. as a confirmation of this complexity, subtle bugs have been uncovered in the designs of commercial processors [cit] ."
"even though one might agree that an ideal state would be for all primary research data in support of scientific publication to be made publicly available, there are still constraints even in a digital environment with vast amounts of inexpensive storage available. a typical organic synthesis article might include three or four nmr experiments for each of 30−40 substances. generating the supporting information file consisting of graphical representations of the nmr data with headings, structure diagrams, and sampling parameters is particularly time-consuming. furthermore, such documents can grow to be very large, there are examples now of supporting information approaching or even exceeding 1000 pages. 17 an alternative approach would facilitate ways in which the researcher can organize and store the fid files as a package for submission as a dataset or a fileset. because the fid files are the basis for both the data package and the graphical si data, a workflow that allowed for the generation of both forms of si at the same time would remove one of the barriers to submission of full/ raw data. although this approach is consistent with the general concept that content should be stored in canonical form and rendered as appropriate at the time of delivery to the user, 18 it also achieves one of the hallmarks of publicationthat the reader has the option if they wish for a static representation of what the author intended, while at the same time, the interested reader, whether human or machine, has access to both a more interactive and/or a lossless version of the same data."
"during this phase, the lth sensor amplifies the received signal y r l by ζ r l and forwards the resulting signal to the destination. at node d, the signal received from the lth sensor is given by"
"article 4. we include a license for reuse and if necessary, reanalysis, in our case the cc0 creative commons license, 30 this directly addressing the r of fair."
"the most important task in the scaled system design is to ensure the fractal behavior. simply expanding the smallest system without careful consideration can easily break the fractal behavior. we give a specific example to show how to maintain the fractal behavior. as shown in fig. 3(a), for a single block, the internal interface has a child in state o (wned), and all other d-1 children in state s (hared). observed from the external world, the node as a whole (i.e., the internal interface and its children) appears in o since it has the ownership. then the child in o issues a puto (meaning changing from state o to i(nvalid)) coherence request to evict that block. when the puto arrives at the internal interface, we must consider the implication of the puto to determine how the internal interface should respond to it. for the node as a whole, the puto means it gives up the ownership and becomes a sharer of the block. so the final state should be s instead of i. if the internal interface simply sends a puto to the interface above it, the above interface cannot distinguish it from a common puto and will send an acknowledgement back for it to change to state i. but the transition we need here is from o to s. considering that this o to s transition is impossible for a single node, we need to add a new message type, called put-from-o-to-s (putotos), to both the scaled system and the smallest system to maintain fractal, shown in fig. 3 (a) and (b) . the putotos notifies the above interface the transition is from o to s. the above interface will send acknowledgement back for this putotos request."
"verification process: as proven in the original fractal coherence paper [cit], fractal coherence protocols can be formally verified to maintain cache coherence for any number of cores, using two scalable verification steps. first, we must verify that the minimum system is coherent second, we must verify the fractal behavior, i.e., the larger-scale systems behave like the minimum system. both verification steps can be performed with existing automated formal tools. the inductive proof that these two steps are sufficient for the scalable verification is based on the correctness of the minimum system (as the base case) and the fractal behavior of fractal coherence protocols (as the inductive step)."
"the given example is one of the many specifically designed transitions to maintain the fractal behavior. it shows the internal interface has to decide in what form a request should be displayed to the outside world depending on its actual implication. with a properly designed internal interface, we can scale the system to any number of nodes while maintaining the fractal behavior."
"in dynamic verification of memory consistency (dvmc) [cit], the authors decompose the process of verifying memory consistency into three simpler verification steps. 1 of those three verification steps, two are simple and scalable, but the third step is not. to make that step simpler and scalable, we use the result of the other relevant prior work: fractal coherence [cit] . we now discuss the relevant aspects of both dvmc (section 2.1) and fractal coherence (section 2.2)."
the automatically generated odt file 22 can be edited and used to prepare the final supporting information (si) file intended for submission along with the manuscript and along with inclusion of experimental procedures and other appropriate data. the preparation of nmr data for the si document is thus greatly facilitated by use of the mpublish automation.
"in our recommendations, every nmr spectrum submitted for deposition is accompanied by a suitable molecular connection table in the form of standard files such as the mdl molfile (.mol) or the chemdraw .cdx or .cdxml files ( figure s5 ). this is essential if raw filesets such as folder collections produced directly by instruments are submitted (in the form of a compressed zip archive). connection tables can also be embedded in the mnova files resulting from the primary processing of such instrumental data, but currently, openbabel is not capable of extracting such information from this format and hence in practice, these too require separate inclusion of a molecule description file. we suggest that the best practice is in fact to provide both the raw instrument archive as a .zip archive (for reusers who choose to use different software) together with the mnova superset, which includes both the fid and all analysis of the far more \"userfriendly\" frequency domain spectrum (including solvent reference, annotations, assignments, phase and baseline correction, etc.). optionally, other processed data formats such as the open standard jcamp-dx or nmredata noted above can also be provided for further interoperability and even a visual format such as pdf to allow readers the option of access to the traditional format currently found in most supporting information files."
"archiving and sharing of analytical raw data are also pivotal for the development of new signal processing algorithms aimed at extracting information that has been elusive to existing methods. for instance, although the fourier transform (ft) method has proved to be the gold standard for the processing of nmr data because of its computational performance and robustness, it also has well-known limitations such as poor resolving power, leakage artifacts (i.e. gibbs oscillations), and phase and baseline distortions. there are presently many other alternatives to the ft method that can overcome these deficiencies or new ones that might be developed in the future. those new methods could potentially uncover new information (i.e., new signals invisible to old methods) that can be used to, for example, prove or discard a chemical structure as inferred with different methods. obviously, this can be only done if the primary acquired data is preserved. furthermore, even raw data is generally stored with some loss of information. for example, nmr data is acquired by accumulating many scans to improve the signal-to-noise ratio. however, only the final averaged fid is kept, whereas all the individual transients are thrown away. although this is usually not a concern, there are some advanced processing techniques that could be used to take advantage of the information contained within the individual scans. 6 for experimental data other than that for crystal structures, instruments have the ability to export in standard formats. for spectroscopic data, jcamp-dx is a veritable common export format for data sharing, although it has yet to achieve widespread adoption. 7 although the format has some disadvantages in the nonstandard ways in which instrument manufacturers have added custom parameters, it is still useful for understanding the basic data. however, there are two disadvantages with using export formats for spectroscopic or instrumental data. first, the exported data may be processed data and hence subject to manipulation. second, the steps to generate the data objects for publications either as selected lists of peaks for the experimental section in the body of the article or as figures in the body of the article or in the supporting information are labor intensive and result in some loss of information. once those traditional publishable objects have been prepared, it would be an additional human effort to generate and organize the machine-readable data files for submission as supporting information. although many publications allow submission of supporting information data files, most publications do not require the raw data. there have been few other incentives for researchers to do the extra work to prepare the data files for publication, but most do not. this situation is changing. for instance, a new nmr file format has been recently proposed, nmredata, 8 although its scope is limited to structure characterization of small molecules (i.e., nmr assignments). this format is essentially an extension of the existing structure data format (sdf) and includes the socalled nmr record, which is a compressed folder that, in addition to the nmredata file, contains all related 1d and 2d spectra including the raw data such as the fids and all the acquisition and processing parameters."
"where d u will consist of the required u signals, and d l−u will consist of the remaining (l − u ) signals of d. the rayleigh quotient can now be employed to determine the optimized p as follows:"
"the coefficients a i are chosen to minimize the mse [cit] . in the context of the tpa-assisted reduced-rank mmse detection, the preprocessing matrix p can be finally expressed as"
"to facilitate verification of cache coherence, the nonscalable verification step from section 2.1, we leverage fractal coherence [cit] . fractal coherence is a design methodology for cache coherence protocols that can be formally verified using existing, automated, easy-to-use formal verification tools. fractal coherence protocols are designed in a way that enables fractal behavior, i.e., each scale of the system has exactly the same behavior. using existing, automated formal verification tools, we can both verify the smallest scale of the system, called the minimum system, and we can also verify that the behavior of the system is fractal. we can then use induction to prove that the system, regardless of size, is coherent."
where u is the length of the filter. the received signal y will be convolved with p to generate the output out of which u is selected to be processed by the reduced optimal weight vector w. the output of the convolution can be written as
"we require an optimum value of p, which maximizes the energy of the selected branches and minimizes the energy of noise and the (l − 1) remaining branches. we can reduce the given problem to a rayleigh quotient by placing a constraint on the energy of the (l − 1) remaining branches and the noise such"
"or a shared memory multiprocessor, the memory consistency model specifies the correct behavior of the memory system [cit] . in particular, the consistency model specifies the legal, software-visible orderings of reads and writes performed by different threads. the memory consistency model is a software-visible interface and enables the programmer to write multithreaded code without having to reason about the hardware implementation."
"2) allowable reordering. to improve performance, many consistency models reorder memory operation between when a core issues them and when they are performed in the cache. we must verify that the system does not reorder memory operations in ways that violate the consistency model. this is the only one of the three invariants that depends on the specific consistency model. 3) cache coherence. cache coherence requires that, for each block of memory, (a) its lifetime can be divided into epochs in which there is either a single writer or multiple readers, and (b) the value of the block at the beginning of an epoch is the same as the value at the end of the most recent read-write epoch. we must verify that the memory system is coherent, and this is the non-scalable verification step, because coherence involves all cores."
"similar to pc, this technique utilizes the eigenvalue-based technique to determine the preprocessing matrix [cit] . it has been shown in the literature that selecting the u strongest eigenvalues does not necessarily represent the best set of u eigenvectors, as measured by the lowest mse [cit] . to minimize the mse, we maximize the power of z, which can be represented as"
"coherence protocols instead of traditional snooping or directory protocols. however, we have experimentally shown [cit] that fractal coherence protocols have comparable performance to traditional snooping and directory protocols. therefore, this constraint will not hinder performance. the above two aspects give us confidence in the performance of fractal consistency."
"verification process: to verify the reordering mechanism is correct, we must verify that it does not permit reorderings that are prohibited by the consistency model."
"due to the intractability of the exact cdf of γ v, we should find the upper bound of γ v, which does not affect the diversity order of γ v . by omitting some parts of the denominator, γ v is upper bounded such that"
"the third part of an fcm system is the cache-coherent memory system. we must design the memory systemincluding caches, interconnection network, memories, and coherence controllers-such that the coherence invariants are maintained. then we need to verify that the design does maintain coherence. traditional snooping and directory protocols are not scalably verifiable, which motivates us to choose for fcm a coherence protocol design, called fractal coherence, that is scalably verifiable. fractal coherence structure. fractal coherence protocols need a hierarchical logical structure, such as a tree. the physical interconnection has no restriction. the components in a fractal coherence protocol are shown in fig. 2 . the shadowed square components are basic nodes, which may have a number of caches, cores and memories. the elliptical shape components are interfaces that support the fractal behavior. there exist two kinds of interfaces, named according to their positions in the system, i.e., top interface and internal interface. we use a specific protocol, treefractal, to illustrate the design steps of fractal coherence protocols, as shown in fig. 2(a) and (b) ."
"fcm is a system model that facilitates the verification of consistency. for fcm to be viable, it is important that the performance of systems that conform to fcm is comparable to the performance of traditional, non-fcm systems. fcm could potentially sacrifice performance due to two constraints in the system model."
"the mpublish project aims to address the twin challenges of increasing the findability, accessibility, interoperability, and reusability (fair) of spectroscopic research data 19 and facilitating the preparation of such data for publication using automated workflows. the first two sections will describe two key features of the mpublish project, that is,"
"23 by deposition in a data repository. although the mpublish procedure provides an excellent mechanism for making complete nmr data readily and appropriately available, the procedure also needs to include mechanisms for ensuring it adheres as completely as possible to the fair data principles (findable, accessible, interoperable, and reusable)."
"article mnova software, without the cost associated with the software license. this will allow the reader to analyze the research data more extensively than with a static pdf. 2.2.1. publisher's perspective. to enable this feature, the publisher has to provide mestrelab with a public key, which will be used to sign the data and compose an mnova publication text file (mnpub). the mnpub file will point to the primary research data (mnova or zip file) and hold both the signature and the public key information. an mnpub file will have to be created for each dataset that the publisher would like to make freely available for review using mnova software. to this end, the publisher must use an rsa key pair: (i) the public key is sent to mestrelab to allow them to generate a certificate file (.mncrt), which is then made available to the publisher and (ii) the publisher uses their private rsa key to \"sign\" the dataset, which generates a signature. overall, the mnpub file will be composed with the signature, the certification file (mncrt) and a url to the signed dataset (see example of mnpub file). 22 the process is simple and can be executed with very little overhead by the publisher (see supporting information for further details). the procedure can be fully automated as a part of the submission process and will not require any additional input from the authors. for example, as described in the last section 2.3, the procedure can be implemented in a data repository submission web service where the authors only need to upload an mnova file or a zip file, with the mnpub file generated automatically by the web service."
"2.3.4. fair compliance. fair compliance can be evaluated using an objective, automated, and community-governed framework. 36 the evaluation 37 for the repository used here 25 is based on the features described above. we recommend that any repository providing access to fair-enabled data such as described here should be submitted to such an evaluation."
"combining the results from the prior work discussed in section 2 allows us to decompose the verification of fcm systems into three simpler, scalable verification steps. these three verification steps correspond to three distinct self-contained portions of the system model, as illustrated in fig. 1 : core (section 3.1), reordering mechanism (section 3.2), and cache-coherent memory system (section 3.3)."
"recent studies have examined the rationale for greater sharing of primary research data among scientists, including in chemistry. 1 a number of benefits have been suggested including research integrity and replicability and reuse by peers. some prominent cases of data manipulation and fraudulent data have been uncovered in recent years, 2,3 the expectation is that upon publishing the primary research data associated with an experiment, it becomes more difficult to manipulate data in a way that cannot be detected. the ease with which data can be manipulated or manufactured varies by technique, and although some attempts can be detected by human inspection, others would undoubtedly escape even simple algorithmic techniques. upon the discovery of manipulated data, the crystallography community recently recommended that cif data can be accompanied by structure factors in an attempt to reduce fraud. 4 structure factors had been considered of minimal importance once the cif format for crystal structures became standard for publishing and sharing crystal data, but it is now realized that these parameters are less susceptible to manipulation than the processed data recorded in the cif files. because of the cultural norms in the crystallography community, the expected deposit of cif files has resulted in relatively straightforward methods to store both cif and structure factors, to validate the formats and the data using tools such as checkcif, and to display the data in both publications and interactively via online publication."
"funding agencies are starting to issue policies that require researchers to preserve and share the research data collected during the course of a research grant. government funding agencies, for example in the united states, u.k., eu, and australia, as well as private foundations, such as the howard hughes medical institute, wellcome trust, and gates foundation, are mandating that research data in support of journal articles be published in a reusable form. in addition, while data repositories have been common in discipline specific areas for many years and in fact have appeared and disappeared, more generic data repositories such as figshare, 9 data dryad, 10 and dataverse 11 are now available for researchers to store their experimental details. data journals are also being established to allow researchers to publish and get credit for data collection. these include, for example, data science, 12 nature scientific data, 13 methodsx, 14 and softwarex. 15 when publishing in these data journals, researchers can get credit in the form of a citation for data and/or software publication and at the same time also get credit for publishing a traditional article describing the results and conclusions of the research. community norms are still emerging as to how these publications will be treated in terms of scientific recognition."
"we can easily observe that the random variable x is irrelevant of ρ, and y is the central chi-square random variable with 2l degrees of freedom with a probability distribution function (pdf) [cit]"
"2.2.2. reader's perspective. the reader can freely download mnova software (a purchased license is only needed for general use) and open the .mnpub file. this file will be read by mnova and verified with the public key information contained in the mnpub file. if the verification is successful, the dataset pointed by the url field of the mnpub file will be retrieved for use within mnova with full functionality, without the need for a license. the reader will be able to review, reprocess, and reanalyze the research primary data, benefiting from all the functionality available within the mnova software package, albeit only for the digitally signed dataset. this effectively makes such signed data open for review and reproduction independently of access to licensed (in this example analytical chemistry) software applications. the impact of such submission of primary research data associated with articles is that it becomes available for detailed review by a wide range of stakeholders such as article reviewers, editors, data curators, publishers, peers, the general public and artificial intelligence, and machine learning systems. metadata associated with the primary research data as generated by the original instrument and then uploaded to the relevant repository also facilitates subsequent automated or workflow machine processing of the data on as large an aggregated scale as needed."
"note that these three invariants are sufficient but not necessary for a system to be memory consistent. for example, memory consistency can be satisfied by a system without cache coherence."
"5. a much less frequently applied but what we consider crucial metadata component is a declaration of a socalled resource map enabling object reuse and exchange (ore), 31 which addresses the a of fair. this in turn would allow any machine-driven automated procedures for retrieving the data files on a scale larger than a human would adopt. 6. a media type is also declared for specific files in any uploaded fileset. for the mpublish project, these include chemical/x-mnova for the basic mnova files and chemical/x-mnpub, which is automatically generated using the cryptographic keys, which control the singleuse license on which mpublish is based."
"11 indeed, enclosing the data with the rich toolkit provided by mnova addresses in large measure the i of fair. the other attributes can be accomplished by deposition into a data repository, a process that also includes generating metadata and registering it appropriately to ensure standardized publication. here, we look at the steps taken to achieve this as implemented in the imperial college mpublish pilot project. 24, 25 2.3.1. data granularity. the first task is to decide upon appropriate granularity for the metadata. mnova files (generated from mnova 11.0+ or using the automated mpublish workflow described above) can be generated for entire collections of spectra in a single file containing many molecules, or they can be restricted to just a single molecule, containing if necessary spectra for different experiments and nuclei. we strongly favor the latter approach, because metadata for single molecules can be generated far more easily and transparently, and we believe also more usefully than for molecule collections. this is largely based on the availability of the inchi molecular identifier 26 and the possibility of generating it for a specific molecule using a simple machine processable workflow procedure. we in fact apply it using openbabel 27 in which every file submitted to the data repository is automatically screened by openbabel, and if molecular content is identified, then both an inchi string and an inchi key are generated from it."
"we are unaware of any other protocols that are fractal. traditional snooping and directory protocols are not fractal, nor are existing hierarchically designed protocols. logical hierarchy is necessary to achieve fractal behavior, but it is not sufficient. our experience has shown that achieving fractal behavior requires detailed, explicit effort and does not occur \"naturally\", even in hierarchical protocols."
"one constraint is that we draw hard lines between the cores and the reordering mechanisms and between the reordering mechanisms and the cache-coherent memory system. this rigid system partitioning may sacrifice performance compared to a system that integrates these parts more tightly. however, given that many commercial machines already enforce these partitions, we believe that fcm is probably not leaving much performance on the table due to partitioning, but it is an interesting open question whether tighter integration offers significant performance benefits."
"verifying the allowable reordering invariant is a simple process that is confined to just the reordering mechanism itself. for example, consider a system that is sup-"
"the mpublish author workflow has been implemented as a plugin of the mnova software platform 20 and is now being integrated into the mgears automation system within mnova. although this limits its use to researchers and scientists that have licensed access to the mnova software for the processing of their nmr, lc/gc/ms, and optical spectroscopic primary data, the mnova software does have the advantage that it is widely adopted by the chemistry research community."
"therefore, n is a complex gaussian with zero mean and variance σ. variance σ will be a diagonal matrix of size l and can be expressed as"
"the registration of such metadata with a global aggregating agency such as datacite in return for association with a persistent identifier (a doi) can transform the data into a rich resource where all four attributes of fair data are at least partially addressed. these include the provenance of the data and unique descriptors of the molecule associated with the spectra to enhance findability via rich searches of the indexed metadata. a metadata resource map allows rich (machine) access, a toolkit facilitates interoperability, information is included in the metadata about any associated data relating to other aspects such as computational simulations and models and an appropriately declared license facilitates reuse. we suggest this model for spectroscopic publication could serve as a starting point for extension to many other forms of molecular spectroscopy and instrumentally generated primary or raw data. although ideally fair data might imply software agnostic and software independent tools, realistically the generation of fair data must take into account tools available now for researchers to prepare and publish research data. the methods described above do not purport to be a complete solution, they are merely an attempt to suggest some usable tools that can achieve the goals of fair data now for a specific type of data and to describe an approach, which could be extended to other types of data by other researchers in related domains."
"where e r l is the average signal energy at the lth sensor. equations (3) and (4) are called as fixed-gain and variable-gain amplification factors, respectively. in the fixed-gain amplification factor, the sensor ensures that the average or long-term power constraint is maintained but allows the instantaneous transmit power to be much larger than the average [cit] . however, in the variable-gain amplification factor, each sensor uses the channel state information from the source-sensor link to ensure that an average output energy per symbol is maintained for each realization [cit] . this operation is performed at all the sensors."
the main contributions of this paper are summarized as follows: 1) arrangement of the sensors according to snr ordering as an improvement over cs-based techniques; 2) application of reduced-rank techniques to provide the compromise between computational complexity and performance; and 3) development of a diversity-order analysis for reduced-rank techniques.
"as we can only process u elements of a, we require u elements to be nonzero and the other (l − 1) elements to be zero ideally. the location of these u nonzero elements may be anywhere within a, but for simpler processing, they should be consecutively placed as follows:"
"article enable this, a doi normally points to what is called a landing page from which further parochial navigation is required to allow access to individual files is needed. normally, a human performs this navigation (because it is rarely standard in any predictable sense), but with data, it is essential to have a formal declaration that a machine can traverse automatically. thus, the a of fair ideally relates both to visual access by a human and also to applications such as data mining by machines. one example 18 of the use of an ore map to automatically retrieve and display data based only on knowledge of the doi and the desired media type is based on the jsmol molecular visualizer. such a feature could in principle also be developed for more specific spectroscopic tools such as mnova."
"verification process: because cores are already logically in-order, validating that a core satisfies uniprocessor ordering is a process that already occurs during processor development. this validation process is wellunderstood and constrained to just the core, and thus there are no scalability challenges in validating uniprocessor ordering."
"the given two algorithms require the computation of the eigenvalues and eigenvectors, which can be difficult to implement in real-time applications [cit] . in some applications, computational complexity of calculating the eigenvalues will be similar to computing the inverse of the autocorrelation matrix. in these cases, eigendecomposition-based techniques cannot reduce the detection complexity. however, the krylov subspace methods can be used to minimize the mse as they do not depend on the eigendecomposition of the autocorrelation matrix r [cit] ."
"in fig. 6, the number of operations is plotted with respect to the selected size of u . it can be observed that, as u increases, more operations are required to detect a bit. the complexity of the cs-based scheme quadratically increases and is more than that of the tpa-based scheme. the complexity of the pc-and csm-based schemes is similar. for high u, the complexity of pc-and csm-based techniques is lower than that of the csand tpa-based techniques."
"the mpublish workflow generates a complete package ready to be submitted to a specific journal from primary datasets (.mnova or raw data files) that has been satisfactorily processed, analyzed, and annotated within the mnova platform (see further details on the mpublish author workflow in supporting information and figure s1 ). using the mpublish user interface, the author will be able to"
"throughout this paper, the following notations are used. upper case and lower case boldfaces are used for matrices and vectors, respectively. given a matrix a, symbols a *, a t, a h, and a −1 denote the complex conjugate, transpose, hermitian transpose, and inverse of a, respectively."
"another proposed benefit of making primary data available is the utilization of artificial intelligence/machine learning (ai/ ml) approaches to process data, to enhance existing methods for the prediction of properties of chemical structures, to develop new materials, to discover new drugs, and so on. 5 ultimately, the goal of these ai/ml techniques would be to understand the science well enough to propose new scientific theories. whether the latter is feasible, the use of ai/ml techniques requires a large quantity of data in an understandable and reusable format."
"scaled system design. the second step is the scaled system design. the smallest system can scale to any arbitrary n-node system by adding internal interfaces between the top interface and the basic nodes as shown in fig. 2(b) . in the scaled system, requests and replies also go up the tree and forwarded requests and forwarded replies also go down the tree, however, they do not go all the way to the top interface each time as the smallest system does. they only need to go up to the highest common ancestor interface of all destinations."
"it is also intractable to compute the exact cdf of γ f, and we first find the tractable lower bound of γ f . the lower bound on the diversity order in terms of snr will emphasize that the system will have at least the diversity order of the lower bound of γ f . intuitively, γ f is bounded by γ f, lb, which is represented as"
"where (·) indicates that the vector is now reduced to size u instead of l. in the second mode, thisȳ is passed through a u -dimensional filter. the modified cost function can now be given as"
"smallest system design. the design process involves two steps: the design of the smallest system, which includes only the basic nodes and the top interface, and the design of the scaled system, which includes all different kinds of components in the system. the smallest system for a d-degree tree in treefractal is shown in fig. 2(a) . the top interface holds copies of the cache tags and coherence states of its d children, and it serves as the seri- treefractal is similar to both snooping and directory protocols, but it is not the same as either of them. the coherence controller in the basic nodes responds to load and store requests from the core. depending on whether the requests can be satisfied within the node itself, the coherence controller decides whether to issue a coherence request up to the top interface. the top interface is responsible for responding to the coherence requests from its children cores. after receiving a request, the top interface looks up the state of the block in all its children. based on these states, the top interface might need to forward the request down to its child/children, similar to directory protocols. as the serialization point for all transactions, the top interface always needs to forward a request back to the requestor, so that the requestor knows when its request is ordered with respect to other coherence requests, similar to snooping protocols."
"1. the orcid identifier for the depositor along with any further such identifiers for appropriate collaborators. for the repository, 28 we use orcid as an intrinsic part of the login process and so its inclusion in the metadata records is automatic, as is the affiliation of the depositor. 2. the deposition date and time are also recorded automatically, together with the identity of the formal publisher of the data, which can be the research institution as in the example here. 3. a title and description are recorded, these being part of the dublin core metadata elements. 29 although these can have any value, we favor including the systematic name of the molecule as part of the title field and a brief description of the experimental procedures used to prepare the compound in the description field."
"2.3.2. metadata. further metadata is also generated in a workflow manner (figure s5 ), as implemented in the data repository. 25 a minimal set would include the following:"
"the mpublish tools available within the mnova nmr analysis program allow the workflow generation of publication-ready supporting information (si) files, which can be submitted to a publisher and/or a data repository. both the original primary/ raw data and the generated si files can be integrated with cryptographic license keys to allow access to individual datafiles without the requirement of first obtaining full commercial licenses. further workflows can be used to generate a set of standardized metadata describing the datasets."
"how to design a memory system and completely verify that it complies with the corresponding memory consistency model is difficult. traditional methods usually aim to find bugs after the design has been completed, which is laborious and expensive. we propose a system model, fcm, based on three invariants. if the designer fits their system into this model, the verification can be decomposed into three steps that are self-contained and scalable. this system model eases the verification of the memory system by considering verification early in the design stage."
"here, we propose the design of the preprocessing matrix p with the assistance of reduced-rank techniques. in these techniques, we will utilize all the l signals to design the preprocessing matrix p instead of selecting u signals out of l and losing the energy of (l-u ) signals. we propose three different techniques, where the first two techniques are based on an eigenvalue decomposition, whereas the last technique utilizes the tpa."
"depending on the specific consistency model, the fcm permits the insertion of a reordering mechanism between each core and its cache hierarchy. the goal of the reordering mechanism is to improve performance. sequential consistency does not permit a reordering mechanism, but other models permit reordering mechanisms [cit] such as fifo write buffers (processor consistency, tso, x86) and coalescing, un-ordered write buffers (weak ordering, alpha, power)."
"and then, a u and d u will consist of the first u strongest signals and, by applying the similar process in section v-a and b, the optimal preprocessing matrix p can be easily carried out."
"the receiver schematic block diagram for the ideal mmse receiver is shown in fig. 2(a) . to estimate the desired data bit, the receiver consists of a linear filter characterized by"
"n wireless sensor networks (wsns), the fundamental task is to broadcast data from the origin sensor to the destination. however, due to the limited size, power, and cost of these sensors, a low-power signal is often transmitted to the destination [cit] . this low-power signal is further attenuated due to the propagation loss. to combat this problem, the signal is sometimes measured by as many sensors as possible [cit] . these sensors form a distributed cooperative sensor network, enabling them to achieve spatial diversity that will help combat fading effects and extend network coverage [cit] ."
"2.3.3. discovery and findability. the final aspect of fairenabling the mpublish data addresses f, the findability. the registration of the metadata with datacite allows the search interface provided there to be used to discover data with the appropriate properties. machine processable examples are included below to illustrate this."
"the dvmc paper [cit] proved that if a memory system satisfies three invariants, then the implementation conforms to the memory consistency model. the three invariants are as follows. 1) uniprocessor ordering. in a single-threaded system, a core's read from a given memory location should get the value from the last write to that location in program order. we must verify that this rule is not violated in a multithreaded system unless other cores access this memory location."
"the main issue of symmetric cryptography is how to share secret keys without a secure channel. fortunately, it is possible to use public key cryptography for efficient pairwise key agreement, which then allows the use of symmetric cryptography. to that purpose, we consider the approach of id-based cryptography, which we review next."
"it is worth noting that bob and alice can exchange authenticated messages without using encryption. this is due to the fact that alice already sends encrypted messages to the ap, using bob as the intermediate. however, some use cases require that they exchange messages confidentially. for example, direct communication between alice and bob either started by an application or a handshaking procedure."
"in the handshaking procedure, alice is requested to respond to a challenge, so that bob is able to verify alice's identity. it is worth noting that bob and alice should know each other's identity for exchanging authentication messages. this allows them to compute a shared key, which is used for authenticated encryption of the challenge. thus, bob can encrypt a message to alice, which decrypts the ciphertext to obtain the challenge that can be encrypted again and sent back to bob. figure 3 shows in detail this process, in which alice and bob use a counter in order to protect messages from replay attacks."
"to control the access of network resources it is required not only to authenticate a new node x, but also to ascertain membership eligibility and bootstrap security services such as data confidentiality and authenticity. in a typical wireless scenario, access control can be achieved by the 802.1x standard, including the radius/eap protocol. in 802.1x, the authenticator is the end of the link requiring authentication. it usually operates as a pass-through, forwarding packets between the backend server and the user. the eap framework requires that the backend server (here the controller) can only distribute cryptographic keys to the authenticator (here the gw). this is referred to as \"the principle of mode independence\" 4 and shows that radius has lack of compatibility with non-collocated authenticator function and encryption/decryption function. note that, in our scenario, even though the gw is able to perform authentication, it cannot decrypt packets from alice; thus, different keys still need to be established between alice and the other players, namely the ap and gw. in order to enable user authentication via the gw device, we propose to have the gw run an authentication module (am) integrated with the sm. membership eligibility and distribution of cryptographic material are performed by means of an application on top of the controller."
"in the aforementioned scenario, one can also consider sdn applications that exploit many wireless networks, as demonstrated by prior works [cit] . for example, alice can be provided with services by two gateways simultaneouly (here, bob and charlie). as one can see, this is another application made possible by a sdn based architecture, not possible or very challenging and prone to errors on the traditional scenario."
"after the handshaking procedure, alice needs to establish pairwise keys with the ap and the controller. it allows her to send encrypted messages that the gw cannot decrypt. in addition, alice needs a shared key with the controller for proving her identity, which prevents a malicious bob from pretending to be a gateway for users that actually do not exist. such a key can also be used for check_out messages, which are requests for being disconnected from the gw (and not being charged for services)."
"in this paper we proposed an efficient, flexible, yet secure sdn-based framework for capacity sharing in hybrid networked environments. our proposed framework extends the scope of the existing network infrastructure and enables a number of new applications and services for mobile users. from the network service provider's point of view, by performing efficient sharing of network resources, our framework aims at minimizing the need for over-provisioning the network."
"2 ) do not provide adequate security for these types of scenarios and applications. for instance, in the particular scenario of figure 1, \"bob\" may need to authenticate \"alice\" to make sure she is a legitimate user, etc. furthermore, \"bob\" should not be liable for misbehaving users (here \"alice\") connecting through him."
"it is worth noting that the aka procedure considered here has the main goal of avoiding public key encryption. it means that, once a key is agreed between two nodes using public key cryptography (i.e., ibc), they can use the shared key for confidentiality and data authentication (i.e., symmetric cryptography), which is very efficient."
"based on our prior work, we can compute the pairing in about 1 second using a cell phone of 434 mhz and 128 mb sdram memory. since this computation is only performed in the first steps of the protocol, we argue that the proposed security solution is feasible in real scenarios. moreover, our implementation is based on java and can easily be easily integrated into openflow controllers such as floodlight 6 ."
"the am installed on bob's device will communicate with alice so that mutual authentication is performed. in order to do that, a node only needs the other node's identity for computing a shared key, which is then used in a challenge/response procedure (as defined in section iv-c). after node admission, the computed shared key is also used for confidentiality and data authentication. the am will then notify the controller that might accept or reject alice's request. note that the controller is only notified by bob after alice is authenticated, which protects the central entity against possible denial of service attacks."
"we employ the sok protocol [cit] in the aka procedures. figure 4 illustrates the exchanged messages for key agreement between alice and the other participants, namely the ap and controller. note that the purpose of such messages is not only to establish pairwise keys 5 . the message check-in_req has the goal of requesting the controller authorization for secure communication. if a user is not authorized by the controller, neither flows or keys will be created. that can be possible by means of an application running on top of the openflow controller."
"the controller now knows that charlie might be another candidate for alice. for example, bob's mobility can cause low signal strength and bob can also be disconnected if out of the ap's range or by system outage. the controller is able to set a flow table entry at charlie's openflow software switch, in order to make charlie the responsible for forwarding alice's traffic. consequently, it might delete the corresponding flow table entry from bob's software switch. note that the corresponding entry would expire in case bob cannot be reached by the controller."
"in this work we propose the instantiation of a software module at the gateway device (in our case, bob's device). we call it the switching module (sm). from an implementation point of view, the sm can be instantiated as an openflow software switch running on the gateway (gw) device and is responsible for forwarding incoming traffic, maintaining flow tables, and communicating with the controller when needed."
"the concept of sdn is centered around controlling forwarding policies and has been thus far put into practice in infrastructure-based networks. however, in infrastructure-less environments such as mobile ad hoc networks (manets) or vehicular networks (vanets), the devices involved in data forwarding are also end devices themselves. consequently, end devices should be able to perform the functions of sdn switches, i.e., communicate with controllers and understand how to handle forwarding rules. since end devices in infrastructure-less networks are typically portable and do not have access to continuous power sources, the deployment need to be lightweight in terms of its code, storage, communication, and power consumption footprint."
"2) gateway device incentive: similar to other capacity sharing solutions, our framework assumes that end users will be incentivized to contribute to sharing network resources, which in our case will be in the form of relaying traffic for other users. incentives, their policies and implementation are outside the scope of this paper."
"traditional scenario: assuming the ad hoc network learns to route to bob as a gateway, and bob allows his device to be used as a nat box by other users, the mobile data service provider is not aware of the existence of alice. bob's connection is not assigned additional bandwidth, possibly harming performance; the internet service provider is not able to differentiate alice from bob and cannot apply any qos rules, access restriction, or any sort of policies on alice's traffic without also impacting bob's; furthermore, bob will be held responsible for alice's traffic by the service provider for any possible data overages or illegal activity."
"below, we enumerate issues that should be addressed in order to enable the scenario in figure 1 . for each case, we propose a sdn-enabled solution. it is worth noting that we use the capacity sharing scenario described in this paper as an example of how our sdn-based capacity sharing framework can be used. the proposed framework aims at enabling a number of new applications and services in heterogeneous network environments by sharing network capacity efficiently and securely."
"in this paper, we explore solutions to capacity sharing in wireless access networks. more specifically, we examine scenarios such as the one depicted in figure 1 in which a wireless infrastructure-less network can be used to extend the scope of the existing infrastructure-based network. for example, a user, say \"alice\", who may be temporarily without access to her network service provider, may connect to the internet through the infrastructure-less network (in this example, through \"bob\"). participants in the infrastructure-less network may receive incentives from their service provider to serve as \"gateways\" to other users. clearly, security is a major concern as existing standards (e.g., 802.1x"
"identity based cryptography (ibc) [cit] allows a user to calculate a public key from an arbitrary string. using the users' identity as a public key has advantages such as: (1) there is no need to verify the public key using an online certification authority (c.a.); and (2) a user only needs the recipients' identities in order to calculate public keys (i.e., there is no need to ask for public keys). in addition, cryptographic protocols are simple and efficient under the paradigm of ibc."
"in the case of a scenario with multiple gateways, as depicted by figure 1, another user (here \"charlie\") joins the network and is able to act as a gateway. charlie also has the sm and am running on his device. the sm communicates the controller that charlie has a new interface on, in ad hoc mode."
"even though ibc was introduced by shamir [cit], it was only realized with bilinear mappings, or pairings [cit] . pairings also provide practical implementation for authenticated key agreement over ibc, which is an elegant alternative to non-authenticated schemes such as the diffiehellman interactive key exchange."
"motivated by the vision of a fully-connected world in which wireless access networks extend the scope of the wired infrastructure [cit], we propose a sdn-based framework for flexible, efficient, and secure capacity sharing in current and emerging hybrid network environments. to the best of our knowledge, this is the first sdn-based capacity sharing solution targeting hybrid networks. additionally, it incorporates security as an integral part of the proposed approach. in this paper, we describe our framework in detail and showcase its use in the context of the application shown in figure 1 ."
"current openflow version 1.3, already allows qos polices to be enforced by means of creating virtual ports on the switches and applying weighted fair queuing (wfq). in our case, when alice joins the network, after authenticated, the controller might choose to restrict her access to certain applications (e.g. deny/restrict bittorrent connections or more bandwidth demanding applications such as video streaming) in order to preserve, not only bob's access, but also the access and quality of experience of other users connected via bob in the same manner as alice."
"our framework explores the synergy between sdn controllers and ttps, so that security initialization (e.g., key distribution) relies on existing network infrastructure. it is worth noting that such an approach requires additional mechanisms for controller coordination. in addition, given that the sdn paradigm is typically centralized, inter-domain roaming should rely on different key management schemes or protocol extensions."
"even though bob provides alice with the service of message forwarding, he should not be able to decrypt her traffic. thus, alice needs to establish a secure channel with other entity. however, she might not have another peer that is capable of providing a vpn service. our proposed architecture suppresses the need for additional vpn services and confidentiality is intrinsically included. since all the wireless traffic should be received by the ap, it turns out that the best candidate for implementing a secure channel with alice is the ap. that is possible by agreeing on a shared key with alice."
"end-to-end security is implemented using symmetric encryption. generally, encryption is provided with authentication, since encryption-only schemes might succumb to attacks (e.g., encryption-only configurations of ipsec [cit] ). for this reason, we consider authenticated encryption of messages exchanged between alice and bob, as well as alice and the ap. this is illustrated in figure 6, in which security is transparent between the gw and the ap (e.g., using wpa2). moreover, regarding alice's traffic, messages relayed to and from the ap cannot be interpreted by the gw (or by an eavesdropper). on the other hand, the gw (i.e. bob) is able to authenticate messages it relays to the ap, which protects it from relaying unauthorized messages. to summarize, not only can the gateway authenticate messages from alice, but also transmit authenticated encryption messages to her. the latter is used in the handshaking procedure."
"as part of our ongoing research, we intend to demonstrate the feasibility of our approach by implementing it in an real testbed. we also plan to demonstrate the ability to perform seamless and secure handover as well as extend access to devices participating in multi-hop wireless networks, e.g., manets or vanets. 6 http://www.projectfloodlight.org/floodlight/"
"in order to provide simple and efficient security services, we employ id-based cryptography (ibc) [cit] . by eliminating the need for generating and managing users' certificates, ibc significantly reduces the complexity of a cryptographic system. moreover, ibc does away with manual configuration of shared keys among backend players, which is required in protocols such as radius/eap 3 . this is achieved through key agreement procedures, which also allow users to compute shared keys. in addition, we prevent restrictions imposed by credentials based on username/password, such as delay of typing this information and the use of devices with no keyboard."
"upon acceptance, the controller will insert the appropriate new flow table entries to bob's sm, in order to grant alice access to the outside network via openflow protocol. concurrently, the controller proactively sets the appropriate flow table entries in every forwarding device under its control, in alice's data path to the internet, allowing her access to the infrastructure of the network. this proactive setting of forwarding rules prevents all forwarding devices in the data path from the user to the internet send packet-in messages to the controller, saving resources and increasing scalability."
"as mobile devices equipped with multiple network interfaces become commonplace, users will expect \"anywhere, anytime\" connectivity regardless of location or type of network access. as a result, a major challenge facing future networks is to provide ubiquitous connectivity in a resourceefficient fashion. efficient utilization of network resources is critical since expanding network resources at the same rate as network traffic increases is not economically viable. the need to utilize network resources efficiently is exacerbated in wireless (access) networks, where resources are inherently more constrained and traffic is expected to double every year in the next few years 1 . this trend is generally referred to as \"capacity sharing\" [cit] ."
"confidentiality can be provided by means of symmetric ciphers (e.g., aes [cit] ), which are generally more efficient than public key encryption. data authentication can also be implemented efficiently by means of symmetric cryptography using message authentication codes (macs) (e.g., cmac [cit] ). a mac operates over a symmetric cipher for generating authentication tags, which are sent together with messages. the recipient can use its own key to calculate the expected tag; the message is accepted (i.e., authenticated) only if the calculated value matches the received tag."
"we are currently implementing our framework atop the openflow protocol leveraging our prior work on secure sms transmission [cit] . the most expensive cryptographic operation used in the scenario proposed here is the computation of the pairing function, which is used in the handshaking and key agreement procedures."
"note that all secret keys can be computed by the ttp. fortunately, in the scenario explored here, there is a synergy present between controllers and ttps. controllers can be α openflow controller β openflow access point (ap) idx identity of node x s master secret key (controller's secret key) sx private key of node x px public key of node x (derived from idx ) kx,y key established between nodes x and y ctr counter authenc(msg, k) authenticated encryption of msg using key k enc(msg, k) encryption of msg using key k dec(msg, k) decryption of msg using key k mac authentication tag e(·, ·) pairing function considered trusted entities, since they provide interfaces to applications that perform management tasks. thus, in the context of ibc, a controller could be responsible for generating (and possibly distributing) private keys to users in its domain."
"we assume the network model illustrated in figure 1 . in this paper, we focus on scenarios where a user \"alice\" wishes to connect to the internet and access, for example, the world wide web. however, she is unable to connect to the existing network infrastructure (e.g., because she is out of range of the closest ap). another user \"bob\" advertises his gateway services providing \"alice\" the option to connect to the internet through him. note that alice can connect to bob directly or through a wireless, multi-hop ad-hoc network (manet)."
"considering that each participating node already possesses its unique identification id x and private key s x (as defined in the setup procedure in section iv-b), we now describe the scenario in which a new node (e.g. alice) will connect to an ad hoc network and access the infrastructure network via a gw device (i.e. via bob)."
"when alice joins the network, the service provider (through its sdn controller) is made aware of alice's presence. it may then decide to offer service to alice via bob and provision bob's connection accordingly. the service provider may decide to sell alice a temporary connection plan on the spot, or alice may have an existing contract on another device; available resources, past user behavior, or any number of factors can be used in deciding whether to offer service to alice, and if so, what kind of service to offer. the service provider is thus able to maintain control of its network resources and how they are utilized and shared, while being granted opportunity for additional business. alice is able to seamlessly connect to the internet using an existing service plan or on a \"pay-per-use\" basis. for his part, bob may be offered incentives by the service provider, while avoiding performance loss or being held liable for alice's traffic."
"given that a user's public key is tied to an unique identity, the issue becomes how to obtain the corresponding secret key. a trusted third party (ttp) is responsible for secret generation, which is performed by using its own secret key, also known as master secret key, and the public key of the target user."
"our proposal is based on openflow [cit] and, to the best of our knowledge, is the first to address security services such as admission control, data authentication and confidentiality for end users and openflow software switches (the gw and the ap are software switches extended to support the security functionalities)."
"since the node attached to the ap operates as an openflow switch, it can send packet-in messages to the controller. then, upon verifying the user and the associated flow, packet-out messages can be sent to the entities that comprise the overall application. this is illustrated in figure 5 . security for packetin and packet-out messages is provided by the openflow standard, which is based on tls. thus, this procedure does not depend on the security architecture that is proposed here."
"the study of wmns used in coal mine mainly focuses on the application of emergency rescue communications. the research covers multihop transmission performance, network planning and coverage, routing protocol, channel allocation, and resource management, which can be divided into two categories: single-chain and strip (double-chain), in accordance with the topology of the backbone routing layer."
"international journal of distributed sensor networks 5 where (, (, )) denotes the energy consumption for data transmission from node to, ce(i) denotes the current remaining energy of node, ie( ) denotes the initial energy of node, and ce( )/ie( ) denotes the current remaining energy level of node . the above part of energy cost function reflects the efficiency of energy consumption for data transmission, and the under part considers the energy balance. the less residual energy the sending node has available, the greater the value of the energy cost is, and the less probability this node is selected as the transmitting node. because the mesh router has no energy limitation, if it transmits data, the energy cost is defined as zero."
"this study accessed the validity of the cart for evaluating the fit accuracy of fixed dental prostheses by using statistical agreement analysis and confirmed that the measurement value obtained from the cart agreed with that of the rt, with high accuracy and adequate precision. thus, based on the results of this study, it is conceivable that the cart is a feasible method for 3-dimensional analysis of fixed dental prostheses."
"as coal mining and tunnel excavation, coal face and tunnel face are constantly moving, with the harsh environment and complex conditions. so, the wired communication networks could not reach these special areas and there are some blind spots existing in the wired communication and monitoring systems in the underground mine [cit] . at the same time, rescue members need to transmit video, voice, and environmental monitoring to the commanders on the ground and the rescue staff underground through the wireless emergency rescue communication system. the coal mine needs wireless monitoring and wireless communications. recently, wireless mesh networks have been considered as an alternative solution."
"during the process of route discovery, the mesh client will broadcast the route request (rreq) messages. when rreq passes through each hop, the node will calculate the path energy cost and the hop count according to (4) . the mesh router receives the rreq messages from multiple paths and calculates path cost according to (5) . the route reply (rrep) message will be routed back along the reverse path to the source node and the path with the least path cost will be selected:"
"end-to-end delay. in figure 9, the average end-to-end delay in eor-hwmn starts to increase after 220 s. with the increase of the energy consumption of mesh clients, eor-hwmn needs to generate routes with more hops to avoid low-energy nodes, which will increase the average end-to-end delay. from 350 s in the simulation, the traffic and the energy consumption of nodes decrease, so the average end-to-end delay tends to stability. in aodv, the route with the least hop count is selected to transmit data, which causes the average end-to-end delay to be lower than eor-hwmn and aodv."
"if a mesh client is within the coverage of a mesh router which has a routing path to the gateway, it is the ap covering network state. the mesh client can access the backbone transmission network directly through mesh routers to communicate with the gateway, without the process of route discovery, and reduce the energy consumption of mesh clients for broadcasting rreq messages."
"low-energy nodes. the above energy optimization strategy can significantly balance the energy consumption of the mesh clients. if most of nodes in a route have much residual energy, but only a few nodes have little remaining energy, in this way the path cost is not very high and this path may be chosen. however, when this route is used to transmit data, the nodes with little residual energy will deplete its energy, which will interrupt the processing of the traffic and decrease the network lifetime."
"in order to solve this problem and further optimize the energy consumption among nodes, we propose the avoiding strategy for low-energy nodes. this strategy will delay the low-energy nodes of which the residual energy is less than a certain threshold for a period of time to broadcast rreq messages. so, this strategy can indirectly control the arrival time of rrep messages. the arrival time of rrep message with the route including low-energy nodes is much later than other routes; therefore the source node can select other routes avoiding low-energy nodes. since the delay is only for low-energy nodes, this strategy can avoid long delay during routing discovery process."
"as a promising wireless access network, wireless mesh networks (wmns) are multihop wireless networks which consist of three types of nodes, mesh routers, mesh clients, and gateway nodes, with self-healing and self-configuring [cit] . they have the features of high transmission rate, wide coverage, rapid deployment, flexible networking, and scalability [cit], which can effectively solve some problems that the wired network and traditional wireless networks could not solve. the architecture of wmns can be classified into three main groups on the functionality of the nodes [cit] . client wmns. in this type of architecture, client nodes constitute the actual network to perform routing and configuration functionalities as well as providing end-user applications to customers. hence, a mesh router is not required for these types of networks. infrastructure/backbone wmns are as 2 international journal of distributed sensor networks follows. wmns include mesh routers forming an infrastructure for clients that connect to them. client nodes in this type do not have the routing function and could not communicate with each other directly. it needs to connect to mesh routers to achieve wireless broadband access. infrastructure/backbone wmns are the most commonly used type. hybrid wmns are as follows. this architecture is the combination of infrastructure and client wmns. mesh clients can access the network through mesh routers as well as directly meshing with other mesh clients, which will enhance the connectivity of the network and expand the coverage. so, the hybrid architecture will be the most applicable case."
"according to the relationship between mesh clients with backbone routing layer and the status of mesh routers, the network state can be divided into three types: ap covering, network edge, and backbone-recovery."
"the current wmns in coal mine are infrastructure/ backbone wmns. adapting to the long and narrow underground tunnel, the topology of the backbone routing layer is single-chain or strip (double-chain), with low connectivity and poor robustness. the failure of mesh routers could easily break the backbone transmission network, causing that the corresponding clients could not communicate with the gateway and forming blind monitoring spots. so, the infrastructure/backbone wmns in coal mine are difficult to provide reliable communication support for special underground region monitoring and emergency rescue."
"based on the above description of different network states, we can find that the mesh clients participating in routing or not depend on the network state. designing the energy optimized routing algorithm should comprehensively consider different network states and achieve the optimized combination of energy efficiency and energy balance for mesh clients."
"the designed algorithm eor-hwmn demonstrates its superiority to power-and node-type-aware routing (pntara) and ad-hoc on-demand distance vector routing (aodv) on energy efficiency, energy balance, and quality of service metrics. simulation results show that eor-hwmn has balanced the energy consumption among mesh clients, extended the network lifetime, and rapidly rebuilt the route when mesh routers are failed."
"where ec path( ) represents the energy cost of path, path( ) represents the hop count of path, and adjusts the proportion between the energy cost and the hop count."
"the route discovery is initiated whenever a source node needs to communicate with the gateway and has no routing information in its routing table. in the energy optimized routing algorithm proposed in this paper, the route discovery and recovery mechanism is designed based on the network state. when a source node wants to send data to the gateway node and does not have a valid route to it, will first check its local state."
"if is in the network-edge state, it will start the route discovery mechanism based on rreq/rrep and make routing decisions according to the path cost."
"though the performance of eor-hwmn has been demonstrated by simulation study, the understanding of this approach can be deepened by building theoretical foundations. two theoretical works are being considered. one is the building of the analytical models of the combination of energy efficiency and energy balance, and another is the perfect weight ( ) in the path cost obtained through theoretical analysis. in the future, we plan to build a real hybrid wireless mesh network in underground mine, through deploying mesh clients, mesh routers, and the gateway in the tunnel. eor-hwmn will be tested on the real network."
"in wmns with infrastructure/backbone architecture used in underground mine, the breakdown of mesh routers could damage the backbone transmission network, causing corresponding clients out of contact with the gateway and forming blind monitoring spots. mesh clients in hybrid wmns can participate in networking and routing, which will improve the connectivity and reliability of wmns. hybrid wmns are more suitable to provide communication for safety monitoring and emergency rescue in underground mine. energy is one of the most critical resources for mesh client in hybrid wmns. the energy of mesh clients should be consumed optimally."
"to achieve the effectiveness of the energy consumption of the mesh client, it is required that the data transmission for the mesh client to the gateway node consumes less energy. however, most energy-efficient routing algorithms tend to route data via nodes on energy-efficient paths and thereby drain their energy quickly. to achieve energy balance of the mesh clients, the nodes with more residual energy should be used to forward data, which will often lead to data relaying among many nodes, long routing paths, large data transmission delay, and the waste of energy. so, to prolong the lifetime of wireless mesh networks, the routing algorithm must be designed to achieve both energy efficiency and energy balance together. it should not only reduce the energy consumption for data transmission to extend the lifetime of a single mesh client, but also balance the energy consumption for the whole network. based on the above requirements, we have designed a new energy cost function."
"within the limitations of this statistical agreement study, it is confirmed that cart is as accurate as the rt for evaluating the fitness of fixed dental prostheses. therefore, this digital approach can be applied to the analysis of fit accuracy of fixed dental prostheses for visualizing and quantifying the fit discrepancy 3-dimensionally. [cit] ;9:358-63"
"3.1. network architecture. wmns deployed along the tunnel in underground mine with single-chain topology based on the backbone architecture are shown in figure 1 . the mesh clients in this network do not have the capability of networking and routing and can only access the wireless backbone transmission network through mesh routers. any mesh router's failure will cause some mesh clients interrupting the communication with the gateway, and the security monitoring system will lose some monitoring sites, which will threaten the life of emergency rescue members. mesh clients in wmns with hybrid network architecture in underground mine could participate in networking based on its routing function, which is shown in figure 2 . when mesh routers are broken down, the mesh clients could establish new communication with the gateway through other mesh clients, avoiding monitoring blind spot and enhancing network reliability. however, the performance of mesh clients (e.g., cpu and storage space) is lower than mesh routers, especially its limited energy supply. therefore, in the process of routing, a joint optimization combining backbone routing layer and mesh client layer should be employed depending on the network states."
"if one or several mesh routers are broken down, the backbone transmission network will be interrupted and the mesh clients covered by the faulted mesh routers cannot achieve communicating with the gateway. in this status, the mesh clients can repair the interruption of the backbone transmission network using its function of networking and routing. the mesh routers and related mesh clients will start the request-reply route repairing mechanism and rebuild the route to the gateway."
"an indispensable prerequisite for use of the cart is to have a congruent area for image-superimposition of stl files. without such a suitable reference area, high-quality image alignment cannot be performed. in this in vitro study, large and notch-including base parts of the die were used as a reference for superimposition. for clinical conditions, adjacent teeth or non-movable mucosa can be used as the reference area. additionally, it is necessary to use a suitable optical scanner for the cart. contemporary certified optical scanners show high accuracy and reproducibility. 4 further large-sized clinical prospective trials are required to confirm the results of the present study and to evaluate the feasibility of applying it to various clinical conditions."
"cost. the basic idea of the routing algorithm based on the energy cost is to build a path with the minimum energy cost from data source node to the gateway node. the total energy cost achieving data transmission, in which node was selected as the relay node, is the sum of the energy cost from node to and from node to (one of the neighbors of node ) it is shown as"
"if is in the ap covering state, it can communicate with by the backbone network through the mesh router, which is used as the mesh access point. the route to the gateway node is constructed among mesh routers by proactive routing 6 international journal of distributed sensor networks protocol in advance, and the data will be transmitted along the backbone network toward ."
"based on the definition of the total energy cost, the mesh client will not only consider the energy consumption for data transmission from itself to the candidate node, but also consider the energy consumption for the subsequent data transmission, which will measure the energy efficiency of the candidate node selected as the next hop accurately."
"the rest of this paper is organized as follows. in section 2, we present related works. section 3 provides a brief review of the network architecture of the hybrid wireless mesh networks for mine emergency rescue and describes the network state. section 4 proposes the energy optimized routing algorithm that could be used for hybrid wmns in underground mine. in section 5, we show how the algorithm has been implemented. the simulation model and the comparative performance evaluation of the proposed routing algorithm are presented in section 6. finally, section 7 concludes this paper."
"the agreement was defined as follows: the within-sample total deviation was not more than 31.6% of the total deviation. 9 this translates into a least-acceptable ccc of 0.90 (1-0.316 2 ). lower 95% confidence limits were obtained from the bootstrap percentile using a fixed, large number of bootstrap resamples."
"6.2.1. packet delivery ratio. in figure 8, the mesh clients with energy depletion appear from 220 s in aodv, and the number of energy exhausted nodes increases over the simulation time, so the packet delivery ratio (pdr) has declined. eor-hwmn and pntara have employed the energy optimization strategy, delayed the time in which nodes with energy exhaustion emerged, and decreased the number of nodes running out of energy. but the hop count of the route in eor-hwmn and pntara is more than that in aodv, which will increase the packet loss probability. so the pdr in these three algorithms is basically the same in the beginning period of the simulation. to verify the routing recovery capacity of eor-hwmn, we set arbitrary two mesh routers' failure at 260 s in the simulation. from figure 8, we can see that eor-hwmn has conducted the route discovery and maintenance mechanism based on network states when the mesh routers are broken down. although the value of pdr in eor-hwmn has slightly decreased, it has quickly recovered stability, which shows better routing recovery capacity than pntara and aodv."
"the above studies are based on infrastructure/backbone wmns and the problems of backbone routing layer with strip topology are solved in a certain extent. however, the wmns with single-chain or strip topology in underground mine have low connectivity and the failure of mesh routers in backbone routing layer could easily lead to the disconnection of backbone transmission network. mesh clients in hybrid wmns are provided with the functions of data forwarding and routing, which will achieve rapid recovery from failure when mesh routers are broken down. but these will consume additional energy and deteriorate the network lifetime of wmns, because of the limited energy supply of mesh clients. since the radio transceiver typically consumes more energy than any other hardware components onboard a mesh client, designing energy optimized routing algorithm is of great importance to prolong network lifetime [cit] ."
"if is in the backbone-recovery state, whether mesh router or mesh client, it will start the request-reply route discovery mechanism and rebuild the route according to the path cost."
"the advantages of converting a gap space to a virtual image include the convenient data handling and increased data utilization. since the measurement object is a digital image, outcome parameter analysis can be performed with ease by using the editing and measurement functions of computer software. this computer-aided approach allows various geometric analyses, such as the overall mean discrepancy, quantification of discrepancy in a specific region, and the entire volume of the gap space. a 3-dimensional, color-coded map showing discrepancies is also a useful way to reveal the distribution and degree of error. 14, 27, 28 another advantage of digitization is the decrease in the possible error sources in methodology. the cart eliminates the manual replica sectioning process and reduces the need for decision-making during the selection of measurement points. thus, this computer-aided approach could aid in obtaining more reliable results."
"in figure 5, it shows that the average residual energy of mesh clients decreases slowly with the increase of the number of mesh routers, when the first mesh client exhausts its energy. the average residual energy of mesh clients in aodv is higher than that of pntara and eor-hwmn. in aodv, the hop count is used as criterion of route decision and the algorithm does not consider the characteristics of energy supply of nodes with different types, which will lead to heavy load, fast energy consumption, and prematurely energy depletion of mesh clients in the shortest path. eor-hwmn has designed a new energy cost criterion for mesh clients, which combines the energy consumption for data transmission and residual energy of mesh clients into path cost calculation. the less energy consumption for data transmission and more residual energy of data sending node will lead to less energy cost, which has achieved the optimized combination of energy efficiency and energy balance. at the same time, eor-hwmn reduces the probability of the low-energy node relaying the data through the avoiding strategy for low-energy nodes, which will further balance the energy consumption of mesh clients."
"china is a major coal producer. coal accounts for about 70% in disposable energy consumption in china currently, and it will play an important role in economic and social development of china as a kind of important strategic resource in a long period of time. normal radio systems can provide at best a very limited and ineffectual communication capability in confined spaces. as a result, the problem of providing communications capability in hostile underground environments such as tunnels in coal mine, both for exploitation and emergency situations, has been an important issue in recent decades, especially after a series of unfortunate underground disasters and accident [cit] ."
"mine hybrid wireless mesh networks. the mine hybrid wireless mesh network can be expressed by an undirected graph (, ), in which denotes the set of nodes, including mesh clients, mesh routers, and gateway nodes, and denotes the set of wireless links among nodes which can communicate directly."
"in this paper, we have designed a new energy cost criterion for mesh clients considering the energy consumption for data transmission and residual energy of data sending nodes, which has achieved the optimized combination of energy efficiency and energy balance. energy optimized routing algorithm for hybrid wmns in underground mine is proposed on account of different network states. the route with the least path cost will be selected to transmit data."
"if it is backbone-recovery state, it shows that the mesh router connected to the transmitting node is broken down. the node which received the rreq message may be the mesh client or the mesh router. if it is the mesh client, it will perform action 1 or action 2 according to its own state, ap covering state, or network-edge state, respectively. if it is the mesh router, it will perform action 3. action 1. the receiving node automatically sets up the reverse route to . then it will generate the rrep message based on its routing information to the gateway node and transmit the rrep to along the reverse route. action 2. the node automatically sets up the reverse route to . then it will rebroadcast the rreq to its own neighbors after updating the hop count and the path energy cost. action 3. the mesh router compares its hop count with . it will drop the rreq message, if its hop count is larger than the hop count of . otherwise, it will compute the path cost based on the hop count and energy cost in the rreq message according to (5) and generate the rrep message including the path cost and transmit it to along the reverse path."
"in the network initialization phase, the mesh routers establish the path to the gateway using proactive routing protocol. hello messages are used to establish, maintain, and update neighbor set. nodes broadcast initialization messages with the preset transmission power based on the neighbor distance, which include the identification number and residual energy of nodes. if the broadcasting node is the mesh router, the hello message contains its hop count to the gateway in addition. every node receiving this setup message will determine its distance to the transmitting node by the received signal strength and extract the information from the message to establish its neighbor set. the mesh clients join the mesh router with the least hop count to the gateway from its neighbor set and record the hop count of the router."
"international journal of distributed sensor networks in order to further balance the energy consumption, we have proposed the avoiding strategy for low-energy nodes to decrease the probability of these nodes to relay data."
"when the intermediate node forwards rreq messages, it should examine its residual energy firstly. by comparing the residual energy with the threshold, this strategy will determine when to broadcast rreq messages after a period of time, which is defined as follows:"
"in this paper, we use the standard deviation of the residual energy in all mesh clients to measure the unbalanced degree of residual energy. the energy consumption of mesh clients is not balanced and the energy of nodes is easily depleted when the value of unbalanced degree of remaining energy of nodes becomes large. in figure 6, the standard deviation of the residual energy of mesh clients in eor-hwmn is lower than that of pntara and aodv, which means that eor-hwmn has better performance on energy balance thanks to the new designed energy cost criterion and avoiding strategy for low-energy nodes. at the beginning of the simulation, the residual energy of mesh clients is the same and the standard deviation of residual energy is zero. along with the data transmission, different mesh clients have different energy consumption, and unbalanced degree of residual energy of mesh clients increases. but the growth of eor-hwmn is slower than pntrar and aodv, and the value of the standard deviation is always lower than pntara and aodv, which shows significant effects on energy balance. in pntara, the residual energy of mesh clients is used to calculate the path cost, which is divided into three levels by setting the upper and lower threshold. but the residual energy in the same level may have many differences, which will affect the result of energy balance. since the traffic of cbr randomly starts from 0 to 340 s and lasts 60 seconds, the traffic and the energy consumption of mesh clients decrease from 350 s, and the unbalanced degree of residual energy tends to be stable."
"where elec denotes the energy/bit dissipated by the transmitter electronics. amp denotes the energy consumed in the transmission amplifier and represents the path loss exponent. the value of is 2 for space channel model and 4 for multipath fading channel model. when the mesh client receives an -bit packet, the energy consumed is"
"in some scenarios of mine hybrid wireless mesh networks, for example, network-edge state and backbone-recovery state, the mesh clients will consume extra energy to receive, process, and transmit relayed packets. but the mesh client is energyconstrained, so it is important to design energy optimized routing algorithm to optimize the energy consumption of mesh clients."
"the two methods showed a linear association, with a strong positive correlation in the gap measurements (fig. 5) . the data points were clustered around the 45-degree line, with little variation. the 95% limits of agreement range between the gaps measured by the rt and the cart were 3.84% and 7.08%, respectively, of the mean for the marginal and occlusal gap measurements (fig. 6) . the relative deviations were smaller in the marginal region than in the occlusal region. table 1 and table 2 show the agreement statistics for the marginal and occlusal gap measurements. the ccc estimates and one-sided lower 95% confidence limits were greater than the allowance level for both regions. a value exceeding 0.91 was considered to represent good agreement. the accuracy and precision estimates for the marginal gap were 0.9999 and 0.9838, respectively, while these values for the occlusal gap were 0.9805 and 0.9726, respectively. in both these regions, the accuracy estimates were greater than the precision estimates. the lower 95% confidence limits showed similar tendencies to these estimates."
"if a mesh client is not under the coverage of any mesh router, it could not access the backbone transmission network directly. it needs to start the requestreply route discovery mechanism and form a multihop ad hoc network through adjacent mesh clients. then, it will access the backbone transmission network via a mesh client which has already established a wireless connection to the mesh router."
"based on the features of networking and routing of mesh clients in hybrid wmns, the architecture of hybrid wmns used for underground mine was proposed in this paper. we use mesh clients to participate in networking to improve the connectivity and reliability of wmns, which will provide a new method for constructing the mine broadband wireless networks. but the client nodes have limited energy. so, in this paper, comprehensively considering the efficiency and balance of energy consumption of mesh clients for data transmission, a new energy cost criterion for mesh clients is designed. energy optimized routing algorithm for hybrid wireless mesh networks (eor-hwmn) in coal mine is proposed on account of different network states, which will effectively extend the network lifetime and rapidly rebuild the route when mesh routers in the backbone network are failed."
"firstly, it will check the network state of rreq message. if it is network-edge state, it indicates that the upstream mesh client is at the edge of the wireless mesh network. so, the node which received the rreq message may be only mesh client. this mesh client will check its own state. if it is in ap covering state, it will perform action 1. otherwise, if it is network-edge state or backbone-recovery state, it will perform action 2."
"a prepared premolar with a 1.0-mm deep chamfer margin and total convergence angle of 6 degrees 25 was designed in dedicated software (catia v5 r18; dassault systemes, suresnes cedex, france). the die design contained the base part, with notches. the virtual die was imported into cad software (ceramill match2; amann girrbach, koblach, austria) to design a crown. the cementation space of the crown was set at 50 µm, starting 1.0 mm above the margin. 26 thereafter, the designs were processed to the metal die and crown by using a millable cobalt-chromium block (ceramill sintron; amann girrbach, koblach, austria) and a 5-axis milling machine (ceramill motion 2; amann girrbach, koblach, austria) (fig. 1) ."
"this section provides an example to clarify the mechanism of the energy optimized routing algorithm. in figure 4, mesh client has data to be sent to the gateway. but it has no route, so it will start the request-reply route discovery mechanism and broadcast rreq messages. when rreq passes through a hop, the node will accumulate the energy cost and hop count. when rreq has arrived at the mesh router from different paths, which has the route to the gateway, it will calculate the path cost. rreq containing path cost will be routed back along the reverse path to mesh client, and the route with the least path cost will be selected. for each alternative route, the energy consumption for data transmission of each route can be obtained from accumulating the energy consumption of wireless link along the path (the third column of table 1 ), which is shown in the second column of table 2 . based on the residual energy level of transmitting node (the second column of table 1 ) and the energy consumption of wireless link, each node on the path calculates the energy cost of each link, accumulating the energy cost of each path (the fourth column of table 2 ) at router . the path cost (the fifth column of table 2 ) is calculated based on the energy cost and hop count of the path according to (5), and the route 2 with the least path cost will be selected. although the energy consumption for data transmission of route 2 is the same with route 5, the energy cost of route 2 is less than route 5, because the node on route 2 is mesh router, which has no energy limitation. route 1 has the least energy consumption for data transmission along the path, but the node has less residual energy (30%), so the path cost is more than route 2."
"we compare the performance of eor-hwmn algorithm with power-and node-type-aware routing algorithm (pntara) [cit] and ad-hoc on-demand distance vector routing (aodv) [cit] on energy efficiency and energy balance, such as average residual energy of mesh clients, unbalanced degree of residual energy of mesh clients and the number of nodes with energy depletion, and quality of service (qos) metrics including packet delivery ration and average end-toend delay."
"from figure 7, we can see that the mesh client with energy exhaustion first appears in aodv and the number of energy depleted nodes rapidly increases over the simulation time, because routing decision does not include the energy factor of mesh client. the nodes on the shortest path have heavy data forwarding task and the energy of mesh clients is consumed rapidly. the mesh client runs out of energy at 300 s in pntara and at 320 s in eor-hwmn. the time in which the mesh client exhausted energy and the number of mesh clients with energy depletion in eor-hwmn are later or less than in pntara and aodv, which means that eor-hwmn has consumed energy more balanced, prevented the mesh clients exhausted energy prematurely, and achieved the efficient and balanced energy consumption among mesh clients. in wmns used in the underground mine, if there are nodes with energy exhausted, there will be lost monitoring on some sites. if this node is used as intermediate node for routing, it would rebuild the route and cause packet loss and more data transmission delay. therefore, eor-hwmn is more suitable for safety monitoring and emergency rescue in underground mine, which requires higher reliability."
"our generic data flow framework concept is comprised of several loosely coupled components aiming to provide endusers with a coherent environment for handling the main scientific activities defined in a human workflow (figure 1 ). the central underlying component is the 'data workspace' (see section vii) which offers an integrated view on different data sources ranging from binary objects stored in file systems to data stored in relational databases or data warehouses. based on these workspaces, added-value services and portals can be built which are wrapped in standardized sensor descriptions (section iii)."
"the total energy consumption of the systems that are presented in this paper is strongly influenced by the consumption of the imo. following the steps that are described in subsection 5.1, figures 12, 13, 14, and 15 present the first outcome from the experimental evaluation. figures 12 and 13 show the power breakdowns that are related with the hbd algorithm, whereas figures 14 and 15 show the power breakdowns that are related with the aes algorithm. in these figures, the components of the processor core are grouped. apart from seeing how the power distribution changes from a design based on a general-purpose processor to an asip design, these figures demonstrate that the total energy consumption of these systems is strongly influenced by the consumption of the imo."
"port to both near real-time data ansfer. given that our research operates in arctic and antarctic ation neumayer iii is located in me data using satellite commuve. therefore, we are currently sensors only at 10-min intervals. here, temperature is defined as sensor ou vocabulary p07 [cit] and online resources and calibrat being archived in our publications repository using a identifier. figure 3 shows the simple data model for time data in a relational postgresql database [cit] extension for geographic analysis purposes. individual data points are identif identifiers from platform over device, sensor well as the timestamp of measurement universal time (utc) [cit] . given that th near-real time data increases rapidly and f required, the data table has been divided in pa quarter in order to be able to conduct operat or averages for graphical overviews."
"in this paper, an experimental framework is developed in order to evaluate the sclb architecture and the banked central loop buffer (bclb) architecture from energy consumption point of view."
"the 'data processing' and 'data analy mainly subject to discipline-specific scientif produce a 'data publication'. under this activity, individual datasets, ready-to-use d respective peer-reviewed publications are com as standard scientific output."
"in addition to protein-based molecules, electrolytes can also be analyzed using pads. in a recent study, quantification of tear electrolytes was performed because they can be indicative of various eye conditions [cit] . a device was designed and fabricated to contain channels with a rapid wicking time (3 min). fluorescent sensors, diaza-15-crown-5 and diaza-18-crown-6, were utilized for selective sensing na + and k + ions. a smartphone application was used to detect the emitted fluorescence and provided a concentration value for the electrolytes."
"analysing table 8, it is possible to see that also in these architectures, there is a decrease in the dynamic power of these systems in relation with the baseline architectures. however, we can see that these architectures sometimes do not offer as good energy savings as the sclb architectures offer, because the system suffers an increase in both dynamic and leakage power consumption with the introduction of these loop buffer architectures. firstly, in the dynamic power consumption, the loop buffer controller of the bclb architecture has higher complexity than in the sclb architecture. secondly, in the leakage power consumption, apart from the higher complexity of the loop buffer controller, there is more loop buffer memories. in these architectures, the importance of the loop buffer controller is increased in the imo, which now accounts for 10% of the power consumption of the imo in the aes algorithm when it is running on the general-purpose, and for 32% in the hdb algorithm running on the processor that is optimised for this algorithm. using the same information and methodology as in the analysis of the sclb architectures, we can analyse if our configurations for the bclb architectures are power efficient."
"a workflow for data acquisition from shipborne devices along with ingestion procedures for the raw data into institutional archives has been well-established at awi for many years [cit] . however, an increasing number and complexity of research platforms and respective devices and sensors along with heterogeneous project-driven requirements towards satellite communication, sensor monitoring, quality assessment and control, processing, analysis, and visualization has recently lead us to build a generic and cost-effective framework. this framework, hereafter named o2a, enables the seamless flow of sensor observation to archives and compliance with ogc standards [cit], assuring interoperability in international context."
"embedded systems have different characteristics compared with general-purpose systems. on the one hand, embedded systems combine software and hardware to run a specific and fixed set of applications. however, they differ greatly in their characteristics, because they demand different hardware architectures ranging from multimedia consumer devices to industry control systems. on the other hand, unlike general-purpose systems, embedded systems have restricted resources and a low-energy budget. in addition to these restrictions, embedded systems often have to provide high computation capability, meet real-time constraints, and satisfy varied, tight, and time conflicting constraints in order to make themselves reliable and predictable."
"1. the ecg signal is analysed within a window of 3 seconds, where the cwt is calculated over this interval and a mask is applied to remove edge components."
"human tear contains different molecules ranging from enzymes, proteins, and lipids to electrolytes. therefore, tear offers immense potential for detection of diseases. in one study, the amount of lactoferrin, which is a glycoprotein in human tear, was detected and quantified (as low as 0.30 mg/ml) from tear samples using an antibody-free paper-based microfluidic device [cit] . the lightweight, portable, and inexpensive µpad provided rapid and accurate results. when lactoferrin formed a complex with terbium chloride hexahydrate, the resulting compound emitted a fluorescence, which was then quantified to determine the concentration of the lactoferrin present in the tear sample. the decreased levels of lactoferrin can possibly indicate presence of diseases, therefore, monitoring the concentration of lactoferrin is a clinical need."
"given that each data type is associated with an individual quality control and post-processing technique (e.g., algorithms related to illumination and distortion compensations for seafloor images), we additionally plan an extensive algorithm harmonization effort within the fram project. furthermore, we anticipate the use of web processing services (wps) [cit] to bundle the various existing algorithms in a coherent fashion and thus improve their long-term usability in the future."
978-1-4799-8736-8/15/$31.00 ©2015 ieee the second activity is named 'data tra the data is ready for proper use via our 'data section iv). two distinct scenarios chara transfer' activity:
"electrochemical sensors consist of three electrodes: a working electrode, a reference electrode, and a counter electrode [5, 35, [cit] . the sample is detected when the working electrode and the counter electrode makes a connection with the electrolytic solution to provide current to the sample on the working electrode [cit] . these electrodes can easily be screen printed on chromatography paper using carbon ink and silver/silver chloride (ag/agcl) ink [cit] . recent developments utilize graphite pencils as an alternative source for fabrication of electrodes [cit] . high concentrations of metals in the body, such as lead and cadmium, can lead to tumor formation, therefore, their detection is critical for evaluating different health conditions. such heavy metal concentrations can be accurately detected using electrochemical approaches. this technique has been used for detection of cancer biomarkers, for which tumor cells were tagged with gold nanoparticles [cit] ."
"as the input registers of the mul unit can be loaded directly, a new modification can be performed. the parallel load and mul instruction are combined, by adding another stage in the pipeline and creating a custom instruction that integrates both instructions. the mul instruction is then executed in the second stage of the pipeline, while the parallel load instruction is executed in the first stage of the pipeline. after this last modification, the mul operation that is included in the main critical loop of this algorithm is performed using only one assembly instruction."
the processor that is optimised for the aes algorithm is based also on the processor architecture that is presented in subsection 4.1. this subsection presents the modifications and optimisations that are performed in order to build this optimised processor.
"cellular components of blood interfere with the analysis in diagnostic tests making it essential to separate the plasma portion from the rest of the blood (figure 4) . the most familiar techniques of blood separation include agglutination or centrifugation. to improve these techniques, a paper-based microfluidic device was fabricated to separate plasma from a blood sample and evaluated the separation efficiency with a colorimetric assay [cit] ."
"this section describes all the components of the system that form the experimental framework. on the one hand, subsections 4.1, 4.2, and 4.3 describe the processor architectures that are used in this paper. on the other hand, subsection 4.4 presents the rest of the components that form the experimental framework and explains how the experimental framework is built based on a platform that can contain any processor."
"from the deep analysis that has to be performed to design the application-specific instruction-set processor (asip) for the hbd algorithm, a loop is pointed out as the performance bottleneck in this specific algorithm. this loop performs the convolution operation, which is the core of the cwt. a signed multiplication, whose result is accumulated in a temporally variable, is performed inside of this critical loop. the execution of this instruction is 72% of the execution time of the algorithm according to profiling information. therefore, in order to improve the performance, the mul unit is modified to multiply two signed integers and accumulate, without shifting, the result of the multiplication. this optimisation saves energy and at the same time reduces both the complexity of the mul unit and the execution time of the application."
"of scientific activities in our nd that that the solutions to be the hands of scientists who are x software systems, we have pecific sensorml profile optiof our scientists. in this profile, lated attributes, we are assuring lineage, and data governance for re-use purposes. figure 2 nd solution using the thermoe. because the use of common prerequisite towards consistency e adopted the terms describing eters measured provided by the 0 [cit] which are widely used in e project seadatanet [cit] ."
"open standards are being used in the two interface layers between the pangaea components listed above. on the ingest side, data sources like shipborne sensors may readily be fed into pangaea. from the retrieval perspective, other systems, publishers, and portals can easily harvest pangaea using oai-pmh [cit] 9 metadata format [cit] and further web services."
"the experimental platform is automatically generated for any of the processors described in subsections 4.1, 4.2, and 4.3. the experimental platform is composed of a dmh, an imo, an i/o interface, and a processor that is used as core of the platform. on the one hand, the program memory and the data memory are sram memories designed by virage logic corporation tools [cit] . on the other hand, the i/o interface that provides the capability to receive and send data in real-time is connected with the i/o interface that is described in subsection 4.1."
"in a similar way as the mul operation, another critical loop is optimised by combining load, select, and equal instructions in order to be executed in parallel. this instruction is created adding the functionality of the equal and select instructions, and combining both of them with a normal load operation. the functional unit alu 2 is created for this specific operation."
"analysing this algorithm, the critical functions are identified and optimised in order to improve performance in terms of clock cycles and memory accesses. custom techniques like source code transformations (e.g., function combination, loop unrolling) and mapping optimisations (e.g., use of look-up tables, elimination of divisions and multiplications, instruction set extensions) are applied to generate a more efficient code."
"the transition states s1, s3, and s5 are necessary in order to give the control of the instruction supply from the program memory to the loop buffer architecture and vice-versa. the transition between s4 and s1 is necessary because the body size of a loop can change in real-time (i.e., in a loop body with if-statements or function calls). in order to check in real-time whether the loop body size changes or not, a 1-bit tag is used. this tag is associated with each address that is stored in the loop buffer. the loop buffer controller checks this tag to know if the address is already stored in the loop buffer or not. figure 10 shows how the bclb architecture is composed of different loop buffer memories. in a bclb architecture, every memory is connected to the processor architecture and the program memory through multiplexers. the loop buffer controller, based on the loop body size of the loop that is on execution, decides which of the available loop buffer memories is connected directly with the program memory and the processor. the logic circuit that decides if the loop buffer architecture is activated is the same as the one used in the sclb architecture. in order to make all the decisions that are described previously, the complexity of the state-machine is incremented. however, figure 10 shows that this modification allows the design of the loop buffer architecture to be scalable."
"microfluidic paper-based analytical devices reduce high costs and allow for rapid diagnosis of diseases in resource-deprived settings. these devices are alternatives to standard laboratory tests and have been extensively studied due to their sensitivity and reliability. disease diagnostics can be studied using various biological fluids such as blood, urine, sweat, and tears in paper-based microanalytical devices (µpads) [cit] ."
"in addition to the applications of paper-based biosensors for detection and diagnosis of biomarkers related to diabetes, and liver and kidney disorders, the inexpensive and portable poc devices can also serve as a resourceful tool in other areas such as monitoring food quality or in agricultural applications. paper-based sensors also make it possible to monitor toxins and heavy metals in environmental samples, such as in contaminated water sources. paper microfluidics has not only made simple diagnostic devices readily available globally, but also became a key enabling technology for biomedical research."
"actual permanent archive. raw cessed data for long-term preere. in contrast to areas 's', 'a', restricted and read-only in area data flow framework approach, 'dbms' and 's' areas over 'a' he access-restricted 'c' area."
"chemiluminescence involves the detection of light generated from a chemical reaction, when reactive intermediate molecules generate an emission upon returning to ground state from their excited state. chemiluminescent detection of uric acid was performed via an enzymatic reaction that produced hydrogen peroxide as a by-product while decomposing the substrate [cit] . hydrogen peroxide then reacted with rhodamine derivatives, which were added to the microfluidic channels of the sensor. luminescence was observed after injecting luminol and iodophenol using a computerized rfl-200 detector (xi'an yima opto-electrical technology co., ltd., xi'an, shaanxi, china), producing a signal with the peak height representing its concentration. chemiluminescence is considered one of the simplest detection techniques because it does not rely heavily on sophisticated instruments. in addition, chemiluminescence is one of the most precise and sensitive detection technique [cit] . however, it requires a high concentration of enzymes, which might block the porous structure of paper and can possibly reduce the overall efficiency of detection."
"because biomedical wsns have ultra-low power requirements, the proposed algorithm supports only 128-bit key. in addition, only the encryption mode of the aes algorithm is supported. however, with a very small change in the design, both encryption and decryption can be supported. in this algorithm, the input data frame is fixed to 1,460 bytes of information, whereas the output is a data packet where the information is encrypted."
"the use of origami (folding of paper) [cit] and kirigami (cutting of paper) [cit] techniques during the fabrication of microfluidic devices has given researchers new opportunities for fabricating their devices ( figure 2 ). the principles of origami were used to create a unique device in which two zones are separated by a crease [cit] . one zone is the detection zone and the other is the enzyme immobilization zone. in this work, the researchers designed a paper-based analytical device to electrochemically detect glucose using an origami-inspired device [cit] ."
"similarly, a portable biosensor was fabricated on paper to measure two enzymatic markers of the liver (aspartate aminotransferase (ast) and alkaline phosphatase (alp)) and total serum proteins ( figure 4) [cit] . blood contains the common markers of liver function, which are absent in urine, making blood a suitable medium for examining the levels of analytes of interest. a healthy liver will yield low concentrations of ast (5-40 u/l) and alp (30-120 u/l), whereas a damaged liver will release higher concentrations into the bloodstream, which can be monitored using the paper-based device. the onset of liver disease also decreases the total serum protein in the blood, which normally ranges from 60-83 g/l. a multiplexed vertical-flow device was fabricated via wax printing. the microfluidic device enabled the separation of plasma from the red blood cells (rbcs). this biosensor demonstrated formation of a red color for ast, green for serum protein, and blue for alp. the concentrations in the samples were found as −4000 u/l for ast, ∼26,000 u/l for alp, and ∼50 g/l for protein indicating liver dysfunction."
"the load operations that are related with the previous mul operation are combined in a customised instruction in order to be executed in parallel. however, in the general-purpose processor, it is only possible to load and store data from the same memory once per stage of the pipeline. to solve this bottleneck, the main data memory is split in two identical data memories: data memory (dm) and constant memory (cm). in order to access two memories in parallel, another address generator (ag2) is created such that the load and store operations from the dm and cm can be performed at the same stage of the pipeline."
"the framework presented in this paper is focused on data that originates from heterogeneous devices and sensors mounted in various types of platforms. each platform offers an automated and specific 'data acquisition' system which stores measurements on the platform itself, in proprietary real-time databases or high-volume data in file systems (see figure 1 ). high-volume data comes in a variety of specialized data formats (e.g., seg y format for seismic). the data is commonly stored in one second intervals or even in higher frequency as typical for data from research aircraft."
2. the square of the modulus maxima of the cwt is taken in order to emphasise the differences between coefficients. values above a chosen threshold are selected as possible r-peaks.
"the configurations of the sclb and bclb architectures that are analysed in this paper are based on the loop profiling presented in tables 1, 2, 3, and 4. on the one hand, the selection of the sclb configurations is based on the small size of the loops that have bigger percentage of execution time. with this strategy, we assume that these configurations are the most energy efficient. this assumption is based on the fact that these configurations provide the highest energy savings among all the possible configurations. these major energy savings help to reduce the penalty related with the introduction of the loop buffer architecture in the system. on the other hand, the selection of the bclb configurations is based on the strategy of taking the maximum loop body size of the application, and chop it by the granularity of the smaller loop body size that the applications contains. this strategy is used in these architectures, because the exact energy consumption of the extra logic that has to be added in the loop buffer controller is unknown. table 5 presents the initial configurations that are evaluated."
"in this paper, the loop buffer concept was applied in two real-life embedded applications that are widely used in biomedical wsns. the loop buffer architectural organisations that were analysed in this paper were the single central loop buffer and the banked central loop buffer architecture. an analysis of the experimental applications that were used in this paper was performed to show which type of loop buffer scheme was more suitable for applications with certain behaviour. to evaluate the power impact, a post-layout simulation was used to have an accurate estimation of parasitics and switching activity. the evaluation was performed using tsmc 90 nm low power library and commercial memories. from the experimental evaluation, gate-level simulations demonstrated that a trade-off exists between the complexity of the loop buffer architecture and the power benefits of utilising it. this confirms our results, showing that the central banked loop buffer does not always bring benefits. therefore, the use of loop buffer architectures in order to optimise the imo from the energy efficiency point of view should be evaluated carefully. two factors have to be taken into account in order to implement an energy efficient imo based on a loop buffer architecture: (1) the percentage of the execution time of the application that is related with the execution of the loops included in the application, and (2) the distribution of the execution time percentage, which is related with the execution of the loops, over each one of the loops that forms the application."
"the near-real time database is accessed java web-application that provides rest-b requesting data of selected sensors and param data formats like csv or json. delayed-mode data is trans or tapes being imported in ou figure 5 ). in order to synchr delayed-mode data, we are usin combination with the geograph data point. quality checks are on tracklines from our research serve as master positions for these are stored in pangae object identifier (doi). for an e v. monitoring our web-based monitorin dashboard-oriented approach archived in the near real-tim environment offers three main f"
"for the aes algorithm running on the processor that is optimised for this algorithm, we have to analyse only the loop buffer configurations that has 32 instruction words, because all the loops can fit in a loop buffer of 32 instructions words (see table 4 ). however, from figure 23, we can see that only loop buffers of 32 and 64 instruction words bring us energy savings. therefore, we will analyse only the loop buffer configurations that has 32 instructions words. figure 27 shows the configuration of two loop buffers, where one of them has a fixed size of 32 words. from this figure, we can see that the best configuration is a loop buffer of 8 words together with the loop buffer of 32 words. if we compare the energy savings from the bclb and the sclb architecture, we can see that for this specific scenario it is also better to have the sclb architecture. based on all the previous results and discussions, we can conclude that the use of loop buffer architectures in order to optimise the imo from the energy efficiency point of view should be evaluated carefully. in the case studies that are presented in this paper, the sclb architecture is normally more energy efficient than the bclb architecture, as can be seen in figure 28 . however, the sclb architecture is not always more energy efficient than the bclb architecture. the higher energy efficiency of the sclb architecture is because the whole execution time of all benchmarks is concentrated in a few loops with similar loop body size. if we can find a benchmark where this percentage is shared between loops with different loop body sizes, the bclb architecture will then bring us more energy efficiency than the sclb architecture. therefore, the two factors to take in account in order to implement an energy efficient imo based on a loop buffer architecture are:"
"the device was fabricated by a wax patterning method and used different blood separation membranes. the device performed remarkably well compared to existing paper-based blood separation devices due to incorporation of two different types of paper that provided a single step separation. this microfluidic device was capable of separating the plasma from a single drop of blood (15 l-22 l) which contained the normal human hematocrit content (24%-55%). albumin from the separated plasma was detected using a bromocresol green (bcg) assay. plasma was directed to flow in the detection zone where bcg was added, and a color change was observed within 2 min from clear to deep blue. images of the detection zones were captured using a digital camera and the color intensity was analyzed using adobe photoshop cs2 in the \"gray scale\" mode for quantification of the results."
"the experimental framework uses an i/o interface to provide the capability of receiving and sending data in real-time. this interface is implemented in the processor architecture by fifos that are directly connected to the register file. the data memory that is required by this processor architecture in order to be a general-purpose processor is a memory with a capacity of 16k words/16 bits, whereas the required program memory is a memory with a capacity of 2 k words/16 bits. figure 5 presents the data-path of this processor, where the main blocks are data memory (dm), register file (r), arithmetic logic unit (alu), shift unit (sh), multiplication unit (mul), and address generation unit (ag1). the address generation unit specifies the next address as normal word instruction in the case of word label, as negative offsets to the stack pointer register in the case of the nint9 label, and as relative offset of short jump instructions in the case of the sbyte label."
"there are different types of paper employed in paper-based sensors depending on the fabrication method and the application of the sensor. the most extensively used material is whatman brand chromatography paper due to its superior wicking ability [cit] . this particular type of paper has medium retention and flow rate owing to its thickness (180 μm) and pore size (11 μm) . other types of paper such as the whatman filter paper no. 4, was used due to its larger pore size of 20-25 μm and higher retention rate [cit] . more recently, filter paper has been used in paper-based sensors [cit], which is also manufactured by whatman (maidstone, united kingdom). this type of paper has been used for its relatively uniform thickness and wicking properties as well as superior adsorption and retention of reagents compared to the similar types of paper [cit] ."
"the general-purpose processor architecture is designed using the tools from target compiler technologies [cit] . the instruction-set architecture (isa) of this processor is composed of integer arithmetic, bitwise logical, compare, shift, control, and indirect addressing i/o instructions. apart from support for interrupts and on-chip debugging, this processor supports zero-overhead looping control hardware, which allows fast looping over a block of instructions. once the loop is set using a special instruction, additional instructions are not needed in order to control the loop, because the loop is executed a pre-specified number of iterations (known at compile time). this loop buffer implementation supports branches, and in cases where the compiler cannot derive the loop count, it is possible to inform the compiler through source code annotations that the corresponding loop will be executed at least n times, and at most m times, such that no initial test is needed to check whether the loop has to be skipped. the special instruction that controls the loops introduces only one cycle delay. the status of this dedicated hardware is stored in the following set of special registers:"
"the fabrication method that is used to produce the biosensors can impact the simplicity and their applications. there are numerous approaches available that involve chemical modification or physical deposition onto the paper, both of which alter the material characteristics of the cellulose matrix. the approaches that will be covered in this review are wax printing, photolithography, inkjet printing, laser cutting, polydimethyl-siloxane (pdms), hot embossing, hydrophobic silanization, and the use of origami-and kirigami-based approaches. several advantages and disadvantages of each procedure are listed in table 1 ."
"the development of wax-printed paper-based diagnostic platforms has become increasingly popular to those aiming to grow cells, test drugs, or regenerate tissues [cit] . for example, a paper-based microfluidic cell culture model was designed to observe cell proliferation in a three-dimensional (3d) microenvironment, in comparison to the standard 2d cell culture in a petri dish [cit] ."
"paper-based sensors represent an emerging approach with applications ranging from disease diagnostics, and detection of environmental agents to monitoring forensic samples. these inexpensive and portable devices have the ability to detect analytes with high sensitivity up to fm. due to their low fabrication cost, they are not only useful in fully equipped laboratories but also in resource limited settings and austere environments. the paper-based devices present various advantages because they (i) are simple and user-friendly; (ii) are equipment-free (limiting energy expenses because pumps or external apparatus are not required for driving the fluid flow); (iii) are portable; (iv) are easy to dispose of; (v) require only small volumes of analytes (in the microliter range); and (vi) have the ability to detect multiple analytes at the same time (multiplexed analysis). furthermore, biosensing approaches presented in this review, such as the color change, provides a simple and practical detection, which eliminates the need of highly-trained medical staff to execute and evaluate the results of the diagnostic test."
"the fram project entails not only near real-time data but also in-situ observations delivered yearly in delayed mode and shipborne data delivered after the end of individual expeditions. to date, the raw data transfer process includes a semi-automatic file structure and a simple quality check. we are currently in the process of designing a flexible solution for extended validation checks and automated quality assessments including the assignment of quality flags as well as dynamic aggregation of data files within a geographic region or time ranges. moreover, we will be building automated solutions for extracting metadata from data files and annotating these accordingly based on sensor descriptions. by adopting o2a, a seamless transfer of the data (including aggregated data) into the long-term archive pangaea including the minting of dois will be assured without additional efforts for fram data scientists or pangaea curators."
"detection of nitrite, ammonia, and heavy metals can be performed through colorimetric and chemical detection [cit] . a low-cost paper-based-capacitive sensor was developed for identification of chemicals according to their dielectric properties [cit] . the capacitive sensor was built by stacking filter paper and aluminum. the aluminum acts as a conductor, in which liquid current flows through a multimeter, where its capacitance can be quantified. by measuring capacitance, the chemical can be identified based on its dielectric constant [cit] ."
"however, it is important to note that there are still limitations of paper-based sensors that must be addressed before their commercialization. paper sensors can pose an issue regarding the accuracy and sensitivity of analytes [cit] . this drawback must be removed before these sensors can be relied upon for an accurate disease diagnosis. in addition, the efficiency of the production of paper sensors must be improved before mass manufacturing of these devices. currently, some of these devices require complicated manufacturing procedures [cit] that must be simplified before commercialization. similarly, the cost of these devices must be lowered in order to truly make an impact in resource-depleted environments."
"the generic workflow framework presented here is currently in its pilot phase and is being tested in the context of the large-scale multi-disciplinary \"frontiers of arctic monitoring\" (fram) [cit] . in this project, data collected from a wide range of platform types (e.g., research vessels and aircraft, sea ice tethered buoys, moorings, floats, gliders, autonomous underwater vehicles, ocean-based stations, sea floor crawlers, drones, etc.) is being automatically captured from an early stage on, adopting the data flow framework described above. so all scientific devices and respective sensors are being optimally described by fram engineers and data scientists using the sensor description solutions mentioned in section iii. this includes not only sensor characteristics but also links to online resources like manufacturer descriptions, user manuals, and sensor calibration documents archived in our institutional publication repository (see example for these in figure 2 ). in addition, all events associated with each sensor (e.g., maintenance, calibration, device recovery, etc.) are being individually recorded so as to increase the re-usability of the data in the future."
"because the standard scientific work acquisition to data publishing typically takes crucial that, during this long chain of activ information associated with the sensor centrally archived and transparent to all for usability. also for example, by knowing r istics for sensors already deployed (e.g., dep sors fixed on a mooring wire), the design an processes of future observational experiments in order to provide a me characteristics and related onli the ogc standard sensorm mechanism for describing se sensor web services specificat are able to describe not only th sensor (physical characteris platform, accuracy, etc.) but als calibration, device deployme resources relevant for the sc facturer factsheets, user manu calibration, etc.). these resour tional publication repository identifier (handle) is being mint aiming at the support o institution and keeping in min built must become effective in usually not fond of complex developed a compact awi-sp mized for the practical needs o besides the standard sensor-rel that the needed provenance, information is also archived depicts our web-based fronte salinograph sbe21 as exampl vocabularies is an important p and interoperability, we have platforms, devices, and parame nerc vocabulary server v2.0 the pan-european infrastructure another relevant aspect wit profile is the use of range va sensor outputs (e.g., temperatu fall in the range -5 to +35 degr upper and lower threshold val various sensor outputs, we wil dashboard-oriented monitoring and notifications are currently b administrators."
"the algorithm that is used in this paper is an optimised c-language version for biomedical wsns. it does not require pre-filtering and is robust against interfering signals under ambulatory monitoring conditions. the algorithm works with an input frame of 3 seconds, which includes 2 overlaps of 0.5 seconds between consecutive frames in order to not lose data between frames. figure 3 shows the flowchart of this algorithm. the algorithm performs the following steps to process an input data frame:"
"the core components of our gis infrastru arcgis for server [cit] and postgresql da the spatial database engine (sde). data u sql can be accomplished by using the application. the gis server is accompanied b adaptor, delivering proxy and load balanci as a result, our gis infrastructure can be e adding additional servers in order to m requests. moreover, a generic web-based gi leaflet [cit] has been developed providin environment with time sliders, customizable as further information associated with gis pr besides giving assistance to awi s developing their own gis data products, w creating base maps by integrating existing d awi data (e.g., gebco08 [cit] ). these base being used in individual gis projects but a like expedition [cit] ."
"embedded systems constitute the digital domain of wireless sensor nodes (wsns). they are widely deployed in several types of systems ranging from industrial monitoring to medical applications. particularly, for the biomedical domain, the information that is processed and transmitted is confidential or requires authentication in the majority of the cases. due to this fact, it is not unusual that two applications like a heart beat detection (hbd) algorithm and a cryptographic algorithm such as advanced encryption standard (aes) algorithm can be found in biomedical wsns. these two real-life embedded applications are used in this paper as case studies to evaluate the energy reduction achieved by the use of imos that are based on the loop buffer concept."
"the dashboards shown in javascript with html5 using jquery [cit], leaflet [cit] for m the data itself. by using this lig able to build highly custom dashboards as widgets which c solutions and content managem sferred manually via hard disks ur workspace on land (compare ronize near real-time data and ng consistent utc timestamps in hic position for each transferred being systematically performed vessels and aircrafts which then geo-referenced measurements. ea along with a citable digital example see [cit] ."
"different types of common paper have also been successfully used in the production of paper-based sensors. conventional printing paper was selected for use in a wearable device, intended to be foldable and flexible [cit] . the paper was subject to wax printing and then carbon black ink was spread along the surface of the paper. in addition, paper towel has been utilized as a surface for printing of carbon black-modified electrodes [cit] . paper towel has also been used as wicking layer for a biosensor produced from filter paper [cit] . paper towel is cheaper than filter paper and possesses a high porosity, which makes it a viable material for analysis of a wide range of analytes."
"iv. transfer of data from f as illustrated in figure 1, with data acquisition in the fiel of data to workspaces. for th activities, the \"raw data i developed in close cooperatio responsible for the various dev be seen as an individual compo from sensor to archive describe within o2a, we offer supp transfer and delayed-mode tra vessel polarstern commonly o regions and our land-based sta the antarctic, the near real-tim nication is very cost intensiv transferring data from selected echanism for capturing sensor ine resources, we have adopted ml 2.0 [cit], a standardized ensor resources related to the tions [cit] . with this standard we he details related to the specific stics, positioning within the so the related events (e.g., sensor ent and recovery) and sensor ientific workflow (e.g., manuuals, documentation on sensor rces are archived in our instituwhere a citable digital object ted for each individual item."
two real-life embedded applications that can be found in biomedical wsns are used as case studies in this paper. both applications are described in the following subsections.
"w measurement values; out-ofed in sensorml, are highlighted n of time series for sensors and y users aiming to assist in the rs and gaps, dual measurements are shown in specially relevant for moving vessels. on environment ific analysis and publication of referenced data products, we are that focuses on the publication as well as their accessibility. all n ogc compliant wms [cit] or due to performance and security i established two independent gis system standard scientific workflow illustrated in internal exchange and analysis purposes and open access publication of gis data produc administration rights are required. the in designed to allow scientists to build the independently just by using their desktop arc their gis projects in a specific internal fold shared with arcgis server."
"hydrogen sulfide is a flammable gas and can be dangerous to human health even if it is present in low concentrations in the blood [cit] . a paper-based sensor that can detect hydrogen sulfide gas at low concentrations was developed [cit] . the sensor employed filter paper which included palladium (ii) and ethylene glycol in it. palladium (ii) was found to bind to sulfide ligands efficiently, which then formed pds. a spectrophotometer was used to measure the fluorescence spectra and quantify the results."
"different techniques such as colorimetric assays, chemiluminescence, electrochemiluminescence, electrochemical, and fluorescence based techniques can be employed in paper-based sensors to detect the presence of an analyte of interest [cit] . these techniques have been extensively used since they provide highly sensitive results rapidly compared to traditional techniques such as the elisa."
"the ccm (ctr-cbc-mac), which is presented in the nist special publication 800-38c [cit], encrypts and authenticates the message and the associated data. depending on the size of the message authentication code that it produces (4, 8, or 16 bytes), three different variations of aes-ccm exist: aes-ccm-32, aes-ccm-64, and aes-ccm-128."
"in additon to amperometric devices, the presence and quantity of analytes can also be measured potentiometrically [cit] . furthermore, conductomeric sensors can measure the capability of an analyte to produce a current between electrodes. these types of electrochemical devices have been used for detection via enzymatic reactions. the conductance of a solution changes following an enzymatic reaction making it possible to correlate the amount of the analyte with conductance [cit] ."
"these membranes can be modified via wax printing that is followed by heating [cit], however wax penetration through the nitrocellulose membrane is slow when compared to that of the filter paper."
"there are different types of paper employed in paper-based sensors depending on the fabrication method and the application of the sensor. the most extensively used material is whatman brand chromatography paper due to its superior wicking ability [cit] . this particular type of paper has medium retention and flow rate owing to its thickness (180 µm) and pore size (11 µm) . other types of paper such as the whatman filter paper no. 4, was used due to its larger pore size of 20-25 µm and higher retention rate [cit] . more recently, filter paper has been used in paper-based sensors [cit], which is also manufactured by whatman (maidstone, united kingdom). this type of paper has been used for its relatively uniform thickness and wicking properties as well as superior adsorption and retention of reagents compared to the similar types of paper [cit] ."
"because different physical and/or chemical characteristics may be required for biosensors, other categories of paper have been explored depending on the target application. nitrocellulose membranes have been used due to their chemical functional groups that enable covalent immobilization of biomolecules. nitrocellulose allows for charge-charge interactions, weak hydrogen bonds, and van der waals interactions with protein-based substrates [cit] . as a result of their high protein-binding abilities, these membranes are commonly used in elisa and gold nanoparticle-based assays [cit] . these membranes have also been used because they prevent diffusion and leaching of samples through the membrane, which allows for a higher degree of retention and subsequently a longer reaction time within the sensor [cit] . nitrocellulose membranes are smooth and have a uniform pore size of 0.45 µm."
"the interface between a processor architecture and an imo is depicted in figure 8 . the interconnections of the processor architecture, the program memory, the loop buffer memory and the loop buffer controller are included in this figure. every component that forms the imo is explained in the next paragraphs. in our experimental platform, the loop buffer architecture, which is composed of the loop buffer memory and the loop buffer controller, can be configurable to be used as an sclb or bclb architecture. for simplicity, the sclb architecture is used in the next paragraphs to explain the loop buffer concept operation. in essence, the operation of the loop buffer concept is as follows. during the first iteration of the loop, the instructions are fetched from the program memory to both loop buffer architecture and processor. in this iteration, the loop buffer architecture records the instructions of the loop. once the loop is stored, for the rest of iterations, the instructions are fetched from the loop buffer architecture instead of the program memory. in the last iteration, the connection between the processor and program memory is restored, such that subsequent instructions are fetched from the program memory. during the execution of non-loop parts of the application code, instructions are fetched directly from the program memory. the loop buffer controller monitors the operation of the loop buffer architecture based on a statemachine. this state-machine is shown in figure 9 . the six states of the state-machine are: s0 initial state. s1 transition state between s0 and s2. s2 state where the loop buffer is recording the instructions that the program memory supplies to the processor. s3 transition state between s2 and s4. s4 state where the loop buffer is providing the instructions to the processor. s5 transition state between s4 and s0."
"lung cancer is one of the common causes of death in the usa [cit] . radiation therapy is a prevalent treatment used to target carcinogenic cells. hypoxia occurs when tissues are unable to get sufficient oxygen and, in some cases, would lead to the growth of malignant tumors. oxygen produces free-radical species that lead to dna damage depending on the placement of cells, in which the cells can be no more than 150-250 µm away from a capillary. therefore, it is crucial to monitor cancer cells to prevent them from proliferating after radiation therapy. a personalized cancer treatment approach was developed using pads to observe the metabolic activity of lung tumor cells in response to ionizing radiation [cit] . it was found that decreased levels of oxygen displayed a reduction of proliferation and metabolic activity of the lung tumor cells. the paper-based 3d cell culture system provided a metabolic understanding of how tumor cells would grow after radiotherapy in vivo."
"3. in order to separate the different peaks, all modulus maxima points within intervals of 0.25 seconds are analysed in turn as search intervals. in every search interval, the point with the maximum coefficient value is selected as r-peak."
". conceptual schematic of stretchable bead network. the bead and spring represent the pri and inkjet-printed stretchable interconnect with gradual strain-absorbing design, respectively. gradual design of the spring network where gradual compression/expansion absorbs mechanical stress is exactly matched with the vertically wrinkled gradual strain-absorbing design of the printed interconnects with a stress-matched amplitude."
"colorimetric assays include methods utilized to detect the presence and concentration of an analyte by evaluating the color formation or color change via i) direct imaging using a single-lens reflex (slr) camera, mobile devices, or low-cost desktop scanners in combination with software such as matlab for quantification [cit], or ii) traditional spectrophotometers by measuring the absorbance of the sample at specific wavelengths. this detection technique is the most commonly used one as it provides accurate results at a lower cost. color formation or color change can be induced by using enzymes, elisa-based immunoassays, or gold nanoparticles (figure 3 )."
"analysing table 7, it is possible to see that there is a decrease on the dynamic power of these systems in relation with the baseline architectures. this is because the majority of the instructions are fetched from a small memory instead of the large memory that forms the program memory. on the other hand, the sclb architectures have an increase in the leakage power consumption in relation with the baseline architectures, due to the introduction of the loop buffer architecture. we can see also the importance of the loop buffer controller in the imo, which accounts from the 5% of the power consumption of the imo in the system where the aes algorithm is running on the general-purpose processor, to 30% in the system where the aes algorithm is running on the processor that is optimised for this algorithm."
"according to our generic d data is being transferred from and 'b' areas to finally reach th figure 5 : schematic diagram illustra databases (dbms) and scratch or cach user working data and (b) storing proje only\" archive area (compare [cit] )."
"environmental contaminants, pesticides, heavy metals, and toxins are the major components that contribute to water pollution, and are harmful to the environment and human health [cit] . providing access to safe water depends on proper detection of such pollutants in water. although there are various methods used for water analysis including spectrophotometry and conventional elisa, assessing the quality of water with these techniques are costly, time-consuming, and require sophisticated instruments and highly-trained personnel [cit] . therefore, there is a need for simple, inexpensive, and efficient strategies for analyzing environmental sources including water. one way to determine the presence and amount of water contaminants is through the use of µpads [cit] ."
the input of this algorithm is an ecg signal from mit/bih database [cit] . the output is the positions in time-domain of the heartbeats that are included in the input frame. the testing of this optimised algorithm results in a sensitivity of 99.68% and a positive predictivity of 99.75% on the mit/bih database.
"different techniques such as colorimetric assays, chemiluminescence, electrochemiluminescence, electrochemical, and fluorescence based techniques can be employed in paper-based sensors to detect the presence of an analyte of interest [cit] . these techniques have been extensively used since they provide highly sensitive results rapidly compared to traditional techniques such as the elisa."
"ating of the technical data flow from he area (s) over logical areas (a) storing ect-related working data to (c) as \"readkflow solutions presented in this m scratch (area 's') to the archive a semi-automatic way. the vast n our institute is being archived angaea [cit] which is jointly m (center for marine environof the icsu world data system data management and archival en access data library. the three d with the information system ation system jointly acting as the is the central hub for data on, generation and description of of citable dois to datasets, orresponding publications, data any communication between d data curators is handled and ell."
the processor that is optimised for the hbd algorithm is based on the processor architecture that is presented in subsection 4.1. this subsection presents the modifications and optimisations that are performed in order to build this optimised processor.
"th regard to our awi sensorml alues associated with individual ure values measured by sbe21 rees celsius). because we insert lues for describing the range of ll be able to re-use these in the solutions (see section v). alerts being set manually by dashboard field to workspaces at land, the scientific workflow starts ld along with a seamless transfer he purpose of supporting these ingest framework\" has been n with engineers and scientists vices and sensors [cit] . rdif can onent in the data flow framework ed in this paper."
"another good example on how the distinct o2a components will assist us in developing enhanced data products for fram is the use of coloring schemes for parameters measured along tracks of mobile platforms (vessels, aircrafts, gliders, etc.). we are planning to develop a platform-specific averaging algorithm which will be optimized according to platform velocity as well as parameter range and accuracy as described in the awi-specific sensorml profile (section iii)"
"in this paper, the loop buffer concept is applied in the two real-life embedded applications described in the previous paragraph. the loop buffer architectures that are analysed in this paper are the single central loop buffer and the banked central loop buffer architecture. the contributions of this paper include:"
"for the hbd algorithm running on the general-purpose processor, figure 20 shows the power reductions that we can achieve for all the possible configurations. in the configuration of eight words, the 73% of the execution time of the application is on loops, while in the rest of the configurations this percentage is 79%. we can see that in this scenario, the best configuration is a loop buffer memory of 16 words, because the increase of use of the loop buffer memory compensates the penalty introduced by using a bigger loop buffer architecture. figure 21 shows the energy reductions we can achieve for all the possible configurations when the hbd algorithm is running on the processor that is optimised for this algorithm. in the configuration of eight words, the 2% of the execution time of the application is on loops. this percentage is 11% in the configuration of 16 and 32 words, whereas in the configuration of 64 words this percentage is 81%. we can see that in this scenario, the only configuration that brings energy savings is the loop buffer of 64 words. the percentages of the execution time of the rest of configurations do not compensate the penalty introduced by using a loop buffer architecture. for the aes algorithm running on the general-purpose processor, figure 22 shows the energy reductions we can achieve for all the possible configurations. in the configuration of eight words, the 47% of the execution time of this application is on loops; in the configuration of 16 words this percentage is 70%; in the configuration of 32 words this percentage is 75%, whereas in the configuration of 64 words this percentage is 77%. we can see that in this scenario, the best configuration is a loop buffer of 32 words, because the increase of use of the loop buffer architecture compensates the penalty introduced by using a bigger loop buffer memory. on the other hand, the small increase in the percentage of execution time from the configuration of 32 words to 64 words does not compensate the increase in leakage consumption that this last loop buffer architecture has. figure 23 shows the energy reductions we can achieve for all the possible configurations when the aes algorithm is running on the processor that is optimised for this algorithm. in the configuration of 8 words, the 5% of the execution time of the application is on loops; in the configuration of 16 words this percentage is 6%, whereas in the configuration of 32 and 64 words this percentage 90%. we can see that in this scenario, the best configuration is a loop buffer of 32 words. the percentages of execution time for the 8 and 16 words configurations do not compensate the penalty introduced by using a loop buffer architecture. also in this scenario, the small increase in the execution time percentage from the configuration of 32 words to 64 words does not compensate the increase in leakage power consumption that this last loop buffer architecture has. figure 23 . aes algorithm running on the optimised processor using different configurations for the sclb architecture."
"it is necessary to remark that, apart from the specialised instructions that are described in previous paragraphs, custom techniques like source code transformations (e.g., function combination, loop unrolling) and mapping optimisations (e.g., use of look-up tables, elimination of divisions and multiplications, instruction set extensions) are applied to generate a more efficient code. all the optimisations and modifications that are described in this subsection result in a new processor architecture shown in figure 6 . basically, an address generator (ag2) and a second alu (alu 2) are added, in addition to some pipes and ports. apart from that, the program counter (pc) is modified to handle instruction words that use 32-bit immediate values. in order to handle ecg signals sampled at 1,000 hz, the memories that are required by this processor architecture are a dm with a capacity of 8 k words/32 bits, and a cm with a capacity of 8 k words/32 bits. besides, the program memory that is required by this processor architecture is a memory with a capacity of 1 k words/20 bits. this optimised processor is an implementation that is based on the work presented in reference [cit] ."
besides the web gis solutions m lightweight device-specific visualization so developed in order to be displayed in u environments. these solutions are designe address the challenge of displaying selected moving platforms.
"using the profiling information presented in tables 1, 2, 3, and 4, and the power results obtained from the simulations of the systems presented in table 5, we can evaluate if our initial configurations for the sclb architecture are selected correctly from the energy consumption point of view."
"because different physical and/or chemical characteristics may be required for biosensors, other categories of paper have been explored depending on the target application. nitrocellulose membranes have been used due to their chemical functional groups that enable covalent immobilization of biomolecules. nitrocellulose allows for charge-charge interactions, weak hydrogen bonds, and van der waals interactions with protein-based substrates [cit] . as a result of their high protein-binding abilities, these membranes are commonly used in elisa and gold nanoparticlebased assays [cit] . these membranes have also been used because they prevent diffusion and leaching of samples through the membrane, which allows for a higher degree of retention and subsequently a longer reaction time within the sensor [cit] . nitrocellulose membranes are smooth and have a uniform pore size of 0.45 μm. these membranes can be modified via wax printing that is followed by heating [cit], however wax penetration through the nitrocellulose membrane is slow when compared to that of the filter paper."
"as a final step, the information of the average power consumption is extracted with the help of primetime [cit] . figure 11 shows the inputs and outcomes of each step described above. figure 11 . simulation methodology."
"we can see from these tables that the systems that are optimised for the experimental applications always consume less power than the general-purpose systems. therefore, the introduction of the sclb and bclb architectures does not affect this energy consumption trend."
"vii. data flow to archives -data w as illustrated in figure 5, [cit] . th system' is managed as a large cluster of re tape storages with a hierarchical storage mana a 'dispatcher' middleware component well as export data requests from portals or and information systems to the actual data sto underlying logical file systems can be divided"
"over the last two decades, the alfred wegener institute (awi) has been continuously committed to develop and sustain an infrastructure for coherent discovery, view, dissemination, and archival of scientific information in polar and marine regions [cit] . most of the data collected by scientists originates from research activities being carried out in a wide range of research platform types operated by awi: vessels, aircrafts, land-based stations, ice-based stations, moorings, floats, gliders, in-situ ocean floor stations, drones, and ocean floor crawling systems. archival and publishing in the information system pangaea [cit] along with doi assignment to individual datasets is a typical end-of-line step for most data owners."
"thanks to the data and work paper, the transfer of data from area 'c' area is performed in a majority of all data produced i at the information system pa operated by awi and marum mental sciences). as member o [cit], pangaea follows its policies and operates as an ope major components associated pangaea are:"
"this section shows the results of the experimental evaluation of the sclb and the bclb architecture. firstly, subsection 5.1 describes the methodology that is used in our energy simulations. secondly, subsection 5.2 analyses the experimental applications that are described in section 3 based on profiling information. finally, subsection 5.3 shows and discusses the results of the power simulations."
"paper-based electrochemical sensors were also utilized for multiplexed detection of glucose, lactate, and uric acid [cit] . for detection of these analytes, corresponding enzymes and electron-transfer mediators were stored in test zones to react with the analytes and produce electrical signals. the carbon electrodes were connected with screen-printed silver strips which served as contact pads for electrical interfacing with the metal clips of the potentiostat. the limit of detection (lod) for this device was found to be 0.35 mm for glucose, 1.76 mm for lactate and 0.52 mm for uric acid."
"in this paper, we describe the distinct components of our framework as well as the added value of establishing relationship metadata among the various content types. moreover, we show how our pragmatic sensor characterization effort can be re-used in the context of our sensor monitoring environment. finally, we illustrate how the distinct o2a components presented in this paper can be used to support the scientific data workflow using the \"frontiers of arctic monitoring\" project [cit] as a use case."
"for the hbd algorithm running on the general-purpose processor, we have to analyse only the loop buffer configurations of 8 instruction words, because all the loops can fit in a loop buffer of 16 instructions words (see table 1 ), and every configuration in a bclb architecture with a loop buffer of 16 instruction words is worse in power consumption than a sclb architecture of 16 instructions words. figure 24 shows the possible configurations of two loop buffers, where one of them has a fixed size of 8 words. from this figure, we can see that the best configuration is two loop buffers of 8 words. if we compare the energy savings from the bclb and the sclb architecture, we can see that for this specific scenario, it is better to have the sclb architecture. for the hbd algorithm running on the processor that is optimised for this algorithm, we have to analyse only the loop buffer configurations of 64 instruction words because any configuration without a loop buffer of this size will not bring us energy savings (see figure 21 ). figure 25 shows the configuration of two loop buffers, where one of them has a fixed size of 64 words. from this figure, we can see that the best configuration is a loop buffer of 16 words together with the loop buffer of 64 words. if we compare the energy savings from the bclb and the sclb architecture, we can see that for this specific scenario it is also better to have the sclb architecture. for the aes algorithm running on the general-purpose processor, we have to analyse all the possible configurations because the execution time of the application is spread (see table 3 ). the configuration with two loop buffers of 64 instruction words each is not analysed, because this configuration is worse in energy efficiency than the sclb architecture of 64 instructions words, due to the increase in energy consumption of the loop buffer controller. from figure 26, we can see that the best configuration is a loop buffer of 8 words together with a loop buffer of 32 words. in this case, if we compare the energy savings from the bclb and the sclb architecture, we can see that for this specific scenario it is also better to have the sclb architecture."
"the dashboard-based monitoring environment described in section v allows fram engineers and data scientists to keep track of individual sensors in near real-time as well as to view the data values within a selected temporal range and/or geographical coverage. more specifically, the monitoring environment provides valuable assistance to scientists in terms of early detection of malfunction of sensors (e.g., alerts / notifications sent by email / sms when measurements are out-ofrange), filtering of data values for a certain range (e.g., temperature values above a certain range), and data aggregation (e.g., calculation of daily averages)."
"the device was fabricated by a wax patterning method and used different blood separation membranes. the device performed remarkably well compared to existing paper-based blood separation devices due to incorporation of two different types of paper that provided a single step separation. this microfluidic device was capable of separating the plasma from a single drop of blood (15 µl-22 µl) which contained the normal human hematocrit content (24%-55%). albumin from the separated plasma was detected using a bromocresol green (bcg) assay. plasma was directed to flow in the detection zone where bcg was added, and a color change was observed within 2 min from clear to deep blue. images of the detection zones were captured using a digital camera and the color intensity was analyzed using adobe photoshop cs2 in the \"gray scale\" mode for quantification of the results."
"in order to conclude the analysis of the experimental applications, it is necessary to remark that due to time requirements, a system frequency of 100 mhz is fixed. at this frequency, the hbd algorithm running on the general-purpose processor spends 462 cycles in order to process an input sample contained in the data frame. however, if this algorithm is running on the processor that is optimised for this algorithm, the number of cycles in order to process an input sample contained in the data frame is 11 cycles. on the other hand, the aes algorithm running on the general-purpose processor spends 484 cycles in order to process an input sample contained in the data frame. if this algorithm is running on the processor that is optimised for this algorithm, the number of cycles in order to process an input sample contained in the data frame is only 3 cycles. tables 6, 7, and 8 present the power results for each system that is evaluated. these tables show the dynamic power, the leakage power, and the total power for all the configurations that are presented in table 5 . as it can be seen, the power consumption of the imo is the sum of the power that is consumed by the components that the imo contains (i.e., the loop buffer controller, the loop buffer memory and the program memory)."
"loops dominate the total energy consumption of the imo. figures 16, 17, 18, and 19 show profiling information based on the accesses that are done in the program address space. figures 16 and 17 show the profiles based on the number of cycles per program counter that are related with the hbd algorithm, whereas figures 18 and 19 show the profiles based on the number of cycles per program counter that are related with the aes algorithm. we can see from these figures that there are regions that are more frequently accessed than others. this situation implies the existence of loops. apart from this fact, it is possible to see from these figures that the application execution time of the selected applications is dominated by only a few loops. in order to implement energy efficient imos based on loop buffer architectures, more detail information related with loops is needed. tables 1, 2, 3, and 4 provide this information. tables 1 and 2 present the loop profiling information of the systems that are related with the hbd algorithm, whereas tables 3 and 4 present the loop profiling information of the systems that are related with the aes algorithm. in these tables, loops are numbered in the static order that they appear in the assembly code of the algorithm. a nested loop creates another level of numbering. thus, a loop named 2 corresponds to the second loop encountered, while a loop named 2.1 corresponds to the first sub-loop encountered in the loop named 2. these tables corroborate the fact that the execution time of the loops dominates the total execution time of the application. for instance, the execution time of the loops represents approximately 79% of the total execution time of the hbd algorithm in the case of the general-purpose processor, and 81% in the processor that is optimised for this algorithm. in contrast, in the aes algorithm, the execution time of the loops represents 77% of the total execution time in the case of the general-purpose processor, and 90% in the processor that is optimised for this algorithm. it is necessary to remark that differences exist between algorithms of the same application due to the source code transformations and mapping optimisations that are applied in the optimised algorithms in order to generate efficient codes."
"an immunoassay demonstrates the interaction between an antigen and an antibody that is specific to a particular antigen. conventional elisa requires large volume of reagents, long procedures, and tedious washing steps. the paper-based elisa approach is advantageous due to the superior adsorbent properties of paper, significantly smaller volume (1-5 µl) of sample requirement, and rapid blocking step [cit] . in one study, rabbit igg was immobilized on paper and blocked using bovine serum albumin (bsa). alkaline phosphatase (alp)-conjugated igg antibody was then added to bind the rabbit igg [cit] . yellow colored substrate 5-bromo-4-chloro-3-indolyl phosphate/nitro blue tetrazolium (bcip/nbt) was placed on the sample zone to form a conjugate which subsequently produced a purple precipitate. a desktop scanner was used to scan the colorimetric result, which was then quantified using the nih imagej software (bethesda, md, usa)."
"the indigo runtime evaluates an operator tree much like an interpreter traverses an abstract syntax tree. during evaluation, a batch size parameter enables a trade-off between performance and memory footprint. to understand the effects of the batch size, consider a scenario in which an operator is right-multiplied by a dense matrix with n columns. the runtime can schedule this as one sparse-matrix-times-densematrix (spmm) on n columns, n spmvs on 1 column, or generally n b spmms on b columns. these cases can have very different memory requirements depending on the structure of the indigo operator. if the operator tree contains a product node, then memory proportional to b must be allocated to hold the intermediate result. similarly, if the tree contains an fft, the underlying fft library might allocate scratch space proportional to b. in memory-limited settings, it is desirable to minimize these allocations so the problem instance fits in memory. to this end, the runtime can use a smaller batch size b to maintain residency, at the cost of some performance. batch sizes can be set at any node in the operator tree."
"one final benefit of indigo is its ability to reduce a collection of complex, application-specific linear operators to a handful of low-level mathematical operations (spmms, onemms, axpbys, and ffts). this simplifies the task of performance optimization, which can be informed by extensive work on fast linear algebra, and enables optimizations developed for one application can propagate to other applications without source-level changes. the indigo representation also allows performance experts to search for well-performing implementations faster than if they were developing them from scratch. ultimately, indigo aims to advance the state of science and medicine by making image reconstruction codes faster and more widely available."
"the interactive customization system is based on computational and physical strategies of adaptation, where it organizes the architectural space in a building-up process creating a number of zones appropriate with the particular program required within an intelligent constructional approach as in figure 6."
"our application code generates a representation of the sense operator in indigo. given an nufft operator f and sensitivity maps stuffed into diagonal matrices s, the sense forward operator is expressed in matrix form as"
"we have further challenges concerning the economical feasibility and costeffectiveness for the wide propagation needed to bring about out the notion of selfawareness within the autonomous robotic systems addressing real changes during daily behavioral cycles within a sustainable manner, as the hardware, software, and fabrication domains are cost-exorbitant, in addition to the necessity for software simulations rather than physical experimentations. however, nowadays a number of diverse factors and demands have substantially accelerated the ease of responsive prototyping as well as full-scale implementation. much of this feasibility have begun from various approaches, technologies, and frameworks that have been expanded upon other intelligently-driven fields paralleling architecture, such as smart home technologies, which have highly participated in developing a number of responsive systems within the labor market. this propagation in smart notions has contributed in reducing the hardware and software's price, which have led to higher degrees of environmental coherence and performance relevant to the appropriate resources."
"indigo builds on ideas from domain-specific languages, sparse linear algebra, and high-performance medical imaging. a) domain-specific languages: substantial work has shown that domain-specific languages are effective tools for achieving high performance. examples include halide for feed-forward image-processing pipelines [cit], simit for finiteelement simulations [cit], pochoir for stencils [cit] performance breakdown of one iteration of the phase-space microscopy reconstruction operator (section vi-d). fraction of peak data are computed with respect to the roofline peak for each operation. the indigo operator is within a factor of two of peak performance on the cpu and gpu platforms using our custom backends."
"1) matrix-stuffed derived operators: when no operator exists for a particular operation, one can be created via matrixstuffing. indigo provides factory routines for building sparse matrices that perform common operations like element-wise multiplication, padding, cropping, and blurring. the factory routines can be parameterized to enhance generality. for example, the factory routine for building a padding operator accepts both a pad width and a padding scheme (center-padded or end-padded). these factory routines all return matrixops. the operator is then optimized by a transformation recipe, written by a performance expert, into a mathematically equivalent operator tree with better performance characteristics. finally, the tree is evaluated by an indigo backend on one of several computational platforms."
"in the phase-space forward operator, a 3d volume is split into multiple 2d depth planes. then, each plane is padded around the edges, fourier-transformed, and multiplied by a kernel related to a multiplex code and the corresponding depth (kernels are precomputed). the depth planes corresponding to a single code are summed, inverse fourier-transformed, and cropped, yielding the images received by the microscope."
"the paper primarily focuses on introducing three diverse approaches for reconfiguring an architectural space -for a smart campus-through the internet of things as an autonomous paradigm for anticipating and receiving parameters of particular inputs within a relevant context of students' interaction with university programs, analyzing and processing data, and adjusting its mode through augmenting the human functionality within a self-aware framework as described in figure 1. the particular methodologies are based on computational and physical strategies of adaptive configuration that can self-regulate, respond to changes and support both communication and interaction, impacting the way students interact with their space, through variable adjustments in its geometry to accommodate diverse automated lecture-seminar halls, workshop zones, and workspace within a single space that can physically reconfigure to face assorted changing needs. and hence, the described physical space would accommodate a new way of manifesting human-computer interaction in architectural design systems, which communicate, exchange information through direct engagement integrating the upsides of automation with responsive systems and smart configurations, where the objects within the smart campus might physically exist only when fundamentally required and transform when they are no longer practically needed."
"the hstack, vstack, and blockdiag operators are realized by assembling a new, larger matrix from the constituent blocks. performance and memory footprint are largely unchanged."
"to produce an indigo operator, the application programmer first instantiates operators from the matrixop and fastroutineop classes. these operators represent leaves which can be arranged into an operator tree. the most general class of operators is the matrixop, which represents a matrix implementing any linear transformation. operators that can be evaluated via matrix-free methods are classified as fastroutineops. these include ffts, identity matrices, and matrices of ones (onematrix)."
"it can be widely recognized the robust correlation of adaptive responses and interaction within architecture during the preliminary constructional phases, taking into account its wide spreading as an advanced framework in recent years, we highly face sort of challenges comprising the integration of various embedded systems within a machine learning framework that delivers algorithm driven-design configurations for generating architectural spaces following diverse particular inputs and variables."
b) sparse linear algebra: indigo's collection of linear operators is similar to those provided by the scipy [cit] and matlab [cit] . neither of these packages employ transformations to achieve better performance.
"taken together, these results indicate that the indigo implementation strategy can achieve at least 85% of the performance of an optimal matrix-free implementation. this result can also be interpreted as the marginal cost of the csr storage format. increasingly specialized storage formats and their associated evaluation routines can further reduce memory traffic."
"in terms of structural performance, the mobile self-reconfiguring system needs higher demands to develop newfangled structural potentials other than the accustomed ones, generating an adaptive and mobile architectural space which can respond to diverse inputs while holding its structure up against physical forces."
"we evaluate indigo on reconstruction problems from four imaging modalities: magnetic resonance imaging (mri), ptychography, fluorescent microscopy, and magnetic particle imaging (mpi). indigo implements five backends that we test on the cpu, gpu, and knl platforms described in detail in table i . our backends include:"
"here, the f routine could be drawn from a standard fft library, but the m and s routines are less standard and would have to be re-implemented on each platform."
"the paper introduces indigo, an embedded domain-specific language for implementing fast image reconstruction codes. by restricting users to a limited set of concepts (linear operations), indigo separates concerns about algorithmic specification and performance optimization and enables portable performance. we demonstrate indigo on four applicationsmagnetic resonance imaging, magnetic particle imaging, fluorescent microscopy, and ptychography-and show that indigo achieves a substantial fraction of peak across a variety of backends without changes to the source code."
"through diverse aggregation modules, the autonomous system demonstrates high capacities in achieving a transformational context within the scale of furniture units according to the required function as described in figure 5. figure 5 . aggregation process for smart sets developing self-reconfiguring furniture corresponding to data-driven inputs from smart-phone"
"the kroni operator is realized by explicitly replicating its child matrix. the realized matrix is larger by the replication factor, and immediate performance likely suffers by the same factor."
"we use the sense operator [cit] to model how an image is transformed and acquired by the scanner. in sense, a candidate image ρ with shape (x, y, z) is first multiplied by c number of sensitivity maps s of the same dimensions, yielding an array of channel-images k with shape (c, x, y, z). then, each channel-image undergoes a non-uniform fourier transform f to yield the acquired data k. mathematically, the forward sense operation is"
"product operators are realized via matrix-matrix multiplication. the performance implications of this transformation depend on the sparsity structure of the underlying matrices, since the product of two sparse matrices could be more or less sparse than the original matrices. we've found in practice that realizing products tends to maintain sparsity because the sparsity pattern reflects some real-world structure. one example that illustrates this effect can be found in the nufft, which contains a product of an fft-shift matrix and an interpolation matrix-the fft-shift matrix is diagonal, so multiplying it by the interpolation matrix yields a new matrix with identical structure to the original interpolation matrix. we liken this transformation to operator fusion."
"the adjoint operator is realized by explicitly transposing and conjugating the underlying matrix. this transformation can have a substantial effect on performance since indigo primarily uses a compressed sparse row (csr) storage format for sparse matrices. for certain sparsity structures, a csr format maintains better temporal locality in the input or output vectors."
"recent developments have started to initiate a move from a mechanical ideology of versatility to a biological one. the commonness of the organic paradigm is starting to change the conceptual ideology that we enforce with a view to comprehend our environment and, accordingly, design within its surroundings. [cit] 's, with a distinctive progress toward comprehending and recognizing the field of cybernetics within the architectural framework by developing his well-known theories. [cit] s [cit] s, developed a \"conversation theory,\" a specifically consistent and potentially the most prolific theory of interaction and communication comprising human-to-human, human-to-machine and machine-tomachine configurations in a common framework. [cit] the theory served as the premise of the implied processes implicated in complex and intelligent learning methodologies through humans and machines that enable learners to realize commonsensical comprehension of the complex pragmatic processes for a multilevel system through diverse models of communication within participants, transferring what they have learned in a continuous discourse. [cit] ´ [cit] ´s. negroponte presented the implementation of computers in architectural systems and embraced their incorporation in spatial structures and architectural spaces at the \"software\" exhibition through a responsive model inscribed in a plexiglass case, embedded with an intelligently-controlled environment of small cubic blocks and a number of gerbils changing the blocks' arrangement while a programmed robotic arm autonomously lay out the blocks in an order trying to follow the gerbil's movements. [cit] the objectives were to develop a responsive paradigm within the architectural platform in addition to creating an adaptive environment that opens up the possibilities to interact and respond to the requirements and desires of users. it is a fundamental model of a cybernetic reactive cycle placed in a building environment in which the current output is influenced according to a particular input. [cit] negroponte's responsive architecture is well-considered as a notion of smartness and versatile response. negroponte believes that comprising artificial intelligence with architectural environments is crucial to delivering an adaptive, responsive architecture which have the capacity for reacting autonomously according to assorted needs."
"indigo instead allows a performance expert to devise a transformation recipe that is applied to operator trees constructed by an application programmer. a transformation recipe is a series of imperative statements that perform tree manipulations such as those described in section iii. indigo implements a number of transformations, and new ones can be written by the user. we have found that transformation recipes are general enough to cover variation among trees from a particular application area, so it is not necessary to rewrite them frequently. they also enable experts to quickly try out new configurations without substantial programming effort. the use of transformation recipes does not rule out automatic techniques, as future transformation recipes could be generated by an autotuner or heuristic engine."
"even with fast individual operators, end-to-end performance remains elusive because forward operators typically comprise multiple sub-operators arranged in some manner. substantial data reuse opportunities exist across operators, so it is insufficient to optimize them in isolation-we must consider operator fusion. with matrix-free operators, automatic fusion might be attainable via static analysis and code generation. however, this approach is subject to the limitations of static analysis and requires substantial machinery to implement. furthermore, it can be difficult to reason about the performance characteristics of matrix-free operators because of their abstract representation."
"operator implementations can be classified into two groups. early convex optimization frameworks used a \"matrix stuffing\" approach that expressed linear transformations as explicit sparse or dense matrices. this concrete representation is general enough to express any linear transformation, but it is suboptimal when operators admit fast algorithms. for example, the cooley-tukey algorithm can compute an n-length fft in o(n) space and o(n log n) operations, versus o(n 2 ) space and o(n 2 ) operations for a general matrix-vector product. later work sought to leverage fast algorithms like ffts in \"matrix-free\" implementations that implement linear transformations via fast subroutines. high-performance libraries for common linear transformations are generally available, so it is straightforward to obtain portable performance for single instances of these operators."
"the indigo representation of linear operators enables transformations between numerically-equivalent operators that possess different computational performance and memory footprint characteristics. indigo employs three flavors of operator specialization: tree transformations, operator realization, and matrix inspection."
"so far, we have described the space of transformations that can be applied to an operator tree to change its performance characteristics. it remains an open question how to choose good transformations that yield fast operators, operators with small memory footprints, or some balance thereof. automatic methods guided by heuristics or empirical tuning might succeed in finding good configurations, but several factors compound the search problem. first, the optimization space is highly non-smooth. a good tree is often found by applying a long series of seemingly pointless or suboptimal transformations. second, it's difficult to enumerate the search space because the tree transformations are bi-directional, so care must be taken to identify equivalent, previously-visited points. third, many of the transformations cannot be assessed without performing them. in particular, it's difficult to predict whether multiplying two matrices will yield a new matrix that's faster or smaller than its predecessors. one could compute the new matrix via matrix-matrix multiplication, but it significantly slows down a search procedure."
"the third major category of specialization is an inspection step in which indigo examines the remaining matrices in search of structural properties that enable faster evaluation than a general spmm routine can achieve. in particular, indigo checks matrices for properties we term row or column write exclusivity. row write exclusivity holds when a matrix has no more than one nonzero per row (or per column for column write exclusivity). the presence of write exclusivity allows matrix multiplication routines to avoid synchronization in some operations. more specifically, a sparse matrix multiplication routine (spmm) computing the product of an implicitly-transposed, column-exclusive, csr-format matrix needn't synchronize on accumulations into the output vector because there won't be more than one element accumulated. this optimization is especially important on gpu platforms where atomics are relatively costly."
the experimental approach is based on delivering diverse programmed motions and rotations as an output following a particular input through the internet as in figure 2 figure 2. part of script for rotational response following particularly-analyzed data
"a number of linear inverse problems arising in computational imaging can be solved via iterative optimization methods. central to these methods is the definition of a \"forward model\" linear operator that represents how an intrinsic image is transformed when observed by an imaging system. when the forward operator is used in gradient-based methods such as conjugate gradient, the intrinsic image can be reconstructed from the acquired scan data. evaluation of the forward operator is often the dominant cost in image reconstruction, so computational performance is critical. at odds with the demand for performance are concerns about extensibility-operators vary across both imaging domains and problem instances from the same domain-and portability-practitioners wish to run on a variety of computational platforms such as multi-core cpus and gpus. it remains a challenge to achieve both portability and performance for linear operator implementations."
custom algorithms and programming scripts have been utilized developing a conceptual online program which is directly linked to an experimental customized prototype via the internet emphasizing its autonomy within a versatile kinetic notion of responsiveness.
"mri is a popular in-vivo imaging modality because of its excellent soft-tissue contrast, but its use is hindered by long scan times during which the patient must lie motionless. compressed sensing and parallel imaging techniques are being used to reduce scan times by integer factors, but reconstruction of the resulting data requires solving a linear inverse problem in a clinically-acceptable three minutes [cit] . a pure python implementation can take as long as eight hours to produce a diagnostic-quality image, so we turn to indigo to implement a fast reconstruction code."
"the transformation of the architectural paradigm through the internet of things is driven by the potentials to embed intelligence into building's systems and connect them in real-time, where moving physical objects can share a common physical space creating versatile and spatial adjustments, encompassing together physical world of objects, humans and virtual conversation and interactions."
"it is theoretically possible to realize fft operators, but typically impractical to do so on realistic problems because the resulting matrix would be large and dense. the mri application described in section vi would require 41 petabytes to store its 3d-fft matrix."
"the row and column write exclusivity properties are cheap to discover. indigo determines if a csr matrix is row exclusive by a single traversal of the row ptr vector (in netlib terminology [cit] ); a matrix is row exclusive if no adjacent row pointers differ by more than one element. determining column exclusivity is slightly more difficult. indigo computes a histogram of column indices (the col ind array) and certifies exclusivity if no histogram bin has more than one element. this procedure is similar in cost to one matrix-vector product. indigo determines exclusivity information once, as the final specialization step."
"ptychography is an imaging modality which uses patterns from diffracted light to reconstruct an image of an object [cit] . in x-ray ptychography, x-rays are shined through an object, ψ, with some illumination pattern ω. the x-rays shine through the object to land on a detector, such as a ccd sensor. a number of diffraction samples a (i) x-ray beam at the object from different angles. the diffraction of the x-rays can be described by the equation"
"c) high performance image reconstruction: previous work has sought to reduce time-to-solution in a variety of iterative reconstruction problems. one rewarding approach has been algorithmic modifications that offer different performance profiles [cit], or that trade accuracy for performance [cit] . another body of work has focused on fast, ad-hoc implementations of reconstruction codes; these include bart [cit], powergrid [cit], impatient [cit], and gadgetron [cit] for mri, and sharp for ptychography [cit] ."
"to use indigo, an application programmer constructs an operator tree by instantiating operator classes. next, the operator tree is specialized according to a transformation recipe written by a performance expert. the optimized operator is then available for use in the mathematical optimization routine of the user's choice. indigo provides implementations of the conjugate gradient and fista methods [cit], though more are possible. typically, the mathematical optimization routines alternate between linear operator evaluations and blas-1 evaluations specific to the particular routine. on each evaluation, the operator tree is interpreted by an indigo backend on cpu, knl, or gpu platforms. this process is illustrated in figure 1 ."
"phase-space fluorescent microscopy enables 3d reconstruction of fluorescence by leveraging the position and angular information of light. biologists often want to study the activity of a 3d creature or cells in vivo, hence the need to capture a 3d video. both the fast acquisition of the phase-space information and fast 3d reconstruction from the data are important in order to visualize the sample. in recent work [cit], multiplexed phase-space imaging aims to tackle this challenge. the slow mechanical scanning or angle scanning part is replaced by applying multiplex codes in the angular space (pupil). an image is captured for each of the codes while the sample is uniformly illuminated by a steady laser source, and the 3d sample is reconstructed from those images."
"progresses in ubiquitous and pervasive computing are opening up new scopes for developing a newfangled vision of a smart, responsive architecture that adapt efficiently to complex site and programmatic necessities tending to today's dynamic, flexible and constantly changing needs."
"many compositeops admit a distributive property. there are many such rules-we count up to 504-so we sketch a few here. the remaining ones can be reasoned about straightforwardly. for example, the kroni operator distributes over a product:"
the previous two subsections presented the operator classes implemented by indigo. these classes are general enough to derive a variety of other operators. derived operators are instantiated via factory functions provided by indigo.
"the paper introduces an autonomous behavior for an architectural system controlled through the internet of things that enables a programmatic, extensive and dynamic interaction in real time, trying to explore the dynamics of architectural space by rethinking architecture beyond conventional static and single-function spatial design. it proposes a programmed control system through the internet for a summer school's program encompassing a number of workshops, seminars and courses, each customized for a particular architectural space organization which is directly linked to the number of registering students. the authors would propose three sequential platforms for autonomous reconfigurations through the internet of things."
"the indigo approach is not without limitations. first, it avoids the challenge of automatic optimization by relying on transformation recipes written by performance experts. to be widely useful, it might be necessary to have generic recipes that yield good average performance, or an analysis engine that can synthesize good recipes. secondly, real-world reconstructions can employ an expensive nonlinear regularization like total variation or locally-low-rank metrics. these fall outside the domain of indigo, but they face similar portability and performance challenges. we wrote our regularization routines in openmp-parallelized c code. third, construction of indigo operators can be unintuitive because it entails programmatic construction of a abstract syntax trees rather than construction via a language syntax and parser. fortunately, mathematical and visual structure abounds in this domain, and we are optimistic that better interfaces can facilitate faster and more correct operator construction."
"the paper particularly emphasizes the correlation between technology and architecture, in which advances in one field empower or demand changes in the other, as introducing computer science discipline within the architectural context has altered the way people interact with their built environment."
"the second major category of specializations is operator realization, which converts matrix-free operators into explicit matrices. many realization transformations seem inadvisable because they increase the memory footprint and/or inhibit performance. however, like tree transformations described previously, they can enable other, more profitable transformations. each matrixfreeop has an associated realization transformation, described here. in order to be eligible for realization, a compositeop's children must all be realized matrices (matrixops)."
"thereby the authors attempt to enable an ideological alteration in the cognition of the inhabited space through proposing an autonomous model of self-aware system, supporting the architectural space within hybrid control networks, and controlled through the internet of things."
"using the internet of things, users have been capable to directly choose their own architectural space, where its elements are complexly layered and continuously differentiated within a time-based system through a stimuli-responsive behavior. the particular approach is based on simulating a specific model within a cell-phone application system. depending on the continuous change in needs, students choose from a number of customized architectural proposals as in figure 4."
"the vendor-tuned fft library (mkl) using fft dimensions suggested by the vendor's tuning script. assuming the mkl fft is well-implemented, and recognizing that optimizing it is beyond our control, we conclude that our cpu and knl backends are achieving a reasonable fraction of peak performance. on all platforms, our best-performing backend is within the range of clinical feasibility."
"application programmers next use compositeops to arrange sub-operators into an expression tree. compositeops reflect mathematical or structural relationships between operators. common examples include product and sum operators representing the inner product and sum, respectively, of child matrices; adjoint operators representing the conjugatetranspose of their sub-operator; and scale operators representing their child operator scaled by some scalar value. structural composite operators include the blockdiag operator, which represents sub-operators arranged along a diagonal; the vstack and hstack operators, which represent vertically and horizontally stacked sub-operators; and the kroni operator, which represents the kronecker product of an identity matrix and its sub-operator. the kroni operator is similar to a blockdiag operator, but captures replication of its suboperator along the diagonal. furthermore, evaluation of a kroni operator is particularly efficient because the matrixvector operation"
"on registering a particular program through a specific number of students, the autonomous system starts to aggregate through a self-assembly process following a particular algorithm-driven design logic which exhibits methods of self-awareness and versatile configuration with respect to constantly changing needs within a notion of selfassembly which is described in figure 3."
"1) background: during a scan, the scanner plays a \"pulse sequence\" of radio waves that excite protons or other species of interest with a particular volume. additional magnetic fields induce spatially-varying resonant spinning of the protons, effectively encoding their position as frequency and aggregate density as amplitude. one or more inductive coils arranged around the volume of interest record the emitted signal in separate receive channels, and a fourier transform of the channel data yields spatially-weighted variants of the intrinsic image. compressed sensing pulse sequences speed up acquisitions by collecting fewer measurements than typically necessary. linear reconstruction will result in aliasing in the spatially-weighted images, but the aliasing can later be resolved via knowledge of the spatial sensitivity of each receive coil and redundancies in the image statistics. in mathematical optimization setting, we are looking for an image that most closely transforms into the aliased images acquired by the scanner."
"3) performance: the indigo sense operator achieves 91% of the roofline peak on the gpu platform, and the reconstruction task finishes within one minute. figure 3 illustrates the performance of the sense operator during the transformation process, and table ii gives the performance breakdown of the fastest sense operator across the suite of platforms. the cpu and knl platforms achieve a smaller fraction of peak-33% and 10%, respectively. we still consider this a worthy result because two-thirds of the evaluation time is spent in vi-a) . the best platform achieves 91% of the roofline peak. these data suggest the use of explicit sparse matrices isn't a significance hindrance since the majority of the evaluation time (59%-72%) is spent in vendor-tuned fft routines."
"we seek to combine the matrix-free and matrix-driven approaches to constructing linear operators. we still leverage matrix-free subroutines for ffts and other fast transforms, but we revisit matrix stuffing for \"accessory\" transforms like padding, cropping, blurring, shifting, and element-wise multiplication. we implement our approach in a language called indigo. indigo simplifies analysis and specialization of operators because their structure is statically known, and it aids portability because new platforms must only implement axpby, fft, and matrix multiplication routines. indigo operators are also amenable to performance modeling because the performance characteristics of the underlying subroutines are well understood. we give a roofline analysis [cit] that reveals indigo operators run within a significant fraction of machine peak, and we describe techniques for directing future optimization efforts. indigo is available for download at https://pypi.python.org/pypi/indigo. this paper is organized as follows. section ii presents the indigo representation of linear operators. sections iii and iv describe transformations that the compiler and runtime can apply to move between mathematically-equivalent operators with better performance characteristics or smaller memory footprints. section v explains how profitable transformations are selected from the broad set of possible transformations. section vi evaluates indigo on reconstruction tasks from four imaging modalities on three modern shared-memory platforms. section vii compares indigo to related work and section viii concludes with directions for future work."
"and hence, our current study mainly addresses problems dealing with the continuous change in functions within the interior spaces for universities comprising numerous variables ranging from number of students attending a particular course, time schedule for professors, diverse sorts of programs integrating courses, lectures, workshops and seminars where each require a particular organization for the architectural space encompassing the precise program held."
"indigo is a domain-specific language for constructing fast, structured linear operators. its fundamental object is an operator that represents an arbitary linear transformation. operators are broadly categorized as matrixops, which are backed by explicit matrices, or matrixfreeops, which use alternate evaluation strategies. matrixfreeops can be further classified as compositeops, which represent arrangements of operators, and fastroutineops, which call subroutines that implement fast linear transformations. operator trees can be constructed according to the following grammar."
the study introduces the internet of things as a smart control discourse within the architectural space through three models of computational platforms which have been utilized to develop autonomous configurations among three-dimensional intelligent geometries.
"our technique of separating the transformation recipe from the code to be transformed has been explored in other dsls. we examine two: chill [cit] and halide [cit] . in these dsls, the recipes are procedural in nature, but differences arise in their expressiveness due to considerations from the application domain and intended platform(s). chill uses transformation recipes to optimize scientific stencil computations. the chill compiler ingests loop annotations that specify loop reordering transformations and application-level concepts like ghost zones. halide's transformation recipes (\"schedules\" in halide parlance) enable transformations to be applied to kernel objects in an image processing pipeline. common halide transformation include tiling, parallelization, and vectorization. recent work on heuristics for scheduling halide programs has been promising [cit] and affirms the transformation recipe approach."
"the remaining performance discrepancy arises on gpus, where the atomic accumulate operations found in adjoint csr multiplication kernels are particularly expensive. here, we leverage the row-wise exclusive write property of the maps+ matrix to dispatch to spmm multiplication routines that perform non-atomic accumulations, rather than atomic ones. the operator tree undergoes four major transformations: reordering, realization, a transpose of the maps matrix, and discovery of the exclusive write property for the transposed maps matrix. realization is required on the gpu to fit into the 12gb of available memory, hence we omit the first two results. we also omit the write exclusivity result for backends which don't leverage it. we denote the region in which operators are fast enough to be clinically viable with a green background."
"the user-defined operator tree ( figure 2a ) is functionally correct but slow to execute for various reasons. first, evaluating each of the many matrixop nodes requires reading and writing its input and output vectors, which is expensive in aggregate. we would like to merge some of the these nodes to avoid intermediate memory traffic, but the matrix-free fft node inhibits this transformation. thus, we develop a transformation recipe that separates the fft from concrete matrices by flattening nested products and distributing the kroni operator over its child products. the resulting tree is depicted in figure 2b . then, we realize vstack and product nodes into new matrixops. we are left with two sparse matrices: a grid+ matrix that performs gridding, shifting, and scaling; and a maps+ matrix that simultaneously applies sensitivity maps, apodization, padding, and fft shifting to each coil-image. the resulting tree in shown in figure 2c ."
"the corresponding indigo operator tree is illustrated in figure 2a . the adjoint operator is implicitly defined at the same time as the conjugate transpose of the forward operator. when used with -1 regularization techniques, the sense operator is sufficient for implementing a reconstruction pipeline for compressed sensing mri."
"we can characterize the performance of a fully-matrix-free sense implementation (rather than our hybrid approach) if we make two assumptions: 1) the intermediate vectors are memory-resident before and after the ffts. to do otherwise would be to ignore re-use in the maps+ and grid+ matrix-free operators, and 2) the sensitivity maps are incompressible, i.e. they do not permit more succinct representations than dense arrays, the standard representation in the field. given these assumptions, we can quantify how much faster each operation could be:"
"throughout history, we can obviously realize the miscellaneous approaches representing conventional architecture are highly restricted by the lack of adaptive responses, flexibility and dynamism, where spatial configurations are static and can be barely capable of accommodating the range of contingencies that depict daily life as shifts in environmental circumstances, changes in the behavior and needs of people. [cit] our buildings are inconsistent with the connotation of change. constant utilities, inflexible materials, static infrastructure and lack of real-time adjustments impede our ability to affect our surroundings. architects have no approach to access the future user requirements in the design process while there are constant demands for adding versatility and dynamism within our built environments ranging from programmatic and site-context response to spatial dynamics. [cit] it can be widely-recognized the importance of addressing responsiveness within accustomed architectural spaces as they are not depicted by their capacity to adapt continuously in real time and stay aware with the continuously assorted requirements of various users, where the general state of the space may be optimized for accommodating various users. however, it is not adjusted to alter its physical shape adapting with their diverse particular functions within a time-based system."
"indigo operator trees can be reorganized according to common algebraic principles. these transformations have minimal immediate effects on performance; however, they can enable profitable transformations later in the specialization process."
"to determine the fraction of roofline peak for our codes, we compute the arithmetic intensity of our kernels as follows. for three-dimensional ffts of shape n 3, we expect 5n 3 log(n 3 ) flops (per the fftw model [cit] ) and 6n 3 elements of memory traffic (one read and write of the dataset is required per dimension when no n 2 slab fits in cache). for all other operations (onemms, axpbys, csrmms), we expect them to run at the stream [cit] bandwidth: they should read the input data once and write the data once. we also consider sparse matrix structure-we account for input elements that need not be read, and output elements that need not be written."
"this paper proposes a two-level parallelization method for distributed hydrological models by simultaneously utilizing the parallelizability at both the sub-basin and the basic simulation unit levels, and in this way improve parallel-computing scalability. section 2 describes the basic concept of the two-level parallelization method. section 3 describes the implementation of the method, taking a grid-based distributed hydrological model as an example. section 4 validates the effectiveness of the proposed method through a case study. section 5 concludes and discusses future research directions."
"for message confidentiality, the sender of the message wants to make sure that only the intended receiver can share the message. the sender can encrypt the message by using a onetime secret key. using the digital envelope technique, the sender can encrypt the one-time secret key in a digital envelope by using the receiver's authenticated long-term public key. thus, the digital envelope can only be opened by the intended receiver by using the corresponding long-term private key. actually, only the intended receiver can obtain the session key and use this secret key to decrypt the message. this solution can achieve confidentiality, and, at the same time, it provides full deniability since any user can generate the digital envelope. using the dhkey exchange solution, the sender can compute a short-term dh public key and send this short-term public key to the intended receiver. then, the sender and the receiver can share a one-time secret key based on the sender's short-term dh public key and the receiver's long-term dh public key with a digital certificate. this solution can also achieve confidentiality, and, at the same time, it provides full deniability since any user can generate the short-term dh public key."
", p is the transmission power by t, and h t,r and h t,e are the channel gains to the receiver and eavesdropper, respectively. the statistical characteristics of the message signal and the noise are the same as those in section ii-a."
"we use the 16 bits of the bitmask of the lost packet in the feedback control information field to feed back the receiver's confirmation of the covert message. among them, 8 bits are used to indicate the number of received covert information bits, and the other 8 bits are used to store the last eight bits of the received covert message. moreover, in order to ensure undetectability of the covert storage channel, not all rtcp packets are used to feedback the reception of covert message, and the interval frequency of the selected rtcp is determined according to the capacity of the sender-to-receiver covert timing channel."
"we experimentally study the practical runtime costs of the algorithms in two data parallel applications, matrix multiplication and fast fourier transform, on a cluster in grid'5000 platform. we demonstrate that their practical runtime and memory costs are low making them ideal for employment in self-adaptable applications. we also show that the parallel algorithms exhibit tremendous speedups over the sequential algorithms. finally, using theoretical analysis for volume 6, 2018 a forecast exascale platform, we demonstrate that the parallel algorithms have negligible execution times compared to the matrix multiplication application executing on the platform."
"the algorithm eopta solving eopt has code structure similar to popta (algorithm 1). the inputs to eopta are size of the workload, n, given as multiple of x, the number of processors, p, the minimum granularity, x, and the dynamic energy function represented by two discrete sets, x and respectively containing problem sizes and dynamic energy consumptions. m is the cardinality of the sets x and . the outputs are the optimal load distribution, d opt, and the optimal dynamic energy consumption, opt . the number of processors selected by eopta in the optimal workload distribution may be less than p. for example, if the dynamic energy function is concave, then eopta may select just one processor to execute the workload if the workload size lies in the domain of the dynamic energy function."
"from our experience, there are multiple classes of applications that benefit from our data partitioning algorithms. a dominant class contains applications where the speed of an application is a function of problem size, which is defined as a set of one, two or more parameters characterizing the amount and layout of data stored and processed during the execution of a computational task. some classes are the following:"
"for the hydrological model used in this paper, channel routing causes dependent relationships between sub-basins, and the hillslope processes between sub-basins are independent. from the perspective of geographic space, for headwater sub-basins, both hillslope and channel-routing processes are independent; for nonheadwater sub-basins, hillslope processes are independent, but the channel-routing process depends on its upstream sub-basins."
there are two types of power consumptions in a component: dynamic power and static power. dynamic power consumption is caused by the switching activity in the component's circuits. static power is the power consumed when the component is not active or doing work. static power is also known as idle power or base power. from an application point
"return (d opt, t opt ) 46 : end function volume 6, 2018 optimal workload distribution, d opt, where the distributions are given in multiples of x, and the optimal execution time, t opt . the optimal number of processors that are selected by popta in the optimal workload distribution may be less than p. the traditional load-balancing algorithm returns the workload distribution,"
"for distributed hydrological modeling, a watershed can be divided into spatial units of different levels. the spatial units from top to bottom are usually sub-basins, hillslopes and basic simulation units such as grid cells and hydrological response units [cit] . parallel computing for distributed hydrological modeling can be conducted at different levels. the granularity of parallel tasks, which refers to the number of computations for each parallel task and the communication overhead among parallel tasks, are different for parallel computing at different levels. parallel computing at the sub-basin and hillslope levels is coarse grained, with a relatively low number of parallel tasks and low communication overhead, and it is suitable to run on computer clusters. parallel computing at this level has good scalability with respect to hardware, but suffers from limited parallelizability caused by the limited number of parallel tasks and the dependences among them [cit] . in contrast, parallel computing at the basic simulation-unit level (e.g., grid-cell level) is fine-grained, with a relatively large number of parallel tasks but also with high communication overhead. although there is good parallelizability at this level, it requires an extensive level of communication and so need to run on shared-memory hardware. the hardware environment is usually the limiting factor for parallel computing at this level [cit] . the coarse-and finegrained parallel computing methods described above are complementary on the aspects of parallelizability and hardware. combining them to conduct two-level parallel computing may lead to better parallel-computing performance [cit] ."
"in the siso case, the beamformer design is reduced to scalar power control. similar to (6), the optimization problem for siso system is defined as"
"covert storage channels usually hide the covert message encoding into the protocol field. covert storage channels generally use the following construction methods: (1) the covert channel encodes the covert message using the size of the header element or the protocol data unit (pdu) [cit] . (2) the covert channel changes the sequence of the header or pdu elements to encode the covert message [cit] . (3) the covert channel creates a new space within a given header element or within a hidden data pdu, or the encapsulated ip packet size is smaller than the size specified in the ethernet frame size to use the ip packet tail space for covert message [cit], etc. (4) the covert channel encodes the covert message as a reserved header or pdu element [cit] . (5) the covert channel hides the covert message by modifying the data in the packet [cit] ."
"is the time of execution of the problem size x. n and x are considered one-dimensional. since the processors involved in the execution of the workload are identical, the input to the problem is a single speed function. we do not specify how to build the speed function. it may be constructed using one or more processors."
"there are several emerging trends in distributed hydrological modeling, which plays an important role in watershed research and management [cit] . first, high spatial resolutions are needed to characterize the detailed spatial distribution of hydrological variables such as soil moisture and soil erosion [cit] . second, long-term simulations are needed to assess the impacts of land-use change and climate change on watershed sustainable development [cit] . third, integrated simulations coupling multiple geographic processes are needed in order to evaluate the effects of watershed management practices comprehensively [cit] . to conduct such distributed hydrological modeling over long periods, a large amount of computation are required. this makes the parallelization of distributed hydrological models an inevitable choice [cit] ."
"we now present the parallel data partitioning algorithm paraleph employing the dynamic programming approach. the number of processors available for its execution is p, which is also the number of processors available for execution of the data-parallel application in which paraleph is applied. we illustrate paraleph using an implementation employing hierarchical two-level parallelism. the first parallelism is intra-process using openmp where each process executes t threads and the second parallelism is inter-process using q mpi processes."
", where i q is the index of point q. for example, figure 14 shows these points in a segment of the energy function of openblas dgemm application."
"for the overland-flow routing process, there are sequential dependences among upstream and downstream grid cells, so in order to conduct parallel computing, this approach divided grid cells into layers according to flow topology (fig. 5) . the single-flow direction (d8) method [cit] was used to define the flow topology. the outlet cell is labeled layer 1 and the units draining directly to cells in layer 1 are defined as layer 2. the cells draining directly to cells in layer n are defined as layer nþ1 and so on until the most upstream cells are reached [cit] . in such a way, there are no upstream or downstream relationships among grid cells within each layer. therefore, the calculations on grid cells in the same layer are independent and can be conducted in parallel."
"after encoding, the value of the code symbol s i is a discrete random variable of the cumulative density function represented by f s (·). as can be seen from the formula 3, f s (·) can be calculated by accumulating the pmf of the code symbol."
"however, the scalability of parallel computing using the parallelizability among simulation units at a single level is usually limited. for example, there exist maximum speedup ratios (msrs) for parallel computing at the sub-basin level [cit] . when the number of processors exceeds certain thresholds, the speedup ratio will not keep increasing due to the load imbalance caused by limited numbers of sub-basins and the dependences among them [cit] . parallel computing at the gridcell level also has scalability problems mainly due to communication overhead [cit] . how to address the scalability problem and improve the efficiency of parallel computing for distributed hydrological models has become an important research topic."
"when the source of a message must be assured, the message sender can compute the mac of the transmitted message commonly, two methods are used to distribute the common session key sk between the sender and the receiver, i.e. the symmetric-key solution and the asymmetric-key solution. when the symmetric-key solution is used, a secret key sk must be pre-shared between the sender and the receiver. then, one participant selects the session key sk, encrypts it under the preshared secret key sk and then sends the ciphertext to the other participant. since both communication parties know the preshared secret key sk, the symmetric-key solution can provide only 1-out-of-2 deniability."
"the first category deals with data partitioning algorithms employed for performance optimization on hpc platforms. the second category presents self-adaptable applications and dynamic runtime schedulers. the third category surveys efforts that investigate loop and data transformations to improve performance of regular and irregular codes. final category deals specifically with works that have proposed data partitioning techniques targeted for self-adaptable applications on heterogeneous platforms. static algorithms, such as those based on data partitioning [cit], use a priori information about the parallel application and platform. these algorithms are also known as predicting-the-future because they rely on accurate performance models as input to predict the future execution of the application. they are particularly useful for applications where data locality is important because they do not require data redistribution. they however are unsuitable for non-dedicated platforms, where load changes with time. dynamic algorithms, such as task scheduling and work stealing [cit], balance the load by moving fine-grained tasks between processors during the execution. they do not require a priori information about execution but may incur large communication overhead due to data migration. they can use static partitioning for the initial step due to its provably nearoptimal communication cost, bounded small load imbalance, and lesser scheduling overhead."
"each cell in the tables contains four values, (t app, t paraleph, t comm, speedup), where t app is the execution time of the data-parallel application, t paraleph is the execution time of paraleph, t comm is the time of communications in paraleph, and speedup is the speedup of paraleph over the sequential data-partitioning algorithms (popta or eopta). the speedup is the ratio of execution time of the sequential data-partitioning algorithm over the execution time of paraleph."
"by self-adaptable applications, we mean applications that automatically adapt at runtime to any set of heterogeneous processors with a priori unknown performance characteristics [cit] . they are typically executed in dynamic environments or environments where the number of available processors and their performance characteristics can be different for different runs of the same application. they must adapt at runtime to dynamic changes in the environment in even a single run. such applications should be able to optimally distribute computations between the processors of the executing platform assuming that this platform is different and a priori unknown for each run of the application."
"the results of the serial and parallel simulations were confirmed as identical, and then three experiments were designed to test the parallel performance of the proposed two-level parallel-computing method. first, parallel computation was conducted under different combinations of processes and thread numbers to test the overall performance. the number of processes ranges from 1 to 48 and each process may contain 1, 2, 3 or 6 threads as long as the number of processes multiplied by the number of threads within each process was less than or equal to 48, which was the total number of cores in compute nodes. the 90 m-resultion dataset with 67 subbasins was used for the experiment. in order to investigate the impact of the number of sub-basins on parallel performance, twolevel parallel computation was then conducted using the 90 mresolution dataset with different numbers of sub-basins (43, 67, and 89). each process contained 2 threads. in order to study the impact of data volume on parallel performance, a two-tier parallel computation was finally conducted using datasets of different resolutions (i.e., 270 m, 90 m and 30 m) with a fixed number of subbasins (i.e., 67). each process also contained 2 threads. each experiment was repeated three times to get an average computing time. the computing time here refers to the time for the actual simulation excluding the time for data input/output (i/o)."
"in recent years, e-mail has been one of the most important and widely used network applications. it has been used in communications between individuals, business organizations and governmental agencies around the world. e-mail is one of the most popular, non-interactive, communication applications in network environments. pgp and s/mime are two wellknown and useful secure e-mail solutions. both solutions use a combination of conventional, symmetric-key (or secretkey) techniques and modern, asymmetric-key (or public-key) techniques to provide message confidentiality and message authentication."
"found this configuration of paraleph to be the optimal (appendix g). the sequential data-partitioning algorithms (popta, eopta) are executed using one core in one single node."
"in this work, we studied the secrecy energy efficiency, ζ, and its trade-off with the secrecy spectral efficiency, η, in miso and siso wiretap channels. optimal beamformer was designed to maximize ζ for the cases with and without considering the minimum required η (i.e., η 0 ) at the receiver in a power limited system. we saw that as η 0 increases, the performance of the optimal beamformer and the zf beamformer designs gets closer. furthermore, as the number of antennas decreases, the performance gap between the optimal and the zf design increases. it was observed that there is a specific η below which increasing η leads to higher secrecy energy efficiency (i.e., ζ), and above which the opposite trend occurs. depending on the power value corresponding to the optimal ζ, increasing η can increase or decrease ζ. in addition, it was shown that adding more antennas to the transmitter side increases ζ considerably and sustains the optimal ζ for a longer range of η 0 ."
"runtime schedulers such as kaapi [cit], starpu [cit], and dague [cit] schedule an application described as a direct acyclic graph (dag) or task graph onto parallel 69080 volume 6, 2018 platforms. the dag expresses different types of tasks and the data dependencies between them and is created either statically or dynamically. little information exists on the computational performance and memory utilization of dag schedulers. they cater to particular classes of applications (sparse, irregular, etc) that are not applicable to our data partitioning algorithms. for the data-parallel applications, it is a edifying research task to conduct a comprehensive comparison between them and static and dynamic load-balancing and load-imbalancing algorithms."
"(1) if the message contains the routing-calculation results of a sub-basin, the master process will record the results in memory and check whether they are needed by other slave processes. if they are needed as input, the master process will transfer them to that slave process. (2) if the message is a request for upstream inflows of a subbasin, the master process will check whether the upstream inflow data are already available in memory. if so, it will send these inflow data back to the slave process; otherwise, it will add the request to the waiting list, so that the requested data will be sent as soon as they are available. (3) if the message contains the information that all tasks have been completed by the slave processes, the master process will quit."
"in order to improve the robustness of the covert channel, the gray code (gc) is used to encode the covert message. after the covert message is encoded, packet loss and packet out of order in the network transmission will only cause one bit error, that is, all consecutive code words are exactly different in one bit position, so this feature of gray code can greatly reduce the bit error rate."
"all the works examined in this category strive to achieve dynamic load balance. they use a simple principle of ''using the past to predict the future'' where they employ the information (speeds, execution times, etc) from the current iteration to redistribute work for the ensuing iterations. self-adaptable applications, which are typically executed in dynamic environments, may invoke a data partitioning algorithm multiple times due to which cost of data redistribution or migration is incurred."
"the outputs are the optimal workload distribution, d opt, and the optimal execution time or total dynamic energy consumption, opt . the optimal number of processors selected by paraleph in the optimal workload distribution may be less than p."
"self-adaptability is essential not only due to the changing underlying execution environment but also due to the specific characteristics/requirements of the application domains (for example: adaptive mesh refinement, particle simulations, transient dynamics calculations, etc), and autotuning softwares. we furnish real-life use cases that highlight its importance below. they are detailed in the appendix b, supplemental."
"the basic simulation units of the distributed hydrological model used in this paper are grid cells. [cit] was used. this approach was based on the shared-memory programming model, in which threads were the basic scheduling units. to conduct parallel computing, hydrological processes are divided into two types: runoff generation processes and overland-flow routing processes. for runoff generation processes, calculations for different grid cells are independent; parallel computing can therefore be conducted simply by dividing grid cells into equal parts."
"the efficiency of the variable code length scheme is difficult to determine since it is related to the selected variable code length sequence, that is, the number of sid packets used to hide the secret information. generally, the larger the average code length ofl, the higher the transmission efficiency of secret information bits. the gray code results in low bit error rate and good robustness. in fact, when using the variable length code length scheme, the smaller the average code lengthl, the higher the bit error rate. accordingly, when the sender selects a variable code length scheme, different code length sequences will produce different error rates."
"because parallel computing at the sub-basin level is suitable to run on computer clusters, and at the basic simulation-unit level it is suitable to run on shared-memory hardware, multi-core cluster (i.e. a cluster having nodes with multi-core cpus), is selected as the hardware platform for the two-level parallel computation in this paper. with the development of multi-core cpus, most current computer clusters are multi-core clusters, which made the proposed method in this study widely applicable. the architecture of the two-level approach is as follows: first, parallel computing at the sub-basin level is conducted among multiple nodes in a computer cluster using the message-passing programming model, and then parallel computing at the basic simulation-unit level is conducted among multi-cores within each node using the shared-memory programming model (fig. 1) . it is worth noting that although a job management systems like pbs (portable batch system) could also be used to conduct the coarse-grained parallel computation, the intense interaction among parallel tasks at the sub-basin level requires a flexible communication mechanism and makes the message-passing parallel computation (e.g. using mpi (message passing interface)) a necessary choice."
"for each function, solid lines connect the points in its graph to highlight the variations. to make sure the experimental results are reliable, we follow a statistical methodology described in the appendix d. briefly, for every data point in the functions, the automation software executes the application repeatedly until the sample mean lies in the 95% confidence interval with a precision of 0.025 (2.5%). for this purpose, we use student's t-test assuming that the individual observations are independent and their population follows the normal distribution. we verify the validity of these assumptions using pearson's chi-squared test."
"the associate editor coordinating the review of this article and approving it for publication was xiaojiang du. locations (memory unit, resource status, network data packet, etc. [cit], and the receiver restores the information from the sender by observing the storage location. the covert timing channel means that the influence of the sender on system events (performance, behavior, etc. [cit] can be observed by the receiver, and the two parties use the sequence of events, interval, frequency and other time factors to transmit covert message. covert channels have both legitimate and malicious applications. an example of a malicious use of a covert channel is that criminals use it to leak the secret information of a business. an example of a legitimate application is that a network administrator can use covert channels to hide communications related to network management or to transmit authentication data. [cit] and archibald [cit] give more details on various malicious and legitimate covert timing channel applications."
an exascale architecture roadmap [cit] . the salient parameters for the platform are shown in the table 12 . we do not consider parallel fast fourier transform application since we do not have theoretical complexity of cost of communications for it. this we would address in our future work.
"the algorithm 8 returns the number of cells, myncells, in an anti-diagonal l belonging to process me and the number of cells, ncellsbeforeme, that precede the cells belonging to process me. the input ncells is the total number of cells in an anti-diagonal that are distributed between the q processes. this is also a local routine."
"a computer cluster with 1 head node and 4 compute nodes was used as the hardware platform. these nodes were connected by 1-gigabit ethernet. each node had 2 intel ® xeon e5645 cpus and each cpu had 6 cores. there were 48 cores in all in the compute nodes of this cluster. the centos 6.0 operating system, gcc 4.1.2 compiler and openmpi 1.3.3 were used."
"4.2.1. overall performance fig. 7 presents the execution time, speedup ratios and parallel efficiencies for the two-level parallel computation using different combinations of processes and thread numbers. a 90 m-resultion dataset with 67 sub-basins was used for the experiment. the number of cores in fig. 7 refers to the number of cores used in parallel computation, which was equal to the number of processes multiplied by the number of threads within each process. when there was one thread within each process, only a one-tier parallel computation at the sub-basin level was conducted. when there was more than one thread within each process, a two-level parallel computation was conducted."
"in our design, full deniability is achieved by the fact that the transmitted ciphertext can be generated by any user. however, the security of the one-time key can only be shared between the sender and the receiver. in addition to the two contradictory objectives of deniability and security, we also need to consider the property of authentication."
where h is the depth of water on the surface (m); t is the time (s); n is the manning coefficient; s 0 is the bed slope (m/m); x is the length of the slope (m); i is the rainfall intensity (m/s); f is the infiltration rate (m/s). this equation was solved using a combination of the 4-point implicit finite difference and newton's iteration method.
"due to the lack of high-resolution meteorological and discharge data in our study area, we just used designed storm instead of meteorological observation data to drive the model and test its parallel performance. in the future, we will collect more data and conduct real-world assessment of the parallelized distributed hydrological model. in addition, in the proposed two-level parallelization method, the number of cores used in a specific compute node equaled the number of processes multiplied by the number of threads within each process. before submitting an execution command to a multi-core cluster, users need to coordinate the number of processes running in each node and the number of threads within each process to avoid overburdening some compute nodes. this introduces some inconvenience for users. the next step would be to study the ease of software use and to utilize the multilevel parallelizability in hydrological models using only the message-passing programming model, which is the most widely used model for massive parallel computing. -basins (i.e., 67) . n p in this figure means the number of cores and n t means the number of threads within each process."
"in this section, we demonstrate the practical cost of applying paraleph in two data-parallel applications, openblas dgemm [cit] and fftw [cit] . we also demonstrate the tremendous speedups of the parallel algorithms over the sequential algorithms. we conclude with theoretical analysis of paraleph using parallel matrix-matrix multiplication on extreme-scale platforms."
"we experimentally demonstrated the low practical cost of our algorithms for two data parallel applications, matrix multiplication and fast fourier transform, on homogeneous extreme-scale multicore clusters. we show that the parallel algorithms exhibit tremendous speedups over the sequential algorithms. using simulations based on a forecast exascale platform, we show that the algorithms also have negligible execution times for large values of n and p."
"ii. signal and system model consider a wireless communication network comprised of a transmitter denoted by t, a receiver denoted by r, and an unintended user denoted by e. note that to obtain the secrecy rate, the legitimate user needs to be aware of the instantaneous channel to the eavesdropper. this knowledge for the most general case with a passive eavesdropper is not practical. in this work, the unintended user is assumed to be part of the network. therefore, the transmitter t is able to receive the training sequence from e, in order to estimate its channel. the signal model and the secrecy rates are derived in the following parts."
we present how the cost of communications during the execution of a data-parallel application employing our data partitioning algorithm can be seamlessly integrated in par-aleph.
"for such large values of p, the runtime cost is in the order of minutes and the memory cost is also high causing severe degradation of performance due to paging. therefore, these two prohibitive costs render them unsuitable for employment in self-adaptable applications executing on extremescale multicore platforms."
"we consider the execution of parallel matrix-matrix multiplication application based on summa [cit] and employing paraleph on a theoretical exascale platform published in data-parallel application, the execution time of paraleph, the time of communications in paraleph, and the speedup of paraleph over the sequential data-partitioning algorithms. 'f' indicates failure of the sequential algorithm."
"transcript . actually, the transcripts (r 2, c 1, c 2 ) (or (r 1, c 3, c 4 ) ) in simulation are indistinguishable from those of alice (or bob). therefore, bob (or alice) is not able to prove to a third party that the transcripts were produced by alice (or bob)."
"similarly, we can prove that proposed second protocol as illustrated in fig. 2 also achieve full deniability, since anyone can claim to be the creator of the digital envelope r 1 (or r 2 ) and knows the one-time secret key k 1 (or k 2 )."
"ike is the protocol used to set up a security association in the internet protocol security (ipsec) [cit] protocol suite. ike uses the dh-key exchange to set up a shared secret, from which cryptographic keys are derived. public-key techniques are used to mutually authenticate the communicating parties; alternatively, a pre-shared key can be used for this purpose. to allow for a variety of exchange methods, the ike protocol includes defined modes for the phases. here, we focus our analysis on key exchanges in the main mode."
"in order to perform a comparison, we also design the optimal beamforming vector to maximize the secrecy energy efficiency when zero-forcing (zf) strategy is used to null the received signal at the eavesdropper. using (7), the zf beamformer design problem can be defined as follows"
"in order to analyze the characteristics of volte traffic, we developed packet capture software for mobile devices to capture packets in the volte video calling. two samsung a5108 mobile phones that support volte calling are selected, where android system version is 5.1.1 and the kernel version is 3.10.61. the network environment is china mobile 4g network. figure 1 shows the ipds of volte voice traffic. comparing (a) and (b), we can see the difference between the ipds of the sender and the receiver. for voice traffic, as shown in figure 1, the ipds of the voice packets sent by the sender are basically 20ms. the ipds of the voice packet at the receiver vary greatly from the ipds at the sender, and the regularity is not obvious. jitter is the amount of network delay variation. it is generated by any two adjacent packets of the same application during network transmission. jitter is very important for real-time applications, and any type of streaming media is susceptible to jitter. excessive jitter is usually a symptom of a network congestion or insufficient bandwidth to handle traffic. figure 2 shows the jitter of volte voice traffic. the voice traffic jitter varies greatly (−36.3 ∼ 69.9)ms, and the average value is -0.08ms. there is no regular change in the voice traffic jitter. by analyzing the ipd and jitter of volte traffic, it can be seen that the ipds of volte traffic are limited to a small range and have obvious regularity, and both video traffic and voice traffic have a large jitter variation. according to the characteristics analysis of volte traffic, since the ipds of volte traffic are obviously regular when the sender sends, if the traditional ipd-based covert timing channel construction method is used, the modulation and modification of the ipds of the volte traffic will be easily detected by the opponent. at the same time, due to the large variation of the jitter of volte traffic, the ipd-based covert timing channel construction scheme will also face challenges in decoding and accurate secret information cannot be obtained, which will result in high bit error rate. these features make the ipd-based hidden channel construction method not suitable for volte traffic. therefore, this paper proposes a two-way covert channel construction method for volte traffic, which can ensure the constructed covert channel undetectable and robust."
"process will check whether the downstream sub-basin is in the same process. if so, the routing results of the sub-basin are saved in memory in order to be used by its downstream sub-basin; otherwise, the routing results are sent to the master process so that they can be transferred to the slave process that needs them."
"according to the analytical procedures of ban logic, each round of the protocol must be transformed into an idealized form. first, we describe some notations of ban logic in table 2 ."
"remark. in order to provide 1-out-of-∞ deniability with confidentiality and authentication, the key exchange method of ike, revised public-key method (section 2), which employs techniques of both dh-key exchange and digital envelope. but this hybrid technique increases the computational overhead. in our design examples, we showed that using either the dh-key exchange method or the digital envelope can provide 1-outof-∞ deniability with confidentiality and authentication. (see table 1 .)"
"at the end of the execution of popta, the optimal workload distribution is returned in d opt and the optimal execution time is returned in e opt . volume 6, 2018"
"(ii) analyses of the deniability of the following security standards are provided: pretty good privacy (pgp) [cit], secure/multi-purpose internet mail extensions (s/mime) [cit], secure socket layer (ssl) protocol [cit] and ike [cit] . (iii) the proposed design can be implemented by using any public-key cryptography. (iv) the proposed design is illustrated by two practical examples. (v) analyses of the correctness of the proposed protocols are conducted using logical rules."
"to analyze the performance of the proposed covert channel, we used volte traffic between two mobile phones. as long as the devices and systems support volte, our solution can be successfully implemented on different mobile devices and different versions of android. we chose two samsung a5108 phones, of which the android version is 5.1.1 and the kernel version is 3.10.61. we test our solution by the two phones as sender and receiver. we capture the volte voice traffic of the sender and receiver in the experiment. due to that the existing software cannot capture the volte voice packets processed by the baseband program, we have developed a capture program based on the android kernel. according to our solution, covert traffics are generated by encoding and modulating overt traffics."
"1) static energy consumption is a hard constant (or a inherent property) of a platform that can not be optimized. that is, it does not depend on the application configuration and will be the same for different application configurations. 2) although static energy consumption is a major concern in embedded systems, it is becoming less compared to the dynamic energy consumption due to advancements in hardware architecture design in hpc systems. 3) we target applications and platforms where dynamic energy consumption is the dominating energy dissipator. 4) finally, we believe its inclusion can underestimate the true worth of an optimization technique that minimizes the dynamic energy consumption. we elucidate using two examples from published results."
"so, for each data point, the function meanusingttest is invoked and the sample mean mean is returned at the end of invocation. the function measure measures the execution time or the dynamic energy consumption using the library [cit] based on the input, time or energy."
"3.2. parallel computing at the sub-basin level 3.2.1. assignment of parallel tasks parallel computing at the sub-basin level was conducted using message-passing programming models, with parallel tasks dispatched to different nodes in a cluster to run in parallel. process is the basic scheduling unit for message-passing parallel computing, and one or several processes run on a node. for the assignment of parallel tasks, the amount of computation in each process should be distributed as evenly as possible, while the amount of communication should be as minimal as possible. there is a tradeoff between these two goals, and this problem has been proved to be np-hard [cit] ."
"before we present the formulations, we would like to define the meaning of the terms ''problem size'' and ''workload size'' used in this work. these two terms are used synonymously in the literature. the problem size is defined as a set of one, two or more parameters characterizing the amount and volume 6, 2018 layout of data stored and processed during the execution of a computational task. it also represents the size of a computational task that is allocated to a processor during the parallel execution of a data-parallel application. the workload size is defined as the size of the workload of the data-parallel application that is executed using one or more processors. it is a multiple of one or more computational tasks, whose size is defined to be the problem size. by data-parallel workloads, we mean computations involving dense objects such as dense matrices or grids (for example, dense linear algebra routines such as matrix-matrix multiplication of dense matrices, fast fourier transform of a dense signal matrix, etc.)."
"and confidentiality based on dh-key exchange in ssl protocols, there are three algorithms that use the dhkey exchange with confidentiality, i.e. one with full deniability but no authentication, one with 1-out-of-2 deniability and authentication, and one with no deniability and authentication. here, we propose a protocol with full deniability that also has message authentication and confidentiality."
"where s is the speedup ratio; t s is the serial execution/computing time; t p is the parallel execution/computing time. parallel efficiency is defined as the ratio between the speedup ratio and the number of cores used in the parallel computation (rauber and rünger, 2010 ) and the equation is as follows:"
"ensuring voice quality is important to mask the existence of covert channels, especially for volte calling that handles high-definition voice communications. in addition, a number of methods provided in the literature on multimedia analysis of detection require that the covert channel should fully maintain the overall quality of the call. therefore, in order to evaluate the impact of embedding covert message in a volte session, we performed a mean opinion score (mos) and discussed the appropriate length of secret information bit embedding."
"in this section, we describe the terms related to energy predictive models used in this work. we also explain the rationale behind using only the dynamic energy consumption in our problem formulations and algorithms."
"to analyze our first authentication protocol, we made some assumptions without loss of generality, as follows: the assumptions a1 1 and a1 2 are basic assumptions of ban logic. we analyze the idealized form of the proposed protocol using the earlier-mentioned assumptions and the rules of ban logic. we show the processes of the proof as follows:"
"we use gray code to mitigate channel noise since gray code is characterized by two consecutive values that differ only in one bit resulting in robustness to packet reordering and packet loss. the covert message is encoded by the gray code according to the variable length l determined by the number of sid packets in the silence period. the covert information bits are encoded into code symbols with a variable length of l, where each silent period carries one piece of information bits of length l. the larger the average bit length of the symbol, the higher the transmission efficiency of the covert message. therefore, if you want to increase the capacity of the covert channel, you can select the voice traffic with a longer average silence period as the carrier."
"consider a data-parallel application workload of size n executed using p number of identical processors. let the speed function of a processor executing a problem size x be represented by s(x). here the speed can be measured in floating point operations per second or any other fixed-size computation units per unit time. the size of workload can be characterized by the problem size (for example, the number of cells in the computational domain or the matrix size) or just by the number of equal-sized computational units. the speed s(x) for a problem size x is calculated as"
"we verified experimentally that the optimal solutions returned by the sequential and parallel data partitioning algorithms (popta, eopta, paraleph ) are the same. tables 4, 5 and 6, 7 shows the execution times of paraleph in dgemm solving optimization problems for performance and energy. tables 8, 9 and 10, 11 shows the execution times of paraleph in fftw solving optimization problems for performance and energy."
"hendrickson and devine [cit] survey approaches addressing the dynamic load balancing problem that arises in computational mechanics applications where the computation must adapt dynamically during the simulation (for e.g., adaptive mesh refinement, particle simulations and transient dynamics calculations). among the essential properties identified by them that a dynamic load balancer should possess are parallel speed and modest memory usage."
"the execution time and the energy functions are built separately experimentally using an automated build procedure using 144 parallel processes where one process is mapped to one node. to make sure the experimental results are reliable, an experimental methodology described in detail in the appendix d is used. the inputs to the automation procedure are the application and application parameters (problem size, number of threads, etc), range of problem sizes, and granularity. to obtain a data point for each function, the software executes the application repeatedly until the sample mean lies in the 95% confidence interval with precision of 0.025 (2.5%). for this purpose, we use student's t-test. the software outputs a set of points, which represents the discrete function. the total dynamic energy consumption during the application execution is obtained using watts up pro power meter."
"v. trade-off between ζ and η in this section, we study the trade-off between secrecy energy efficiency and secrecy spectral efficiency (i.e. ζ and η) for miso and siso systems."
"the remainder of our paper is organized as follows: in section ii, we review related work on existing covert channels. we present the preliminaries including volte traffic analysis and gray code in section iii. then, in section iv, we give an overview of the proposed two-way volte covert channel. we show how to construct a two-way covert channel that meets performance requirements in section v and discuss the performance evaluation including undetectability and robustness in section vi. we provide the experimental results and analyses in section vii. finally, we conclude with discussion and future research directions in section viii."
"in pgp and s/mime applications, each user is assumed to have two pairs of public and private keys selected for long-term use. one pair of keys is used for message encryption, and the other pair is used for the digital signature. it is assumed that the public keys of all communication partners already have been stored securely in each user's public-key ring."
proposition a.9. the proposed protocol as illustrated in fig. 1 achieves the property of deniability. proof. we prove that all transcripts transmitted between alice and bob could be simulated by anyone else as follows.
"with the in-depth deployment of lte networks and the popularity of smartphones, data services are beginning to emerge. lte has successfully penetrated into the cellular communication market and has become the mainstream of communication technology. as an all-ip technology, lte has many advantages, including robustness, low latency, and high bandwidth. however, since all-ip networks are inherently incompatible with voice processing and cannot utilize traditional circuit-switched-based voice services, there are many challenges in providing voice services based on an all-ip architecture. to meet the diverse and personalized needs of users, operators need volte to provide innovative and rich data services such as hd voice and video calling. lte networks provide higher data capacity and lower latency for mobile broadband. to compensate for the shortcomings of circuit-switched voice, volte is widely used in the mobile industry as an ip-based lte voice and video calling solution. it is a global interoperability solution that provides the advanced and innovative communication service."
"the infiltration process was simulated using a quadratic approximation of the green-ampt method [cit] . the interception and depression processes were simulated using methods in the wetspa model [cit] . for simulation of the interception process, the fill-and-spill mechanism was used and the maximum interception storage was calculated using a statistical equation containing leaf area index, vegetal species, and date [cit] ."
"at present, the research on the network covert channel for real-time interaction mainly focuses on constructing covert channels for internet-based voice over ip (voip) voice and video traffics. with the rapid development and popularity of long term evolution (lte) networks, voice over lte (volte) provides a smooth transition path from hybrid network voice services to ubiquitous all-lte network voice. although the emergence of internet-based voice services such as skype, wechat, and google talk poses new challenges to the cost and portability of volte, volte outperforms these voip applications in terms of user experience and quality of service. for example, ip traffic overload in networks and devices will affect voip service quality, which is not the case in volte, and volte provides better interoperability. in addition, the mobile network is about to enter the 5g era, and volte fits well with 5g, making it the foundation for carrier-grade voice and video calling services in future 5g networks. mobile communication brings many benefits, but also brings threats such as information leakage, and covert channels can be an effective means to transmit confidential information in mobile networks. therefore, it is necessary to study the construction method of volte covert channels."
"we use ks test, a standard statistical test, to visualize and verify undetectability. in figure 5, the ks p-values are all greater than 0.05 for the number of sid packets in the silence period of the five overt traffic, which indicates that the number of sid packets of these traffics fit the same distribution. on the other hand, figure 5 also shows that ks p-values are all greater than 0.05 for the number of sid packets of the five corresponding covert traffics which represents that the covert traffics fit the same distribution with the overt traffic. figure 6 shows the kld test results for the number of sid packets in the silent period of the overt and covert traffics. as can be seen from figure 6, the kld values of the overt traffics are in the range of (0.047 ∼ 0.312), and the kld values of the covert traffics are in the range of (0.003 ∼ 0.013). obviously, the kld values of the numbers of sid packets in the silent period of all covert traffics are lower than ones of the public traffic, and the kld values of almost all covert traffics are less than 0.01, indicating that the distributions of the sid packets of these covert traffics are very close to the distributions of the original overt traffics."
"the optimal beamforming vector for η ⋆ shall be the same as for ζ ⋆ and the optimal value of η can be derived similar to the one for ζ. hence, the pair (ζ, η) is available."
"for a main step, if the execution time of the parallel execution (max(t r, t l )) is less than the t opt, we save the improved solution (lines 37-39). for each problem size n l solved using p − r processors, the ending index l, which contains the range of points already examined, is saved (line 44). so, if an invocation for solving this problem size recurs, then recursion is avoided using the memorized arrays (line 26). therefore, this memorization ensures that the total number of examined points (including those in the recursive invocations) for a point on a line lx,"
"the speedup ratio and parallel efficiency were used to measure the performance of the proposed method. the speedup ratio is defined as the ratio between the serial computing time and the parallel computing time (rauber and rünger, 2010) and the equation is as follows:"
"the communication between alice and bob, shown in fig. 1, includes the following processes: if the earlier-mentioned processes can be executed successfully, both message authentication and confidentiality between alice and bob can be ensured."
"the research focus in this work is focused entirely on traditional hpc and does not target data analytical or big data applications such as mapreduce, hadoop, spark, etc. we believe that mapreduce like applications are more applicable to domains that are highly data-driven and not computedriven in the sense that the ratio of in-memory compute times to the data processing times (due to specialized storage in filesystems such as hdfs, etc) is low. our research work, however, is mainly directed towards scientific applications with high in-memory computational complexity and, therefore, the most important concern is to reduce this complexity."
"for the type 3 computational tasks, only those sub-basins with available upstream inflows could be processed, and a queue was used to record these sub-basins. there is a loop to add processable sub-basins to the queue and then conduct the channel-routing calculations of sub-basins in the queue. if the queue becomes empty and there are still sub-basins to be processed, the slave process will send a request for upstream inflows to the master process and wait for responses. when the needed upstream inflow data from the master process are received, the loop will continue until all the sub-basins are processed."
"there are two types of deniability: plausible deniability and full deniability [cit] . for plausible deniability, the message sender only can deny the transmission of a particular message. however, the sender is unable to deny the fact that he/she has communicated with the other participant. on the other hand, full deniability allows the message sender to totally deny that he/she has communicated with the other participant. in this article, we explain how our proposed protocols can achieve full deniability."
"the proposed data-partitioning algorithms are, however, sequential, recursive, and have high practical runtime and memory costs for large values of p (in the order of hundreds)."
"in this section, the overt voice extracted from the original overt traffic is compared with the covert voice extracted from the covert traffic embedded with covert message, and then the call voice quality is evaluated. voice quality is assessed using mos voice testing, which considers speech items that are extracted from five overt traffics and corresponding covert traffics. figure 9 gives the mos value and the maximum and minimum range of the speech quality of the overt voice and the covert voice. as can be seen from the figure 9, the overt voice has a higher mos value as expected. the comparison between the overt voice and covert voice mos value shows that the covert voice mos value is very close to the overt voice mos value, which indicates that the voice quality of the covert voice is very good. the voice quality of the covert traffic is good since the constructed covert channel has no influence on the packets containing the voice information, and only the length of the silent period has a certain change. obviously, as the average number of sid packets in the silent periods increases, the average code length ofl increases, and the voice quality gradually deteriorates. therefore, if the undetectability of the covert channel is to be maintained, a smallerl must be chosen to ensure that the adversary cannot detect the existence of a covert channel by voice quality."
"the formulation for optimization problem for energy is based on an energy model, which represents the dynamic energy consumption of a processor by a function of problem size. the dynamic energy consumption of execution of a problem size x by a processor is represented by (x). we explain the rationale behind using dynamic energy consumption in appendix c. since the processors involved in the execution of the workload are identical, the input to the problem is a single energy function. we do not specify how to build the energy function. it may be constructed using one or more processors."
"at the same time, the receiver-to-sender covert storage channel feedbacks the transmission of the covert message through rtcp packets. if the feedback information indicates that the current bit error rate is high, the sender will adjust the covert timing channel modulation parameters or even suspend the delivery of the covert message to resist the active attack of the adversary."
"in this section, we maximize ζ in a miso system by obtaining the optimal beamformer for the cases with and without qos constriant at the receiver."
where p c is the circuit power consumption. we define our problem so as to maximize the secrecy energy efficiency subject to the peak power and qos constraints as follows
"in order to build an effective covert channel, many research solutions have been proposed [cit] . however, the existing covert channel construction scheme based on inter-packet delay (ipd) cannot be directly applied to volte because the ipds of volte traffic is limited to a small range and has strong regularity, which makes the modulation of ipds easy to detect, and it is difficult to hide covert message into the ipds of volte traffic. therefore, based on the research of volte traffic characteristics, we propose a volte twoway covert channel, which includes a sender-to-receiver covert timing channel that modulates covert message through actively dropping packets during the silence periods and a reverse covert storage channel that hides the acceptance of the covert message as feedback information into the feedback control information field of the rtcp packet. in order to verify the effectiveness of the covert channel created, we conducted experiments and analysis in the real volte environment, and gave the test results. in this paper, the research on the construction method of volte covert channel will provide useful reference for construction and detection technology of covert channels over mobile networks."
"3.2.2.2. master process. there are two stages in the execution of the master process: the task-allocation stage and the message-listening stage. during the first stage, the master process allocates computing tasks to slave processes according to the task assignment solution obtained from the metis algorithm. as there are close data connections between the hillslope processes and the channel-routing process in the same sub-basin (e.g., lateral flow from hillslope to channel), the computational tasks of one subbasin are dispatched as a whole from the master process to slave processes. once the task allocation is complete, the master process enters the message-listening stage, in which it continues to receive and respond to messages. there are three cases that vary according to the type of messages received from slave processes:"
"we demonstrate the low runtime cost of paraleph using experiments performed in the grid'5000 platform hosted in france (http://www.grid5000.fr). the platform contains 24 clusters distributed over 10 sites (nine in france and one in luxembourg), which includes 1006 nodes, 8014 cores. we used the graphene cluster in nancy site for our experiments. we used a total of 576 cores from 144 nodes. each node has a disk of 298 gb storage, 16 gb of memory, and a quad-core intel xeon x3440 cpu. the nodes in the cluster are interconnected via 20 gb/s infiniband. for the mpi communications, openmpi-1.6.5 is used. gcc compiler version used for compilation is 4.9.2."
"in this paper, sub-basins were treated as a graph ( fig. 2 ) and the widely used metis graph-partitioning algorithm [cit] was used for the assignment of parallel tasks. each sub-basin was represented as a node in the graph. the weight of the graph was defined as the number of grid cells in this sub-basin, because generally a higher number of grid cells requires more execution time. the dependence relationship between two subbasins was represented as an arc in the graph. for the distributed hydrological model used in this paper, there are only dependencies between upstream and downstream sub-basins due to channel routing."
"in the revised public-key method, each party generates a onetime dh public key and encrypts this key under a one-time secret key to produce c 1 . the one-time secret key is encrypted using the other party's public key to create a digital envelope as c 2 . the pair (c 1, c 2 ) is sent to the other party. after receiving (c 1, c 2 ), the digital envelope c 2 can be opened with the corresponding private key, and, then, the one-time dh public key can be obtained. these two one-time dh public keys are combined to generate the common secret between the two parties. the digital envelope enables the sender and a specified receiver to share a secret. since the sender can be any user, the digital envelope provides 1-out-of-∞ deniability. by using the digital envelope in both communication directions, the sender and the receiver can share a common secret. thus, this method can achieve confidentiality and authentication with 1-out-of-∞ deniability."
both overland-flow routing and channel-flow routing were performed sequentially from upstream to downstream according to the single-flow direction defined by the d8 method [cit] . surface flow was simulated by a onedimensional kinematic wave model combined with the manning's equation [cit] . the equation for surface-water depth is
"(i) a new model of deniability, called 1-out-of-∞ deniability, that can provide full deniability is proposed. a fully deniable protocol allows the originator of the message to deny generating the message since the number of potential generators is infinite. in addition, messages can be protected and authenticated between the sender and the intended receiver."
"we measure the robustness of covert channels with ber of the decoded covert message. in figure 7, the ber of the proposed covert channel reaches 10 −3 when r l is less than 0.2%, whereas the ber increases to 10 −1 when r l is greater than 5%. the sort of performance for all covert traffics are similar in different network conditions. in summary, the proposed covert channel remains valid even under high network jitter. specifically, the covert channel with larger average number of sid packets can achieve better robustness."
"this paper proposed a two-level parallelization method for distributed hydrological models, which can utilize parallelizability at both the sub-basin level and the basic simulation-unit level (e.g., grid-cell level) simultaneously using a multi-core cluster as the hardware platform. the basic concept of the proposed two-level approach is to first dispatch parallel tasks at the sub-basin level to multiple nodes in a computer cluster in order to conduct parallel computation using the message-passing programming model. parallel tasks at the basic simulation-unit level are then dispatched to multi-cores within each node to conduct parallel computation using the shared-memory programming model. to illustrate the method, a grid-based distributed hydrological model was parallelized as an example. the performance of the parallelized model was tested in different scenarios. the results showed that the proposed method had better performance than parallel computation at the sub-basin level alone, and the parallel scalability increased with data volume and the number of sub-basins."
"in the ssl protocol, there is one algorithm based on rsa digital envelope, but this method does not provide authentication. the digital envelope is one useful method used to provide message confidentiality. below, we show that digital envelopes can also be used to provide both message confidentiality and authentication with full deniability."
"to be truly fair, full deniability should be referred to as 1-out-of-∞ deniability, meaning that there are an infinite number of possible message generators. the computational complexity of a practical, fully deniable protocol should be as simple as a normal message authentication protocol. in this article, we propose two fully deniable communication protocols with message confidentiality and authentication. these proposed protocols are computationally efficient, can provide full deniability for the message originator and ensure the confidentiality and authentication of the message."
"in the pre-shared key method, the sender and the receiver have shared a secret key during the initialization. then, these two parties exchange random nonces. their common secret is calculated using a keyed-hash function of nonces and the preshared secret. since the pre-shared secret is known by both entities, this key exchange method can achieve confidentiality and authentication. in addition, this method can provide 1-out-of-2 deniability."
"the two-level parallelization method is a combination of parallel computing at the sub-basin level and at the basic simulationunit level. to illustrate the implementation of this method, a gridbased distributed hydrological model was parallelized as an example."
"security is one of the most important services in various network communications. in most secure communications, the following two basic security properties are commonly considered."
"if a covert timing channel is available for any distinguisher d with a runtime polynomial in δ, and such that adv d (d n, d n ) is negligible in δ for any n with a polynomial in δ, then the covert timing channel is undetectable for the security parameter δ. [cit] the probability mass function (pmf) of the code symbol s i is given by the formula 2"
"then, we determine the lines b l and e l for the recursive invocation of popta solving the problem size n l to the left of b using p − r processors using the function, getbe (line 19). we invoke popta to solve the problem size n l to the left of b using p − r processors only if n l is not divisible by p − r and the execution time of the point b l given by e l in the recursive invocation is greater than the execution times of the points beyond it (line 21). this can be determined using the sorted arrays, (x ↑, s ↑ ). otherwise, the optimal workload distribution is given by the point b l for the recursive invocation (lines 32-34). if the memorized execution time of the recursive invocation (t l ) is less than or equal to e l, then we use the memorized workload distribution and avoid recursion (line 26). essentially, if a range of points have already been examined, then they will not be re-examined due to the memorization. for the recursive invocation solving the problem size n l to the left of p using p−r processors, the lines b and e are set in b l and s(b l ) but less than or equal to l x ) because those points will have worse execution times. that is, when popta is considering the points on a line l x, this line will always be the limiting line for the recursive invocations."
"volte voice services exist in two states: talk spurts and silence period. in talk spurts, the sending interval of the voice packet is 20ms and the voice packet size depends on the currently used coding rate. in silence period, the sending interval of sid (silence insertion descriptor) frame is usually 160ms."
"in distributed hydrological modeling, watersheds are divided into different types of simulation units, such as sub-basins, grid cells and hydrological response units [cit] . it is a common way to dispatch computing tasks of different simulation units to multiple processors for parallel computing . for example, parallel-computing methods at different levels (e.g., sub-basin or grid cell) using different types of computer hardware (e.g., cluster or multi-core cpu [central processing unit] ) have been proposed and proven to be effective with speedup ratios ranging from 2 to 80 [cit] ."
"the rest of this article is organized as follows. in section 2, we present our analysis of the deniability of some wellknown security standards. in section 3, we describe our design concept and two design examples that provide confidentiality and authentication. in section 4, we provide the analysis of the proposed protocols. our conclusions are presented in section 5."
"to summarize, on platforms composed of uniprocessors, the shapes of the performance and energy functions are smooth with minimal variations. the performance functions comfortably satisfy the conditions imposed by the fpms that are crucial for the correct operation of the load balancing algorithms."
"the concept of covert channels was first proposed by lampson [cit], who saw covert communication as a process of communicating data through a transmission channel that is neither designed nor expected. the emergence of information theory, coding theory, and high-performance systems interconnected by high-speed networks has led to the development of covert channels, especially covert timing channels, from conceptual ideas to potential practical tools. the network covert channels that use the network communication medium to transmit information in a covert manner have become the focus of covert channel research. with the security threat of privacy in new environments such as mobile networks and cloud computing [cit], covert channels have attracted more attention."
"the channel-routing results of upstream sub-basins are needed for the channel-routing calculation of their downstream subbasins. so when a sub-basin's type 1 and 3 computational tasks, which contain channel-routing processes, are complete, the slave 3 . classification of parallel tasks for distributed hydrological modeling at subbasin level. type 1: computational tasks of hydrological processes (including both hillslope and channel-routing processes) in headwater sub-basins; type 2: computational tasks of hillslope processes in non-headwater sub-basins; type 3: computational tasks of the channel-routing process in non-headwater sub-basins. rectangles represent hillslope processes; circles represent channel-routing processes."
"ssl is an interactive protocol that provides confidentiality and data integrity for communications over tcp/ip networks. ssl has become a widespread security technology that is used in client-server applications, such as web browsing, internet commerce and voice-over-ip (voip). the ssl protocol supports three kinds of dh-key exchange modes, i.e. two authenticated modes and one unauthenticated mode. dh-key exchange allows the client and the server to establish a common secret key by exchanging public information over an insecure channel. the general goal of the key exchange process in ssl is to establish a pre-master secret known only to the two participants. the pre-master secret will then be used to derive keys for message confidentiality and mac keys for message authentication. in unauthenticated (anonymous) ssl mode, the pre-master secret is determined by the short-term dh public keys exchanged between the client and the server. since the short-term dh public keys are unauthenticated, this protocol can provide 1-out-of-∞ deniability. however, anonymous dhkey exchange might suffer from the man-in-the-middle attack. in authenticated mode, the pre-master secret is determined either by fixed dh public keys with digital certificates or by short-term dh public keys signed by signatures (also called ephemeral dh), which are exchanged between the client and the server. in the fixed dh public keys with digital certificates, both participants know the common pre-master secret; so this protocol can provide 1-out-of-2 deniability. the main problem of this protocol is that the pre-master secret is never changed. this feature increases the risk of exposing the pre-master secret. in the short-term dh public keys signed with digital signatures, the pre-master secret is different dynamically. however, since each participant signs the short-term dh public key, this protocol provides no deniability."
"the performance of two-tier parallel computation with different volumes of data was tested using datasets of different resolutions (i.e., 270 m, 90 m and 30 m) with a fixed number of sub-basins (67) and the results are shown in fig. 9 . the figure shows that data volume had a direct influence on parallelization performance. the larger the dataset, the higher the speedup for a given number of cores. this is because the time spent on actual calculation increased with data volume, which made the communication overhead decrease at both the sub-basin level and the grid-cell level, leading to better parallelization performance."
"unfortunately, the 1-out-of-2 deniable protocol has one potential problem. when both parties are allowed to deny generating the message, a dispute might occur between these two parties. this can be an issue because, when a dispute occurs between two parties, the general public often makes a subjective judgment against the party who has made prior mistakes, such as a criminal record or a bad credit history. thus, the 1-outof-2 deniable protocol can result in an unfair resolution of the dispute."
"the remainder of the paper is organized as follows. in section ii, we introduce the system model. the optimization problems are defined and solved in sections iii and iv. in section v, the trade-off between secrecy energy efficiency and secrecy spectral efficiency is studied. numerical results are presented in section vi, and the conclusion is drawn in section vii."
"the construction methods of covert timing channels are mainly based on ipd and packet reordering. the ipd-based covert timing channels are implemented by modulating the inter-packet delay of the data packets of the overt traffic. on the other hand, the covert timing channel based on packet reordering are built by encoding covert message into a sequence of data packets in a single stream or multiple streams."
"therefore, the variation observed is not noise but is an inherent trait of applications executing on multicore servers with resource contention and numa. in a function (speed or energy f ), it is the difference of function values between two subsequent local minima (f 1 ) and maxima (f 2 ) defined as following:"
"in light of these factors, the computational tasks at the sub-basin level were divided into three types ( fig. 3): (1) the computational tasks of hydrological processes (including both hillslope and channel-routing processes) in headwater sub-basins; (2) the computational tasks of hillslope processes in non-headwater subbasins; (3) the computational tasks of the channel routing process in non-headwater sub-basins. the first two types of computational tasks are independent of each other, so they can be conducted in parallel directly. for the third type of computational task, there are dependent relationships between upstream and downstream subbasins, so computational tasks should be conducted following a sequence from upstream to downstream. in other words, the computational tasks of a downstream sub-basin cannot be conducted until the computations for all of its upstream sub-basins have been finished. if adjacent upstream and downstream subbasins are not in the same group of parallel tasks, communication is needed."
"return (d opt, t opt ) 16 : end procedure optimal as its energy consumption of workload n using the p processors cannot be improved. the optimal distribution may utilize number of processors (q) less than or equal to p."
"the algorithm popta (algorithm 1) solves popt. the inputs to popta are the size of the workload, n, given as multiple of x, the number of processors, p, the minimum granularity, x, and the speed function represented by two discrete sets, x and s, containing problem sizes and speeds. m is the cardinality of the sets x and s. the outputs are the"
"a covert storage channel is built for feedback from the receiver to the sender, and it hides the acceptance of the covert message as feedback information into the feedback control information field of the rtcp packet back to the sender."
"to simplify the exposition, we will consider parallel matrix multiplication application employing summa algorithm [cit] . we will use an analytical model (hockney) to parameterize the cost of communications, (c n,p ), to solve a problem size n using p processors. the execution time of communications considering that summa uses scatter-allgather broadcast is estimated using the hockney model [cit] . scatter is implemented using a binomial tree and allgather, a ring algorithm in which the data from each processor are sent around a virtual ring of processors in p − 1 steps."
"in ssl, there is another key exchange based on the rsa scheme. in rsa key exchange, a digital certificate for the server's public key must be made available. the client selects a pre-master secret randomly and then encrypts this pre-master secret with the server's rsa public key to create a digital envelope. since the digital envelope can only be decrypted by the server's corresponding private key, this method protects the confidentiality of the pre-master secret. however, there is no authentication for the sender of the digital envelope. this key exchange method provides 1-out-of-∞ deniability."
"in the digital signature method, the sender and the receiver must produce public-key certificates to verify the digital signatures. the digital signatures of all messages that are exchanged are used for authentication. then, these two participants exchange nonces and one-time dh public keys. finally, the common secret between these two participants can be calculated by a keyed-hash function of nonces and the one-time dh public keys. the use of the digital signature in this method allows confidentiality and authentication, but this method does not provide the property of deniability."
"here, we consider a multiple-input single-output (miso) and a single-input single-output (siso) scenario while a singleantenna unintended receiver, which is part of the network, is listening. the secrecy rate over the power ratio, named \"secrecy energy efficiency\" and denoted by ζ, is maximized with and without considering the minimum required secrecy spectral efficiency, denoted by η 0, at the destination. for comparison, we derive the optimal beamformer when zeroforcing (zf) technique is used to null the signal at the eavesdropper with considering the minimum required secrecy spectral efficiency. note that the zf can only be used for the miso scenario. furthermore, the trade-off between ζ and secrecy spectral efficiency, denoted by η, is studied."
"in this article, we propose a new concept of full deniability, called 1-out-of-∞ deniability. with 1-out-of-∞ deniability, when the sender sends an authenticated and encrypted message to the receiver, the sender can deny transmitting this message since anyone else could have generated the transmitted message."
"in general, the execution time decreased and the speedup ratios increased with the number of cores used in parallel computation for all the experiments. however, for the parallel computation at sub-basin level alone (i.e., the experiment with 1 thread per process), the speedup ratios reached a plateau when the number of cores exceeded 24. for the two-level parallel computation, the scalability was better than the parallel computation at sub-basin level alone. when the number of cores exceeded 24, the speedup ratios of two-level parallel computation were larger than those of the parallel computation at sub-basin level alone. this is because the parallel efficiencies at the sub-basin level were limited by the load imbalance of computing tasks and communication overhead at the sub-basin level. the more the number of tasks that were divided at sub-basin level, the worse the load imbalance and communication overhead problem will become, thus the parallel computation is less efficient. by utilizing parallelizability at a finer level (the grid-cell level in this case) as well as that at the sub-basin level, two-level parallel computation required a smaller number of parallel tasks at the sub-basin level to make use of a given number of cores compared to parallel computation at the sub-basin level alone. in this way, the load imbalance and communication overhead problem could be relieved. the speedup ratios of the two-tier parallel computation would reach a plateau when the number of cores used in parallel computation exceeded a larger threshold. this threshold, on one hand, depended on parallelizability at the sub-basin level. the shape of the watershed and the number of sub-basins affected parallel performance [cit] . on the other hand, this threshold depended on parallelizability at the grid-cell level. if the scalability of parallel computation at the grid-cell level was good enough, the higher the number of threads within each process, the better the scalability of the two-level parallel computation. although the threshold still existed, the scalability of the two-level parallel computation was effectively improved compared to that of the parallel computation at sub-basin level alone."
"in any public-key cryptosystem, the common session key sk can be constructed by the shared secret between the communicating parties. there are two usual approaches that can be used to share a secret between two parties, i.e. the dh-key exchange method and public-key based encryption. in the dh-key exchange method, the shared secret can be determined by exchanging short-term public keys between two parties. in public-key encryption, a participant is responsible for selecting the secret and then encrypting it under the other participant's long-term public key to create a digital envelope. note that only the receiver can open the digital envelope with the corresponding private key."
". the procedure to solve (19) using dinkelbach method is summarized in algorithm 1. using the closed-form solution of (20) given in (24), the following recursive relation is used to merge steps 3 and 4 of algorithm 1 as"
"to find the trade-off between ζ and η, we solve the optimal beamforming design problem to maximize ζ and η separately for a specific power constraint, p . as a result, the pair (ζ, η) is available for different values of p . for ζ, the optimization problem is as follows"
"before we present the comparison of execution times of the data-parallel applications and the data partitioning algorithms, we present a brief on how the execution time and energy functions are built since these are input to the data partitioning algorithms. the execution times of the algorithms do not include the cost of building these functions since they are assumed to be the inputs."
"both pgp and s/mime use a digital envelope to provide message confidentiality. a digital envelope is a technique used by the sender to transmit the message in such a way that only the intended receiver can read the content of the message. first, the sender selects a session key randomly and uses this session key to encrypt the message. then, the sender uses any public-key encryption algorithm to encrypt this session key by using the receiver's public key. after receiving the encrypted message, the receiver uses her/his private key to decrypt the message and obtain the session key. then, the receiver uses the session key to decrypt the ciphertext. this approach for achieving message confidentiality provides 1-out-of-∞ deniability, since anyone can be the generator of the digital envelope."
"in appendix f, we provide examples of classes of applications where our data partitioning algorithms can be employed. from our experience, there are multiple classes of applications that benefit from our data partitioning algorithms. a dominant class contains applications where the speed of an application is a function of problem size, which is defined as a set of one, two or more parameters characterizing the amount and layout of data stored and processed during the execution of a computational task."
"to elucidate the first challenge, we compare the typical shapes of real-life scientific data-parallel applications on platforms consisting of uniprocessors and modern multicore cpus. for this purpose, we select two widely used and highly optimized scientific routines, dense matrix-matix multiplication (openblas dgemm) [cit] and fast fourier transform (fftw) [cit] ."
"the function meanusingttest, shown in algorithm 5, describes this step. for each data point, the function is invoked, which repeatedly executes the application app until one of the following three conditions is satisfied:"
"the existing covert channel solution based on ipds cannot be applied to volte due to the limited and regular ipds. therefore, we proposed a volte covert timing channel that modulates covert message through actively dropping sid packets during the silence periods and a reverse covert storage channel that embeds the acceptance of the covert message as feedback information into rtcp packets. to improve the robustness, we use gray code to encode the covert message for mitigating the effects of packet loss. to remain undetectability, we employ controllable active packet dropout to fit the packet distribution of covert traffic to the distribution of the overt traffic. we discuss the performance of the proposed covert channel including undetectability and robustness. experiment results show the potential of the proposed approach adaptive to mobile network environment. moreover, there are still many future works worth completing, for example, the capacity can still be further improved by applying other encoding and modulation methods, and the feedback channel can adapt more efficiently to real-time mobile network environments."
"where f (p ) and g(p ) are the numerator and denumerator of (19), respectively. also, s shows the feasible domain of p . to calculate the optimal p for (20), denoted by p ⋆, the derivative of f (q) with respect to p is calculated as follows"
1) the maximum number of repetitions (maxreps) is exceeded (line 3). 2) the sample mean falls in the confidence interval (satisfying the precision of measurement eps) (lines 15-17). 3) the elapsed time of the repetitions of application execution exceeds the maximum time allowed (maxt in seconds) (lines 18-20).
"the proposed two-level parallelization method was implemented using standard cþþ programming language. for the message-passing parallel computing at the sub-basin level, the mpi communication protocol was used. mpi is the de facto standard for message-passing parallel programming, and provides a set of send and receive functions that allow for communication among different processes. for shared-memory parallel computing at the grid-cell level, the openmp (open multi-processing) application programming interface was adopted. openmp is based on the fork/ join programming model and parallel computation was conducted by embedding openmp compiler directives in source code [cit] ."
"the difference between talk spurts and silence period is that the size of voice packet is larger than that of sid frame, and there is a significant difference between the time intervals of the adjacent voice packets and the time intervals of the adjacent sid frames. we build a covert timing channel from the sender to the receiver by actively dropping packets during the silence periods, and the covert message is modulated into the numbers of sid packets in the silence periods since a moderate change in the silent period is not easily detected."
data partitioning algorithms aiming to solve the optimization problems of minimization of time and energy of computations in self-adaptable data-parallel applications on modern extreme-scale multicore cpu platforms must address two formidable challenges.
"the qingshuihe watershed, located in the hebei province of china, was selected as the study area (fig. 6) (table 1 ). the finer the spatial resolution is, the larger the data volume used in a simulation will be. in order determine the impact of data volume on parallel performance, the watershed was divided into the same number of sub-basins (i.e., 67 in this case study) for these three datasets to keep the communication costs among sub-basins roughly the same. and in order to determine the impact of the number of sub-basins on parallel performance, sub-basins of different numbers (i.e., 43, 67, and 89) were delineated for the 90 m-resolution dataset. all experiments were performed using a 24-h storm with a 50-year recurrence interval at a 1-min time step. the default input parameters extracted from dem, land-use map, and soil map, and from values described in the literature were used in the simulations. it is worth noting that the simulation results would change with the changes of spatial resolution, watershed subdivision and model parameters [cit] . however, because the major focus of this study was to test the performance of parallel computation, the relationships between the simulation results and their affecting factors were not investigated."
"once we designed the basic building blocks of a dynamic programming solution (recurrence relations, tabular computation, traceback), our next main objective was to transform the main loop so that table cells in the dp technique can be evaluated in parallel. to summarize, the loop containing recursive invocations in the sequential algorithms is carefully restructured to allow parallel computation of the loop iterations."
"the two-way volte covert channel is composed of a covert timing channel from the sender to the receiver and a reverse covert storage channel. on one hand, the covert timing channel from the sender to the receiver is implemented by actively dropping packets during the silence periods, and the covert message is modulated into the numbers of sid packets in the silence periods. the silence period is a normal phenomenon in a voice call, and a moderate change in the silent period is not easily detected. at the same time, the use of gray coding ensures the robustness of the covert channel against the adversary's intentional packet loss attack. the changes of silent periods may affect the covert channel undetectability and reduce the voice quality of the conversation, so the variable length coding is employed to meet the undetectability and voice quality requirements. on the other hand, a covert storage channel is built for feedback from the receiver to the sender, and it hides the acceptance of the covert message as feedback information into the feedback control information field of the rtcp packet back to the sender. these certain bits of the fields are selected to serve as acknowledgment bits for the covert message transmission. the sender evaluates the current attack severity according to the feedback and adjusts the real-time parameters of the covert timing channel to weigh the relationship between the robustness of the adversary's active attack and other performance of the covert timing channel. after many rounds of feedback, the security confrontation against the active attack of the adversary is finally realized. the two-way feedback covert channel is shown in figure 4 ."
"self-adaptable data-parallel applications executing on modern extreme-scale multicore cpu platforms pose two formidable challenges to data partitioning algorithms aiming to minimize the execution time and energy of computations in these applications. the first challenge arises from the new inherent complexities introduced in multicore platforms such as severe resource contention and non-uniform memory access (numa) due to tight integration of cores that contend for shared on-chip resources such as last level cache (llc) and interconnect (for example: intel's quick path interconnect, amd's hyper transport). the second challenge is that the runtime and memory overheads of a data partitioning algorithm employed in these applications must be insignificant compared to that of the application."
"the data partitioning algorithm that we propose has noteworthy differences. first, it is a parallel algorithm. second, it takes as input a functional performance model and not a constant performance model such as an execution time to determine the workload distribution. third, workload distribution (which is globally optimal) found by it may not loadbalance the application."
"3.2.2.3. slave process. the slave processes are in charge of conducting actual calculations of hydrological simulations. after receiving computing tasks from the master process, the slave processes read input data and parameters from the hydrological database. the calculations of hydrological simulations, which contain time loops, then begin. during the calculation, the execution sequence of computational tasks was determined according to the type of task (as shown in fig. 3 ). the computational tasks of hydrological processes (including both hillslope and channelrouting processes) in headwater sub-basins (type 1) and those of hillslope processes in non-headwater sub-basins (type 2) are independent among sub-basins, so they were conducted first at each time step. the computational tasks of the channel-routing process in non-headwater sub-basins (type 3) depend on calculation results from the first two types of tasks, so they will not be conducted until the type 1 and type 2 tasks are finished."
"unlike popta, which examines a subset of points in the speed function, eopta examines only the convex points in the energy function (x, ). a point q is defined as convex if"
"from the above analysis, we prove the mutual authentication of the message between alice and bob. fig. 2 similarly, we transform our second protocol, illustrated in fig. 2, into the idealized form according to the rules of ban logic. the first message m2 1 and the second message m2 2 of this protocol are omitted, since these two messages do not provide any of the logical properties of ban logic. we describe the other messages in idealized form as follows:"
"for message authentication, the receiver of the message wants to make sure that only the specific sender can share the one-time secret key. using the digital envelope technique, the receiver can select a one-time secret key and then encrypt the key in a digital envelope by using the sender's authenticated, longterm public key. thus, only the sender can open the digital envelope by using the corresponding long-term private key. this solution can achieve message authentication, and, at the same time, it provides 1-out-of-∞ deniability since any user can generate the digital envelope. using the dh-key exchange method, the receiver can compute a short-term dh public key and send this one-time public key to the sender. then, the sender and receiver can share a one-time key based on the receiver's short-term dh key and the sender's long-term dh public key with digital certificate. this solution can achieve message authentication and, at the same time, it provides 1-out-of-∞ deniability since any user can generate the shortterm public key."
"moreover, we also compare the robustness of the constructed hybrid covert channel (hcc) with the five covert channels mentioned above. these covert channels include the spreading code timing channel (stc) [cit], the code-based covert timing channel (coco) [cit], fountain code timing channel (ftc) [cit], analog fountain code timing channel (aftc) [cit] and packet reordering time channel (prtc) [cit] . as shown in figure 8, the ber of the ftc is about 10 −1, and the ber of the covert channel based on the sid packet dropout proposed in this paper reaches 10 −2 . the bers of the other four covert channels also reach 10 −2 . in the case of normal communication, the robustness of the constructed volte covert channel is better, because this scheme only modulates the sid packets, and gray code provides a guarantee for improving the robustness of the covert channel according to the proposed construction scheme."
"in order to investigate the impact of the number of sub-basins on parallel performance, a two-level parallel computation was conducted using the 90 m-resolution dataset with different numbers of sub-basins (43, 67, and 89). the results are shown in fig. 8 . the parallel computation with a greater number of subbasins had better parallel performance when the number of cores exceeded 36. this could be attributed to the increasing parallelizability at the sub-basin level with the increase in the number of sub-basins, which then led to better scalability of the parallel computation. however, it should be kept in mind that sub-basin delineation should not be driven by the needs of parallel computation but should be determined by the needs of hydrological studies in certain scenarios. the number of sub-basins cannot be increased arbitrarily in order to get better parallel performance."
"one way to improve the fairness of deniability is to increase the number of possible message generators from 2 to n, where n is a large positive integer. as we know, the ring signature [cit] can provide anonymity for the message signer. in a ring signature scheme, the message signer selects n ring members, including herself/himself, who could have possibly signed the message. the real signer can generate the ring signature by using her/his private key and the other (n − 1) ring members' public keys without their assistance or even awareness. however, the generated ring signature can convince any verifier that the ring signature indeed was signed by one of the ring members when the real signer's identity is fully anonymous to the verifier. thus, the ring signature can provide 1-out-of-n deniability. one of the problems of the ring signature is that the computational complexity of generating and verifying a ring signature is proportional to the number of ring members. in addition, we need to point out that there is one major difference between the ring signature and the deniable authentication protocol. in the ring signature, the receiver cannot identify who the real message signer is, but, in the deniable authentication protocol, the receiver can authenticate the sender of the message."
"both pgp and s/mime use a digital signature to provide message authentication. the message sender uses his or her private signing key to generate a digital signature on the message digest. the digital signature is attached along with the message, and both are sent to the receiver. the receiver can use the sender's public key to verify the digital signature. since the digital signature is an evidence of non-repudiation, this approach for providing message authentication has no deniability at all."
"a.1.1. authentication proof for the proposed protocol illustrated in fig. 1 we use the rules of ban logic to transform our first protocol, illustrated in fig. 1, into the idealized form. the first message m1 1 of the protocol is omitted, since the message just includes a nonce and the certificate of the public key k 1 . this message does not provide any of the logical properties of ban logic. we describe the other messages in idealized form as follows:"
covert channels are generally classified into two types: covert storage channels (cscs) and covert timing channels (ctcs) [cit] . the storage covert channel means that the sender directly or indirectly writes information to certain storage
"the core idea is the use of analytical approach to estimate the cost of communications. [cit] formulate and implement this idea where they find the optimal communication scheme without expensive testing on the executing platform to estimate the communication cost of different configurations of the application. they propose and discuss an extension of the τ -lop communication performance model to cover heterogeneous architectures. so, to summarize, there are now two performance models, which are input to our data partitioning algorithm that optimizes data-parallel application for performance. they are an experimentally constructed computation performance model and a communication performance model employing analytical formulas. energy models for communications is a open research problem and hence we do not consider our data partitioning algorithm for energy optimization in this discussion."
"while some prior art also provides voice services over lte, volte has seen significant growth due to its higher spectral efficiency and compatibility with rich multimedia voice experiences. ott (over the top) refers to the provision of voice services through third parties such as skype or google talk, which is relatively inexpensive and become very popular. however, there is no guarantee of qos and service continuity using this method, especially when user equipment moves outside of the lte coverage area, where dropped calls and call failures occur frequently, while volte provides better voice and video quality through dedicated bearers and bandwidth. since the queued tasks in the pipeline compete for bandwidth, but the ott application is at a higher level and cannot control the lower layers of the device software stack, latency is a major problem in ott applications. in contrast, volte has better control over transmit and receive processing because volte can be integrated with low-level drivers and network interfaces. with support for smartphones and networks, volte has been able to provide users with a seamless, high-quality experience."
"in non-centralized algorithms [cit], load is migrated locally between neighboring processors, while in centralized ones [cit], load is distributed based on global load information. non-centralized algorithms are slower to converge. at the same time, centralized algorithms typically have higher overhead. the centralized algorithms can be further subdivided into two groups: task queue [cit] and predictingthe-future [cit] ."
"we can conclude from the tables that the execution times of paraleph are negligible compared to the execution times of the applications. paraleph gives tremendous speedups over the sequential algorithms. for large values of n and p, the sequential algorithms fail due to their high memory cost indicated by f in the tables."
(4) nondegrading: during the lifetime of the sensor (up to 24 hours) both the metamaterial and the hydrogel maintain its optical properties even when immersed in body fluid.
"this section presents results and evaluations of implemented dnn based ids models. comparisons of dnn ids models are provided with each other and with different conventional models. we used prominent metrics to evaluate classification quality of implemented dnns, which include receiver operating characteristic (roc), area under curve (auc), precision-recall curve, accuracy on test datasets and mean average precision (map). these evaluation metrics are computed using confusion matrix which presents four measures as follows:"
the previous sections discussed the web based learning systems developed in personalized and collaborative environment using edm techniques with concept maps. this section attempts to summarize the general features of these algorithms used in personalized and collaborative learning applications discussed above and propose a set of parameters for comparing them.
"the edge c1  c3 is an association rule and indicates that the concept c1 should be learnt before concept c3 [cit] . similarly, learning concept c3 is a prerequisite to learning c2. in other words, if a student fails to learn c3, it is due to the failure of mastery over concepts c1."
"there exists a confidence level 0.45 associated with the rule c1c3, which states that if the student fails to understand c1, then the probability for him failing to understand c3 is 0.45 [cit] ."
"for cnn, the input x consist of a multi-dimensional array called tensor. the core computational block of cnns are convolution conv and pooling layers. a conv layer takes input as a tensor and convolves it with a set of filters (kernels) to produce output tensor. for a single filter k of dimension d k, h k and w k, convolution is performed by sliding filter k overall spatial positions of the input tensor and calculating dot product between input chunk and filter k to produce a feature map. since conv layer contains set of filters, it produces a feature map for every filter, and these feature maps are stacked together to produce output tensor. formally, assuming input shaped as greyscale images, the operation of convolution layer is specified by following steps [cit] where"
an alternative approach developed recently uses an in vivo glucose sensing method that can be placed in the tear canal and that therefore reduces variability due to probe extraction technique [cit] . it allows measurements to be carried out in situ. this amperometric sensor is comprised of three electrodes that are screen-printed on a flexible polyamide substrate which allows the sensor to be wound into a tight roll that fits in the tear canal for in situ monitoring.
"the undeniable advantage of estimating blood glucose levels through tear fluid lies in the facts that tears are more simply and noninvasively accessible than other body fluids, more continuously obtainable, and less susceptible to dilution than urine. tear fluid provides a unique opportunity to develop a noninvasive interface between a sensor and the body that could be used to monitor several physiological and metabolic indicators, most notably glucose. the noninvasive feature would be the main advantage of this sensing scheme."
network intrusion detection refers to the problem of monitoring and differentiating such network flows and activities from the normal expected behavior of network which can adversely impact the security of information systems. the search of reliable solutions by governments and organizations to protect their information assets from unauthorized disclosures and illegal accesses has brought intrusion detection and prevention at the forefront of information security landscape.
sparsity helps to thwart overfitting of an ae which would otherwise act as an identity function for training data. the penalty term is usually a regularizer added to bottleneck layer.
"following jong's [cit] classification on the basis of applications, it has been found that concept mapping technique has mostly been used for collaborative applications. in typical applications, learners have been found to develop their own concept maps based on domain knowledge. this concept map is then finalized after discussion with peers. support may also be taken from the instructors or materials from the web. some of these applications in e-learning and m-learning environment are next discussed. over the years several researchers have used collaborative learning strategy in e-learning environment. [cit] has proposed a four stage web based collaborative inquiry learning system that uses the web as source of knowledge. concept map has been used for organizing this knowledge. these stages of learning were used to generate the concept map by a set of 17 students in a university in taiwan. were divided into groups of 3 to 5 students each and were asked to generate concept maps on a given topic in learning theory. they found that students preferred sharing concept maps between themselves and is a good tool for generating and structuring ideas. stayanova [cit] have conducted a experimental study to investigate the learning effectiveness of concept map in computer supported collaborative problem solving design (cscpsd). three scenarios of interaction were investigated: distributed, moderated and shared. it was found that shared scenario was most appropriate for establishing a supportive environment for cscpsd. in mobile environment collaborative learning strategy has generally been applied to learning those subjects that have two learning components: a theoretical component taught in a class room and a practical component learnt using field work. these groups collaborate using tools like sms, e-mail etc. [cit] has proposed a mobile learning approach based on concept maps with remediation mechanism. two concept maps were constructed. the initial one was constructed by the teacher. students then developed their concept maps based on the knowledge learnt from the text books in the computer class room. these concept maps were then compared. the concept map was finalized when students performed relevant field work in the subject. [cit] reports their implementation of a handheld concept mapping tool to support cooperative learning in a nursing class. they used picomap software to enable students construct concept maps. they found that handheld tools enhanced interaction among students when aided by proper class management and technology support. [cit] has used a mobile collaborative concept mapping (mococoma) learning system using the sms property of mobile phones. the application software consists of php and java applet module and the database shared with these. the system was tested for a set of students for studying natural science courses. acharya and sinha [cit] in their work developed a collaboration schema in which a set of students studied a subject having both theory and practical components by exchanging sms. a prototype learning system was again developed using android emulator. a class of students learning the course \"introduction to java programming\" were divided into groups and asked to study using the proposed method for evaluating their group dynamics. a comparative study of some of these learning systems are presented in table 3 on the basis of the collaboration objective, environment, application and the learning theory modeled in collaborative applications."
recommendation model for students in similar situations in the future [cit] . grouping learners based on certain traits group students based on the contents of web page they are visiting [cit] . interesting patterns characterizing the strong and weak students [cit] .
"for a first overview, we simulated spectra for a broad concentration range of aqueous glucose solutions on top of different metamaterials, namely, a simple plasmonic dipole and a stacked eit-type metamaterial. starting with pure water, we added"
statistical patterns of learner interaction in a group [cit] . overall contribution of a learner in a group [cit] . amount of posting vs. replies in a group [cit] . providing mechanisms for betterment of learning activity
"however even after 70 years of research, there are no clinical studies that have satisfactorily resolved the relationship between tear and blood glucose concentrations. disagreements between reports may not invalidate the correlation between tear and blood glucose because, regardless of the exact mechanism of glucose transport into tear fluid, the individual accuracy holds true for each set of experimental conditions."
"on top of that, the eit shape offers four specific wavelengths with a large slope, compared to the dipole shape with only two points. this increases the flexibility for choosing a specific wavelength, as not every wavelength is available in commercial lasers."
"the primary contribution of this work is filling abovementioned research gaps by designing and implementing anomaly detection models based on state of the art deep neural networks and their evaluation using standardized classification quality metrics. the first gap is filled by developing anomaly detection models using deep cnn, lstm and multiple types of autoencoders. to the best of our knowledge, the dnn structures (dcnn, contractive, and convolutional autoencoders) investigated in this study have not been analyzed for anomaly detection. in addition, comparisons of deep learning based anomaly detection models are provided with well-known classification schemes including svm, k-nn, decision-tree, random-forest, qda and extreme learning machine. to fill second research gap, we opted to train all models on training dataset without ever exposing test dataset to the model during training and then tested/evaluated the models on testing datasets. this approach provided a fair estimate of model capabilities by using unseen data instances at evaluation time. to bridge the third research gap, deep learning based anomaly detection models were evaluated amongst themselves and with conventional machine learning models by using unseen test data and employing standard classification quality metrics including roc curve, area under roc, precision-recall curve, mean average precision (map) and accuracy of classification."
slack variables ξ ( * ) i (ξ i and ξ * i ) are introduced to account for outliers [cit] . this leads to the lagrangian function
"the reflected laser ray is detected by the photodiode. figure 5 shows a scheme of the circuitry which is used to amplify the current of the photodiode [cit] . a characteristic feature for this kind of feedback amplifier is the virtual ground at the node of the inverting input terminal which enables a higher bandwidth [cit] . the bias resistor r b reduces the effect of the bias current, which is nonzero for any real operational amplifier. in the simulation model, the current i(t) of the photodiode is converted to an output voltage y(t) using unit amplification."
"the previous section leads to a position where a detailed discussion of the web learning systems developed using concept maps in web based environment using edm techniques can be presented. discussion has been done on two axes. articles are first classified based on the nature of applications (personalized and collaborative). within themselves they are farther classified based on environment (elearning and m-learning). jong et. al. [cit] suggests two main applications of concept maps. in the first type of applications, students learn a subject and construct concept maps based on it. the instructor then constructs his own concept map and compares it with the student's to find his learning deficiencies. he named this method concept mapping. in second application, named diagnostic concept graph, a concept map is constructed and the student is asked to evaluate his learning status from it."
"metamaterials are able to detect even minute changes in the dielectric properties of their environment, hence selectivity to a particular type of molecule has to be added. this is achieved by covering the metamaterial with a glucose-sensitive hydrogel [cit] . when using inverse opal photonic crystals, the optical diffraction changes upon glucose exposure [cit] . in figure 1 a schematic of our proposed design is shown: a contact lens material supports a few nanometers of a gold-based metamaterial which is functionalized with glucose-sensitive hydrogel. this design is transparent in the visible and near-infrared range and thus can be designed as contact lens to be inserted into the patient's eye. the readout is carried out by an external light-emitting diode (led) in the infrared (eye-safe range at wavelengths longer than 1.4 μm) which is used as light source, and the reflected light is captured by a photodiode whose intensity response is evaluated. signal postprocessing stages based on regression methods allow the reliable estimation of the tear glucose content."
"existing methods of fluorescent glucose sensing apply fluorescence resonance energy transfer (fret) [cit] . this method is based on the dual measure, that is, the fret and fluorescence intensity measurements. fret is an inexpensive and very sensitive method to apply to molecule imaging. however, barriers to secure a feasible contact lens sensor include the photobleaching of fluorescence molecules, low concentration of tear samples, low fluorescence intensity, and vision influence. in addition, one safety concern is that some harming substances may be released from the lens into the body."
"high sensitivity of our detection scheme is warranted because metamaterials are able to detect even minute changes in the dielectric properties of their environment. the basic concept relies on a contact lens material that supports a few nanometers of a gold-based metamaterial which is functionalized with glucose-sensitive hydrogel. this design is transparent in the visible and near-infrared range and thus can be designed as contact lens to be inserted into the patient's eye. the readout is carried out by an external led, and the reflected light is captured by a photodiode whose intensity response is evaluated. signal postprocessing stages based on regression methods allow the reliable estimation of the tear glucose content."
"f is the frobenius norm of the jacobian matrix, which is the sum of squares over all elements inside the matrix. frobenius norm is regarded as the generalization of euclidean norm and represented by following equation."
"in the presented systems the glucose level concentration corresponds to the concentration used in the spectrum simulation. in order to obtain a predicted concentration, support vector regression is employed. the training data is the simulated photodiode current as independent variable and the associated glucose concentration as dependent variable. the results are validated using k-fold cross-validation. support vector regression will also be employed in the actual measurement device. in that case, the training data consists of the measured photodiode current as independent variable as well as the associated glucose concentration as dependent variable. given a measured photodiode current, the svr is used to predict the corresponding glucose level."
"in the definition of the structure as well as in the calculations, the design is separated into single layers, beginning at the superstrate, down to the substrate, each homogeneous along the z-axis. the first step is to solve maxwell's equations for every layer."
"where a n denotes activations of bottleneck layer. deep aes offer many advantages. one major benefit of deep ae stems from universal approximator theorem. [cit] . universal approximator theorem states that a feed-forward neural network with at least one hidden layer can represent an approximation of any function to an arbitrary degree of accuracy which means a deep ae with more than one encoder layer can approximate any mapping from input to bottleneck arbitrarily well. adding depth has the effect of exponential reduction in computation cost and amount of training data needed for representing many functions [cit] . different ae structures are described in the literature, but we will discuss the aes relevant to our study."
"from input space to feature space is introduced. usually f n holds true. the nonlinear regression in input space corresponds to a linear regression in some feature space. instead of actually performing the mapping, which might be computationally expensive, the so-called kernel trick is applied. it depends on the fact that the training data only occurs in the form of scalar products and that scalar products in feature space can be calculated in input space using the kernel k(·, ·) according to"
"convolutional neural networks (cnns) [cit] are neural network architectures specially crafted to handle high dimensional data with some spatial semantics. examples of such data include images, video, sound signals in speech, character sequence in the text, or any other multi-dimensional data. in all of the abovementioned cases, using fully connected networks becomes cumbersome due to larger feature space. cnns are preferred in such cases because of awareness of spatial layout of input, specific local connectivity, and parameter sharing schemes [cit] ."
profile analysis for searching and ranking learning objects based on predicted user interest [cit] . using tools for organizing knowledge construction of learning paths based on students historical test records [cit] identifying learning barrier and overcoming it [cit] .
"where (x i, y i ) is the ith pair of training data. this can be thought of as a punishment for the deviation of the estimated value f (x i ) from the given value y i . the affine ansatz"
software toolchain used to implement all dnns consist of jupyter development environment using keras 2.0 [cit] on theano [cit] backend and nvidia cuda api 8.0 [cit] . both training and testing datasets were manipulated in the form of numpy arrays. python scikit-learn [cit] library was used for various ml related tasks. figures and graphs were created using python matplotlib and seaborn libraries.
a complex simulation environment is built to evaluate the main signal contributions together with the most important noise sources as well as the most relevant parameter uncertainties. the simulation results have shown that estimation errors below 2% at physiological concentrations are possible.
"finally an evaluation of the measurement sensitivity is performed. in figure 10, the photodiode current is shown as a function of the glucose concentration for both dipole metamaterial and eit metamaterial. one can observe that the dynamic range of the current is larger for the eit metamaterial due to the steeper slope."
the next section introduces the use of data mining in education along with various entities involved in the process and their functionalities. the collection of techniques used for this purpose is called educational data mining (edm). a brief survey of edm tasks already performed in personalized and collaborative environments is also presented. web based learning systems may be used for enhancing the efficacy of edm techniques. section 3 provides a brief justification on that count. web based learning systems often uses a tool for organizing knowledge. one such tool studied in this article is concept map. section 4 discusses the applicability of concept maps in organizing knowledge in web based learning systems. section 5 presents the web learning systems developed on the basis of edm techniques which use concept maps for knowledge organization. section 6 attempts to infer the general features of edm techniques used for constructing concept maps in personalized and collaborative environments and compare their performances. section 7 presents the limitations of the current works. the article concludes by providing certain pathways that may be explored by the future researchers.
"it is likely that contact-lens-based glucose sensors have great potential to realize continuous and noninvasive diabetes control, that is, contact lenses have applications beyond vision correction. luminescent/fluorescent contact-lens-based sensors represent a feasible technique because they require no electrodes or electric circuits. further efforts are needed to improve the resolution and sensitivity of the new device and to determine a physiologically relevant and baseline tear glucose concentration [cit] ."
"in this paper, intrusion detection models were proposed, implemented and trained using different deep neural network architectures including convolutional neural networks, autoencoders, and recurrent neural networks. these deep models were trained on nslkdd training dataset and evaluated on both test datasets provided by nslkdd namely nslkddtest+ and nslkddtest21. for training and evaluation of deep models, a gpu powered test-bed using keras with theano backend was employed. to make model comparisons more credible, we implemented conventional ml ids models with different well-known classification techniques including extreme learning machine, k-nn, decision-tree, random-forest, support vector machine, naive-bays, and qda. both dnn and conventional ml models were evaluated using well-known classification metrics including roc curve, area under roc, precision-recall curve, mean average precision and accuracy of classification. both dcnn and lstm models showed exceptional performance with 85% and 89% accuracy on test dataset which demonstrates the fact that deep learning is not only viable but rather promising technology for information security applications like other application domains. our future research will be directed towards investigating deep learning as feature extraction tool to learn efficient data representations for anomaly detection problem."
"support vector machines emerged from the field of learning theory. they are constructed using training data. compared to other learning methods, overfitting is avoided by implementing the paradigm of structural risk minimization (srm) [cit] ."
"finally, demodulation and filtering are performed. in order to avoid higher frequency noise contributions and to detect the steady state value, low-pass filtering is performed and its output signal y is then evaluated."
"like its predecessor, nslkdd dataset consists of 41 input features as well as class labels. features 1 to 9 represent the basic features which were created from tcp/ip connection without payload inspection. features 10 to 22 comprised of content features, generated from the payload of tcp segments of packets. features 23 to 31 were extracted from time-based traffic properties while features 32 to 41 contain application based traffic features that were designed to measure attack within intervals longer than 2 seconds. a class label was provided with each record, which identified the network traffic instance either as normal or an attack. original kddcup99 dataset listed different types of attacks shown in table 1 ."
"in order to overcome this issue, temperature will be included as independent variable in the svr in future work. on top of that, the system will be calibrated in order to correct the wavelength deviation of a specific laser."
"the most important aspect of concept map generation using edm techniques is the computational complexity of the map construction algorithm. it has been observed that for p-learning general data mining algorithms like apriori [cit], dhp [cit], sprt [cit] have been used for concept map computation. for c-learning concept maps have been constructed based on discussion between the learners [cit] and thus edm techniques have not been used. however in both the types of environments the most important factor in complexity computation is the number of learners (n) involved in concept map generation. [cit], learners perception of the subject or learning inputs received from the peers or the web [cit] . student's historical test records have traditionally played an important role in concept map computation in p-learning environment. sometimes instructor inputs [cit] have also been used. edm techniques have been applied to these for deriving concept maps. on the other hand, student's knowledge of the subject has often been used in c-learning environment as input to edm techniques [cit] . web based learning systems have often used various learning theories based on which they have been implemented. these learning theories propose a set of step based on which learning is conducted. these steps have been implemented in the form of modules while designing the system architectures. typical examples of learning theories used for learning system construction are item analysis for norm referencing [cit], activity attention network [cit] and theory of meaningful learning [cit] . in this context no difference was observed between p-learning and c-learning environment. next in line is computation of confidence between concepts. confidences are used in determining the degree of association between the concepts. acharya and sinha [cit] have used them for proposing weighted concept maps. in p-learning environment they are typically computed using the algorithms used for determining association rules between the concepts. in c-learning environment however confidences may be assigned by the learner intuitively. weights may also be assigned to the concepts to indicate their degree of importance [cit] . just the computation of concept map may not be enough for all types of applications. optimization of the map may be required in certain circumstances. one such optimization is removal of cycles in concept maps. a concept map containing several concepts is almost found to contain cycles. these cycles introduce a certain degree of redundancy. if a cycle exists between two concepts, it is impossible to determine which concept should be learnt first. various researchers have proposed variety of graph algorithms for removal of redundancy. in p-learning the general consensus is removal of the edges with least weight which removes cycles [cit] . c-learners however may avoid cycles based on observation [cit] . even after this if cycles exist, algorithms similar to p-learning environments may be used. the usage of edm techniques is again dependent on the focus area of the developed application. data mining algorithms are often used in p-learning environments whose major application area is remedial learning [cit] . c-learners however rely more on discussion among themselves [cit] . however, for both types of learning environments, web based tools have been used for prototype development. as examples [cit] have used php and mysql for a p-learning application whereas android emulator has been used by acharya and sinha [cit] for both types of applications. finally the efficiency of the learning system is determined in terms of learning result. for both p-learning and c-learning environments the efficiency is determined by conducting tests before and after the learning process and applying certain statistical algorithms on these test results. statistical algorithms include t-tests, anova and multivariate regression analysis [cit] . a survey is also used to evaluate learner satisfaction. additionally clearning systems often use these statistical methods to determine the collaboration dynamics [cit] . based on the literature surveyed in the previous section, personalized and collaborative web learning systems developed using concept maps are compared parametrically in table 4 . a serial # is attached to each parameter for ready reference. on observation it is found that for p-learning serial #s 1, 3, 4, 5, 7 use edm techniques extensively whereas for clearning only serial #s 3, 7 use edm techniques. thus it can be concluded without a shadow of doubt that p-learning environment uses edm techniques a lot more compared to c-learning environment."
"(3) biocompatibility: the metamaterial is made of a several nanometers thick gold structure, transparent for the human eye, and absolutely biocompatible due to the properties of noble metals. the hydrogel is commonly used for contact lenses and therefore well characterized. the optical readout is based on an eye-safe led."
"our eit metamaterial uses a 60 nm displacement of the dipole bar from the central symmetry axis. the length of the dipole bar is 340 nm, whereas the quadrupole bar is 345 nm long. their width is 80 nm, the gold thickness is 40 nm, and the spacer thickness is 70 nm."
"when comparing simple dipole plasmonic structures with plasmonic eit sensors, we find that in terms of concentration sensitivity, the eit concept is superior by at least a factor of two over the simple dipole. however, as a drawback, the eit concept due to its steeper resonances is also more prone to wavelength shifts due to temperature variations. however, this problem can be circumvented by using a temperature stabilization scheme for the laser diode."
the number of parameters in each filter is f * f * d 1 for a total of (f * f * d 1 ) * k weights and k biases purpose of pooling layer is to control overfitting by decreasing the size of representation with a fixed downsampling technique (max-pooling or mean-pooling) without any weights. pooling layers operate on each feature map separately to reduce its size. a typical setting is to use maxpooling with 2x2 filters with a stride of 2 to downsample the representation precisely by half in both height and width.
"in lstm ids model, each record was processed as single member sequence of 41 dimensional vector to 32 lstm units. two dense layers with ten (10) and one neuron respectively were attached with lstm outputs to make predictions for two class problem. first dense layer used relu activation while classification layer with single unit used sigmoid activation to make predictions. a drop-out layer was introduced between lstm output and mlp input to thwart overfitting. lstm ids model was trained on combined training dataset for 15 epochs."
"the analysis of measurement errors in glucose monitoring systems presents a particularly troublesome problem, because the importance (that is, the clinical consequence) of any particular error depends on the absolute value of both the reference and measured values and not just on the percentage of deviation. moreover, this dependence is not easily described by any simple mathematical relationship. although error grid analysis (ega) [cit] s [cit], an evaluation based on standardized signal processing and statistical tools is more meaningful for a preclinical analysis, which suits our purpose."
"(ii) instructors: the teachers can get a feedback about the courses offered by them instantaneously. they can also detect student learning patterns and their nature of mistakes. learner historical record could be used to predict their performance in examinations. this in turn could be used for identifying where they need additional learning support. (iii) course developers: their main aim is to create, maintain and evaluate courseware. data mining process could be used to improve the quality of courseware based on student learning experiences. they could also be used to develop student and learner models in an automated fashion. (iv) academic administrators: their aim is identify the courses that would be suitable to a class of learners. data mining process could be used in selecting the best candidates for admission by identifying who would do well in the later stages of learning. further they can help in finding the most effective way of improving student grades. (v) system administrators: they are involved in looking after the learning systems. data mining process could be used to organize the institution's resources in an optimal fashion so that they can be utilized more effectively. they can also be used to enhance the quality of the learning systems. the entities that stand to gain out of the data mining process were previously discussed. the important educational tasks performed by these entities using data mining process and their use in various types of learning environments are next discussed: (i) analysis of student learning patterns: the objective of this analysis is to study student learning activities to get useful information for decision making. statistical algorithms are mostly used for this purpose. when applied on educational data, they can provide summary learning systems can be classified into two categories based on the nature of applications: personalized learning (p-learning) and collaborative learning (c-learning). in plearning, each student can plan his curriculum to meet his needs [cit] . again several learners may attempt to learn something together capitalizing on one another's resources and skills resulting in increased learning efficacy leading to c-learning [cit] . some examples of edm tasks discussed above in p-learning and c-learning environment are enumerated in table 1 . table 1 . examples of p-learning and c-learning applications using edm in referred task domains task domain p-learning c-learning analysis of student learning patterns most popular web pages used by the learner [cit] . number of pages browsed by the learner [cit] . patterns of users through various time periods [cit] ."
"roc is a plot of false positive rate (fpr) against true positive rate (tpr) of binary classifiers. fpr is defined as fp/(fp + tn ). it corresponds to the proportion of negative data points mistakenly predicted positive to all negative data points. tpr, also called sensitivity or recall, is defined as tp/(tp + fn ). tpr corresponds to the proportion of positive data points that are correctly predicted positive to all positive data points. roc shows a trade-off between sensitivity and specificity of classifier. the closer the roc curve is to top-left border, the better the quality of predictions by the prediction model and vice versa. roc curves of implemented dnn models for both nslkddtest+ and nslkddtest21 dataset are shown in figure 4 and figure 5 respectively. comparison of roc curves for both dnn and conventional ml models is shown in figure 6 and figure 7 ."
"precision is defined as a measure of relevancy of results, while recall provides us a measure of how many genuinely relevant results are returned. high scores for both show that the model is returning accurate results (high precision), while also returning the majority of positive results (high recall). each classifier exhibits a trade-off between precision and recall. due to the fact that individually both precision and recall provide only a puzzle piece of classifier performance, they are combined to form precision-recall curve which presents the relationship between them in more meaningful manner. the stair-step nature of precision-recall curve provides insight into the relationship between precision and recall. a small change in the threshold at the edges of stair-step considerably reduces precision with only a small increase in recall. figure 9 and 10 depicts precision-recall curves (prc) and mean average precision (map), shown as area under precision-recall curve in legends section, of dnn models for both test datasets. mean average precision (map) summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with differential increase in recall used as the weight. map for all tested models is shown in legends section of figures 9,10, 11 and 12 . except sparseae, all dnn models showed very good results. in nslkdd+, both lstm and dcnn model share top position with map scores of 97% while dcnn showed marginally improved performance for nslkdd21 with 98% score. three models including contae, convae and lstm achieved 97% map score for nslkdd21. figure 11 and 12 shows prc and map performance of all models. top six map scores are shown in table 6 ."
"tear fluid is the aqueous layer on the ocular surface and has many functions as part of the optical system, that is, lubrication and nourishing. tear fluid consists of over 20 components, including salt water, proteins, lactate, urea, pyruvate, ascorbate, glucose, as well as some small metallic ions. its average rate of production lies in the range of 0.52-2.2 μl/min; about 0.72-3.2 ml of tears are secreted per day."
"special care has to be taken when the temperature or the wavelength differ from their nominal values. as the svr does not contain those parameters, the prediction deteriorates. the relative errors for the determination of the glucose concentration are presented in section 3.3. they become very large even for small variations of those parameters."
"in the next step, the amplitudes of the waves in the single layers have to be concatenated. therefore, the respective solutions of maxwell's equations have to be separated into a set of eigenvectors parallel to the z-axis. the amplitudes of the plane waves can now be written as vectors"
"in general, learning algorithms benefit from standardization of the dataset. since different feature vectors of nslkdd dataset contained different numerical ranges, we applied scaling to convert raw feature vectors into more standardize representation for dnns. as datasets contained both normal and anomalous traffic, to avoid the negative influence of sample mean and variance, we used median and interquartile range (iqr) to scale the data for better results. we removed the median and scaled the data according to iqr."
"removal of edges with least weights which creates cycles [cit] cycles avoided by observation [cit] 6. focus area of the developed application diagnostic and remedial learning [cit] learning a subject using discussion [cit] 7. efficiency of the algorithms in terms of learning result regression analysis to estimate the degree of significance between pretest and post test marks; surveys used to estimate learner satisfaction [cit] similar procedure used [cit] . statistical algorithms used in evaluating collaboration dynamics [cit] secondly, as indicated in the introductory section, kdd process contains multiple steps like data cleaning, integration, selection, transformation, mining, pattern evaluation and knowledge presentation [cit] . however in almost all the applications discussed above, the focus is on using edm techniques for learning system construction only. web learning applications are to be developed which integrates various applications like preprocessing student data, learning objects development and storage, designing test modules along with data mining functionalities. thirdly, the domain of use of p-learning and c-learning has not yet been clearly established. in general terms, when certain remediation measure is to be designed for a small group of learners p-learning has been used. typical examples are [cit] . c-learning has been used when a group of learners wanted to study together. typical examples are simone [cit], silander [cit] etc. acharya and sinha [cit] have used c-learning for two groups of learners learning a subject simultaneously, one group learning in theory class and another group learning in practical class. this classification is however too vague to have any conclusive effect. finally, using data mining algorithms in constructing web based learning systems is not an easy task. proper understanding of functions like classification, clustering, association along with a host of statistical and machine learning algorithms is necessary. this process may be cumbersome and time consuming for most learners, educators and academic administrators."
"all components heading to the positive (negative) z-direction are labelled with + (−). with the aid of a so-called transfer matrix, those vectors are linked at different positions (zvalues) in the layer:"
"the novelty of the approach lies in the fact that this sensor does not take advantage of the rather poor optical differences between the glucose molecule and other substances contained in the surrounding fluid (blood stream, tear fluid, etc.), but rather on the ability of the glucose to change selectively the refractive index of a specific metamaterial."
where fwhm denotes the full width at half maximum. these values have in common that a spectrometer is needed to determine both. this rather complex and cost-extensive method is only applicable in scientific research. in commercial products it is more likely that intensity changes at a specific wavelength are evaluated. this leads to the intensity dependent sensitivity
"in this subsection, we provide the train and test timings of models used in this study. for dnns, gpu is used as training and testing device while conventional models were trained and tested using cpu. in dnns, convae proved to be the most expensive algorithm because the training time included both autoencoder model training and mlp classification model training. collectively convae ids model took approximately 367 seconds on gpu. dcnn and lstm models took 109 and 208 seconds respectively. smallest training time from dnn models was that of sparse autoencoder but it did not show comparable results. svm with rbf kernel proved to be the most expensive model among conventional ids models and took approximately 314 seconds. fastest among conventional category was random-forest closely followed by decision tree. with smallest training time, decision-tree model showed remarkable results and performed comparable to other complex models. remaining conventional models took each under volume 6, 2018 100 seconds for training. training times of all models used in this study are shown in 13. evaluation times of all models used in this study are shown in figure 14 for both nslkddtest+ and nslkddtest21 datasets. overall longest time to evaluate complete dataset was approximately 8 seconds shown by k-nn based ids model. both best performing dnn models i.e. dcnn and lstm took approximately 2 and 4 seconds for nslkddtest+ and 1 second for nslkddtest21 dataset. decision tree based ids model performed imperceptibly fast during evaluation of both test datasets."
"educational data mining (edm) is the application of data mining techniques to educational data for its analysis [cit] . it is thus concerned with implementing data mining algorithms for finding unknown student learning patterns. in other words, they help the learner and the instructor in improving the teaching learning process. there are two perspectives to educational data mining process: firstly increased automation of educational institutions creates large repositories of student data. secondly, as mentioned above, the use of internet in education means that this dataset can be accessed in a real time fashion. edm is thus concerned with developing methods to explore large datasets in educational environment for better understanding of student learning patterns resulting in the benefit of the learners [cit] . there are several entities involved in the edm process. the advantages gained by them are next discussed: (i) learners: the data mining process recommends activities and resources to the learner so that they may further improve their learning. it also identifies interesting learning activities which could be used to identify optimal learning paths. further, courses, discussions and books could be identified as a result of this activity."
"the metamaterial shape (dipole or eit) plays a key role for the available maximum slope in the spectrum. a steeper slope in turn leads to a broader dynamic range of the photodiode current. therefore, the use of the eit shape is preferable."
"(1) glucose selectivity: this sensor does not take advantage of the rather poor optical differences between the glucose molecule and other substances contained in the surrounding fluid (blood stream, tear fluid, etc.), but rather on the ability of the glucose to change selectively the refractive index of a specific material, that is, the hydrogel in the vicinity of the metamaterial. (2) sensitivity: due to the fact that the metamaterial is sensitive to even minute changes in the refraction index (molecular changes in the hydrogel) the measurement can be performed in the range of physiological glucose concentration in the tear fluid."
"in general, broadband electromagnetic radiation in the optical domain is used to investigate the respective properties of nanostructures in sensing applications. one possibility is the recording of transmittance or reflectance spectra which exhibit characteristic dips and peaks. due to the localized electric field in and around the metallic pattern, the resonance positions are highly sensitive to changes of the electric permittivity or the refractive index, respectively, in the nearest vicinity of the plasmonic nanostructures. exploiting this fact allows to monitor, for example, the concentration of pure solutions on top of the structure by evaluating the shift of a distinct spectral feature [cit] . however, such gold structures are not able to detect specific substances in an unfunctionalized fashion. to realize a chemically selective sensor, we have to assure that the changes in the refractive index are exclusively caused by the desired analyte. for biological sensing, the existence of molecule pairs with strong affinity can be beneficial. ranking among the strongest noncovalent interactions known in nature, the biotin-streptavidin complex, for example, is a commonly used system for proof of concept experiments (see figure 2 ). the vitamin biotin can be functionalized with a thiol group by utilising polyethylene glycol as a spacer. this allows the whole molecule to bind to the gold nanostructures. if the structure is now rinsed with an analyte containing streptavidin, the molecules will attach to the biotin and due to their presence affect the dielectric environment of the gold structure. this effect, and therefore the detectable change in the optical spectrum, will remain even after washing away other substances that may have an impact on the measurement [cit] ."
"however, none of these devices has been made commercially available or was approved to substitute direct invasive glucose measurement. in order to overcome these shortcomings, alternative approaches have been developed to measure glucose concentration in an accessible body fluid, including urine, saliva, and tear fluid."
the structured slab couples the incident light with frequency ω and wave vector k to all bragg-orders retrieved from maxwell's equations with the same frequency and wave vector
"the method can be accelerated and improved in accuracy by using adaptive spatial resolution and the customisation of the coordinate system, depending on the individual structure."
"in general, it would be possible to calculate the propagation of light in layered structures using the transfer matrix formalism. however in case of evolving evanescent waves, this method may fail. this is the reason for using the scattering matrix algorithm. all amplitudes of waves incident on the sample, as well as the outbound waves, are combined into one vector:"
"furthermore, the hydrogel is quite biocompatible, in particular for the human eye environment. in fact, soft contact lenses already use those kinds of hydrogels as surface layers."
"deep learning is an area of machine learning which applies neuron like mathematical structures [cit] for learning tasks. neural networks have been around for many decades [cit] and have been gaining and losing the favor of research community. the latest rise of this technology is attributed to alexnet [cit], a deep neural network, which won the imagenet classification challenge. alexnet achieved top-1 and top-5 error rates of 37.5 % and 17.0% on imagenet dataset [cit] which were considerably better than the previous state-of-the-art mechanisms. since then, deep neural networks (dnns) have attracted the attention of research community once again and multiple dnn structures including convolutional neural networks (cnns) [cit], recurrent neural networks (lstm) [cit], deep belief nets (dbns) and different types of autoencoders including sparse, denoising [cit], convolutional [cit], contractive [cit] and variational autoencoders have been proposed. these dnn structures have been successfully applied to devise state of the art solutions in multiple disciplines."
"rest of the article is divided into vi sections. section ii highlights prominent works related to ids problem. section iii provides the architectural designs of dnns used in this study. section iv sheds light on implementation details including hardware setup and software toolchain. in section v, we present results of proposed dnn based models along with comparisons of results and timing information. this section is followed by section v and vi which describe the conclusion and references of research respectively."
"sheraz naseer received the m.s. degree in information security along with distinguished professional certifications of information security, including cissp, cobit, and itil. he is currently pursuing the ph.d. degree with the university of engineering & technology, lahore. he has over 10 years of experience in information security and it. he is an assistant professor with the university of management and technology, lahore, pakistan. he has been with various information security positions in financial, consulting, academia, and government sectors. he is very active in academic research with over six research publications in conferences and journals. his research interests include cryptography, data driven security, intrusion detection, malware detection, and application of deep neural networks for information security. his other skills include iso 27001, policy and procedure development, it security reviews and audits, vulnerability assessment and pentesting, secure software development, cryptography, log monitoring, and information security trainings. volume 6, 2018"
"anomaly detection is modeled as a classification problem in supervised learning. supervised learning uses labeled data to train anomaly detection models. the goal of this type of training is to classify the test data as anomalous or normal on the basis of feature vectors. unsupervised learning, on the other hand, uses unlabeled or untagged data to perform the task learning. one of the popular unsupervised learning technique is clustering [cit], which searches for similarities among instances of the dataset to build clusters. instances sharing related characteristics are assumed to be alike and placed in the same cluster. semi-supervised learning (ssl) is a combination of supervised and unsupervised learning. the ssl approach utilizes both labeled and unlabeled data [cit] for learning. ssl methods learn feature-label associations from labeled data and assign the labels to unlabeled instances having similar features that of a labeled instance on the basis of learned feature-label associations."
"the features of p-learning and c-learning applications pointed out in the previous sections are not without limitations. this section points out the limitations of these studies so that researchers in future may address them. firstly, most of the researchers implemented the modules for p-learning and c-learning systems on the basis of certain established learning theories. as an example, [cit] in their study have used item analysis for norm referencing for developing their learning system in plearning environment. similarly acharya and sinha [cit] have used the theory of meaningful learning for their p-learning system. for generation of c-learning system they have added a new phase to this system and hence called it 'extended' theory of meaningful learning [cit] . learners and academic administrators however often fail to get a deeper picture of the relationship between learning theories and their proposed implementation using edm techniques. learner's subject knowledge [cit] 3."
"is made, where ·, · denotes the scalar product. implementing the srm requires minimization of the weighted sum of the capacity (1/2) w, w of the machine and the training error"
"in the future, we are going to implement a glucose selective layer on the plasmonic structure. this includes a functionalization layer with a glucose-specific hydrogel [cit] ."
"autoencoder, whose training criterion involves both reconstruction error and sparsity penalty (h) on the bottleneck layer, is known as sparse autoencoder. sparse ae can be represented as:"
"determine relationship between each learning behavior pattern [cit] . enable teachers to analyze, refine and reorganize reading materials [cit] . identifying future performance of a learner identifying students who may perform poorly in term end exams [cit] ."
"in order to scale up erudite, we need to develop automated methods to assign concepts from our ontology to the collected learning resources (i.e., tagging). for this purpose, we explored both machine learning and information retrieval methods. we defined an experimental procedure that used the same source data, cross-validation folds, and performance measurements for every method tested. we developed a gold standard by manually tagging 413 resources focused on data science, including massive open online courses (moocs) from coursera, udacity, and edx, and videos selected from videolectures.net. for the inputs to the methods, we created text documents for each resource consisting of the resource title, subtitle, description, and syllabus, which were then vectorized as bag-of-words tf-idf vectors."
"to address these challenges, the bd2k training coordinating center is developing a web portal (bigdatau.org) to provide a dynamic, personalized educational experience for biomedical researchers interested in learning about data science. the portal is powered by erudite, the educational resource discovery index for data science, an enhanced collection of existing web-based training materials on data science. in order to build erudite, we are developing novel, automated methods to identify, collect, integrate, describe, and organize web-based learning resources. in this paper, we describe several steps of this process."
"as a first semi-automated method, we developed a system that analyzes the textual information associated with the learning resources (including titles, descriptions, syllabi, transcripts, slides, etc.) to automatically generate concepts from bigrams, trigrams, nouns, and shallow noun phrases 12 extracted from sentence trees constructed by the stanford parser [cit] . in evaluating these automatically identified concepts, we found that shallow noun phrases from the parser provided the richest terms. we reviewed the 8,160 automatic concepts from the parser and eliminated ambiguous and irrelevant ones. we also added tags related to the depth, domain, and format of the course. this process identified a total of 861 tags."
"we have represented the concept ontology for learning resources as a simple knowledge organization system (skos) 14 vocabulary, with the hierarchal relationships encoded by the broadertransitive property. we call this skos representation the data science education ontology (dseo); it can be viewed and downloaded at:"
"where d miss is the missile miss distance and t is the flight time of the missile in the dive phase. k d, k θ, k ω, and k δ are the weight coefficients for measuring the miss distance, the error of the terminal angular, the jitter of rate of the angle, and the jitter of fin deflections. the initial population size is set to 60. after genetic optimization, the optimal controller parameters were obtained. the average fitness value changes with the genetic algebra as shown in figures 6 and 7 . the optimized parameter values are shown in tables 2 and 3 ."
"we plan to explore personalization methods in erudite through recommendations tailored for an individual user via collaborative filtering. to do this, we are instrumenting the web portal to collect user activity data. this will allow us to benefit from a large, consistently engaged user base to build our recommendation engine."
"the principle of coordinated control shown in figure 4 is to translate the change of the lift coefficient into a change of the attack angle and the morphing rate, so that the span 8 international journal of aerospace engineering variant can withstand the change demand of the lift command and reduce the change demand of the attack angle control. while in the traditional igc method with invariable span, the change demand of the lift command is only reached by the attack angle. it would improve the stability and velocity of missile igc mission. that is,"
"the shape parameters and morphing modes are shown in figure 2 . the lift of the morphing missile was provided by a pair of horizontal front wings, and the four-rear wing was a +-shaped layout. the span variant of the wing is synchronously changed on both sides of the wings without differential changes. the synchronous or differential rotation of the four tails controls the rotational motion of the missile."
"2. does the proposed concept capture an abstracted phrase or concept that cannot be automatically extracted from text (i.e, that it would not be easily found by an information retrieval search over the resource text)? this reduced the ontology to a total of 117 concepts, which we organized hierarchically along six dimensions. figure 1 contains a selection of concepts from two dimensions, domain and data science process. a visualization of all concepts for the six dimensions is available at: http://bigdatau.org/explore_erudite. each of the hierarchical dimensions of the concept ontology aims to answer a specific question a learner may have about a resource. these are listed below, along with how many of the 117 concepts currently identified fall under each dimension. programming tool (13) what programming tool is used in or taught by this resource?"
"in the current bd2k tcc web portal (bigdatau.org), learners can search through the resource index with keywords, can filter the resources based on our multi-dimensional, hierarchical concept ontology, and can obtain a recommendation of similar resources. however, we want to provide a stronger organizational structure that indicates which resources a learner should start with given a learning goal and state of expertise, and which resources should be studied before others. consequently, we have begun to experiment with methods to extract resource dependencies and prerequisites. by conveying resource dependencies and prerequisites, we aim to provide a sequence of concepts and resources that will guide learners toward known learning paths as they develop and complete their own self-directed curriculum on the bd2k tcc web portal."
"in summary, the entire closed-loop system with the control law can be composed of the dynamic surface vector, the filtered error vector, and the upper bound estimation error of the uncertainty as"
"members of the erudite team previously developed techknacq, a system that uses cross entropy to infer conceptual dependencies from collections of technical or scientific documents [cit] . we have applied techknacq methods on erudite resources, but the initial results were not satisfactory. a likely explanation is that most of our learning resources offer less -and noisier -text than the journal articles and other publications organized by techknacq. we plan to extract and automatically clean additional textual data from erudite resources, including transcripts of videos and text found in associated materials, including lecture slides."
"in the collection stage, we have built a web-scraping framework that allows us to rapidly incorporate new sources and extract relevant data from them. in the integration stage, we have designed a unified schema for learning resources to integrate heterogeneous data into a single, consistent model. under this model, the system also exposes the metadata of learning resources as linked data [cit], so these resources can be easily cross-referenced by other web services. in the description stage, erudite uses methods from machine learning, information retrieval, and natural language processing to tag resources with concepts from a hierarchical, multidimensional ontology designed to provide an extensible, lightweight description of core aspects of the field of data science."
"missile. in order to verify the effectiveness of the igc method with a variable span auxiliary control in the dive phase of the hypersonic missile, the model is numerically simulated and the control group is set up to observe the effect of the igc method under the condition of variable span and invariable span. the initial state is shown in table 2 ."
"although erudite currently focuses on knowledge about data science, we expect that the erudite platform can be applied to other fields. most careers demand continuous, self-directed learning well outside of degree programs, and few tools exist to help learners navigate through the heterogeneous resources on the web. consequently, erudite has the potential to expand interaction with an important subset of scholarly data: educational resources. historically, when thinking about the web of scholars, we look at journal publications and citations, but now, in the age of digital learning, scholars also produce open-access educational resources, creating a source of data that connects, informs, and educates not only scholars but also anyone interested in learning more about a field, concept, or technique. with this type of educational, scholarly data, the web of scholars can strengthen across disciplines, for understanding of others' work is easier through an open educational resource in comparison to a journal article, and can grow because many more people have access to the materials they need to learn in order to become scholars themselves."
"concept tags add an organizational structure for browsing resources on the bd2k tcc web portal, but we also want users to be able to explore resources through a visualization that conveys the landscape of resources in erudite. for this, we used t-distributed stochastic neighbor embedding (t-sne) [cit] to reduce the vectorized representations into a two-dimensional space."
"most of the above researches were aimed at the igc system of the hypersonic vehicle and the stability control of the morphing aircraft. there were few reports on the control research of the hypersonic morphing missile and how to use the span variant to assist the ballistic control. the research in this paper is aimed at the igc for a class of hypersonic variable span missiles. based on the adaptive dynamic surface method, an igc method in the dive phase with terminal angular constraint is designed and the span variant is skillfully applied to the auxiliary control of the igc to enhance the control accuracy and reduce the workload of the control system. the variable span auxiliary control strategy was hardly studied for hypersonic missiles before. the results of simulation of the new method proposed in the paper show that the total flight time and the miss distance of the variable span are less than invariable span shown. the variety of span could adjust the lift-to-drag ratio of the missile so that it is easier to realize the terminal angular constraint, and the variable span reduces the workload for the attitude control system. at the same time, the robustness of a variable span missile is stronger than that of an invariable span missile."
"we used a one vs. rest approach to train random forest, multinomial naïve bayes, logistic regression, support vector machines, and k-nearest neighbors classifiers. we show the training flow for the classifiers in table 2 shows the performance for each classifier. for each of the classifiers, we did use non-negative matrix factorization (nmf) [cit] to produce reduced document vectorizations, but we found that, across all classifiers, the tf-idf representation performed best. several classifiers performed comparably, but logistic regression produced the best results overall, with an f1-score of 0.73 when trained on exact tags, and of 0.83 when exploiting the concept hierarchy."
"when looking at erudite as its own data science project, we have made significant progress on the data collection and data integration steps, and we have begun the data exploration and data analysis steps. in the development of erudite, so far, we have designed and implemented a flexible scraping framework, a unified schema, a tagging ontology, a visualization approach for resource exploration, and a collection of automated tagging algorithms. we are halfway toward completing the vision of making erudite a platform that aggregates and organizes relevant resources and provides a personalized and engaging experience for the selfdirected data science learner."
"in eq. (13), b g is the transformation matrix of the launch coordinate frame to the body coordinate frame, and the other components of each variable are expressed in the body coordinate frame."
"to date, we have collected a total of 8,600 resources, which vary in granularity from individual videos to online courses that include multiple video lectures and associated training material. table 1 describes the current sources, the number of learning resources per source, and the types of information extracted, such as resource descriptions, video transcripts, and supporting slides or other written materials."
"1. is there enough support for the concept within our resource collection? (currently, we require more than five resources to be relevant to the concept.)"
"for the tagging task, we also experimented with using ranked similarity between resource documents as a way to assign tags. in this approach, we first vectorized each resource document in the training set. second, for each incoming resource in the validation fold, we compared the incoming resource vector to all resource vectors in the training set that belong to a tag. third, we aggregated the similarities for all of the comparisons, and that aggregated value is the similarity metric used for that incoming resource-tag pair. finally, once a similarity value has been calculated for every resource-tag pair, we sort the similarities and then use a rank cutoff to assign the top-n most similar tags to a resource. figure 3 shows a diagram of this training and validation approach. table 3 shows the performance for the information retrieval methods for different methods of vectorizing the resource documents. we considered tf-idf, as in the machine learning methods, plus two dimensionality reduction methods: nmf and latent semantic analysis (lsa) [cit] . unlike the machine learning classifiers, in the case of the information retrieval approaches, we determined that including the hierarchy in the training and validation was necessary in order to guarantee that enough vector comparisons can be made across all tags. consequently, to compare the performance of the machine learning methods versus the information retrieval methods, we used the tag & descendants performance. overall, with our current training data, the logistic regression classifier emerged as the best performing method for automated tagging."
"data science demands knowledge from many branches of mathematics and computer science, notably statistics and machine learning, and can be applied to multiple fields of study. given the field's interdisciplinary nature and its growing popularity, many open learning resources have been published on the web for anyone interested in learning about data science. however, these resources vary greatly in quality, topic coverage, difficulty and presentation formats, making entry into the world of data science confusing and daunting for learners."
"as shown in figures 8 and 9, the igc method designed by the adaptive dynamic surface method can well complete the falling point attack in the dive phase in the case of variable span and invariable span. the miss distance of the variable span missile is less than that of the variable span missile. in figure 9, the total flight time and the lateral distance error of the trace of the variable span are less than those in the invariable span."
"international journal of aerospace engineering span characteristics of the wing, the synergistic change of the span and the attack angle is completed to realize the control of the lift and achieve rapid and stable control of the centroid motion. the parameters of the igc method for span variant auxiliary control established above are optimized. the simulations of the variable span and invariable span are set for comparison. results show that the adaptive dynamic surface method can well complete the motion mission of the dive phase of the hypersonic variable span missile. at the same time, the variable span missile has a smaller miss distance and flight time than the invariable span missile and the terminal angular is closer to the design value. the variations of the attack angle pitching fin deflection angle are smaller. the monte carlo simulation method is used to analyze the robustness of the igc method for variable span missiles. compared with the invariable span of the exhibition, the results show that the cep of the variable span missile is less than that of the invariable span, which indicates that the robustness of the variable span missile is stronger than that of the invariable span missile."
"to study the effect of the span variant on the igc method, the hypersonic missile adopts the btt control strategy in the dive phase to show the influence of the span variant more clearly. therefore, the lateral force can be neglected. the influence of the lateral force in equation (2) is regarded as the uncertainty term. substituting eq. (18) into eq. (2) yields [cit]"
"resource quality and relevance are essential to the development of erudite. consequently, our initial resource collection focused on curated, reliable sources. while some sources provide resource data through public apis (e.g., coursera.org, udacity.com), most sources require scraping of websites intended for human navigation. for this, we built a modular framework using the popular python packages beautifulsoup and dryscrape to handle both static websites and dynamic, javascript-based pages, which have historically been problematic."
"span auxiliary control 3.1. igc model in the dive phase. in this paper, the igc design of the dive phase of the hypersonic variable span missile is studied. the fixed point on the ground is selected as the target, and the igc model in the dive phase with the terminal angular constraint is established. figure 3, the east-north-up (enu) coordinate frame is formed by a plane tangential to the earth's surface and is attached to the target. the east axis is labeled x, the north axis is labeled y, and the up axis is labeled z. define the line-of-sight (los) coordinate frame so that its origin is on the target where the s x -axis points to the vehicle, the s y -axis is in the horizontal plane of the target, and the s z -axis is determined using the right-hand rule. η d is the angle between the direction of velocity and the los, and γ d is the longitudinal velocity azimuth in the dive plane. r t is the distance between the center of mass of the variable span missile and the target point."
"(1) geometric parameters of the missile. l 1 is the length of the warhead. l 2 is the length of the total missile. d is the diameter of the main body and bottom of the warhead (2) variable span geometry. l 3 is the distance from the front end of the wing root to the nose of the warhead. c 1 is the chord length of the wing root. c 2 is the chord length of the wing tip. b 1 is the span of the wing. χ 1 is the sweep angle of the leading edge of the wing (3) geometric parameters of the tails. l 4 is the distance from the front end of the tail root to the nose of warhead. c 3 is the chord length of the tail root. c 4 is the chord length of the tail tip. b 3 is the span of the tail, and χ 2 is the sweep angle of the leading edge of the tail b 1 is the span of the invariant wing, and b 2 is the maximum span of the variable span wing. when b is assumed as the span of the wing in a variable span state, the morphing rate of the wing span is defined as"
"the missile characteristic parameters are shown: in table 1, the mass of the missile is m, and the mass of the wing is m 1 and m 2 and m2. the moments of inertia are i x, i y, and i z . the reference area is s 0 . the longitudinal reference length is c, and the lateral-directional reference length is b. international journal of aerospace engineering"
"during the simulation process, it is necessary to control the amplitude and rate of change of the attack angle command and the rate of change of the bank angle command as follows."
"the initial conditions and parameter settings are the same as those in table 3 and table 4 . after 100 times of monte carlo simulation experiments, the results of the variable span missile are as follows."
"our performance metric is the f1 score [cit], which is the harmonic mean of precision (positive predictive value) and recall (sensitivity). we calculate the average f1 metric over 5-fold cross-validation, and we perform a grid search over each method's specific hyperparameters with scikitlearn [cit] . 15 specifically, for each tag, we calculated the f1 score of the validation fold. then, we calculated the weighted average f1 across all tags with the weights equal to the number of true positives of each tag in the validation fold. then, we computed the average of the weighted f1 average across all validation folds to get the final performance metric for between-method comparison. to predict if a concept applies to a resource, we trained the systems under two conditions:"
"our relational database uses views to map source tables to our standard schema in order to remain flexible for any future changes in the schema. the scraping framework outputs source-specific tables, and the views in the database integrate the source data into a single schema model. we then use an additional reporting materialized view that joins relations defined by the schema to form a composite table that generates the data for resource detail pages for display and use on the bd2k tcc web portal (bigdatau.org). we generate an elasticsearch (elastic.co) index from a query to this table, and that index powers the search interface on the web portal. the resources are also tagged with concepts from an ontology (cf. section 2.3) that are used in a faceted search interface in the web portal."
"at the bd2k tcc, we have a strong focus on automated techniques. however, since we want to ensure that we serve high-quality learning resources, we plan to introduce a level of human curation into our pipeline. we have begun the development of a curation interface that will reduce the effort required to tag resources. the curation interface will allow users to assess the quality of a resource and validate the concepts predicted by our algorithms, as well as to suggest missing concepts from our concept ontology. the curated concept/tags will be used to retrain our tagging algorithms and improve their performance. we plan to measure the improvements in tagging accuracy and on the efficiency of human curators."
"we have established a collaboration with elixir tess to develop joint metadata standards for learning resources and to share data synergistically. beyond this collaboration, we want to help inform global learning resource standards. since schema.org is one of the largest and most popular standards in use today, researchers from both projects have joined the world wide web consortium (w3c) schema course extension group 11 in order to participate in the design of schema.org's course class extension, which we expect will provide the core metadata to describe learning resources."
"there are a variety of large-scale efforts across the world developing training resources, including mooc providers as well as large research consortia like the bd2k program. one effort of particular importance in the biomedical space is the elixir consortium, 10 which seeks to provide a distributed infrastructure for life-science across europe, in a spirit akin to the nih bd2k initiative. the elixir programme includes a training component, the training e-support system (tess), analogous to the erudite index in the bd2k tcc."
"where c d, c l, c n, m x, m y, m z, and ρ are, respectively, the nominal aerodynamic coefficient and atmospheric density. ν is the uncertainty parameter vector and satisfies"
"the erudite system is under active development. to reach our vision for erudite as a dynamically updated, personalized system suited for self-directed learning, we are pursuing the following research directions."
span missile 2.1. the shape and parameters of hypersonic variable span missile. the outline of the shape of the variable-span hypersonic missile selected in this paper is shown in figure 1 .
"in addition, in the study of the igc method of hypersonic variable span missile, the accuracy of the hypersonic aerodynamic model needs more attention in the subsequent research."
"by the combination of eq. (26), (29), and (34), an igc model for hypersonic variable span missiles with terminal angular constraints can be obtained as"
"in section 2 in the paper, the geometric model, aerodynamic model, and 6 dof motion model of the hypersonic deformation missile are established. in section 3, the igc model in the dive phase of the hypersonic morphing missile is established and the variable extension control method adapts to the igc method. section 4 displays the numerical simulation results and analyses of the igc method. section 5 is the conclusion for the paper."
"control. the igc model in the dive phase guidance control with terminal angular constraint is shown in eq. (37). the adaptive dynamic surface back-stepping control method is designed to make the system stable and make the output as close as possible to zero and make the system have strong robustness for uncertain factors. therefore, let us assume to make g 1 nonsingularity as"
"erudite currently contains 8,600 resources, but our collection efforts have been skewed towards materials from manually-selected high-quality sources, such as mooc providers. much pedagogically valuable written material is available online. we plan to increase the number of relevant written documents indexed in erudite. similarly, youtube provides many data science videos. we are expanding our resource collection efforts on written materials and youtube videos and developing automated techniques for scoring the quality of resources based on resource metadata (views, likes, length, etc.) and instructor and provider publications and affiliations."
"in order to exert the advantage of the variable span hypersonic missile, the morphing rate and attack angle of the wing are coordinated controls. the roadmap of the igc method for a hypersonic morphing missile based on variable span auxiliary control is shown in figure 4, while the igc method with invariable span is shown in figure 5 ."
"in the design of the ontology, we followed a multi-pronged approach. first, we identified a collection of concepts based on our knowledge of the data science domain and organized them hierarchically along six dimensions, which were defined and agreed upon by the authors based on the different facets that learners would want to use as filters on the resource collection. second, we collected and reviewed the categories used to describe resources in each of the existing sources (e.g., videolectures.org provides a categorization of its video collection). finally, we used two semi-automated methods to refine and extend the ontology."
"as shown in figure 10, the trajectory height of the variable span in the dive phase is lower than that of the invariable span. the velocity and mach number are larger than those of the invariable span. this is because the variable extension characteristic can adjust the lift-to-drag ratio of the missile, so that the resistance and velocity loss is smaller. at the same time, the terminal angular of the variable span missile is closer to the predetermined value than that of the invariable span missile which indicates that it is easier to realize the terminal angular constraint by the rapid change of lift due to the variable span. as shown in figure 11, the final dynamic pressure of the variable span missile is much larger than that of the invariable span, which is caused by the difference in the final velocity. it demonstrates that the variable span characteristics are beneficial to it can be seen from figure 12 that the attack angle of the variable span in the dive phase is gentler than the invariant span and the total average value is smaller. because the required change of the lift is performed by the change of span and attack angle, the change in the attack angle is gentler than the invariant span and the amplitude is also smaller. because of the influence of the span variant on the aerodynamic coefficient, the changing amplitude of the side-slip angle under the variable span is smaller than the invariant span, while the changes in the bank angles of the two are more similar. it can be seen from figure 13 that the rolling rate of the variable span has little difference from the invariable span while the yawing rate and the pitching rate have smaller amplitudes and smaller oscillation. as shown in figure 14, the variation angle of the pitch fin deflections of the variable span is smaller than that of the invariant span which indicates that the variable span reduces the workload for the attitude control system. figure 15 shows the morphing rate of the variable span missile in the dive phase. it can be seen that the variation of the span in the initial stage is more severe while in the middle and later stages the variation of the span is gentler, but the amplitude is increased. the combination of the change of span and attack angle achieves the change of lift request in the whole mission."
"as a first level of organization, we designed a hierarchical, multi-dimensional ontology to provide descriptions of the learning resources. this ontology provides learners with concepts that can assist them with resource exploration and discovery."
"in eq. (12), g is the gravity vector and v o is the velocity vector of the missile centroid. the component expressions in the body coordinate frame are [cit]"
"we implemented this model in two ways in the erudite system. internally, we store all the course metadata into a relational database. externally, in bigdatau.org, each web page for an individual learning resource includes its metadata in the json-ld 8 format to facilitate data exchange and indexing by search engines."
"we are also exploring other approaches for learning resource dependencies, such as exploiting the ordered entries in course syllabi and the tables of contents in textbooks. the discovery of dependencies or prerequisites for general concepts or for individual resources is essential to creating a rich learning experience."
"in this framework, each source website is handled by a module designed for the site's structure and idiosyncrasies. these require some manual authoring, but, once created, the site-specific module automatically collects resource data. the scraping framework is packaged as a docker image so it can be used without locally managing its dependencies. as a result, we were able to increase our resource collection efforts quickly because team members could simultaneously build new site-specific modules without disturbing the core infrastructure of the scraping framework."
"the terminal angle can be constrained by eq. (25). by substituting eq. (20) and eq. (21) into eq. (19), the relative dynamics of the target and the hypersonic variable span missile are denoted as"
"in eq. in (14), m x, m y, m z are the roll, yaw, and pitch moment coefficients, respectively. similar to eq. (11), the variant in the reference area caused by the span variant is converted into the equivalent aerodynamic torque coefficient as"
"as a second semi-automated method, we used non-negative matrix factorization [cit] to discover topics in our resources. we analyzed the most significant words associated with each topic and then defined a concept for each of the topics. much of this analysis confirmed the concepts identified earlier, but it also yielded ten additional tags."
"span missile. in order to verify the robust performance of the integrated guidance control method with variation of span, the influence of the aerodynamic coefficient uncertainty and the atmospheric density uncertainty of the system model is considered in the 6dof simulation verification model. the distributions of the above uncertainties are as follows."
"where k i min is the minimum value of the diagonal elements in each gain matrix k i . since the uncertainty w 0 satisfies the following inequality,"
"in summary, both in its design and in its creation, erudite uses the concepts and methods of the data science field that it aims to teach. erudite will enable students and researchers to make the best use of the diverse data science learning resources available online."
"in the whole dive phase. and ω is the bounded closed set in r 2, so ω is a tight set. the design steps of the adaptive dynamic surface backstepping igc method are as follows [cit] ."
"international journal of aerospace engineering in eq. (16), the subscript of each coefficient represents the variable contained in the coefficient matrix. according to the model identified by the obtained aerodynamic data, the structural form of each variable in eq. (17) is ma,ξ relative to the morphing rate. for the designing of the igc method, the aerodynamic load model needs to be processed as"
"where υ 2 and μ 2 are positive constants. in summary, the adaptive dynamic surface back-stepping igc method for variable span control of the system (3.20) can be expressed as then, the derivative of the filtering error is"
"the variant rate of the three-axis moment of inertia caused by the variable span is difficult to calculate accurately, so it is regarded as an uncertainty in the eq. (36)."
"to integrate the heterogeneous resource data, we designed a single metadata standard to represent learning resources in the erudite domain. to develop our standard, we reviewed and incorporated existing standards to facilitate cross-institution data sharing, including classes and properties from the dublin core, 3 learning resource metadata initiative (lrmi), 4 ieee's learning object metadata (lom), 5 exchanging course related information (xcri), 6 metadata for learning opportunities (mlo), 7 and schema.org vocabularies. our standard has three classes: learningresource (with 27 properties), person (with 8 properties), and provider (with 10 properties)."
"the national institutes of health (nih) launched the big data to knowledge (bd2k) initiative (datascience.nih. gov) to fulfill the promise of biomedical \"big data\" [cit] . nih recognized that \"the ability to harvest the wealth of information contained in biomedical big data will advance our understanding of human health and disease; however, lack of appropriate tools, poor data accessibility, and insufficient training, are major impediments to rapid translational impact\". 1 the nih bd2k program has funded 15 major centers 2 to investigate how data science can benefit diverse fields of biomedical research including genetics, neuroimaging, precision medicine, and mobile health. ensuring that the advances produced by these centers, and other research efforts, permeate the biomedical research community and yield the expected benefits for human health, requires a significant increase in the number of biomedical researchers trained in data science. to address this need, the nih has funded the bd2k training coordinating center (tcc)."
"the linked data movement [cit] seeks to make the data available on the web not only readable to humans but also to machines. the json-ld format is a popular way to insert structured data into regular web pages and contribute to the web of linked data. these structured data snippets can then be easily extracted by external tools and indexed by search engines. in particular, google encourages the use of json-ld over the schema.org vocabulary for this purpose. 9 in a spirit of open data sharing, we expose all the metadata for each of the learning resources in the erudite collection as linked data in the json-ld format. as part of our integration pipeline, we developed an automated mapping functionality from the resource database directly to a json-ld format using our previous work on data exchange embodied in the karma system [cit] . augmenting our published learning resources with json-ld structured data allows current and future collaborators to easily cross-reference any resource we collect, increasing data interchangeability across global efforts for educational resource indexing."
"while curation will initially be internal to the project, we envision later opening it to users of the web portal or crowdsourced workers, allowing us to re-train and validate our automated tagging algorithms at scale."
"since erudite is itself a data science project, its construction reflects some of the key stages in the data science workflow, namely data collection, integration, modeling, and visualization. we describe these processes next."
"each variable component of eq. (6) is expressed in the body coordinate frame. in eq. (2), l, d, n are the lift, drag, and lateral force of the missile, respectively, as"
"we analyze the computational cost of the algorithm by considering the delay between when a node is first proposed (i.e. based on distance traveled) and the time at which it is added to the map. this measure reflects the overall time required of the algorithm, since it will not add nodes until it has finished incorporating the most recent description and proposed loop closures. we consider the delay for the three longest datasets, namely the indoor/outdoor large tour, the mit 32-36-38 tour and the killian court tour. table 2 summarizes the performance for each of these datasets. note that the implementation has not been optimized to run in real-time and each particle is currently processed sequentially (i.e. particle updates are not parallelized). the variance in the delays is due to periods of increased computation that correspond to instances when language annotations are processed. this delay is dominated by two components of the algorithm. the first is the time required to ground allocentric descriptions using the g 3 framework for all particles. the second is the time taken to scan-match the semantic-based loop closures that are subsequently proposed between nodes with updated label distributions. allocentric language grounding requires computational effort that is linear in the number of unique particles. similarly, the scan-match verification is linear in the number of nodes that are updated with new label information, which is independent of the size of the map. the computational requirements for verification are dominated by a scan-match procedure that is exhaustive in its search due to the potentially large error in the prior pose-to-pose transform. table 3 outlines the accuracy of the resulting semantic maps for four datasets, where we calculate the accuracy as follows. first, we identify the regions for which language contributed to their label distributions. we compute the ground truth label for each of these regions and compute the cosine similarity between the ground truth multinomial (assumed to have a likelihood of 1.0 for the true label) and that of the label distribution."
"pie charts that compare the semantic map label distributions that result from (a) the egocentric language description \"this is the gym\" with that of (b) the allocentric language description \"the gym is down the hall.\""
"one distinction among existing approaches to semantic mapping, and topological mapping in general, is the means by which the environment is segmented into different places. [cit], for example, rely on a user to push a button each time the robot transitions to a new region. a straightforward automatic strategy is to segment regions based upon distance, placing vertices at a fixed spacing as the robot travels in the environment [cit] . this is the approach that we take in this paper. an alternative is to use heuristics such as door detection to separate regions [cit], which yields segmentations that can be more semantically meaningful within indoor environments. meanwhile, others have found success partitioning the environment by clustering observations of local metric [cit] and semantic (martínez [cit] ) properties. [cit] employ multiple methods to define regions, including manual segmentation at the location of gateways (e.g. doorways, junctions) and automatic segmentation based upon changes in visual appearance [cit] ."
"we evaluate our algorithm through six experiments that involve a human giving a robotic wheelchair (figure 1) [cit] a narrated tour of several buildings and courtyards on the mit campus. the robot was equipped with forward-and rearward-facing lidars, wheel encoders, and an imu. speech was recorded using a wireless microphone worn by the user. in the first two experiments, the robot was manually driven while the user interjected textual descriptions of the environment. in the third experiment, the robot autonomously followed the human who provided spoken descriptions. speech recognition was performed manually."
"early work in semantic mapping includes the spatial semantic hierarchy (ssh) [cit] that represents a robot's spatial knowledge as a coupled hierarchy. at the lowest level, the local environment is modeled as a collection of control laws, each expressing the relationship between sensory input and motor output, that facilitate localization and generating local geometric maps. above the control level is the causal level, which provides a discrete model of the actions that transition between each of the control laws. the topological level represents the environment as a collection of regions, places, and paths that abstract states and actions from the causal level. while the topology serves as the primary global map of the environment, the local geometric maps from the control level can be merged via the topology to formulate a global metric map. [cit] describe an extension to the ssh that employs a hybrid metric and topological representation to better represent environments at both small and large scales. the hybrid ssh treats the environment as a collection of interconnected locations, each being small in scale. the method employs metric maps to model the local geometry of distinct regions from which they use local paths to induce a symbolic global topology that describes the large-scale environment. by decoupling the map in this manner, this approach more efficiently models ambiguities in large-scale loop closure with multiple compact topologies, without requiring that the set of local metric maps be registered consistently in a single global reference frame. this is a distinct benefit over submap approaches to slam [cit] that similarly employ local metric maps but also seek to ensure that these submaps are consistent in a global reference frame. [cit] ) that the representation allows uncertainties to be handled more effectively by factoring them into individual components that capture local metrical, global topological, and globally metrical uncertainties. our algorithm also consists of a hybrid metric and topological representation and factors the joint distribution into separate metrical and topological terms, employing different hypotheses over the topological map to represent the distribution over the space of loop closures. however, we maintain a globally metric map of the environment with respect to a single frame of reference, which can make our algorithm sensitive to global inconsistencies within large environments. another difference lies in our definition of regions, which we segment based upon distance traveled. the hybrid ssh, in contrast, defines regions based upon their local geometric structure (e.g. separated by doorways). unlike our approach, however, the hybrid ssh does not model the semantic labels or colloquial names associated with different regions of the environment."
"we estimate a joint distribution over the semantic, topological, and metric maps, conditioned on the language and the metric observations from the robot's proprioceptive and exteroceptive sensors. the space of semantic graphs, however, increases combinatorially with the size of the environment. we efficiently maintain the distribution using a raoblackwellized particle filter [cit] to track a factored form of the joint distribution over semantic graphs. specifically, we approximate the marginal over the space of topologies with a set of particles, and analytically model conditional distributions over metric and semantic maps as gaussian and dirichlet, respectively. the algorithm updates these distributions iteratively over time using descriptions and sensor measurements as they arrive. we model the likelihood of natural language utterances with the generalized grounding graph ( g 3 ) framework [cit] . given a description, the g 3 model induces a learned distribution over semantic labels for the nodes in the semantic graph that we then use to update the dirichlet distribution. the algorithm uses the resulting semantic distribution to propose modifications to the graph, allowing semantic information to influence the metric and topological layers. this paper builds on our earlier work, which presents the initial semantic graph framework. we better place the contributions of our method in the context of the current state-of-the-art in semantic mapping and provide a more detailed description of our estimation framework, including the means by which we interpret natural language descriptions. additionally, we describe a new capability whereby the method reasons over and learns from anticipatory descriptions that refer to regions in the environment not currently in the map. the user can then describe locations that the robot may or may not have previously visited, enabling the robot to more efficiently learn semantic maps of the environment."
"in the third experiment, the robot autonomously followed a user during a narrated tour along a route similar to that of the first experiment . using a headset microphone, the user provided spoken descriptions of the environment that included ambiguous references to regions with the same label (e.g. elevator lobbies, entrances). the utterances included both egocentric and allocentric descriptions of the environment. the speech was recorded as it was uttered in synchronization with the lidar and odometry data. the audio was later manually transcribed into text that was inserted alongside the sensor observations according to the time that the audio was initially recorded. in this manner, the algorithm handled the text, lidar, and odometry data as they were received, emulating a scenario in which a speech recognizer was used to parse the user's utterances during the tour. (figure 11(c) )."
"our current framework was designed to learn a semantic map of an environment from an initial tour, with the idea that this map can then be used for localization, navigation, and grounding natural language commands during long-term operation. as with most pose graph approaches to slam, our algorithm may add regions that duplicate the same part of the environment, building maps that grow with time rather than space. this is particularly undesirable when the robot operates for extended periods of time within the same environment. we have recently updated our representation so as to reuse existing nodes in the graph, updating their metric, topological, and semantic properties, rather than adding new redundant nodes [cit] . in summary, we described an approach to learning human-centric maps of an environment from userprovided natural language descriptions. the novelty lies in fusing high-level information conveyed by a user's speech with low-level observations from traditional sensors. by jointly estimating the environment's metric, topological, and semantic structure, we demonstrated that the algorithm yields accurate representations of its environment."
"when proposing edges to the topology based upon the label distributions, we perform exhaustive scan-matching to check the validity of each proposed loop closure. while this helps to filter out the large majority of erroneous edges, the matching may yield false positives in regions that are perceptually aliased (figure 19 ). however, since the hypothesis space of potential language edges is large, the likelihood that all particles sample invalid edges is low, confining such occurrences to a small subset of particles. empirically, we have found that the weight of these particles is quickly reduced as their metric maps are inconsistent with subsequent sensor measurements. these particles then tend to be removed during resampling."
"the space of possible graphs for a particular environment is spanned by the allocation of edges between nodes. the number of edges, however, can be exponential with respect to the number of nodes. hence, maintaining the full distribution over graphs is intractable for all but trivially small environments. [cit] that the distribution over graphs is dominated by a small subset of topologies while the likelihood associated with the majority of topologies is nearly zero. in general, this assumption holds when the environment structure (e.g. indoor, man-made) or the robot motion (e.g. exploration) limits connectivity [cit] . in addition, conditioning the graph on the descriptions further increases the peakedness of the distribution, thereby increasing the validity of this assumption, because it decreases the probability of edges when the labels and semantic relations are inconsistent with the language."
"we evaluate our algorithm when the user provides descriptions in the form of egocentric language, in which case there is no ambiguity in the landmark and figure that are implicitly the robot's current location. figure 9 (b) presents the semantic graph corresponding to the highest-weighted particle that our algorithm estimates. by considering the semantic map when proposing loop closures, the algorithm recognizes that the second region that the user labeled as the gym is the same place that was labeled earlier in the tour. at the time of receiving the second label, drift in the odometry led to significant error in the gym's location much like the baseline result (figure 9(a) ). the algorithm immediately corrects this error in the semantic graph by using the label distribution to propose loop closures at the gym and elevator lobby, which would otherwise require searching a combinatorially large space. the resulting maximum likelihood map is topologically and semantically consistent throughout and metrically consistent for most of the environment. the exception is the courtyard, where only odometry measurements were available, causing drift in the pose estimate. attesting to the model's validity, the ground truth topology receives 92.7% of the probability mass and, furthermore, the top four particles are each consistent with the ground truth."
"we describe an approach first presented by the authors that enables robots to efficiently learn human-centric models of the environment from a narrated, guided tour ( figure 1 ) by fusing knowledge inferred from natural language descriptions with conventional low-level sensor data. our method allows people to convey meaningful concepts, including semantic labels and relations for both local and distant regions of the environment, simply by speaking to the robot. the advantage is that the robot can learn concepts that people are arguably better-able to convey from its opportunistic interaction with humans. the challenge lies in effectively combining these noisy, disparate sources of information. a user's descriptions convey concepts (e.g. \"the second room on the right\") that are ambiguous with regard to their metric associations: they may refer to the region that the robot currently occupies, to more distant parts of the environment, or even to aspects of the environment that the robot will never observe. in contrast, the sensors that robots commonly employ for mapping, such as cameras and lidars, yield metric observations arising only from the robot's immediate surroundings."
"we consider an additional experiment in which the robot was driven throughout different labs on the third floor of mit's stata center. the narrated tour involved both egocentric and allocentric descriptions of the environment, the latter of which were anticipatory in nature with the user referencing locations in the environment that the robot had not yet visited. figure 14 presents the maximum likelihood semantic map that our framework learned from the narrated tour using a total of 10 particles. the final topology contained 71 nodes. the system correctly grounds each of the allocentric descriptions despite the ambiguity that exists in the landmark and figure locations, as we discuss in more detail shortly."
"our algorithm differs from the existing state-of-the-art in semantic mapping in three fundamental ways. first, our framework employs a learned model of free-form utterances to reason over expressions that are less constrained than those handled by other methods. to be precise, we currently assume that these descriptions involve labels for and spatial relations between one or two locations, though the structure of these expressions is only limited by rules of grammar and the amount of training data. second, by using scene classification, existing methods can only infer semantic properties of areas that are within the field-ofview of the robot's sensors. similarly, previous efforts to incorporate user-provided labels assume that the object is within view or that the user is referencing the robot's current location. in contrast, our method reasons over more expressive descriptions that enable robots to learn concepts like labels and spatial relations for distant areas as well as regions of the environment that the robot has not yet visited. third, existing methods allow updates to the metric layer to influence the topological and semantic maps, but don't use information at the semantic layer to improve the rest of the hierarchy. by maintaining a joint distribution over the metric, topological, and semantic properties of the environment, our framework uses updates to any one layer to improve the other layers in the hierarchy. for example, we show how the semantic map can be used to recognize loop closures, a fundamental problem in slam, and thereby add edges to the topology that, in turn, correct errors in the metric map."
"an additional limitation of the algorithm is that it treats a region's colloquial name (e.g. \"carrie's office\") and its type (\"office\") jointly as being labels. thus, any subsequent reference to a labeled region is required to use the label that was originally given. we have recently extended our representation to model the type-name hierarchy for each region, where we infer the type from the aforementioned scene classifiers [cit] ."
"attributes (e.g. room type) that can be inferred from imageand lidar-based classifiers. however, general purpose classifiers are unable to identify many useful properties of an environment, such as the colloquial names associated with each region. for example, it would be difficult to recognize and infer the meaning of the question mark (figure 2) that indicates the location of the information desk at mit's stata center, which people frequently use as a reference point. furthermore, the dependence on onboard sensor streams prevents the algorithms from reasoning about parts of the world that are outside the field-of-view of these sensors. this has implications for the efficiency with which robots can learn human-centric representations of their environment."
"a known issue with sample-based methods such as ours is the problem of particle depletion [cit] whereby a majority of samples evolve to support regions of the distribution with negligible likelihood. this results in a poor approximation to the target distribution and can cause the filter to diverge. resampling the particles based upon a measure of the variance in their weights, as we do, reduces the likelihood of particle depletion. in practice, we have not found depletion to occur, as suggested by the results. we partially attribute this to using the distribution over the semantic map as part of the proposal, which reduces the frequency of erroneous samples. nonetheless, particle depletion may occur and can be mitigated by adding additional particles to hypothesize new topologies in the event that the distribution appears to misrepresent the target distribution, for example, as suggested by the particle weights [cit] ."
"for the indoor/outdoor large tour and the killian court tour, we also compared the results for the maps that did not propose language edges. since large segments of these maps were metrically and topologically inaccurate, we assigned a minimum score for regions that were significantly inaccurate. in effect, this corresponds to assigning these regions a uniform multinomial over labels. as can be seen for the first two datasets, the use of our approach improves the semantic accuracy of a number of regions. this improvement stems both from the metric and topological accuracy of the learned maps as well as the algorithm's ability to integrate allocentric language. in the mit 32-36-38 and the stata center tours, we also achieve reasonable accuracy for most categories. we do note that in case of allocentric language, some expressions can be ambiguous, either due to the presence of multiple potential landmarks or due to the ambiguity in the expression. for example, given the description \"the lobby is down the hallway,\" there may be multiple regions whose location is consistent with being \"down\" the hallway, of which only one is the lobby. in these situations, each of these regions will receive high likelihood of being the figure and the label distributions for each will be updated accordingly. additionally, we find that the accuracy of the semantic maps is sensitive to our choice for region decomposition. for example, hallways score fairly low under our fixed-size segmentation, which can significantly underestimate their spatial extent. we see these issues as inherent to our definition of regions that would be diminished with a more sophisticated segmentation strategy that takes into account local appearance [cit] and semantic (martínez [cit] ) properties of the environment."
"for utterances with allocentric language, our algorithm was able to generate reasonable groundings for the figure and landmark locations. however, due to the simplistic way in which we define regions, groundings for \"the lobby\" were not entirely accurate due to the sensitivity to the local metric structure of the environment when grounding paths that go \"through the entrance.\" we discuss this in more detail in section 6.1."
"we formulate the proposal distribution by first augmenting the graph to reflect the robot's motion. specifically, we add a node v t to the graph that corresponds to the robot's current pose with an edge to the previous node v t−1 that represents the temporal constraint between the two poses. we denote this intermediate graph as g we formulate the proposal distribution (5) in terms of the likelihood of adding edges between nodes in this modified graph g − t . the system considers two forms of additional edges: first, those suggested by the spatial distribution of nodes and second, by the semantic distribution for each node."
"an advantage of having a probabilistic model over the space of groundings is that it provides a means of recognizing when there is not enough information contained in the semantic graph to ground the language. this allows us to recognize many of the situations in which the user describes areas that either the robot hasn't yet visited or they reference landmarks whose labels were never added to the map. for example, it's not uncommon for the user to mention regions that are within sight but they have yet to reach (e.g. the user may say \"the lab is across the lobby,\" but the robot has never been to the region being referred to as \"the lab.\"). we refer to descriptions of this form as anticipatory."
"the results highlight the ability of our method to tolerate ambiguities in the labels assigned to different regions of the environment. this is a direct consequence of the use of semantic information, which allows the algorithm to significantly reduce the number of candidate loop closures that is otherwise combinatorial in the size of the map. this enables the particle filter to efficiently model the distribution over graphs. while some particles may propose invalid loop closures due to ambiguity in the labels, the algorithm is able to recover with a manageable number of particles. in this experiment, the algorithm employed 10 particles to approximate the distribution over topologies. the final topology contained 213 nodes."
"the algorithm learned a distribution over semantic maps from the stream of descriptions, odometry, and lidar data, using 10 particles to hypothesize the different topologies. the final topology contained 276 nodes. figure 17 shows the resulting maximum likelihood semantic graph overlaid on an approximately aligned map of the mit campus. qualitatively, the map is metrically, topologically, and semantically accurate with the exception of the map of building 14 where a glass hallway between buildings 2 and 14 forced the algorithm to use odometry for the inter-pose constraints. as with the previous evaluations, we ran our framework without language-based constraints to emulate the current state-of-the-art in language-augmented semantic mapping. while we omit the figure for space, we note that the resulting map is significantly warped."
"a fundamental contribution of our method is the ability for the semantic map to influence the metric and topological maps. this capability results from the use of the label distributions to perform place recognition. the algorithm identifies loop closures by sampling from a proposal distribution that expresses the semantic similarity between nodes. in similar fashion to the spatial distance-based proposal, computing the proposal requires marginalizing over the space of labels:"
"we note that semantic observations are not the only source of information that is useful for place recognition. many mapping algorithms build local laser scan patches for each region and correlate these patches to identify loop closures [cit] . however, these techniques are prone to perceptual aliasing when the local geometry is not distinctive, such as in the case of hallways. more recent methods consider a region's visual appearance as a more discriminative means of performing place recognition [cit] . [cit] learn a generative model of region appearance using a bag-of-words representation that expresses the commonality of certain features. by effectively modeling this perceptual ambiguity, the authors are able to reject invalid loop closures despite significant aliasing, while correctly recognizing valid loop closures. this and related approaches in effect choose the maximum likelihood loop closure, relying on the assumption that the place model is sufficiently descriptive that the resulting distribution over the space of loop closures is peaked around the true correspondence. our approach differs in that it uses semantic information to maintain a distribution over the space of loop closures rather than only that which is most likely."
"the left-most expression in this factorization explicitly models the dependence of the labels on the topology and the location of each region. the middle term encodes the conditional distribution over the metric map given the topology and, in this way, mimics pose graph formulations to slam, given the loop closure (i.e. the topology). the rightmost expression denotes the distribution over the graph conditioned upon the sensor history and language."
"is known (e.g. there are likely many \"halls\" in the environment). we use the label distribution to reason over the possible nodes that denote the landmark. in doing so, we make the additional assumption that the landmark exists in the graph and normalize the likelihoods for candidate \"hall\" nodes. we later relax this assumption as we describe shortly. we account for the uncertainty in the figure by formulating a distribution over the nodes in the topology that expresses their likelihood of being the figure. formally, we model the likelihood that each node v i is the figure by marginalizing over the space of candidate landmarks"
"the field is at the point where robots need to reason over human-centric models of space due in large part to the extensive progress that has been made in solving the simultaneous localization and mapping (slam) problem. not only have contributions to slam allowed robots to operate robustly in unstructured environments like our homes, but many existing approaches to semantic mapping are built upon slam algorithms. [cit], slam is predominantly concerned with building either globally metric or, to a lesser extent, topological maps for the purpose of navigation. our algorithm differs in that we represent the map as a hierarchy that jointly models the metric, topological, and semantic properties of the environment. the latter two layers are particularly useful for human-centric mapping as the semantic map models properties useful in grounding natural language commands [cit], while the topology is consistent with the representation that humans use to model space [cit] . we use the topological layer in our semantic graph to induce a pose graph in the same fashion as many of the state-of-the-art slam algorithms [cit] . given this topology, we represent the distribution over the metric map as a gaussian and, like information filter-based slam algorithms [cit], parametrize the distribution in the canonical form for computational efficiency."
"in an effort to better understand the accuracy with which the algorithm learns from environment descriptions, we consider regions whose semantic properties were inferred from allocentric utterances. figure 16 presents close-up views of the regions that were labeled as part of the multibuilding tour (figure 15 ). the portion of the semantic graph shown in figure 16 (a) results from two descriptions, \"the lobby is down the hallway\" and \"the elevator lobby is down the hallway,\" which were uttered at the locations indicated by the numbers \"1\" and \"2,\" respectively. the former utterance was anticipatory as the robot had not yet visited the lobby area when the description was given. nonetheless, the framework successfully labels that region of the environment when the robot later visits it, without any aliasing effects. however, grounding the second utterance results in high likelihoods associated with some nodes that are not actually in the elevator lobby, causing the label to \"bleed\" into other areas. we attribute this to the ambiguity that results from not reasoning over frame-of-reference without which the nodes are consistent with being \"down\" the hallway. the performance improves for the anticipatory utterance in figure 16 (d) where the algorithm waits to infer the location of the lab until it is visited. we see similar effects for the descriptions in figure 16 (c) where the system correctly infers the location of another elevator lobby but attributes the \"office\" label to nodes that are actually in a hallway. this results from a simple set of features that encode the \"near\" relation based upon distance. additionally, our algorithm uses a fixed separation to define regions and does not reason over their geometry (e.g. the shape of hallways is typically distinct from that of offices.) meanwhile, figure 12 (a) depicts the semantic information inferred for the utterance \"the lobby is through the entrance\" from the large indoor/outdoor tour where we see that the algorithm correctly grounds the location of the lobby without any aliasing."
"we evaluate our algorithm through six \"guided tour\" experiments that take place within mixed indoor-outdoor environments. we show that, by maintaining a joint distribution over the metric, topological, and semantic maps, the algorithm learns models of the environment that are richer and more accurate than can be achieved with existing language-based semantic mapping algorithms. we analyze the effectiveness with which the algorithm integrates semantic knowledge from natural language descriptions and demonstrate the utility of the learned maps for navigation."
"our goal is to induce a distribution over the semantic graph, including the locations, topology, and semantic labels, given information about an environment obtained from a robot's range sensors, odometry readings, and the user's descriptions of the environment."
"in order to verify the validity of the algorithm in different environments, we consider an extended tour of three connected buildings on the mit campus (buildings 32, 36, and 38). the robotic wheelchair was manually driven throughout the office-like environment, visiting offices, elevator lobbies, conference rooms, and lab spaces whose appearance and structure varied between each building. text was added at several points throughout the tour to emulate recognized natural language descriptions. we provided both egocentric and allocentric utterances, including several instances of anticipatory descriptions when the robot had not yet visited the referenced portions of the environment (both the figure and the referent). we ran our framework with 10 particles to model the distribution over topologies. the final topology contained 148 nodes. figure 15 denotes the maximum likelihood semantic graph that resulted from our algorithm (see also figure 16 for inset views). the text indicates the allocentric descriptions that were given to the system in the numbered order."
"semantic mapping [cit] addresses this need by providing robots with human-centric models of their environment. approaches often take as input low-level sensor (e.g. lidar, images) and odometry streams and infer metric, topological, and semantic properties of the environment. existing algorithms populate the semantic map with scene fig. 1 . a user gives a tour to a robotic wheelchair designed to assist residents in a long-term care facility."
"the algorithm incorporates allocentric language into the semantic map using the g 3 framework as described in section 4.3 to infer the nodes in the graph that constitute the figure (i.e. the \"gym\") and the landmark (i.e. the \"hallway\"). this grounding attributes a non-zero likelihood to all nodes that exhibit the relation of being \"down\" from the nodes identified as being the \"hallway.\" figure 10 compares the label distributions that result from this grounding with those from egocentric language. the algorithm attributes the \"gym\" label to multiple nodes in the semantic graph as a result of the ambiguity in the figure's location as well as the g 3 model, which yields high likelihoods for several paths as being \"down from\" the landmark nodes. when the user later labels the region after returning from the courtyard, the algorithm proposes a loop closure despite significant drift in the estimate for the robot's pose. as with the egocentric language scenario, this results in a semantic graph for the environment that is accurate topologically, semantically, and metrically (figure 9(c) )."
"the algorithm partitions the environment by instantiating regions at a fixed distance apart as the robot travels. this results in regions that are not semantically meaningful, with multiple regions being used to model the same area. consequently, the algorithm may ground language to the wrong node in the topology, either by inferring an incorrect landmark or by diffusing labels across multiple nodes. our latest work [cit] employs spectral clustering to segment the environment into regions based upon the (figure 15 ) in which (a) the algorithm accepts an invalid edge between different regions that have similar geometry for one particle. however, the majority of the particles did not propose erroneous edges and the weight of this map soon decreases to 1/10 th of that of the correct particle and is removed upon resampling."
"the information that we are currently able to infer from a user's descriptions is limited to a region's colloquial name and its relation to another region in the environment. it does not support a user's ability to convey general properties of the environment, such as \"you can find computers in offices,\" or \"nurses' stations tend to be located near elevator lobbies.\""
"given the posterior distribution over the semantic graph at time t − 1, we first compute the prior distribution over the graph g t . we do so by sampling from a proposal distribution that is the predictive prior of the current graph given the previous graph and sensor data, and the recent odometry and language."
"3 here, the algorithm identifies candidate loop closures between different \"entrances\" in the environment and accepts those (shown in green) whose local laser scans are consistent. note that some particles may add invalid edges (e.g. due to perceptual or semantic aliasing), but their weights will decrease as subsequent measurements become inconsistent with the hypothesis."
"this assumption ignores dependencies between labels associated with nearby nodes, but simplifies the form for the distribution over labels associated with a single node. we model each node's label distribution as a dirichlet distribution of the form"
"where f j is the figure being evaluated, p j is the path from the robot's location at the time of the description to the figure, φ j is the corresponding likelihood of the grounding, and l i is a corresponding landmark. for both types of expressions, the algorithm updates the semantic distribution according to the rule"
"these solutions rely upon scene classifiers and object detectors to infer the properties that make up the semantic map. the effectiveness of these approaches is a function of the richness of the training data. as such, they perform best when the environments have a similar appearance and regular geometry, and when the objects are drawn from a common set. even in structured settings, it is not uncommon for the regions to be irregular and for the objects to be difficult to recognize, either because they are out of context or are singletons (figure 2 ). furthermore, scene classification doesn't provide a means to infer the specific labels that humans use for a location, such as \"carrie's office\" or \"kiva conference room.\""
"3 . before proceeding, we briefly describe the g 3 [cit] . given natural language text, g 3 provides a distribution over the space of possible mappings between each word in the parsed description and the corresponding groundings in the external model. this distribution takes the general form"
"as with the smaller tour, we compare our method against the baseline semantic mapping algorithm. figure 11 (b) presents the baseline estimate for the environment's semantic graph. without incorporating allocentric language or allowing semantic information to influence the topological and metric layers, the resulting semantic graph exhibits significant errors in the metric map, an incorrect topology, and aliasing of the labeled places that the robot revisited. in contrast, figure 11 (c) demonstrates that, by using semantic information to propose constraints in the topology, our algorithm yields correct topological and semantic maps, and metric maps with notably less error. figure 12 presents the inset views for the lobby and second cafeteria portion of the map that were labeled with allocentric descriptions. the resulting model assigns 93.5% of the probability mass to the ground truth topology, with each of the top five particles being consistent with ground truth."
"starting in the north-east corner (figure 17), we gave the robot a tour along the infinite corridor that spans from left to right in the figure. after entering one of the main lobbies (upper-left), we proceeded through buildings 5 and 3 and then exited into the courtyard. we took a u-shaped path outside, entered building 4, and then traveled through buildings 6, 6c, and 14 before returning to the start. we provided both egocentric and allocentric language descriptions at different points during the tour to assign labels to and spatial relations between different regions. these descriptions took the form of text that was interjected in synchronization with the lidar and odometry streams as the data was post-processed."
"until recently, robots that operated outside the laboratory were limited to controlled, prepared environments that explicitly prevent interaction with humans. there is an increasing demand, however, for robots that operate not as machines used in isolation, but as co-inhabitants that assist people in a range of different activities. if robots are to work effectively as our teammates, they must become able to efficiently and flexibly interpret and carry out our requests. recognizing this need, there has been increased focus on enabling robots to interpret natural language commands [cit] . this capability would, for example, enable a first responder to direct a micro-aerial vehicle by speaking \"fly up the stairs, proceed down the hall, and inspect the second room on the right past the kitchen.\" a fundamental challenge is to correctly associate linguistic elements from the command to a robot's understanding of the external world (figure 1 ). we can alleviate this challenge by developing robots that formulate knowledge representations that model the higher-level semantic properties of their environment."
"we identify instances of anticipatory descriptions by using our distributions over the landmark and figure locations to evaluate the likelihood that the landmark matches a labeled region in the graph and that there are one or mode candidate figure regions consistent with the language. when the method is sufficiently confident in the ability to ground the language (we use a threshold of 0.2), we update the label distributions as described above. however, when the grounding likelihoods suggest an anticipatory description, the algorithm adds the expression along with its timestamp to a per-particle queue of anticipatory descriptions. as the robot proceeds through the environment and new nodes and semantic information are added to the map, the algorithm continues to evaluate the grounding likelihood (14) for the queued descriptions. specifically, we consider candidate pairs of landmark and figure nodes and determine the landmark's likelihood according to its label distribution. we express the figure likelihood as the probability of the landmark-to-figure path under the learned language model (16), where we consider the shortest path that runs from the robot's pose at the time of the description, through the landmark region, and on to the figure node. the logic is that the description is most useful when the robot has visited the regions to which the user refers and, thereby, the map has regions whose labels and inter-region paths are consistent with the expression. the algorithm performs this process separately for each particle, which may result in some particles incorporating the description sooner than others."
"the first experiment ( figure 9 ) took place on the first floor of the stata center at mit, which includes lecture halls, elevator lobbies, a gym, and a cafeteria, as well as the adjacent courtyard. starting at one of the elevator lobbies, the user proceeded to visit the gym, exited the building and, after navigating the courtyard, returned to the gym and finished at the elevator lobby. the user provided textual descriptions of the environment, twice each for the elevator lobby and gym regions. we compare the performance of our method based upon different forms of language input against a baseline algorithm that emulates the current state-of-the-art in language-augmented semantic mapping. in all cases, the algorithms were run with 10 particles to approximate the distribution over the space of topologies. the final topology contained 137 nodes."
"we consider a baseline approach that directly labels nodes based upon egocentric language, but does not propose edges based upon label distributions. it does, however, propose loop closures based upon the distribution over the metric map (section 4.1.1). the baseline emulates typical solutions by augmenting a state-of-the-art isam metric map with a semantic layer without allowing semantic information to influence lower layers. figure 9 (a) presents the resulting metric, topological, and semantic maps that constitute the semantic graph for the highest-weighted particle. the accumulation of odometry drift results in significant errors in the estimate for the robot's pose when revisiting the gym and elevator lobby. without reasoning over the semantic map, the algorithm is unable to detect loop closures. this results in significant errors in the metric map as well as the semantic map, which hallucinates two separate elevator lobbies (purple) and gyms (orange)."
"the final experiment considers a tour of killian court, a set of interconnected buildings on mit's campus, which has served as a benchmark environment for previous mapping algorithms. we consider this environment in an effort to see how the algorithm performs when tasked with mapping larger spaces that involve significant geometric and semantic aliasing. specifically, this part of the mit campus consists primarily of several long hallways with nearly identical structure, including the so-called \"infinite corridor\" that serves as one of the main hallways at mit."
"the second experiment (figure 11 ) considers an extended tour of mit's stata center as well as two neighboring buildings and their shared courtyard. in order to evaluate the algorithm's ability to deal with ambiguity in the labels, the robot visited several places with the same semantic attributes (e.g. elevator lobbies, entrances, and cafeterias) and visited some places more than once (e.g. one cafeteria and the amphitheater). we accompanied the tour with 20 descriptions of the environment that took the form of both egocentric and allocentric language."
"in order to understand an expression like \"the gym is down the hall,\" the system must first ground the landmark phrase \"the hall\" to a specific entity in the environment. it must then infer an entity in the environment that corresponds to the word \"the gym.\" one can no longer assume that the user is referring to the current location as \"the gym\" (the figure 4 ) or that the location of the \"hall\" (the landmark)"
"we have described an algorithm that estimates metrically accurate semantic maps from a user's natural language descriptions. the novelty lies in learning the joint distribution over the metric, topological, and semantic properties of the environment, which enables the method to fuse the robot's sensor stream with knowledge inferred from the descriptions. we have presented results from several experimental evaluations that demonstrate the algorithm's ability to infer accurate metric, topological, and semantic maps. however, there are several limitations to our current approach."
"to handle ambiguity, we propose a representation referred to as the semantic graph that jointly combines metric, topological, and semantic models of the environment. the metric layer takes the form of a vector of poses for each region in the environment together with the resulting occupancy-grid map that captures the perceived structure. the topological layer consists of a graph in which nodes correspond to reachable regions of the environment, and edges denote pairwise spatial relations. the semantic layer contains the labels with which people refer to regions. this knowledge representation is well suited to fusing concepts from a user's descriptions with the robot's metric observations of its surroundings."
the metric map models information contained in the robot's low-level sensor readings. the topological map models the connectivity between regions that can be inferred from navigation as well as natural language descriptions. the semantic map represents categories that the user conveys.
"the proposal step results in the addition, to each particle, of a new node at the current robot pose, along with an edge representing its temporal relationship to the previous node. the proposal step also hypothesizes additional loop-closure edges. next, the algorithm incorporates these relative pose constraints into the gaussian representation for the marginal distribution over the map"
"the algorithm performs this grounding process for each particle, and updates those for which the likelihood of the top pair is sufficiently high (0.2). in this example, the likelihood of the candidate groundings for most of the particles is low and the algorithm postpones language integration. as the tour proceeds (figure 18(b) ), the guide labels the robot's position as being the \"hallway,\" which updates the label distribution for the adjacent node. the algorithm again attempts to ground the language, this time using the newly added hallway nodes as the landmark. however, paths that start at the pose from which the description was first given and pass through the landmark to other nodes do not resemble the learned model for the \"down\" relation. after the robot and user continue and more nodes are added to the topology (figure 18(c) ), the framework again attempts to ground the description, this time returning highly confident estimates for the locations of the landmark and the figure, per the induced path. however, not all of the inferred locations are correct, which is consistent with what we see with other allocentric expressions. in this case, the system assigns \"elevator lobby\" labels to nodes that preceded the hallway as well as several nodes beyond the true location of the lobby (green box). we attribute this to the difficulty in dealing with frame-of-reference when grounding language as well as to using features for the \"down\" relation that attempt to accommodate a wide range of scales (i.e. the length of hallways differs significantly across the environments that we consider)."
where −1 t and η t are the information (inverse covariance) matrix and information vector that parametrize the canonical form of the gaussian. we utilize the isam algorithm [cit] to update the canonical form by iteratively solving for the qr factorization of the information matrix. [cit] for more information. figure 6 shows the resulting metric poses and their uncertainties.
"a consequence of maintaining a joint distribution over each layer of the semantic graph is that the framework is able to use knowledge of the semantic properties of the environment to update the topology and metric map. this improves the accuracy of the resulting semantic graph and, in turn, facilitates navigation. to better understand the effects on navigation efficiency, we consider the task of finding the optimal path between two nodes in the topology, as if the robot were asked to use the semantic graph to navigate from its current location to a named region in the environment. we examine the semantic graphs that we learned with and without language-based constraints for the two indoor/outdoor scenarios, the autonomous tour, and the killian court dataset. for each, we randomly picked 1000 pairs of start and goal nodes in the graph and used a graph search algorithm to find the shortest path through the topology, with equal cost for each edge in the graph. the same node pairs were used for each of the semantic graphs for a given environment. table 4 compares the average optimal path length through the graphs that result from our method and the baseline, which does not infer constraints from the descriptions. the graphs that we estimate when language influences only the semantic layer give rise to optimal paths that are noticeably longer than the paths reflected in the graphs that we learn by jointly estimating the semantic graph. this difference stems from the fact that our representation provides semantic-based edges that allow the planner to identify shortcuts in the topology that are otherwise not suggested by the baseline map, which mimics the current state-of-the-art in language-augmented semantic mapping."
"learning from allocentric expressions is challenging because their groundings are ambiguous-the places to which the user refers are often not obvious. consider the scenario outlined in figure 7 . the semantic map includes an area that has a high likelihood of being a \"lobby\" and a second believed to be a \"hallway.\" as the robot (triangle) continues to explore the environment, the user utters the description \"the gym is down the hall.\" descriptions like these are often ambiguous. for example, there may be multiple \"hall\" regions in the map or it may be that the robot has yet to visit the region that the user is referring to, or if it has, it is not aware of its label. similarly, several regions in the map are candidates for being the \"gym,\" but the user may also be identifying a region that is not yet in the map."
"where θ l t denotes the label-dependent likelihood that edges exist between nodes with the same label. in practice, we assume a uniform saliency prior for each label. equation (9c) then measures the cosine similarity between the label distributions. we sample from the proposal distribution (9) to hypothesize new semantic map-based edges. as with distancebased edges, we validate proposed edges by building local maps for each region and performing scan-matching between these maps. in practice, we additionally introduce a bias that penalizes matches between frequently occurring regions like hallways. figure 5 shows several different edges sampled from the proposal distribution at one stage of a tour."
"the algorithm operated in this fashion using 10 particles to approximate the distribution over the space of topologies. the final topology contained 135 nodes. figure 13 presents the maximum likelihood semantic graph that our algorithm estimates. by incorporating information that the descriptions convey, the algorithm recognizes key loop closures that result in accurate semantic maps. the resulting model assigns 82.9% of the probability mass to the ground truth topology, with each of the top nine particles being consistent with ground truth."
"a contribution of our work is the use of natural language descriptions to produce consistent semantic maps from spatial relations and labels inferred from language. the advantage of this capability is that it allows robots to more efficiently acquire human-centric maps of their environment. the challenge to learning from these expressions is that their groundings are ambiguous-the user may refer to regions that may be distant from the robot and outside the field-of-view of its sensors. additionally, it may be that the descriptions are anticipatory, when the robot has yet to visit the figure that the user is describing or the landmark that they are referencing. figure 18 depicts the process of learning from an anticipatory description as part of the stata center lab tour (figure 14) . figure 18(a) shows the robot traversing a hallway when the user states that \"the elevator lobby is down the hallway.\" at this point, the semantic graph includes several nodes with a high likelihood of having the label \"hallway.\" however, the robot has yet to visit the specific hallway that the person is using as the landmark and, as a result, the semantic graph does not include nodes for this region. the graph also lacks nodes for the region that the user refers to as the \"elevator lobby.\" the algorithm attempts to ground the description using the language model as described in section 4.3, which yields a likelihood for each pair of nodes as being the landmark and the figure."
"next, we consider the algorithm's performance when the figure and landmark regions that the user's descriptions reference can no longer be assumed to be the robot's current position. specifically, we replaced the initial labeling of the gym with an indirect reference of the form \"the gym is down the hallway,\" with the hallway labeled through egocentric language. the language inputs are otherwise identical to those employed for the egocentric language scenario and the baseline evaluation."
"this factored distribution is represented as a graphical model using a factor graph, such as the one shown in figure 8 for the \"the gym is down the hall\" utterance. the g 3 algorithm uses a log-linear model for each of the factors"
"which allows us to utilize the conditional measurement model. in the experiments presented next, we model the measurement as an observed scan-matched transformation between poses, which we compute via scan-matching. we model this distribution (first term in the integral) as gaussian, which we have empirically found to be accurate. [cit],"
"our method is capable of inferring semantic information only from the user's descriptions. this means that the algorithm can only model a region's label if it was specifically referenced by the user. further, it precludes the method from incorporating allocentric descriptions for which the user never labels the landmark. for example, the algorithm cannot learn from the description \"the gym is down the hall\" unless the user identifies the location of the hallway. our recent work [cit] alleviates this requirement by also using geometric-and appearance-based scene classifiers to infer semantic information from lidar and vision."
