text
"in order to accurately quantify dietary food intake from images, it is important to address the issue of occlusion. with the proposed approach, the issue of tackling self-occluded food object items can be regarded as a view synthesis problem. with the advances of artificial intelligence, several research groups have proposed view synthesis based on the deep neural network [cit] . however, to the best of our knowledge, there has still been no research conducted on object volume estimation based on view synthesis. with a 3d database of sufficient size, a neural network can implicitly learn the 3d shapes of different object items and further explore how the objects will look like with different viewing angles. besides, with the generalization capability of deep learning, the network can predict the results of using input images from an unseen viewing angle or even using unseen object items. in this approach, the occluded object items can be restored at any convenient viewing angle, and this undoubtedly facilitates accurate quantification of dietary intake."
"the procedure of using deep learning view synthesis to access dietary intake is presented in the following text, and the system diagram is shown in figure 1 : (1) a mobile phone with depth sensors or a depth camera (intel realsense/microsoft kinect) is used. (2) rgb and depth images are captured from any convenient viewing angle. note that rgb and depth images should be synchronized. (3) rgb image should be segmented and classified based on the image segmentation approach (ex. mask r-cnn [cit] ) (4) the corresponding regions in the depth image are labeled accordingly. (5) point cloud completion with the deep learning view synthesis is then applied to each labeled object item to perform 3d reconstruction and estimate food volume as shown in figure 2 . (6) once the volume is measured, the data information can be linked to usda national nutrient database for further dietary analysis [cit] . note that this work mainly focuses on the application of deep learning view synthesis in volume estimation, so food classification and dietary analysis are not discussed in this paper."
"given the inferred depth image, a completed 3d point cloud of the targeted object item can be obtained by registering the camera coordinates of the initial and opposite depth images into the same world coordinate. by doing this, the traditional method is to firstly obtain the extrinsic calibration matrices of the initial image. once the extrinsic matrices have been determined, the synthetic points can be fused with initial points to obtain a completed point cloud through a transformation matrix. however, the extrinsic matrices are unobtainable without the presence of fiducial markers in normal situations. with regard to this issue, a simple technique is proposed to reconstruct 3d point clouds without the use of fiducial markers. first, the position of the origin should be moved to the center of the initial camera so that the depth image can be reprojected into a world coordinate which is shown in equation (2):"
"to the best of our knowledge, this paper is the first study to use deep learning view synthesis to tackle the problem of occluded view and to perform volume estimation. since there is no prior work, this paper aims to conduct a preliminary study to evaluate the feasibility, efficiency, and accuracy of using deep learning to estimate object volume under the circumstance of occluded views. thus, most of the experiments were designed for the research investigation and are limited to laboratory settings. nevertheless, to facilitate the development of objective dietary assessment techniques to enhance the performance of dietary assessment, a pilot study would be an important step forward. the main contributions of this paper can be summarized as follows: (1) the effectiveness of a depth camera-based dietary assessment technique is examined. (2) a comparison among different start-of-the-art food volume estimation approaches is summarized (3) a novel neural network framework is proposed to reconstruct 3d point clouds of different food object items based on view synthesis. (4) a new database is constructed through image rendering to evaluate the performance of the network architecture. (5) a modified icp algorithm is proposed to enhance the performance of 3d reconstruction. the rest of the paper is organized as follows: section 2 details the methodology of image rendering, the neural network architecture, and our proposed volume estimation techniques. section 3 presents the experimental results of the proposed point cloud completion and icp algorithms in volume estimation. the discussion is presented in section 4. the paper concludes in section 5."
"despite current approaches showing satisfactory results in the measurement of dietary food intake from camera images, there are several limitations and constraints in the practical application of these approaches. an integrated approach based on the depth-sensing technique and deep learning view synthesis is proposed to enable accurate food volume estimation with a single depth image taken in any convenient angles. apart from its efficiency, this technique also aims to handle food items with self-occlusion, which is one of the major challenging issues in volume estimation. we demonstrated that the proposed network architecture and point cloud completion algorithms can implicitly learn the 3d structures of various shapes and restore the occluded part of food items to allow better volume estimation. through validating with 3d food models from the yale-cmu-berkeley object set, the results show that the proposed technique achieves an accuracy in volume estimation of up to 93%, which outperforms other techniques proposed in previous studies. overall, we found that integrating different approaches could be one of the potential solutions to handle challenging issues in dietary assessment. image-based dietary assessment will definitely play a key role in health monitoring."
"where θ is the rotation angle of the camera along the y-axis. the translation matrix refers to the translation between the initial and opposite camera positions. this extrinsic parameter can be obtained through the proposed neural network as mentioned in the previous part. once the rotation and translation matrices have both been obtained, the synthetic point cloud can be registered to the same world coordinates by equation (4)."
"in the field of view synthesis, most of the experimental evaluations that have been carried out in previous works have only involved comparisons on a qualitative basis. the reason for this is that there is still no conclusion as to which quantitative measure is the most suitable one to evaluate the quality of the generated images. on the contrary, with the use of the depth image as the image input in this study, the scale of the object items can be determined. thus, the volume can be used to evaluate the performance of the proposed algorithms quantitatively. a comparison of the volume estimation result of our approach with the ground truth to determine the accuracy of the volume estimation method is shown in table 4 . given the dimensions (height, width, and length) of the object items in the yale-cmu-berkeley object dataset, the ground truth volume can be computed based on the given object dimensions. for the objects with a general shape, such as the orange, cube, tuna fish can and pudding box, the volume was easily computed based on geometric calculations. for irregular objects, however, the volume could not be accurately computed using the dimensions provided. it is for this reason that we used the mean estimated volume v g as another reference volume to evaluate the performance of the proposed algorithms for objects without ground truth volume as prior knowledge. the mean estimated volume v g was computed based on the point cloud completion algorithm (listed in equations (2)- (4)) using the known extrinsic parameters generated during image rendering. fifteen trials were carried out for each object item. as shown in the table, the results of the volume estimation based on point cloud completion were promising with only a 2.4% error (accuracy of 97.6%). this error was obtained by taking the average of the error of the volume estimation-based on point cloud completion for different object items with ground truth volume. this robust performance implies that this technique can be used as a reference to measure the volume of object items without the ground truth. the mean estimated volume v p was computed based on the icp algorithm (listed in equation (5)) using the extrinsic parameters estimated by the proposed neural network and optimized by the icp algorithm. the error of v p was compared between the ground truth volume and the estimated volume v p when the ground truth volume was provided. for the object items without ground truth, the error was computed using the mean estimated volume v g and mean estimated volume v p . the overall performances of the proposed algorithms are shown in table 4 . small standard deviations in the error can be found in the results due to different initial viewing angles and the graph of estimated volumes with standard deviations is shown in figure 9 . furthermore, in order to better visualize the performance of the point cloud completion algorithms, the experimental results of 3d reconstruction on chosen object items with unseen captured viewing angles are presented in figure 10 . in addition, the processing time for carrying out point cloud completion was examined in this work to evaluate its real-time operability. the whole procedure took 0.84 ± 0.02 s to estimate the volume of a single food object, thus outperforming stereo-based volume estimation methods."
"after the point cloud map has been completed, the next step is to estimate the volume of the food items. the common technique to carry out volume estimation is to mesh the object item based on the convex hull algorithm [cit] . in using this approach, however, object items should be assumed to be convex which largely affects the performance of volume estimation. on the contrary, the alpha shape is applied in our approach which has no limitation on the shapes of object items [cit] . by using alpha shapes, a sphere with a fixed radius should firstly be defined and a starting point should be chosen from the contours of the object items. the sphere is then rotated with its circumference around the object item from the starting point until the sphere hits another point on the contour. the sphere is then transfered to this point and the process repeats until the loop closes. in figure 6, the global point cloud of a 3d banana model has been converted into a 3d mesh based on the alpha shape. once the 3d mesh has been obtained, the volumes of the object items can be easily estimated. figure 6 . the global point cloud of a 3d model has been converted into a 3d mesh using the alpha shape."
"in our experiment, 3d models with irregular shapes, including a banana, orange, pear, cube, potted meat can, lemon, tuna fish can and a pudding box, from the yale-cmu-berkeley object dataset were chosen to explore the feasibility of our integrated dietary assessment approach. the image rendering technique was used to render depth images for each of the object items. to examine the feasibility of the proposed network in inferring depth image from unseen viewing angles and to avoid over-fitting, it is essential to carry out a thorough evaluation. similar to the method used in previous works of point cloud completion [cit], the holdout method, a simple kind of cross validation, was used to evaluate the performance of the model in which 70% (20 k images per object item) of the rendered depth images were used to train the neural network, while 10% (2.85 k images per object item) and 20% (5.71 k images per object item) of the images with unseen viewing angles were selected as the validation dataset and testing dataset, respectively. the advantage of this validation method is that it shows the generative ability of the approach without the computationally expensive process of training and re-training large convolution networks. to compare the results of depth estimation with our proposed architecture and the naive version, the training and testing loss are listed in table 3 . from the table, it is shown that the proposed network with inception layers and extrinsic parameter prediction outperformed the naive version and the network without inception layers in both the training and testing sets. furthermore, the graphs of training and testing loss versus iterations for the naive version and the proposed version (with inception layers and extrinsic parameter) are plotted respectively in figure 7a,b. both figures show that the testing loss was comparable to the training loss which indicates that the trained models are generic and able to tackle the images captured from unseen viewing angles without over-fitting. the graphs of testing loss versus iterations for both of the models are plotted together to evaluate the improvement of the network with inception layers and extrinsic parameters as shown in figure 7c . from the figure, we can see that the loss of the naive model dropped and converged faster than the proposed model. nevertheless, the testing loss of the proposed model was lower after millions of iterations while the naive model saturated in the middle of the training. from the experimental results, it is possible to conclude that the network with different kernel size and extrinsic parameters prediction can implicitly learn the object details in a more efficient way and which leads to improvement of the accuracy of depth estimation. in addition, the inferred depth images based on the proposed architecture are shown in figure 8 . experimental results generated by the proposed network architecture to evaluate the performance by using input images with unseen viewing angles. each row refers to different object items. each column refers to the input image, ground truth image, and inferred depth image respectively. table 3 . comparison of the performance of the naive version, the proposed network with extrinsic parameter prediction, and the proposed network with both inception layers and extrinsic parameter prediction."
"several important issues related to image-based food volume estimation have been raised, such as view occlusion, scale ambiguity, and feature extraction problems. these challenging issues have long been discussed in the field of vision-based dietary assessment, yet there are still no solutions that can address all of the mentioned problems. for instance, the stereo-based approach is one commonly used technique to measure food portion sizes. this approach relies strongly on feature matching between frames in which the volumes of certain food items with deformable shapes can be measured. in using the stereo-based approach, a larger variety of food items can be estimated compared to other approaches. however, one major concern of using the stereo-based approach is that the 3d models cannot be reconstructed and the volume estimation will fail if the food surface does not have distinctive characteristics or texture (e.g., fruits, rice). furthermore, another concern is that it requires users to capture multiple images from certain viewing angles, which, in turn, makes this approach very tedious to use. these findings show that dietary assessment based on a single image seems to be one of the future trends in dietary assessment. as we know, the model-based approach, which is based on a single image, is one of the most reliable volume estimation methods nowadays. despite the robust performance of this approach, it involves high levels of human intervention and requires participants to rotate, shift, and scale the pre-built food models to find the models that best fit the food items in the images. in addition, another drawback is that the pre-built food models library usually consists of 3d models with simple shapes only, such as sphere, cylinder and hemisphere in which the performance of volume estimation is largely affected when food items are of irregular shapes. in this paper, a deep learning view synthesis approach is proposed to address these major issues in food volume estimation. through evaluating with food items of irregular geometric shape and non-obvious external features, the experimental results show the promising performance of the proposed technique in volume estimation with accuracy levels of up to 97.6% and 93.1% (using the proposed point cloud completion and icp algorithms respectively). these findings show that the proposed method outperforms the state-of-the-art volume estimation techniques. this implies that the combination of the depth-sensing technique and deep learning view synthesis has strong potential for accurate image-based dietary assessment. for view occlusion, the proposed algorithms have shown the ability to restore the self-occluded part and provide a complete point cloud of the object items. nevertheless, one of the problems worth looking into is that we found that the neural network cannot efficiently handle object items with serious center shifting, as shown in figure 3 . the object item in the inferred depth image will appear in an inaccurate position which affects the following point cloud completion. it is for this reason that an inaccurate position of the object item refers to a wrong rotation matrix. in future work, we are planning to train the neural network with more input parameters such as by giving the center points of the items in order to speed up the convergence of the network. for scale ambiguity, we proved the feasibility of using the depth camera to obtain the actual distance information and assess food intake without fiducial markers. this finding shows that depth-sensing technique could have growing potential for accurate dietary assessment. in addition, since this is the first study to use deep learning view synthesis to estimate food volume, most of the experiments were designed for research purposes and were limited to laboratory settings. further works are required to evaluate the performance of the algorithms with real scenes. though the proposed model can only handle certain food items from unseen viewing angles at this stage, there has been a lot of progress on the basis of the existing approach (e.g., the model-based approach). to better leverage the generalization capability of deep learning, a more comprehensive 3d model database is being built to train the network. with sufficient training data, the models should then be able to handle various geometric shapes or even unseen food items that are not in the training dataset."
"to access dietary intake, a variety of aforementioned visual-based dietary assessment methods have been proposed which can be further classified into two categories: image-assisted and image-based approaches. compared to image-assisted approaches, image-based approaches can be more objective and unbiased in assessing users' nutritional intake as they do not require much human intervention, such as manual image recognition and analysis. in this paper, a novel image-based technique using deep learning and depth sensing technique is proposed to address several long-standing problems in the field of image-based dietary assessment in which view occlusion and scale ambiguity are major challenges. in this section, the detailed information and methods about deep learning view synthesis are discussed as follows: (1) the procedure of using deep learning view synthesis to perform dietary assessment is presented to show how the proposed technique works. (2) image rendering is followed to demonstrate how the new database is constructed to evaluate the proposed architecture in volume estimation. (3) the proposed network architecture for depth image prediction is explained. (4) a modified point cloud completion algorithm is then presented to show how the initial and inferred depth images are registered and reconstructed into a complete global point cloud. (5) a new icp algorithm is shown to optimize the performance of point cloud registration and to enhance the accuracy in volume estimation. (6) three-dimensional meshing, a well known technique for volume measurement, is discussed."
"a major drawback of naive upscaling is that mass increases with the cube of the rotor radius. the industry avoids the prohibitive mass increase by improving the blade design, which has resulted in blades that are more slender for a given power rating, where the increase in loads (and therefore mass) can be kept low. this further results in blades with increased capacity factors."
"the above work does not model the rotation, which is important to get the correct local angle of attack along the blade and thus accurately compute the forces acting on the blade. several 3 [cit], and another which studied the mexico rotor [cit] . used a continuous adjoint formulation to perform single-point aerodynamic shape optimization using a compressible rans model. in 2-d, they reduced drag starting from a naca 4412 profile baseline by 4.86 % under imposed thickness constraints. they used a total of 50 design variables and completed 10 design iterations. in 3-d, they improved the torque coefficient on a mesh with 7.9 million cells by 4 % using 84 shape variables with no constraints imposed on geometry or loads. the free-form deformation (ffd) box covered part of the blade such that both the trailing edge and the innermost part of the blade could not deform."
"the case study is defined with a cut-in speed of 4 m s −1 and a cut-out speed of 25 m s −1 . within this range, we use the eight operational conditions defined in table a1 to compare the solvers."
"gradient-based, gradient-free, and hybrid approaches have all been used to optimize airfoils using panel codes. an example of a gradient-based optimization approach is the risø-b1 airfoil family, which currently is in commercial use by several manufacturers. [cit] described the design and experimental verification process, where they used an in-house mdo tool. they carried out the numerical design studies using xfoil [cit] and used the velux wind tunnel for 2-d experimental verification. due to concerns with xfoil's accuracy in predicting separation, they opted to verify the optimization results using the cfd code ellipsys2d, thus combining fidelities in an attempt to balance speed and accuracy."
"where f c is the continuum value, f 1 and f 2 are the values obtained using the l0 and l1 meshes, respectively, and r is the grid refinement ratio."
"having verified that the resulting shapes for the multipoint full shape optimizations are much improved, we now compare the multipoint optimization results to other optimization results in table 9 . whereas the single-point bem1 result (8.06 %) is close to the single-point planform optimization result (11.07 %), the multipoint bem2 result (22.46 %) is comparable to the multipoint full shape optimizations result (23.76 %) since relative thicknesses can change in both cases. the multipoint result (23.76 %) is somewhat higher than the single-point full shape optimization result (15.89 %), which can be explained by the relaxed thrust constraint for multipoint optimizations. here, we use the thrust from the 12 m s −1 case instead of the 8 m s −1 case to define the initial constraint values for thrust and bending moment. indeed, the thrust constraint relaxation results in the constraint not being active at convergence for the cfd-based multipoint full shape optimization, as seen in table 9 ."
"in this study, we run ellipsys3d using the third-order quadratic upwind interpolation for convection kinematics (quick) scheme and the k−ω sst [cit] model to calculate the turbulent eddy viscosity, which compares favorably to other turbulence models for wind turbine applications [cit] ."
"single-point planform optimization: this is the same as the iea wind task 37 problem (eq. 8), except with the objective of maximizing torque for a single wind speed. we solve this problem because it is well suited for comparison with bem."
"medium-fidelity vortex methods are popular aerodynamic models in wind turbine applications. vortex theory is based on potential flow, which does not model the viscous effects modeled in rans cfd. however, it does provide a more realistic solution than bem codes while still keeping the computational cost low compared to cfd. well-established vortex codes in the wind energy community include the general unsteady vortex particle (genuvp) code [cit], the aerodynamic wind turbine simulation module (awsm) [cit], and the method for interactive rotor aeroelastic simulations (miras) ."
"in the adjoint equation (eq. 5) and total derivative equation (eq. 6), we need to provide two matrices and two vectors of partial derivatives. as mentioned above, these derivatives involve only explicit operations and are in principle cheap to compute. however, they still require the differentiation of parts of a complex cfd code, and a good implementation is essential to preserve the accuracy and efficiency of the adjoint approach. traditionally, adjoint method developers have derived these partial derivatives by differentiating the equations or code manually and programming new functions that compute those derivatives. this process is labor intensive and prone to programming errors. [cit] pioneered the use of automatic differentiation to compute the partial derivatives. automatic differentiation is a technique that takes a given code and produces new code that computes the derivatives of the outputs with respect to the inputs [cit] . using a pure automatic differentiation approach to compute our derivatives of interest, df/dx, would mean applying the automatic differentiation tool to the whole cfd code, including the iterative solver. while this produces accurate derivatives, it is not an efficient approach. by selectively using automatic differentiation to produce code that computes only the partial derivatives, which do not involve the iterative solver, we lower the adjoint implementation effort while keeping the efficiency of the traditional adjoint implementation approach. there are still many details involved in making our adjoint implementation approach efficient; these details have been presented in previous work [cit] ."
"the ffd boxes are used to apply the pitch, twist, chord, and shape variables to each blade. since we want all three blades to have the same pitch and shape, the variables are forced to be the same. furthermore, the ffd boxes have two fixed sections close to each other at the root to ensure c 1 continuity there, while the seven outer sections are free to move and deform the blades. pitch, x pitch, is achieved by rotating all free ffd sections by the same amount along the reference axis, which is at 35 % of the chord from the le. twist, x twist, is achieved by rotating each spanwise section of ffd control points independently. the chord variables, x chord, are achieved by scaling each spanwise section in the chord and thickness direction. thus, the relative thickness at each section is preserved during the cfd planform optimization. only for the full shape optimizations, where the shape variables are added, can the relative thickness change. the shape variables, x shape, move each control point independently in the direction perpendicular to the chord to control the airfoil shape."
"the slice at 35 m shows the least consistent comparison, which we suspect is due to the large amount of separation present both at suction and pressure side. given that the solvers use different turbulence models, it would be surprising to find a perfect match at this position. we also note that the pressure side separation results in a c p curve with a typical flat, squeezed shape in the 30 % closest to the trailing edge (te). the c p curves for the sections at 64 m span and 84 m span show, in general, a better likeness to one another. early investigations showed that the chordwise distribution of cells has a distinct impact on the solvers' ability to capture the stagnation point and suction peak. therefore, we chose a distribution that seemed to have enough cells close to the stagnation point while still having an adequate amount of cells to resolve the te area. in general, the adflow suction peaks seem to be more pronounced than those from ellip-sys3d. the same can be said for the blunt te, where the adflow c p curve again has a more pronounced spike. figure a1 . total thrust (a) and torque (b) as a function of wind speed for the rotor geometry used as the starting point for the optimization computed using mesh l0. as expected, the torque increases rapidly from cut-in speed to the rated speed at 12 m s −1, which is also where the thrust peak occurs. from rated to cut out, the torque curve flattens. here, the pitch setting found with steady-state bem results using hawcstab2 (seen in gray) clearly does not result in the cfd solvers tracking rated power accurately due to the model changes. adflow consistently overshoots the ellipsys results, which is consistent with the trend seen in table 4 . operational conditions for the eight simulations are given in table a1 . figure a2 . spanwise distribution of the normal force (a) and driving force (b) for the 8 m s −1 case listed in table a1 . figure a3 . surface-restricted streamlines from the ellipsys solution for a wind speed of 8 m s −1, both for the pressure side (a) and the suction side (b) for the perturbed design we use as a starting point for the optimization. the operational conditions are listed in table a1 . figure a4 . surface-restricted streamlines from the ellipsys solution obtained using the original dtu 10 mw wind turbine geometry for the 8 m s −1 case in table a1 both for the pressure side (a) and the suction side (b)."
"we now briefly describe all components of the optimization framework. the overall workflow is shown in fig. 1 using an extended design structure matrix (xdsm) diagram [cit] ). an initial set of design variable values, x (0), is given to the optimizer. the optimizer passes the current design variables to the surface deformation module, prompting it to update the surface mesh (except for the very first iteration). the surface deformation module also provides analytic derivatives of the surface mesh with respect to the design variables, dx s /dx. after the surface mesh has been updated, it is passed to the volume deformation module, which updates the volume mesh and computes its analytic derivatives with respect to the surface mesh, dx v /dx s . then, the flow solver computes the flow states, w. these states are passed to the adjoint solver, which computes the total derivative. finally, the objective function, f (e.g., torque), as well as its derivatives, df/dx, are provided to the opti-mizer, which computes a new step for another optimization iteration. both the surface and volume deformation steps are fast explicit operations. on the other hand, the flow and adjoint solvers are costly iterative operations that take up the vast majority of the computation time. the optimization process involves o(10 2 ) major iterations, which is an absolute minimum bound on the number of cfd solutions and mesh updates; there are additional cfd solutions within each major iteration."
"there has been an increasing interest in blade extensions and winglets for wind turbines, since they can offer a cost-2 lrp stands for light rotor project. 3 https://energiteknologi.dk/node/1197 (last access: 18 [cit] ) effective alternative to a complete blade redesign for sitespecific performance enhancements. [cit] explore such a design problem. they used 12 design variables to maximize the energy production while satisfying certain load constraints from the original blade design. [cit], they also used a surrogate model that they trained using a random sampling strategy. here, they seek a more balanced design by using multiple wind speeds throughout the sampling. using gradient-based optimization on the resulting surrogate model, they obtain a power increase of 2.6 % by adding a winglet, while not increasing the flapwise bending moment at 90 % radius."
multipoint full shape optimization: this is the same as the iea wind task 37 problem (eq. 8) but with the addition of blade shape variables.
"cfd-based aerodynamic shape optimization is still rarely used in wind energy research, but both the aerospace and the automotive communities have been using it increasingly often [cit] . however, when it comes to low-fidelity shape optimization, the wind energy community has a large body of work."
"the use of 3-d cfd is particularly valuable near the turbine blade root and tip, since the blade element momentum (bem) method uses empirical models to capture 3-d effects for these regions. the increase in fidelity also allows us to explore out-of-plane features such as blade pre-bend and winglets, which is outside the scope of traditional bem approaches."
"there were a few necessary changes we made to the iea case study, but only for the full shape optimizations. one such deviation is the dashed segment connected to the thickness limit curve in fig. 6, which prevents negative cell volumes. furthermore, there are constraints applied to the le and te of the ffd box. the le/te constraints (shown in red in fig. 6 ) are only implemented for the single-point and multipoint full shape optimizations. these constraints force each pair of points to move exactly the same amount in opposite directions, so that the midpoint in the segment remains stationary. this ensures that the individual ffd control points do not apply skewing twist, since they are meant to control only airfoil profiles. finally, we mention that the thickness limit is fully imposed only for the fourth thickness constraint (counting from the le), while the remaining nine constraints in a section are relaxed to not unnecessarily restrict the possible design space."
"bem has also been coupled to structural models with different levels of fidelity. [cit] to study possible configurations to achieve bend-twist coupling resulting in load alleviation. they found that the highest load reduction is obtained by combining (passive) bendtwist coupling and (active) individual pitch control instead of using only a single approach. [cit], who maximized aep without exceeding the original overall loads of a 10 mw reference wind turbine (rwt). they achieved a 8.7 % aep increase through passive load alleviation without an increase in the blade mass and only minor increases in the loads, despite blades that were 9 % longer. the parameterization was comprised of 60 [cit], they computed the gradients with finite differences. after an initial step size study, they ran a reduced set of design load cases to obtain the final turbine design, which was then evaluated on the full design load basis. their work is a demonstration of the power of integrating design approaches."
"for the planform optimization, described in sect. 5.1, both twist and chord are controlled at the seven outer ffd sections along the blade, which results in 14 design variables. the high-fidelity planform optimization results are visualized in figs. 10-12, which show the final chord and twist distributions as well as the history of the convergence and merit functions."
"constraints ( figure 20 . comparison of normal (a) and driving (b) forces for baseline and optimized designs. the shape optimization increases the normal force, and the peak has also moved further inboard. the driving force is increased considerably both at the root and close to the tip region."
"to analyze the optimized designs from single-point and multipoint shape optimizations in more detail, we plot the spanwise forces for both optimized and baseline designs in fig. 20 ."
we now turn to the shape and pressure (c p ) distributions for the baseline and optimized geometries in fig. 15 . the optimized blade increases the chord near the root. this design trend agrees with the planform optimization result.
"to optimize with respect to large numbers of variables, gradient-based algorithms are the only hope if one wishes to achieve convergence to an optimum in a reasonable amount of time [cit] . the efficiency of gradient-based optimization is dependent in large part on the cost and accuracy of computing the gradients. finite differences provide a way to compute gradients that is easy to implement, but they are subject to numerical errors, and they scale poorly with the number of design variables [cit] ."
"the motivation for this multipoint optimization is to take a whole range of wind speeds into consideration to achieve a more robust design. we consider both cases for normal power production and also cases leading to peak loading conditions. the design optimization problem and model are the same as those for the single-point optimizations (detailed in sect. 5.1), except for the objective function. the objective function here is the aep estimate, which we describe in sect. 5.1."
"where we have only partial derivative terms that can be found analytically at a low computational cost. the linear system in this equation can either be solved by computing the solution jacobian, dw/dx, from the linear system from eq. (3) or by solving the adjoint system:"
"the cost of the adjoint method is independent of the number of design variables because the adjoint equation (eq. 5) does not contain x. however, if there are multiple functions of interest f, we need to solve eq. (5) for each f with a different right-hand side. given that our problem has o(10 2 ) design variables and only a few functions of interest, the adjoint method is particularly advantageous."
"these vortex codes have been widely used in analysis, but applications to design optimization have been less frequent. [cit], who use the complex-step method to carry out the gradientbased optimization of a winglet. researchers have also developed analytic gradient computation for vortex methods by reformulating the vortex dynamics using the finite element method (fem) [cit] . however, bem is still well entrenched and is currently the default choice for optimization."
"it is clear from fig. 5 that mesh level l2 is very coarse and yields very different results. as we will demonstrate later, the suggested design trends from such a coarse mesh can sometimes lead to savings in computation time and, other times, lead to completely wrong design trends. thus, one should use such coarse meshes with care. we report the results ob-tained with l2 throughout the presented work to substantiate this claim."
"1. enforcement of geometric constraints to ensure structural feasibility, 2. normal operation rotor load constraints limiting thrust and flapwise bending moment, 3. more precision and stability in the convergence of flow and adjoint solvers, 4. inclusion of a turbulence model in the adjoint solver, 5. a comprehensive set of design variables, and 6. modeling and deformation of the entire blade shape."
"in table 3, the present work is compared to the above-cited 3-d shape optimization efforts on wind turbine rotors. as previously mentioned, structural considerations are crucial in wind turbine design. [cit] partially addressed this issue by coupling the nsu3d rans solver with the astro structural finite element solver through a fluid-structure interface to converge on realistic, steadystate loads on the swift rwt. they used abaqus to make a finite element model with shell elements. they performed a purely structural optimization of the composite blade, with the loads computed by the cfd. the optimization's objective was to, using gradient-based optimization, minimize the off-axis stress with respect to 16 310 ply orientation variables. they completed 10 optimization iterations considering five different load cases and achieved a reduction in the maximum fatigue stress between 40 % and 60 %. they did so without adding any constraints, but they did assume the material to be a single-ply, unidirectional fiber composite for each blade section. the logical next step would be to perform the simultaneous optimization of the structural sizing and aerodynamic shape optimization, as is already done in aircraft wing design [cit] ."
"in table 4, we can also see that the two solvers tend to converge towards the same thrust and torque continuum values -0.3 % difference for thrust and 0.7 % difference for torque. based on the results in this table, we determine that the l0 mesh represents a reasonable compromise between accuracy (less than 10 % error) and speed."
"integrated loads, in the form of thrust and torque, have been computed for each simulation in table a1 and are visualized in fig. a1 . as seen, the adflow results are consistently higher than the ellipsys3d results. this trend could partially be accounted for by applying the mentioned prandtl-glauert correction to the incompressible computations but is also a result of adflow results on mesh l0 not being fully mesh independent, as shown in table 4 . as a low-fidelity reference, we have added the integrated loads (in gray) from steadystate bem results using hawcstab2. a general agreement between the cfd results and the hawcstab2 results can be seen, save for the torque value at 25 m s −1, which could be corrected with a slight change in pitch setting given in table a1 . agreement is expected between ellipsys3d and bem since the airfoil data used in bem are computed using ellipsys2d."
single-point pitch optimization: this is used to maximize torque on the turbine with respect to blade pitch for a single wind speed (which in this case is equivalent to maximizing aep). the purpose of this case is to validate the newly implemented rotational terms in the adjoint solver.
"adflow is a compressible rans solver based on sumb [cit], a structured fvm cfd solver written in fortran 90 that uses cell-centered variables on a multi-block grid. unlike ellipsys3d, adflow uses the spalart-allmaras (sa) turbulence model [cit] and works with state variables computed using the jameson-schmidt-turkel (jst) scheme. [cit] implemented overset mesh capability. adflow is wrapped with python to provide a more convenient user interface and to facilitate integration with optimization algorithms and other components of an mdo framework."
"we now derive the adjoint equations and briefly explain how they are assembled and solved. a detailed description of the implementation is provided in previous work [cit] . the cfd solver computes the flow field, w, for a given set of design variables, x, by converging the residuals r(x, w) of the governing equations to zero. then, any function of interest, f (x, w), can be computed. gradient-based optimizers require the gradient of the objective and constraint functions with respect to the design variables. to compute this gradient, we use the equation for the total derivative:"
"wind turbine rotor optimization aims to maximize wind energy extraction and has been an important area of research for decades. a common metric is to minimize the levelized cost of energy (lcoe), which can be decreased by lowering installation costs and operating expenses or by increasing the annual energy production (aep). simply upscaling the turbine leads to an increase in swept area, which in turn extracts more energy. however, a naive upscaling does not capture the complexity of the problem [cit] ."
"the multipoint optimization problem presented in this section is functional but should be further improved in the future table 9 . overview of optimization results. as further detailed in sect. 5.1, the single-point and multipoint optimizations use the operational conditions for the 8 m s −1 case and 5, 8, and 11 m s −1 cases, respectively. operational conditions are listed in table a1 ."
the volume deformation tool is called idwarp and is based on the inverse distance weighting function [cit] . idwarp is a fast and unstructured deformation algorithm that has been demonstrated in aerodynamic [cit] and aerostructural applications [cit] .
"industry still relies heavily on bem, given that the 3-d cfd shape design of rotors poses several challenges. one of these challenges is modeling all the load cases that drive the design during an optimization. much work has been done in steady-state computations with steady uniform inflow, but to truly generate realistic loads, one should transition to turbulent inflow and accurately resolve the time domain. this poses an immense challenge in terms of memory and computation time and is an active area of research."
"we use the sparse nonlinear optimizer (snopt) [cit] for all optimizations herein. snopt implements a sequential quadratic programming (sqp) algorithm. we use it through the open-source python wrapper py-optsparse 4, which provides a common interface to this and other optimization software. the convergence in snopt is set through the \"major optimality tolerance\" setting [cit] . we aim at converging all optimization problems to 10 −4 ."
"as we can see in fig. 10, the optimized shape for the finest mesh level has a large increase in chord towards the root and a decrease in chord towards the tip, just as we would expect for an aerodynamically optimized blade. the optimized chord distribution is reminiscent of the dtu 10 mw turbine's chord distribution from fig. 2, which was also designed for maximum power. however, the dtu 10 mw root chord is not as high due to a constraint on maximum chord of 6.2 m. turning to the optimized twist (green curve) in the lower plot in fig. 10, we see it exhibits a large variation to- wards the tip compared to its baseline. the result is a more aggressive twist distribution."
"the complex-step derivative approximation method is an alternative to finite differences that is much more accurate but still scales linearly with the number of variables [cit] . this method has been widely used, including in some wind energy applications [cit] . some efforts tried to reduce the computational cost by using semi-empirical gradients [cit], surrogate models [cit], and mixed-fidelity models [cit] ."
"in this section, we first introduce the design optimization problems for all the cfd and bem cases we solve. we then explain the ffd parameterization, geometric constraints, and rotor load constraints."
"in this work, we use adflow as the cfd solver in the design optimization due to its adjoint gradient computation and integration with geometry parameterization, mesh deformation, and optimization tools. however, ellipsys3d has been more thoroughly validated for wind turbine rotor flows, so in this section, we verify adflow against ellipsys3d for a threebladed pitch-regulated rotor geometry. in this section, we only include a mesh convergence study for one operational condition. a more detailed flow comparison is included in appendix a."
"for the single-point shape optimization results, we see, as expected, an overall large increase in tangential loading across the blade, and we observe that a high loading is achieved in the root region of the blade as well. this is partially due to the chord increase but also due to the fact that the blade is optimized based on modeling that accounts for the complex three-dimensional flow field, which is particularly dominant in the root region. the thrust constraint and moment constraint were both essential for the design to be industrially relevant for the single-point result: the thrust constraint helped lower the overall thrust values to maintain structural feasibility. the bending moment constraint resulted in a change in the normal force distribution, where the peak moved farther inboard to reduce high loads close to the tip region, as one would expect. based on the optimization output, we can verify that both constraints are active for the single-point optimization, meaning that thrust and moment have reached the upper limits of 14 % and 11 % increase in thrust and moment, respectively. in the multipoint full shape optimization, the moment constraint is again active at an 11 % increase in bending moment. however, the thrust constraint is only at 11 % and is, as mentioned, not active at convergence due to the relaxed constraint. with these constraints, we could add span as a design variable in future work."
"the normal force acts normal to the rotor plane and, integrated over all three blades, yields the rotor thrust. likewise, the torque can be derived from the driving force by integrating its first moment along all three blades."
"we adapt and extend the design optimization problem from the iea wind task 37 case study, which is to maximize the aep for a range of wind speeds by varying chord and twist, while constraining the increase in thrust and bending moment to be no more than 14 % and 11 %, respectively. thickness constraints are enforced over the blade to ensure structural integrity. mathematically, the iea wind task 37 design optimization problem can be expressed as follows: maximize aep with respect to twist chord (8)"
"we start the remainder of this paper with a literature review on wind turbine optimization. we then explain the methodology (sect. 3), followed by a comparison between the compressible flow solver and an incompressible flow solver (sect. 4). the design optimization problem is presented in sect. 5, followed by the optimization results in sect. 6. we end with our conclusions in sect. 7."
"we investigated the advantage of using higher-fidelity models by comparing our optimization results to low-fidelity bem results from the same case study. we did this through a planform optimization with chord and twist variables, where shape changes were restricted to keep the design case comparable with the bem-based optimization. the overall design trends were the same across fidelities, with differences due the parameterizations and models. the same overall amount of improvement was observed."
"here, the partial derivatives correspond to derivatives of explicit functions, while the total derivative involves the iterative solution of the governing equations. thus, the partial derivatives can be found analytically at a low computational cost, but the direct computation of the total derivative dw/dx should be avoided. a similar total derivative equation can be written for the residuals, which must remain zero for the cfd solution to hold, and thus"
"the history of convergence and merit functions are shown in fig. 17 . just as for the single-point optimization, the se- lected threshold is not quite met. however, as before, the scaled merit function flattens enough that we determined that the design is close enough to the optimum."
"a3 spanwise forces, pressure distribution and flow visualization figure a2 shows the spanwise forces and shows that the difference between solvers is more or less spread out over the entire span. not surprisingly, the adflow values are consistently higher. we will revisit the distribution of spanwise forces after the optimization to inspect where performance increase occurs on the blade. turning to the surface-restricted streamlines in fig. a3, we first note the rather large amount of separation. even the pressure side shows a distinct area of separation from 19 to 41 m span. comparing said area with the pressure side separation for the unperturbed dtu 10 mw rotor in fig. a4, where only a small separation area at the root is seen, it is clear that the perturbed design we use as a starting point for the optimization seen in fig. a3 suffers a more poor aerodynamic design owed to the reduced chord distribution and increase in relative thickness. the suction side in fig. a3 looks more like one would expect, save for the expanded separation area reaching just above 37 m in the spanwise direction. here, the dtu 10 mw only has separation below the 32 m span, as seen in fig. a4 ."
"ellipsys3d is an in-house, structured, multi-block, finite volume method (fvm) flow solver developed at dtu wind energy by michelsen (1992 michelsen (, 1994 and sørensen (1995), and we use it in the present work to perform the comparison between cfd solvers. it discretizes the incompressible rans equations using general curvilinear coordinates and couples velocity and pressure through the simple algorithm."
"as we will detail later, gradient-based optimization algorithms, combined with an adjoint method for computing the gradients, provide a powerful approach to address largescale problems. for multidisciplinary systems, it is necessary to compute coupled derivatives, which presents additional challenges [cit] . [cit] introduced the application of the coupled adjoint method to the mdo of wind turbines."
"in fig. 6, the thickness constraints are highlighted in blue. the thickness constraints in the bem comparison are only enforced on the inner 80 % of the blade, as detailed in the definition of the iea case study. this is also visualized in fig. 6c ."
"we now solve the full shape optimization problem as a single-point optimization. as stated in sect. 5, this problem is equivalent to optimizing for torque, when only a single wind speed is used. figure 14 shows convergence (left) and scaled merit function (right) histories for the free-form shape optimizations. since we typically request an optimization convergence tolerance that is smaller than what is possible for the level of the cfd solver convergence, the optimizer stops before the optimization convergence tolerance is met. comparing the convergence history to similar plots for the pitch and planform optimizations ( figs. 8 and 11 ), we see that as the mesh is refined, the optimization is better converged, and the finest mesh level almost meets the requested tolerance (black dashed line). however, the scaled merit function plots (fig. 14b ) do seem flat for l2 and l1 (albeit the latter curve is less smooth), hinting that the merit function could have plateaued. table 8 shows the improvement achieved by the optimization. the achieved improvement on the finest mesh (15.89 %) is higher than that of the planform optimization (11.07 %, table 7 ), which is expected because this case includes all the planform design optimization variables plus the additional freedom to optimize the airfoil shapes. one should not compare these results to the pitch optimization results since they do not include any thrust constraint. a comparison to the bem code results is given farther down in table 9 once the multipoint optimization results have been presented."
"in spite of the contributions cited above, many improvements are needed before we achieve the ultimate goal of providing a \"push-button solution\" for wind turbine manufacturers. this paper contributes with some of these improvements by including all of the following features in a comprehensive high-fidelity 3-d rans-based shape optimization framework:"
"in this work, we presented results from the high-fidelity rans-based shape optimization of a 10 mw rwt. based on our literature review of the high-fidelity shape optimization efforts in wind turbine design, we determined that this was a promising area of research. we compared two state-of-the-art compressible and incompressible cfd solvers to quantify the mesh dependence and discrepancies across different rans models applied on the same rotor. the results were compatible, and future work involving classical compressibility corrections was identified."
single-point full shape optimization: this is the same as the single-point planform optimization but with the addition of blade shape variables. this problem takes advantage of the additional design freedom that is not available for bem-based models.
"the results are split into the four main problems listed in table 5 . first, we perform a single design variable optimization where pitch is varied to maximize the torque (sect. 6.1). this simple optimization is included to validate the adjoint formulation for rotating frame of reference flows. second, we perform a planform optimization where chord and twist are varied (sect. 6.2). this optimization is well suited for comparison with bem results because the airfoil shapes do not change. the two final optimizations are full shape optimiza- tion problems where all variables, including airfoil shape variables, are allowed to change. first, we solve the problem as a single-point optimization (sect. 6.3). then, we solve it as a multipoint optimization (sect. 6.4)."
"the thickness is handled by interpolating between the predefined airfoil data. while both bem1 and bem2 use specified airfoil polar data, bem2 can change the relative thickness of the airfoils. the airfoils vary from 72 % to 24 % in relative thickness."
"ideally, one would include all the relevant disciplines in such an optimization. this has been addressed in previous work using bem-based aeroelastic tools combined with various cross-sectional analytical or finite-element-based structural tools. [cit] showed that simultaneous design of the aerodynamic shape and structural layout of a blade leads to passive load alleviation. this was achieved through bendtwist coupling, which increased the aep without increasing loads and blade mass. the lcoe has been minimized by other researchers while taking aerodynamics, structures, and controls into account, thereby truly treating it as an mdo problem both for 5 mw turbines [cit] and for 20 mw turbines [cit] . while we could tackle high-fidelity aerostructural optimization using tools that have already been demonstrated in aircraft wing design [cit], we focus solely on aerodynamic shape optimization in the present work."
"comparing the results across mesh levels, there is a much larger spread than for the pitch optimization. the result using the coarsest (l2) mesh is significantly different from the ones obtained with the finer meshes (l1 and l0); therefore, the l2 mesh is too coarse to obtain physically representative results, which is consistent with the mesh convergence study (table 4) . we cannot rule out that, in some cases, the l2 result can be useful to perform a warm start sequence, as shown for the pitch optimization ( fig. 7 and table 6 ). however, the planform results certainly show that one should use the l2 mesh with care and not for final results. figure 11 shows the convergence history for the three mesh levels. again, all optimizations were converged to at least 10 −4 . in fig. 12, we see a similar trend to that of the pitch optimization ( fig. 9), where much of the improvement is gained in the first half of the optimization. thus, an easy way to speed up the design process would be to take an intermediate design. however, one should make sure to check the constraint feasibility, since sqp methods often explore infeasible regions before fully converging. the sharp initial decrease for l1 is due to the (infeasible) warm start from l2. note that the function is scaled differently for each mesh level to accommodate all the results in one figure."
"it has long been known that the design of wind turbines is inherently a multidisciplinary endeavor. there have been more than two decades of research where bem has been coupled with elastic beam models to account for structural deflections and material failure [cit], 2001; [cit] ."
"for large numbers of variables, the adjoint method provides an efficient way to compute the required gradients [cit], a fact that has also been verified in the wind energy community [cit] . the adjoint method is the subject of the next section."
"the bem optimizations are performed with snopt. the baseline and optimized chord and twist distributions are wind energ. sci., 4, 163-192, 2019 www.wind-energ-sci.net/4/163/2019/ shown in fig. 13 . although both chord and twist distributions show clear discrepancies for the final designs, there are several similar traits. when it comes to chord, there is a large difference in maximum chord. bem1 converges to a 26 % increase, bem2 converges to a 74 % increase, and the cfd optimization converges to somewhere between these two (43 %). bem1 is the surprising result of the three, because it seems that the relation between power and thrust is so poor that it makes little sense to increase the chord at the root. this is owed to the fact that bem1 has fixed relative thickness for all sections. it makes sense that bem2 can increase the chord further since it can change the relative thickness. given that our cfd-based planform optimization also has fixed relative thickness, it also makes sense that the bem2 chord values are larger than those from the cfd-based planform optimization. both the bem1 and the bem2 results have a steeper, more pronounced increase in chord values, which we suspect our cfd framework could not reproduce due to difference in the parameterization. the two innermost fixed ffd sections ensuring the c 1 mesh continuity make such a steep increase in chord impossible so close to the root. as a final comment on the discrepancies at the root, we suspect that bem profile data for such thick airfoils are far from precise. besides, the empirical 3-d correction used on said 2-d profile data is also likely to be imprecise. needless to say, the combination of the two could yield shaky results. to make matters worse, we know from the comparative analysis ( fig. a3 ) that separation reaches up to about 37 m span, which further complicates the situation. a more uniform picture is seen for the tip region where the chord distributions have converged to a reduced chord, where only minor differences can be seen. in conclusion, the overall trends in optimal chord distribution are mirrored across the bem and cfd models, and the discrepancies are less pronounced towards the tip."
"bem codes have been used extensively throughout the wind energy community for aerodynamic optimization. these codes are easy to implement and incur low computational cost. robustness has been an issue in bem codes, as they do not always converge [cit] . robustness is critical, especially when the analysis is part of an optimization cycle. a lack of robustness will slow down the convergence in the best case, and interrupt the optimization altogether in the worst case. [cit] re-parameterized the bem equations using a single local inflow angle, resulting in guaranteed convergence."
"as briefly mentioned in the introduction, there are two modes for automatic differentiation: the forward mode and the reverse mode. [cit] had used automatic differentiation in forward mode to compute and store the flow jacobian, ∂r/∂w, as well as the other partial derivatives. then, these stored matrices are used by the adjoint solver to compute transpose-matrix-vector products to converge the adjoint solution, . using the reverse mode, no storage of the jacobian is needed. instead, a matrix-free approach is used, where the transpose-matrix-vector products required to converge the adjoint solution are computed directly through the reverse mode derivative routines. while the reverse mode is more efficient in terms of memory usage, the reverse mode implementation was missing the rotation terms required for wind turbine modeling. we have fixed this for the implementation in the present work and use the reverse mode instead. the implemented reverse ad routines may also lead to speed up depending on the number of krylov iterations needed to converge the adjoint system."
"to obtain truly practical wind turbines. first, the laminar to turbulent boundary layer transition should be modeled, since this affects the optimal airfoil shapes. in this work, we just assumed the boundary layer to be turbulent throughout. second, a wider range of operating points should be considered by, for example, varying the rotation rate or pitch setting for a given wind speed."
"finally, full shape optimization was performed with respect to twist, chord, and airfoil shape design variables, which raised the number of design variables from 14 to 154. here, the planform results were further improved with a factor of 1.44. the improvement was enabled by a decrease in relative thickness as well as the novel airfoil shapes."
"we find the same overall trends for the multipoint results as we did for the single-point optimization. the relaxed thrust constraint for the multipoint optimization results in a rotor with slightly higher loads, which explains why the more robust design from the multipoint optimization outperforms the single-point result."
"traditionally, the blade design optimization process has been sequential, where the optimization of airfoils and planform are performed in two distinct steps. in the present work, we optimize the airfoils and the planform concurrently using 3-d computational fluid dynamics (cfd). this concurrent design optimization process is vital for the industry because, as previously shown, concurrent design processes result in a larger gain compared to sequential counterparts [cit], which is the main principle in multidisciplinary design optimization (mdo) [cit] ."
"we first turn to the airfoil shape to assess the effect of adding geometrical constraints while taking multiple angles of attack into consideration. the airfoil shapes for the multipoint optimizations are compared to the single-point ones in fig. 18 . as we can see, the le shapes are somewhat improved but still unrealistically sharp. this points towards the necessity of including off-design operational cases resulting in wider ranges of angles of attack, where such a sharp le would result in deterioration in performance."
"this literature review on wind turbine optimization is divided into three overall approaches: those that use lowfidelity and multi-fidelity models (sect. 2.1), approaches that use cfd models without adjoint sensitivities (sect. 2.2), and approaches that use cfd models with adjoint solvers (sect. 2.3)."
"as shown in fig. 8, all optimizations converged to an optimality of at least 10 −4 (black dashed line). figure 9 shows the merit function, which combines the scaled objective function value and constraint feasibility. the merit function value is equivalent to the scaled objective function value when all constraints are satisfied towards the end of the optimization process. as we can see in fig. 9, the curves flatten towards the end, and further iterations are not worthwhile because the optimizer reaches the limit of what it can achieve with the provided precision of the function evaluations. the pitch optimizations are summarized in table 6 ."
"the single-point optimizations are all performed for a wind speed of 8 m s −1 and rotational rate of 6.69 rpm at zero blade pitch, which is one of the conditions listed in table a1 in appendix a. for the multipoint optimizations, we use the wind speeds 5, 8, and 11 m s −1, and the relevant operational conditions can again be found in table a1 in appendix a. furthermore, we use the initial values at 12 m s −1 in the thrust and flapwise bending moment constraint for the multipoint optimizations because we know from the solver comparison (appendix a, fig. a1 ) that the maximum thrust occurs at that speed."
"to quantify the mesh dependence for each solver, we compute the integrated metrics -torque and thrust -for the three mesh levels (l0, l1, and l2) and list them in table 4 . the operational condition corresponds to a wind speed of 8 m s −1 and rotor speed of 6.69 rpm at zero blade pitch, which is one of the conditions listed in table a1 in appendix a. as is evident from the results for meshes l2, l1, and l0 in table 4, adflow does not produce a sufficiently mesh-independent solution on mesh l0. this agrees with an earlier mesh convergence study [cit], table 1), where up to 22 million cells were used without reaching convergence. therefore, we generated a finer mesh with more than 47 million cells called l-1. the l-1 mesh is made exclusively for the present grid convergence study and will not be used in the ensuing optimizations. table 4 shows that error reduction from l0 to l-1 for adflow is much lower (with reductions of about 4 % in thrust and 7 % in torque) than the error reduction from l2 to l1 (15 % and 21 %) or from l1 to l0 (22 % and 41 %). the errors are computed using the richardson extrapolation values from fig. 5, which are based on an estimate of the continuum value (in the limit of an infinitely fine mesh), [cit] :"
"in this paper, we present results from a high-fidelity aerodynamic shape optimization of a 10 mw offshore wind turbine rotor. by \"high-fidelity\", we mean a detailed modeling of the rotor in 3-d and the use of reynolds-averaged navier-stokes (rans) equations to model the aerodynamics throughout the optimization. the optimization is based on the case study from the international energy agency (iea) wind task 37 1, which allows for a comparison with the low-fidelity bem results from this case study. low-fidelity tools offer a fast and reliable modeling approach. however, bem does not capture the physics as completely as highfidelity cfd-based tools that solve the rans equations. in the present work, we aim to quantify the pros and cons of each approach."
"one obstacle in using bem codes is that the lift and drag data must be at hand. typically, one uses data from wind tunnel experiments or low-fidelity numerical models, such as a panel code [cit], 2018) . [cit] combine bem with both panel and 2-d rans cfd in a comparison between two integrated blade design approaches (\"precomputational\" and \"free-form\") and a sequential approach. they used a panel code iteratively to converge the bem residual and then either a panel code or cfd to generate the final lift and drag coefficients. [cit], they argued for the integrated design approach, but they found that the precomputational approach achieved most of the benefits yielded by the free-form approach. this is impressive, since the precomputational approach took marginally more computation time than the sequential approach."
"we now compare our l0 result from the planform optimization to our results from the bem1 and bem2 optimization problems. we obtain the bem results by running hawtopt2, which uses hawcstab2 [cit] as the underlying analysis code. since this is a comparison between results obtained with completely different models, we do not expect an exact match, but we expect similar trends. as previously mentioned, the cfd planform optimization problem and the bem1 optimization are completely identical in problem definition, and the relative thickness is fixed in both optimizations. for the bem2 optimization, the main difference is that it is solved as a multipoint optimization and that the relative thicknesses can be changed through interpolation. we refer to sect. 5 for further information."
"these results do not show that the industry can necessarily gain a 20 % increase simply by using high-fidelity optimization. indeed, the amount of improvement depends on the performance of the baseline turbine. since we study an intentionally poor baseline design, we therefore get a large improvement."
"comparing the airfoil shapes and corresponding c p distributions at the bottom of fig. 15, we can see that the optimization reduced the thickness and slightly increased the camber. the thickness reduction is expected when considering only the aerodynamics with no structural strength constraints. since we use thickness constraints as a surrogate for structural feasibility, the optimizer exploits this by produc- figure 14 . convergence history (a) and scaled merit function history (b) for the single-point shape optimizations. figure 15 . comparison of c p distributions for the baseline and optimized result from the single-point shape optimization. there is an increase in te camber, especially at the root, as well as a less pronounced suction peak. ing the thinnest airfoils that satisfy these constraints. [cit], but the increase in camber here is more modest because the optimizer can increase the torque by tailoring camber, chord, and twist instead of just camber. the incentive to operate at high lift coefficient is due to the fact that high c l /c d is most easily achieved by operating at high c l, especially for airfoils designed assuming a fully turbulent boundary layer. another feature of the optimized airfoil shapes is the sharper le. this is expected due to the fact that we are max-imizing the performance at a single wind speed. this shape is not robust to changes in wind speed and would perform poorly at other wind speeds. this issue can be addressed by enforcing the le radius constraints or by considering the performance for multiple wind speeds in the objective function, as we will see in the next section."
"to obtain more realistic le shapes, we added an le thickness constraint to the optimization problem. the geometric constraint was enforced as a thickness constraint close to the le. the resulting shapes are shown in fig. 19, where we compare them to the shape obtained by the multipoint optimization without the le constraints. while we choose to focus solely on the 2-d profile improvement from single-point to multipoint optimizations, the optimizations are indeed all 3-d rotor optimizations. as we can see, enforcing the geometric constraint results in a more round le shape that is much more similar to previously published wind turbine airfoil shapes."
"in fig. a5, we compare the obtained c p curves at three spanwise positions: 35, 64, and 84 m (positions marked in red in fig. a3), where the c p distribution is found using the dynamic pressure, and the far-field pressure, p ∞ :"
"the process of swapping a rfu with a sfu happens with the system being totally unaware of it, since the reconfiguration manager has the ability to \"freeze\" the involved circuits while the reconfiguration takes place. the \"freezing\" technique can be used by the reconfiguration manager to discriminate transient faults from permanent ones, thus avoiding unnecessary reconfigurations and transient error propagation at the same time."
"in this paper we have formally defined the inter-domain vn embedding problem and presented polyvine -a novel policy-based inter-domain vn embedding framework -to address it. polyvine allows embedding of end-to-end vns in a distributed and decentralized manner by promoting global competition in the presence of local autonomy. we have laid down the workflows of inps and sps throughout the polyvine embedding process and identified the most crucial stage in the inp workflow, vn request forwarding. in this respect, we have proposed a hierarchical addressing system (cost) and a location dissemination protocol (lap) that jointly allow inps to make informed forwarding decisions. we have also presented preliminary performance characteristics of polyvine through simulation."
"as previously discussed, an inp, inp a j, will report back embed failure in case it fails to map the remaining portion of an embedding. failure reasons include: 4. the processing fee budget has been exhausted. 5. internal inp policies do not allow the embedding to proceed."
"we examine four sets of experiments. in our first set of experiments, we look at some of the properties of interdomain embeddings generated by polyvine as the vn request size (node count) is varied. in our second set of experiments, we look at some of the properties of the polyvine embeddings as the pricing model attributes are varied (embedding and processing budgets). in our third set of experiments, we look at properties of polyvine embeddings as we vary the maximum number of inps involved in a mapping. in our last set of experiments, we examine the reliability of the polyvine protocol and the cost of embeddings generated in the case of lost or stale lap information."
"finally, the pipeline freezing control logic is responsible of both blocking the cpu operation during the reconfiguration and restoring it once it is completed. during normal operation the error signal is connected both to the reconfiguration manager and the pipeline \"freeze\" signal: as soon as the error signal is asserted, the pipeline is blocked and the reconfiguration manager starts an internal timer to check if the fault is permanent or transient. if at the end of the detection window the error is still present, the reconfiguration manager overrides the freeze signal to ensure that any glitch that may occur during reconfiguration doesn't propagate to the registers, and starts the reconfiguration."
"once an embedding is successfully completed, an inp replies back to its predecessor with a price and m. pred state id is a unique identifier used to call up the relevant state stored at the predecessor entity (sp or inp). the succ id is a unique identifier indicating which inp sent the message."
"in the final set of experiments, we wish to assess the reliability of the polyvine protocol when faced with inps that have stale lap information. the propagation rate of lap data can vary by relationships between inps and by location and so we wish to ensure that polyvine is able to function under a variety of conditions."
"since each inp will try to selfishly improve its own performance and will not expose its internal information, inps can lie to or hide information from each other. from previous studies it is known that it is hard to use mechanism design or game theory to thwart such behaviors in a large scale distributed system [cit] . our solution against such behavior is the use of competitive bidding at each step of embedding to expose the market price of any leased resource."
"once the functionality has been clearly identified, a new entity (the hardware sfu) with the minimal interface to provide the same functionality has been developed, which implements the following operations:"
another interesting direction of research for this problem would be to model it as a distributed constrained optimization problem (dcop) and to try to solve that with minimal information exchange between inps.
"when an inp is instructed to reject an embedding, it first recursively rejects any partial embeddings by successors, any inter-domain paths leading to it from predecessors, and finally deallocates all resources allocated locally for the given embedding request instance. once all that has completed, it reports an acknowledgement to the predecessor that issued the embed reject message."
we have written a 12000 line multi-threaded c++ simulator that allows independent responses from various entities in the controller network. the simulation comprises a complete implementation of the entire set of protocol messages discussed in section 3.6.
"in order to exchange information between the sp and the inps, and to organize the distributed embedding process, a communication protocol must be established. we refer to this protocol as the polyvine protocol, which is based on eleven types of messages. these messages are sent and received asynchronously between concerned inps and the sp to carry out the embedding process from beginning to end. the protocol messages are described in the following: as well as processing allocation remaining. if inp a i determines that it requires more money to process the message than is allocated by the sp, then it will report failure. if an inp processes the request but later determines that the allocation would go over budget remaining, it will cancel the reservation, and report failure. an inp also uses this message to outsource the unmapped part of the request after appending itself to inpset, and updating g and the partial embedding m as necessary. req id and state table together uniquely identify a particular instance of the vn request (see section 4.5)."
"when an inp receives an embed message with virtual network g and finds a mapping m of some subgraph of g, the inp reserves those resources for that particular vn request. the resources are reserved until either the polyvine protocol determines that there are no successors that can satisfy the remaining portion of the request or a predecessor has rejected the mapping provided by the current inp. subsequently, when an inp receives a link message, it must call up the previous resource allocation associated with the current instance of the virtual network mapping request and bundle any link allocations it performs with the previous resource allocations. figure 5, inp #3, and inp #4 both map subgraphs of the vn request issued by the sp. they, then, both independently send link messages to inp #1 to map inter-domain paths from their respective subgraphs to the subgraph mapped by inp #1."
"obviously the actual reconfiguration time depends on the size of the partial bitstream (table iii), which becomes another important parameter from a system point of view: trying to minimize the \"area\" occupied by a component not only allows table iii bitstream sizes to improve device utilization, but could lead to a slightly faster reconfiguration time, as well."
"once the sp selects an embedding, it proceeds toward instantiating its vn by sending embed accept messages to the inps involved in the selected embedding and sends embed reject messages to the inps involved in unwanted embeddings."
"one of the biggest challenges in end-to-end vn embedding is to organize the inps under a framework without putting restrictions on their local autonomy. each inp should be able to embed parts or the whole of a vn request according to its internal administrative policies while maintaining global connectivity through mutual agreements with other inps. moreover, inps (i.e., network operators) are notoriously known for their secrecy of traffic matrices and topology information. as a result, existing embedding algorithms that assume complete knowledge of the substrate network are not applicable in this scenario. each inp will have to embed a particular segment of the vn request without any knowledge of how the rest of the vn request has already been mapped or will be mapped."
"the geographic location representation and related information dissemination protocol proposed in polyvine is inspired by the previous proposals of geographic addressing and routing in ipv6 networks [cit] as well as the predominant global routing protocol in the internet, bgp [cit] . however, unlike these works, polyvine does not use the information for addressing or routing purposes; rather it uses the location information to find candidate inps that will be able to embed part or whole of the remaining unmapped vn request. moreover, such location information is disseminated between and stored in controllers instead of border routers as in bgp or giro [cit] . the concepts of controllers in inps and controller network connecting multiple inps' controllers are discussed in the imark framework [cit] ."
"resource reservation blowup is a major issue in any implementation of polyvine. in our implementation of a simulation of the polyvine protocol, we quickly realized that in the worst case, the resources allocated in a single inp could grow exponentially as a function of d, the maximum search depth. the total number of flows explored by polyvine to find a good inter-domain vn at inp a j with embeddingid k can be thought to be composed of two resource allocation vectors, one for nodes and one for links, indicating the resources allocated by that state object in addition to its parent state object: ) resources available for other vn requests. as all the mutually exclusive flows are for the same virtual network, the allocations at any given inp will be relatively similar and so, the component-wise maximum will typically not be much more than those required of any one flow."
"the component is equipped with 2 different interfaces to the amba bus. at the same time it is an ahb master and an apb slave; the master interface is used to fetch the configuration bitstream from the storage memory (the main system memory or another external memory), while the slave interface is used to provide the software support facilities."
"while an sp's workflow is straightforward with a single decision at the end, it shifts much more work to the inps. an inp has to work through several steps of decision making, organizing, and coordinating between heterogeneous policies to complete the embedding process."
"to accommodate such location aware forwarding, we introduce a hierarchical geographic addressing scheme with support for aggregation, named cost. inps in polyvine must associate cost addresses with all the substrate nodes and sps must express location requirements in terms of cost. controllers in different inps publish/disseminate information about the geographic locations of their nodes along with the unit price of their resources. they can then aggregate and disseminate data collected from all neighboring controllers to build their own knowledge bases of location to inp mappings, each accompanied by path vectors of inps in the controller network and corresponding prices. we propose location awareness protocol (lap) to perform this task. careful readers will notice in the following that cost and lap are significantly influenced by bgp."
"two messages in the protocol can initiate resource allocations within an inp: embed, and link. thus, corresponding roll-back messages must exist in the protocol: embed reject and link reject. in order to simplify the implementation of a controller's polyvine message handling system and avoid race conditions, associated acknowledgement messages embed reject ack and link reject ack act as barriers to ensure that rollback occurs in the opposite order to allocation. note that link allocations corresponding to the set l v m a j for a given inp a j are unordered as there are no dependencies among them. however, state dependencies exist between subgraph allocations on one inp and the next, and so polyvine ensures that roll-back occurs in the opposite order to allocation through the * ack messages."
"inps participating in a polyvine embedding will face major computation overheads while trying to map the vn request and minor communication overheads due to relaying of the rest of the request. since for each vn embedding every inp in each step except for the winning bidder will fail to take part in the embedding, the overheads can be discouraging. we are working toward finding incentives for the inps to partake in the embedding process."
"due to the particular description style used in gaisler libraries [cit], the whole processor pipeline is described in a behavioral way, with just 2 processes, one implementing the full pipeline stages functionality, and the other one implementing the sequential logic."
"the internals of our running example: let's consider the substrate network of inp #2 in figure 6 of our running example. recall from figure 5, inp #2 has three embeddingids (0, 1, and 2) associated with state for vn ij . in figure 6, we look at the resources allocated by the mappings associated with each of the three embeddingids. embeddingid 0 has no associated resource reservations. embeddingid 1 and 2 refer to mutually exclusive resources allocated by inp #2 on behalf of inp #3 and inp #4 respectively. both resource vectors correspond to mappings of virtual links a, and c from border node b 2 to border node b 3 . note that the two resource vectors correspond to similar, but not identical mappings onto inp #2's substrate network. at most, the sp will accept one of the flows, and so we don't need to allocate resources so that all of the vectors can be satisfied simultaneously. instead we take the component-wise (i.e. per-resource) maximum resource requirements and reserve that (cmax) vector. note that the last row of the table in figure 6 indicates significant resource savings as a result of this technique. typically, we can expect the savings to grow linearly with the number of flows, enabling much larger search spaces."
"given the particular toolchain used for the synthesis on a xilinx device [cit], the top level design had to be modified by using bus macros to connect the static and reconfigurable part, which is instantiated as a black box. during place & routing the reconfigurable area size has been set, and the bus macros had to be statically placed on the boundary; during the placement we kept into account the effects that the placement of bus macros have on the resulting layouts of both the static and reconfigurable parts."
"in the same way, if the fault is permanent, the computation will resume after the time needed by the reconfiguration, still without any extra rollback operation. in addition, the top level cpu interface must contain all alu inputs and outputs, which need to be properly connected to the execute stage signals and to the reconfigurable area interface, a multiplexer will select between the output coming from the internal alu and the one coming from the external \"spare\" alu. the alu input signals will be simply connected to the reconfigurable module interface."
"in this model, the sp sets an upper bound on the processing fees as a ratio relative to the embedding budget (e.g. 1 : 2 processing fee to embedding fees). for example, an inp may wish to embed a virtual network for a maximum of $5000 and pay no more than an additional $2500 for processing. the processing fee cap implicitly limits the search space. we leave it up to the discretion of the inp to choose how to distribute the processing fee allocation to successors, and how many successors to relay the vn request to (k inp ). k inp may, for example, be expressed as a function of the processing fee allocation such that as the allocation grows so does the branching factor."
"polyvine decision making and embedding process is deeply rooted into the location constraints that come with each vn request. after an inp embeds a part of a vn request, instead of blindly disseminating the rest of the request, it uses geographic constraints as beacons to route the request to other possible providers. polyvine aggregates and disseminates location information about how to reach a particular geographical region in the controller network and which inps might be able to provide virtual resources in that region."
"in figure 19, we delve a bit deeper to see how polyvine is so resilient. we observe that the number of flows explored actually increases as the drop rate of lap messages increases (with no changes to the processing budget) this is very counterintuitive. how are we able to explore more flows? figure 20 sheds some light on this. we see that the number of inps involved per flow increases as the lap drop rate increases. this means that each inp is mapping a smaller portion of the vn request, and so the embedding budget allows for more inps to be involved per flow. each additional inp spawns off k inp more flows."
"in this paper, we introduce polyvine, a policy-based end-to-end vn embedding framework that embeds vns http://www.jisajournal.com/content/4/1/6 across multiple inps in a globally distributed manner while allowing each concerned inp to enforce its local policies. polyvine introduces a distributed protocol that coordinates the participating inps and ensures competitive pricing through repetitive bidding at every step of the embedding process."
"in case of a failure, the inp will send back an embed failure message (optionally with reasons for the failure). however, sometimes the inp might know of other inps that it believes will be able to embed part or whole of the vn request. in that case, it will relay the vn request forwarding the embed message to that inp after adding itself to the inpset. in figure 3, inp#2 is relaying the vn request g' to inp#3."
"in the experiment in figure 16, we see that there is a notable correlation between the maximum number of inps involved in a flow, and the success rate. given a fixed processing budget, increasing the maximum number of inps involved (and thereby decreasing the branching factor k inp ) tends to increase the success rate. this is significant because it suggests it may be possible to lower the processing budget without impacting the success rate by increasing the value of d."
"replaceable functional units can be critical or non critical, and corresponding spare functional units can be hardware or software: critical rfus are equipped with a concurrent error detection system and can be replaced by a hardware spare unit, while non critical rfus may or may not have a bist facility and can be replaced by a software sfu."
"this model disincentivizes entities (sps and inps) from flooding the controller network to search for a cheaper solution. as each inp takes a processing fee, we eventually run out of money in the processing fee allocation, effectively reducing search depth. on the other hand, given a fixed fee allocation, a smaller branching factor increases search depth."
"the system awareness about current system configuration is granted through the \"reconfigurable area status register\", which can be read by other modules to perform operations coherently with the available hardware resources."
"the rfus/sfus that need to be hosted inside the reconfigurable area need almost no modification compared to a normal design. the only requirement is that they must share the same entity interface declaration: the apb des interface must contain the alu interface signals, and the external alu interface must contain the apb signals, even if these are left unconnected inside the entity."
"naïvely an inp can forward a vn request to a set of inps in the controller network at random. however, this decision is blind to the location requirements of the virtual nodes and the availability of virtual resources at the destination inp to satisfy the constraints for the vn request. this may result in high failure rate or prices well above the fair value. to avoid flooding a vn request or sending it to random inps which might be unable to meet the constraints of the request, we propose using location constraints associated with unassigned virtual nodes to assist an inp in making this decision. location constraints of the virtual nodes together with the location information of the underlay will allow informed vn request forwarding in the controller network."
"recursive processes, by definition, can go on for a long time in the absence of proper terminating conditions resulting in unsuitable response times. combining iterative mechanism wherever possible and limiting the level of recursion at the expense of search completeness can improve the response time of polyvine. however, the question regarding suitable response time depends on the arrival rate and the average life expectancy of vn requests."
"in figure 18, we see that polyvine is extremely resilient to dropped lap update messages. the success rate is largely unimpacted by dropped lap updates until about 95% of updates are dropped after which, we see a significant drop in success rate of flows. polyvine is designed to always forward vn requests to some neighboring inp, even if it cannot find an inp that matches the location constraints of any of the unmapped nodes in the vn request. if an inp cannot map any nodes, it acts as a relay and then uses its own lap data to determine where to forward next. this makes polyvine extremely resilient to failing as a result of stale or lost lap data. however, missing lap data might affect the quality (cost) of an embedding."
"embedded systems are nowdays widely used for many safety-critical applications that impose very strict, often conflicting, requirements, including fault-tolerance, real-time data processing, and reduced use of resources and power."
"we do not claim polyvine to be the best or the only way of performing end-to-end vn embedding. however, to the best of our knowledge, this is the first foray into this unexplored domain in the context of network virtualization, and we believe this problem to be absolutely critical in realizing network virtualization for most practical purposes."
"in our fourth experiment in figure 12, we look at the the impact on embedding cost as we increase the vn request size. we see a linear relationship between the vn request size and the embedding cost (as is expected), where roughly each node added to a vn request costs about 250 units. somewhat surprisingly, the higher cost of virtual links mapped across inter-domain paths is not apparent here. this may be due to the relative sparseness of the vn requests we have examined."
"we define a flow to be an ordered set of inps visited to map a given virtual network vn ij with a unique identifier i, requested by some service provider, sp j . it is evident that the flow f"
"at a given stage j of the embedding process, inp a j receives an embed message that contains an ordered set of inps participating so far, inpset containing if all the virtual links specified by the link message are mapped successfully by the receiving inp (no policies are violated and physical resources are available to satisfy the paths), then it responds with a link success to the sender. otherwise, the recipient will respond with a link failure message to the sender."
"polyvine currently uses lap for informed forwarding of vn requests. however, location information is not the only information available to an inp about other inps. an inp should be capable of \"learning\" from past experience. that is, it should be able to collect data on previous embeddings, and make more informed decisions in the future based on its observations of the past."
"in our second experiment of this set (figure 14), we vary the processing and embedding budgets again but this time, we observe their impact on the total cost of an embedding. we observe a linear growth in total embedding cost as we increase the processing budget and the embedding budget. in our experiments we were unable to find a peak that balances the tradeoff between the processing budget (and hence the size of the search space) and the total embedding cost. looking at the results in figure 14 we see that much of the total embedding cost is going into processing fees. as the processing budget is on a per-first-hop inp basis, increasing k sp also increases the processing costs."
"however, in a rapidly changing environment with continuously fluctuating prices, gossip may not be sufficient to disseminate updated prices in a timely fashion. to reduce the number of failures stemming from staleness of pricing information, we propose extensions to lap using a publish/subscribe mechanism along with its basic gossip protocol. by using this mechanism, any inp will be able to subscribe to announcements of controllers that are not its direct neighbors. while we leave vn request routing decisions to the discretion of inps, an inp may use the pricing information to prefer forwarding the vn request to a lower priced inp, all other things being equal. http://www.jisajournal.com/content/4/1/6 the question that remains open to more investigation is why would an inp be honest when announcing pricing estimates? we believe that a reputation metric -indicating long-term accuracy of an inp's pricing estimate to the actual cost of establishing a vn request -is necessary to remedy this situation. we would like to integrate such a reputation metric within lap to allow dissemination of path vectors attributed with corresponding prices and overall reputation score of the inps on the paths. an inp will then be able to use pricing and reputation scores to rank multiple paths to a common destination to make a forwarding decision."
polyvine expects each inp a j to complete a mapping of all virtual links in the set l v m a j prior to forwarding the remainder of the vn request to new participants. this ensures that no additional inps will be brought in to participate in the mapping before current participants are sure inter-domain paths are feasible between them and satisfy their respective policies.
"the vn request proceeds from one inp to the next, until either the maximum number of participants d has been reached, there are no available inps to send the request to or the vn request has been satisfied completely. in case of a successful embedding of a vn request, the embed success message carries back the embedding details and corresponding price. at each step of this back-propagation of embed success and embed failure messages, the sender inp can select mappings based on internal policies or lower price or some other criteria and rejects the other successful embeddings by issuing embed reject messages to the appropriate successors."
"the fault tolerant architecture presented in this paper comprises a set of \"replaceable functional units\" (rfu), a set of \"spare functional units\" (sfu), a reconfiguration manager and a reconfigurable area. based on health status monitoring and other parameters, the reconfiguration manager decides if to use a sfu instead of the corresponding replaceable functional unit."
"this model also provides inps an additional incentive to participate in finding an embedding for a given virtual network as it will receive compensation for its work. when an entity sends an embed message to another entity, it enters a contractual agreement to pay a processing fee up to an upper bound it specifies."
"by using the \"freeze & resume\" technique we showed that we can tolerate transient and permanent faults on a critical replaceable functional unit by the means of fpga dynamic partial reconfiguration. moreover, we demonstrated that by using the appropriate techniques, the reconfiguration (which actually removes an ip core from the device) doesn't affect system functionality, but only causes performance degradation when executing some specific tasks."
"though this approach is general and can be applied to a wide range of designs, in this paper we focus on a real-world scenario: a complex soc synthesized on a fpga, whose block diagram is depicted in figure 3 . such architecture addresses the error detection and recovery of the combinational logic of a processor's pipeline stage. it requires a reconfigurable area which is initially preallocated to a device of the soc, a properly adapted processor, and a reconfiguration manager in charge of managing the resource allocation of the reconfigurable area. once an error is detected, the cpu pipeline is \"freezed\" disabling the processor clock, in order to keep all the combinational logic inputs constant, and the recovery process starts. this process consists in waiting few clock cycles before starting the reconfiguration, thus ensuring that the error is caused by a permanent fault. in case of transient fault, once the logic starts to behave normally, the error signal is immediately de-asserted and the execution resumes normally on the next clock cycle. the conceptual recovery process is described in algorithm 1."
"each inp charges an embedding processing fee of 1 unit (±20%). a maximum inp count per flow (d) is a property of the vn request. based on this property, an inp estimates the maximum branching factor it can use so that up to d inps can be involved in a flow and uses that branching factor. thus, the entire processing budget is always consumed. the processing fees are not refunded if the flow fails."
"upon receiving a vn request, an inp must decide whether to reject or to accept the request. it can reject a vn request outright, in case of possible policy violations or insufficient processing budget provided by the predecessor, returning an embed failure message to its predecessor. even if there are no discernible policy violations, it might still need to reject a vn request if it fails to profitably embed any part of that request or if it fails to find an embedding that meets the budget constraints. in order to decide which part of a vn request to embed, if at all, the inp can use existing intra-domain vn embedding algorithms [cit] that can identify conflicting resource requirements in a vn request. this can be done iteratively by looking into the output of the linear programs used in both [cit] without modifying the actual algorithms presented in those work, and trimming out parts of the virtual network until a feasible solution is found. however, we argue that this heuristic may not be sufficient for high quality or even feasible partial embeddings. in particular, we must ensure that if an inp maps a virtual link, it also maps the two nodes incident to it. we also wish to minimize the number of virtual links that map across multiple domains as inter-domain paths tend to be long and thus are more costly."
"in the future we would like to address issues such as pricing models, inp interactions, reputation management, and incentives for inp truthfulness. relative advantages and disadvantages of contrasting choices (e.g., recursive vs iterative forwarding) in different stages of inp workflow should also be scrutinized. finally, the scalability, stability, and performance characteristics of polyvine require further studies through larger simulations and distributed experiments with a heterogeneous mix of intra-domain vn embedding algorithms and policies."
"time pre-reconfig (hardware) 183 ms post-reconfig (software) 1500 ms measurements showed that the system is able to keep working normally after a reconfiguration, since only the des encryption/decryption time is affected. table ii shows one order of magnitude performance degradation due to the lack of hardware support). it is interesting to note that the resulting layout kept working at the original clock frequency, which has been set with a wide safety margin over critical path length."
"in our first experiment in figure 9, we look at the number of nodes mapped by the first set of inp neighboring the request-generating sp as we vary the vn request size. figure 9 demonstrates that the number of nodes mapped by the first-hop inps grows linearly with the size of the vn request. with small requests, virtually the entire network is mapped by the first inp. as request sizes approach the limits of the resources available at the first-hop inp, the number of nodes mapped by the first-hop inp flattens out at about 35 nodes. when we attempted random vn requests larger than 45 nodes, we found that no solutions are found by the polyvine protocol, regardless of the size of the search space."
"moreover, the reconfiguration manager is responsible for the monitoring of cpu error signal, starting a reconfiguration once a fault has been detected as permanent, and has to deassert the \"freeze\" signal responsible of disabling the cpu clock once the reconfiguration has completed."
"to simulate a fault, the error signal has been connected to one of the board switches instead of the error detection logic. this allowed us to cause a full reconfiguration cycle during the execution of the program."
"a the words 'embedding', 'mapping', and 'assignment' are used interchangeably throughout this paper. b we will use the terms inp and substrate network interchangeably throughout the rest of this paper. c each inp uses its own pricing mechanism by which it attaches a price to any embedding it provides."
"this particular style showed both advantages and disadvantages for the purposes of our design: on one side it allowed the easy implementation of the pipeline freezing logic, with a simple modification to the \"sequential\" process; on the other, it required additional reverse engineering work on the \"combinational\" process in order to isolate the alu/shifter logic from the other logic implemented in the execute stage of the pipeline."
"recently proposed v-mart [cit] framework approaches the inter-domain vn embedding problem using an auction-based model, where the sp performs the partitioning task using heuristics for simplification. as a result, v-mart cannot enable local and inter-inp policy enforcement and fine-grained resource management."
"the intra-domain vn embedding problem is well-defined in the literature [cit] . in this section, we formally define the inter-domain vn embedding problem. for simplicity, we avoid intra-domain aspects (e.g., node and link attributes) wherever we see fit. we use the notation introduced here to discuss the details of the polyvine protocol in section 3."
"each experiment is run to completion (a vn request has completed) multiple times per data point to produce the averaged results presented here. we found that that variation in tests was very small and so we did not include confidence intervals. unless otherwise specified, we have used the following settings: for each experiment, we randomly create a controller network with 60 inps. each inp network consists of 120 to 150 nodes and 540 to 600 links on average. each node has a maximum cpu capacity uniformly chosen from 1 to 100 cpu units, and each link has a maximum bandwidth capacity of 100 bandwidth units. locations of substrate nodes are sampled from a normal distribution with a mean, and variance chosen uniformly from 0 to 255 representing 256 different major cities. inps with low variance location distributions are effectively local or regional inps, while high variance inps have nodes that span the globe. the per unit cost per resource is chosen from a normal distribution with a mean sampled from a prior, per inp, normal distribution (of mean 4, variance 1) and a variance of 1. this http://www.jisajournal.com/content/4/1/6 means that some inps will tend to be cheaper than others, on average."
"polyvine is an enabling framework for multi-step distributed embedding of vn requests across inp http://www.jisajournal.com/content/4/1/6 boundaries. in its simplest form, an sp forwards its vn request to multiple known/trusted inps; once they reply back with embeddings and corresponding prices, the sp chooses the vn embedding with the lowest price similar to a bidding process."
"to implement the pipeline freezing logic, the \"freeze\" signal has been conceptually connected to the enable input of pipeline registers. practically the sequential process has been modified to take into account that if the freeze signal is high, no action has to be done in that process."
"network virtualization has gained significant attention in recent years as a means to support multiple coexisting virtual networks (vns) on top of shared physical infrastructures [cit] . the first step toward enabling network virtualization is to instantiate such vns by embedding a vn requests onto substrate networks. but the vn embedding problem, with constraints on virtual nodes and virtual links, is known to be n p-hard [cit] . several heuristics [cit] have been proposed to address this problem in the single infrastructure provider (inp) scenario. however, in realistic settings, vns must be provisioned across heterogeneous administrative domains belonging to multiple inps to deploy and deliver services end to end."
"leon3 is a highly configurable 32-bit processor core conforming to the sparc v8 architecture. it is designed for embedded applications, combining high performance with low complexity and low power consumption."
"location awareness protocol (lap) is a hybrid of gossip and publish/subscribe protocols that assists an inp in making informed decisions about which inps to forward a vn request to without making policy violations, and thus progressing toward completing the vn embedding. controllers in different inps keep track of the geolocations of their internal substrate nodes in cost format and announce availability and prices of available resources to their neighbors using lap updates in the controller network. this information is aggregated and propagated throughout the controller network to create global view of the resources in the underlay in each controller's lap database."
"the simple pricing model we suggested in this report succeeds in accomplishing the goal of incentivizing inps to participate in a highly competitive environment and disincentivizing flooding the controller network to find feasible, low-cost embeddings. however, we did not study the practical implications of this pricing model. it may be possible for an inp to abuse this model. for example, the d th inp may be able to gouge prices, leaving the predecessor with no option but to accept the higher price. in the future, we will investigate alternative pricing models that will accomplish our primary goals while studying the strengths and weaknesses of each model."
"however, we observe a much clearer correlation between the embedding budget and the success rate. a higher embedding budget tends to improve the flow success rate. also, we see that as we increase the embedding budget, the variation in success rate between processing budgets decreases, suggesting that a larger processing budget does allow for more flows, but most of those new flows go above the embedding budget. as we increase the embedding budget, fewer of the new flows go over the embedding budget."
"the paper is organized as follows: in section 2 we present the general concepts behind our proposed architecture; section 3 presents a case study with related experimental results, while in section 4 we draw some conclusions and outline some possible extensions and future work."
"in this paper we introduced the concept of \"fault-tolerance on demand\"; thanks to the proposed architecture we have been able to verify that concept. we showed that it is possible to create a dependable system, which becomes \"fault-tolerant\" only when a fault is actually detected, thus limiting the compromises typically associated with the design of such systems."
"in our second experiment in figure 10, we looked at the number of inps that are involved in a successfully satisfied vn request. in this experiment, we only consider inps that contribute substrate node resources to the vn mapping, and not inps that simply reserved bandwidth as relays. we see that the the number of inps involved appears to grow linearly with the size of the vn request but with a very small slope."
"critical rfus are initially hardwired on the device (i.e. they are placed in a non reconfigurable area), and when found defective, they are replaced by a sfu mapped inside the reconfigurable area. non critical rfus instead are initially mapped inside the reconfigurable area, and are replaced by a software sfu if the reconfigurable area is needed to host a critical spare unit, as depicted in figure 2 . the figure shows a critical functional unit (fu1) and a non critical functional unit (fu2); when fu1 fails, the corresponding sfu is placed inside the reconfigurable area, erasing the previously allocated fu2, which will be executed in software. the reconfiguration manager holds a repository of spare functional units (in the main system memory, or a mass storage device), corresponding to an archive of device configuration bitstreams (for hardware sfu) and an archive of compiled modules (for software sfu)."
"in figure 15, we drop the processing fees, and look at the impact varying the budget and processing fees has on just the embedding cost and not the total cost to the sp. we observe that varying the processing budget has relatively little impact on the cost of the embedding. this supports the argument above that states that a large change to the processing budget is necessary to observe a measurable change to the search space. this suggests that under this pricing model, an sp should pick its processing budget high enough to produce an acceptable success rate, and not to find cheaper solutions."
"as a proof of concept, we implemented the architecture described in the previous section using a soc based on the leon3 cpu ( [cit] in order to tolerate the alu faults by replacing a previously allocated des (data encryption standard) crypto-core. according to the taxonomy previously introduced, we identify the alu as a critical rfu, while the des crypto-core as a non-critical rfu. both of them have the corresponding sfus (hardware for the alu, software for the des core). we decided to use this example because of the self-contained and general nature of both components, which also have a comparable area occupation. the resulting design has been synthesized and tested on a xilinx virtex 4 fpga [cit] ."
"access to the fpga configuration memory is ensured through the 32-bit virtex 4 icap port [cit], which is instantiated as a black-box and mapped on the target device during place&routing. a buffer is used to store temporary bitstream data in order to achieve high performance, since this port can write up 32 bits per clock cycle [cit] ."
"once an inp, inp a j finishes mapping a subgraph g v a j, it sends a link message to each of its predecessors ."
"within a single domain, a vn embedding is transactional in nature, as an embedding must be completed as a whole, or not at all. in the multi-domain scenario, each inp is free to map a subgraph of the embedding and so the algorithm used to perform that partial mapping may or may not be transactional (it's up to the discretion of the inp how to implement it). however, from the sp's perspective, the multi-domain scenario is the same as that of the single domain: it expects either a completed embedding reservation or a report of failure. in other words, in a given flow, either all participating inps succeed in reserving resources or none of them reserve resources. this means that the polyvine protocol itself must provide a mechanism to roll-back the work done by other inps in a given flow once a failure or rejection occurs. an sp is expected to accept only one flow, and reject all other flows. rejection initiates a roll-back process of all resources allocated for that flow."
"the maximum number of inps involved in a single flow (d) impacts the size of the search space explored. as d decreases, the branching factor per inp tends to increase. in this experiment, we investigate the tradeoff between increasing the number of flows (k inp ) and the maximum number of inps involved per flow (d) given varying processing budgets ratios (processing budget : embedding budget), and a fixed embedding budget of 25,000."
"to illustrate the details of the polyvine protocol, we introduce a simple running example in figure 2 . in this example, an sp issues a vn request (figure 2a) to inp #1. inp #1 proceeds to map virtual node a, and b and virtual link d in figure 2 (b)(c). it then forwards the remaining portion of the vn request to inp #2. inp # 2 is unable to map nodes from the vn request, and so it serves as a relay inp that may allocate bandwidth resources for virtual links that span multiple domains as need be (links a and c in this example). in turn, inp #2 forwards the remaining portion of the vn request to both inp #3 (figure 2b ) and inp # 4 ( figure 2c ). in the recursive process, a sequence of inps that terminates in failure or ultimately finds a feasible solution that spans that sequence is called a flow (see section 4.5). in the example in figure 2"
"the design of polyvine allows for a fairly straightforward analysis of message complexity. if we assume that the implementation of the protocol at each inp does not involve inter-domain paths through new inps that have not seen the given instance of the vn request, then in the worst case, with n participating inps, each will visit all predecessors to map virtual links to interdomain paths. the number of participants is bounded by d. thus, the message complexity to map a virtual it compares the price it receives from inp #3 ($300) with that of inp #4 ($400), rejects inp #4's solution, and selects inp #3's solution (see figure 4 ). it adds its own local embedding price ($0 in this case, as it did not map any nodes, and the link allocations in inp #2 were accounted for by inp #3's offer) to produce a total that it sends back to inp #1 within an embed success message. 12. inp #1 receives inp #2's embed success message."
"in figure 21, we look at the impact dropped lap messages have on the embedding cost. presumably, with less lap data at every inp, vn request forwarding is effectively blind. we see that this intuition appears to be correct, after about 80% of lap updates are dropped. as we lose lap information, forwarding becomes less informed and so partial mappings are not always done at the cheapest inps. 80% is also about when we begin to notice additional flows (figure 19), and so at least some of the increase can be attributed to the additional inter-domain paths required by partitioning the vn across more inps."
"a variation of our running example: to illustrate the roll-back process, we consider a variation of our running example shown in figure 7 . #3 (steps 12-13) . subsequently, inp #3 releases its subgraph embedding for the vn request and reports embed failure to its predecessor, inp #2 (step 14). inp #2 also sees that inp #4 has failed. thus, inp #2 has seen that all its successors fail, and so it must also fail. inp #2 has no subgraph embedding, and so it simply reports embed failure to its predecessor inp #1 (step 15). inp #1 has only one successor and so it must fail as well. it releases its subgraph embedding for the vn, and reports embed failure to the sp."
"in order to keep the complexity low, thus fitting the fpga available for the implementation, the leon3 has been configured with a minimal setup (no hardware mul/div unit and no cache)."
"in order to reduce the number of signals crossing the reconfigurable area boundary, the apb interface has been splitted in a static and a dynamic part: the static one contains fixed configuration data used for pnp (plug and play) device detection, while the dynamic part is used for actual data transfers, interrupt routing and device addressing, and is the only one that actually passes the reconfigurable area boundary through bus macros."
"the system has been synthesized on the xilinx ml403 board, equipped with the xc4vfx12 fpga [cit] and several experiments have been executed, in order to validate the concepts introduced in previous sections. the final design occupied almost all of the logic resources available on the fpga (as reported in table i ), and the operating frequency remained unchanged (66mhz) compared to the static reference design provided with gaisler libraries for that board. the system has been tested with a bare c prototype program which executed a full des encryption/decryption cycle; the program included all the facilities to measure the execution times and to dynamically use the software version if the hardware device is not present anymore. monitoring execution times allowed us to quantify the performance degradation due to the use of software libraries instead of the hardware device, and to analyze the reconfiguration time impact on the whole execution time."
"the prototype presented in this paper can be extended with a more complex architecture, in order to cover the faults happening in any pipeline stage, and could be the starting point to evaluate more complex recovery techniques for fpgabased designs."
"the error signal must be carried out of the processor to inform the reconfiguration manager of the problem, and has to be connected to the pipeline freezing logic. once an error is detected all pipeline registers must be disabled in order to retain the previously stored data, avoiding error propagation. as illustrated above, this allows automatic tolerance of transient faults: if the error signal is deasserted within the few clock cycles that are counted between the error detection and the reconfiguration initiation, then the computation will resume without any loss or costly rollback operation."
"in our first experiment of this set ( figure 13 ), we vary both the processing budget and the embedding budget, and observe their impact on the success rate of flows of the vn request. we observe that relatively small changes to the processing budget have little effect on the success rate of a vn request flow, with significant variance up and down as the processing budget increases. this can be attributed to the high cost of increasing k inp at any given inp. budget allocation is distributed at each inp assuming a full n-ary subtree. thus, at least half of the inps in the search space are last hop inps given a fixed search depth. since processing budget is evenly distributed across all subtrees, to explore more space, the processing budget would need to increase significantly (dependent upon the current average branching factor on the second last hop)."
"several solutions do exist to guarantee hardware fault tolerance depending on the requirements of the target design. nevertheless, choosing among them often requires compromises in terms of cost, performance, fault detection delay, etc. moreover, most of these techniques have been originally developed to be applied on asics (application specific integrated circuit), while nowdays several custom embedded socs (system on chip) are realized on fpgas (field programmable gate array). even if the proposed techniques still maintain their effectiveness, they are not optimized to take advantage of most recent fpga technologies, and they do not consider the nature of fpga-specific faults."
"in any case, the forwarding decision is a non-trivial one and requires careful consideration. we believe that instead of blindly forwarding based on some heuristics, we can do informed forwarding by utilizing the location constraints attached to all the virtual nodes in a vn request. details of this forwarding scheme are presented in the next section."
"on the other hand, each substrate node has a complete cost address associated with it. this address indicates within which city lies the given substrate node. if an inp is not willing to share the exact location, it can always choose a higher level address. for example, instead of announcing nodes in toronto using na.ca.on.toronto, the inp can announce na.ca.on.*. however, such announcements can result in receiving of vn requests that it may never be able to satisfy, which will affect its reputation among other inps."
"scalability concerns in polyvine come from several fronts: size of the search space, dissemination time of location information, and storage of location and price information among others. as the number of inps increases in the controller network, the amount of control traffic will increase even with the tweaks proposed in this paper. moreover, the size of stored location and path information will grow very quickly with more and more inps joining the controller network. we can limit the number of stored paths to a certain destination based on some heuristics (e.g., keep only the top m paths and flush the rest after each update), but such loss can result in degraded embedding. finally, the freshness of the location information is dependent upon the update frequency and the total number of inps in the controller network."
"we have made the following design choices for polyvine aiming toward decentralization of the embedding process, promotion of policy-based decision making, and support for local agility within a flexible global framework."
"the last modification had the scope of driving \"out\" of the cpu all signals needed to properly connect the external alu. outgoing signals are simply driven by the original signals, even if the external alu is not present, while the incoming signals are connected to a 2-way multiplexer (implemented as a simple \"if\" statement in the combinational circuit)."
"a virtual node may restrict its location preference to any prefix in this addressing scheme. for example, to restrict a node within canada, one may assign the address na.ca.* to a virtual node. this indicates that beyond requiring that the node be mapped within canada, the sp does not care where in the country it is ultimately mapped."
"unless otherwise specified, each vn request has an expected value of 30 nodes, and 120 links (±20%). each virtual node has a maximum cpu capacity uniformly chosen from 1 to 25, and each virtual link has a maximum bandwidth capacity of 1 to 15, chosen uniformly as well. locations are chosen uniformly from 256 major cities represented in the controller network."
"in our third experiment in figure 11, we look at the fraction of successful flows relative to the total number of flows of a vn request. we see that up to 35 nodes, all flows are successful. after 35 nodes the success rate drops dramatically, and after 50 nodes (not shown), it reaches 0."
"using the icap interface, reprogramming the reconfigurable area with the external alu partial bitstream required on average 0,5 ms, which in terms of bandwidth is equivalent to about 150mb/s. this figure is acceptable for most systems, and scales almost linearly with clock frequency, as far as the memory throughput doesn't become a bottleneck."
"the rest of the paper is organized as follows. section 2 formally defines the inter-domain vn embedding problem. in section 3 we describe the design choices and the distributed embedding protocol used by polyvine, followed by a discussion of its enabling technologies in section 5. section 6 and section 7 respectively provide preliminary quantitative and qualitative evaluations of polyvine. we discuss related work in section 8. finally, section 9 concludes the paper with a discussion on possible future work."
"in general to allow the online replacement of a part of the combinatorial logic of a cpu's pipeline stage, some modifications are required to the cpu architecture. as an example in this paper we aim at tolerating transient and permanent faults on the alu, which is typically involved in the \"execute\" stage of the pipeline."
"however, a complete end-to-end vn request may not be mappable by any individual inp. instead, an inp can embed a part of the request and outsource the rest to other inps in a similar bidding process giving rise to a recursive multistep bidding mechanism. not only does such a mechanism keep a vn embedding simple for an sp (since the sp does not need to contact all of the eventual inps), but it also ensures competitive prices due to bidding at every step. is defined to be the maximum branching factor at participating inps. as discussed below, the budget for processing is fixed, and effectively, so is k inp max ."
"the polyvine protocol operates under the assumption that every entity in the controller network is behaving in its own best interest, attempting to maximize its profit. each entity provides a service (embedding reservation) to its predecessor in the recursive process and requests a service from its successors. it then selects the service that provides the best price and rejects the other services. however, when an inp reserves resources for a partial embedding for its predecessor, it incurs an opportunity cost: those reserved resources could have been used to service another vn request. for simplicity, we assume the opportunity cost is some constant per inp per vn request. thus, an inp charges its predecessor a processing fee. this has the effect of producing a trade-off between exploration of the space of possible embedding solutions, and price. the more inps visited and solutions explored, the more processing fees incurred. thus, a high branching factor (k inp ) at an inp can be extremely expensive while a lower branching factor reduces the search space (potentially increasing prices), and increases the chance of failure (not finding a feasible solution to the vn constraints in the search horizon)."
"polyvine argues for using a distributed (decentralized) vn embedding solution over a centralized broker-based one. in a centralized solution, the broker will have to know the internal details and mutual agreements between all the inps to make an informed embedding. however, inps are traditionally inclined to share as little information as possible with any party. a distributed solution will allow for embedding based only on mutual agreements. moreover, in a distributed market there will be no single-point-offailure or no opportunity for a monopolistic authority (e.g., the broker)."
"initially, lap operates as a path vector based gossip protocol. every inp in the controller network informs its neighbors of where its nodes are located along with estimated unit prices for its resources on a per location basis. whenever a controller receives a lap update, it updates its lap database and before announcing updates to its neighbors it adds itself to the path vector. note that keeping complete paths allows avoiding unnecessary forwarding toward and through inps that might violate sp's policies or originating inp's policies. inps can also tune this price to encourage or discourage vn request forwarding to them. in steady-state, each inp should know about all the inps with nodes in a given geographic region along with price estimations of embedding on their substrate networks. figure 8 shows an example lap database."
"polyvine allows each inp to use its own policies and algorithms to take decisions without any external restrictions. however, it also creates a high level of competition among all the inps by introducing competitive bidding at every level of distributed vn embedding. even though each inp is free to make self-serving decisions, they have to provide competitive prices to take part and gain revenue in polyvine. to keep track of the behavior of inps over time, a reputation management mechanism can also be introduced [cit] ."
"an embedding at a given inp is considered successful if and only if at least one node is mapped by the inp and all inter-domain paths are successfully mapped. if one or more predecessor inps are unable to map inter-domain paths, then the resource reservations must be released. the current inp issues a link reject message to all inps that responded with a link success message. the inp then waits for acknowledgement that resources have been freed through a link reject ack message. once all pending acknowledgements have been received, the inp releases the resources it allocated locally and issues http://www.jisajournal.com/content/4/1/6"
"the throughput of ncom with cls + mccack consistently higher than pacifier on average 69%. with 2 receivers, the throughput gain is around 50% and increased to more than 100% in the case of 6 receivers. with the more receivers, the average throughput difference between ncom and pacifier get higher. that is because the overlay tree of ncom gets more advantage compare to the pacifier's tree."
"in pacifier, source constructs a multicast tree by merging the shortest paths (based on the etx metric) to all receivers. coded packets are forwarded along the multicast tree to receivers. one component of the pacifier is the round robin batch selection. pacifier moves to the next batch when one receiver acknowledges the completion of receiving the current batch. source node keeps repeating the uncompleted batches in the round robin fashion until all receivers successfully received all batches. that will reduce the influence of the well-known \"crying-baby\" problem when one receiver with poor connectivity slowing down the performance of the whole multicast group."
"the prr p ij of link i, j theoretically depends on the distance between nodes i and j, node density and traffic around i and j, and the mac scheduling scheme. as commonly assumed [cit], packet losses on different links are independent. at a high level, ncom is an overlay multicast scheme developed for wireless networks. overlay networks have been widely employed to deliver multicast services on the internet [cit] . overlay multicast does not require native network multicast support, and can be easily deployed based on unicast primitives. in ncom, given prr on wireless links, an overlay steiner tree is first established to connect the source with all receivers. the overlay tree serves as a virtual multicast backbone. at the overlay level, packets are multicast to all receivers along the overlay tree. neighbors in the overlay tree are not necessarily neighbors in the underlying wireless network."
"specifically, on node j, if equation 1 is satisfied, it is inferred that the coded packet with coding vector u has been received by i."
"the grid size is 250m. we randomly choose one source and four receivers. the average packet loss rate on each wireless link is around 10%, which is determined by the distance between nodes, physical layer setting and data link layer scheduling scheme. the source sends multicast traffic to receivers with three different routing schemes as referred in section iv-a. for pacifier, the multicast tree based on the etx metric is plotted as the thin arrows. for ncom routing schemes, the overlay steiner tree based on the or distance is plotted as the thick arrows. routes from the source to receivers ranged from 4 to 5 hops. table i . ncom routing schemes got much lower source redundancies than pacifier because of the mccack deployment. mccack mechanism helps to significantly reduce the number of redundant packets sent out from source. source redundancy of ncom with cls + mccack is only 2.16 while pacifier get more than 3 times higher than that with 6.79. due to the backpressure effect, cross layer scheduling helps the source quickly adjusts its sending rate to match the network congestion, so ncom with cls + mccack gets even lower source redundancy compare to ncom with mccack."
"the same as more [cit] and pacifier [cit], node could deploy the bootstrap progress to measure the average packet loss rates on wireless links. that values will be flooded all over the network. every node will deploy the simple algorithm 1 to calculate the overlay steiner tree and algorithm 2 to find out if it could involve in the or transmission progress. nodes will self-calculate their candidate receiver set and candidate forwarding set toward each of their candidate receiver node. after the initial overlay steiner tree construction, an overlay node knows its parent and children in the overlay tree. whenever a node receives a packet from its parent, it will forward the packet to its overlay children using multicast or. when a node sends a coded packet to its downstream nodes in the overlay steiner tree, we temporarily refer the node as the sender and the downstream nodes as the receivers for this multicast or. similar to more and pacifier, we implement multicast or with network coding. specifically, the original data file is divided into batches of eight packets. if the sender is the source node, coded packets are generated by random linear combination of the original data packets. if the sender is a forwarder node in the overlay tree, coded packets are generated by random linear combination of innovative coded packets it received and stored in queue b v . all coded packets are generated from the packets in the same batch. receivers can decode a batch of original packets upon receiving eight independent coded packets from the same batch."
"b v stores innovative coded packets for every wireless node. note that \"innovative\" is the only criterion for a packet to be stored in b v . the packet could come from any node around i, no mater it is a upstream or downstream node of i. even if i overheard an innovative packet for a receiver k from one of its downstream node towards k, i still puts that packet into its queue b v . because this packet might be innovative for i's candidate receivers other than k. this is different from ccack where b v only store the innovative coded packets coming from upstream nodes toward the receiver."
"b u stores the coefficient vectors of the coded packets received by i from its upstream nodes. for each receiver k in the crs d i, i has one separate queue b k u . when i gets a packet from an upstream node towards receiver k, it will store the coefficient vector of that packet in b k u . for a received packet, i might update multiple queues b k u if the upstream node has i as forwarder for multiple receivers. (in figure 1, node b updates b r1 u and b r2 u upon receiving an innovative packet from t )."
"as described in section iii-d, to help mac layer determines the contention windows size for a packet transmission, the application layer calculates ∆q and embeds this information in the packet header. the ack vectors towards each receiver is also calculated and embedded in the header at application layer. whenever sending out a data packet, a node always piggyback every ack vector towards each receiver in its crs."
"when an upstream node of i, say j, receives the ack vector z piggybacked in a data packet sent out from i, it can detect which vectors in its b u and b w queues have been received by i."
"one challenge of or is how to coordinate transmissions of forwarders to guarantee reliable data delivery without incur high data redundancy. specifically, a forwarder needs to determine which received packets it should forward, and at what rate. by employing nc [cit], the coordination between forwarders can be significantly simplified. with nc, packets are randomly mixed"
the sender knows the receivers' mac addresses and the or distances from its neighbors to the receivers. it constructs the candidate forwarding sets (cfs)s consisting of neighbors that have shorter or distances toward the receivers than itself. the mac addresses of forwarders in cfss toward each receiver are embedded in the header of the packet sent out.
we set the cases with 40 nodes and change the number of receivers from two to six. we then measure the average multicast throughput for all three routing mechanisms. figure 5 (a) presents the throughput comparison.
"in this paper, we present ncom, a network coding based overlay multicast design, that efficiently integrates network coding (nc), opportunistic routing (or), and cross-layer link scheduling to achieve high efficiency and reliability multi-hop wireless multicast. in ncom, we built up the minimum multicast overlay steiner tree based on or distance, which connecting source to all receivers. with random linear network coding is deployed, coded packets are sent multicastly along the overlay links toward receivers. ncom fully exploit the broadcast nature of wireless transmission and the opportunistic packet reception cross adjacent overlay links. the transmissions of adjacent nodes in the overlay multicast are coordinated by a novel multicast acknowledgement mccack and cross-layer mac scheduling. ncom corrects some weaknesses of other multicast or schemes on: multicast tree design, node coordination rely on offline link state measurements and lacking of cross-layer link scheduling."
"thanks to the advantage of the overlay steiner tree over multicast tree of pacifier. pacifier have a significant throughput reduction since its deploy a static physical multicast tree, which is more sensitive than ncom's tree on dynamic packet loss rates. overlay steiner tree is more robust because it deploys more relay nodes when forwarding packets along the overlay link. the performance may less depend on the packet loss rate on a specific wireless link. more than that, mccack is also robust on wireless link loss rate since it deploy \"online\" link loss rates to control the start/stop transmission process at each wireless node. all on all, those mechanisms help ncom more robust on dynamic packet loss rate on wireless network. this is a very critical advantage of ncom over pacifier."
"we implement ncom in opnet by customizing the ieee 802.11b modules. through opnet simulations, we demonstrate that ncom can achieve a higher throughput and lower source transmission redundancy than the existing nc and or based wireless multicast designs."
"we also would like to thanks yuting zheng, chiang liu, sha hua and xiwang yang from department of electrical and computer engineering, polytechnic institute of new york university for their helps and exchange ideas on network coding and opportunistic routing implementation on opnet."
the rest of the paper is organized as follows. we briefly review the related work in section ii. the ncom scheme is presented in section iii. the ncom protocol design and implementation in opnet is presented in section iv.
"in this paper, we adopt a different approach, network coding based overlay multicast (ncom), that efficiently integrates nc, or and cross-layer scheduling in wireless multicast. in ncom, a source is connected to its receivers by an overlay steiner tree. at the overlay level, packets are multicast to all receivers along the steiner tree. in the underlying wireless network, packet transmission on each overlay link is realized by a multi-hop nc-based or transmission. the unique features of ncom is summarized as the following:"
"some limited efforts have been made to integrate nc and or in wireless multicast. in more [cit], a source node firstly calculates for each receiver the or forwarder set and the expected number of transmissions of each forwarder based on the link etx metric [cit] . then the forwarder sets for multiple receivers are merged and the transmission credits of each forwarder are updated. however, the merge of forwarder sets for different receivers is not necessarily efficient. as a result, more incurs high data transmission redundancy on source and relay nodes in multicast. a more recent work of adopting nc and or in wireless multicast is pacifier [cit] . the source first calculates the shortest path tree to reach all receivers based on the link etx metric. to exploit the or gain, a node not only receives packets from its ancestor nodes, but also can overhear packets from its sibling nodes. in pacifier, the construction of multicast tree does not explicitly take into account the opportunistic packet reception between sibling nodes. the constructed tree is therefore sub-optimal under or. in their experiments, pacifier increases the average throughput over more by 171%. however, it still suffers very high source redundancy. their experiments showed that for pacifier, the source transmits on average 5.84 times the original data size while in more the source transmits on average 17 times the data size."
"the increasing popularity of wireless devices and new wireless applications makes it important to deliver multicast services efficiently over multi-hop wireless networks. it is well-known that the general minimum-cost multicast routing problem is np-hard. wireless multicast additionally has to deal with lossy packet transmissions on volatile wireless links. recent research advances network coding (nc), opportunistic routing (or), and optimal crosslayer scheduling present new opportunities to achieve high efficiency in wireless multicast. with nc [cit], packets are mixed at the source node as well as relay nodes. a receiver can decode the original packets upon receiving enough number of independent coded packets. nc has been adopted to improve the efficiency of multicast in wireline networks [cit] and multihop wireless networks [cit] . or [cit] exploits the broadcast nature of wireless transmission and opportunistic packet receptions to significantly reduce the number of transmissions necessary to deliver a packet. it was theoretically shown that the network capacity can be achieved through cross-layer operations involving source rate control, network routing, and link scheduling, for both unicast flows [cit] and multicast flows [cit] ."
"we would like to deeply thanks professor y. charlie hu and dimitrios koutsonikolas at school of electrical and computer engineering, purdue university for exchanging and discussing with us on pacifier and ccack works."
"the \"crying baby\" issue has common influences to every multicast routing scheme and the proposed method of pacifier could apply to all routing scheme like more, pacifier or ncom with the trade-off advantages on throughput and delay. to get the direct comparison on the performance of the multicast oppotunistic routing efficiency, we do not implement the \"crying baby\" solution for pacifier and ncom in our experiments."
"in unicast or, each node finds nodes with shorter distances to the unicast destination as its candidate forwarders. in the overlay tree of ncom, each overlay node (except for the multicast source s) is a potential receiver of or transmissions. consequently, a node can be chosen as a forwarder for multiple receivers on adjacent overlay links. algorithm 2 -\"candidate receiver set algorithm\" is used to calculate the set of candidate receivers d i towards which node i should forward innovative packets that it received from its upstream nodes along the overlay links. in algorithm 2, s and d is an overlay sender-receiver pair on the overlay link (s, d)."
"in our experiment, multicast or will be deployed on top of ieee802.11b wireless lan standard. at mac layer, packet send out in broadcast mode. the csma/ca mechanism with disabled option of cts/rts will be deployed. to avoid the collision, during the duration that medium is idle, node waits for a random number of timeslots in the range from 0 to the contention window (cw ) before attempting to assess a channel. cw is the contention window length, defining the number of timeslots that a channel needs to be idle before a transmission can take place. this value is initialized before each transmission attempt. the cw counter will be on hold when the channel is assessed to be busy and resume to count when channel return to idle . nodes with higher value of ∆q will have smaller value of contention window size cw ."
"mccack provides feedback for nodes to determine when they should stop transmitting packets for a given batch, but it does not say anything about how fast nodes should transmit before they stop. in more [cit], pacifier [cit], a downstream node is triggered to transmit coded packets by receptions from upstream nodes. the sending rate of a node is controlled by the pre-computed transmission credit. the ccack protocol [cit] also uses a simple credit scheme, which is oblivious to loss rates but aware of the existence of other flows in the neighborhood. all the above schemes assume a given link level scheduling, such as the standard ieee 802.11 mac, do not have direct control on the wireless channel access."
"15: end for 16: end main program at the source and forwarders. both more and pacifier compute offline the number of expected transmissions for each forwarder using heuristic based on periodic measurement of link loss rate. then innovative coded packets are transmitted by forwarders at the pre-computed rate. unfortunately, such an \"open-loop\" design leads to significant performance loss if the link measurement is not accurate or the link losses are dynamic."
we finally keep the case with 4 receivers and simulate the case when packet loss rates of the wireless links dynamically changed in bound ±50% every 5s after 20s of the simulation progress. figure 5(b) shows that ncom get much higher resilience to dynamic packet loss rate with 14% throughput reduction compare to 50% of pacifier.
"for every receiver, it broadcasts a ack packet to its upstream nodes whenever received a code data packet sent to it, no mater if that packet is innovative or not. the ack packets sent out without data payload since it sent from receiver nodes. this is necessary to inform its upstream nodes whether they should temporarily stop transmitting."
"opnet simulations, we demonstrate that ncom can achieve higher throughput and lower source transmission redundancy than the existing nc and or based wireless multicast designs. in our experiment, the average thoughput improvement of ncom over pacifier, the state of art multicast or protocol, by approximately 63%. the average source redundancy is as small as 50% that value of pacifier. ncom get higher throughput gain and smaller packet latency due to its deployment of more forwarding nodes in the or transmission progress."
"the optimal link schedule can be obtained by solving a maximum weight matching problem. this work has recently been extended to study the optimal cross-layer scheduling for multicast flows with network coding and broadcast wireless links [cit] . it was theoretically shown that a broadcast link can be weighted by the summation of the \"information gain\" towards all multicast receivers, and the throughput-optimal cross-layer scheduling can be obtained by solving a maximum information weight matching (miwm) problem [cit] . however, it is np-hard to exactly solve the miwm problem."
"following the spirit of miwm type of cross-layer scheduling, nodes i with higher value of ∆q i will have higher probability to access the channel. ncom employs a csma/ca type of distributed mac layer design. on each node, we use its information gain ∆q to regulate the packet delay time g in csma/ca. we set"
"to test the performance of ncom, we implement the ncom protocol in opnet modeler version 14.0 by customizing the ieee 802.11b wireless local area network simulation modules. we inherited the implementation of the physical layer and the data link layer of the ieee 802.11b from the opnet standard library. ncom is implemented by customizing the application layer, network layer and mac layer of ieee 802.11b."
"motivated by the miwm multicast link scheduling, we design a simple cross-layer link scheduling scheme for ncom to control the wireless channel access for high wireless multicast efficiency. we first quantify the potential number of innovative packets q k i, that a node i could provide for its cf s k i toward receiver k:"
"we then evaluate the performance of the ncom with mccack. the or transmissions and piggyback mccack mechanism are fully multicast. lastly, we turn on the cross-layer link scheduling for ncom for additional performance improvement."
"it has been theoretically shown that the network capacity can be achieved through cross-layer operations involving source rate control, network routing, and link scheduling [cit] . in a pioneer work [cit] for wireless unicast, the maximum weight matching (mwm) type of wireless link scheduling is proved to be throughput-optimal."
"we also measured the latency for each batch of packets. batch latency is defined as the time lag from the source sends out the first packet of the batch until it gets the information that all receivers get enough number of coded packets to decode that original batch of data. we also looks at the number of nodes involved in the forwarding progress in figure 4 in figure 5 (a), we try to measure the performance of ncom and pacifier with the different number of receivers."
is a decreasing function such that nodes with larger information gain ∆q have higher priority in accessing the channel. g 2 is used to keep the randomness of g to desynchronize the collision avoidance on competing nodes. we will elaborate the implementation of the proposed link scheduling in the following section.
"we start with a brief introduction of ccack. in ccack, every node maintains three different queue structures in order to estimate how many innovative coded packets that a node could provide for its downstream nodes. when a node has zero innovative coded packet for its downstream nodes, it will temporarily stop sending until it get new innovative packets from its upstream nodes. on a node i, queue b v stores only innovative coded packets that i received from its upstream node. queue b u stores the coefficient vectors of the coded packets (no need to be innovative) successfully received by i from its upstream nodes. queue b w stores the coefficient vectors of the packets that i sends out. when it is the turn for i to send out a data packet, it generates a coded packet through a random linear combination of packets in its queue b v and sends it out. since the packet will be overheard by i's upstream nodes, i also piggyback a ack vector to inform its upstream nodes about the coded packets in its queue b u . to improve the efficiency of the ack and reduce the header size, i does not directly piggyback the coefficient vector of each packet in its queue b u . instead, only one special ack vector z is generated and transmitted. z is orthogonal to a matrix ∆ created from a group of vectors u in i's queue b u and m hash matrices"
"in this section, we present the ncom design. we start with our network assumptions and an architecture overview of ncom. we then present three major design components of ncom: or-based overlay multicast tree, nc-based or transmission along overlay links, and cross-layer link scheduling."
"oa is a multifactorial disease that causes joint degeneration, affects 27 million u.s. adults, 1,2 with symptoms such as stiffness, limited joint function, and pain which lead to severe disability and impact the overall quality of life. oa primarily affects weightbearing joints such as the knee and hip joints, and pain is one of the most important outcome measures in oa. late stage, symptomatic knee or hip oa is treated by total or partial joint replacement, and while this surgically invasive remedy offers relief, joint replacements often fail after 10-15 years, with shorter life spans in obese individuals. 3 cartilage loss, meniscus changes, subchondral bone changes, ligament, bone marrow changes, and changes in other joint tissues are all implicated in oa. preventive efforts and interventions targeting early stage oa are essential to prevent these tissue level changes, in order to reduce the number of total joint replacement procedures. thus, identifying subjects at high risk for disease development at an early stage when tissue degeneration is potentially reversible is essential. ideally, joint degeneration should be proactively prevented with strategies that target the onset and early stages of oa. if subjects at early stages and at high risk of oa are identified, various therapeutic interventions such as lifestyle changes could be advised. preventative efforts such as weight reduction 4, 5 and various levels of exercise 6, 7 decrease oa progression and should be rigorously enforced if individual assessment demonstrates a high long-term risk for oa."
"in order to test the precision of proposed mechanism in the operator's network, we use a platform based on cloudera's distribution including apache hadoop (cdh4) [cit] version on four nodes including one cluster name node, with computations powers corresponding to each node with intel xeon cpu e5-2670 running @2.6 ghz, 32 core cpus, 132 gbyte ram, 20 tbyte hard disk. in the following section, we describe the data extraction process on this platform."
"we have studied a proactive caching scheme for 5g mobile cellular networks where huge amount of available data is exploited for content popularity estimation via machine learning tools. in particular, we have demonstrated the collection/extraction process of our real traces on a big data platform, then make use of machine learning tools for content popularity estimation. subsequently, caching at the base stations is carried out through numerical studies, to show the benefits of our approach for 5g wireless networks. we led to a conclusion that several gains in terms of users' satisfaction and backhaul offloading are possible, and depend on available rating density and storage size."
"indeed, today's smart phones with anywhere at any-time connectivity, mobile video streaming, resource-intensive and over-the-top (ott) applications, communications in diverse domains (e.g., machine-to-machine communications, smart home, healthcare, connected cars, etc.) with many characteristics (i.e., structured/non-structured) participate to this traffic explosion and falls into the framework of big data (see [cit] for a recent survey). given the fact that large amount of data is available in operators' network, the actual reactive cellular network paradigm relying on base station-centric design with dumb terminals, should clearly exploit this information to move towards proactive context-aware user-centric networks. in other words, big data can enable many solutions for wireless network optimisation such as aggregation of external data sources at the cache-enabled base stations, by exploiting public data from social networks like facebook and twitter, localisation info, user velocity, local events, etc. hence, having caching/computing capabilities at the edge of network and relying on predictability of human behaviour, unleashes further gains in network resources by employing predictive resource management methods and moving strategic information/contents to the edge of network based on these motivations, this article explores the potential of big data in wireless cellular networks from a proactive caching perspective, for further improvements in users' experience and backhaul offloading. indeed, the estimation of content popularity matrix for caching at base stations is a highly challenging task as one has to track spatio-temporal behaviour of users under high data sparsity, large amount of users and content catalog. in order to tackle this, we exploit a big data enabled platform which parallelizes the computation of content popularity via machine learning tools and cache contents at the base stations. as a real world case study, we collect/analyze large amount of data from a telecom operator in turkey, one of the main mobile operator serving more than 16.2 million of active subscribers. after collecting/analyzing these traces in hours of time interval from several base stations under regulation and privacy concerns, we then study various caching scenarios to assess performance gains for 5g wireless networks."
"in addition to radiographs, magnetic resonance (mr) imaging has evolved over the last two decades to respond to this challenge. while mri can exploit the complexity of the joint with capacity in imaging soft tissues from morphological and biochemical point of views, substantial challenges in image analysis and quantitative image biomarker extraction still hamper clinical translation of promising quantitative techniques widely used in research setting. lack in standardization of image interpretation, tedious manual post processing pipelines, including image segmentation and registration, feature hand crafting for morphology and relaxometry analysis are just few of the issues related to high-throughput usage of imaging. however, coupled with advanced quantitative imaging techniques, novel computerized image post processing and more recently machine/deep learning techniques the quantitative characterization of early joint degeneration is now a tangible goal."
"the impact of storage size on the backhaul load/usage is shown in fig. 4b . in this figure, it is clear that increment in storage size yields higher offloading gains (namely less backhaul usage). for example, we see that having 87% of storage size reduces 98% of backhaul usage. on the other hand, the performance of ground truth is higher than the cf approach in intermediate values of storage size. this is due to the fact that popularity-based cache placement with non-identical content sizes in catalog results in high backhaul usage, especially when relatively popular (but big sized) contents are not cached (see fig. 3b ). this clearly points out the importance of size distribution in cache strategies, in addition to consideration of content popularity distribution."
"while the analysis of mri derived morphological oa grades coupled with novel image processing and deep learning techniques have great potential, it is well known that morphological changes are preceded by changes in the cartilage extra-cellular matrix, which are not captured by the metrics above. quantitative mr, including t 1r and t 2 mapping has been extensively used to probe biochemical changes in the articular cartilage in the early stage. while t 2 relaxation time is primarily affected by hydration and collagen structure due to dipolar interactions, 47 the spin-lock techniques used in t 1r reduce dipolar interactions. 48 chemical exchange on proteoglycan and water protons was suggested to contribute to t 1r in cartilage, although t 1r changes in cartilage may be affected by hydration and collagen structure as well. 49 the task of determining quantitative mri degenerative changes is usually accomplished through region of interest (roi)-based approaches. [cit] in this class of techniques, compartments of the cartilage are segmented and each roi is described by average encompassed relaxation time values. sensitivity of relaxation time evaluation trough the extraction of simple averages of cartilage global compartments was widely discussed and previous studies reported that spatially assessing mr images of the knee cartilage relaxation times using laminar and subcompartmental analyses could lead to better and probably earlier identification of cartilage matrix abnormalities. 55, 56 while the value of quantitative mri techniques is extensively proven by the body of recent literature, [cit] the clinical translation of these advanced imagining techniques is still hampered by the tedious and non-scalable post processing or image segmentation required and the too simplistic feature extraction techniques used for the quantification. accordingly, the last few years of quantitative mri research were characterized by a growing interest in exploring fully automatic techniques to assess spatial distribution and local patterns in relaxation time maps. extraction of second order statistical information or texture analysis 60, 61 has been widely used to overcome the limitation of the average roi-based approaches with promising results. however, texture analysis does not address the problem of regional or compartmental differences between the two groups, does not enable the extraction of salient relaxometry patterns and still require segmentation."
"using 1487 mri datasets, a recent study proposed the use of 3d convolutional neural networks to detect and stage severity of meniscus and patellofemoral cartilage lesions in oa and acl subjects. 32 in this study, a \"weakly supervised\" method was used by annotating the presence or absence of lesions in the meniscus or patellar cartilage, obviating the need for time-consuming identification of the lesion at the level of image pixel. figure 1c shows the cascade of deep learning networks used in this study to detect image regions of interest and classify the presence of abnormalities in cartilage and meniscus. results from this study showed that the sensitivity was 89.81% and specificity was 81.98% for meniscus lesion detection and sensitivity was 80.0% and specificity was 80.27% for cartilage. figure 1d shows roc analysis for binary classification. the best performance for staging lesion severity was obtained by including demographic factors, achieving accuracies of 80.74%, 78.02%, and 75.00% for normal, small and complex large lesions, respectively. another study aimed to classify cartilage lesions using similar mri knee data and deep learning. 33 this method was tested on a smaller dataset of 175 subjects with a 2d patch-based approach and \"hard supervision\", with two radiologists (readers) identifying of the presence or absence of cartilage lesions such for 2d image patches (64 â 64) localized to the cartilage region. in this study, the accuracy in binary lesion detection was comparable to, 32 sensitivity and specificity equal to 84.1% and 85.2% respectively for evaluation from the first reader and 80.5% and 87.9%, respectively, for evaluation from the second reader."
"where τ (f d ) and τ (f d ) represent the arrival and end time of delivery for the request f d respectively. then, the average request satisfaction ratio is expressed as:"
"this research has been supported by the erc starting grant 305123 more (advanced mathematical tools for complex network engineering), the sharing project under the finland grant 128010, tubitak teydeb 1501 project grant (numbered 9120067) and the project bestcom."
"a fully automatic, local and unbiased algorithm for studying knee t 1r and t 2 relaxation times by creating an atlas and using voxel-based relaxometry (vbr) was recently proposed. 39 this technique allows for the investigation of local cartilage compositional differences between two cohorts, or between different time points in the same cohort. an atlas-based technique aligned all images onto a single template and was used for fully automatic articular cartilage segmentation. 39 vbr has been used in a multicenter study with the aim to explore how cartilage lesions at the time of acl injury influence the longitudinal progression of cartilage degeneration, 62 to study differences between the acl injured and contralateral knee in the composite r 2 -r 1r metric and to explored this metric as a possible associative biomarker for patient-reported outcomes. 63 vbr coupled with principal component analysis (pca) has been used to compare biochemical patterns in acl and oa subjects. 64 vbr was also employed in analyzing the effect of posterior meniscus lesions on adjacent cartilage composition, in absence of morphological cartilage defects, 65 and correlation between relaxation time and patellofemoral joint stress in pfjoa subjects. 66 figure 2a and b show two examples of vbr group analysis. in figure 2a group average for 64 acl subjects on the injured side, the corresponding contralateral side and the paired average % difference is shown; in figure 2b group average for 49 patello-femoral osteoarthritis (pfjoa) subjects, 34 matched controls, average % difference and pvalue map is shown. figure 2c shows an example of local correlation analysis with a non-imaging variable. local r-value and p-value maps depict the associations between t 2 values and pfjoa stress computed from gait kinematic data."
"oa does not affect just the geriatric population; knee injuries, specifically anterior cruciate ligament (acl) injuries are well known risk factors for development of post traumatic oa (ptoa) in younger subjects. 8, 9 at the hip joint, well known as a risk factor for oa, femoroacetabular impingement (fai) is a morphological abnormality of the hip joint, which causes abnormal joint loading patterns and may cause acetabular cartilage delamination. 10 increased shear forces within the hip joint, particularly due to the cam-type impingement, may cause enlargement of the cartilage flap and lead to complete detachment from the adjacent cartilage thereby producing loose bodies and fullcartilage thickness defects. 11 this alarming burden of knee and hip oa that affects subjects at different age and activity levels calls for the development of quantitative biomarkers for joint degenerative disease to fill the void that exists for early diagnosis, monitoring, and assessing the extent of whole joint degeneration."
"in the simulations, d number of requests from traces-table are considered, spanning over 6 hours 47 minutes of time interval. more precisely, the arrival times (frame time), requested contents (http-uri) and content sizes (size) are taken from this table, and associated to m base stations pseudo-randomly. the following two methods are used for comparison:"
"the impact of storage size on the users' request satisfaction is shown in fig. 4a . note that 100% of storage size in the figure corresponds to caching of entire catalog (17.7 gbyte in our case), and 0% corresponds to no caching. from the figure, we observe that the satisfaction is monotonically increasing as the storage size increases, whereas a performance gap between the ground truth and cf is experienced until 87% of storage size, mainly due to the estimation errors. for example, the ground truth and cf achieve 92% and 69% of satisfaction respectively when considering 40% of storage size."
"in order to get the final table table has approximately over 420.000 of 4 millions http requesturis with size field returned as not zero or null due to unavailability of http response for some requests. note that different http request-uris can appear in a given session with a specific teid. each teid corresponds to a specific user. also, each user can have different teids with different http request-uris. we summarize the steps of data extraction process in fig. 2 ."
"savic et. al. 95 also explored correlations between voxel based relaxation time t 1r and t 2 in the cartilage with sodium fluoride uptake metrics (slope k i and suv), showing correlations not only not only in adjacent regions but also in compartments that are not adjoining. correlations between suv and cartilage compositional data were also recently shown in acl subjects. 93 the ability to simultaneously monitor early changes in bone remodeling and cartilage composition, linked with the assessment of morphological shape changes has the potential to make a tremendous impact of defining oa as a total joint disease. the translation of these tools to the clinic would potentially bring not just advanced diagnosis to the point of patient care, but also open the path for drug discovery and the development of new therapeutic regimes."
"the ability of 3d ssm to quantify hip bone shape features from mri was also recently published. in this study bone shapes were associated with morphological and biochemical degeneration of the articular cartilage and may be early signs of hip oa. figure 3b shows vbr analysis of the relationship between coxa valga shape variation of the proximal femur and prolongation in t 1r relaxation time in subjects with hip oa. associations between bone shapes with hip joint mechanics during gait were also observed. this study confirmed previous results that used radiographs to establish a link between proximal femur bone shapes and oa, 86, 87 showing further evidence of the critical role of the bone shape in the development of oa and of the ability of mri and 3d shape modeling to quantify those features."
"in the numerical setup, the storage capacities, backhaul and wireless link capacities of sbss are set to identical values for ease of analysis and revealing of caching gains. the values of parameters in the numerical setup is summarized in table i ."
"first, we parse raw data using wireshark command line utility tshark in order to extract relevant fields of cell-id (or service area code (sac) in our case, used to uniquely identify a service area within a location area 2 ), lac, hypertext transfer protocol (http)-request-uniform resource identifier (uri), tunnel endpoint identifier (teid) 3 and teid-data for data and control plane packets respectively, and frame time indicating arrival time of packets. the http request-uri is a uniform resource identifier that identifies the resource upon which to apply the request. the control packets contain information required for future data packets. in particular, it contains cell identification id (cell-id), lac and teid-data fields. the data packets contain http-uri and teid fields."
"while the body of literature on imaging applied to oa is vast, and many are the review articles that aimed to summarize such effort [cit] a void exists in linking recent technical advancements in image processing and deep learning to relevant musculoskeletal clinical questions. with this manuscript, we aim to fill this void by summarizing cutting edge technologies which have plausible promise in changing clinical practice in the near future."
"advanced imaging and deep learning techniques discussed above allow us to extract an extraordinary rich array of heterogeneous information on populations with large sample sizes. while the complexity of knee the same network is shown colored by kellgren and lawrence (kl) grade (oa severity). the combined network showed differences in osteoarthritis severity between the subnetwork 1 (prevalence of blue nodes low kl grading) and subnetwork 2 (prevalence of red nodes high kl grading). (c) the subjects in the progression cohort are located in both subnetworks. however, in subnetwork 1 progressors are located in a specific region marked with a dashed circle. combined metric r2-r1r is the first variable able to significantly distinguish between progressors within subnetwork 1 and the rest of subnetwork 1. second row shows an example of topological data analysis to study associations between progression in cartilage degeneration and symptom worsening 128 hip oa patients. (d) extracted network based on biomechanics, bone shape, and compositional mri variables showing colored by kellgren and lawrence (kl) grade (oa severity). the combined network showed differences in osteoarthritis severity in three subnetworks. (e) topology colored by percentage of subjects for each node that showed significant t 1r /t 2 progression in 36 month \"compositional progression\". (f) topology colored by percentage of subjects for each node that showed significant hoos progression in 36 month \"symptoms progression.\" dashed circle showed opposite trends between compositional and symptoms progression in the oa early stage subnetwork."
"looking at bone remodeling macroscopically, several investigators have proposed that aspects of bone shape are associated with an increased risk of incident oa and with the severity of oa, based on anthropometric measures, cross-sectional findings, or shape modeling of knees. [cit] studies based on 2d radiographs also reported bone shape gender differences, an association with incidence oa 78 and differences in bone shape in subjects with lateral and medial oa. 79 statistical shape modeling (ssm) is a widely-used tool to describe shapes in a brief but comprehensive features vector. ssm has the ability to characterize complex shapes using principal component analysis (pca) to reduce the dimensionality of the data. pca provides an orthonormal basis. therefore, each component of the features vector (mode) describes a different aspect of the bone shape uncorrelated with the other components. the effect of each mode on the average surface can be modeled individually, synthesizing new instances. ssm has the capability to analyze shape differences without a priori assumptions, instead identifying the geometrical features empirically. ssm coupled with the 3-dimensional nature of mri has great potential in identifying knee oa risk factors and in studying disease pathogenesis and large body of work was done recently, 75, 77, [cit] other studies also used this technique to evaluated the contribution of knee shape to acl tears, 83 to assess the association between bone shape and the progression of cartilage degeneration 84 and alternated knee kinematics 85 after acl reconstruction. figure 3a shows modeling of the shape variation associated with abnormal knee kinematics after acl reconstruction."
"one possible direction of this work is to provide a more detailed characterization of the traffic which can reflect various spatio-temporal content access patterns. in this regard, design of novel machine learning tools are also needed so that cache placement at the base stations can be applied more efficiently. design of deterministic/randomized cache placement algorithms is also of high interest and should not purely rely on content popularity."
"bone cartilage crosstalk is consider as one of the key factors in understanding oa pathogenesis. 88 a hypothetical model for oa pathogenesis has been proposed by burr et. al., 89 whereby repetitive joint loading causes an initial increase in bone remodeling, that is associated with increased vascular invasion of the deep layers of cartilage, which allows access to the cartilage by chondrolytic enzymes unopposed by inhibitors of the degradative proteinases, that cause a breakdown of the extra-cellular matrix, loss of simultaneous pet-mri was also recently used to study cartilage and bone interactions. [cit] savic et. al. 95 performed a pilot study analyzing sixteen oa subjects and showed increased uptake of naf in the knee in oa subjects. similar results were then obtained by kogan at. el., by analyzing 22 subjects with knee pain or injury. both studies showed increase in standard uptake values (suv) in proximity of morphological abnormalities as bone marrow edema. figure 3d shows an example of a subject with knee pain and bone marrow edema like lesion (bmel) abnormality with clear localized increased pet uptake in correspondence to the morphological abnormality."
"in addition to degeneration of soft tissues, it has been suggested that changes also occur in the subchondral and trabecular bone. articular cartilage and subchondral bone act in tandem with regards to the mechanical loading of the joint. the subchondral mineralized zone plays an important role in reducing the impact forces typically encountered during dynamic joint loading and adapts to the mechanical demands during normal and abnormal joint loading. [cit] both earlystage increased bone remodeling and bone loss, as well as, the late-stage reduced bone remodeling and subchondral densification are important components of the pathological process that leads to oa."
"in order to minimize the access delays to the contents in such a network, especially during the peak hours, we prefetch the strategic contents at the sbss, aiming to obtain higher satisfaction ratio and less backhaul load. to show this, let us define the cache decision matrix of sbss as"
"hadoop is one of the key big data framework that supports massive volumes of transactional data at the lowest level of granularity. it supports implementation and execution of distributed applications on large clusters. a computational model named map-reduce enables execution of an application in parallel, by dividing it into many small fragments of jobs on multiple nodes. the storage module, called hadoop distributed file system (hdfs), is in charge of handling the data storage across the clusters."
"so far, we have compared the performance of these approaches under the setting of 10% of rating density in cf. indeed, the rating density is crucial in cf as non-sufficient amount of ratings might lead to high estimation errors (sometimes called cold-start problem). to show this, the impact of rating density on the root-mean-square error (rmse) is shown in fig. 5, whereas the error is defined as the rootmean-square difference of users' request satisfaction in both methods over all possible storage sizes. as confirmed in fig. 5, the performance of cf highly depends on the availability of ratings, where increase of ratings results in less errors thus yielding higher performance."
"robustness and generalization of these techniques when applied on images acquired using different mri sequences or those acquired on different (vendor) systems still needs to be tested. mechanisms of continuous learning will need to be designed in the initial phases of clinical usage. this however, raises several questions, for example, the levels of reliability of the new ground truth coming from the daily use would depend on individual user radiologist preferences and training, and methods to handle artificial intelligence techniques with this sort of reinforcement learning will need to have a defined evolution methodology to avoid instability."
"the spatial variation of cartilage relaxation times in the hip joint, and the need for a regional analysis to extract imaging biomarkers from relaxation parametric maps has also been recognized. 67, 68 in an attempt to address this challenge, vbr has been also proposed for the hip joint and was adopted to study the local association between t 1r and t 2 relaxation times and longitudinal changes in patient-reported outcomes ( figure 2d) . 69 combination of texture analysis and vbr was recently proposed as imaging biomarker to detect delamination in subjects with fai. 70 automatic vbr analysis makes clinical translation of quantitative compositional mri a more tangible goal, bringing the reality of characterizing early changes in the extracellular matrix closer to the point of patient management. however, challenges related to pulse sequence and hardware sensitivity of relaxation time measures are still open questions that need to be addressed."
"in this review, we present a summary of recent advances in quantitative imaging, the application of image processing and deep learning techniques to study knee and hip osteoarthritis (oa). we aim to give a prospective on how these techniques can help clinical translation of morphological and functional musculoskeletal imaging. we assume a basic knowledge of the current state of the art image acquisition methodologies."
"in practice, in order to estimate p in (6), we use a big data platform of the aforementioned network operator. based on this estimation, we then store contents at the cache-enabled base stations whose cache decisions are represented by x. by doing so, minimization problem of backhaul offloading in (5) is attained. the network model consists of big data platform and cache-enabled sbss is illustrated in fig. 1 . the following section is dedicated to details of our big data platform and characterisation of users' traffic pattern."
"the rest of this paper is organized as follow. in section ii, we describe our network model for proactive caching. in section iii, a practical case study on a big data platform is presented for content popularity estimation, where data extraction process and characterization of user's traffic are given. in section iv, we show the performance of proactive caching via numerical studies and discuss our results accordingly. finally, we conclude in section v and provide future directions."
drafting of the article was done by valentina pedoia. obtaining funding was done by valentina pedoia and sharmila majumdar. final approval of the article was given by valentina pedoia and sharmila majumdar
"in our work, we collect the streaming traces in a server with high speed link of 200 mbit/sec at peak hours. this is done by initializing a mirroring task via real-word gn interface data. 1 after the mirroring stage, network traffic is then transferred into the server on the platform. for our practical case study, we have collected traffic of approximately 7 hours starting from 12 pm to 7 pm on saturday 21 [cit], and is processed on hadoop platform."
"mobile cellular networks are nowadays facing an exponential wireless data traffic, forcing mobile operators to operate their ever-growing networks in a more complex manner. the next generation 5g wireless networks aim to fulfil this demand i.e., by device-to-device communications, edge caching (namely caching at base stations and user terminals), massive multiple-input multiple output massive multiple-input multiple-output (massive-mimo), ultra-dense networks and millimetre wave communications (see [cit] and references therein). in fact, continuous efforts for improvement of spectral efficiency and maturity of air interface in current standards (i.e., lte-advanced) say that no major gains can be expected in spectral efficiency, thus novel approaches are urgently needed."
"the big data platform used in this work analysis user's data traffic and runs in the operator's core network. in other words, the purpose of this platform is to extract useful information for proactive caching decisions and store users' traffic data. in brief, the operator network under investigation covers more than 10 regional core areas in several districts in turkey. the average total traffic consists of approximately over 20 billion packets in the downlink direction and over 15 billion packets in the uplink direction over all territorial areas. consequently, the core network of the mobile operator is loaded by approximately over 80 tbyte of total data in uplink and downlink daily. also, an exponential increase of data usage is observed in this mobile operator. [cit], the daily uplink and downlink traffic was approximately over 7 tbyte."
"in conclusion, quantitative imaging is growing as an area of research. the advent of machine/deep learning methods may potentially be used for standardized grading of x-rays, mr morphological changes and disease progression. the standardization of quantitative imaging, relaxation time imaging lays the fundamental groundwork for characterizing early changes in cartilage, meniscus, ligaments, with the inclusion of pet methods, further expands the realms to include bone remodeling. these methods when combined with multi-dimensional, big-data analytics further opens up the horizons for understanding the etiology and progression of joint degeneration and osteoarthritis."
"the next step consists of transferring data with these relevant fields into hdfs for further analysis. this allows to process data, and apply different data analytics over header information of both control and data planes using high level query languages such as hive query language (ql) and pig latin, where the concept of map-reduce is inherently embedded. for instance, in order to calculate the http requesturis at given location, the http-uri can be merged with cell-id-lac fields over the same teid and teid-data fields for data and control packets respectively. in our analysis, due to the limitations on collected number of rows for http-uri fields matching with cell-id-lac fields, we have proceeded with http request-uris and teid mappings."
theoretical values for the attenuation coefficient of both lead and tungsten were calculated using the national institute of standards and technology (nist) crosssectional database (xcom) [cit] and the reported density of the raw metal. the nist database was constructed through the combination of incoherent and coherent scattering cross sections. incoherent (compton) scattering cross sections were obtained from a combination of the klein-nishina formula and non-relativistic hartree-fock incoherent scattering functions. the coherent (rayleigh) scattering cross sections were calculated from a combination of the thompson formula and relativistic hartree-fock atomic form factors. the photoelectric cross sections were obtained by a phase-shift calculation for a central potential and a hartree-slater atomic model.
"the total tungsten thickness (based on the micrometre measurements of the previous section) between the source and collimator hole varied from 0 to 3.4 mm. acquisitions were also performed using 0.5 mm thick lead disks, with tc-99m (140 kev) and i-131 (364 kev) point sources to assess the attenuation properties of both materials. energy spectra of the collimated beam were generated with 300-s acquisitions. spectra were corrected for background and scatter by subtracting spectra data acquired in a similar geometry with the absence of the 1 mm collimation hole. for each energy spectra, the net counts within a 20% energy window centred at the photopeak were measured and corrected for deadtime and radioactive decay. net decay-corrected count rate verses disk thickness was plotted on a log-linear scale and the gradient of the fit to the data was used to determine the material attenuation coefficient, μ, such that:"
"given the head motion in the horizontal and vertical directions, the goal is to capture the oscillatory characteristics t of the pre-defined points, considering each vector component as an independent time-series. although this independence assumption does not strictly hold, the computation is greatly simplified. finally, the feature vectors are constructed by concatenating the fourier transform outputs. typical fourier features for speaking, nodding while silent, and not nodding while silent are illustrated in figure 3."
"measured attenuation properties indicate that shielding could be reduced by 10% when replacing lead with slm tungsten. performance of slm tungsten has been shown to be superior to lead alternatives and the unique opportunity for bespoke designs. current manufacture technology is limited by the size of the powder bed chamber, which for an eos m280 results in a maximum foot print of 250 mm by 300 mm and maximum collimator thickness of 200 mm. this current limitation restricts the use of slm to small fov collimators, such as for portable and small animal scanners. however, as technology advances and larger printing chambers become available, clinical-sized collimators could become a possibility; alternatively, collimators tessellation could be investigated to increase the fov."
"in this study, we developed and evaluated a multimodal method to detect natural head nods in face-to-face interactions. our work brings the novel angle of examining in detail the effect of the speaking self-context on head nod detection. two nodding models were trained, depending on the speaking status of the person under analysis. compared to the baseline vision-only method, results demon- strated that audio-based self-context improved the detection of head nods, underlining the difference of head gesture dynamics conditioned on the speaking status of the person, as suggested by previous work in psychology. the developed method yielded competitive results on this dataset, allowing to detect subtle nods while keeping the number of false positive low. the method presented in this paper could be extended for the detection of head shakes by using the same frequency domain motion features. other possible avenues for future work would be to use more detailed audio context, e.g. taking into account the prosody or lexical information of both protagonists in the interaction. the presented method could also be seen as a first step towards the classification of head nods defined by their communicative functions; we hypothesize that context from both protagonists in the form of visual, prosodic, and lexical features would have to be used for this task."
"the precision, recall graph for p varying from 0 to 8 is plotted in fig.6 . it can be observed from fig 6 that, the precision of the system is 100% for values of p from 2 to 8."
"in face-to-face interactions, head nods occur in every discussion. in most cases, people producing the head nods are not even aware of the social signal they emit: head nods are often the result of automatic processes. independently of their function or meaning, head nods can be defined as vertical up-and-down movements of the head, rhythmically raised and lowered [cit] ."
"the optimal match to measured data was found to be 80% density and + 10% additional septal thickness. comparison of this simulation with measured data and a similar lead collimator are shown in fig. 13 . measurements of the full width half, tenth and twentieth maximums at different distances for the physical and simulated collimator are shown in fig. 13b . a slight deviation is observed between the measured and simulated profiles at the tails, which is beyond the physical edge of the phantom and thought to be due to additional scatter through the lead shield surrounding the collimator. this scatter is negligible compared to peak counts in the profiles and in general, a good match for all full width measurements is observed. the numerical significance of the reduced density is summarised in table 7, using results generated from the monte carlo simulation. the matched collimator is highlighted in grey. the percentages of photons within the line source image that have scattered or penetrated are shown against those that have passed through geometrically. the effect of the additional septal thickness is demonstrated in the geometric efficiency which is 6% lower for the matched collimator."
"for handling ease and to allow removal from the printing plate, a 2.0 mm tapered annulus was included around the periphery of each disk. a photograph of a typical disk and schematic cross-section in the printing plane are given in fig. 1 . disks were printed in an orientation such that the central section was orientated in the same printing plane as collimator septa."
"as stated in the introduction, head nods are defined as vertical up-and-down movements of the head rhythmically raised and lowered. this implies an oscillatory pattern in the vertical axis, while the motion in the horizontal axis is limited. in order to encode this effect, we constructed features based on fine-grain motion detection and transformation into the frequency domain. the extraction of the features to characterize head nods follows a similar spirit than [cit], but relies directly on the motion estimates derived from the video sequence rather than on the output state of a head tracker, which might not be so sensitive to subtle movements of the head. a binary classifier is then used to assign frames to one of two classes, nodding and not nodding."
"video was recorded during the natural interactions. illustrations of the experimental setup and the recorded data are displayed in figure 1 . two 1280x960 monocular cameras were used, recording both protagonists of the dyad at 26.6 frames per second. camera views were quasi-frontal, filming the upper part of the body. in addition to the images, the timestamps for each video frame were also recorded with a resolution of 1µs. in total, the dataset comprises approximately 160 minutes of recording (∼260'000 frames). average video duration was 10 minutes."
"the social psychology community was the first to examine the functions of head nods during face-to-face interactions. apart from the obvious function of signaling a 'yes', head nods are used inter alia to display interest, enhance communicative attention by occurring in synchrony with the other's speech, or anticipate an attempt to capture the floor (i.e., permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. signaling a turn claim) [cit] . head nods form a major mode of communication in backchannelling, that is, during listener turns [cit] . additionally, head nods can be used during speaker turns to elicit feedback from the listener [cit] . the psychology literature suggests that the frequency of head nod events in face-to-face interactions can reveal personal characteristics or even predict outcomes. for instance, job applicants producing more head nods in employment interviews have been reported to be often perceived as more employable than applicants who do not [cit] . in this sense, the ability to automatically detect head nods could be useful to build automatic inference methods of high-level social constructs."
"head nods of small amplitude and duration are in general accurately detected by the proposed method. additionally, because of the switching dynamics conditioned on the speaking status, the number of false positives is kept low. in other words, speaking can be seen as an attenuation factor of the head nod detector."
the attenuation properties of slm tungsten have been characterised. slm tungsten was shown to have superior attenuation properties compared to lead and a valid alternative for collimator design. the surface texture of the tungsten can have a significant impact on apparent material density and careful selection of the scanning strategy is required when using slm to ensure optimum design performance.
"selective laser melting (slm) using powder deposition techniques are the most common methods employed for generating metal objects. a high-power laser is used to fuse small particles of metal powders into a mass that has a desired three-dimensional shape. the laser selectively fuses powdered material by scanning cross-sections of the part on the surface of a powder bed. after each cross-section is scanned, the powder bed is lowered by one layer thickness, a new layer of material is applied on top via an additional supply bed and roller and the process is repeated until the part is completed."
"of the five sets of disks, a and d demonstrated approximately 70% of the expected attenuation. disks from sets b, c and e performed less well with only 40% of the expected attenuation."
"considering the retrieval time and precision are the parameters of importance from accuracy point, the following combinations are recommended for the best performance in the developed prototype system."
"being more commonly used to generate plastic parts, additive manufacturing techniques can also be employed using other materials such as paper, wood, ceramic, glass and a variety of metals."
"we then computed the velocity at three arbitrarily defined points (see figure 2) inside the bounding box, using the parameters of the optical flow model (equation 1), providing the horizontal and vertical components of the motion at these three points. roughly speaking, these points are around the mouth and eyes of the participant. typical motion time-series for speaking, nodding while silent, and not nodding while silent are illustrated in figure 2 . the figure illustrates that head nod activity does present differences depending on the self-speaking status, and that building nodding models separately for the speaking and silent cases could reduce the confusion between, for instance, a head nod and a quasi-random head gesture displayed during a speaking turn."
"measurements of attenuation were made by placing an increasing number of disks from each sample set within a 20-mm-thick lead holder, housing a radioactive point source. a 1-mm-diameter hole collimated the photons from the source at a 200-mm-diameter nai(tl) detector positioned 50 mm from the aperture and connected to a signal amplifier and multi-channel analyser. a schematic of the set-up is shown in fig. 2 ."
"the goal is now to assign each feature vector to either of these two classes, nodding or not nodding. for each of the speaking status values, we trained a separate linear support vector machine (svm) to perform the classification. this multimodal approach directly takes into account the switching dynamics of head movements, depending on the speaking status of the person under observation, as suggested by previous work in psychology [cit] ."
"the training set was defined as follows. the positive set was composed of all frames labeled as obvious nods. the negative set was selected randomly from the set of frames labeled as non-nod. frames labeled as subtle nods were not used for training because they can be too similar to not nodding features. in addition to this, transitional frames were discarded to attenuate the dependence to time-related annotation inaccuracy. the training set was balanced, i.e. the number of positive and negative examples was equal. approximately 5000 training examples were used for each class. we then further segmented the data into speaking and silent, training each separate model on its own training set."
"in order to benchmark head nod detection methods, we collected a dataset of 8 natural interactions (16 videos treated individually). pairs of participants were asked to sit at both sides of a table and have a relaxed conversation on a topic of their choice. dyads were acquainted before taking part to the experiment. in total, there were 9 different people (one person was in all conversations)."
"nodding does not occur in a void. it is known that the speaking status of people influences the dynamics of the displayed head gestures [cit] . when a person is speaking, the motion of his head has typically greater amplitude, larger frequency range, and follows a close to random pattern [cit] . on the other hand, when the person is listening, his head tends to be more static as a result of both attention to the speaker and the fact of being silent. in this sense, head gestures are multimodal: the dynamics are conditioned on the speaking status of the actor."
"in order to train and test the algorithm, annotations were performed on the dataset. depending on the amplitude and duration of the up-and-down oscillatory movements, head nods can be difficult to code; two classes of head nods were therefore defined: obvious and subtle. head nods were annotated by one of the authors, thus well acquainted with the concept of nodding, who noted the onset and offset time of an event, and qualitatively decided the nod class based on nod amplitude and duration. speaking status was also manually annotated, marking the beginning and the end of a speaking segment. in total, nodding occurred during 858 seconds (22'812 frames), of which 92% occurred when the person under analysis was silent. average head nod duration was 1.2 seconds. on average, speaking and silent times were split equally for each speaker in the dataset."
"this hypothesis explains the improved attenuation properties of sets a and d over that of the other scan strategies. furthermore, the textural difference in scan patterns can be observed visually. figure 15 is a photograph of the disks produced using scan strategies d and e where the absence of the contour scan is clearly apparent. the implications of \"surface roughness\" will vary depending on the object thickness, scan direction and spot size, and will become negligible for bulk objects. however, the importance at small thicknesses should not be overlooked. this is demonstrated in fig. 16 where an apparent difference is observed between 0.5 and 0.3 mm disks (although statistically not significant due to the small number of 0.3 mm disks)."
the evaluation of the head nod detection method was conducted at the frame level. leave-one-out cross validation was performed at the sequence level: the algorithm was trained on all except one sequence and tested on the remaining one. the binary output of the svm classifier was compared to the annotated ground truth (including obvious and subtle nods).
"henning muller [cit] in his review article brings out the need for an efficient medical image retrieval application with reference to large number of imaging modalities being used for the patient care in tertiary hospitals. the article lists out some of commercially developed systems such as query by image content (qbic), virage, candid, photobook and netra. some the freely available application such as gnu image finding tool kit (gift) being mentioned in the article. manjunath k n [cit] brings out the advantages of implementing cdf based on the least square line fitting method. the article mentions the advantages of cdf method in case of registration difficulties."
"direct measurements of attenuation of tc-99 m and i-131 through lead disks were made. measurements were used to determine the linear attenuation coefficient, results of which agreed to within 4% of the theoretical value for tc99m and 10% of the theoretical value for i-131. the higher variation observed with i-131 is likely due to the higher amount of scatter present in the i-131 spectra (fig. 6 ) and although a scatter correction was applied, isolating penetrating photons from scattered photons is difficult and the correction may not be 100%. however, the results with tc-99m were considered sufficient to validate the measurement technique."
"in order to provide efficient patient care many hospitals are implementing picture archival and communication systems (pacs) now a day. in a practical scenario mid-sized hospitals cannot afford to have the pacs due to the initial investment which is quite huge. these hospitals can have the services of pacs through a wide area network (wan). size of the medical image being very large, transmission of these images to a distant healthcare setup is limited by the bandwidth of the wide area network (wan) or internet. the image viewing at a remote station will be a very slow process and likely to cause severe delay which is could be a great concern for a physicians if he/she wants to view these images before treatment. hence it is necessary to have an efficient content based image retrieval system which is user friendly, user definable, accurate and fast."
"a line profile simulated for tungsten at 80% and 100% density is compared to the measured data in fig. 12a difference in the scatter tails of the data. similar profiles are shown in fig. 12b, for 80% density and septal thicknesses of 0.2 mm (nominal) and 0.24 mm (nominal + 20%)."
"we used the affine motion model defined in equation 1, where (xi, yi) denotes a point in the image, v (xi, yi) the flow vector modeled at point (xi, yi). visual motion estimation over the whole face region provides an accurate estimation of head movements and therefore allows to capture subtle patterns using a multiresolution robust estimation method [cit] . parameters tx, ty, a1:4 were estimated using least-meansquares, implemented by the software package motion2d"
"the retrieval time (tr) tabulated in the table 1 is dependant on the number of images dynamically compared in the database. the above results is for a database of 1720 medical images. it can also observed that, for the values of p greater than 1 (the cd f of the image is represented by 2 line segments) the values of precision (p) and recall (r) remains to be constant for values of p greater than 1."
"of the five disk sets manufactured, sets b and c showed visible signs of cracking at thicknesses below 1.0 mm and an insufficient number of disks were available for an adequate measurement of μ. however, with the disks available, a measure of attenuation through 0.5 mm of each tungsten disk could be determined. figure 7 shows the total beam attenuation of 140 kev photons through a 0.5 mm tungsten disk of each sample set. results are expressed as a percentage of the expected attenuation based on nist data given in table 3 . where multiple disks from each set were available, results were repeated and the standard deviation expressed as error bars on the chart."
"most methods for automatically detecting head nods have been developed in the context of human-computer interaction (hci). the primary goal of these studies was to enable a machine to detect a 'yes' signaled by a head nod. within this context, some studies proposed to track interest points of the face [cit] and use state-based approaches such as finite state machines [cit] and hidden markov models [cit] to detect nodding. these approaches show good performance in restricted contexts where the head motions are explicit. however, these methods do not allow to detect subtle head movements that occur quite often in natural human face-toface interaction."
"monte carlo simulations of the experimental measurements were performed using the simind monte carlo software [cit] which allows the user to specify input criteria for the geometry and material for the source, phantom, collimator and detector. simulations of the collimator measurements were performed using input criteria that matched a b the siemens symbia gamma camera. input data for crystal thickness, backscatter layer, intrinsic energy and intrinsic spatial resolution were based on validated input criteria from an earlier study [cit] and summarised in table 2 . changes to the simulation inputs for the standard gamma camera collimator were made to match the specifications of the tungsten collimator and a 1-mmdiameter line source positioned across the full length of the detector field of view. two hundred million isotopically distributed 140 kev photons were simulated. events from primary, scattered and septal penetrating photons incident on the detector were recorded. summed profiles across the length of the generated line source images were generated using imagej and compared to experimental measurements. the collimator material density value contained within the simind cross-section file for tungsten was altered for decreasing levels of density (100-70%) and the analysis of the simulation data repeated with different septal thicknesses (± 20%); these values were chosen based on the observed results obtained for the slm disks."
deviations from the nominal disk thickness are summarised in fig. 5 . there was no significant correlation between printing method and thickness accuracy. production accuracy ranged from − 40 to 300 μm with an average deviation of 20% from the nominal thicknesses set within the central section of the disk.
"the effect of reduced attenuation properties of the slm tungsten were highlighted in the collimator acquisitions. the \"tails\" at either side of the line source and the non-gaussian shape of the line profile indicate scatter and septal penetration within the collimator. iterative monte carlo simulations of differing collimator properties indicated that the slm collimator was a similar match to 80% density tungsten and 110% nominal septal thickness. these results are comparable to the deviations observed in the measurements of μ and thickness determined in the disk experiments. results from the simulations suggest that 33% more scatter and penetration occurs in the collimator as a result of the reduced attenuation properties, compared to \"pure\" 100% density tungsten. equally, the slm collimator exhibits 17% less scatter and penetration that the nominal lead collimator. however, in each case, the total scatter to geometric photons is low and the total difference is less than 4.5% of the total photons within the energy window. due to the apparent reduced density of slm tungsten, attenuation performance of the material is less than the theoretical density (assuming 100% density). however, with careful selection of scanning strategies and a knowledge of material performance, design optimisation can be achieved. additional optimisation may also be achieved by further investigation of additional scan parameters such as laser powder, scanning speed and hatch distance. microstructure analysis would also be useful to help understand the differences from each scan strategy and also to analyse some possible defects like cracking, porosity and a lack of fusion that will impact on the material performance."
"to validate our hypothesis that self-context in terms of speaking status improves nodding detection, we implemented a baseline method using the visual modality only. for the visual-only method, we used a single svm trained on the full training set (i.e. not separating it into speaking and silent)."
"the contextual nature of nodding has been used in the recent past. work addressing the prediction of backchannel feedback makes use of these findings. the goal of this line of research is to enable robots or conversational agents to produce natural backchannels. in this type of setting, the contextual information such as lexical information or prosodic cues are used to predict head nods [cit] ."
"while the value of using audio-based context from the perspective of the speaker to improve the detection of listener head nods has been established, one aspect that to our knowledge has not been studied in detail is the effect of the audio-based self-context on head nod detection. this study develops a multimodal method using the self-context to detect head nods in fully natural conversations where both protagonists freely interact. we create an annotated dataset of natural dyadic interactions in order to benchmark our method. this paper is structured as follows. in section 2, we present the dataset on which our method was trained and tested. in section 3, we explain our method to automatically detect natural head nods. we discuss our results in section 4 and conclude in section 5."
"the density of the finished part depends on gaps between the sintered material generated during manufacture. these can be reduced by adjusting peak laser power, scan speed, hatch spacing, particle size and chamber pressure. reported densities for slm vary depending on the geometry and complexity of the finished piece and can range from 80 to 96% of the theoretical density [cit] ."
"the reproducibility of collimator production has not been assessed in this study. for 0.5-mm-thick disks, produced using scan strategy d, the coefficient of variation of disk thickness was 2.7% and it is therefore possible that a similar variation in collimator septa could also be present (although no image distortion or significant variation in image resolution was observed). it is also possible that collimator production may vary between print batches. to assess batch reproducibility, the performance characteristics of identical collimators manufactured at different times would have to be analysed. in addition, further variation may be observed across different slm machines and it is therefore advisable that adequate acceptance tests are performed on all novel collimator designs produced using slm. in addition, a thorough quality control procedure should be performed if this technology is used for mass production. imaging application of slm collimators will likely be most suited to high-energy isotopes, such as i-131 where thicker septa would be less affected by surface roughness."
"production accuracy was assessed against micrometre measurements of the central disk section using a digital display micrometre (mitutoyo, 293-766-50) with a product fig. 1 photograph of slm disk and schematic diagram of disk dimensions measurement accuracy of ± 2 μm. for each disk set, different scan strategies were investigated, and summarised in table 1 ."
"plots of the normalised count rate (intensity) as a function of total lead disk thickness measured at 140 kev and 364 kev are shown on log-linear plots in fig. 4 . the linear attenuation coefficient is measured as the gradient of the fits to the data and are summarised in table 5 with comparison to the theoretical ideal (solid line). for each energy, the measurement uncertainty range fell within the expected theoretical range and therefore validated the measurement technique. the linear attenuation coefficients of lead measured at 140 kev and 364 kev are summarised in table 5 ."
"an informative and non-ambiguous bba should have a high degree of specificity and a low degree of entropy. to illustrate these notions, consider the following pedagogical examples with the considered frame of discernment for a given cell:"
"similarly, the si of the phantom remained virtually the same for tuned and detuned markers at 42.31 ± 1.74 and 44.19 ± 3.90, respectively. figure 7a were collected sequentially and depict the alternate tuning and detuning of markers #3 and #4 during the activation of dof-3, and the graph in figure 7b shows the si projections in the same positions. the average si (a.u.)"
"when applied to intelligent vehicle applications, udisparity maps are widely used to detect obstacles [cit] . indeed, with upright obstacle pixel accumulation on the same disparity value, one is able to distinguish obstacles from ground. our approach is similar to this idea. the occupied mass in the u-grid comes from the obstacle pixel accumulation. figure 2 shows the u-disparity computed from stereo-vision images at the bottom. one can notice that the space occupied by upright obstacles contributes to bright cells in the u-disparity map. in our approach, we compute a refined obstacle u-disparity map to compute the bbas for the u-grid."
"when interpreting sensor data into occupancy, uncertainties inevitably arise because of unperceived space and sensors measurements errors. to tackle these problems, bayesian methods are the foundations of usual frameworks. the information can be transformed into probability to take into account uncertainties. the above cited works are based on bayesian theory except [cit], in which the authors proposed to use the theory of dempster-shafer [cit] to handle uncertainties and the lack of knowledge in new perceived cells. this is the core of an evidential grid."
"in order to better analyze the performance of the stereovision evidential mode, we report here a quantitative comparison with the classical lidar approach. in the theory of dempster-shafer, one can evaluate the bbas using speci- [cit] . the entropy of a mass function is defined as:"
"the paper is organized as follows: section ii presents the evidential theory foundations. section iii details the proposed evidential sensor model for the stereo-vision. section iv shows experimental results based on real road data carried out on two different datasets with different stereo-vision systems. entropy and specificity are used to compare the information management of the stereo-vision mapping with a classical lidar approach. finally, conclusions are given is section v."
"as the inputs in our case are continuous-valued and not limited to a certain range, the training process of dbn by greedy layer-wise scheme from lower layers to higher layers is demonstrated in figure 2 . the bottom rbm layer is selected as gaussian units and the remaining hidden layer above this use binary units. in details, gaussian rbm ℎ 1 is trained firstly and then taken as the inputs of rbm ℎ 2 . next, the training of ℎ 2 can be accomplished in the same manner. the output of each rmb is the extracting feature of previous output by maximizing its probability. in other words, the high level rmb represents the most representative feature of input data, and the low level rmb is the low-level extraction of input data. it is noted that, during the training process, there is no target variable involved, which is denoted as unsupervised pre-training."
"a larger number of fiducial markers are needed to track an articulated manipulator such as a large portion of a robotic arm, or a bendable tool such as a catheter."
"eight process variables are selected as model inputs and they are inlet gas flow rate, inlet gas pressure, inlet gas temperature, co2 concentration in inlet gas, mea circulation rate, lean solvent flow rate, lean solvent temperature and reboiler duty, which influence the capture efficiency and process performance [cit] ."
"our aim is to build a consistent environmental model based on two complementary sources of information. the two sources of information are both reliable, thus allowing the use of the dempster's rule. the fusion process is shown in equation 3 . for denotation purpose, let m u and m v represent respectively the mass functions of u-grid and v-grid at time t. the resultant stereo-grid is denoted as m stereo ."
"the models for co2 capture processes can be categorized into three groups: mechanistic, statistic and artificial intelligence based models. [cit] have developed a mechanistic model to present how disturbances affect the carbon capture process performance. in their study, the mechanistic model is able to predict the column temperature profile and co2 loading with high accuracy. on the other side, it consumes a lot of time consuming and also requires extensive knowledge of the process underlying physics. further, [cit] have proposed a statistical model to instead of mechanistic model, for predicting heat duty, co2 production rate, co2 lean loading and capture level. in their statistical analysis, the selection of process variables in the model was affected by experts' opinion. however, the statistical model cannot describe the irregular non-linear relationships between process variables. to tackle these problems, they explored a neural network model later and then compared its performance with the previous statistical model [cit] . from that research, they found using neural network model was able to predict co2 production rate with much higher accuracy than the statistical model."
"unsupervised pre-training of dbn is important to improve the model performance. it was interpreted by bengio [cit] as follows: injecting unsupervised training may help to put the parameters of that layer towards the better direction in the parameter space. a greedy layerwised training algorithm was proposed to train each layer at one time [cit] . specifically, start to learn from the lowest weight matrices and keep all the higher weight matrices tied. in this work, rbm is used to pre-train each layers of dbn networks to lead the initial weights to optimum solution. after the unsupervised steps of dbn are finished, the supervised fine-tuning by back-propagation method is conducted to modify the weights between each different layers."
an amount of 1076 data samples were collected from the gproms based simulator at university of hull. the nonlinear dynamic models in this study are developed as following form:
"the one-step-ahead prediction performance comparison of slnn and dbn models on batch 1 is shown in figure 6 . the predictions shown as red dashed curves are almost identical to the true values shown as blue solid curves in both plots. however, slightly large prediction errors are seen clearly in the top plot when there are step changes in inputs. this demonstrates that the dbn is able to catch the underlying feature of the data and represent the dynamics of process accurately. in details, the rmse values of the dbn and slnn models are 0.0259% and 0.0312% respectively, and the former one is little lower than the latter one. figure nevertheless, it still exits some problems. for instance, the training of dbn procedure requires much more time than slnn. as well, the modelling parameters is expected to be adjusted for the further results improvement."
"we report results for a 20 seconds data sequence carried out with the carmen vehicle and calculate the average specificity and entropy for every evidential grid. for comparison purpose, stereo-vision and lidar grids were constructed at the same time index. in figure 8 and figure 9, specificity and entropy are displayed. the green and blue points correspond to the stereo-grid and lidar-grid respectively. in general, the stereo-grid has larger specificity than the lidar-grid which means that the masses are more concentrated on nonvacuous propositions in the stereo-grid than in the lidargrid. thus, less uncertainty about the environment exist in the stereo-grid. in other words, the environment is better characterized. this can be explained by the fact that the stereo images contain more information than the sparse lidar scans. meanwhile, the stereo-grid also has larger entropy than the lidar-grid which means the mass is more dissonant. this is due to the fact that the stereo-grid originates from the fusion of two complementary grids which may contribute to contradictory information in some cells. it is interesting to notice that the entropy remains very low which indicates a good modeling of the u and v disparity grids. the lidar has almost no entropy because the evidential sensor model has been tailored to avoid this issue. in this work, an evidential sensor model has been developed for a stereo-vision camera based on disparity maps. the model makes full use of the disparity space and yields two evidential occupancy grids which provide complementary information about the environment. the u-grid from the u-disparity map contains all information about obstacles in the scene and the v-grid from v-disparity map models the free space information. the fusion of these two grids offers a complete model of the environment. experimental results based on real road data with different systems shows that the proposed evidential occupancy grid model can correctly represent the environment, and a quantitative comparison with occupancy grids from lidar data indicates that the stereo grids are more informative and less ambiguous than lidar grids. the main perspectives of this research consist in studying the influence of the parameters used in the stereovision model and the information fusion between the stereovision and the lidar."
the algorithm models the ground confidence of the pixels based on the pixels' disparity distance from expectation. figure 3 shows the ground plane confidence map projected onto the original left image. the green level in the map illustrates the confidence degree for each pixel. only pixels with high confidence are displayed.
"the hidden layer of slnn contains 20 hidden neurons, which is also determined by considering a range of hidden neuron numbers and inspecting the performance on the testing data. as to predicting co2 production rate, the root mean squared errors (rmse) values on training, validation and testing data are given in table 1 . it clearly shows that dbn model gives much lower rmse values than slnns model. as a result, dbn model has ability to model the co2 capture process more accurately than the slnn model. this is because dbn can extract the data characteristics by unsupervised learning, thereby accelerate the learning convergence and avoid local minimum. to further prove this point, the data of batch 1 is used to verify the dbn model. figure 4 compares the one-step-ahead prediction performance on co2 production rate by slnn model (top) and dbn model (bottom the rmse values of co2 capture level prediction by dbn and slnns models are given in"
"amongst the various technologies of ccs, the post-combustion carbon capture technology with chemical absorption has been considered as the most suitable way to reduce co2 emission. this is because it can retrofit the existing coal-fired power plant easily and treat flue gas stream with low co2 partial pressure [cit] . however, it still has some disadvantages, one of which is the large energy requirement for absorbent regeneration. the thermal energy for regeneration usually comes from extracted steam from the low pressure steam turbines, which will reduce the efficiency of the coal-fired power plant. therefore, it is particularly important to find out the trade-off between co2 capture level and energy consumption by using process optimisation. in order to carry out process optimisation, it is necessary to develop an accurate model for the post-combustion carbon capture process."
"where is learning rate. in eq(6), the calculation of the gradient (ʟ ) is particularly important. to better understand this, the gradient of likelihood function at a single data point v is calculated as:"
"prior to building the models, the data should be pre-processed to avoid missing values and outliers. as the variables have different physical units and magnitudes, each variable should be scaled to zero mean and unit variance. in developing the dbn models, all the input data is used for the unsupervised training process to extract their feature, which is stated in section 2.3."
"in figure 7, we show a global map constructed by adopting a different stereo-vision system in a private test site of the university with the robotized zoe moving at 20 km/h. the the blue points in the center of the free space represents the host vehicle's trajectory during the data collection. one can notice that the map is correctly built particularly in the roundabout."
"in this study, the performance of dnb modelling technique is compared with traditional neural network modelling technique, namely, slnns. as mentioned above, the structure of 2 hidden layer is determined for dbn, in which the bottom hidden layer is gaussian rbm and top hidden layer is binary rbm. the numbers of neurons in these two hidden layers are 20 and 17"
"where ai and are mean and standard deviation respectively of the gaussian distribution for visible unit i, v is the continuous valued input layer, and h is the binary layer."
"the authors are with 1 sorbonne universités, 2 have chosen evidential theory because the proposed sensor model interprets the disparity space of two sources of complementary information: the u-disparity map serves to model the obstacle information, whereas the v-disparity map permits to model the free space information. moreover, the framework of dempster-shafer theory offers powerful tools to merge different sources of complementary information. in this work, we make the hypothesis that the host vehicle runs on a flat road. the road surface is then considered as free space whereas any obstacles is considered as an occupied space."
"however, the amount of co2 emission per unit of electricity released by coal-fired power plants is twice as much as their natural gas counterparts [cit] . as a result, many researches have been explored to reduce the co2 gas emission from coal-fired power plants. carbon capture and sequestration (ccs) is identified as an appropriate technique for the sustainability of coalfired power plant, because of its efficiency and effectiveness in reducing co2 emission [cit] ."
", given the visible unit v, which is easy to compute. however, the computation of the positive term, which is expectation of (ṽ,ℎ) for joint distribution (ṽ, ℎ), is an intractable problem. it is causally linked to (2 + ℎ ) items in v,h, giving rise to computation complexity of ( + ℎ ). therefore, gibbs markov chain on the pair of variables is usually considered to resolve the problem. however, it is still intricate, because a large quantity of frequency samples is always required to guarantee the precision."
"the actuation-to-marker correspondence is stored in a two-column text the manipulator-driven marker selection was tested using the arm of a previously described mr-compatible manipulator. the algorithm of figure 1b was implemented for the particular kinematic structure of this robotic arm, and its operation is illustrated in figures 3d-h . to monitor the motion of a particular dof, the following subsets of markers were assigned per dof: markers #1 and #2 for dof-1, markers #3 and #4 for dof-2 and markers #3 and #4 for dof-3. when dof-1, −2 and −3 were actuated, the markerstates were 1100, 0011 and 0011, respectively."
"entropy characterizes inconsistency in the distribution of the masses. non conflict information results to zero entropy in m 1, while in m 3, as there exists conflicting information between free and occupied, the entropy is large. specificity characterizes degree of dispersion. the specificity is larger if the mass distribution is less doubtful, thus in m 1 and m 3 the specificity is large."
"of these markers relative to the coordinate system of the mr scanner can be measured with one-dimensional (1d) projections as established by dumoulin 1 and others 6, 7 or by two-dimensional imaging. 8 the number of fiducial markers needed to track a tool increases with the complexity of its articulation and/or its threedimensional space. a single fiducial marker can track a single point (e.g. the tip of a tool) and two markers are needed to measure the orientation and position of a straight tool or a segment of a tool."
"this rule accomplishes the fusion with a normalization process, which distributes the belief from the conflict to free and occupied, according to their respective mass. the complementary u-grid and v-grid can be conflicting in some cases. the normalization process of the fusion strengthens the highest belief in the result, which is exactly the desired operation."
"as mentioned above, for a motion involving more than one dof, the code assigned markerstates calculated from the bitwise or operator, e.g. when dof-1 and -2 were actuated together, then markerstates was set to 1111."
"the prototype setup described was designed primarily for the initial registration and tracking of the end-effector of an articulated manipulator. 13, 14 however, it is straightforward to modify the hardware and software for use with virtually any number of markers placed at any position on an articulated or bendable tool. the ability to activate markers individually, or in groups if necessary, addresses another issue, i.e. the potential spatial overlapping of signal in a case where the semi-active markers need to be placed close to each other or the tool is subject to high degrees of bending or articulation."
"as can be seen from the process details, there are a huge number of variables in the postcombustion co2 capture process. however, in this study, only two quality variables, co2 capture level and co2 production rate, are considered as the main indicators of the process performance. capture level is the amount of co2 extracted from the inlet flue gas in the absorber. it is calculated as follows: and ɛ represent co2 mass fraction in gas out of absorber, gas flow rate out of absorber, co2 mass fraction in inlet flow gas of absorber, and inlet gas flow rate of absorber, respectively."
"benchtop studies included assessing the detuning of the rf coils it is noted that, if ϕ ≠ 0 (i.e. not horizontal) then there are two options for plane assignment that prescribe (i) an oblique plane, i.e. an axial one that is rotated around the x axis to be orthogonal to the l-2 link and include both markers, or (ii) an axial thick slab to include both markers #3 and #4. in the latter case, the measured distance between markers #3 and #4 will be the projection of δ onto the axial plane (xy) and thus must be adjusted for inclination ϕ."
"global climate change, as a result of the accelerated build-up of greenhouse gas (ghg) emission in atmosphere, has become a key concern of our society. the main component of ghg gas is carbon dioxide (co2). in the past few decades, numerous climate change policies were launched, but nonetheless, annual ghg emission still increased by 1.0 gtco2-eq (2.2%)"
"in this paper, we propose a fitted evidential sensor model based on the disparity space to interpret the stereo-vision information onto a 2-dimensional occupancy grid map. we"
"many researches have shown that dbn can produce models with higher accuracy and precision, especially with respect to image recognition [cit], time series forecasting [cit], and robotics [cit] . in this paper, dbns are used to model a post-combustion co2 capture process. undirected connections to a layer of vision layer. theoretically, it is a special type of generative energy based model which can learn probability distribution over its inputs. as there are no connections between hidden units in rbms, it has an advantage that the hidden unit is conditionally independent to each other. both visible units (v) and hidden units (h) are stochastic binary variable nodes and hypothesis that the joint probability distribution of (v, h) fits boltzmann distribution. v is connected to h through undirected weighted connections. the reason why they are restricted is that, there is no connection between hidden variables or visible variables. a probability distribution p(v,h) is defined via an energy function, e(v,h; ), which can be written as:"
"respectively, which are determined by considering a range of hidden neuron numbers and inspecting the performance on the testing data. the learning rates for both unsupervised training and supervised training for dbn are selected as 0.1 to avoid low learning speed and local optimisation. slnns are trained by the levenberg-marquardt optimisation training algorithm with regularisation and cross validation based \"early stopping\". the regularisation parameter is set as 0.1 as the data are generated from simulation and do not contain much noise."
"where y is the process output variables (co2 capture level and co2 production rate), u represents the process input variables mentioned above, t is discrete time, and f[ ] is the nonlinear function represented by the neural network."
"however, the rmbs with binary nodes can only deal with discrete inputs. when inputs are continues values, gaussian rbms are suitable to apply [cit] as shown in eq(4)."
"then, the data samples are randomly split into three sets: training data (64%), test data (16%) and validation data (20%). to evaluate the dynamic model performance, the data of batch 1 are used to further evaluate model prediction performance. accordingly, two dbn models are constructed for the quality predictions of co2 production rate and co2 capture level. crossvalidation is used to select the network architecture and both models are found to have the structure of 26-20-17-1. that is the dbn has 26 input nodes, 20 hidden nodes in the first hidden layer, 17 hidden nodes in the second hidden layer, and 1 output layer node. as the neural network learning depends on random initial weights, it is necessary to repeat the training procedure for several times and the result with least training error is selected. in this study, the training procedure is repeated for 20 times."
"a rapidly growing population plus industrialization, with corresponding increase in energy demand, is likely to result in increasing amount of ghg emission. [cit], a 50% reduction of co2 [cit] ."
"in this study, authors are proposing a ready-to-use generic performance management model -that meets the needs of all types of organizations; regardless of their size, industry, strategy or culture-that encompasses pivotal performance management characteristics, among which organizations should consider during their performance management processes. however, the methodology provided for use allows customized modifications based on strategical and cultural aspects of the organization."
"organizations of various size, industry, age, or structure have all strategy to pursue, cultural dimension where employees create certain value, and these strategic and contextual factors are tied to the objectivity and functionality of the system. these criteria have been drawn through a brainstorming process within a focus group study, together with a total of five professional and academic experts in the field. all the attributes and sub attributes explained below were drawn at that session. these criteria were confirmed through some one-to-one talk and consultations, where only later the theoretical and academic basis was researched. these confirmed criteria are strategic congruence, contextual congruence, objective implementation, and functionality; which each has some sub attributes that is assumed to constitute an effective performance management system as illustrated in figure 1 and explained below."
"the control law is designed for r 1 and r 2, similar to the design for the other robots of a multi-robot formation. changing the form of the state equation in eq (28), the reduced statespace kinematic model of the multi-robot formation is equivalent to eqs (33) and (34) is obtained by taking the time derivative of a 1 dy 1 à y 2,"
"as aforementioned, the appraisal is only a part of the performance management system. as a first step, managers need to define the aspects of performance, through determining the individual goals in relation to job analysis. only later, based on clear indicators, managers may implement some standardized evaluation methods that best suit to the specific unit needs. finally managers deliver feedback to the appraisees. feedback is the pivotal aspect that differentiates performance management from performance appraisal, as it leads the process to a next level for advancement and recognition. first of all, the evaluation results facilitate identification of the problems to performance, predicting the areas necessary for training, development, motivation, or modifications; serving to the advancement purpose. second, the feedback delivers concrete indicators for rewarding, which may result as increase in pay or incentives, promotion, and career opportunities like succession planning; where it serves the administrative purpose."
"the rest of this paper is organized as follows. in section ii, the leader robot localization problem based on the bearing-only observation is presented. section iii presents leader-follower formation control for multi-robots based on the bearing-only ukf algorithm and input-output feedback control. simulation results are given in section iv, and in section v, we offer our conclusions."
"in order to test the differences between the proposed gmbo and the other five methods, wilcoxon's rank-sum tests with the 5% significance level were used. table 11 records the results of rank-sum tests for kp1-kp15. in table 11, \"1\" indicates that gmbo outperforms other methods at 95% confidence. conversely, \"-1\". particularly, \"0\" represents that the two compared methods possess similar performance. the last three rows summarized the times that gmbo performed better than, similar to, and worse than the corresponding algorithm among 50 runs. table 14 shows the statistical results of the six methods based on the worst values. the ranking order of the six methods was gmbo (1.20), mbo (2.07), abc (2.60), cs (3.93), de (5), and ga (6), which was identical with that in table 12 ."
"the operational activities of the health sector involved in the demand, acceptance, and distribution of medicines such as puskesmas, pharmaceutical installations, and health department have conducted supply chain practices. maximizing supply chain practices is done by integrating all existing information systems. the integrated information system will help them to access the pharmacy distribution and distribution division. this allows pharmaceutical supply planning and the fulfillment of clinical needs to be met on demand at the right time. systems in each sector have been integrated. people can access information related to inventory and distribution, establish cooperation with outsiders, and enable communication with other institutions."
"network densification through employing a large number of distributed small cells (scs) is a promising approach to satisfy the required data rates of future wireless applications [cit] . while the macro base-stations (bss) serve highly mobile users and extend cell coverage area, scs can potentially provide local capacity enhancements. however, intercell interference is a dominant limiting factor and can potentially exacerbate the overall network performance in a dense two-tier architecture. although coordinating all distributed antennas within a cell region via forming a very large virtual antenna array achieves an interference-free operating point, it incurs high data-rate backhaul communications and a prohibitive overhead for channel training. therefore, devising distributed interference management schemes based on clustering and exploiting local information is desirable."
"system development can be a control system to monitor the ups and downs of disease patterns in each clinic. it can also be integrated with other units. additionally, this technology offers a new way of making healthcare policy. the implementation of this technology will be constrained by the availability of a good internet network and limited human resources who will manage the system. this solution will also need support from the government. this system can also be developed by involving biomedical, so it collaborates with the fields of engineering, science, and complete technology."
"a system that is considered to serve strategic needs ought to be flexible, and be open to modifications and corrections [cit] . such a principle enables optimum evaluation opportunity for different situations; meanwhile it ensures constant improvement of the process. therefore, a system open to corrections, especially at challenging times of unjust decisions, may be considered to be highly functional."
where function exprnd(x) returns random numbers of an exponential distribution with mean x and ceil(x) gets a value to the nearest integer greater than or equal to x. maxgen is the maximum number of iterations. the parameter ω is the weighting factor which has inverse proportional relationship to the current generation
"as we can see from table 3 and figure 3, p is the most important parameter and needs a reasonable selection for the 0-1 kp problems. a small p signifies more elements from subpopulation_2. conversely, more elements were selected from subpopulation_1. for peri, the curve was in a small range in an upward trend. this implied individual elements from subpopulation_2 had more chance to embody in the newly generated monarch butterfly. for bar and smax, it can be seen from figure 3 that the effect on the algorithm was not obvious."
"the necessary lie derivatives of h k (s, z) and the corresponding gradients are computed, and the observability matrix, m, is obtained. in the first case, the observability condition that a robot observes a landmark is analyzed."
"upon communicating the project with the selected experts, at first hand the individual rating and aggregation was carried. the matrices for individual ratings have been digitally sent to all three experts. each independently filled in the matrices. later, the study leader digitally collected all ratings and aggregated them through combining them into decision matrices. the results of the first phase have been analysed and interpreted below in section 3.1."
"proof: if the robot does not move along the line that joins the robot and the landmark, the m ik 1 matrix in eq (17) can be transformed into the simplified form shown in eq (18) using a finite sequence of elementary row operations."
"in this section, the procedure of gmbo for 0-1 kp is described in algorithm 6, and the flowchart is illustrated in figure 2 . apart from the initialization, it is divided into three main processes."
"as mentioned earlier, joint processing within each cluster can be executed in order to cancel intra-cluster interference. therefore, it is favorable to group those scas which potentially impose the strongest interference on ues of each other, i.e., adjacent scas, in order to remove the dominant interfering sources. this section explores two different sc clustering schemes, namely, k-means and ap-clustering, and compares their performance and complexity against the previously proposed sp-clustering scheme [cit] ."
"the introduction of a global position operator coupled with an efficient two-stage repairing operator is instrumental towards the superior performance of gmbo. however, there is room for further enhancing the performance of gmbo. firstly, the hybridization of the two methods complementing each other is becoming more and more popular, such as the hybridization of hs with cs [cit] . combining mbo with other methods could indeed be very promising and hence worth experimentation. secondly, in the present work, three groups of high-dimensional 0-1 kp instances were selected. in the future, a multidimensional knapsack problem, quadratic knapsack problem, knapsack sharing problem, and randomized time-varying knapsack problem can be considered to investigate the performance of mbo. thirdly, some typical combinatorial optimization problems, such as job scheduling problems [cit], feature selection [cit], and classification [cit], deserve serious investigation and discussion. for these challenging engineering problems, the key issue is how to encode and process constraints. the application of mbo for these problems is another interesting research area. finally, perturb [cit], ensemble [cit], learning mechanisms [cit], or information feedback mechanisms [cit] can be effectively combined with mbo to improve performance."
"wherein r c i,k and r b k are exponentially weighted average rates of ue k associated with cluster i and ue k associated with the macro bs, respectively. let k i c and k b be the number of scheduled users at cluster i and the macro bs in order. assuming downlink zfbf, the precoding matrix of cluster i and the macro bs are given, respectively, as"
"where i c ik and i b k denote the total interference power experienced by ue k associated with the i th cluster and the macro bs, respectively. this information can be measured at each ue and fed back to its corresponding transmitter."
"the optimization problem formulated in (3)- (6) is, in general, non-convex. this paper proposes a hierarchical approach to obtain a local solution in an efficient manner."
"supply chain technology that only adopts business processes is not a good strategy. it cannot create benefits or influence on the inter-firm relationship. it ultimately does not show improvement of company performance [cit] . this situation demands the new creation of various models and designs to support supply chain networks [cit] for example, it can be visibility process and global flow control support for intelligent transactions that provide realtime business process information, demand planning and distribution scheduling, a collaboration of activities, and information synchronization [cit] ."
"here function rand (a, b) returned a random integer uniformly distributed in [a, b] . for each instance, the maximum capacity of the knapsack equaled 0.75 times of the total weights. the procedure is as follows:"
"operational activities of pharmaceutical warehouses are procurement, reception, storage, distribution, and recalling [cit] . the pharmaceutical warehouse should ideally perform drug stock calculations, and provide information on the amount of stock regarding available drugs in real time [cit] . in addition, collaboration, coordination, and integration among entities must be met for work efficiency [cit] . information technology (it) can be the solution to help the business process in the pharmaceutical warehouse to become simpler, integrated, automated and optimize health services [cit] ."
"the proposed method includes two different approaches (ie. compromised preferences, and aggregated preferences) for multi-expert decision making. the steps of those approaches are given below:"
"with regard to the mean values, they were very similar to the best values. the improvements in kp1-kp15 were 1.51%, -0.02%, 1.15%, 2.16%, -0.09%, 0.77%, 0.83%, 1.37%, 0.96%, -0.24%, 0.11%, 0.09%, 0.07%, 0.00% and 0.00%, respectively."
"there are several points to explain. firstly, all 5 comparative methods (not including ga) used the previously mentioned dichotomy encoding mechanism. secondly, all 6 comparative methods used gmo and goo to carry out the additional repairing and optimization operations. thirdly, abc, cs, de, ga, mbo, and gmbo were short for 6 methods based on binary, respectively."
all of the simulation results indicate stable performance of the proposed formation control solution and a quick response to changes as long as the leader robot system is observable.
"an effective performance management model requires to be strategically congruent. performance management is the backbone in assuring organization's strategies and goals. in that sense, when modeling a performance system congruent with organization's strategies, (1) alignment to the organizational goals, (2) organizational support, and (3) the role of the executive, are the three main important aspects to establish."
"a simple greedy strategy, namely gs [cit], is proposed to choose the item with the greatest density pi/wi first. although the feasibility of all individuals can be guaranteed, it is obvious that there are several imperfections. firstly, for a feasible individual, there is a possibility that the corresponding objective function value may turn to be worse by applying gs. secondly, the lack of further optimization for all individuals can lead to unsatisfactory solutions."
"the main contributions of this paper are as follow. building upon the work [cit], we propose two sc clustering schemes based on k-means and ap-clustering algorithms in a heterogeneous network. the proposed similarity measure is merely a function of pairwise distances between scas. hence, the cluster formation process is not susceptible to instantaneous channel variations and is stable over long time scales. due to the small number of scas within each cluster, exchanging ues' data symbols within each group and acquiring channel state information are feasible. hence, the intra-cluster interference can be completely cancelled by designing downlink zero-forcing beamforming (zfbf) vectors. furthermore, using the idea of interference pricing power control [cit], we devise a simple inter-cluster coordination scheme which only relies on exchanging simple low-rate messages amongst cluster representatives. we show that sc clustering in heterogeneous networks strikes a balance between the network-wide performance and required backhaul capacity. in particular, clustering renders the coordinated transmission within each cluster viable and the proposed power control scheme provides significant performance gains. our simulation results show that the proposed approach outperforms conventional percell processing. moreover, it enables partial interference fig. 1 . a cellular network overlaid with two scas. solid arrows denote the direct channels and dashed arrows denote the interfering channels."
the proposed model can take different experts' intuitionistic assessments into account in both aggregated and compromised assessments. thus the model has the ability to consider both vagueness and diversity in human reasoning. it is applicable even if you do not have any tangible attributes. it can be used when all the evaluation data are linguistic. an ms excel work-sheet easily allows you to formulate the whole process.
"a system that aims at serving the strategic, developmental and administrative goals should by all means be constructed on measurable aspects. performance systems tend to involve biased errors because of the spontaneity and subjectivity aspects. furthermore, they are observed to lack analytic capability and are insufficient to carry out predictive, measurable modeling. moreover, measurement tools based on quantifiable indicators deliver results with meaningful, interpretable and usable information."
"the three experts were then invited to a series of meetings where they mutually decided through a discussion session of evaluating each criterion again by pair-wise comparisons. to eliminate any loss of creativity, prior to mutual agreement sessions, experts were encouraged with enough time to have their individual analysis on the criteria. meanwhile, the expert selection was on the basis of a mutually spoken language, so as to overcome any communication barriers, and to ensure that all discussions are clear for all. furthermore, a moderator facilitated the sessions, so as to balance any dominant member influencing the group decision. later, the study leaders analysed the data and interpreted as explained below in section 3.2."
"management in the company begins to aware of the need to automate tasks and control inventory through the benefits of it implementation [cit] . the practice of scm in pharmaceutical warehouses can be automated using it. this means that only companies that have managed their supply chain with good it are companies with the right strategy to run a business operation. it also has an opportunity to improve performance [cit] . the illustration of it is shown in figure 3 . it is one of the programming models that can be applied to supply chain model. this model shows that in every process, there is a buffer on each stage. this model is the order of the overall process of the supply chain. then, the buffer is the output of the results of one process. it will be the input of the next process [cit] ."
"in the literature, there are two different methods to deal with multi-expert ahp problems. the first is mathematical aggregation of individual ratings, whereas the second is agreement of experts on a compromised rating through discussion sessions. if the former is the case, separate matrices are aggregated by an aggregation operator for interval-valued intuitionistic fuzzy sets. on the other hand, the latter is generally preferred when there is an easy communication and discussion medium among the decision-makers. in most studies, regardless of any reason stated, any one of these two methods is used and the decisions made. however, in cases where they yield different results, counter to random selection, it is advisable to analyze which method is most adequate to the situation at hand and only then to select the method. therefore, as an additional contribution of this study, results from both methods have been obtained and analyzed to determine whether there is a significant variation."
"performance management system has a broad perspective, covering organizational dimensions, as well as the individual ones; assisting human resource functions for organizational and individual goal achievement [cit] ."
"this section presents the results of the simulation conducted to validate the observability conditions of the leader robot system and the leader-follower formation control discussed in the previous section. for the leader robot localization system, the simulation environment consists of one leader robot and two landmarks. for the leader-follower formation system, the simulation environment consists of one leader robot and three follower robots, where the followers (r 2, r 3, r 4 ) follow the leader (r 1 ). to demonstrate the validity of the proposed formation control approach, simulations are designed using both webots 7 and matlab. the simulation scenario given in webots 7 is shown in fig 4. the leader robot system performance and the leader-follower formation system performance are given in figs 5 and 6, respectively. in tables 1 and 2, the algorithmic (eif versus pf) performances are given."
"the research phase involves industries that have been using supply chain-based integration systems. this is intended as a comparison of integration systems built with systems that have successfully expanded the company's performance. the results of research on industry become the basis for creating a prototype of systems integration in the health sector. the researchers involve pharmacists for functional system design. the pharmacists will examine the need for drug usage data, streamline data entry, and system of flow determination process. for the current process path of the pharmaceutical warehouse, pedagang besar farmasi (pbf -pharmaceutical wholesalers) will meet the needs or drugs in pharmaceutical warehouses and clinics. this fulfillment will always be coordinated to health officer as the responsible person. the next operational activity will be an scm practice where puskesmas, pharmacy installation, and health office will mutually collaborate. the activity begins when clinics submit lplpo to the pharmaceutical warehouse. this submission will be verified by health department first. the verification is the process of matching between request submitted with previous stock and usage. the verification results will be submitted to the pharmaceutical warehouse to determine the distribution schedule. the distribution schedule can be known immediately by clinics and health department. this manual process will be the basis for determining the model of computerized systems [cit] ."
"it plays a critical role in scm practice in the company [cit] . it is because it can record all transaction activities for clarification or addition of requests. it can create a good communication between the parties involved in scm [cit] . it can minimize information distortion and create strong interaction between agencies or work units involved in scm, shorter cycle times, effective information flow and waste reduction [cit] . it can reduce operational costs and improve performance such as the improvement of health clinic services by shortening lead time and availability of data [cit] ."
"multi-robot formation control has long been a topic of interest in both academic research and industrial applications. furthermore, the advantages of multi-robot systems over a single robot include greater flexibility, adaptability and robustness [cit] . based on these characteristics, typical applications for multi-robot formations include underwater or outer space exploration, shop floor transportation, guarding, escorting, airport snow shoveling, surveillance and patrolling missions [cit] . however, multi-robot formation control is challenging, especially when the observation information is poor and the system is highly nonlinear. a variety of formation control methods have been proposed, such as the virtual structure approach [cit], the behavior-based approach [cit], the leader-follower approach [cit], the artificial potential approach [cit] and the graph theory approach [cit] . among these approaches, the leader-follower approach has been the most widely used method owing to its simplicity, scalability and reliability. however, the majority of existing leader-follower approaches require the distanceangle information or more information [cit] ."
the remainder of the stock (h) can only be determined by subtracting the supply (f) by the amount of use (g). equation (2) automatically provides information about the state of the drug stock in the warehouse.
"a pharmaceutical warehouse is a place that serves the needs of health clinic medicine, work area on inventory and distribution planning, and demand scheduling [cit] . it is a supply chain management (scm) in the health sector by involving multiple entities/work units [cit] . each activity will be coordinated by the district health office [cit] . the minister of health and social welfare of the republic of indonesian through policies and strategies for the development of sistem informasi kesehatan nasional (siknas -national health information system) has sought to collect the coordinated data. it can be the integration of existing health systems such as assignment, responsibility, and interconnected mechanisms [cit] ). however, the need for data integration has not been met. this is reinforced by the health sector difficulties in making the right decision because of the limitations or unavailability of accurate, precise, and fast data and information. these conditions indicate the problem of health information system in strengthening the improvement and efforts through data integration [cit] ."
"(2) in the butterfly adjusting process, partial genes of the global best individual are passed on to the next generation. moreover, lévy flights come into play owing to longer step length in exploring the search space. this process can be considered as exploration, which may find new solutions in the unknown domain of the search space."
"assuming a fixed set of scheduled ues and downlink beam vectors, the final step of the algorithm is coordinated power allocation in order to maximize the weighted rate sum problem formulated in (3)-(6). however, this problem is non-convex and only local optimality can be guaranteed via karushkuhn-tucker (kkt) conditions. consider the objective function (11), the dual problem can be stated as"
"kp belongs to the category of discrete optimization. the solution space is a collection of discrete points rather than a contiguous area. for this reason, we should either redefine the evolutionary operation of mbo or directly apply a continuous algorithm to discrete problems. in this paper, we prefer the latter for its simplicity of operation, comprehensibility, and generality."
"in order to cancel intra-cluster interference, a linear zfbf is employed within each cluster. the zfbf precoding matrix of cluster i w c i and macro bs w b are given as"
"alignment to organizational goals (aog): when the system is strategically congruent, not only raters and ratees are more confident in the process of performance management, but all parts own the whole process. in essence, 'line of sight' theory [cit] - [cit] attempt to identify the linkages between firm's strategy, its human resources, and performance outcome, complements the required strategic congruence in the performance management system. thus, when considering an effective performance management system, it is highly essential to involve strategic aspects of the organization into the whole process."
"the main feature of particle swarm optimization (pso) is that the particle always tends to converge to two extreme positions viz. the best position ever found by itself and the global best position. inspired by the behavior of swarm intelligence of pso, a novel position updating operator was recently proposed and successfully embedded in hs for solving 0-1 kp [cit] . after that, the position updating operator combines with cs [cit] to deal with 0-1 kp."
"in this subsection, the time complexity of gmbo is simply estimated (algorithm 6). it is not hard to see that the time complexity of gmbo mainly hinges on steps 1-3. in"
"to restrict multi-robot formation errors, the system must be observable, and the estimation techniques must be used to solve the localization problem. estimation techniques include the unscented kalman filter (ukf) [cit], the extended information filter (eif) [cit], the particle filter (pf) [cit] and the extended kalman filter (ekf) [cit] ."
"objective implementation is one of the key characteristics of an efficient performance management system. a fair and just assessment that is bias free may be quite optimal; however, for efficient results, resolving objectivity issue is crucially important. according to p. prowse and j. [cit], removal of 'top-down' ratings and replacement with multiple-rater reporting -360-degree appraisal-will reduce the subjectivity and inequity of appraisal ratings, gender bias ratings, etc. an objective system is meant to be fair and just [cit] ."
"our approach in this paper can be applied to situations with n robots. this paper offers two main contributions. first, to achieve multi-robot formation control, a pf estimation algorithm is used to solve the leader robot localization problem. furthermore, we ensure that the leader robot system is completely observable if the leader robot can observe no less than two different landmarks. in this case, the trajectories of the leader robot for formation control are not required to be ideal. second, control of the leader-follower formation for multi-robots is studied based on the bearing-only ukf algorithm and input-output feedback control to achieve rapid multi-robot formation that can remain consistent."
"with reference to fig 2, the kinematic model of a formation with one leader and one follower in a polar coordinate system can be readily defined as shown in eq (28). the kinematic model of a formation with one leader and n followers can be defined as shown in eq (29). s n :"
"applicability (a): to define an effective performance management model, it needs to be an applicable and functional system which is accepted and used by all in the organization; if otherwise, the aforementioned negative attitude towards performance appraisal and management will be inevitable."
"in it on business operational, scm can help companies in completing the work. it can support business needs both internally and externally [cit] . it used in companies is more likely to maximize business processes. then, the most popular function is to provide information on stocks in which a customer has purchased or a notice to the customer about the remaining stock, delayed delivery order information, and inventory conditions. it should lead to a complex scm implementation [cit] ."
"in this section, in order to make a comprehensive investigation on the optimization ability of the proposed gmbo, test set 3, which included 5 uncorrelated, 5 weakly correlated, and 5 strongly correlated large-scale 0-1 kp instances, were considered. the experimental results are listed in tables 8-10 below. the best results on all the statistical criteria of each 0-1 kp instances, i.e., the best values, the mean values, the worst values, the standard deviation, and the approximation ratio, appear in bold. as noted earlier, opt and time represent the optimal value and time spending taken by the dp method, respectively."
"(1) in the migration process, the position of each monarch butterfly individual in subpopulation_1 is updated. we can view this process as exploitation by combining the properties of the currently known individuals in subpopulation_1 or subpopulation_2."
"the clustering problem is np-hard and both ap-clustering and k-means can only guarantee local optimality. the per iteration complexity of both schemes grows as o s 2 . moreover, as it is shown in section iv, the performance and required backhaul capacity of both schemes are essentially identical. however, there are some fundamental differences between these two schemes. in particular, k-means requires the number of clusters to be chosen a priori. however, there is no obvious way to characterize the effect of choosing different values of k on the ultimate performance of the proposed algorithm in advance. furthermore, k-means relies on the geographical locations of the scas. assuming that each sca has access to gps information, scas' positions need to be exchanged across the network and each sca joins the cluster to which it has the minimum distance. on the other hand, ap-clustering does not choose the number of clusters a priori. further, cluster sizes can be controlled by adjusting the self-similarity measures. moreover, similarity values should be exchanged amongst scas in a synchronized fashion."
"basically, the system is running as shown in figure 2 . it is the practice of scm in the field of health. scm practice starts from procurement of pharmaceutical warehouse stock and inventory to the distribution for the health clinic. the process of planning in the needs of drug and health supplies begins from the data of lplpo in public health clinic. this data is based on the type and amount of drugs according to the pattern of disease. then, it is submitted to district health office for verification and matching between stock, usage, and demand. further verification results are submitted to the regency pharmacy installation. it is compiled into a plan for the need of public drugs and health supplies in the regency. furthermore, in planning the stock and needs of the central and provincial health clinic, it will adjust to the needs of public drugs and health supplies in the regency/city and refer to daftar obat esensial nasional (doen -national list of essential medicines) [cit] . figure 2 is the requirement in the analysis phase. it will assist the system design. the file system based on the flow is the initial support of the system to computerize the manual process systematically [cit] . the utilization of it in the pharmacy field should involve pharmacists from designing and building information systems and technology to ensure the quality and safety. involving pharmacists can help to design functional systems by examining drug usage data, streamline data entry, and to reduce the risk of excessive drug stocks. however, in the end, the pharmacist's involvement will be constrained by a lack of understanding of how pharmacists can engage in it design [cit] ."
step 1. the compromised linguistic pairwise comparison matrices of the criteria given in figure 1 are obtained as in table 10 . step 2. the linguistic data are then converted to their corresponding interval-valued intuitionistic fuzzy sets using table 1 .
"based on the bearing-only observations, the nonlinear observability properties of the leader robot system and the leader-follower formation control are studied. when the leader robot system is observable, the leader-follower formation can be formed rapidly and then maintained. simulation results are presented to demonstrate that the proposed approach can efficiently control the desired formation of multi-robots. future research for multi-robot formation control should consider dynamic obstacles and formation transformation. funding acquisition: qh."
"where r c i,k is computed as in (7) using the generated random beams and assuming equal power allocation across individual clusters. furthermore, maximizing the proportional fair objective in (3) is equivalent to maximizing the weighted rate sum problem wherein weights are updated according to the time-averaged rates for each ue. hence, the objective function in (3) can be modified as"
"this paper investigates a cluster-based coordinated intercell interference management scheme for a downlink of heterogeneous networks wherein transmitters are equipped with multiple antennas. the proposed ap-clustering and kmeans cluster formation processes are merely functions of the network topology; hence, they are stable over long time scales. the key point here is that clustering facilitates partial interference cancellation in dense deployments in an efficient manner. in particular, ues' data symbols are merely shared amongst a small fraction of active scas. moreover, channel training is only required within each cluster. each cluster forms a virtual mimo network system which serves its associated ues via zero-forcing spatial multiplexing. moreover, a simple interference pricing power allocation is devised to reduce the intercell interference without exchanging ues' data symbols. the proposed clustering scheme concatenated with coordinated power allocation provides significant networkutility improvements compared to the conventional per-cell processing. furthermore, it requires significantly less backhaul communication and channel training overhead as compared to the global coordination scheme."
"ik is the beam vector assigned to ue k by cluster i and w b j is the beam vector assigned to ue j by the macro bs, p c i is the maximum available power across the antennas of cluster i and p b denotes the maximum available power at the macro bs. 2 moreover, the instantaneous achievable rates of ue k associated with cluster i and the macro bs are as follows"
"the rest of the paper is organized as follows. section 2 presents a snapshot of the original mbo, while section 3 introduces the gmbo for large-scale 0-1 kp in detail. section 4 reports the outcomes of a series of simulation experiments as well as to compare results. finally, the paper ends with section 5 after providing some conclusions, along with some directions for further work."
"for the sake of fairness, the population sizes of six methods are set to 50. the maximum run time is set to 8 seconds for 800, 1000, and 1200 dimensional instances but 10 seconds for 1500 [cit] dimensional instances. 50 independent runs are performed to achieve the experimental results."
"another highly essential dimension in effective performance management system is objectivity and fairness of the implementation process. most appraisal processes fail to succeed because of biased ratings, unintentional subjective conclusions, or simply lack of any standard scientific system. nonetheless, (1) an objective system requires to treat the whole process in fairness and justice; (2) the measurement tools need to be valid and open; (3) the implementation process needs to be such thorough that it is entirely owned by all members of the organization; and finally (4) the results need to be measurable, meaningful and usable."
"organizational support (os): [cit] are two additional essential aspects, which enable the performance management system to be strategically congruent. according to their argument [cit], \"managers are instructed to design goals for their subordinates that would simultaneously address their unit business goals (interest of the company) and enable expression of subordinates' strengths that would promote subordinates' well-being. \" this win-win principle, creates both a climate of organizational support over the employee, and also attains a supportive and positive role on the executive. role of the executive (re): furthermore, the executive is the first hand supporting agent within a supportive organization; hence the role and attitude of the executive is highly crucial in maintaining performance congruent with organization's strategy. managers should have the ability and skills not to use the performance assessment through 'orthodox' technique, but see it as a remedy to weakness and means for performance development [cit] . therefore, the role of the executive, the motivation and value he/she gives on the performance management process determines the effectiveness of the whole performance management system."
step 1. the linguistic pairwise comparison matrices of main and sub-criteria for each expert are collected as in table 6 . those matrices are converted to their corresponding intervalvalued intuitionistic fuzzy sets using table 1 .
"where c is the output transition matrix and o and n are white gaussian noises with zero mean and zero covariance matrices p o and p n, respectively. for simplicity, we assume that s(0) and that o and n are uncorrelated, and then we apply the euler forward method with a sampling time t c, to discretize the state dynamics in eq (30), so eq (32) is obtained."
the reciprocal value of ( ) step 2. aggregate the interval-valued intuitionistic fuzzy pairwise comparison matrices using eq. (5) to obtain the aggregated interval-valued intuitionistic judgment matrix g r  .
step 7: the weights vectors obtained in the previous stage are normalized giving the local weights of the criteria and sub-criteria. the local weight of each sub-criterion is then multiplied by the weight of the corresponding main-criterion to end up with the global weights.
"its distance to all centroid points and joins the closest cluster. then, the cluster centroids are updated based on the locations of their members. this process iterates until the algorithm converges."
step 1. collect the linguistic pairwise comparison matrix of criteria for each expert and convert the linguistic data to their corresponding interval-valued intuitionistic fuzzy sets using table 1 to obtain individual interval-valued intuitionistic judgment matrix r  for each expert.
"first, it is it for scm practice in clinic health. the clinic's lplpo contains information about the trends of the drug requested. this can be the basis for determining the pattern of public illness. lplpo will be compiled by pharmaceutical installations to become a plan for the needs of public drugs and health supplies in the district. this lplpo health office becomes a control system to formulate a health action plan. it also determines the intervention or policy determination of community disease management. figure 5 is a gui clinical subsystem. graphical user interface (gui) in figure 6 is a clinical subsystem for the submission of this lplpo. it is the beginning of the process to be performed in the pharmaceutical warehouse. second, it is it for scm practice in pharmaceutical warehouses. scm practices in pharmaceutical warehouses are more likely to be in inventory management. gui pharmaceutical storehouse subsystem is presented in figure 7 . the researchers really maximize system performance in here. to calculate inventory, the researchers use the equations as follows."
"as mentioned earlier, gmbo included 4 important parameters: p, peri, bar, and smax. in order to examine the effect of the parameters on the performance of gmbo, orthogonal design (od) [cit] was applied with uncorrelated 1000-dimensional 0-1 kp instance. our experiment contained 4 factors, 4 levels per factor, and 16 combinations of levels. the combinations of different parameter values are given in table 1 ."
cancellation while requiring significantly less backhaul communications as compared to coordinating all active transmitters. it is noteworthy that the key step here is sc clustering that facilitates interference coordination across all transmitters with reasonable complexity and overhead.
"th percentile ues compared to global coordination with epa and per-cell processing with cpa, respectively. fig. 3 and 4 illustrate the log utility gains and required backhaul capacity 3 of different schemes. as can be seen from the figures, global coordination with cpa provides 170% rate enhancements for 20 th percentile ues and 30% improvement in log utility across the network. however, this gain comes at a price of requiring almost 1000% more backhaul capacity. moreover, the performance of the global coordination is unrealistic, as in any practical deployment, there will always be out-of-cell interference. therefore, sc clustering via either k-means or ap-clustering scheme concatenated with cpa can be considered as a reasonable alternative approach for future wireless networks. table ii evaluates the performance of ap-clustering and kmeans with respect to log utility, required backhaul capacity (bits/s/hz), and the variance of obtained cluster sizes 4 against sp-clustering. for ap-clustering, the self-similarity values are set as the minimum value of mutual similarities. this corresponds to generating more populated clusters. in this case, the average number of clusters is 6.2. although spclustering achieves the highest log-utility, it requires high data-rate backhaul links. the reason is, as the cluster size variances confirm, the obtained clusters via sp-clustering are non-homogeneous, i.e., a few number of clusters include most of the active scas, while the remaining scas form single-member clusters. however, ap-clustering and k-means performances are almost identical."
"fourth, it is system testing. the system testing is done by black box testing method. it involves pharmaceutical warehouse staff as the system users. the test results are presented in table 3 . portal system can be visited in http:// scm.fikom-unasman.ac.id. last, there is a recommendation for the management. the graph presented in the system is sourced from clinical drug demand data. the system can be a strategic decisionmaking tool to assist management knowledge in mapping out health care needs. it is by determining disease patterns and ensuring public health policy intervention planning. this technology is a sustainable product that shows system integration with scm concept. this is considered as a tool to improve performance and services in the health sector. the implementation of this it should be supported by a good internet service to produce good results as well."
"the performance comparisons of the six methods on the five large-scale uncorrelated 0-1 kp instances are listed in table 8 . it can be seen that gmbo outperformed the other five algorithms on the six and five evaluation criteria for kp1 and kp4, respectively. in addition, gmbo obtained the best values concerning the best and the mean value for kp3 and was superior to the other five algorithms in the worst value for kp2. [cit] -dimensional 0-1 kp instances (kp5). mbo beat the competitors on kp5. moreover, an apparent phenomenon can be observed, which points out that abc has better stability. the best value of kp2 was achieved by cs. obviously, de and ga showed the worst performance for kp1-kp5. meanwhile, the approximation ratio of the best value of gmbo for kp1 equaled approximately 1.0. additionally, there was little difference between the worst approximation ratio of the best value (1.0242) of gmbo and the best approximation ratio of the best value (1.0237) of mbo for kp5. table 9 records the comparison of the performances of six methods on five large-scale weakly correlated 0-1 kp instances. the experimental results in table 9 differ from that in table 8 . it is clear that gmbo had a striking advantage in almost all the six statistical standards for kp6-kp9. for kp10, similarly to kp5, gmbo was still not able to win out over mbo. it is worth mentioning that the approximation ratio of the best value of gmbo for kp6-kp7, and kp9 equaled 1.0. moreover, the standard deviation value of kp6-kp7 and kp9 obtained by gmbo was much smaller than the corresponding value of the other five algorithms."
"(3) in the global position updating process, we can define the distance of the global best individual and the global worst individual as the adaptive step. obviously, the two extreme individuals differ greatly at the early stage of the optimization process. in other words, the adaptive step has a larger value, and the search scope is broader, which is beneficial to the global search over a wide range. with the progress of the evolution, the global worst individual tends to be more similar to the global best individual, and then the difference becomes small at the late stage of the optimization process. meanwhile, the adaptive step has a smaller value, and the search area narrows, which is useful for performing the local search. in addition, the genetic mutation is applied to preserve the population diversity and avoid premature convergence. it should be noted that, unlike the original mbo, in gmbo, the two newly-generated subpopulations regroup one population at a certain generation rather than each generation, which can reduce time consumption."
"f v i and f o i are the same as in eqs (9) and (10), respectively. because a robot observes two different landmarks, h k (s, z) is defined as"
"the pf method is used to determine the location of the leader robot based on bearing-only information and to approximate the state of the leader robot; the position estimation of the leader robot is critical to improving the localization accuracy. the leader robot plays a crucial role in the leader-follower multi-robot formation control. when the leader robot system is observable, the formation errors are bounded, and the leader robot's trajectories are very close to the true trajectories. based on the nonlinear observability rank criteria [cit], we derive the linearly independent rows in the observability matrix for the leader robot that observes a certain number of landmarks. to compute the lie derivatives, the nonlinear kinematic equation in eq (1) is changed into the following convenient form:"
"when the compromised rankings of the sub-criteria are analysed, results are quite distinctive from the aggregated ones. alignment with organizational goals (aog) and role of the executive (re) − the sub-dimensions of sc, rank at first and second degree. for effectiveness of a performance management system, the role and leadership of the executives is pivotal; both in identifying and in linking the individual goals with organization's strategic goals. further, a bias-free measurement, through systematic process delivering objective and meaningful results are ranked as third prioritized sub-criteria. an effective performance management system needs to deliver measurable and manageable results. peter drucker's famous quote, saying \"if you cannot measure it, you cannot manage it\" are once again justified as among main priorities. additionally, these three first ranking sub-criteria have priority weights above 0.1 each within the model."
"this paper evaluates the advantages of sc clustering in a downlink of a dense heterogeneous network wherein the macro bs and the sc access points (scas) are equipped with multiple antennas. we argue that, given reasonable cluster sizes, each cluster can form a virtual multiple-input multipleoutput (mimo) network wherein user equipments' (ue) communications are separated via spatial multiplexing using jointly designed downlink precoding vectors. the benefits of this approach is that the intra-cluster interference is completely removed. further, since only a small fraction of active scas form each cooperative cluster, the resulting overhead and complexity is manageable. moreover, to fully control inter-cluster interference, this paper assumes an architecture in which clusters can coordinate their transmit power via exchanging low-rate messages over the backhaul without sharing the ues' data symbols."
"with regard to the worst values, gmbo can still reach better values for almost all the 15 instances except kp3, kp5, and kp10 in which mbo was a little better than gmbo. the improvements in kp1-kp15 were 1.73%, 0.23%, -0.29%, 0.80%, -0.17%, 0.94%, 1.00%, 0.37%, 1.05%, -0.44%, 0.10%, 0.02%, 0.00%, 0.00%, and 0.01%, respectively."
"step 7. if the termination criterion is already satisfied, output the best solution found, otherwise, go to step 3. where dx is calculated by implementing the lévy flights. it should be noted that the lévy flights, which originated from the lévy distribution, are an impactful random walk model, especially on undiscovered, higher-dimensional search space. the step size of lévy flights refers to equation (2) ."
"each robot uses the exteroceptive sensors to measure its bearings relative to the other robots and the known landmarks that are in the field of view of the sensors. as shown in fig 1, the leader robot can obtain bearing measurements by measuring two different landmarks, and relative bearings from the ith robot to the jth robot or landmark can be written as"
"then iifwa is called an interval-valued intuitionistic fuzzy weighted averaging (iifwa) operator, where q is the set of all interval-valued intuitionistic fuzzy numbers (ivifns), ∑ [cit] . the iifwa operator can be further transformed into the following form:"
"a human resources manager wants to determine the weights of four performance criteria. these criteria are initiative, adaptability, leadership, and teamwork. let the initial linguistic pairwise comparison matrix be as in table 2 ."
"the superscript \"ide\" refers to the desired state, and c is the auxiliary control parameter. eqs (34) and (35) act as a feedback linearizing control in eq (33), so that the closed-loop dynamics are shown in eq (36)."
"motivation (mo): organizations need to be cautious about its aim when constructing a performance management system, as it is a double-edged sword. \"the single largest pitfall for performance management system is a negative side-effect which undermines the motivation, morale and behavior of human resources\" [cit] suggest that designing a performance management process to foster employee engagement will lead to higher levels of performance. to maintain and enhance an effective performance management process, organizations need to focus on employee engagement [cit] and promote managers' motivation to the process [cit] ."
"the second originality of this paper comes from its first time usage of interval-valued intuitionistic fuzzy sets to determine the weights of the main and sub-criteria for an effective performance management system in a firm. hence, particularly at times of uncertainty, the methodology will guide managers in assigning the appropriate weight for each criterion to ensure an effective performance management system. literature indicates that there are different levels of analyses and different measures of performance [cit], performance may be measured at three levels, -organizational, group and individual levels, each contributing to the overall performance. the third distinctive originality of this model is that; it provides decision-makers a performance management mindset that is available for use at all three levels of appraisal and analysis. then, for each level of performance appraisal measure, a single best-prioritized method can be driven, where flexible, adaptable, and nonstandard -but tailor-made method-may be used based on the needs of the individuals, work-teams, business unit, or the whole organization."
"the second set, which includes 25 0-1 kp instances, was taken from references [cit] . for all we know, the optimal value and the optimal solution of each instance are provided for the first time in this paper. the primary parameters are recorded in table 6 . the experimental results are summarized in table 7 . compared to table 5 above, three new evaluation criteria, that is \"arb\", \"arw\", and \"arm\", are used to evaluate the proposed method. \"opt.value\" represents the optimal solution value obtained by the dp method. here, the following definitions are given:"
"employees' performance is measured through a systematic process of performance appraisal that is embedded in the performance management system. an effective performance management system facilitates valuing the top performers, encourages communication between employees and managers, establishes a uniform standard for evaluation, and enables the organization to determine its strengths and weaknesses."
"the 0-1 knapsack problem (0-1 kp) is a classical combinatorial optimization task and a challenging np-complete problem as well. that is to say, it can be solved by nondeterministic algorithms in polynomial time. similar to other np-complete problems, such as vertex cover (vc), hamiltonian circuit (hc), and set cover (sc), the 0-1 kp is intractable. in other words, no polynomial-time exact algorithms have been found for it thus far. this problem was originated from the resource allocation involving financial constraints and since then, has been extensively studied in an array of scientific fields, such as combinatorial theory, computational complexity theory, applied mathematics, and computer science [cit] . additionally, it has been found to have many practical applications, such as project selection [cit], investment decision-making [cit], and network interdiction problem [cit] . mathematically, we can describe the 0-1 kp as follows:"
"after the above repair process, it is easy to verify that each optimized individual is feasible. the significance of gmo and goo seems particularly prominent while solving high dimensional kp problems [cit] . the pseudo-code of gmo and goo can be shown in algorithms 4 and 5, respectively."
"firstly, the original mbo was proposed to address the continuous optimization problems, i.e., it cannot be directly applied in the discrete space. for this reason, in this paper, a dichotomy encoding strategy [cit] was employed. more specifically, each monarch butterfly individual is represented as two-tuples consisting of a real-valued vector and a binary vector. secondly, although bmbo demonstrated excellent performance in solving 0-1 kp, it did not show a prominent advantage [cit] . in other words, some techniques can be combined with bmbo for the purpose of improving its global optimization ability. based on this, an efficient global position updating operator [cit] was introduced to enhance the optimization ability and ensure its rapid convergence. thirdly, a novel two-stage repair operator [cit] called the greedy modification operator (gmo), and greedy optimization operator (goo), respectively, was adopted. the former repairs the infeasible solutions while the latter optimizes the feasible solutions during the search process. fourthly, empirical studies reveal that evolutionary algorithms have certain dependencies on the selection of parameters. moreover, certain coupling between the parameters still exists. however, suitable parameter combination for a particular problem was not analyzed in bmbo and cmbo. in order to verify the influence degree of four important parameters on the performance of gmbo, an orthogonal design (od) [cit] was applied, and then the appropriate parameter settings were examined and recommended. fifthly, generally speaking, the approximate solution of an np-hard problem can be obtained by evolutionary algorithms. however, the most important thing is to obtain higher quality approximate solutions, which are closer to the optimal solutions more profitably. in bmbo, the optimal solutions of all the 0-1 kp instances were not provided. it is difficult to judge the quality of an approximate solution obtained by an evolutionary algorithm. in gmbo, the optimal solutions of 0-1 kp instances are calculated by a dynamic programming algorithm. meanwhile, the approximation ratio based on the best values and the worst values are provided, which clearly reflect the degree of the closeness of the approximate solutions to the optimal solutions. in addition, the application of statistical methods in gmbo is one of the differences between gmbo and bmbo, cmbo, including wilcoxon's rank-sum tests [cit] with a 5% significance level. moreover, boxplots can visualize the experimental results from the statistical perspective."
"the generation form of test set 3 was firstly given. since the difficulty of the knapsack problems was greatly affected by the correlation between the profits and weights [cit], 3 typical large scale 0-1 kp instances were randomly generated to demonstrate the performance of the proposed algorithm."
"this paper showed how the criteria for an effective performance management system are weighed in the presence of intuitionistic fuzzy assessments. the weights of the criteria are obtained by both aggregation of individual assessments and compromised assessments of experts to question any differences in between. the results exhibit a difference between the final ranking results of the two methods. thus, in a multi-expert decision, aggregation and compromised solutions cannot be used interchangeably and the most appropriate method needs to be chosen considering the case at hand."
"we investigate the performance of a two-tier network where scas act independently (per-cell processing), form clusters using k-means or ap-clustering schemes, and form one very large virtual mimo system (global coordination) with equal power allocation (epa) and the proposed coordinated power allocation scheme (cpa). as fig. 2 suggests, a network employing the proposed cpa scheme always outperforms epa transmission strategy. moreover, since the number of interfering sources decreases with clustering, cpa with clustering provides additional performance gain compared to all three schemes with epa and also per-cell processing with cpa. in particular, it provides 75% and 55% rate improvements for"
"step 1. initialize the parameters of mbo. there are five basic parameters to be considered while addressing various optimization problems, including the number of the population (np), the ratio of the number of monarch butterflies in subpopulation_1 (p), migration period (peri), the monarch butterfly adjusting rate (bar), the max walk step of the lévy flights (smax). step 2. initialize the population with np randomly generated individuals according to a uniform distribution in the search space. step 3. sort the individuals according to their fitness in descending order (here assumptions for the maximum). the better np1 (p*np) individuals constitute subpopulation_1, and np2 (np-np1) individuals make up subpopulation_2."
"proof: to meet the three prerequisites above, the m ik 12 matrix in eq (26) can be transformed into the simplified form shown in eq (27) using a finite sequence of elementary row operations."
"validity and openness (vo): a performance management system needs to be valid and open [cit], where there is an ongoing two-way communication process among supervisor and employee. therefore, not only the goals and objectives of the system need to be clear and specific, but also the feedback, information exchange and performance standards need to be clearly communicated for an open and valid system."
"the rest of the paper is organized as follows. the following first section introduces the pillar characteristics of effective performance management system, and extensively explains their sub-attributes upon which each attribute is based. in the further section, the intuitionistic fuzzy multi-expert and multi-criteria system is introduced. in the application section, the multi-expert aggregated and compromised rating results are obtained and results were drawn based on the proposed performance management model. the paper is concluded with management implications and further research suggestions."
pharmaceutical warehouse operational activity is a repeated monthly routine. it implementation to automate the process will improve work efficiency. figure 4 shows the pharmaceutical warehouse operational activities designed for it implementation. it is the proposed process flow of the it system.
"equation (1) explains that f is supply, d is initial stock, and e is acceptance. the sum of the initial stock (d) and the receipt (e) is the stock inventory for the future. for calculating the remaining stock (h), the researchers use the following equation."
"performance management is \"a continuous process of identifying, measuring and developing performance of individuals and teams, and aligning performance with the strategic goals of the organization\" [cit] . it is important to define an hr system that relates to particular performance criteria [cit], because the field of hr system and performance management generally has inconsistent conceptualizations, and mostly lack of cumulative body of knowledge [cit] . conceptually, performance management systems have influence on organizational and financial outcomes, as well as contextual factors; such as industry [cit] and strategy [cit], for example, state the facilitation effect of forced distribution rating systems on task performance through better motivation and attraction of high talent. [cit] statement \"hrm activities are strategically important because they are the potentially valuable, rare, difficult to imitate and substitute for, and they are central to creating the organizational capability to enact the firm's strategic goals\" may be meaningful in defining the importance of performance management system within hrm. based on 'line of sight' model [cit], [cit] further argument is that, \"superior performance is hypothesized to result from directly linking strategic goals and action plans to organizational capabilities and culture, job-specific group competencies and norms, and individual knowledge-skills-abilities, motivation and opportunity\". it is the employees' understanding of the firm's strategic goals as well as the actions necessary to accomplish the goals that enable superior performance. moreover, if hrm is applied effectively in an organization, it simultaneously improves the job satisfaction of employees, company image both within and outside of the company, and even market share and the overall performance level of the organization [cit] ."
step 2. convert the linguistic data to their corresponding interval-valued intuitionistic fuzzy sets using table 1 to obtain compromised interval-valued intuitionistic judgment matrix c r  .
"to evaluate the performance of the proposed gmbo, the two latest algorithms, namely, moth search (ms) [cit] and moth-flame optimization (mfo) [cit], were especially selected to compare with gmbo. the following factors were mainly considered. (1) the literature on the application of ms and mfo to solve 0-1 kp problem was not found. (2) the gmbo, ms, and mfo were novel nature-inspired swarm intelligence algorithms, which simulated the migration behavior of the monarch butterfly, the lévy flight mode, or the navigation method of moths."
"overall, tables 8-10 and figures 4--6 indicate that gmbo was superior to the other five methods when addressing large-scale 0-1 kp problems. in addition, if we look at the worst values achieved by gmbo and the best values obtained by other methods, we can observe that for the majority instances, the former were even far better than the latter."
"higher order lie derivatives are computed similarly. additionally, mixed lie derivatives are defined. the second-order lie derivative of h k (s, z) with respect to f n j, given its first derivative with respect to f n i, is"
"our suggestions for further research are to use different aggregation operations for interval-valued intuitionistic fuzzy sets or to use different extension types of fuzzy sets to represent the vagueness in the assessments. for instance, triangular intuitionistic fuzzy sets for the representation and different aggregation operators for this type of fuzzy sets can be used."
"the first step to adopting a dichotomous encoding scheme is to transfer the phenotype to genotype. therefore, a surjective function g is used to realize the mapping relationship from each element of x to the corresponding element of y."
"in general, problems arising related to future needs should be adjusted to demands [cit] . in addition, the amount of usage will affect the number of requests. when the request is not performed in accordance with or less than the required thing, the health clinic will re-request this. it may take a long time. on the other hand, it may affect the size of the goods demanded by the pharmaceutical warehouse and provided by the supplier's section [cit] . then, the pharmaceutical warehouse and all its entities involved in operational activities manage the same data. however, because conventional systems applied cannot show the same data, the inventory calculations are performed by each party as a result of frequently different produced data [cit] and resulted in delaying the report of drug use [cit] ."
"step 1. quicksort algorithm is used to sort all items in the non-ascending order according to pi/wi, and the index of items is stored in an array h [cit], h [cit] ..., h[n], respectively."
"it designed for scm practice in the pharmaceutical warehouse has health clinic sub-systems for submission of lplpo. the sub-system of health department has a function of internal control. then, pharmaceutical warehouse system is for drug stock information, lplpo acceptance, and drug distribution. the system is built using php programming language and mysql database. the testing is done using xampp server 6.2 with a black box testing method. it involves the pharmaceutical warehouse staff as the end user. besides that, focus discussion involves several pharmaceutical warehouses to evaluate the design and see how much the support is provided for the user. the research stages are shown in figure 1 ."
"establishing an effective performance management process is a challenging issue for organizations. specially, when complex and vague factors are involved, managers need to take all of them into account and pick the most functional and applicable model. however, a model that is derived based on only a sub-set of important criteria, through limited cognitive ability, will ignore the relative connection and interaction among them. thus, a fuzzy analytic hierarchy (ahp) based evaluation process may facilitate managers, by synthesizing their data and delivering a more rational decision making."
"where γ c ik and γ b k are the weights associated with ue k in cluster i and macro ue k, respectively. given a fixed set of scheduled ues for each cluster and the macro bs, the optimal beamforming vectors can be designed."
step 4: the priority vectors of the interval multiplicative matrices related to main and subcriteria are determined by using eq. (11) where total− is table 8c .
"as an important component of the whole system, being meaningful [cit] ) is another essential characteristic, which brings out interpretable and useful results. furthermore, in attempt to interpret the results of performance measurement, the cause and effect relationships among measures are mostly a fuzzy mess of interactions and interdependencies. they are always linear and one-way [cit] . however, the more non-linear or multi-way interactions are, the more meaningful and useful they may become. furthermore, from the developmental point of view, a useful system, that generates meaningful results will ultimately be establishing learning and growth-based environment."
"thus, the gradients of the second-order and higher order lie derivatives are linearly dependent on the rows of the observability matrix corresponding to the gradients of the first-order and zeroth-order lie derivatives. therefore, we can write the observability matrix with the rows corresponding to g ik 1 using the gradients of the lie derivatives up to the first order as follows:"
"is the sigmoid function. the sigmoid function is often used as the threshold function of neural networks. it was applied to the binary particle swarm optimization (bpso) [cit] to convert the position of a particle from a real-valued vector to a 0-1 vector. it should be noted that there are other conversion functions [cit] can be used. now assume a 0-1 kp problem with 10 items, figure 1 shows the above process, in which each [ ]"
"in figure 4, there are three entities involved in the pharmaceutical warehouse operations activity. each entity manages the transaction data. it can share information without ignoring the independence of each entity. the managed data is basically the same, but the assignment of each entity is different at the time of the transaction. the result of the health department activity has a buffer as the output. then, this output becomes the input for processed activity on the pharmaceutical warehouses. the work of pharmaceutical warehouse is the final information that will be received by the clinic about the drugs to be distributed. it also becomes report for the next needs of public drugs and health suppliers. basically, to control the movement of the materials, each department has roles. it is to control and inform regarding the needs that should be shared [cit] . the functions of each entity are described in table 1 ."
"in table 3, the corresponding intuitionistic fuzzy numbers are substituted into the pairwise comparison matrix in table 2 . the score judgment matrix is obtained as in table 4 . the interval multiplicative matrix is obtained as in table 5 ."
"given a fixed user assignment for each cluster, an essential question is that which ues should be served by the avail- able antennas at every given time slot. this issue is more pronounced when the number of ues assigned to a cluster is greater than available antennas. this paper adopts proportional fair scheduling (pfs) and exploits the idea that given a fixed set of downlink beamforming vectors and power spectrum across the network, user scheduling at each cluster does not affect the interference seen by other ues [cit] . therefore, each cluster can design a set of random, but fixed downlink beams independently and schedule ue k * for the current time if"
"similarly, the observabilities of a robot that observes two different landmarks is also analyzed. the rank of the observability matrix can be easily obtained using the same method as that in lemma 1."
"with regard to the best values, gmbo can gain better values than the others for almost all the instances except kp2, kp5, kp10, and kp15, in which cs and mbo twice achieved the best values, respectively. more specifically, compared to the suboptimal values researched by others, the improvements in kp1-kp15 brought by gmbo were 0.59%, -0.22%, 1.10%, 1.45%, -0.05%, 0.27%, 0.18%, 1.09%, 0.24%, -0.09%, 0.07%, 0.10%, 0.00%, and -0.02%, respectively."
"a strategically and contextually congruent performance management system, as highly objective as can be, needs to be functional. functionality is last of the major pillars of an effective performance management system. even if members own the process, have positive attitude, are as objective as can be, for full functioning, the system needs to (1) be applicable and user-friendly, (2) be inclusive, (3) allow flexibility and corrections, and (4) have a sense of meaning and use."
"this section proposes a hierarchical approach to acquire a practical, albeit local, solution to the optimization problem in (3)- (6) . in this regard, we first utilize the k-means or the ap-clustering algorithms to find optimized partition of the sctier based merely on sca distances. given fixed clusters, ue scheduling, beamforming, and power spectrum management steps are carried out in order."
"involving all individuals in the evaluation process creates thoroughness in the system [cit] claim that it is possible for managers to gain insight about how their actions might affect outcomes if they work with models that integrate management debate, communication, dialogue and experimentation as part of performance management system, which may be a powerful communication and learning tool. for this fact, thoroughness may be considered to enable a credible model of performance management system."
"in the literature, there are several aggregation operators developed for the various types of intuitionistic fuzzy sets. in this study, interval-valued intuitionistic fuzzy averaging (iifa) aggregation operator is used since the experts are assumed to have equal influence on the final decision."
"the data in this research is lplpo. it is obtained in pharmacy installation of mamasa regency. the data will be used to simulate and test the system. lplpo from 17 clinics is shown in table 2 . the researchers analyze this to see the trends or patterns in disease based on drug demand and usage. each clinic has different drug requirements and disease trends. this will certainly affect the demand and usage of medicines in the clinic area. figure 5 is a database system design. moreover, the data analysis results are presented in table 3 the design of the database indicates that the proposed drug demand data (lplpo) will be connected to the acceptance or supply table. it will be accessed by the pharmaceutical installation. based on the data of drug demand, this data will describe the pattern of disease in the clinic. from all drugs in the clinic, the groupings are based on the type of disease. there are general and dental; maternal and child health and nutrition; tuberculosis and leprosy; malaria and filariasis; and immunization. the five types of diseases will be used to monitor and control the development of disease patterns. each clinic has different drug needs. the tendency of drug usage also varies based on the most common consumption and the most frequently requested for pharmacy installation. this level of tendency will be considered to determine the pattern of disease in the community in five diseases groups. the data will be displayed on the system. data on demand, usage, and stock are the results of the overall recapitulation of drug used in each type of disease. the tendency of the disease can be seen in the usage of the drug. it is shown in table 3 ."
"t of the state s given the input, u, and the output, y. assuming that both the state and the observation equations are affected by additive noises, we obtain"
"step 3 (9) and (10), respectively. for the main criteria, score judgment matrices are given in table 8a, and interval multiplicative matrices are given in table 8b ."
"here, \"arb\" represents the approximate ratio [cit] of the optimal solution value (opt.value) to the best approximate solution value (best). similarly, \"arw\" and \"arm\" are based on the worst approximate solution value (worst) and the mean approximate solution value (mean), respectively. arb, arw, and arm indicate the proximity of best, worst, and mean to the opt.value, respectively. plainly, arb, arw, and arm are real numbers greater than or equal to 1.0. table 7, it was clear that gmbo could obtain the optimal solution value for all the 25 instances. among them, gmbo could find the optimal solution values of 13 instances with 100% sr, and the success rate of nine instances was more than 80%. in addition, the standard deviation of 13 instances was 0. in particular, arb can reflect well the proximity between the best approximate solution value and the optimal solution value. arw and arm were similar to this. for the three new evaluation criteria, it can be seen that the values were equal to 1.0 or very close to 1.0 for all the 25 instances."
"in this section, 2 sets of 0-1 kp test instances were considered for testing the efficiency of the gmbo. the maximum number of iterations was set to 50. as mentioned earlier, 50 independent runs were made. the first set, which contained 10 low-dimensional 0-1 knapsack problems [cit], was adopted with the aim of investigating the basic performance of the gmbo. the standard 10 0-1 kp test instances were studied by many researchers, and detailed information about these instances can be taken from the literature [cit] . their basic parameters are recorded in table 4 . the experimental results obtained by gmbo are listed in table 5 ."
"to help managers in building an effective performance management system, the confirmed main and sub-criteria defined in section 2 need to be prioritized. such a decision necessarily requires a multi-expert multi-criteria analysis. in multi-expert decision making, experts' opinions can either be collected individually and then aggregated, or obtained as a compromised assessment after a series of discussion sessions. those two different methods may yield different results that can directly affect the final decision of the managers. considering this, the analysis in this study has been carried out in two phases: (1) individual rating and later analysing the aggregated data; and (2) coming to a consensus through a face-to-face discussion and analysing the mutually driven (compromised) data. the rating team is comprised of three experts in the field of human resources management: the first is a human resources (hr) director in a corporate organization; the second is an academician with a corporateexperience in hr management; and the third is an academician working in the field of hr management and specifically on performance management. the selection of these experts was made on the basis of collaborating both the practice and academic perception, so as to obtain optimum possible result. the availability of both professional background and a researcher's mind-set facilitates unbiased judgment. furthermore, years of experience together with degree of involvement in performance appraisal and management process have been determining criteria on the selection of the involved experts."
"a comparative study of the six methods on five large-scale strongly correlated 0-1 kp instances are recorded in table 10 . obviously, gmbo outperforms the other five methods for kp11-kp14 on five statistical standards except for std. abc obtains the best std values for kp11-kp15. to kp15, gmbo can get better values on the worst. cs, de, and ga fail to show outstanding performance for this case. under these circumstances, the approximation ratio of the worst value of gmbo for kp11-kp15 was less than 1.0019. for a clearer and more intuitive measure of the similar level of the theoretical optimal value and the actual value obtained by each algorithm, the values of arb on three types of 0-1 kp instances are illustrated in figures 4-6 . from figure 4, the arb of gmbo for kp1, kp3, and kp4 were extremely close to or equal to 1. gmbo had the smallest arb for kp1, kp3-kp5, except for kp2, for which cs obtained the smallest arb. similar to figure 4, from figure 5, gmbo still had the smallest arb values, which are 1.0 (kp6, kp7, and kp9) or less than 1.015 (kp8, kp10). in terms of the strongly correlated 0-1 kp instances, gmbo consistently outperformed the other five methods (see figure 6 ), in which gmbo had the smallest arb values except for kp15. particularly, the arb of gmbo was even less than 1.0015 for kp15."
"then, a comparison of the six highest dimensional 0-1 kp instances, i.e., kp4, kp5, kp9, kp10, kp14, and kp15, is illustrated in figures 7-12, which was based on the best profits achieved by 50 runs. figures 7, 9, and 11 illustrate the best values achieved by the six methods on 1500-dimensional uncorrelated, weakly correlated, and strongly correlated 0-1 kp instances in 50 runs, respectively. from figure 7, it can be easily seen that the best values obtained by gmbo far exceed that of the other five methods. meanwhile, the two best values of cs outstripped the two worst values of gmbo. by looking at figure 9, we can conclude that gmbo greatly outperformed the other five methods. the distribution of best values of gmbo in 50 times was close to a horizontal line, which pointed towards the excellent stability of gmbo in this case. with regard to numerical stability, cs had the worst performance. from figure 11, the curve of gmbo still overtopped that of abc, cs, de, and ga, as illustrated in figures 7 and 9 . this advantage, however, was not obvious when compared with mbo. figures 8, 10, and 12 [cit] -dimensional uncorrelated, weakly correlated, and strongly correlated 0-1 kp instances in 50 runs, respectively. as the dimension becomes large, space is expanded dramatically to 2 2000, which represents a challenge for any method. it can be said with certainty that almost all the values of gmbo are bigger than that of the other five methods except mbo. similar to figure 11, the curves of mbo partially overlaps that of gmbo in figure 12, which may be interpreted as the ability of gmbo towards competing with mbo. for the purpose of visualizing the experimental results from the statistical perspective, the corresponding boxplots of six higher dimensional kp4-kp5, kp9-kp10, and kp14-15 are shown in figures 13-18 . on the whole, the boxplot for gmbo has greater value and less height than those of the other five methods, which indicates the stronger optimization ability and stability of gmbo even encountering high-dimensional instances. in order to examine the convergence rate of gmbo, the evolutionary process and convergent trajectories of six methods are illustrated in figures 19-24 . it should be noted that six high dimensional instances, viz., kp4, kp5, kp9, kp10, kp14, and kp15, were chosen. in addition, figures 19-24 show the average best values with 50 runs, and not one independent experimental result. from figure 19, the curves of gmbo and mbo were almost coincident before 6 seconds, but afterward, gmbo converged rapidly to a better value as compared to the others. from figure 20, it is indeed interesting to note that mbo has a weak advantage in the average values as compared to gmbo. from figure 21, mbo and gmbo have identical initial function values, and the average values obtained by mbo were better than that of gmbo before 3 seconds. however, similar to the trend in figure 19, 3 seconds later, gmbo quickly converged to a higher value. as depicted in figure 22, [cit] -dimensional weakly correlated 0-1 kp instances, gmbo was inferior to mbo. figures 23-24 illustrate the evolutionary process of strongly correlated 0-1 kp instances. by observation of 2 convergence graphs, we can conclude that gmbo and mbo have similar performance. throughout figures 19-24, gmbo has a stronger optimization ability and faster convergence speed to reach optimum solutions than the other five methods."
"in this section, 3 groups of large scale 0-1 kp instances with dimensionality varying from 800 [cit] were considered. these 15 instances included 5 uncorrelated instances, 5 weakly correlated instances, and 5 strongly correlated instances. the dimension size was 800, 1000, 1200, 1500, [cit], respectively. we simply denoted these instances by kp1-kp15."
"as a novel biologically inspired computing approach, mbo is inspired by the migration behavior of the monarch butterflies with the change of the seasons. the related investigations [cit] have demonstrated that the advantage of mbo lies in its simplicity, being easy to carry out, and efficiency. in order to address the 0-1 kp, which falls within the domain of the discrete combinatorial optimization problems with constraints, this paper presents a specially designed monarch butterfly optimization with global position updating operator (gmbo). what needs special mention is that gmbo is a supplement and perfection to previous related work, namely, a binary monarch butterfly optimization (bmbo) and a novel chaotic mbo with gaussian mutation (cmbo) [cit] . the main difference and contributions of this paper are as follows, compared with bmbo and cmbo."
"the inclusive aspect of the system [cit] ) is another sub-attribute, which promotes all individuals to get involved and be part of the system. this allows all participants to have a voice in the process of designing and implementing the process."
"since kp is a constrained optimization problem, it may lead to the occurrence of infeasible solutions. there are usually two major methods: redefining the objective function by penalty function method (pfm) [cit] and individual optimization method based on the greedy strategy (iom) [cit] . unfortunately, the former shows poor performance when encountering large-scale kp problems. in this paper, we adopt iom to address infeasible solutions."
"input-output feedback control based on bearing-only ukf to achieve and maintain the desired leader-follower formation, the follower robots require information regarding the relative position of the leader robot to adjust their positions in real time. in our approach, the leader robot position is estimated using ukf, and the follower positions are calculated using the classical input-output feedback control law; this system is shown fig 3. in the following, we explain how to achieve input-output feedback control based on the bearing-only ukf. the variableŝ is the state estimation of the true state s. the ukf is designed to estimate the angle information, i.e., [φ i α i ]"
"in the initial moment, the robots must adjust their original positions to the ideal positions. the original position errors are relatively large, which has no effect on the normal movement of the robot formation. as a result, we do not consider the initial position errors when analyzing the simulations. fig 5a shows the true and estimated trajectories of the leader robot for all three cases. the enlarged insets in fig 5a show that the estimated trajectories with two different landmarks are those closest to the true trajectories. this is because with two different landmarks, the leader robot system is observable according to the observability condition described in section 2.2. fig 5b shows the root mean square errors with no landmark, one landmark and two landmarks. the root mean square errors with no landmark and one landmark are higher than those with two landmarks because the leader robot system is observable in the case with two landmarks. fig 5c shows that the errors of θ for all three cases are very small. in addition, the errors for two landmarks are smaller than the errors for no landmark or one landmark because according to the observability condition described in section 2.2, the leader robot system is observable with more than one landmark. fig 5d shows the relative bearing errors for all three cases. although the relative bearing errors for all three cases are small, the errors for two landmarks are smaller than the errors for one landmark. fig 6a shows the trajectories of the leader and the follower robots and shows that the desired formation is properly maintained. this is because the estimated trajectories of the leader robot are closest to the true trajectories when the leader robot system is observable. fig 6b shows that the observation angle estimation errors are very small. fig 6c shows that the direction angle estimation errors are also very small during the input-output feedback control process except when the leader suddenly adjusts the movement direction from right to left or from left to right; the maximum direction angle error occurs at approximately -0.015."
"the results of this research show an integrated information system and use the scm concept to support pharmaceutical warehouse operations. this system can be the recommendation for management in the pharmaceutical warehouse. it is to repair old systems to assist the operational activity. figure 4 shows the relationship between entities in scm. each entity will describe the shape of scm practice, and the applied it model. it also shows the business process between pharmacy installations, clinics, and health offices as scm actors. it also describes the scope of each major entity in the use of it to be implemented. it to train scm on each entity is as follows."
"third, it is it for internal control. the supervisory function undertaken by the public health service is more as a form of control (internal control). it is to monitor drug management and distribution. figure 7 shows some functional systems that can be performed by the health service such as lplpo verification. it is by receiving reports of pharmaceutical warehouse and lplpo verification, and seeing dashboards about disease patterns. then, figure 8 is a gui sub-system for the health service supervision function. figure 8 and 9 are data of visual analytics. this is the tendency of disease patterns. it can be seen in the high usage of the drug. by district government, this data can be used to support the health policy."
"for each experiment, the average value of the total profits was obtained with 50 independent runs. the results are listed in table 2 . using data from table 2, we can carry out factor analysis, rank the 4 parameters according to the degree of influence on the performance of gmbo, and deduce the better level of each factor. the factor analysis results are recorded in table 3, and the changing trends of all the factor levels are shown in figure 3 ."
"in order to tackle high-dimensional 0-1 kp problems more efficiently and effectively, as well as to overcome the shortcomings of the original mbo simultaneously, a novel monarch butterfly optimization with the global position updating operator (gmbo) has been proposed in this manuscript. firstly, a simple and effective dichotomy encoding scheme, without changing the evolutionary formula, is used. moreover, an ingenious global position updating operator is introduced with the intention of enhancing the optimization capacity and convergence speed. the inspiration behind the new operator lies in creating a balance between intensification and diversification, a very important feature in the field of metaheuristics. furthermore, a two-stage individual optimization method based on the greedy strategy is employed, which besides guaranteeing the feasibility of the solutions, is able to improve the quality further. in addition, the orthogonal design (od) was applied to find suitable parameters. finally, gmbo was verified and compared with abc, cs, de, ga, and mbo on large-scale 0-1 kp instances. the experimental results demonstrate that gmbo outperforms the other five algorithms on solution precision, convergence speed, and numerical stability."
"2) k-means clustering: k-means algorithm is one of the classic clustering methods that partitions the scas into k groups in order to minimize the within-cluster sum of distances, i.e."
"where l − k elements of cir with the smallest power can be considered as noise based on the analysis above. greedy algorithms select a fixed number of atoms during each iteration. as the number of antennas increases in massive mimo systems, the number of iterations rises dramatically, and it incurs an intractably high computational complexity. usually, the power of cir is higher than that of awgn. a clever thing to do to make a distinguish between the nonzero taps and the noise is to find a position with the fastest change of power. the selection of atoms can be more flexibility and efficiency by finding this position. therefore, we calculate the backward difference between two adjacent elements to choose the appropriate entries. those two elements with the maximum backward difference are considered as the boundary of nonzero taps and noise."
"baseband signal processing, such as channel estimation, channel equalization, and channel encoding, are the important parts of wireless communication systems to resist fading channel [cit], where channel estimation is especially critical for massive mimo systems. in massive mimo systems, accurate and efficient channel estimation is a challenging problem and an open research issue, because the number of channel parameters to be estimated is very large as the antennas increase, while the number of pilots adopted by channel estimation is limited to make sure a high spectrum efficiency."
"i n the area of mobile communications, new technologies being able to increase the capacity and spectrum efficiency are forever needed to satisfy the increasing data rate demand from the users. the existing third generation (3g) code division multiple access (cdma) and fourth generation (4g) long-term evolution (lte) systems make the limited wireless bandwidth even more vital. multi-input multi-output (mimo) is a key technology to increase the capacity and system reliability by exploring the spatial domain without any additional wireless bandwidth. in 4g communications, the standard named longterm evolution-advanced (lte-a) first adopts mimo technology to boost the communication capacity [cit] . although mimo can help increase the capacity over single input single output (siso) with 4g systems, it cannot meet the expectation for the future mobile communication systems."
"since the massive mimo channel shows sparse characteristics [cit], cs algorithm is used in this paper to perform the initial channel estimation in step (1) of the proposed clmmse algorithm."
"in research article [cit], wang, m., wang, z., & li, j. proposed a system that combines l lbp and a model of d-cnn. proposed method extracted the features of lbp that corresponds to the facial image supplied as an input to the network of cnn, and a cnn network is trained with the help of lbp feature sets, and later makes use of the network that is trained for the recognition of face. this helped in avoiding the limitations of the weakest stability of gray scale of cnn model and can recognize the cnn trained model more accurately. the accuracy achieved using orl andyale datasets was about 96.6%"
"in their research article [cit], authors have proposed a face recognition system using dominant rotated local binary pattern (drlbp) and scale invariant feature transform (sift) feature extraction which was an innovative approach for classifying the images of human face using ann(artificial neural network ). proposed technique is implemented in stages: in the first stage, all the facial images under the consideration are preprocessed. in the second stage, pre-processed image features are using sift. in the third stage extracted features of sift is then combined with drlbp for the achievement of better accuracy. the accuracy of the existing system is 48%/ whereas the proposed system achieves 75% accuracy."
"2) bsamp-based initial channel estimation: in this section, the proposed bsamp algorithm is applied to massive mimo systems to do sparsity adaptive initial channel estimation."
"third, an algorithm named bsamp is proposed by exploiting the joint sparse characteristics of massive mimo channels to make the proposed clmmse algorithm more efficient."
"massive mimo can make a huge increase on system capacity with a much larger number of antennas configured at the base station (bs). thus, it is regarded as one of the key technologies for future fifth generation (5g) wireless communication systems for its high spectrum and energy efficiency [cit] . by utilizing spatial multiplexing, a massive mimo bs can serve multiple users simultaneously through multi-user beamforming [cit] . recently, some proposals using massive mimo technology for 5g standards have been presented in the third generation partnership project (3gpp) release 15 for one of the key enabling technologies for 5g systems [cit] ."
"in the basic cs principle, it recovers the channel h by calculating the correlation coefficient u between the measurement matrix a and the observation vector y, which can be expressed as"
"in an article [cit], authors have proposed the technique of the unconstrained face verification using deep convolution neural network (dcnn) features. based on deep convolution features, algorithm for corrupt face authentication is implemented in the paper. this method consists of two facial features. training and testing. as per the proposed methodology, initially, face and landmark detection is performed and then, training of dcnn is performed. given a pair of images, match score based on the closeness of the dcnn features is computed. processing, deep face feature representation followed by joint bayesian metric learning is used in obtaining the final results. the dcnn model for some days, the dcnn model was trained using nvidia tesla k40. to extract the various features, it took about 0.006 second for every facial image with 97.15% mean accuracy."
"yu, h., luo, z., & tang, y., i their research article [cit], proposed a technique for a face recognition task, there are very few resources used to learn deep data face model. there are very limited training samples for a face recognition task. these problems were addressed through transferring an already learned model of the facial image using deep learning system considered as a initial type of model. then, higher layer depictions were learned on a minute and a distinct training set. the aimed target model is obtained. the target model achieved 0% error rates on all the three datasets that were used, with very less training samples for every person's image."
"the remainder of this paper is organized as follows: section ii discusses the massive mimo system model in typical 5g communication scenarios. section iii gives the detailed description of the proposed clmmse algorithm and sparsity adaptive determination principle. section iv gives the complexity analysis of the algorithms. in section v, a 5g massive mimo system based on 3gpp release 15 evolved from lte-a is established by matlab, and simulation results and discussions are presented to validate the proposed solutions. section vi gives the conclusion of this paper."
"where τ max is the largest delay of the channel, τ l is the delay of the lth path, and τ rms is the average delay of the channel. however, in actual scenarios, prior information of the channel is hard to know, consequently the channel autocorrelation matrix is difficult to get. although we can estimate the channel information, high computational complexity will be brought in massive mimo systems for its large amount of antennas."
"much work has been done to show that convex optimization methods can be used in signal recovery to further improve the performance [cit] . if for the k-sparse vector h, then there is"
"in the future work, the proposed algorithm is planned to be implemented in an universal software radio peripheral (usrp)based hardware experiment platform and the actual performance will be tested. meanwhile, this algorithm will be optimized by further exploring the sparsity of the massive mimo channel and utilizing the correlation of the channel in time, frequency, and spatial domain."
"from the above simulation experiments, the proposed sparsity adaptive clmmse has great advantages in estimation accuracy and computational complexity, compared with other existing algorithms. by applying it to the existing fdd-lte communication system, the pilot overhead can be reduced and the spectrum efficiency can be enhanced. for the further 5g systems with larger antenna scale, it can also be utilized as an economic estimator. in the fdd massive mimo system shown in fig. 1, the pilots are transmitted in the downlink, and the proposed channel estimation algorithm is performed at the user equipment (ue) side. to obtain a high spectrum efficiency, the design of pilot number should be taken into consideration; meanwhile, the channel estimation performance should be continuously improved under limited pilot overhead."
"research article [cit] entitled \"face recognition in real-world surveillance videos with deep learning method\". in this article, authors have presented a novel dataset which are constructed out of surveillance frames from the real time destination. later, a cnn having labelled sets of data was elegantly improved. proposed method consists of 2 parts. first part consists of a set of data formed by automatically gathering and also data labelling out of the video surveillance frames. in the next stage, the face recognition model of vgg type is elegantly improved. later the network on improvement accomplished the efficiency of 92.1 % of recognition."
"as per the researches carried out, a complete face recognition system includes two patterns of face detection and face recognition: 1) structural similarity and 2) individual local differences of human faces. therefore, it is required to extract the features of the face through the face detection process. the evolution of face recognition is due to its technical challenges and huge potential application in video surveillance, identity authorization, multimedia applications, home and office security, law enforcement and different human-computer interaction activities. facial recognition technology (frt) is one of the most controversial new tools. [cit] s. it has recently become accessible to the mass market-to both law enforcement and private consumers. frt has the capacity to eliminate the need for passwords, fingerprint data, and even keys. however, it could also end privacy. face recognition technology has also developed research and implementation based on mobile phone system. [cit] surveys carried out have proved that the united states suffers from a systemic problem of racial disparities in traffic stops, stop and frisks, and arrests. due to this african americans are arrested at twice the rate and shrivelled at nearly three times the rate of any other race. face recognition is one of the few biometric methods that possess the merits of both high accuracy and low intrusiveness. biometric based time attendance system has been developed using face recognition technology [cit] . hence the demand for automatic face recognition system is increasing potentially in every single area. automatic face recognition involves: 1) face detection, 2) feature extraction and 3) face recognition. face recognition algorithms are broadly classified into two classes: 1) template based system 2) based on geometric feature. template dependent procedures calculate value of the correlation among the face and more than one model templates to match the facial identity. principal component analysis (pca), linear discriminate analysis (lda), methods of kernel based etc. were applied to compose the templates corresponding to the face. face recognition is an initial stage in the identification process [cit] ."
"in this paper, a block samp (bsamp) algorithm is further proposed, exploiting the joint sparse characteristics of massive mimo channels, to make the proposed clmmse algorithm more efficient. when the channel sparsity level is unknown, the proposed algorithm first selects the atoms by setting a proper threshold and finding the position of maximum backward difference position. then the regularized method is used to improve the accuracy of atoms selection. the proposed bsamp algorithm reduces the iteration times greatly by exploiting the joint sparsity of sub-channels. the sparsity adaptive processing is more flexible, for it does not rely on fixed step size, which reduces the computational complexity effectively."
"learning used for facial recognition system. on performing an exhaustive literature review, it is very clearly found out that substantial amount of work is already accomplished under photo-photo identification in facial recognition system using deep learning techniques. many of the research articles have even proposed and implemented good number of works considering different variations like multiexpressions, time-invariant, weight variation, and illumination variation etc of photo-photo matching. after collecting various reviews, it is made very clear that a very few research articles have focused on the implementation of deep learning techniques for facial recognition system using forensic sketches to facial photograph matching. hence still an admirable amount of scope for conducting an active research in the field of photo to sketch matching using deep learning techniques. on identifying various deep learning techniques used for facial recognition system, it is found out that very few papers have used transfer learning approach for the facial recognition system. so, in future a research could be concentrated in the area of photo to sketch matching using a deep learning technique with the combination of transfer learning approach which could prove a novel work. on accomplishment of the research in this area, researchers can possibly work out for boosting the performance of the system in identification. on inspection we have found many different data sets used for the purpose of the research and in future researchers have a very huge scope in building a dataset for photo-sketch images containing the photo-sketch sets of humans."
the problem stated above can also be solved in another way that the elements in r hh can also be calculated by the power delay spectrum of the channel as
"both omp and sp need the channel sparsity level as a priori information to recover the original signal accurately. however, the channel sparsity level k is usually unknown in actual situations. artificially setting a fixed and large experience value to k will bring invalid iteration operation and a low computational efficiency."
"in article [cit], a new approach for image representation was presented. a face recognition model was developed. the model was composed of many advanced techniques. cnn cascade was used for face detection and cnn for generating face buildings. for face recognition, a new approach for image augmentation was proposed. the development of deep learning based attendance system involves several important stages. obtaining training dataset, augmentation, preparing images and then training deep neural networks was done. finally integration into existing system was performed in-order to test the method. with the proposed method of augmentation high accuracy can be achieved, 95.02% in overall."
"in their research article [cit], authors have presented a comprehensive analysis of deep learning based neural network (cnn) architecture known as visual geometry group (vgg) face network is used for the face recognition. vgg face network consists of 2.6 million trained facial images. results presented in the paper proved the achievement of 98.95% accuracy."
"in their research paper [cit], authors have presented an approach of the type modified deep learning neural network system for the purpose of recognition of the faces. proposed technique actually makes use of use of a convolution neural networks (cnn) as its foundation. in the proposed technique, they have used a dataset that improves the generalization power of cnns. the augmented training sets were then formulated such that they contain extra images generated by applying relevant filter techniques. selecting a subset of an image from a dataset, synthetic type of images are later generated by applying a substantial noise factor using poisson or gaussian noise functions. once after that, as a further part, the noise images are sampled to the set of images trained to construct the training set of an augmented type with twice the amount of samples. augmented fragments are later supplied into the convolution neural network (cnn) for the process of accomplishing training. above trained network later tested using testing network and the rate of recognition is traced out based on the total number of correct matches produced for the test datasets with respect to the considered image set trained. cnn with poisson's noise using about 8 datasets achieves accuracy 99.6%"
"at each iteration of the proposed bsamp algorithm, we calculate the maximum backward difference of the elements in t s and denote its position as t. then the nonzero taps can be selected by t, and then regularize this set to improve its accuracy [cit] . in this case, the power of the selected atoms is much larger than that of the unselected ones. the detailed description of the proposed bsamp algorithm is shown in table ii ."
"in an article [cit], authors in their research work has proposed a system for automatic facial sketch to photo matching system. entire work in this article was divided into two broad steps: in the first step, set of facial sketches are constructed. later, in the second step, an automated facial sketch to facial photo corresponding system was built. the proposed article also compares the accuracy between matching photo images of forensic department and the computer generated. in the entire process of implementation, the human based image decomposition is applied on the facial photo and computerized facial sketch after which feature extraction is performed followed by facial component regional weighting and classification. the result is then obtained. facial recognition system is developed to match composite sketches to facial photographs. experimental results were derived for the recognition system using datasets cuhk, feret, tupis achieves accuracies of 80%, 81.5%, 88.5% respectively."
"in their research paper [cit], researchers have proposed a system that aims in knowing a type of deep learning called multi-instance deep learning where regional divisions were discovered for the identification of body part. this frame work aims at 1] discovering the neighbourhood locations 2] learning an image level classifier based on these local regions. the first part of the research in the article aims at learning the classic regional characteristics in a supervised learning style. in the next part of the research article, some selective, information scarce neighbourhood images were captured from the images. finally the run time image categorization was understood adopting a trained convolution neural network (cnn) model. bcnn2 attains the best results among all the other methods with 99.9% recall and precision."
"authors, in their research article [cit] have implemented a methodology that addresses deep hyper sphere embedding for face recognition. this work developed a unique deep learning path for the recognition of faces. according to the proposal, a rendering technique called an angular softmax was implemented that can render good geometric interpretation by constricting the learned characteristics to be selective on a hyper sphere multiform. this technique of connecting the softmax with hyper sphere manifold generated made a softmax a very efficient and a good technique for face recognition in terms of the accuracy. the accuracy of face recognition and verification are 72.73% and 85.56% respectively. these accuracies already surpass most of the existing methods."
"in order to further reduce the complexity of the proposed algorithm, we also adopt svd substitute matrix inversion and the channel autocorrelation matrix can be expressed as"
"where . 0 denotes l 0 norm, i.e., the number of non-zero elements in the channel vector h. 1) sparsity adaptive determination: in this section, the proposed bsamp algorithm is given to adaptively determine the sparsity of the cs algorithm, which can make the clmmse estimation more efficient."
"in their research paper [cit], authors have proposed a technique called joint fine lining in deep neural networks (dnn) used for the system of facial recognition. as per the proposed methodology, technique uses two levels of deep networks. one of the deep networks captures temporary features related to the appearances from the sequences of images were as another one derives geometrical traits. as a result of this, researchers were successful in developing a new integration method for facial recognition. proposed integration system proved better accuracy by yielding better results than the traditional methods implemented prior to this achieving a result of 97.25% accuracy."
"authors in their article [cit], proposed a type of deep learning technique for facial recognition called as \"regularised deep learning\" used for body weight variations which is a natural and intrinsic feature of an individual's process of aging. in this research article, a regularised dependent approach is used to study facial representations corresponding to the variation in weight using two deep learning architectures of different types. 1) a sparse stacked demonizing auto encoder are applied to carry out the training of a complete deep network. later stacking is achieved in an efficient way akin that the generated output of the initial encoder is provided as an input to the next one 2) deep boltzmann machines is used in unsupervised learning for feature representation from the given set. the paper has very efficiently addressed the challenge of obtaining large labelled data for the purpose of training for variations in body weight using a regularised deep learning. the proposed framework with accuracy of 26.0% and of 65.8% for rank1 and ran10 respectively."
"where . 2 denotes l 2 norm, and a s is derived from the observation matrix a by seeking the columns corresponding to s. then the new residual is expressed as"
"in an article [cit], authors have proposed a precise and active face recognition method based on hash coding system. according to the methodology proposed in the article, technique of coding using hash function method and the network of cascaded type are constructed and implemented for the purpose of two step face recognition model. in the first stage, low geometric features and high dimensional features of each of the input image are drawn out in accordance with the various systems used for extraction. in the second stage, the low-dimensional features obtained from the first stage undergo the process of quantization to derive the codes called as hash by making use of a piecewise function. after these two stages, later by the calculation of distance between hash codes, identification is accomplished. contemplating this approach and then trying to compare it with visual geometry group (vgg), the performance of each image of hash-vgg was found to be improved by some milliseconds and the accuracy was increased by 0.7%."
is the estimated channel in frequency-domain form of the ith transmitting antenna. the detailed description of the proposed clmmse algorithm is shown in table i .
"the diagram of the proposed bsamp algorithm in the kth iteration is shown in fig. 4, which combines the speed and ease of implementation of the greedy algorithms with the strong guarantees of the convex program methods. it takes advantages of two major approaches to channel estimation. first, the atoms with the biggest power are picked by setting the proper threshold and finding the boundary position in a h r; second, the regularized method based on convex program during each iteration ensures that the selected atoms are disjoint from those selected from the last one, which further make sure the support is selected correctly."
"omp algorithm, the most representative cs algorithm, is simple to operate and has fast convergence speed. however, the channel sparsity is needed as a priori information in order to recover the original signal accurately. during each iteration process, the atom with the biggest correlation coefficient is selected into the support set, and then ls algorithm is used to calculate the signal and update the residual. the channel state information is recovered after k iteration times."
"in their research article [cit], authors have proposed a system for robust facial recognition technique under varying illumination conditions. in this research article, authors have concentrated on the problems concerning to the textural based illumination handling for the face recognition under both indoor and outdoor lighting conditions. to address the problem, this, authors have implemented a technique that can handle the noise produced with highest recognition rate. a standard histogram equalization technique was adopted to remove illumination from the input facial images. the method is most popular datasets of faces. in the dataset used, some of the front views under changing states were used for evaluation. to extract the features, linear discriminant analysis (lda) and kernel discriminant analysis (kda) were made use to exhibit the front views in low quality. the rate of recognition on each of the database represent that the version corresponding to a kernel of lda gives comparatively more results than plain lda. the result achieved is 7 to 8% high. this particular method achieves an accuracy of about 93% when compared with all the existing methods."
"in an article [cit], research paper concentrated on accomplishing the task of verification of face and the task of individual re-recognition are addressed. both of the tasks under an unconditional atmosphere are complex. this is because, dataset meant for the purpose of testing usually contains characters that are absent in the datasets. hence to escape this complexity, a representation of deep discrimination system is used for learning a model that can cover both untrained and trained representations. from the input data, latent features are extracted. deep discriminative representation learning achieving accuracies of 99.07% and 94.2% on the datasets lfw sets and ytf sets respectively."
"in their research article [cit] entitled \"research on face recognition method based on deep learning in natural environment\" presents a system of time invariant facial recognition using a matching system based on graph. in this research article authors have successfully implemented a graph-based representation system for age invariant face recognition. a simple deterministic algorithm is then used to identify the face of an individual in the contained datasets which also exploits topology of the graphs that is used for the purpose of matching which is considered as the second stage of the overall system. experimental results are extracted on the fgnet dataset. the used technique achieves an accuracy of about 99.94%."
"in this section, we separately give the complexity analysis of the proposed bsamp algorithm and other reconstruction algorithms of cs in terms of iteration times and atoms searching time."
"in an article [cit], authors have proposed the implementation of an evolutionary system of algorithm for composite sketch matching using an approach of transfer learning system. transfer learning approach is majorly used in the methodology which aids in employing various types of sketch images accessible to study matcher of the most relevant type. proposed technique initially performs the operation of preprocessing in which all the images are resized to 192*224 pixels. then in the second step, extraction of various features is later accomplished on the set of images that are pre-processed, as a part of feature extraction, two different types of extractors are used to derive features: him (histogram of image moments) and hog (histogram of oriented gradient). the former consists of moments corresponding to images that caters details pertaining to the information representing the orientation, pixel intensity etc. histogram of oriented gradient was used to accomplish the assignment of detecting pedestrian. methodology developed was considered as an evolutionary algorithm. proposed system was also ably successful in solving the issue pertaining to the computer generated sketch recognition system using a novel evolutionary algorithm. since there are very less composite datasets, other set of artist drawn and images of digital sets were also used. results in this paper produced the accuracy of 34% accuracy for rank 10 of hand drawn sketches and accuracy of 5% for ran1 of computer sketches."
"since the cir of the wireless channel shows sparse characteristics, the maximum number of the nonzero channel taps is small compared with the channel length l and (19) is satisfied."
"first, by utilizing the sparsity of massive mimo channels, a channel estimation algorithm named clmmse is proposed, which gains the channel autocorrelation matrix by cs estimated channel prior information, solving the problem of obtaining the autocorrelation matrix, reducing the complexity of traditional lmmse-based channel estimation, improving the spectrum utilization compared with pure cs estimation for 5g wireless communication systems."
"when the atoms conform to the relevant condition, they will be selected into the support set s. then, the ls method is performed to recover the original signal and update the residual. the estimated signal can be expressed aŝ"
"in this paper, on conducting an exhaustive literature review, we have identified some best and also latest techniques of deep learning particular specifically used in the research articles of facial recognition system which are suited for a type of facial recognition system working better to suffice the requirement of the problem having their own limitations under certain scenarios."
"in their research article [cit] entitled \"deep learning on binary patterns for face recognition\", authors aims in designing a powerful system for face recognition in real time. after pre-processing, number of standard filters were applied. binary patterns are extracted and were supplied as an input into perception of multilayer to carry out the classification of image sets. implemented system was put under the test on various datasets with challenges such as pose variations, occlusions etc. method proposed delivered a better rate of efficiency in the neighbourhood of 91%."
"besides, the proposed bsamp algorithm reduces the iteration times greatly by exploiting the joint sparse characteristics of sub-channels in massive mimo systems. the sparsity adaptive processing is more flexible and it does not rely on fixed step size. during each iteration, at least m atoms can be selected into the support set, which reduces the computational complexity effectively."
"the iteration process of sp algorithm, another representative cs reconstruction algorithm, is very similar to that of omp algorithm. the difference is that the atoms with k biggest correlation coefficients will be selected into the support set during each iteration, which makes it more efficient than omp."
"in an article [cit], authors have presented a technique on how the rotating of face is done utilizing a multi-task deep neural network (m-dnn). this research article presents a innovative system of architecture dependent on multitask system of learning that can earn very large performance in carrying out the rotation of a face image of a target pose. the experiment section consists of 4 parts. in the first part, feature space of each layer is to analyze and study the input. then, the target image is constructed to understand and preserve the identity. in the third stage, multi-model is compared to a single model. finally, experiment is conducted to show the advantages. the input image is reconstructed after the completion of first task model rotates an image to a particular pose. in the recognition task in connection to the random poses and the factors of illuminations, the model presented provably wins against any of the previous traditional techniques more than 4∼6%."
"in a research article [cit] entitled \"matching software-generated sketches to face photos with a very deep cnn, morphed faces, and transfer learning \", authors have proposed a technique that aims at matching software generated sketches to ace photos using deep cnn, faces that are morphed and an approach of transfer learning. for the purpose of synthesizing both photographs and corresponding sketches, a model called \"3d morphable\" is applied. apart from that, vom-sgfs database is extended to consist of a greater number of subjects. [cit] this presented article also anticipated to hook the problems by using the consecutive offerings : (i) deep-cnn was made used to compute the similarities of a subject in a computer generated sketch by correlating it with facial photographs, later then training is performed by using an approach of transfer learning to a model pre-trained for face recognition of face-photo, (ii) a model called \"3d morphable\" was applied to generate photos and computer generated sketches for the purpose of the augmentation on the set of training data that is reachable and (iii) standard \"uom-sgfs\" datasets is expanded to accommodate double the amount of sets after which contains 1200 sets of sketches corresponding to 600 subject sets. results then proved the retrieval rate having an efficiency of above 90% for rank100."
"where α 2 l is the power of the lth path tap and τ l is the lth path delay. based on (7), we obtain the prior information by cs-based initial channel estimation in this paper to improve the r hh calculation efficiency, which is given in the following part."
"in their research article [cit], authors have implemented a type of feature being driven using the concept of deep neural network (dnn), basically learning method for various views of the facial expression. in the proposed technique, scale invariant feature transform (sift) characteristics, analogous to a fixed number of notable regions of every face photo were initially captured from every input considered. the proposed model also employs multiple layers to character which maps the correlation between the semantic information of the sift vectors and the sift vectors themselves. experimental results are extracted out of bu-3dfe and multi-pie facial recognition database containing various facial expressions. the accuracy may be different with different methods. however, if the augmented samples are used, this deep neural network can attain the precision of about 85.2%. this is the highest among these methods."
"in their research article [cit], authors have presented a novel technique to extract two image feature descriptor from components used for sketches to photo matching. in this article authors have initially focused on cropping and aligning the face sketches and photos together. later based on the built-in attributes of composite sketches, scale invariant feature transform (sift) features and histogram of gradient (hog) characteristics were drawn out against the images. the final outcomes of the recommended approach applied using e-prip database exhibited better performance. this technique achieved a result with accuracy 70.1%."
"according to the channel autocorrelation property, highly correlated elements have a great impact on the performance of channel estimation, while the effect from lowly correlated elements is slight. hence, r hh can be approximately expressed in terms of"
"in their research article [cit], authors have presented an active face recognition model using deep learning based linear discriminant classification (ldc). proposed system was implemented using a matrix formed out of the database that corresponds to facial system comprising of columns and rows. it also enhanced the accuracy of the ldrc by keeping a record of the previously trained faces. the suggested method produced improved recognition and separation of biometric face images than the other algorithms built earlier. 92.8% accuracy was accomplished on yale face datasets by implementing this algorithm."
"in (5), traditional lmmse algorithm gets the channel autocorrelation matrix r hh on the assumption that the power of the multipath channel taps obeys negative exponential distribution and that the elements in r hh can be calculated as"
"face recognition techniques have taken a drift undoubtedly over the years. traditional methods were dependent on features, like descriptors, combined with machine learning techniques, such as principal component analysis (pca), support vector machines svm) etc. huge variations in facial features motivated research community to implement some specific systems to handle the variations. specially designed systems such as time invariant systems, variation in pose factors and variation in illumination factors etc. were made use. currently, models developed for face recognition systems are being reinstated and influenced by deep learning (dl) methods based on cnns (convolutional neural networks) [cit] . the huge plus point of using these methodologies is that they are learnable through training process of huge datasets simply reachable over the internet. apart from this efficiency and performance of the developed systems using deep learning concepts is relatively higher than the methods implemented using traditional techniques [cit] . additionally, cnns can also solve broader range of challenging computer vision problems. considering the a fore mentioned facts, in this paper we have conducted a comprehensive review on the deep learning techniques that are of late used in the field of facial recognition system under number of applications and number of domains [cit] ."
"on carrying out an exhaustive literature review, we have identified the outcomes of the various articles in terms of the deep learning technique used, their limitations and the accuracy rate of the system for various datasets used in the article. table1. presents the overview of different deep learning techniques used and their corresponding limitations identified in each of the article. can only be applied for general recognition of images. [cit]"
"in an article [cit], authors have identified the problems existing in the area of forged face images and the aspects that may lead to the variations during the face recognition like noise, deviations in the angle etc. are the primary reasons for the system to lose its transcendence. in this particular method, deep learning was introduced in the sense of providing thorough study about the face patterns existing in the system. the training database matrix and the facial images consisting of columns and rows are present. the input images can be reconstructed. errors or alterations arising because of the reorganization in each of the classes in specifically noted. further, the data projection matrix is determined by gauging every face image to its corresponding substitute space that is learned outlearned subspace. the proposed method accepts to evaluate every sample existing in the system. further, it keeps the record of data from the facial characteristics and performs classification. it was observed that 92.8% accuracy was gained on yale face datasets by implementing the suggested algorithm."
"proposed technique of deep learning is implemented using a convolution neural network (cnn) which is one of the most effective techniques in the field of deep learning. proposed methodology works under two different steps once after preparing the datasets required for the experiment: 1) in the first step, batch size for the cnn is fixed in terms of the number of input sets supplied. 2) in the second step, the technique concentrated on improving the classification of the models. as a part of improvement, proposed technique focused on repetition of training of cnn architecture to fine tune the initial model. this method accomplishes the best results on ck+ database with an accuracy of 71.04%."
"where r(i) is the ith element of r. then sort t in descending order to obtain t s, and denote the corresponding permutation vector by s 1 ."
"sketch recognition system up late evolved to be the next level of facial recognition system in forensic sciences and law enforcement agencies in solving crime related issues. sketches are basically classified into three different types: 1) viewed sketch that is generated by an artist looking to the subjects face or photograph. 2) forensic sketch is another type, that is generated by the trained artist based on the descriptions given by the witness of the crime scene 3) composite sketches are the computer-generated sketches which are generated by using software programs. in the current scenario, computer system produced sketches are being chosen since they proved to be faster to construct and also accurate when compared to the traditional hand drawn sketches. researchers have again proved that, matching computer generated sketches to facial photo images is also a difficult work as these sketches being generated and corresponding photos belongs to two different domains and also composite sketches may lack many of the tiny descriptions that are actually found in the photograph. research paper [cit] gives description about technique used in matching composite images to human faces using transfer learning. with the advent of this technology, these techniques algorithms are utilized in several applications and well-being agendas as well as law administration systems like the security forces of the border and other systems related to forensic science."
"equation (4) can be solved by cs exploiting the joint sparse characteristics of h. let both sides of (4) multiply a h, where a h is the conjugate transpose of matrix a, then"
"in this section, we give the intact diagram of the proposed bsamp-based clmmse algorithm stated above, as shown in fig. 2, and compare its complexity with the traditional lmmse algorithm and the omp (typical cs algorithm with low computational complexity) based clmmse algorithm, to indicate the implementation difficulty, in terms of times of multiplication and addition operation."
"the external efforts acting on the subject during the tasks were the grf&m and, when a load was carried, the lcf&m. the proposed method was composed of three successive steps:"
"an appropriate visualization method will be beneficial for lead optimization. some deep learning-based scoring functions, like densefs that uses 3d cnn [cit], are rather cumbersome in explaining the results of the these four targets were randomly selected and belong to four different protein families: aa2ar (adenosine a2a receptor, gpcr), cdk2 (cyclin-dependent kinase 2, kinase), esr1 (estrogen receptor alpha, nuclear receptor), and dpp4 (dipeptidyl peptidase iv, protease). we showed the contribution of every ligand (or protein) atom to binding by coloring each atom different shades of red. given a protein-ligand complex, the score for each atom pair could be calculated through eq. 2-2 under a certain model. the contribution of an atom was equivalent to the sum of the scores of all atom pairs involving this atom. all of the ligand and protein atoms were initially painted dark gray. then, atoms that contributed positively would be painted different shades of red, and the color of atoms with negative contributions would not change. the atom with the highest positive score in ligand/ protein would be painted in the deepest red. the shades of the red of other atoms indicated the relative magnitude of the contribution of the atom to the contribution of the atom colored deepest red. we randomly selected a positive ligand for each target and analyzed the binding mode of the ligand to the target using above coloring strategy."
"in order to ensure the correct connection of the hardware circuit, power is supplied to the digital signal generator and sequence analysis device respectively, the oscilloscope is opened and set to x-y mode, and the 8-bit effective single-stage trigger word tw is preset through the dial-code switch [cit] .adjust the potentiometer, manually shift the time indicator line on the screen, and the led light displays the indicator line corresponding to the 8-channel digital signal status word sw; button a sets three trigger time positions; button b toggles and displays 2 cycles of 8-channel timing waveforms and normal state timing waveforms stored for playback."
"considering new links, the pair activity extrapolation alone is not able to predict this kind of interactions, and thus yields a null f-score. however, we can see that the f-score associated with the new links is almost constant for a wide range of α when the influence of the number of common neighbors is predominant in the prediction. this behavior is due to the low number of new links appearing during t : as α grows, less and less links are attributed to these pairs. however, as the number of new links actually appearing is much smaller than the total number of predicted links, both the numbers of true positive and false negative remain almost constant. therefore, the recall is also nearly constant. the precision slightly improves as α grows as the number of false positives decreases. when the number of true and false positive are close, it leads to an increase of the f-score corresponding to the prediction of new links for values of α 0.9. similarly, the performance of the prediction of recurrent links improves as more weight is given to the extrapolation of previous pair activity. however, as the number of links appearing between the pairs of nodes of this category is more significant, we do not see the same effect of stagnation for a wide range of α."
"moreover, pre-grip phases are common in handling tasks. in these phases (corresponding to the uncertainty phases in this paper), the motion-based prediction method is not efficient. added sensors seems necessary if pre-grips are specifically studied."
"we compared our results with two previous similar studies to check if our model showed better performance. [cit] . they used 36 targets to train and test their model, so we selected the scores of overlapped targets to make comparison. the results are shown in table 3 and figure 6 . table 3 clearly indicates that deepscore performed better than pleic-svm. the average roc-auc, roc0.5%, roc%1, roc2%, and roc5% (roc10% of pleic-svm was not provided) for all 36 targets increased from 0.93, 0.58, 0.64, 0.69, and 0.77 to 0.98, 0.78, 0.85, 0.89, and 0.94, respectively, by using deepscore. among these metrics, roc0.5% is the most important one since the early enrichment ability of scoring functions is paid more attention in the context of virtual screening. figure 6 shows that deepscore outperforms on most of the targets on roc0.5%. on some targets, such as fnta, the improvement was dramatic (for fnta, roc0.5% increased from 0.31 to 0.92 by using deepscore). however, for gcr, cdk2, bace1, and prgr, deepscore only got a similar or slightly worse performance."
"second, the use of machine learning methods was proposed on static posture, gait and asymmetric movements [cit] . the estimation of grf&m for one side was carried out with a neural network and by using the dynamics equations on the other side. on the same basis, a set of sidestep motions was used to establish a link between the motion and the grf&m [cit] . then, the use of a regression method allowed the grf&m to be predicted using only the motion. these methods, based on a learning phase, have the disadvantage to require a considerable set of motions representative to the studied one and thus are only applicable to standardized tasks."
"a link stream (see figure 1 ) is a sequence of triplets (t, u, v), each triplet indicating that an interaction occurred between u and v at time t. many real world datasets can be modeled and analyzed using link streams, such as email exchanges, contacts between individuals, phone calls or ip traffic [cit] . there have been other attempts to model these systems, like dynamical networks [cit] or time varying graphs [cit], which hold the same information as link streams. analyzing the dynamical and structural properties of these link streams is capital to apprehend the behavior of the system, as it allows to understand the underlying phenomena in the data."
"13 male participants (age: 27 ± 7 years old, height: 177 ± 4 cm, mass: 73 ± 15 kg) participated in this study. all subjects signed an informed consent form before participation and an anonymization protocol was followed for data safeguarding. this study has been approved by a national ethics committee (comité [cit] -a00484-51). the task required to carry a load from one area to another. a set of three elementary movements was consider as a cycle. the first one was from area 1 to area 2, the second one from area 2 to area 3 and the third one from area 3 to area 1 ( figure 1 ). the subject was asked to carry the load with the handles at a self-selected speed. the subject was also free to lift and move his feet as long as each foot stayed in contact with the associated platform (further details about the force platforms in the next section). between every elementary movement, the subject went back to the initial position."
"the external efforts prediction method proposed in this study used only the position of the subject markers as experimental data. it also required additional parameters as the size and the mass of the subject and the inertial parameters of the load. the validation of the method used the position of the markers on the load, the grf&m measured by the two force platforms, the lcf&m measured by the 6-axis force sensor and the video of the experiments."
"finally, the current method estimated the grf&m and the lcf during asymmetric handling tasks with a small margin of error, and with the subject motion as a unique source of experimental data. this estimation could be used to compute kinetics variables as back loading with an acceptable error. thanks to this method, inverse dynamics studies are not limited anymore to motions where the body is in contact with force sensors to estimate the external forces. it may be extremely useful in work tasks assessment or sport gesture analyses, opening a wide range of applications. it indicates that we can develop and analyze more complex experimental protocols with a higher level of generalizability to real tasks in a motion analysis lab."
"first, empirical functions were used to distribute the weight under the feet [cit] . a method based on the zero moment point was proposed to predict the point of application acting of each foot [cit] . this approach is very simple to apply but is limited to the grf&m prediction on gait motion."
"we can observe that the f-score remains relatively low in this type of predictions. the difficulty of this task is mainly due to the class imbalance problem [cit] . given the small number of links occurring compared to the number of pairs of nodes considered, it is a known difficulty in many real world datasets. as expected, this appears more clearly when predicting new links due to the larger number of pairs involved compared to the recurring links."
"the whole test includes hardware and software test.the hardware part is connected to the timing analysis device by the 8-digit digital signal output end and a clock signal.the control panel includes potentiometer, key, dial-code switch, led light and is connected to the timing analysis unit.the timing analysis device leads out x, y and gnd terminals and connects oscilloscope x and y channels respectively.use multimeter, oscilloscope and other instruments to test the hardware connection.after testing, the hardware connection is intact and the modules work normally.the software part displays the result on the built hardware platform, carries on the program adjustment and the revision."
"it is then normalized to the sum of all γ x,y for all pairs of nodes x, y in the stream. we allocate the n links estimated previously proportionally to the normalized pair apparition score to get n u,v, the number of interactions predicted for any pair (u, v):"
"the digital signal timing analysis device and main control module are based on stm32f407vgt6 microcontroller. the microcontroller has the advantages of high integration, flash memory on 1 m byte chip, sram of 192 k bytes, reset, internal rc, pll and so on. in addition, the device also includes art accelerator, 32-bit 7-layer ahb bus matrix, multi-dma controller and so on."
"we focus on the activity prediction problem, i.e. predicting the number of links appearing between each pair of nodes during a given period of time. while this problem shares properties with the more usual link prediction problem, it is also quite different in the sense that we aim at predicting not only who interacts with who, but also when."
"in general, people rely on oscilloscope for signal analysis, and the understanding of multi-channel digital signal analysis lacks the support of hardware platform. at present, most simple logic analysis instruments adopt single-chip microcomputer +fpga mode to realize data display through tft, which not only increases the product cost but also fails to fully apply the display function of oscilloscope. therefore, this design uses oscilloscope x-y mode to do signal input, design 8 channels of digital signal storage, display, playback, arbitrary cursor shift and other functions. a simple multichannel timing signal analysis device is implemented. compared with conventional logic analysis instrument, this device has the advantages of simplicity, low cost, high utilization rate, high accuracy and strong application. the overall design scheme is shown in figure 1."
"third, a contact model with optimization techniques was used to solve the indeterminacy [cit] . the method added a set of artificial muscle-like actuators under each foot of a musculoskeletal model. these additional actuators were considered in a similar way as the muscles of the model in the muscle recruitment algorithm. the optimization problem found muscle forces which minimize a cost function by satisfying the dynamics equations. the grf&m corresponded to the forces contained in the artificial muscle-like actuators. a validation on activities of daily living and on sports-related movements was done [cit] . however, this approach was not validated on handling tasks and was limited to the estimation of grf&m."
"in one case, we only take into account the activity during a fixed period of time: for each pair of nodes, we compute the function a δ, (u,v)"
"thus, the aim of the current paper is to develop a prediction method of the grf&m and the lcf based only on the subject motion during asymmetric handling tasks. this method estimates kinetics variables such as back loading during handling tasks based only on subject motion, improving significantly the ecological aspect of the experiments. the method proposes an estimation of the grf&m and lcf based on optoelectronical motion capture data. prior to this estimation, a step of identification of grip and deposit events and a step of load motion reconstruction are applied. the validation consists in, first, comparing the predicted grf&m and the lcf to the measured ones and second, comparing predicted back loading (evaluated as the l5/s1 joint moments) to those computed from measured data. the results are detailed and discussed at the end of the paper."
our work differs from these methods by focusing on both the dynamic and the structural aspect of the data while avoiding the information loss induced by the use of time windows. we introduce a protocol that combines these information sources and allows a fine temporal resolution. we use this to predict both new and repeated links in the stream.
"these experiments highlight the fact that the metric combination does not have the same impact depending on the dataset considered. while each metric tends to predict preferentially a specific type of activity on the highschool dataset, this is not the case in the infocom dataset, where our structural metric is able to predict both new and recurrent links. it also points out the fact that, by choosing specific metrics combination, the prediction can be focused on different kinds of activity, involving different kinds of links."
"given such a prediction function, a standard prediction method consists in learning on a training period the α i values which optimize a given evaluation criterion. then these weights are used for the actual prediction. in the following, we explore the influence of the weights on cases where the sum has two terms, we do not focus on a specific learning method which is left for future works."
"training five-fold cross validation test was performed on each target in dud-e. for each target, the whole data set was split into five parts at first. within each fold, three parts were used as training set, one part as validation set, and one part as test set. to make it fair, the performance of gscore was also calculated in the same way. it should be noticed that there existed a dramatic class imbalance in our data sets as the number of decoys was almost 50 times of that of actives. to overcome this problem, we adopted the random undersampling strategy. over an epoch, we did not use the whole training set to train the model. instead, parts of decoys were randomly selected out to make sure that the number of actives and decoys was the same in an epoch. the reason why we chose undersampling was that, compared with other methods like oversampling, the training procedure using this strategy was significantly faster."
"in order to accurately display the multi-channel digital timing signals on the oscilloscope display screen, the basic requirements such as standard test signal acquisition and manual trigger condition setting must be met first. in this process, the two-channel d/a conversion circuit is precisely controlled, and two channels of analog signals are output to the x-y channel of the oscilloscope, and the oscilloscope screen is scanned quickly and orderly, and the multi-channel signals are displayed by using the afterglow effect of the ratio. therefore, reasonable function module setting and control become the key [cit] ."
"the first step is to generate docking poses for actives and decoys. we noticed that, in other similar work, a variety of docking methods were used in this step, including glide [cit], autodock vina [cit], dock [cit], plants [cit], and so on. even using the same docking program, sometimes different docking protocols were adopted [cit] . it should be emphasized that, strictly speaking, only the rescoring results from the same docking poses are comparable."
"when the correlation between the statistical errors of multiple models is low, combining the predicted values of these models in a certain way usually performs better than any single one model. this is the basic idea of ensemble learning [cit] . we adopted this strategy and used eq. 4 to calculate deepscorecs for a complex. in eq. 4, c is a coefficient that can be adjusted. more details will be showed and discussed in results and discussion part."
"by considering all the motions (156 elementary movements), 8 predicted grip or deposit events were outside the uncertainty areas (the average duration of the grip uncertainty period is 0.33 s (0.15) and 0.46 s (0.18) for the deposit uncertainty period). it corresponds to 2.5% of the 312 events to predict. for these 8 events, the interval between the predicted event and the uncertainty area was averaged to 0.04 s (±0.03). this error is low compared to the duration of the studied movements (between 4.8 and 10 seconds), which means that these events were close to the uncertainty area."
"in this work, we introduced a novel strategy for training targetspecific protein-ligand scoring functions used for structurebased virtual screening. the model outperformed glide gscore significantly and made progress with respect to some metrics compared with traditional machine learning-based models. these results demonstrate that our model is able to further improve the screening effect by rescoring docking poses generated from docking software. there still remains more space for improving deepscore. like pmf scoring function, energy terms were treated implicitly in deepscore, which made the model more difficult to capture important protein-ligand interactions. the cutoff distance we chose may be too short, causing long-range interactions not to be captured. however, on the other side, during the experiment, we found that a larger cutoff distance would significantly increase the noise and calculation cost. the most valuable aspect of deepscore is that it represents a novel atom-pair-based machine learning scoring strategy. with the deeper integration of deep learning and chemical informatics, we believe that deep learningbased scoring functions will further develop in the future."
"in the current study, the prediction was based on several parameters as the number of contact points, the thresholds distance and velocity, the maximal force available at each point, the friction coefficient between the feet and the ground and the standard deviation used in the neural network. these parameters were inspired by the literature and adapted for this work. even if this set of parameters give satisfactory results, a parametric study may be useful to improve the prediction results and increase the robustness of the method. in this spirit, [cit] conducted a sensitivity analysis on the thresholds distance and velocity and the maximal force available at each point. it consisted in varying the different values at 70% and 130% of their original magnitude and observing the prediction results. such an approach may be applied to the current study. in the same manner, the locations of the contact points have an impact on the method. in this study, for each foot and for each hand, all of the contact points of a given solid were included in a unique plane (corresponding to a flat foot and an extended hand). an evolution of these contact points definitions -adding degrees of freedom or following more accurately the osteoarticular structure, could probably improve the contact model."
"thus we can compute more sophisticated performance measurements: the precision t p t p +f p, the recall t p t p +f n . we also use the f-score to quantify the quality of prediction, which is the harmonic mean of these two indicators: 2 · precision·recall precision+recall . other indicators could be defined in this context, like the roc curve, but we do not use them in this study."
"the identification of grip and deposit events contained a learning and an application phase. the method was validated with a leave-oneout cross validation. for each validation, all movements of one subject were excluded from the learning phase. it enabled to test if the method could be applied on a new subject (no participation to a previous study)."
"when the subject did not carry the load, the prediction method was a classical grf&m prediction method from the equations of motion applied to the subject. thus, at each instant, the external efforts were the solution of the optimization problem (1), solved with a sequential quadratic programming (sqp) method."
"finally, bsip were here based on anthropometric tables. these parameters influence directly the external efforts estimation, thus a subject-specific estimation of the bsip should improve their prediction [cit] (particularly with subjects far from standard anthropometrics). calibration techniques based on classical devices available in any motion analysis laboratory were developed to improve the bsip estimation by using forces platforms [cit] or image-based techniques [cit] . such methods could be associated to the external efforts prediction method to enhance its efficiency."
"deep learning models are usually regarded as black boxes since the information of which features that are important can hardly be interpreted from the model. although cnn based scoring functions, like pafnucy from which the atom features of deepscore were borrowed, have achieved state-of-the-art performance in benchmark test, and become the representative of deep learning-based scoring functions, treating the whole protein-ligand complex as a 3d picture is still counterintuitive. thus, in consideration of interpretation, we chose to reform the classic pmf scoring function. the neural network in deepscore is only used to facilitate the learning of atom-pair potentials; meanwhile, the overall framework of pmf scoring function is preserved. deepscore is able to directly give the score of each atom pair, which makes the model's output easy to explain. to the best of our knowledge, deepscore is the first model to use this framework."
"prediction of external efforts with other contacts than feet on the ground was applied to sit-to-stand tasks [cit] . the contacts were feet, hands and buttocks. on a planar study, each of the 3 contact points was associated to a force and a moment. based on the same idea as defined before, an optimization problem computed the external efforts by minimizing a cost function and by satisfying the dynamics equations. however, this work was limited to a 2d study. handling tasks composed of a large part of asymmetric movements should be analyzed with a 3d study. elsewhere, load contact forces and moments (lcf&m) were predicted based on the motion during symmetrical lifting tasks [cit] . however, grf&m were measured through force platforms in this study."
"concerning the uncertainty phases, higher errors appeared (particularly visible on the vertical grf under the right foot (figure 6) ). in this phase, the subject had indirectly a contact with the ground or the table where the load was placed. this contact was not taken into account in the proposed method, generating prediction errors. moreover, the low values of the measured sagittal lcm indicates that there was very few load rotation around the axis defined by the two hands. this seems to confirm the assumption made for the load motion reconstruction."
"the 8-bit digital signal generator circuit is composed of the minimum system board, voltage stabilizing module, usb module, io port, crystal oscillator, reset and power supply module with stm32f103 as the main chip, realizing a total of 8 digital signals from d0 to d7.the principle is shown in figure 3. digital signal timing analysis device design. digital signal timing analysis device is composed of stm32f103 minimum system board, voltage regulator module, usb module, io port, crystal oscillator, reset and power supply module to realize the display of digital signal to oscilloscope [cit] . the circuit principle is shown in figure 4 ."
the chosen descriptors of the grip and deposit moments are therefore satisfactory and the method seems relevant and robust to get the current state of the task (carrying the load or not).
"the proposed method was based on the subject motion obtained by an optoelectronical motion capture system. however, this kind of system is mostly available in motion analysis laboratory. in the ideal case, ergonomics studies are performed directly in the workplace. to this end, wearable motion capture systems may be used. inertial motion capture system (as the most commonly used xsens [cit] ) permits already to evaluate the handler kinematics [cit] . depth cameras are also proposed for workstation ergonomic postures assessments [cit] . in the laboratory, grf&m are predicted by using these wearable systems by using empirical functions [cit] or by using a contact models on walking motions [cit] . the method proposed in this paper could be extended to wearable motion capture systems and thus be used to evaluate kinetics data in the workplace."
"where a u,v is the function associated to the pair activity extrapolation (see iv-a2), which is considered here as a benchmark to compare with the performance our combination method, f u,v is a function corresponding to one of the other metrics presented."
"in deep learning processes, the usual practice while dealing with a two possible classification problem is to put two units in the output layer and transform the outputs using softmax function. the outputs, which represent the probability of classes 0 and 1, respectively, are then used for calculating the loss with cross entropy loss or other loss functions. however, in practice, we found that the cross entropy loss function did not apply to our model very well. we tried some other loss functions and found that modified huber loss (eq. 3) [cit] ) was more suitable. the formula of modified huber loss is shown in eq. 3, where f(x) refers to the output of the model and y refers to the label (1 for actives and -1 for decoys). it should be noted that, unlike general scoring functions, the possible scoring range of deepscore is the entire real number filed. a score greater than zero indicates that the model considers the compound to be active, whereas a score less than zero is inactive. another important point is that scores between different targets are not comparable."
"after the test, can be displayed on the oscilloscope at the same time 8 road digital signal sequence, can adjust 8-way digital signal timing the triggering time of the waveform, can choose different trigger mode, can be added on the screen can be manual displacement time marker, with eight lines corresponding led display time 8 digital signals, to 8 road logic state of the digital signal acquisition and storage, and on the oscilloscope playback two displacement cycle 8-way digital signal [cit] . the physical diagram of the system test is shown in figure 6 . through testing for many times, can be displayed on the oscilloscope and 8 digital signal sequence, can adjust 8-way digital signal timing the triggering time of the waveform, can choose different trigger mode, can add the manual displacement time marker on the screen, with eight led display time line 8 digital signals of corresponding time status, to 8 road logic state of the digital signal acquisition and storage, and on the oscilloscope playback two displacement cycle 8-way digital signal. however, there are some individual display waveforms and manual cursor display that cannot achieve absolute \"horizontal, horizontal and vertical\", because the d/a conversion output has a certain drift, and the d/a conversion output cannot achieve absolute linearity [cit] ."
"the results of the current study showed that a prediction method based on a contact model allowed external efforts to be predicted on asymmetric handling tasks with only the subject motion. grf&m, lcf and l5/s1 joint moments were estimated with a small margin of error. the prediction method enabled to quantitatively assess handling tasks on the basis of kinetics variables without additional instrumentation as force sensors. the workers follow-up could thus be improved by assessing the ergonomics of simulated tasks more representative of those performed in the workplace."
"through data preparation step, the best poses ranked by gscore were selected for actives and decoys. to rescore the docking poses from glide, we utilized the form of the potential of meanforce (pmf) scoring function [cit] to calculate the score for each protein-ligand complex. in pmf scoring function, the score for a complex is defined as the sum of overall protein-ligand atom pair-wise interactions within a specific cutoff radius:"
"motion decomposition: to facilitate the results analysis, each movement was separated into elementary handling motions, which were divided in three phases: the subject was not in contact with the load, the subject carried the load and the subject was in contact with the load but without completely carrying it. as the method is less efficient in this latter phase due to the lack of information about contacts at the hands, it is defined as the uncertainty phase thereafter. the different periods were identified with a video analysis of the tasks by finding the instants of the first and the last contact between the subject and the load and the instants of the first and the last contact between the load and the ground. the reference grip and deposit events (used in the learning phase of the neural network) were defined as the middle of the associated uncertainty phase."
"here, we introduce a deep learning-based method named deepscore used for constructing tssfs. the purpose of deepscore is rescoring the docking poses generated from docking software like glide. deepscore uses the scoring model of pmf scoring function, where the score for a protein-ligand complex is derived from the sum of protein-ligand atom pairwise interactions within a distance range. the score for a single protein-ligand atom pair is calculated using a fully connected neural network. since consensus scoring methods have shown to be useful in improving the performance considering the results from several different models [cit], we further proposed deepscorecs by combining the results of deepscore and glide gscore together. the directory of useful decoys-enhanced (dud-e) was used as the benchmark to quantitatively assess the model. 12 metrics were calculated and used for making comparison between gscore, deepscore, deepscorecs, and some other tssf models reported by previous studies."
"it is also possible to approach link prediction by focusing on the dynamical aspects of the link apparition between two nodes rather than on the structural properties. the sequence of links between each pair of nodes is then considered as a time series and numerous tools have been developed in this field to predict the future behavior of the system. for example it is possible to focus on the link apparition frequency in the past to predict future interactions [cit] . this approach focuses on predicting the future occurrences of links that have appeared in the past. as such, it is complementary to link prediction in graphs."
"then, we define three other metrics that describe more precisely the temporal behaviors of the system. the first two are adapted versions of the pair activity extrapolation that focus on the most recent activity during the observation period. this choice is made on the ground that the most recent interactions affect more the dynamics than the old ones do."
"our model was implemented using pytorch 1.0 (https:// pytorch.org/) in python. each model was trained using adam optimizer with a batch size of 32, a learning rate of 0.001, and a weight decay of 0.001."
"in the application phase, for a cycle containing one grip and one deposit event, the probability of a grip or deposit event at each instant was evaluated from the positions of markers on each hand with the neural network. the resulting curve was then filtered with a moving average of 100 frames (corresponding to 0.5 second). the grip and deposit events were thus identified as the frames corresponding to the 2 peaks with the highest prominence."
"the figure 5 shows a representative example of an identification of grip and deposit events for one elementary movement. the orange areas corresponded to uncertainty phases where the subject was in contact with the load but without completely carrying it were identified with the video. inside these phases the external forces prediction with the proposed method was not reliable. therefore, it was necessary to detect the grip/deposit instants inside these phases. on the presented example, the grip and deposit events predicted by the neural network were consistent with the reference. in particular, all of them were inside the uncertainty phases."
"above results have shown that deepscore works well with the docking poses generated from glide. to examine whether [cit] deepscore is sensitive to docking program, we regenerated all ligand poses using autodock vina [cit] and repeated the above process. roc-efs of test results were calculated and shown in tables s4 and s5 to quantitatively assess the influence of changing docking program on the virtual screening ability of deepscore. obvious differences can be observed on some targets in table s5 . for example, deepscore-adv (autodock vina) achieved a roc-ef0.5% of 160.65 on hs90a which represented an improvement of 37.01% over the roc-ef0.5% achieved by . but on plk1, roc-ef0.5% dropped by 60.61 (deepscore-adv 84.76 vs. deepscore-glide 145.37). generally speaking, deepscore-adv got a similar performance with deepscoreglide in terms of mean values (see table s4 ). it can be concluded that the screening ability of deepscore is robust and insensitive to the docking program used, on the premise that the docking program can provide reliable docking poses."
"dipeptidyl peptidase-iv (dpp4) inhibitors are used for treating diabetes mellitus. according to a recent review about dpp4 inhibitors, glu205, glu206, and tyr662 in dpp4 are believed to be the most import anchor points helping inhibitors recognize dpp iv. since we used different protein with [cit], for the convenience of comparison, we performed sequence alignment and renumbered all residues so that the residue number we used could match [cit] . in figure 10, it can be seen that deepscore also favored these three residues and gave them fairly high scores."
"in this section, we evaluate the performance of our framework using two datasets which are both real-world contact data between individuals, captured with sensors. we investigate different metrics combinations as well as the influence of the observation period duration. we aim to identify the strengths of our protocol as well as the main challenges to improve the prediction to guide our future works."
"nverse dynamics is a classical analysis tool used to study work tasks at a biomechanical level. in particular handling tasks are one of the core tasks to study in workplace ergonomics since this is one of the most constraining motion to realize in a work situation [cit] . indeed, these tasks are particularly prone to musculoskeletal disorders. in such tasks, the studied biomechanical variables are based on the kinematics (for example the postural asymmetries or the lumbar spine flexion) and based on the kinetics (for example forces and moments in the lumbar spine [cit] . in addition to motion capture data used to compute the kinematics variables, estimation of joint moments and muscle forces requires the knowledge of the contact forces. most of the studies concerning handling tasks are based on a bottom-up approach using force platforms to measure the ground reaction forces and moments (grf&m). the use of such devices affects the ecological aspect of the tasks by constraining the field of acquisition to the force platforms. similarly, the instrumentation of the loads with force sensors limits the tasks to be studied since the subject must only use instrumented handles."
"structure-based drug design (sbdd) has been widely used in industry and academia [cit] . there are three main categories of tasks for sbdd methods: virtual screening, de novo drug design, and ligand optimization. virtual screening generally refers to the process of identifying active compounds among molecules selected from a virtual compound library. by utilizing the three-dimensional information of proteins, structure-based virtual screening is believed to be more efficient than traditional virtual screening methods. the key factor for guaranteeing the success of structure-based virtual screening is the quality of scoring functions. theoretically, a scoring function is capable of predicting the binding affinity of a protein-ligand complex structure, and thus can be used for predicting the binding pose of a ligand or screening a virtual compound library to find potential active compounds."
"these observations indicate that in both experiments, combining a temporal metric with a structural metric may lead to an improvement of the f-score. in these experiments the improvements remain relatively small, from 4.9% to 8.8%, because of the simplicity of the metrics chosen, but it shows that our protocol is able to draw benefit from the combination of temporal and structural information."
"xl and mz designed the study and are responsible for the integrity of the manuscript. dw, xd, and cc performed the analysis and all calculations. dw mainly wrote the manuscript. zx contributed to data processing. hj and kc gave conceptual advice. all authors discussed and commented on the manuscript."
"the knowledge of the grip and deposit events was crucial since it allowed to distinguish phases where external efforts were applied on the hands or not. a statistical method based on a neural network has been used to evaluate, at any time, the probability to have a grip or a deposit event."
"in both cases, we observe that the two partial activity extrapolation metrics (a δ and a k ) provide information complementary to the benchmark, as they perform better when combined with it. this can be interpreted as a better balance between the short and long term dynamics. on the other hand, the activity fit seems to be systematically an improvement to our benchmark but the prediction does not benefit from the combination. this tells us that combining different temporal metrics can improve our prediction performance. it also shows that combining a variety of temporal metrics focusing on specific dynamical properties allows to control the weight of each of these properties in the prediction."
1) detection of grip and deposit events to identify if the load was carried by the subject or not; 2) load motion reconstruction between a grip and a deposit event; 3) external efforts prediction based on the equations of motion applied to the subject or simultaneously to the subject and the load. the first two steps used only the positions of one marker on each hand. the third step used all the markers located on the subject.
"as our method aims to predict a different object from link prediction methods in graphs, we have to define another way to evaluate the efficiency of our protocol. nevertheless, the evaluation method defined here aims to stay as close as possible to the tools used in classification tasks, therefore allowing to compare our method to other prediction algorithms. we adapt the usual definition of true positives, false positives and false negatives to the context of activity prediction in link streams. precisely, for each pair (u, v), we compare n u,v, the number of links predicted, to n u,v, the number of links that have actually occurred between a and ω (see figure 2) . we then define the number of tp, fp and fn as follows:"
"the digital signal timing analysis device can accurately collect complete timing waveform, and the internal high-speed computing unit can process the signal in real time. because the oscilloscope x-y mode waveform display requires high frequency of input signal, low-frequency signal is easy to cause distortion of display graph. therefore, stm32f407vgt6 microcontroller conforms to the main frequency requirements of the system, and can well collect and capture the input signal and make the corresponding output control. its internal high-speed da conversion analog output can ensure that the graph input to the oscilloscope (x-y mode) changes continuously without distortion, and the whole control system can run stably."
"2) load motion reconstruction: the use of the equations of motion applied to the load required the knowledge of its motion. with a prospective view to assess handling tasks as close as possible that it is done in workplace, this measurement with markers would be very difficult. indeed, handlers carry out several stacked loads which prevent the use of markers. in this view, the method was based only on the subject motion. the load markers were used for the validation. the load motion was reconstructed from the positions of one marker on the right hand and one marker on the left hand (the same as those used for the identification of grip and deposit moments). the load motion reconstruction consisted in defining a coordinate system associated to the load (o l, x l, y l, z l ). the geometrical center of the load o l was defined as the center of the two markers. the z l direction was associated to the direction between the two markers. moreover, we assumed that, during the handling task, there was no rotation of the load around z l . thus, we defined x l as the projection of the vertical axis on a plane orthogonal to z l . the y l definition allowed to define a direct orthonormal coordinate system."
"the identification of the grip and deposit events has already been mentioned in the literature as a limitation to predict the external forces. in most cases, a force platform allows these events to be identified [cit] . the use of a neural network proposed in this work permitted their identification with the subject motion only. however, elementary movements studied in this experimentation were imposed for the subject and were standardized, explaining also that the neural network had such great performances. in a workplace, handlers have more freedom to perform their motions. it implies that the grip and deposit moments are more difficult to predict with variable loads without handle. the addition of sensors as instrumented shoes [cit] could allow to improve the external efforts estimation by detecting directly contact phases instead of learning them. combining learning and measures may also be an interesting perspective to solve this issue."
"we can see that the infocom dataset involves less nodes but contains more links and more active pairs of nodes. therefore, comparing between these two datasets allows to get insights about how the density of interconnections affects the performance of the combination of features used."
"when the subject carried the load, the prediction method took into account simultaneously the equations of motion applied to the subject and the equations of motion applied to the load. thus, at each instant, the external efforts were the solution of the optimization problem (2), solved with a sqp method."
"l5/s1 joint moments were estimated by using the predicted and the measured data with a recursive newton-euler algorithm [cit] and a bottom-up approach. as measured efforts were available only for one of the hands, the l5/s1 moments were computed by using the grf&m."
"for the infocom dataset we set the observation from 6:00am to 2:00pm and the prediction from 2:00pm to 4:00pm. in figure 6 (bottom), we can see that our metrics perform in a similar way as what we saw with the highschool dataset."
"therefore, different improvements are considered for future works. the metrics presented in this work are classical metrics used for link prediction in graphs or basic ways to capture the temporal information of the stream. as our protocol is ready to combine new metrics, we intend to design refined measurements that are able to detect more subtle dynamical features of the stream, e.g. we expect that giving weight to recent links would enhance the prediction. we also consider implementing pattern mining techniques to identify typical motifs of the short term dynamics. for example, we could consider that if three nodes u, v, w occasionally interact with each other by short bursts of activity, the occurrence of links between the pairs u, v and u, w suggest a link apparition between v and w shortly after. finally, we made the assumption that the activity remains constant from the observation period to the prediction period. however, this hypothesis is not always satisfied and greatly depends on the data under consideration. models developed in the context of time series prediction, like the arima model which extrapolates precisely the past activity [cit], would certainly allow to better evaluate the number of links predicted."
"as a typical digital signal analysis and test tool, the simple digital timing analysis device can customize the circuit, upgrade and expand functions, and use x-y mode as the signal input oscilloscope display method. compared with the oscilloscope channel input method, it can display multiple signals, manual cursor and other practical functions."
"the table i shows the mean rmse and rrmse between measured and predicted grf&m and lcf for all the handling tasks of all the subjects. considering the phases where no load were fig. 6 . representative example of predicted and measured grf&m and lcf (this elementary movement consisted in carrying the load from area 3 to area 1 with an additional load of 3 kg). the blue curve corresponds to the measured data, the red one corresponds to the predicted data. the orange areas represent the uncertainty phases (identified with the video). inside these areas, the subject was in contact with the load but without completely carrying it. carried, the results can be compared to the literature since only grf&m were considered. the rmse for the vertical forces was 17.9 n and 20.3 n for each foot, corresponding to 0.24 n/kg and 0.28 n/kg. these errors were lower than those presented in the literature for walking motions, whatever the prediction method: empirical functions [cit] (0.90 n/kg), machine learning [cit] (0.73 n/kg) or a contact model [cit] (between 0.52 n/kg and 0.91 n/kg). the rmse for the antero-posterior axis was below 0.10 n/kg since the efforts values were very low due to the performed tasks. the medio-lateral forces component obtained the most important errors (0.37 n/kg and 0.45 n/kg for each foot corresponding to 34.8% and 40.7%) which confirms the observations previously made ( figure 6 ). also, the grm were in the same order of magnitude as those reported previously [cit] . considering all these results for the phases with no load, one can conclude that the method presented here reproduces similar results as in the literature in similar cases. considering the phases with load, the rmse for the grf&m were quite similar. the prediction method previously validated only for the grf&m was adapted to the addition of hand contact and for asymmetric handling tasks. consideration of the hands contact did not introduce additional error on grf&m prediction. the rmse for the vertical axis of the lcf was 0.86 n/kg and 0.50 n/kg according to the load carried. these results are in the same order of magnitude as the errors found for the grf. however, as the load mass is relatively light, the absolute error was below 6 n. in the same manner as for the grf, the most important errors were reported on the medio-lateral forces (12.6 n and 16.9 n according to the load carried). these forces correspond to traction or compression forces applied by the subject to better grip the load. the higher error observed with the additional load could be due to the fact that the subject applied a more important force along the medio-lateral axis to ensure the stability of the load during the motion. even if the load motion reconstruction error is not negligible, prediction results are satisfactory."
"in these experiments, we observe that mixing structural and temporal information improves the prediction on short observation periods but not necessarily on long ones. the boundary between what can be considered as a short or a long observation period is of course a disputable matter, and largely depends on the dataset under study. this is due to the fact that when the observation period is longer, links that have not appeared in the past are less likely to appear in the dataset and recurrent links are therefore predominant. thus, extrapolating the previous stream activity is more relevant than using structural information to predict the future activity of a pair of nodes. we think that in datasets which exhibit a growing activity rate, the structural information would be able to help predict new links even when using long observation periods."
"oscilloscope x-y channel display module. oscilloscope x channel waveform voltage determines the distance from the x-axis. the oscilloscope y-channel waveform voltage determines the distance of the spot from the y-axis. x channel input oblique wave, y channel cycle input and trigger word matching digital signal waveform can be output on the oscilloscope. in order to display 8 waveforms simultaneously on the oscilloscope, x channel needs to input 8 oblique rising waves, which are respectively used for the complete display of 8 waveforms. the 8 waveforms of y channel should be handled by lifting pressure, otherwise the spots will be aliased, so the 8 waveforms need to be set on different reference voltages. oscilloscope time marker line can be set, x channel oblique wave reference voltage can be adjusted."
"let us emphasize the fact that our goal is to define a general framework to allow further study of the interplay between structural and dynamical features for prediction tasks, rather than optimize the prediction performance on these specific datasets. we aim to give evidence of the fact that combining these two kinds of features leads to improvements over the use of only one kind. therefore, we make the simplest possible design choices in order to make our point, even though more elaborate choices would lead to better predictions. our work is indeed a proof of concept, that provides a general scheme to serve as a baseline and motivation for further work in this direction."
"in the other case, we take into account the activity of each pair of nodes between ω and the time of occurrence of the k th link between u and v before ω. the corresponding function is"
the motion of the load used in the equations of motions described in equation (2) is the one obtained at the previous step. inertial parameters of the load were estimated from its digital mock-up on which were applied the corresponding materials for both configurations (with and without the additional load).
"numerous studies predicted the external efforts using only motion data. generally, the external efforts were limited to the grf&m. the use of dynamics equations with the application of a top-down approach is limited by the fact that the inverse dynamics problem is undetermined during the double support phase. three approaches were proposed in the literature to solve this issue: empirical functions [cit], machine learning [cit] and contact models."
"clock and digital signal generation module. clock accurately produced in the role of digital signal control is crucial, this design by stm32f103rbt6 microcontroller as a signal generator, configuration of eight i/o mouth d0 ~ d7 for digital signal output terminal, a clock the clock signal output, internal timer interrupt cycle 10 us, every 10 us raise the interrupt, flip clock io mouth clock level, produce 100 khz clock pulse signal. each interrupt d0 ~ d7 is switched in 8 different state words sw, and every 8 interrupts is a shift cycle, which can display the 8-channel timing waveform at the same time."
"in this work, we proposed an activity prediction protocol adapted to the link stream formalism, making it possible to advantageously use the rich information contained in this modeling. it is built around a flexible way to combine the information from metrics which capture features of the stream. we also proposed an evaluation protocol adapted to our problem. our experiments show that combining structural and temporal features leads to performance improvements. we also showed that the length of the observation period have complex consequences on the prediction, that demands to be studied in depth. this work is a first step towards activity prediction in link streams. our protocol is designed in a modular way, such that each part is independent from the others and can be replaced or improved, depending on the application we are interested in."
"a representative example of predicted and measured grf&m and lcf is presented figure 6 . it corresponds to one elementary movement of one subject. the relative error was the lowest on the vertical grf which corresponds to the weight component. the weight transfer was well detected and estimated during the asymmetric task where all grf were successively applied on each foot. the results in the carrying phase (between the two orange areas) seemed not to be affected by the contact efforts on the hands. thus, vertical lcf was well predicted on feet."
"is the function associated to metric i and k is the number of metrics used. the parameters α i allow to control each metric weight in the prediction function. note that the value of f u,v does not have an absolute meaning, we rather use the relative value of f u,v to other f u,v as we will see in section iv-c. note also that other combination methods are possible, this choice is made for the sake of simplicity."
evaluation of the l5/s1 joint moments estimation: l5/s1 joint moments computed with the predicted and with the measured data were compared. rmse and rrmse [cit] along each axis are presented.
"the directory of useful decoys-enhanced (dud-e) benchmarking set [cit] was used for training and evaluating the model. dud-e is a data set designed for helping benchmark docking software and scoring functions. there are 102 targets in dud-e. each target is provided with 224 active ligands and 13,835 decoys on average. dud-e has been widely used for evaluating the virtual screen ability of scoring functions [cit] . although it has been reported by some literature that there exists noncausal bias in dud-e [cit], we still use it to evaluate our model since there is no better data set so far."
"aa2ar a2a adenosine receptors (aa2ars) belong to g protein-coupled receptors (gpcrs). from the pharmacophore model, we have known that for aa2ar antagonists, basic structures include a hydrogen-bond donor, an n-containing aromatic ring, a large lipophilic region, and a smaller lipophilic region [cit] . in figure 7, the binding mode of an active obeying these pharmacophore rules is presented, and different regions are labeled. it can be seen that deepscore highlighted the importance of the n-containing aromatic ring and the smaller lipophilic region by painting them red. the rest structures were taken as less important."
"where, by isolating the load, m l (q) is the inertia matrix, c l (q,q) is the centrifugal and coriolis force vector, g l (q) is the gravity force vector and e l is the generalized external force vector. the maximal forces for contact points on the feet were the same as defined above. the external efforts vector e s contained the external forces applied on the feet and the external efforts vector e l contained the external forces applied on the hands. the predicted and the measured forces were filtered with a 4-th order butterworth low pass filter with a cut-off frequency of 5 hz and no phase shift."
"evaluation of the load motion reconstruction: during phases where the subject carried the load, its motion reconstruction was compared to the motion of the markers placed on the load. the root-mean-square error (rmse) of the distance and the orientation between the reconstructed and the markers-based coordinate systems was computed."
"note also that the positions of the peaks seem to be closely related to the number of links predicted, it suggests that a possible way to improve the performance of the prediction could be to refine the hypothesis that the activity in l is the same as the activity in l."
"to do so, we capture independently some structural and dynamical features with metrics measuring the link stream properties. we then combine these metrics in order to estimate future activity and we compare our prediction to the ground truth in order to assess the relevance of the approach. the performance of our framework is measured on two datasets of real world contacts between individuals [cit] ."
"in recent years, with the increase of the type and number of electronic communication equipment, the failure of the connectivity between the equipment occurs from time to time, which brings great trouble to the equipment operators. most of the modern communication equipment adopts digital signal to communicate, which puts forward higher requirements on operators' digital logic design ability and digital system analysis ability [cit] ."
"the sum of each of these indicators over all pairs of nodes yields the number of t p, f p and f n for the whole prediction. note that these definitions allow to get the usual relationships between the indicators, that is to say, t p + f p is the number of predictions and t p +f n is the total number of interactions actually occurring during t ."
"to achieve accurate acquisition of clock and digital signals, analyze the timing sequence of d0 ~ d7 signals, timely respond to the external state and control signals, stabilize the output of analog signals through the oscilloscope x-y channel, and display timing sequence graphics on the oscilloscope. the program flow chart is shown in figure 5 ."
"where i is the ligand atom, j is the receptor atom, distance ij is the distance between atom i and atom j, and a is the function used for calculating the pmf between atom i and atom j. in pafnucy [cit], a structurebased cnn model, 19 features were used for describing an atom. in deepscore, almost same features but with minor modifications were used (see table 1 ). the features included the information of atom type, hybridization state, heavy valence, hetero valence, partial charge, and whether the atom was aromatic/hydrophobic/ hydrogen-bond donor/hydrogen-bond acceptor/in a ring. heavy valence and hetero valence were represented as one-hot vectors in deepscore instead of integers in pafnucy."
"in individual prokaryotic cells, the copy number of transcripts from a promoter is usually no more than 10 [cit], much lower than that in mammalian cells. number fluctuation is thus conspicuous. consequently, to uncover the principles that govern mrna production necessitates collecting massive quantities of data via tracing the time series of transcription initiation events or changes in ms2/pp7-labelled mrnas. while this is realizable, it has not been done sufficiently. nevertheless, it was confirmed that mrna production is not a poisson process but can be well fitted by the two-state model [cit] ."
"# of projection fwhm of psf effective resolution 1 (cor.) 2.6 ± 1.1 pixel 16.9 ± 4.6 pixel 2 (cor. + sag.) 2.5 ± 1.6 pixel 7.0 ± 2.3 pixel 3 (cor. + sag. + tra.)"
"for image voxel at the center of the fov, we found that the fwhm of the psf was larger and the shape was less smooth than the psf of an image voxel at the periphery of the fov (fig. 5b) . again, the fwhm monotonically decreased as the number of projections increased. the fwhm for reconstruction using one, two, three projections was 2.6, 1.6, and 1.3 pixels respectively. the effective resolutions were 13.6, 6.7, and 5.0 pixels respectively. taken together, mini can improve the spatial resolution quantified by fwhm by approximately 29% and 39% by using two and three orthogonal projections respectively. the effective resolution can be improved respectively by approximately 53% and 69% when two and three projections are used in mini reconstruction. table 1 shows the average and the standard deviation of the fwhm and the effective resolution of the mini reconstructions using one, two, and three projections. we observed that regardless table 1 the average and standard deviations of the fwhm of psf and the effective resolution in the unit of pixel."
"where psf(x) denotes the value of the psf at the xth voxel and psf(i) denotes the value of the psf at the ith voxel, where a discrete impulse input image was created."
"transcriptional bursting has been a hot topic of recent reviews [cit] . here we focus on findings based on advanced quantitative investigations at individual alleles, aiming to connect the existing phenomenological models with the molecular mechanism of how transcription is dynamically orchestrated. we further elucidate that it is the frequency modulation that transmits information, which fulfills the accuracy and randomness requisites for transcriptional regulation."
"to evaluate the quality of reconstructing a static mini image, we used three fully partition encoded reference scans to generate the forward operator a. simulated mini acquisition y was obtained from different reference scans without partition encoding. ideally, the reconstructed volumetric image x should be a volume of 1, which corresponds to a static reference scan. for visualization, we averaged the sum-of-squares of three reference scans to generate a volume r and calculated the product between x and r. this product thus keeps features of the image. the mini reconstruction error ρ was defined as"
"based on the general structural organization and conformational changes of the ta, the ensemble and probability theories were exploited to probe how the ta dynamically operates [cit] . theoretical analyses and numerical simulation revealed that, for the ta to orchestrate a reliable response to changing activator concentration, transcripts are essentially generated in units of bursts. it was also proposed that the ta operates as dictated by the following four principles."
"the other potential source of deteriorating the consistency between the mini reference scan and the accelerated scan is the physiological noise. fmri physiological noise is signal dependent [cit] ) and can lead to pronounced amplitude variation in the projection images and therefore data inconsistency between the reference and accelerated scans. such inconsistency will be eventually reflected in the increased noise variance in the general linear model analysis and the decreased power of detecting activated brain areas. one way of mitigating this challenge is to include the physiological signals (ekg, breathing, and pulse oxymetry) into the design matrix as confounding vectors as we did. alternatively, it is possible to suppress physiological noises in the projection data before the general linear model analysis using other algorithm ."
"(4) transcriptional bursting in eukaryotes allows digital information conversion, by which regulatory signals modulate the burst frequency rather than burst size. for a given ta, the burst size obeys a specific distribution; experimentally, the burst size might be mistaken as subject to modulation since it is hard to differentiate a burst from a burst cluster."
"we studied the convergence properties of the cgs image reconstruction algorithm numerically. fig. 2 shows the normalized residual data fit error ξ over 200 iterations. we observed a monotonic and sharp decrease of ξ from 1 to 0.01 in the first 10 iterations. after 200 iterations, ξ was reduced to less than 0.01. this convergence behavior was found to be similar among reconstructions using one projection (coronal projection), two projections (coronal + sagittal projections), and three projections (coronal + sagittal + transverse projections). based upon the results of the convergence analysis, 20 cgs iterations were considered sufficient for stable image reconstruction and this value was used for the remainder of this study. fig. 3 shows the mid-coronal, mid-sagittal, and mid-transverse slices of the reconstructed volumes using one (coronal), two fig. 2 . the normalized residual data fit error ξ n (k) and the reconstructed images over cgs iterations. the plot shows the residual errors at different iterations using one (coronal), two (coronal + sagittal), and three (coronal + sagittal + transverse) projections. the reference image and the reconstructed mid transverse images at the 20th iteration using one (coronal), two (coronal + sagittal), and three (coronal + sagittal + transverse) projections were shown in the figure inlet. a, p, r, and l denote the anterior, posterior, right, and left. fig. 3 . reference and reconstructed mini images. row a shows the reconstructed mini images using only coronal projection; row b shows the reconstructed mini images using coronal and sagittal projections; row c shows reconstructed mini images using coronal, sagittal, and transverse projections; row d shows the reference images. r, l, a, p, s, and i denote the right, left, anterior, posterior, superior, and inferior directions respectively. the number at left-down side of each reconstructed images represented the reconstruction error."
"the trajectory of mini is the central partition (no partition encoding) of the echo-volumar imaging. other sophisticated trajectories aiming at a higher spatiotemporal resolution, such as single-shot rosette trajectory [cit] ), have been suggested. recently, multiple concentric shell trajectory has also been realized to improve the spatial resolution of fast fmri [cit] b) . however, the essential difference between mini and these methods is that the mini attempts to improve the conditioning of the encoding matrix, while rosette and concentric shell trajectories still solve pretty under-determined inverse problem with necessary mathematical priors to complete image reconstructions."
"fluctuations due to bursting can be directly utilized by positive feedback loops in gene regulatory networks to induce the bistability of gene expression and thereby cell differentiation [cit] . nevertheless, a second issue arises concerning how the accuracy of cellular signalling is achieved given the fluctuations. recent studies showed that, after the release of a transcript from dna, the mature mrna tends to reside in the nucleus for a long time [cit] . mrnas produced in discontinuous bursts thus accumulate in a pool and are slowly released into the cytoplasm. it was thus suggested that the nuclear retention of mrnas tends to minimize the burst noise."
"the spatial resolution of mini reconstruction was analyzed by the point spread function (psf), which was numerically calculated by 1) creating a discrete impulse input image at one selected location, 2) simulating the ideal measurements across channels of the rf coil array and image indices in the project images (eq. (3)), and 3) calculating the estimatesx using the cgs algorithm (eq. (4)). this procedure was repeated at different locations inside the fov. we considered two ways of characterizing the spatial resolution based on the psf. first, we used the full width at half maximum (fwhm) [cit] of the psf to quantify the spatial resolution. psf was defined as the profile of the reconstructedx and we measured the fwhm of the profile (eq. (4)). we parametrically studied the reconstruction using one projection (coronal projection), two projections (coronal + sagittal projections), and three projections (coronal + sagittal + transverse projections) at two locations: one was around the center of the fov and the other one was at the periphery of the fov. second, in order to take the intensity distribution of the psf into account [cit], we also evaluate the effective resolution of the ith image voxel by"
"(7) the significance of various bursting patterns from different promoters remains to be addressed. firstly, unscrambling the bursting patterns paves the way to reveal the dynamics of both the chromatin and the ta. by statistically analysing the mediator-and tbp-dependent bursting dynamics (such as the burst number, duration, and interval), for example, it can be inferred that the mediator interacts with the dna much more transiently than the tbp [cit] . secondly, it is an open issue why different genes adopt different bursting patterns to realize their functions. in other words, are distinct bursting patterns exploited or accommodated to achieve specific signalling capability?"
"in characterizing the time series of the number of cellular mrnas, the duration of a burst is defined as its rising period that corresponds to the time window of synthesizing transcripts (fig. 1c) . usually, the duration of a burst is no more than several minutes. the refractory period between two successive bursts ranges from seconds to tens of minutes or even longer. the magnitude of a burst can be up to dozens of mrnas. the burst duration and magnitude are collectively termed burst size. both the burst size and burst frequency (defined as the number of bursts per unit time) are potentially modulated by transcriptional activators."
"the modulation of burst frequency is an embodiment of the r tor code, which represents an accurate encoding of time-varying activator concentration; moreover, bursting-induced fluctuations in mrna number can be smoothed by nuclear retention of mrnas. on the other hand, such fluctuations can also be enlarged to achieve sufficient heterogeneity."
"(1) transcriptional activators cyclically bind to and dissociate from the enhancer, with the individual binding time no more than several minutes. some of those cycling activators act to recruit chromatin modifiers, while some control transcription initiation via the mediator."
"the continuum model has an off state and a special on state [cit] (fig. 2c) . during the on period, transcripts are not generated via a poisson process as assumed by the two-state model. the time intervals between two successive initiation events range from several seconds to more than ten seconds. these intervals do not obey a single exponential distribution; instead, they obey a large number of exponential distributions whose expected values range quasi-continuously. the corresponding initiation rate thus spans a nearly continuous spectrum. two mechanisms were speculated to be at work. the on state is composed of a large number of sub-states, each with a distinct initiation rate. these sub-states are defined by specific binding of transcription factors or epigenetic marks and are closely spaced in time, so that they are hard to distinguish or count. alternatively, in the on state the promoter switches between a primed state and a mature state that produces a transcript. the mature state depends on the local and time-varying concentration of pol ii. such a continuum feature was reported previously [cit] ."
"advanced measurements at single-transcript resolution validated these theoretical predictions. for the c-fos gene, for example, the burst frequency is modulated by activator increasing the activator concentration leads to more frequent cycling of activators in the c-space, with the distribution of residence time unaffected. the residence times shape the occurrence of transcription initiation events, thereby controlling the time series of mrna number. as a result, the burst frequency, rather than burst size, is subject to modulation by activator concentration. note that the bursting with high frequency makes it hard to differentiate between two successive bursts. concentration, and the burst duration is determined by the duration of activator binding to dna [cit] . additionally, the transcription initiation rate during activator-dna binding will be affected if the strength of the activator's transactivation domain is altered by mutation. the same conclusions were reached for steroid-receptor-induced gene transcription . increasing the activated steroid receptor leads to an increase in burst frequency, whereas the duration and magnitude of individual bursts are not affected. further analyses on enhancer-promoter communications in living drosophila embryos, mouse, and human cells also confirmed this frequency modulation [cit] . collectively, transcriptional bursting is modulated in a digital manner."
"the multi-scale model stresses that transcription is initiated in units of bursts and the bursts are separated with multiple timescales [cit] (fig. 2d) . each burst is triggered by a pol ii convoy -a group of pol iis (with the number ranging from 1 to 30) successively launches from the promoter within several minutes and then elongates at a rate of ∼4.1 kb/min. with the mediator at the promoter, the refractory period between two successive bursts obeys an exponential distribution with the expected value of ∼100 s. such mediator-dependent two-state dynamics have a precondition that the tata-box binding protein (tbp, one subunit of tfiid) is at the promoter. typically, several convoys form and depart in the presence of the tbp. without the tbp, the promoter enters a non-permissive period, which does not obey a single exponential distribution; presumably, the non-permissive periods depend on different extents of promoter methylation or occupancy by histones."
"the above principles are recapitulated by the wang-liu-wang (wlw) model (fig. 2e), with which simulation results reproduced different profiles of gene expression with the precision reaching the standard deviation of the transcriptional response [cit] . in this model, transcription is initiated via steps from the scf, to pic, to opc; these steps are tightly controlled by activators via the mediator. the destruction of the scf may be followed by histone rebinding. histones can be easily evicted in the presence of enhancer-bound activators that recruit chromatin remodelling enzymes and modifiers. notably, numeric simulation with the wlw model reproduces the multi-scale and continuum features (fig. 5) . additionally, the wlw model can be expanded to embody the ratchet model if a sufficient number of steps on chromatin modifications are included [cit] ."
"however, the matrix (a h a) is too large (262,144-by-262,144) to be calculated and inverted explicitly. we thus use the conjugated gradient squared algorithm (cgs) [cit] to solve eq. (3) iteratively."
"previously it has been suggested that the limit of acceleration along an encoding axis using rf coil sensitivity is between 3 and 4 fold accelerations under the constraint of the allowed noise amplification (gfactor ≈ 1) and isotropic reconstructed image voxels [cit] . this is different from our study, which explored how much the reconstructed image voxels are blurred as more projection data are included. both approaches can be regarded as studying how rf coil sensitivity and k-space trajectory can be optimally combined from two distinct perspectives: either reducing k-space acquisitions from a fully gradient encoded case to see the limit of the maximal acceleration with the allowed image quality penalty [cit], or increasing k-space acquisitions from a minimally gradient encoded case to understand how image resolution is improved."
"in conclusion, we demonstrated the feasibility of mini to improve the spatial resolution of high temporal resolution (10 hz/volume) bold-contrast fmri with the whole-brain coverage. such a temporal sampling rate is already about twenty times faster than that of conventional multi-slice epi. this imaging acquisition technique can be used to study fine temporal features of hemodynamics, to monitor/suppress physiological noises, and in other dynamic mri applications where fast acquisition is critical."
"(1) genes are transcribed in episodic bursts. this feature is ubiquitous from bacteria to mammalian cells, regardless of whether they are constitutive or inducible genes."
"intuitively, transcriptional bursting can be characterized with a simple two-state model ( fig. 2a), where the gene promoter switches between a transcriptionally active state (on) and an inactive state (off) [cit] . a burst of transcripts is generated in the on state, whereas the off state is non-permissive for transcription. the two-state model is usually depicted by three parameters: activation rate k on (reaction rate from the off to on state), inactivation rate k off (reaction rate from on to off), and mrna synthesis rate from the on state k m . k on is positively related to the concentration of transcriptional activators. both k off and k m are constant, i.e. the duration of the on state is exponentially distributed and mrna synthesis is a poisson process."
"for most genes, however, it seems that the promoter largely lacks histones or the chromatin can be easily opened. extensive investigation revealed that the time series of transcriptional bursting is primarily shaped by the ta, whose gene specificity is defined by the cis-regulatory elements and promoter sequence [cit] . additionally, the duration of the on state is exponentially distributed, while the off state cannot be described by one single step. two novel models have been built to describe this type of promoter. one is the 'continuum model' based on a highly expressed housekeeping actin gene [cit] . the other is the 'multi-scale model' based on the fully activated hiv-1 gene [cit] . high expression means that the promoter is largely evicted from histones, and thus the models accurately reflect the dynamics of the ta."
"upstream of the glnap2 promoter, there are two enhancer sequences and three low-affinity sequences for the binding of transcriptional activator nitrogen regulatory protein c (ntrc). [cit] revealed that the initiation is stimulated by an ntrc hexamer at either enhancer (fig. 3) . moreover, the main regulatory mode involves short-lived dna bridging, via which the proximal enhancer is connected to a low-affinity site adjacent to the transcription start site (tts). with the dna bridging, the ntrc hexamer at the distal enhancer is in the vicinity of the tts and thus immediately stimulates the polymerases one by one, leading to a burst. this work stresses the importance of low-affinity sites, unstable protein complexes, and transient molecular interactions in orchestrating transcription."
"compared with the simple two-state model, the ratchet, continuum, and multi-scale models more accurately describe transcriptional dynamics and provide more insights into transcriptional regulation. however, these models appear to differ substantially from each other. it is unclear to what extent these models are condition specific or reveal the general mechanism of transcription. interestingly, these models can be unified in an earlier theoretical framework of how the ta operates [cit] ."
"the spatial distributions of the activated brain areas in a visuomotor task at 4 s after the onset of the visual stimuli using 3 projections mini and epi are shown in fig. 7 . it should be noted that 3 projections mini used 50% and 200% more data (and thus the acquisition time accordingly) than 2 projections mini and 1 projection ini respectively. however, all data show strong activation in the visual cortex and sensorimotor cortices in the medial and lateral aspects of the inflated brain cortex. the spatial distributions of these functional areas were visually similar between mini and epi. quantitatively, the shift of the center of mass between epi and mini in the sensorimotor was 0.96, 0.93, and 0.33 mm using one, two, and three projections, respectively. additionally, activity in supplementary motor area and higher order visual areas in the ventral and dorsal streams were found in both epi and mini. the spatial patterns of activity were not exactly identical, potentially due to intra-subject and inter-scan variability [cit] ."
"the time courses of the hemodynamic responses in the visual and sensorimotor cortices based on mini using three projections are shown in fig. 8 . based on the high temporal resolution mini measurements, we found that the hemodynamic response in the visual cortex preceded that in the sensorimotor cortex. to quantify the timing, the hemodynamic responses with 0.1 s resolution were fitted by a model consisting of two exponentials [cit] . based on the model, three timing indices were obtained: onset, time-to-half (tth), and time-to-peak (ttp). the onset is defined as the instant when a straight line fitted between 10% and 90% of the rising edge of the hemodynamic response function intersects the time axis. the tth and ttp are the instants when the response reaches 50% and 100% of the peak value, respectively. the visual cortex has onset, tth, and ttp of 1.09 s, 2.40 s, and 4.60 s respectively using three projections. the sensorimotor cortex has onset, tth, and ttp of 1.87 s, 3.10 s and 5.00 s respectively using three projections. the hemodynamic response in the visual cortex preceded that in the sensorimotor cortex by 0.78 s, 0.7 s, and 0.4 s in onset, tth, and ttp using three projections. while using different amounts of data may affect the significance level of activated brain areas, the interregional hemodynamic timing (fig. 8 ) was based on amplitudenormalized hemodynamic responses. thus, this result likely remains the same when the amount of mini data is adjusted to be comparable to that of the 1-projection ini data, since the amount of data most likely affects the residual variance of the estimated hemodynamic response function (hrf) and accordingly scales the dynamic statistics of the estimated hrf. such a scaling has no effect on the estimated timing indices of the hemodynamic responses."
"in ini acquisitions, leaving out all partition encoding steps is equivalent to acquiring a projection image. in most bold-contrast fmri experiments, repetitive measurements are usually required to compensate the relatively low contrast-to-noise ratio (cnr) in order to detect activated brain areas with a sufficient statistical significance. such a data acquisition protocol opens up the possibility of combining ini acquisitions across different runs to improve the spatial resolution. here we propose the multi-projection ini reconstruction method (mini) to achieve fast 3d mr inverse imaging with an improved spatial resolution. specifically, rather than repetitively measuring the same projection images, more spatial information can be obtained by changing the axes of frequency, phase, and partition encoding such that different projection images are acquired. the collection of projection images can be combined to improve the spatial resolution in dynamic ini reconstructions. compared to ini using only one projection image, as demonstrated in our numerical analysis, the mini's encoding matrix is likely to be better conditioned because more measurements with distinct spatial information are included. this is due to the fact that different projection images from a coil array with evenly distributed coil elements in 3d can include nonredundant spatial information to thus improve the conditioning of the mini encoding matrix. as the cobra method demonstrated the utility of a multi-projection acquisition strategy in 2d fmri using an eightchannel coil array [cit] ), here we use numerical simulations and in vivo visuomotor fmri data with a 32-channel head coil array to demonstrate that mini's spatial resolution can be improved significantly compared to ini with only one single projection in 3d volumetric acquisition with whole brain coverage."
"in higher eukaryotes, dna is packaged by nucleosomes that consist of histones; both the dna and histones are epigenetically modified [cit] . transcriptional progression begins with dna demethylation, histone modification and nucleosome eviction. the transcription apparatus (ta), which is mainly composed of gene-specific transcriptional activator(s), general transcription factors (tfs) including tfii-b, -d, -e, -f, and -h, the mediator complex, and rna polymerase ii (pol ii), assembles on the promoter to synthesise transcripts. the destruction of the ta may be followed by its reconstruction, or nucleosome recruitment and epigenetic reprogramming. thus, the ta together with the chromatin determines transcriptional dynamics in eukaryotic cells."
"the mechanism for transcriptional bursting in bacteria is largely unclear. however, for those genes that express at high levels and are topologically constrained in short loops (∼10000 base pairs on average), the bursting was revealed to be primarily caused by dna supercoiling [cit] . an elongating polymerase results in positive supercoiling ahead of it and negative supercoiling behind it. a long interval is required to relieve the positive supercoiling due to limited quantities of dna gyrase; positive supercoiling a transcriptional activator nitrogen regulatory protein c (ntrc) hexamer (shown in blue) formed at the remote or proximal enhancer (separately coloured in red and green; the connection between an enhancer and an ntrc hexamer is denoted by white arrows) is able to catalyse the polymerase holoenzyme (outlined by a dashed line) at the core promoter (the −24, −12, and +1 sites are denoted in orange, blue, and red, respectively). for a wide range of ntrc concentrations, these two modes only contribute to a small proportion of messenger rnas (mrnas) produced, since it takes a long time for the hexamers to find the holoenzyme. in the third mode, there exists a dna bridging-an ntrc hexamer connects the proximal enhancer to a low-affinity site (the three low-affinity sites are coloured in yellow). this bridging facilitates the hexamer at the remote enhancer to catalyse multiple rounds of transcription initiation. at very high concentrations of ntrc dimers, ntrc oligomers are formed at the low-affinity sites, rendering the dna rigid and turning the gene off. thus gradually accumulates, slowing down the elongation of the polymerase. on the other hand, negative supercoiling can be rapidly relieved by topoisomerase 1a and possibly helps relieve the positive supercoiling caused by a posterior polymerase. consequently, a convoy of polymerases stops elongating, and the transcription initiation is eventually suspended. once the positive supercoiling is removed, the convoy proceeds rapidly. a newly launched polymerase that is far behind the convoy will lead another burst."
"in order to avoid overly optimistic results in simulations when exactly the same forward operator is used in the genesis of synthetic data and inverse solutions [cit], we used different forward operators from two measurements of three fully partition encoded reference scans. one reference scan was used to create the synthetic measurements in the simulation, and the other reference scan was used as the forward operator to reconstruct images, spatial resolution analysis based on point-spread function, and detection power using receiver-operating characteristic analysis (see below)."
"the volumetric distribution of the hemodynamic response coefficients x(t) at time point t is related to the accelerated mini acquisitions by a linear equation. without loss of generality, we consider the coronal projection acquisitions first:"
"we noticed that the spatial resolution of each projection in ini is quite different: the highest bandwidth is across partition encoding steps and thus blurring due to t2* decay and distortion due to susceptibility is negligible. then frequency encoding direction has also rather high bandwidth (2520 hz/pixel) to avoid spatial smoothing and distortion. the lowest bandwidth is along the phase encoding direction. this study used a read-out bandwidth (2520 hz/pixel) common to fmri experiments. furthermore, combining multiple projections can be regarded as smoothing distortion and spatial resolution across different projections. accordingly, the spatial resolution of mini is rather homogeneous (fig. 6) ."
"one major concern of the mini is that the data were acquired from three consecutive independent runs. the reference scans were also acquired separately. such an acquisition protocol can make the reconstructed mini prone to motion artifacts, which will degrade the consistency between the reference scan and the accelerated scans."
"(6) low-affinity dna binding sites, unstable protein complexes, and dna supercoiling can play crucial roles in regulating transcription. investigating transcriptional dynamics necessitates both live imaging methods with high resolution [cit] and quantitative computer simulations with appropriate theories and models [cit],b; [cit] . specifically, integrating diverse sets of data makes it possible to present a coherent dynamic picture of gene transcription in bacteria ."
"(2) transcriptional bursts in eukaryotes take place with multiple timescales, and the transcription initiation rate is continuous during each on period. such features are consistent with those dictated by four theoretically derived principles that govern how the ta operates dynamically. this in turn suggests that eukaryotic genes likely share similar dynamic principles for ta operation, as they share the same set of general transcription factors."
this metric was chosen as the figure of merit because all quantities can be measured or calculated directly. the error ξ(k) was calculated at each step over 200 iterations.
"(3) the ratchet, multi-scale, and continuum models emphasize different profiles of transcriptional dynamics. these models are applicable to genes of specific categories and can be unified by the wlw model, which depicts the core steps of regulated transcription and can be accommodated to characterize gene specificity. additionally, conclusions derived from the traditional two-state model should be treated cautiously."
"). this suggests that while the encoding matrix still ill-conditioned, the uncertainty of the image reconstruction, which can be related to the spatial resolution of the reconstructed image, is indeed improved by combining more projection data. this was supported by point-spread function analyses (figs. 5 and 6 )."
"ini and more generally pmri reconstruct images by solving a system of linear equations. in contrast to solving over-determined inverse problems in typical pmri applications, the ini image reconstruction has been intrinsically under-determined since the time-consuming partition encoding steps have been left out in order to achieve the temporal acceleration. the absence of gradient encoded spatial information along the partition encoding direction is complemented by rf coil sensitivity information and prior constraints. consequently, ini has anisotropic spatial resolution. this challenge can be partially mitigated by optimizing k-space trajectory [cit] ). however, ini generally has a lower spatial resolution at the center of head and a higher spatial resolution at locations close to the rf coils."
"in this study we demonstrated both theoretically and empirically that spatial resolution of fast (10 hz) whole-head fmri can be substantially improved by integrating spatial information from multiple projections. compared to ini using only one projection, three-projection mini can improve the spatial resolution quantified by the fwhm of psf in the cortex by 17% (1.2 pixels to 1.0 pixels). at the deep brain area, the fwhm of psf was found improved by 50% (2.6 pixels to 1.3 pixels). since fmri typically requires multiple runs to obtain sufficient contrast-to-noise ratio to detect activated brain areas, mini acquisitions can be easily implemented empirically by collecting different projections in different runs. note that these results were calculated from empirical fully gradient encoded data, thus the susceptibility artifacts along different phase encoding directions were included already."
"these principles suggest that transcriptional bursts are separated with multiple timescales and that the initiation rate is continuous during each on period. the first layer that separates the bursts is controlled by the activator's cycling within the c-space, manifested as activator-mediator-dependent two-state dynamics. the second layer is determined by the stability of the scf where the c-space forms, as marked by the tbp's presence or absence. the third and higher layers depend on the (enh) is bound by activators, with the upper parts of the line denoting the bound state. scf and tata-h denote whether the tata-box in the core promoter region is occupied by the scaffold complex (scf) and histones (h), respectively. the dynamics of activators cycling in the c-space, the formation and destruction of the scf, and occupancy of the core promoter by histones endow the bursts with multiple time scales. the time intervals between two successive initiation events do not obey a single exponential distribution, as shown in the inset. extent to which the promoter is epigenetically modified and recaptured by histones. on the other hand, even with an activator in the c-space, synthesizing a transcript requires many steps from the scf, pic, and opc, to elongation. these steps do not have identical reaction rates. that is, the intervals between two successive initiation events do not obey an exponential distribution. given fluctuations in local concentrations of pol ii and general transcription factors such as tfiif, the initiation rate will range over a wide spectrum. in other words, the multi-scale and continuum models touch on different aspects of gene transcription, which are shared by various genes to different degrees."
"the mini reconstruction started from processing the data in the time domain in order to derive the coefficients of the hemodynamic response basis function in each voxel of the projection image from each channel of the coil array. specifically, we used finite-impulseresponse (fir) basis functions [cit] and general linear model (glm) to allow a high degree of freedom in characterizing hemodynamic responses. the fir basis was used to model a hemodynamic response of 30 s duration in time steps of 0.1 s with 6 s of pre-stimulus baseline. the design matrix of the glm included modeled hemodynamic responses as the convolution between the visual stimulus onset and the fir bases. confounds of run-specific dc bias, linear drift, seven sine/cosine oscillations with frequencies of 1/ 480 hz to 7/480 hz (with 1/480 hz step), and 32 grand averages of all projection image voxels for each rf channel were also included. the physiological noise originating from spontaneous cardiac and respiratory cycles has been shown to be a significant noise source at high field fmri [cit] ). thus synchronized physiological time series (cardiac pulses, respiratory cycles, and pulse and pulse oxymetry) were also included as regressors in the design matrix in order to account for the measurement fluctuations due to physiological noise. the coefficients of the hemodynamic response functions for each run and each channel of the rf coil array were calculated by the standard least square estimation. in summary, for each rf coil and every pixel in the coronal, sagittal, and transverse projection images, we obtained separate glm-based fir basis coefficients that were used in the following volumetric image reconstruction. glm fir bases estimates during the baseline (t b 0) were used to calculate the noise covariance matrix c between channels of the rf coil array."
"when and at what rate to transcribe a gene is subject to precise regulation, and accuracy is essential for cell fitness and development [cit] . on the other hand, it is a stochastic process. randomness is an inherent nature of biomolecular interactions, and the resulting heterogeneity is indispensible for cell differentiation and survival [cit] . it is challenging to comprehend such a contradiction, especially given that messenger rnas (mrnas) are widely found to be generated in discontinuous bursts [cit] . traditional biochemical technologies assayed gene transcription in millions of cells simultaneously, leaving an impression that mrnas were produced at a continuous and smooth rate. taking into account the stochasticity of molecular interactions, the transcription rate of a gene (defined as the number of transcripts produced per unit time) was believed to fluctuate slightly around an average [cit] . thus, mrnas were thought to be produced with a constant probability per unit time, namely via a poisson process."
"1.4 ± 0.5 pixel 4.7 ± 1.6 pixel fig. 6 . the distribution of the effective resolution. the top row shows the effective resolution using only coronal projection; the middle row shows the effective resolution using coronal and sagittal projections; the bottom row shows effective resolution using coronal, sagittal, and transverse projections. r, l, a, p, s, and i denote the right, left, anterior, posterior, superior, and inferior directions respectively. of the metrics, the spatial resolution monotonically increased when more projections were used. fig. 6 shows the effective resolution at mid-coronal, mid-sagittal, and mid-transverse slice of the image volume. more projections used in mini reconstruction can provide more uniform and higher effective resolution along different slice direction. the effective resolution is lower at the center of the fov than at the periphery of the fov, consistent with the results in fig. 5 ."
"given a threshold value and a set of testing instances, the corresponding tp rate and fp rate can be calculated for each feature with eqs. (3) and (4). as a result, each threshold value produces a specific point in roc space, and varying the threshold values from the minimal value to maximal value of each associated feature result in a curve through roc space. the classifier (8) or (9) is selected so that the corresponding auc range is between 0.5 and 1.0. for example, when the auc for a classifier defined by (8) is less than 0.5, the classifier (9) should be applied. the estimated auc is then normalized cross all the features to indicate its relative significance, by:"
"this expression has the following meaning. the probability that node k appears in i's view, after a shuffle between i and j, is given by the probability that k was not in the view and it has been added or the probability that k was already in the view and"
"panorama image is a wide view angle than common image. typically, a panorama image will be used for the landscape, and it is able to represent more realistic than the common image, because of the wide range of scene. the advantage of the panorama is a beautiful scene from surrounding such as travel photography, landscape and natural photography, and is also used for various medias. in addition, panorama images used for various applications or scientific research such as creating 3d virtual reality simulation, 3d medical diagnosis (1) (2) (3) ."
"node i receives l elements from j. as an element is sent with probability γ, the expected number of elements that j and v i have in common is:"
"proof. let p and q be two probability vectors of nodes i and j and let p, q be these vectors after a shuffling operation i j. for the sake of simplicity, we denote the maximum probability before the shuffle as"
"let us now show a corollary stating that once local views represent uniform samples of the system, the shuffling protocol keeps this property true forever."
"all user terminals within a beam are time division multiplexed (tdm) on a single downlink carrier and, thus, they do not interfere with each other [cit] . according to the previous statement and (1), (6), the signal received at the k-th terminal belonging to the n-th beam can be modeled as,"
"the first set of experiments is designed to evaluate the performance of the proposed approach by comparing the results with that of wrapper sffs as it is one of the best feature selection methods in the literature. the performance of the selected feature subset is evaluated by the corresponding auc of the independent testing data. table 2 and fig. 7 show the auc of roc for the associated four datasets produced, respectively, by the proposed approach and the classic sffs approach. the result indicates that the feature subset selected by the proposed algorithm is able to produce classifiers with larger auc than that of sffs. this means the feature subsets selected by the proposed approach have a better classification performance than that produced by the classic sffs algorithm."
"let t be the number of shuffling operations executed on a system of n nodes, c 0 be any initial unpartitioned configuration of local views and c t be the configuration of the system after those t shuffling operations. local views built by the shuffling protocol presented in section 3 will converge to uniform random samples of the system, i.e.,"
"the feature pre-selection approach employs a filter approach with the objective of reducing the computational cost of wrapper and sffs procedures and most importantly improving the classification performance. there are two new characteristics of the proposed pre-selection approach, which are different from conventional filter approaches. the first one is the new proposed filter criterion. the conventional filter approaches only consider the discrimination capability of features. the proposed approach takes into account not only the discrimination capability of a candidate feature but also the complementary relationship between the candidate feature and the features that have already been selected. the second difference is the filter selection method. the conventional filter approaches usually use a fixed threshold value to exclude features or select the top ranked features. the proposed method randomly selects candidate features in the light of the discrimination capability and the complementary. this method increases the flexibility of feature pre-selection and avoids unnecessary exclusion of the important features."
"furthermore, to test the effectiveness of the involvement of randomization in the feature pre-selection, the conventional filter approach which selects the top features has been used for the feature pre-selection. in the experiments the top 10 features in terms of the normalized t values were pre-selected in z k at each iteration. the classification performances are shown in fig. 10 . the results show that the proposed algorithm using random selection produces better testing performance than that which does not involve any randomness in the pre-selection."
"receiver operating characteristics (roc) curves provides an effective approach to characterize the performance of classifiers on sensitivity vs specificity, and have been frequently used in biomedical informatics [cit] ."
"t4 t5 t6 t7 t8 t9 (d) distortion of image 3 fig. 11 the example of distortion matrix fig. 11 (a) shows the original image and initial homography matrix (identity matrix), (b) and (c) show the distortion image that the weight are equal (-0.0005 and -0.001). (d) shows that the x is greater than 0, the images is smaller than original, when x tends to 1. in contrast, images is bigger than original when x tends to -1. the value of t7 and t8 value will cumulate distortion in case of panorama."
"as the partner of an exchange is not forced to put any id into the sent view, the view evolution of the initiator, which receives the sent view, remains as described above."
"our biased shuffling operation uses link swap as a technique for preserving system connectivity. the new operation is presented in algorithm 2. the initiator node chooses l nodes in its current view to fill i . then, as explained above, it chooses the partner j at random and replaces it by its own id to obtain i, unless i already appears in i (this is required to keep the view size constant)."
"as shown in section 3, two key elements ensuring the success of the proposed approach are (1) the inclusion of a complementarity factor (l) in defining the pre-selection criteria and (2) the randomness imposed in the pre-selection. the experiments presented in this section are to evaluate the effectiveness of the proposed preselection method in the above two aspects."
"in this section, we derive an analytical model of the shuffling protocol, which captures the variation of the system configuration over time. the main symbols used in this paper are reported in table 1 ."
"a classical sffs method, starting from x 0, continuously performs the loop of feature inclusion, conditional exclusion and continuation of conditional exclusion on the features in x k, based on a specifically defined evaluation function j(x k ). in conventional wrapper approaches, the classification accuracy is normally used to define the j(x k ). in this paper, the j(x k ) is defined by the average auc of the cross-validation of classification using the associated feature subset x k and the svm."
"in this paper, a cross-layer scheduling design has been proposed for the forward link of a broadband multibeam satellite system with aggressive frequency reuse. linear precoding is considered to minimize the interference caused by the frequency reuse, while the proposed packet scheduling takes into account the dvb-s2 framing, takes into account the severe delay constraints of real-time traffic (qos-1 class), and ensures an acceptable throughput to non-real-time packets (qos-2 class). numerical simulations have confirm the advantages of the proposed scheduling scheme, showing the throughput gain achieved with the proposed user clustering and the adjacent synchronous inter-beam scheduling design when compared with a conventional fifo scheduler. unlike previous works devoted to the join scheduling and precoding design, the proposed method is able to deal with broadband heterogeneous traffic, including traffic with uneven qos requirements."
"some shuffling operations might not be able to \"finish on time\". in reality, as c is usually chosen very small, each sent vector is quite small. moreover, we can consider a shuffling operation as a semaphore [cit] . an initiated shuffling operation will delay any following request until the former one is not finished. as we do not consider node failure, our shuffling operation is able to perform under message loss using timeout and reemission. in principle for high message loss probability, the buffer used for delaying requests might overflow. moreover, as nodes will delay new request, a shuffling requester could be blocked for a while. recursively, a chain of blocked node can occurs, implying classical asynchronous issues. how that could affect our theoretical result remains an open question for future works."
"now, the view size must remain constant. the expected number of elements removed from i must then be equal to the number of new elements added into v i, which is equal to l − γm i j . as all elements have to equally likely be removed, the probability to remove the element k is l−γm i j c . from which:"
"the output image still found the unsmooth, thus the future work will propose the blending algorithm and try to increase the efficiency of stitching by automatic estimated focal length."
"where [s q ] k denotes the k-th element of the q-th column vector of s q . by definition, the laplacian matrix is positive semidefinite and, as a consequence, its eigenvalues are non-"
"a formal proof is now given in theorem 5.6 below. first, we have to express in a measure of the effectiveness: definition 5.5 (shuffling effectiveness). the effectiveness of a shuffle operation correspond to the magnitude of difference between an update view and both of the views before the shuffle, i.e.:"
"proof. as shuffling operations are pairwise independent in term of sent vector selection, the probability that after κ shuffle operations s has been refreshed is given by"
"iii. proposed scheduling assuming a nmmse precoder designed to minimize the inter-beam interference, here we focus on the packet scheduling design. in particular, a new packet scheduler capable to maximize the efficiency of the satellite forward link and to satisfy different qos requirements taking into account the acm policy of the dvb-s2 standard is proposed."
"where 2 is the result after warping, 1 is image aspiring warping and is the 3x3 homography matrix. homography matrix is shown in equation (2)."
"in the context of classification, the main goal of feature selection is to search for an optimal feature subset from the initial feature set that lead to improved classification performance and efficiency in generating classification model. during the past decades, extensive research has been conducted by researchers from multidisciplinary fields including statistics, pattern recognition, machine learning and data mining [cit] . dozens of feature selection methods have been developed during the past years and these can be divided into three categories: (i) filter methods, (ii) wrapper methods, and (iii) hybrid methods, in terms of the interaction between feature selection and classification model [cit] . fig. 1 shows, respectively, the procedures of these three feature selection approaches, where d is the training dataset with the initial feature set, x best is the optimal feature subset to be selected, and j(x k ) denotes an evaluation function to measure the performance of a feature subset x k, based on the independent test (m) or the machine learning algorithm (a), respectively, in filter or wrapper methods."
"experimental approaches [cit] point out that, in the design of a gossip-based protocol, l has to be set to the half of c in order to obtain the highest efficiency in term of convergence speed. this conjecture can be intuitively shown as sketched below."
"where w is a weight vector, b is a bias, and f(x): r n ? r is a decision function which yields a final classification by means of a linear classification for each x i :"
"the potential function is a sort of distance measure between a generic configuration and the uniform configuration, namely the configuration with all probabilities equal to c n . for such configuration, let us introduce the following lemma 2.5."
"a desired feature subset contains features that have great discriminative ability and are complementary to each other. the classification accuracy of a single feature classifier (a classifier involving only one feature) has been used as a traditional method to measure the discrimination ability of a feature. as discussed in section 2.3, the auc presents the global performance of the associated features as well as the trade-off between their sensitivity and specificity. in this paper, the auc is used to estimate the discriminative capability of each feature, for which a classifier needed to be generated. various methods can be used to generate a single feature classifier, the simplest binary classifier is developed with a threshold: if a feature value is greater (or less) than the threshold, it is predicted as positive, else negative, i.e."
"the algorithm used for merge multiple images with overlapping area is a well-known stitching image algorithm. moreover, this technique is able to find the relation of image such as image segmentation (4), and moving object detection (5) . the stitching image algorithm is simple. firstly, the features are extracted from the images and computes the matching feature using the euclidian distance then the result is the pair of points with relation or similarity called corresponding point. the output from the previous process shows the overlap area of the images. in generally, the homography is computed from the estimated geometric transform and adjusts the image into the same perspective called warping images. after that, the image with warping set to the main coordinates system, then the output will produce the high resolution panorama image with a wide view angle. however, some problem of stitching image will continually appear in multi image. warped image with homography can be distorted view (6) . this problem can be avoided by converting image to cylindrical surface. for this method, lens (focal length) is important to convert the normal image to the cylindrical image. this paper has organized as following; section 2 discusses how to stitch the regular panorama image, section 3 explains the cylindrical panorama, section 4 discusses how to stitching with cylindrical panorama, section 5 discusses the experimental result, and section 6 discusses the conclusion and direction for future work."
"let now consider how the system evolves. as explained above, we assume that concurrent operations cannot occur. thus, we can serialize parallel shuffles in an arbitrary order and assume that only one shuffling operation may take place at a time. let p ex (i, j) be the probability that i and j make the shuffle, i.e., p ex (i, j) is the probability that the operation i j takes place."
"the key property of a random walk is that, after a suitable number of steps, called the mixing-time, the visited node is the same as drawn from a uniform distribution [cit] ."
"the corresponding rocs are shown in fig. 8, which show that the major part of the rocs produced by the proposed algorithm is above and to the left of the rocs produced by the sffs, indicating that they achieve a better performance than the sffs."
"uniform peer sampling service has been shown recently to be a basic building block for several applications in large-scale distributed systems [cit] as information dissemination [cit], counting [cit], clock synchronization [cit], etc. a peer service is called uniform if it returns a peer id of the whole system with the same probability. working on the top of a not uniform peer sampling can affect the performance of the application using the service and, most importantly its correctness."
"let s and s be two consecutive samples returned by the same node. by definition, for the new sample to be independent from the previous one, for any pair s, s, it must be"
this problem is appearing on the warped image for stitching on many images. example of distortion result is shown in fig. 8 fig fig. 9 homography for warping image process
"normally, wide panorama stitching using regular panorama stitching are distortion. most of distortion occur from horizontal image of four or more images. moreover, the stitching image process is failure. experimental results show that the cylindrical panorama stitching successfully solves these problems."
"stitching image algorithm has been applied in many tasks such as capturing the overview of city, complex image form surface of the earth by satellites and image of travel location (7) . the main deployment will focus on realistic results instead of effective results"
"in the wrapper sffs feature search, the traditional approaches use the classification accuracy to evaluate the performance of a selected feature subset. to fulfil the requirements of biomedical data classification and enhance the reliability of the features selected, the proposed feature selection approach uses the auc of roc with cross-validation. experiments on various biomedical databases indicate that the proposed system achieves a much improved performance, measured by the auc of roc, over the conventional sffs approach. these results clearly demonstrate the great potential of the proposed approach in the classification of biomedical data."
the proposed approach is classified as a hybrid method that combines the filter and wrapper methods. fig. 5 shows the framework of the proposed approach which consists of four main steps: (1) feature characterization; (2) feature pre-selection using the filter approach; (3) wrapper feature selection using the svm and auc of the roc based on cross-validation; and (4) the sffs for feature searching.
"in this paper, we focus on the binary classification problems, in which a classifier yields two discrete results: positive and negative. as shown in fig. 3, in a binary classification, given a classifier and an instance, there are four possible outcomes. when a positive instance is classified correctly as positive, it is counted as a true positive (tp); however if it is classified wrongly as negative, it is counted as a false negative (fn). if the instance is negative and has been classified correctly, it is counted as a true negative (tn), otherwise it is counted as a false positive (fp). the tp rate (tpr) and fp rate (fpr) are calculated by:"
"the first step, represented by step 1 in fig. 3, merges the packets from different traffic classes in a second queuing model, thus resulting in a fifo queue for each group of terminals. for this first step, we propose a weighted round robin (wrr) which gives priority (i.e. more weight) to qos-1 traffic. for instance, the wrr can be designed such as it takes 1 packet from the qos-2 queue for each 2 packets from the qos-1 queue."
"note that the diagonal elements u(m) are equal to one, and ideally, we would like the non-diagonal elements to be zero (orthogonal). based on the previous reasoning, we proposed to select the scheduling combination m * that provides the minimum frobenius norm between the matrix u(m) and the ideal identity matrix of dimension n, i.e. i n . the latter can be formulated as,"
"where tx is the value of translation in x axis, ty is the value of translation in y axis and ( 1, 1, 2, 2 ) is position coordinate system. result is shown in fig. 16 . fig. 19 (a) and fig. 20 (a), the both panoramas result from stitching between 2 images are the same. that are in same perspective and still no distortion. for panorama stitching of 3 images from (fig. 19 (b), fig. 20 (b) ), regular stitching panoramas begin little distortion. however, panorama stitching of 4 and 5 images (fig. 19 (c), (d), fig. 20 (c), (d) ), regular stitching panoramas appear explicit distortion but cylindrical stitching panoramas are still no distortion."
"feature selection is one of the main issues in biomedical data classification and appropriate feature selection has demonstrated a four samples with missing values were removed from the original specte heart data. great promise for enhancing the knowledge discovery and model interpretation. in this paper, a new approach is proposed for feature selection in biomedical data. the proposed method is featured by (i) a so-called feature pre-selection approach embedded in the sffs and wrapper procedures, and (ii) the use of roc and crossvalidation to define the criteria of sffs and wrapper feature searching."
"n and so on. to select the scheduling combination from all the possibilities, we proposed to use the cosine similarity metric (14) and apply it to the different mean channel vectorsh t n(m) defined in (9) . more precisely, for a par-"
"this work has received funding from the european space agency (esa) funded activity satnex iv call of order 2 (part 1), work item 4 entitled \"forward packet scheduling strategies for emerging satellite broadband networks\". the views of the authors of this paper do not necessary reflect the views of esa."
"ideally, we want to cluster user terminal such that they have similar csi vectors both in phase and in magnitude. the combination of both similarity matrices is done as follows,"
"(⇐) let consider the configuration c as uniform, as presented in definition 2.2. as all the views are uniform, all the probability for a node to appear in any view is the same, so called p. thus, from property 2.6, we have:"
"in fact, the probability to send s exactly at the k-th operation is γ (1 − γ) k−1, as γ corresponds to the probability that s is selected in the sent view."
let c be a uniform unpartitioned configuration of local views. a shuffling operation executed by the shuffling protocol presented in section 3 between any pair of two local views x i and x j belonging to c produces a configuration c that is uniform.
"the stitching image algorithm is divided into 3 steps as shown in fig. 1 in this paper, surf algorithm (8) will used to extract the features, which is fast and powerful resistance to change of image such as rotation, scale, blur, and value of illuminance. normally, the similarity between the images is calculated by the euclidian distance. the small value of distance means that the two points is similar. in conversely, they are different. in this paper, the threshold of similarity is set to 0.6 as shown in fig. 3 this problem can be solved by transformation of image to the same view, and use the corresponding points from the previous step for estimated transformation matrix. the wellknown estimated algorithm is the random sample consensus (ransac) (9) . advantage of this method is the robust"
"the proposed filter searching criterion is presented as the following: of the candidate feature, and l is the complementarity between the candidate feature and the selected feature subset, as calculated by eq. (11) ."
"we consider the forward link of a broadband multibeam satellite system that aggressively reuses the user link frequency resources, where scheduling and precoding are applied. we consider a bent-pipe transparent geo satellite architecture, which relays the signal from the gateway to the corresponding final receivers. fig. 1 illustrates a preliminary scheme of the satellite transmitter functional block diagram based on the dvb-s2(x) and extended to incorporate the advanced interference mitigation block (i.e. precoding). the suggested architecture considers a scheduling block in charge of buffering and processing the data according to the addressed user and its corresponding qos requirements, prior to conveying them to the acm modulator. next, the encoded data is delivered in a frame- by-frame basis to the precoder, which properly weights the information streams before its final transmission to exploit the multi-antenna diversity. we assume that each user monitors its own csi and reports this information back to the satellite gateway by means of a return link. we assume perfect csi estimation. the impact of imperfect csi is kept for future extensions of this work. this paper focuses on the scheduling strategy, i.e. how to schedule the packets in the dvb-s2 frames in order to access the shared medium in the most efficient way while meeting the users' qos requirements."
"proof. let p and q be two probability vectors of nodes i and j and let p, q be these vectors after a shuffling operation. moreover, p k, q k, p k and q k denote respectively"
"where g r is the user terminal antenna gain, g k,n denotes the gain from the n-th satellite antenna towards the k-th user served within the n-th beam and d k is the slant range between the satellite and the k-th user. the term √ k b t b represents the noise contribution, where k b is the boltzmann constant and t is the receiver noise temperature. it is common practice to include the noise contribution into the channel model [cit] in order to proceed with the assumption of unitvariance noise."
"this process has similar to the regular image stitching, but it use using the translation transform instead of projective transform as shown in this process is feature extraction from surf algorithm, and using euclidian distance to find the corresponding points as shown in"
"the support vector machine (svm) is a constructive learning algorithm which originated in statistics and has shown great promise in high dimensional data classification [cit] . it has been successfully applied to biomedical data mining [cit] . unlike most of the modeling methods attempting to minimize an objective function (such as the mean square error) for the whole training instances, svm attempts to find the hyperplanes that produce the largest separation between the decision function values for the instances located at the borderline between two classes."
"our work uses a quite different and simpler modeling strategy, which is more suitable for studying symmetric shuffle operations among nodes equipped with fixed view size. we guess that such modeling approach is an interesting contribution per se, which can be generalized for studying membership protocols or used in conjunction with the graph transformation approach. for example, our model allows to formally proof the intuitive result that the maximum convergence speed to the uniform distribution is reached when nodes exchange a half of their views. an extended comparison between this paper and ours is presented in section 7."
a random walk on a given graph is a sequential process that consists in visiting the nodes of the graph according to a random order induced by the way the walker is allowed to move.
the paper also presented a numerical evaluation of the shuffling algorithm on its convergence speed of the local views to uniform random samples. we also formally proved what is the best fraction of the local views to swap in a shuffling operation to get best convergence speed.
"the presented approach has been evaluated by experiments on various biomedical data including breast cancer datasets and the spectf heart data. the four biomedical datasets used to test the proposed approach are summarized in table 1 . the first three datasets were selected from the uci machine learning repository [cit], with the condition that number of features is greater than 30, and all the features are numeric, including: (1) breast cancer wisconsin (diagnostic) dataset; (2) breast cancer wisconsin (prognostic) dataset; and (3) spectf heart dataset. the fourth dataset concerns the detection of microcalcifications for breast cancer diagnosis based on screening mammography. this dataset is prepared by the authors based on the ddsm database (lumisys scanner with a resolution of 50 lm) [cit], in which a total of 1132 suspicious clusters have been collected from 460 full-field mammograms, and a total of 39 features were extracted presenting the suspicious clusters [cit] ."
"having demonstrated the gains in terms throughput, we now focus on the evaluation of the performance of the proposed scheduling in terms of guaranteeing short delivery delay for the traffic class qos1. here, we focus on the delay introduced by the scheduling mechanism rather than the propagation delay which is due to the geometry of the scenario and cannot be avoided."
"where (xc,yc) is the old coordinate of (x,y), (x ',y') is the new coordinate of (x,y), x is the width of image, y is the height of image, and f is a focal length."
"we now consider a distributed protocol in which nodes manage their views by performing elementary pairwise shuffle or shuffling operation, denoted as . the notation i j is used to denote that i performs a shuffling operation with j. the effect of an operation is to update the nodes' view, as detailed later in this section. we then show that the protocol makes the system to converge towards a uniform configuration, namely a configuration with zero potential value."
"a feature to be considered as a good candidate for selection should have good discrimination capability and it should also be complementary to the features that have already been selected in the feature subset. in this study, the complementarity between a feature and a group of features is estimated by:"
this last equation means that the probability vector of a node follows the view evolution presented in equation 1 if it is involved in a view shuffle (equation 3a and 3b) and remains the same if it is not involved in the last shuffle (equation 3c).
"h is able to solve the equation using normalized direct linear transformation (dlt) (11) which is computed by the corresponding points at least 4 pairs. the number of corresponding points effect with the error, and that are categorized into 2 types; inlier, and outlier by ransac classification. inlier points used to compute the homography for warping image as shown in"
"advantages of the filter-based techniques are that they can easily scale up to high-dimensional datasets and that they are computationally fast and independent of the learning algorithm. a common disadvantage, however, is that the interaction with the classifier and the dependence among features are ignored, which leads to varied classification performance when the selected features are applied to different classification algorithms. on the other hand, the advantage of wrapper approaches is that they have a high probability of producing classifiers with better classification performances than the filter approaches as they take into account the feature dependencies and their collectively contribution to model generation. a common drawback, however, is that the wrapper approaches have a higher risk of over-fitting and can be very computationally intensive when processing a large number of features."
"to the best of our knowledge, these results have never been formally proved before, despite the fact that there is empirical evidence shown in many papers [cit], that protocols based on view shuffling can indeed provide uniform sampling."
"the proposed scheduler aims at: grouping the users within a frame according to similar channel conditions. since all packets in a frame are served using the modcod imposed by the worst user contained in that frame, significant performance gains are expected from a scheduler that groups the terminals according to similar propagation conditions. minimizing the inter-beam interference by scheduling users within adjacent synchronous frames according to orthogonal channel conditions. this is expected to ease the work of the precoder, which is implemented after the scheduling block. giving priority to delay-sensitive real-time traffic (qos-1 class), while ensuring an acceptable throughput to nonreal-time packets (qos-2 class). as mentioned before, precoding and scheduling are coupled in the sense that precoding drastically affects the sinr, which is used to determine the packet grouping at the scheduling part. one way to solve this issue is to try all possible combinations of groups with the corresponding precoders and feed the output back to the scheduler in order to choose the best combination. clearly, exhaustive or brute-force search over all permutations has exponential complexity and thus, it is not efficient in practical systems. here, we proposed an heuristic scheduling solution with the main goal of maximizing the overall satellite forward link performance."
"there are two key performance metrics when dealing with delays. one is the reception time which is defined as the time difference between first and last received packet of a particular service, and the second one is the variations between the delays experienced by packets in a single connection (usually known as delay jitter). while certain applications such as noninteractive television and audio broadcasting are not sensitive to these delay performance metrics, the later are crucial for here we focus on the delays introduced by the scheduling. fig. 9 shows the histogram of delay between the first and the last received packet, where the results obtained with the proposed scheme (in yellow) are compared with the conventional fifo scheduling (in purple). from fig. 9, it can be observed that +50% of the users received all packets belonging to qos1 in less than 5 transmission slots, while the fifo benchmark requires 10 transmission slots to achieve the same percentage. fig. 10 shows the average inter-packet reception delay, both for traffic class qos1. it can be observed that the proposed scheduling provides values which concentrate around 0 and 2 transmission slots, while the fifo benchmark average interpacket delay spreads out along higher values."
"we assume that two shuffles involving a common node may not take place concurrently. once a node initiates a shuffle, it will be locked until the operation is terminated."
"it has not been deleted. the evolution of a view is best described as a two states markov chain, see figure 1, where state 1 (resp. state 0) means that k is (resp. not) in the node i's view."
"this paper is organized into five sections. section 1 is the introduction. section 2 introduces the conventional sffs search method, the support vector machine (svm) used to generate the classification model, and the receiver operating characteristic (roc) curve to define the sffs search criteria, while section 3 presents the proposed algorithm including the search criteria and search procedures. section 4 presents experimental evaluation of the performance of the proposed approach with four biomedical applications. finally, section 5 provides concluding remarks."
"thus, we can infer an expression of p · p and q · p . in first hand, p · p corresponds to the number of elements in common between p and p, which is the size of the view minus the number of new element inserted. on the other hand, q · p corresponds to the number of common elements between q and p, which is the size of the sent view (inserted for sure in v i after the shuffle) plus the unsent elements that were in common with p and q. then, we obtain:"
"the feature pre-selection plays an important role in the proposed approach and is designed to exclude the irrelevant features so that the wrapper-based sffs mechanism can derive the suitable features more efficiently, without searching through the whole feature space, which is usually needed in the conventional sffs approaches. in the following the proposed filter criterion and selection method for feature pre-selection is discussed."
"(1) consider the equation 3a. section 4.2.1 presents the view evolution from the initiator point of view. as the view evolution in this case is exactly the same as in the basic shuffle, lemma 5.1 gives us, for all k:"
"proof. first, the claim comes from lemmata 5.1 and 5.2, as a shuffling operation strictly reduce the global potential, independently of the pair involved in the shuffle, while h(c t ) 0. thus, the distance of the current distribution of sample with the uniformity could only monotonically reduce. so, we have:"
"in this paper, we consider the joint scheduling and precoding design in the forward link of a broadband multibeam satellite system by taking into account the severe delay constraints of real-time traffic, while providing acceptable throughput to non-real-time traffic. clearly, the proposed design claims for a cross-layer scheduling design, where the phy layer is considered to retrieve the csi and apply the precoding technique, the mac layer is considered to retrieve the destination of the packets, and the net layer is considered to retrieve the traffic class."
"to exploit the advantages of filter and wrapper approaches, the hybrid technique has been recently emerged [cit] . a typical hybrid approach employs both an independent test and a performance evaluation function of the feature subset. as shown in fig. 1(c), the hybrid search, starting from a given subset x 0, uses the filter approach together with wrapper approach to find the best subsets at increasing cardinality. the filter approach, based on an independent test method (m) and the associated criterion d 1, is used for the selection of candidate features, and the wrapper approach is employed to evaluate the candidate features using a specific learning algorithm (a) and the associated criterion d 2 ."
"allocation. here, we assume that the frame length is fixed and set equal to k packet units. therefore, and as illustrated in fig. 3, we proposed a round robin (rr) scheduler which takes k packets from each queue in a sequential manner. optimization of the scheduler for a particular frame length is kept for future work."
"from fig. 9, the panorama images must stitch the image from left to right. the number image is . in this process, image 2 needs to transform to the same view of image 1, and image 3 needs to transform to the same view of image 2, and respectively. the number of homography matrix is n-1. from these homography matrix, the images are still not stitching. homography matrix must calculate the cross product such as h2 cross product with h1 with present h, h3 cross product with h2 with present h, respectively. the cross product will make for the cumulative value of translation matrix for correct position. in conversely, homography matrix produces the cumulative distort value, that the panorama result will distort, indefinitely. from fig. 10, the position of t7 and t8 in the matrix are controlling the distorting value."
"in this section, we explain the model of the channel matrix h, which gathers the forward link budget information and phase rotations introduced by the over-the-air propagation. in particular,"
"being φ x a uniform random variable between −π and π. the matrixĥ represents the real csi contribution, which is determined by the satellite antenna gain, the path loss, the received antenna gain and the noise power. more precisely, the (k, n)-th component ofĥ is given by,"
"this paper is organized as follows: section 2 presents the system model. two shuffling protocols are presented in section 3 while section 4 provides an analytical model of these shuffling protocols. section 5 proves that the local views shaped by these shuffling protocols converge to uniform random samples of the system, and that the convergence speed can be optimized using correct settings. section 6 provide some stochastic evaluations in order to illustrate these latter formal outcomes according to the system parameters. finally, related works and conclusion are given respectively in section 7 and in section 8."
"as all the nodes ideally initiates an exchange at the same rate, we can consider that the initiator node is selected at random among all the n nodes. the target node j is taken at random from i . hence"
"let consider how the presence of element k in the view of i varies after a shuffling operation among nodes i and j. a shuffling operation between i and j, denoted i j, generates two new characteristic vectors, x i and x j, starting from the original vectors x i and x j . in other words, after the operation, the view of node i (resp. j) is described by x i (resp. x j )."
"indeed, according to the reasoning above, the core idea of a shuffling operation is to mix as much as possible both views involved in the shuffle. thus, having a high difference between p and its initial state p (nor partner state q) is a good metric."
where auc i is the area under the testing roc curve of the classifier trained by the data (d à d i ) and tested by d i .
"let now consider the converge property of both shuffling protocols. in particular, we show that if any of the two shuffling protocol is executed by a system with arbitrary view distribution, then eventually the system converges towards a uniform configuration, i.e., a system in which all the local views represent uniform random samples of the system. in order to show this result, we exploit the notion of potential function, introduced in section 2. we will show that if the potential function of a configuration is greater than zero, then after a shuffling operation the potential function of the configuration is reduced. roughly speaking, this means that a shuffling operation moves the system towards a \"more\" uniform system, or, in other words, makes the system closer to the uniform configuration."
"traditional filter approaches usually select the top ranked features or use a threshold to exclude the irrelevant features. these methods have a significant disadvantage as only the top ranked features are considered, but the top ranked features may not be the optimal candidates. differently, the proposed feature preselection method is designed, based on the idea of the random sampling method that retrieves a number of individuals from the original population without replacement, to select a number of features (z k ) from y k without replacement in terms of their significance. it consists of the following steps:"
"thus, the probability that after the exchange i contains its own identifier is the sum of two contributions corresponding to the following events: (i) i was not present but it was received from j and (ii) i was present before the exchange and it was not replaced."
"on the other hand, in order to make the most out of the limited satellite radio resources, satellite systems are moving from conventional 4-color reuse schemes (4 beams served with 2 different frequencies and 2 different polarizations) to more aggressive frequency reuse strategies, where the same spectrum is used for multiple neighboring beams. however, the reuse of spectrum automatically translate into a co-channel interference problem. linear precoding [cit] can effectively manage such interference assuming that the interference channel coefficients are properly estimated at each user terminal and reported back to the satellite gateway, who will exploit this knowledge by appropriately weighting the transmit symbols (precoded symbols)."
"machine learning and data mining techniques have been successfully applied in various biomedical domains, for example the detection of tumors, the diagnosis and prognosis of cancers and other complex diseases [cit] . one of the core issues in biomedical data analysis and mining is the so-called 'curse of dimensionality' [cit], particularly the biomedical data are characterized by relatively few instances and presented in a high-dimensional feature space. irrelevant features not only lead to insufficient classification accuracy, but also add extra difficulties in finding potentially useful knowledge [cit] . excluding irrelevant features facilitates data visualization and improves the understanding of the computational models, the feature selection has thus become one of the main sub-fields in biomedical data mining [cit] . in addition, appropriate feature selection is able to reduce the requirements of measurement and storage and thus minimize the cost in database storage and management [cit] ."
"an roc curve plots the tpr on the y axis vs the fpr on the x axis, as shown in fig. 4 . an roc curve located completely above and to the left of another curve, i.e. closer to the upper left hand corner, indicates that the associated classifier produces better global performance than another. for example the classifier 'a' always outperform the classifier 'b' as shown in fig. 4 . to measure how well a classifier performs, the area under the curve (auc) is used to measure how close an roc curve is to the upper left hand corner [cit] . the roc curve for a perfect classifier runs vertically from the point (0, 0) to (0, 1) and then horizontally to point (1, 1) at the top right of the graph, and the corresponding auc is equal to 1.0. for an roc curve following a diagonal path from (0, 0) to (1, 1), the auc is 0.5. in practice a curve typically lies in the upper left of the plot, and the associated auc usually ranges between 0.5 and 1.0 and the larger the auc area this is the better the classifier is."
"the remainder of this paper is organized as follows. section ii introduces the satellite system model and the satellite air interface. after that, section iii presents the proposed joint scheduling and precoding design. supporting numerical results are provided in section iv, and section v states the conclusion."
"in addition, as shown in table 2, except for the wisconsin breast cancer (diagnosis) dataset, for the other three datasets the proposed algorithm is able to retrieve a smaller number of features that achieve a better performance than the sffs. this means that the proposed approach tends to retrieve the significant feature subset with a smaller number of features. this is particularly useful for biomedical data mining and knowledge discovery so that effective visualization can be possible."
"one could argue that our simple protocol does not take into account message loss. in fact, since a shuffling operation involves information exchange between two nodes, it takes some time to proceed. if the amount of information exchange is large or the nodes are topologically separated by many ip hops, then"
"in generally, the regular panorama stitching for 2 or 3 images performs faster than cylindrical panorama stitching because of projection process. however, in case of 4 image up the cylindrical panorama stitching performs faster than regular panorama stitching as shown in table 1 ."
"as shown in eq. (13), the proposed criterion attempts to balance (a) the discrimination capability of a feature by itself; and (b) its interaction with the features that have been already selected in the current feature subset (x k ). this ensures that the candidate features are good enough and complementary, as much as possible, to the features already in x k ."
"the importance of energy and resource efficiency is constantly growing, consequently, the significance of innovative and interdisciplinary lightweight technologies rises as well. this applies, in particular, to passenger and freight transport, and mechanical and plant engineering [cit] . for this reason, high-strength fiber-reinforced plastics (frps) are increasingly used, in addition to traditional construction materials, such as aluminum or steel. due to their fibrous structure, high formability and adjustable directional properties, textile reinforcements (e.g., woven, knitted, braided or laid fabrics) are commonly used in the manufacturing of frps [cit] ."
"the geometry data of the time-dependent shear deformation can be imported into a software environment for statistical calculations and graphical output. the software used in this application case was \"r\" [cit] . as described in section 3.1, all data of a test cycle are leveled based on a first time step (initial state of the shear test), in order to eliminate the intrinsic error of the measurement system. the cleansed data can be used in a further process to visualize the deflection of the laser beam over time as a 3d area. figure 16 shows the characteristics of the folds including their minima and maxima over a constant sample width. a color gradation allows the assignment of numerical values to different fold heights. moreover, the amplitude maxima and minima of each time step can be illustrated both in a separate diagram (figure 17a ) and as a sum of their amount (i.e., as peak-to-peak value, figure 17b ) as a function of time and the shear angle. the amplitudes of maxima and minima revealed qualitatively different curve progressions. as an example, a maximum is initially formed in the case of sample 6, which decreased after about 12 s, as the minimum increased from this point forward. in contrast, the amplitude progression of sample 4 shows a continuous maximum, whereas the minimum remained relatively low. the analysis of the amplitude curve explains the differences of samples of a material variant in shear behavior (cf. figure 15 ). the evaluation of the peak-to-peak value, however, clearly indicates qualitatively comparable curve progressions and thus the reproducibility of the developed shear deformation. the deviations that occur (figure 17b ) result from inaccuracies in sample preparation as described above and cannot be avoided even if the test is performed with great accuracy. figure 18 shows the averaged results of the peak-to-peak values and the standard deviations over time for each material and material direction. besides the absolute measured heights, which strongly differ, a closer view on reproducibility is possible. it is obvious that, for the cf-based woven and biaxial non-crimp fabrics, the standard deviations at the beginning of picture frame testing were lower than at higher deflections. furthermore, the increase in standard deviation seems to be correlated with the beginning of folding. for the cf-based monoaxial non-crimp fabric and the gf/pa-based woven fabric, the standard deviations were nearly constant over time, but significantly higher than for the other materials. especially for the gf/pa-based woven fabric, reflections occurred during testing, influencing the base data set and leading to the obtained high standard deviations. furthermore, the observed results for cf monoaxial non-crimp fabric need to be considered critically as earlier investigations [cit] revealed that shearing is not an intrinsic behavior of monoaxial non-crimp fabrics. additionally, sample preparation is much more challenging, given by the loose construction. overall, the high standard deviations are caused by material-specific behavior, not by the accuracy of the extended picture frame test. hence, the additional optical evaluation of fold height is more reliable than an objective evaluation of folding."
"the investigations were based on the picture frame shear test, described above in section 1.1.2. a picture frame modified at the itm was used [cit] . in contrast to the commonly used picture frames, with a clamping mechanism holding the samples at their edges, the samples are held by needle bars. this allows the twisting of the reinforcing threads and the force to be transferred from the picture frame into the sample ( figure 3 ). this picture frame test stand was completed by an option for the continuous optical detection of shear deformation. during the picture frame test, fold deformation is most pronounced at the center of the sample; it is therefore sufficient to measure the central deflection of the sample. in this respect, our device is different from commercially available 3d deformation measuring systems offered by the company gom or by a recent study [cit], which includes the deformation of the entire surface. the test conditions were as follows:"
"the extended test setup can be integrated into the currently established picture frame test without significant efforts. moreover, no additional sample preparation is required, which results in cf monoaxial non-crimp fabric cf woven fabric, twill gf/pa woven fabric, plain this provides the user with a tool for automatically detecting the exact point of time of fold formation in a reproducible manner and for relating it to a critical shear angle. depending on the geometry of the construction part to be produced and on the specific requirements, a limit value can be defined for a critical fold height. if this limit value is exceeded, the critical shear angle can be assigned, based on the time step. consequently, the shear angle serves as a reliable decision criterion in kinematic draping simulations."
"the hardware and software developed for the recording and graphical evaluation of shear deformations by means of a laser beam enables fold formations to be automatically detected and quantified, and to be related to shear force/shear angle curves. hence, a tool offering a wide variety of additional information for analyzing the shear behavior of reinforcing textiles is available. in the future, this tool may help users in practice to decide on suitable materials and to improve transparency in the design and construction process."
"the visual tracking problem can be approached using generative [cit] or discriminative [cit] appearance models. the latter methods apply machine learning techniques to discriminate the target appearance from the background. recently, the discriminant correlation filter (dcf) [cit] based approaches have achieved state-of-the-art results on benchmark tracking datasets [cit] . the success of dcf based methods is evident from the outcome of the visual object tracking (vot) 2014 challenge [cit], where the top three entries employ variants of the dcf framework. related methods [cit] have also shown excellent results on the object tracking benchmark (otb) [cit] . in this work, we employ the dcf framework to investigate the impact of convolutional features for tracking."
"the investigations were based on the picture frame shear test, described above in section 1.1.2. a picture frame modified at the itm was used [cit] . in contrast to the commonly used picture frames, with a clamping mechanism holding the samples at their edges, the samples are held by needle bars. this allows the twisting of the reinforcing threads and the force to be transferred from the picture frame into the sample ( figure 3 ). this picture frame test stand was completed by an option for the continuous optical detection of shear deformation. during the picture frame test, fold deformation is most pronounced at the center of the sample; it is therefore sufficient to measure the central deflection of the sample. in this respect, our device is different from commercially available 3d deformation measuring systems offered by the company gom or by a recent study [cit], which includes the deformation of the entire surface."
"among the discriminative tracking methods, correlation filter based approaches have recently shown excellent performance on benchmark tracking datasets [cit] . these 1 both authors contributed equally to this work."
"in this work, we use a standard dcf framework to investigate the impact of convolutional features for tracking. the dcf framework utilizes the properties of circular correlation to efficiently train and apply a classifier in a sliding window fashion. the resulting classifier is a correlation (or convolution) filter which is applied to the input feature channels. hence, the correlation operation within the dcf acts similarly to a convolutional layer in a cnn. the corresponding learned filter can be viewed as a final convolutional classification layer in the network. unlike the costly methods typically applied for training cnns, the dcf is trained efficiently by solving a linear least-squares problem and exploiting the fast fourier transform (fft)."
"such frps are lightweight, but expensive and often require a high effort of manual labor. [cit] . therefore, it is crucial to significantly reduce costs by shortening product development cycles and making manufacturing resource-efficient. computer-aided methods can greatly help with this as they are particularly suitable for complex and even double-curved frp products. especially in the early stages of designing an frp, when various models are considered, costs can be avoided with kinematic (a) (b) figure 1 . principle of shear deformation in the bias extension test (a) and in the picture frame test (b) [cit] ."
"the paper is organized as follows. section 2 discusses related work in tracking and convolutional neural networks. our tracking framework is described in section 3. the employed dcf and srdcf frameworks are briefly presented in section 3.1 and section 3.2 respectively, while the used convolutional features are discussed in section 3.3. section 4 contains the experimental evaluations and results. finally, conclusions are provided in section 5."
"with a shear deformation of 28° (corresponding to a deflection of 60 mm), the distance was 206 mm. the laser beam was captured over a constant width of approx. 150 mm during the picture frame test. image processing software supported by opencv calculated the deflection curves of the laser beam for every single time step. after calibration (see section 3.1) and interpolation, the geometric data of the deflection curves were determined as a function of time ( figure 6 ) and were available for further analysis. during the shear test, the geometrical change of the laser beam caused by the shear deformation of the sample was continuously recorded with a software development kit (sdk) from basler ag. in the initial state, the distance between the left and right joints of the shear frame was 282.8 mm."
"the system is calibrated with a rectangular device with an led in each corner (figure 9a ). during the calibration process, the rectangle is positioned in the laser lighted plane. this means that the device is aligned in the picture frame in such a way that it is positioned centrally and planarly in all planes, sagittal, transverse and frontal, with the frontal plane corresponding to the position of the shear specimen in the initial state. figure 9b shows an image of the calibration tool. with the measured picture coordinates and the real coordinates of the leds, the eight coordinates of the homogenous transformation matrix can be calculated."
"author contributions: conceptualization, e.w., s.r. and s.k.; methodology, e.w., m.h., s.r. and p.b.; software, p.t., a.b. and s.r.; validation, e.w. and s.r.; formal analysis, p.t. and s.r.; investigation, p.b., e.w. and s.r.; data curation, s.r. and p.t.; writing-original draft preparation, e.w., s.r. and p.t.; writing-review and editing, s.k., m.h., p.t, a.b., e.w. and s.r.; visualization, s.r., p.t. and e.w.; supervision, s.k. and m.h.; project administration, s.k. and m.h.; funding acquisition, s.k. and m.h."
"to evaluate the reproducibility of the results, nine samples were tested for each material and material direction. figures 14 and 15 show the shear force/shear angle curves as well as the recorded surface deformations for the ± 45° biaxial non-crimp fabric with the sewing thread in the tensile direction. the deviations of about 20% in the shear force/shear angle curve result from the properties of the semi-finished product caused by fabric construction. for non-crimp fabric the yarn systems are not crossed but held by a sewing thread to ensure good drapeability. the missing crossing lead to a nearly perfectly stretched out situation of the reinforcing fiber. thus, high mechanical characteristics in the reinforced directions (e.g., mono-, bi-tri-or quadriaxial) can be realized in frp products. but its low resistance to displacement, restricts its handleability, making it almost impossible to achieve exactly reproducible sample preparation in the shear test. figure 6 )."
"monoaxial tensile test machine z 2.5, zwick, ulm, germany the extended test setup consists of a device including a laser line emitter, a camera and a computer with specific data processing software. the equipment is attached to the tensile testing the test conditions were as follows: the extended test setup consists of a device including a laser line emitter, a camera and a computer with specific data processing software. the equipment is attached to the tensile testing machine in addition to the picture frame. the attachment is above the force sensor so that it does not influence the capture of the shear force. the laser line emitter, which is fixed at a defined distance, projects a horizontal beam orthogonally onto the sample surface (constant over 150 mm width). a basler aca 14 µm camera (basler ag, ahrensburg, germany) for the optical recording of the laser beam is placed at a defined angle and distance to the sample plane (figures 4 and 5 ). due to its kinematic design with fixed and guided joints, the equipment ensures a constant position of the projected laser beam in the middle of the sample (figure 4b ). the distance of laser line emitter and camera remains constant over testing. during the shear test, the geometrical change of the laser beam caused by the shear deformation of the sample was continuously recorded with a software development kit (sdk) from basler ag. in the initial state, the distance between the left and right joints of the shear frame was 282.8 mm."
"our tracking approach is based on learning a dcf or a srdcf from samples of the target appearance. for image description, we employ convolutional features extracted from these samples. in each new frame, the learned dcf is applied on the convolutional features extracted from the predicted target location. a location estimate is then achieved by maximizing the detection scores."
"other investigations of the last decade have mainly focused on fe-based simulation methods. therefore, the relationship between tension and shear behavior was examined extensively to improve simulation models and results of forming simulations. in addition, there are several studies dealing with the purposeful generation of pretensions in shear testing and in manufacturing to prevent folding or delay the onset of folding. these studies are necessary for fe-based simulations and contribute to a better understanding of textile deformation behavior in general. however, they are less essential for kinematic draping simulations as they do not determine the critical shear angle or consider the global phenomenon of folding."
"in this work, we investigate the impact of convolutional features in two dcf based tracking frameworks: a standard dcf framework and the srdcf framework [cit] . contrary to in image classification, we show that activations from the first layer provides superior tracking performance compared to the deeper layers of the network. finally, we provide both qualitative and quantitative comparison of convolutional features with standard hand-crafted histogram descriptors, commonly used within the dcf based trackers."
"one of the most important requirements for the application of fiber-reinforced composites in large-scale productions is the reproducible production of frp components with a constant quality. reducing the time and resources necessary for the development of prototypes and to enhance virtual component development, appropriate methods for material tests and forming simulations are required. these methods, in turn, demand an application-oriented and, even more importantly, conclusive test technology to determine the characteristic material properties."
"as discussed above, the conventional dcf tracking approaches have demonstrated impressive performance in recent years. however, the standard dcf formulation is severely hampered by the periodic assumption introduced by the circular correlation. this leads to unwanted periodic boundary effects at both the training and detection stages. such periodic boundary effects limit the performance of the dcf in several aspects. first, the dcf trackers struggle in cases of fast motion due to a restricted search region. more importantly, the inaccurate and insufficient training data limit the discriminative power of the learned model and lead to over-fitting."
"the shear force f sh can be calculated directly for standard hinged picture frames depending on the shear angle, with equations (2) and (3) [cit] :"
"we provide a comparison of our trackers with 11 [cit] (dsst, samf and kcf) and the top 5 existing methods in our otb comparison (srdcf, samf, meem, dsst and kcf). table 3 [cit] toolkit [cit] . the first two columns contain the mean overlap score and failure rate over the dataset. the remaining columns report the accuracy, robustness and final rank for each tracker. our deepsrdcf achives the best final rank on this dataset. figure 7 [cit] dataset. figure 8 [cit] dataset."
"cf biaxial non-crimp fabric 0°/90° the amplitudes of maxima and minima revealed qualitatively different curve progressions. as an example, a maximum is initially formed in the case of sample 6, which decreased after about 12 s, as the minimum increased from this point forward. in contrast, the amplitude progression of sample 4 shows a continuous maximum, whereas the minimum remained relatively low. the analysis of the amplitude curve explains the differences of samples of a material variant in shear behavior (cf. figure 15 ). the evaluation of the peak-to-peak value, however, clearly indicates qualitatively comparable curve progressions and thus the reproducibility of the developed shear deformation. the deviations that occur (figure 17b ) result from inaccuracies in sample preparation as described above and cannot be avoided even if the test is performed with great accuracy. figure 18 shows the averaged results of the peak-to-peak values and the standard deviations over time for each material and material direction. besides the absolute measured heights, which strongly differ, a closer view on reproducibility is possible. it is obvious that, for the cf-based woven and biaxial non-crimp fabrics, the standard deviations at the beginning of picture frame testing were lower than at higher deflections. furthermore, the increase in standard deviation seems to be correlated with the beginning of folding. for the cf-based monoaxial non-crimp fabric and the gf/pa-based woven fabric, the standard deviations were nearly constant over time, but significantly higher than for the other materials. especially for the gf/pa-based woven fabric, reflections occurred during testing, influencing the base data set and leading to the obtained high standard deviations. furthermore, the observed results for cf monoaxial non-crimp fabric need to be considered critically as earlier investigations [cit] revealed that shearing is not an intrinsic behavior of monoaxial non-crimp fabrics. additionally, sample preparation is much more challenging, given by the loose construction. overall, the high standard deviations are caused by material-specific behavior, not by the accuracy of the extended picture frame test. hence, the additional optical evaluation of fold height is more reliable than an objective evaluation of folding. the amplitudes of maxima and minima revealed qualitatively different curve progressions. as an example, a maximum is initially formed in the case of sample 6, which decreased after about 12 s, as the minimum increased from this point forward. in contrast, the amplitude progression of sample 4 shows a continuous maximum, whereas the minimum remained relatively low. the analysis of the amplitude curve explains the differences of samples of a material variant in shear behavior (cf. figure 15 ). the evaluation of the peak-to-peak value, however, clearly indicates qualitatively comparable curve progressions and thus the reproducibility of the developed shear deformation. the deviations that occur (figure 17b ) result from inaccuracies in sample preparation as described above and cannot be avoided even if the test is performed with great accuracy. figure 18 shows the averaged results of the peak-to-peak values and the standard deviations over time for each material and material direction. besides the absolute measured heights, which strongly differ, a closer view on reproducibility is possible. it is obvious that, for the cf-based woven and biaxial non-crimp fabrics, the standard deviations at the beginning of picture frame testing were lower than at higher deflections. furthermore, the increase in standard deviation seems to be correlated with the beginning of folding. for the cf-based monoaxial non-crimp fabric and the gf/pa-based woven fabric, the standard deviations were nearly constant over time, but significantly higher than for the other materials. especially for the gf/pa-based woven fabric, reflections occurred during testing, influencing the base data set and leading to the obtained high standard deviations. furthermore, the observed results for cf monoaxial non-crimp fabric need to be considered critically as earlier investigations [cit] revealed that shearing is not an intrinsic behavior of monoaxial non-crimp fabrics. additionally, sample preparation is much more challenging, given by the loose construction. overall, the high standard deviations are caused by material-specific behavior, not by the accuracy of the extended picture frame test. hence, the additional optical evaluation of fold height is more reliable than an objective evaluation of folding."
"for kinematic draping simulations, a critical shear angle needs to be determined. practical experiments have revealed that simulation parameters cannot be clearly defined for all material constructions based on the shear force/shear angle curve of the picture frame test. instead, additional information is necessary to comprehensively characterize shear behavior."
"here, f −1 denotes the inverse dft. to obtain an estimate of the target scale, we apply the learned filter at multiple resolutions. the target location and scale in the image are then updated by finding the maximum correlation score over all evaluated locations and scales."
"in this paper, we investigate the impact of convolutional features for visual tracking. standard dcf based approaches rely on hand-crafted features for robust image description. we propose to use convolutional features within the dcf based framework for visual tracking. we show the impact of convolutional features on two dcf based frameworks: the standard dcf and the recently proposed srdcf. to validate our proposed tracker, we perform comprehensive experiments on three public benchmarks: otb, alov300++ [cit] . we show that the first convolutional layer provides the best results for tracking, this is suprising considering that the deeper layers are known to be better for general object recognition. we compare our proposed approach with some state of the art methods and obtain state of the art results on three benchmark datasets."
"in order to validate the hardware and software for recording and evaluating the shear deformation that occurs during the picture frame test, a reference geometry with a defined height profile (figure 10a ) was designed and manufactured. the aim of validation is to ensure that the height profile is measured correctly. since shear deformation is only measured centrally, based on the deflection of the projected laser beam, the dimensions of the reference geometry do not need to correspond to the sample geometry, only allow the projection of the laser beam. for this purpose, the reference geometry was positioned on the lateral pivot point of the square picture frame and the laser beam is projected. the optical measurement of this height profile and its comparison to nominal dimensions is made stationary state and requires all measuring system components to be properly aligned and a validated calibration. in order to validate the hardware and software for recording and evaluating the shear deformation that occurs during the picture frame test, a reference geometry with a defined height profile (figure 10a ) was designed and manufactured. the aim of validation is to ensure that the height profile is measured correctly. since shear deformation is only measured centrally, based on the deflection of the projected laser beam, the dimensions of the reference geometry do not need to correspond to the sample geometry, only allow the projection of the laser beam. for this purpose, the reference geometry was positioned on the lateral pivot point of the square picture frame and the laser beam is projected. the optical measurement of this height profile and its comparison to nominal dimensions is made stationary state and requires all measuring system components to be properly aligned and a validated calibration."
"we perform experimental evaluation on three public benchmark datasets: the online tracking benchmark (otb) [cit], the amsterdam library of ordinary videos for tracking (alov300++) [cit] and the visual object tracking (vot) [cit] . table 1 . the spatial size and dimensionality of the convolutional features extracted from the employed network. layer 0 denotes the input rgb image, after the necessary preprocessing steps."
"the lack of reproducibility and traceability as well as the discontinuity and high effort of existing solutions (for both pft and bet) for determining the critical shear angle by means of fold dimensions were the main drivers for the developed test stand presented in this study. this initial situation motivated the joint research project of the institute of textile machinery and high performance material technology of tu dresden, and the department of polymer engineering of tu budapest. the objective was to exploit the detection and quantification of folds during the picture frame test and, subsequently, to achieve a cost-efficient solution. therefore, we investigated image processing methods in terms of suitability and added necessary equipment to the picture frame test stand."
"to determine the accuracy of the extended picture frame test stand, the reference geometry was measured after calibrating. the deviations between measured and nominal height were mostly less than 5%, for experimental investigations, different reinforcing textiles which are widely used, e.g., in automotive and wind power plant engineering, were selected, table 1 . they vary in terms of fabric construction and/or fiber material, e.g., cf, glass fiber (gf), polyamide (pa) or polyester (pes), thus representing the group of commonly investigated materials."
"the laser beam projects a planar curve onto the material to be measured. taking a picture of the projected curve on the material surface is a plane-to-plane perspective transformation as a bijection. a perspective transformation by homogenous coordinates is a linear transformation [cit] projecting a quadrangle to a quadrangle. the transformation matrix has eight independent coordinates (p0, p1, …, p7), as shown in equation (4):"
"another approach is to identify folding by means of a gray-scale image. however, subsequent image analysis is extremely time-consuming and the results are often inaccurate. commercial 3d deformation detection systems are provided by gom. however, this technique is quite costly and laborious in terms of sample preparation (pontos system) and the evaluation of the recorded data (aramis system). our own experience has shown that both solutions are prone to error as data points were missing during the progressive test process."
"the geometry data of the time-dependent shear deformation can be imported into a software environment for statistical calculations and graphical output. the software used in this application case was \"r\" [cit] . as described in section 3.1, all data of a test cycle are leveled based on a first time step (initial state of the shear test), in order to eliminate the intrinsic error of the measurement system. the cleansed data can be used in a further process to visualize the deflection of the laser beam over time as a 3d area. figure 16 shows the characteristics of the folds including their minima and maxima over a constant sample width. a color gradation allows the assignment of numerical values to different fold heights. moreover, the amplitude maxima and minima of each time step can be illustrated both in a separate diagram (figure 17a ) and as a sum of their amount (i.e., as peak-to-peak value, figure 17b ) as a function of time and the shear angle. the recorded surface deformations show generally similar results ( figure 15 ). minor differences in the characteristics are the result of the above-mentioned issues in sample preparation. moreover, it is obvious that similar shear force/shear angle curves do not lead to similar deformation behavior. to determine the exact beginning of fold formation and the geometric fold characteristics under shear stress, further analysis of the measured surface deformation is needed."
"one of the most important requirements for the application of fiber-reinforced composites in large-scale productions is the reproducible production of frp components with a constant quality. reducing the time and resources necessary for the development of prototypes and to enhance virtual component development, appropriate methods for material tests and forming simulations are required. these methods, in turn, demand an application-oriented and, even more importantly, conclusive test technology to determine the characteristic material properties."
"the visual object tracking (vot) challenge is a competition between short-term, model-free visual tracking algorithms. for each sequence in the dataset, a tracker is evaluated by initializing it in the first frame and then restarting the tracker whenever the target is lost (i.e. at a tracking failure). the tracker is then initialized a few frames after the occurred failure. the trackers in vot are evaluated in terms of an accuracy score and a robustness score. these scores figure 6 . [cit] dataset. we show success plots for four attributes: scale variation, in-plane rotation, fast motion and occlusion. the number in each plot title indicates the amount of sequences associated with a particular attribute. our trackers provide consitent improvements compared to existing methods."
"to evaluate the reproducibility of the results, nine samples were tested for each material and material direction. figures 14 and 15 show the shear force/shear angle curves as well as the recorded surface deformations for the ± 45° biaxial non-crimp fabric with the sewing thread in the tensile direction. the deviations of about 20% in the shear force/shear angle curve result from the properties of the semi-finished product caused by fabric construction. for non-crimp fabric the yarn systems are not crossed but held by a sewing thread to ensure good drapeability. the missing crossing lead to a nearly perfectly stretched out situation of the reinforcing fiber. thus, high mechanical characteristics in the reinforced directions (e.g., mono-, bi-tri-or quadriaxial) can be realized in frp products. but its low resistance to displacement, restricts its handleability, making it almost impossible to achieve exactly reproducible sample preparation in the shear test. cf woven fabric, twill gf/pa woven fabric, plain figure 13 . measured height profiles of the laser beam as a function of time (one sample per material is shown); x-axis: time; y-axis: width; z-axis: height (also see figure 6 )."
"recently, convolutional neural networks (cnns) have significantly advanced the state-of-the-art in many vision applications, including object recognition [cit] and object detection [cit] . these networks take a fixed sized rgb image as input to a sequence of convolution, local normalization and pooling operations (called layers). the final layers in the network are fully connected (fc), and are typically used to extract features for classification. cnns require a large amount of training data, and are trained on the large scale imagenet dataset [cit] . it has been shown that the deep features extracted from the network (the fc layer) are generic and can be used for a variety of vision applications [cit] ."
"to evaluate the reproducibility of the results, nine samples were tested for each material and material direction. figures 14 and 15 show the shear force/shear angle curves as well as the recorded surface deformations for the ± 45° biaxial non-crimp fabric with the sewing thread in the tensile direction. the deviations of about 20% in the shear force/shear angle curve result from the properties of the semi-finished product caused by fabric construction. for non-crimp fabric the yarn systems are not crossed but held by a sewing thread to ensure good drapeability. the missing crossing lead to a nearly perfectly stretched out situation of the reinforcing fiber. thus, high mechanical characteristics in the reinforced directions (e.g., mono-, bi-tri-or quadriaxial) can be realized in frp products. but its low resistance to displacement, restricts its handleability, making it almost impossible to achieve exactly reproducible sample preparation in the shear test. the recorded surface deformations show generally similar results ( figure 15 ). minor differences in the characteristics are the result of the above-mentioned issues in sample preparation. moreover, it is obvious that similar shear force/shear angle curves do not lead to similar deformation behavior. to determine the exact beginning of fold formation and the geometric fold characteristics under shear stress, further analysis of the measured surface deformation is needed."
"visual tracking is the task of estimating the trajectory of a target object in an image sequence. it has many important real-world applications, such as robotics [cit] and road scene understanding [cit] . in the generic tracking problem, the target can be any object, and only its initial location is known. this problem is challenging due to several factors, such as appearance changes, scale variations, deformations and occlusions. most state-of-the-art approaches tackle the tracking problem by learning a discriminative appearance model of the target object. such approaches [cit] rely on rich feature representations for describing of the target and background appearance. this paper investigates robust feature representations for visual tracking."
"as discussed above, the common strategy is to extract deep features from the activations of the fc layer of the pre-trained network. other than the fc layer, activations from convolutional layers of the network have recently been shown to achieve superior results for image classification [cit] . these convolutional layers are discriminative, semantically meaningful and contain structural information crucial for the localization task. additionally, the use of convolutional features mitigates the need of task-specific finetuning employed with standard deep features. in such approaches, it has been shown that activations from the last convolutional layer provides improved results compared to other layers of the same network [cit] ."
"profile for experimental investigations, different reinforcing textiles which are widely used, e.g., in automotive and wind power plant engineering, were selected, table 1 . they vary in terms of fabric construction and/or fiber material, e.g., cf, glass fiber (gf), polyamide (pa) or polyester (pes), thus representing the group of commonly investigated materials. for experimental investigations, different reinforcing textiles which are widely used, e.g., in automotive and wind power plant engineering, were selected, table 1 . they vary in terms of fabric construction and/or fiber material, e.g., cf, glass fiber (gf), polyamide (pa) or polyester (pes), thus representing the group of commonly investigated materials."
"for kinematic draping simulations, a critical shear angle needs to be determined. practical experiments have revealed that simulation parameters cannot be clearly defined for all material constructions based on the shear force/shear angle curve of the picture frame test. instead, additional information is necessary to comprehensively characterize shear behavior."
"however, both methods are questionable for several reasons. first of all, these shear force/shear angle diagrams correspond to ideal shear behavior. for realistic data sets, it is more challenging to define different linear zones (figure 2c) . also, the subjective definition of linear zones leads to minor deviations. additionally, in souter's method, the definition of the posterior linear zone is strongly dependent on the procedure parameter of maximum transverse distance, and consequently, influences the intersection point. as can be seen, the post-determination of critical shear angle from shear force/shear angle data is not reliable and prone to error."
"cf biaxial non-crimp fabric 0°/90° this provides the user with a tool for automatically detecting the exact point of time of fold formation in a reproducible manner and for relating it to a critical shear angle. depending on the geometry of the construction part to be produced and on the specific requirements, a limit value can be defined for a critical fold height. if this limit value is exceeded, the critical shear angle can be assigned, based on the time step. consequently, the shear angle serves as a reliable decision criterion in kinematic draping simulations."
"we evaluate the impact of using the convolutional features in the dcf (section 3.1) and srdcf (section 3.2) approaches. we name our trackers deepdcf and deep-srdcf respectively. for the deepsrdcf, we reduce the feature dimensionality of the first layer to 40 using principal component analysis (pca). the pca basis is computed in the first frame and then remains fix through out the sequence. our trackers are evaluated on the full otb dataset (containing 50 videos) and compared with 15 stateof-the-art trackers: srdcf [cit], dsst [cit], kcf [cit], samf [cit], act [cit], tgpr [cit], meem [cit], struck table 2 . the mean overlap precision (op) and distance precision (dp) in percent on the otb dataset containing all 50 videos. the two best results are shown in red and blue respectively. we only report the results of the top 10 performing trackers."
"in addition, there are a few studies also investigating multi-and uni-directional non-crimp textiles (ncf and ud-ncf, respectively) [cit] . for ncfs, slippage was observed starting at a shear angle of 40° [cit] . this is a major disadvantage of bet and leads to a minor deviation between the mathematically calculated shear angle and the optically measured shear angle. consequently, the applicability of bet is questionable for ncfs, because of the missing pin joint arrangement and the spuriously made assumption regarding slippage. nevertheless, such effects are also encountered in the practice of manufacturing frp products and need to be considered."
"the geometry data of the time-dependent shear deformation can be imported into a software environment for statistical calculations and graphical output. the software used in this application case was \"r\" [cit] . as described in section 3.1, all data of a test cycle are leveled based on a first time step (initial state of the shear test), in order to eliminate the intrinsic error of the measurement system. the cleansed data can be used in a further process to visualize the deflection of the laser beam over time as a 3d area. figure 16 shows the characteristics of the folds including their minima and maxima over a constant sample width. a color gradation allows the assignment of numerical values to different fold heights. moreover, the amplitude maxima and minima of each time step can be illustrated both in a separate diagram (figure 17a ) and as a sum of their amount (i.e., as peak-to-peak value, figure 17b ) as a function of time and the shear angle. the recorded surface deformations show generally similar results (figure 15 ). minor differences in the characteristics are the result of the above-mentioned issues in sample preparation. moreover, it is obvious that similar shear force/shear angle curves do not lead to similar deformation behavior. to determine the exact beginning of fold formation and the geometric fold characteristics under shear stress, further analysis of the measured surface deformation is needed."
"comprehensive experiments are performed on three benchmark datasets: the online tracking benchmark (otb) [cit], the amsterdam library of ordinary videos for tracking (alov300++) [cit] and the visual object tracking (vot) [cit] . our results demonstrate that superior performance is obtained by using convolutional features compared to standard hand-crafted feature representations. finally, we show that our proposed tracker achieves state-of-the-art tracking performance on all three benchmark datasets. figure 1 provides a comparison of our tracker employing convolutional features with commonly used feature representations within the same dcf based tracking framework."
"the hardware and software developed for the recording and graphical evaluation of shear deformations by means of a laser beam enables fold formations to be automatically detected and quantified, and to be related to shear force/shear angle curves. hence, a tool offering a wide variety of additional information for analyzing the shear behavior of reinforcing textiles is available. in the future, this tool may help users in practice to decide on suitable materials and to improve transparency in the design and construction process."
"we employ the first layer layer for the remainder of our experiments. figure 3 shows a comparison of the first convolutional layer with hand-crafted features commonly employed in correlation-based trackers. we compare with using grayscale intensity (i), histogram of oriented grandients (hog) [cit] and color names (cn) [cit] . the success plot displays the mean overlap precision over all 35 color videos in the otb dataset. similar results are obtained using hog and cn. the combination hog+cn achieves slightly better performance, with an auc of 50.2%. however, the convolutional features provides improved performance, with an auc of 52.1%. the activations of the various convolutional features are shown in figure 4."
"the system is calibrated with a rectangular device with an led in each corner (figure 9a ). during the calibration process, the rectangle is positioned in the laser lighted plane. this means that the device is aligned in the picture frame in such a way that it is positioned centrally and planarly in all planes, sagittal, transverse and frontal, with the frontal plane corresponding to the position of the shear specimen in the initial state. figure 9b shows an image of the calibration tool. with the measured picture coordinates and the real coordinates of the leds, the eight coordinates of the homogenous transformation matrix can be calculated. (5) there are eight unknown coordinates and eight equations, equation (6):"
"the extended test setup can be integrated into the currently established picture frame test without significant efforts. moreover, no additional sample preparation is required, which results in considerable time savings as compared to currently available 3d deformation measuring systems. therefore, this method is a cost-efficient solution tailored to small and medium-sized companies. moreover, the accuracy of the extended picture frame test stand has been demonstrated for several high performance textile fabrics. additionally, further knowledge was gained regarding the influence of fabric construction, e.g., the linear density of sewing thread, on deformation behavior under shear stress. this is a very interesting inspiration for more detailed investigations in future, because this affects not only the users of such materials, but also the producers. another interesting application is the possibility to evaluate fe-based simulation by comparison of the attained surface deformations."
"a group of academic and industrial researchers conducting suitable picture frame test methods as well as bet methods was set up for benchmarking. five picture frame devices were investigated and compared [cit] . all presented picture frames were based on rigid clamps, but the dimensions differed and consequently lead to different sample sizes. three normalization methods were compared. finally, a normalization method with an energy approach considering both the side length of the fabric and of the picture frame [cit] was recommended [cit] . besides different dimensions, two picture frames use a lever mechanism, for which the amplification needs to be considered in the calculation of shear force. a main disadvantage of all presented picture frames were the induced in-plane tensions due to rigid clamping as they significantly influence the beginning of folding [cit] . additionally, the frame itself and each sample require elaborate preparation. in particular, precise alignment of the sample while being clamped on the frame is essential, or else additional tensions are generated, which also cause folding to begin earlier."
"ours intensity intensity+cn intensity+hog figure 1 . a comparison of the proposed feature representation with three commonly employed hand-crafted features, namely image intensity, color names (cn) and histogram of oriented gradients (hog). tracking results from our dcf tracker on three example sequences are shown. the convolutional features used in our tracker provides a richer description of target apperance, leading to better performance."
"with a shear deformation of 28° (corresponding to a deflection of 60 mm), the distance was 206 mm. the laser beam was captured over a constant width of approx. 150 mm during the picture frame test. image processing software supported by opencv calculated the deflection curves of the laser beam for every single time step. after calibration (see section 3.1) and interpolation, the geometric data of the deflection curves were determined as a function of time ( figure 6 ) and were available for further analysis. the error of the measurement system resulting from an incompletely level position of the sample is eliminated by comparing the geometry data of each time step with the first time step (initial state). the force/displacement diagram and the shear force/shear angle diagram are recorded simultaneously in accordance with the optical detection of the laser beam. this allows a distinct force and a corresponding shear angle to be related to the respective shear deformation for each time step ( figure 7 ). the error of the measurement system resulting from an incompletely level position of the sample is eliminated by comparing the geometry data of each time step with the first time step (initial state). the force/displacement diagram and the shear force/shear angle diagram are recorded simultaneously in accordance with the optical detection of the laser beam. this allows a distinct force and a corresponding shear angle to be related to the respective shear deformation for each time step ("
"another approach is to identify folding by means of a gray-scale image. however, subsequent image analysis is extremely time-consuming and the results are often inaccurate. commercial 3d deformation detection systems are provided by gom. however, this technique is quite costly and laborious in terms of sample preparation (pontos system) and the evaluation of the recorded data (aramis system). our own experience has shown that both solutions are prone to error as data points were missing during the progressive test process."
knowing the real positions of four points in the lighted plane (t i ) allows the real coordinates of other points on the lighted plane to be computed.
"the dcf based tracking approaches learn a correlation filter to discriminate between the target and background appearance. the training data is composed of observed samples of the target appearance and the surrounding background. [cit] initially proposed the mosse tracker, which is restricted to using a single feature channel, typically a grayscale image. [cit] introduced a kernelized version of the tracker, to allow non-linear classification boundaries. more recent work [cit] have achieved significant increase in tracking performance by investigating the use of multi-dimensional features in the dcf tracking framework."
knowing the real positions of four points in the lighted plane (t i ) allows the real coordinates of other points on the lighted plane to be computed.
"we evaluate our tracker by providing an attribute-based analysis on the otb dataset. the sequences in the dataset are annotated with 11 different attributes: occlusion, outof-plane rotation, in-plane rotation, low resolution, scale variation, illumination variation, motion blur, fast motion, background clutter, out-of-view and deformation. figure 6 shows success plots of four different attributes: scale variation, in-plane rotation, fast motion and occlusion. for clarity, only the top ten trackers in each attribute plot are shown. both our deepsrdcf and deepdcf trackers achieves superior performance compared to the existing methods. in case of scale variation, the standard srdcf method obtains an auc score of 59.3%. our proposed deepsrdcf provides a gain of 4.3% compared to the standard srdcf approach with hand-crafted features. in case of in-plane rotation, the two dcf based trackers srdcf and dsst provides the best results among existing trackers. our approach based on deep features and srdcf achives the best performance with an auc score of 60.2%. similarly, our deepsrdcf approach obtains favorable results for fast motion and occlusion, compared to existing trackers."
"to mitigate the periodic boundary effects, [cit] recently proposed spatially regularized correlation filters (srdcf), leading to a significant performance boost for correlation based trackers. the authors introduced a spatial regularization function w that penalizes filter coefficients residing outside the target bounding box. this allows an expansion of the training and detection regions without increasing the effective filter size. instead of (1), the following cost is minimized,"
"we assume the longest path from s to n is denoted by l. the last half of l for any node j on l is also a longest path from j to n, which is called the principle of optimality. we can utilize the feature to search out the original problem's optimal solution via the sub-problems' optimal solutions. specifically, the longest path from s to n can be obtained by figuring out the following recurrence formula from node n − 1 to s. in the formula, the longest path from j to n is denoted as f (j) . f (j) is called the node j's evaluation value. f (n) is defined as s for terminal node n. f (s) represents the optimal solution for initial node s."
"a s personalized medicine expands, increasingly detailed biomedical data must be integrated to better understand normal function and evolution of multifactorial chronic disease in clinical trials and individual treatment decisions. the complexity of molecular, cellular, and tissue interactions, however, is a fundamental barrier to extracting the complicated relationships that underlie human physiology."
"ptm data can be visualized with ad-hoc software packages, that we describe in the next, allowing the interactive relighting of the scene that can be used to find the best way to discover relevant information. specific methods to enhance structures using multiple light information have been proposed as well [cit] ."
"typically utilized to figure out who is responsible for what. in essence, accountability means that the system is recordable and traceable, which implies that making any entity in the system accountable for all its actions. under such a consideration, our scheme is accountable as the evaluators can verified each other and work together to obtain the optimal result, which can be used as an evidence for dispute resolution; therefore, no one can deny its actions. thus, we claim that the scheme has the property of accountability."
"as shown in fig. 2, our system model consists of three major entities: mask publisher (mp), evaluators (e), weight publishers (wp). in the following, we briefly summarize the major functions of each entity."
"note that our approach does have one disadvantage: if the number of nodes is very large, our scheme may be invalid sometimes because the combinatorial auction's winner determination problem is np-complete."
"from the maps i (normalized dividing by its maximum) and θ estimated we can then apply the classical steps of canny edge detection [cit] e.g. non-maxima suppression and hysteresis thresholding. the first step checks if the intensity values are maximal perpendicularly to the edge direction and put to zero all the matrix elements that do not meet this condition. angles representing edge directions are in the implementation rounded at sampled values of 0, 45, 90 and 135 degrees. the result is a list of candidate edge points that are sorted by decreasing values. edge lines are finally traced connecting edge candidates with hysteresis thresholding, e.g. using two different thresholds. edge candidate location with the highest intensity is removed from the list and used to start paths if the corresponding value is larger than the highest threshold, then neigboring candidates (that are as well removed from the list) are joined until their values are higher than the second threshold. the procedure is repeated until all the possible starting points have been removed."
"otherwise, r i is false, which means that e i might be a cheater. the share will be discarded. 3) recover the polynomial: the polynomial hm i can be uniquely determined as follows:"
"in order to obtain the experimental quality factors of the coils at different frequencies, we used a series-resonant network and measured the resistance at the resonant frequency by using an hp4194a impedance analyzer. appropriate values of capacitors were used in order to tune the resonant frequency. fig. 3 shows both the measured resistances and the resulting quality factors. as can be seen, resistance values increased with frequency, which is due to the joint combination of skin and proximity effects and the losses of the ferrite [cit] . quality factor increased steeply at low frequencies achieving a maximum (ca. 40) around 40 khz. in order to estimate the coupling factor (k) over the separation distance (d) of the coils, we used the simulation program comsol. fig. 4 shows an axisymetric model for the primary and secondary coils, and illustrates the parameter d. sizes of the inductors were in accordance with those presented in figure 2 . the contour areas r1 to r6 were defined as ferrite whereas c1 and c2 (wire coil) were defined as copper. a relative permittivity (µ r ) [cit] was used for the ferrite. spherical domain boundaries were used and set to zero magnetic insulation. 5 shows the simulated results of k over d. as can be seen, k steeply increased for shorter distances."
"in this article, we shall review and further develop the salient features of diffusion geometry and approximation theory needed to ''learn locally'' from the acquired data. in contrast to common clustering methods used in biomedicine, we explicitly use that the clusters represent disease stages, i.e., are ordered quantitatively in a progressing fashion. thus, some unspecific ordering is a-priori fed into the clustering process and is specified in the final result. moreover, instead of interpolating on the training data, which usually leads to instabilities with large training sets, we allow our algorithm to correct misclassified training points."
"in this subsection, we discuss the security properties of the proposed scheme in terms of resistance against active attacks, resistance against passive attacks, nonrepudiation, and accountability."
"in many practical applications, one needs to go beyond an understanding of the manifold and answer queries based on the data. these queries can be modeled mathematically as functions in the (unknown) manifold. this function may be known to us on few training points, and we aim to accurately predict the value of the function of items that are not yet observed. models for this have been developed as eigenmaps/ diffusion maps [cit] ), multiscale approaches [cit] b; [cit], and nonlinear dimension reduction [cit] ."
"the optimization approach has the advantage that the penalty functional may be chosen to reflect some domain-specific knowledge about the target function. also, even if one does not expect any function underlying the phenomenon, one gets some smooth model to work with. on the other hand, when we do not know with absolute certainty any physical model that underlies the data, and are seeking a function on x as a model anyway, then it is more natural to assume that there is an underlying function from which the data is sampled, even though the function itself is yet unknown, so that the approximation approach seems more natural."
"finite sum, and f n, as a function of only one variable x or y, is contained in p n . under the technical assumptions collected in appendix a, the estimate"
"6.1.1. cleveland heart disease database: learning disease stages. the cleveland heart disease database (chdd) [cit] contains 297 patterns with 13 attributes and is grouped into five progressive heart disease stages (values 0,1,2,3,4), where 0 corresponds to normal heart conditions. we removed six patterns due to missing values. due to homeostasis and its failure in progressed disease, we expect the query function f to be smoother on normal heart conditions and early disease stages, while later stages may form a more heterogeneous group. we compare our method to support vector machines (svm), in which clusters are derived through sequential binary clustering. clusters are evaluated by means of binary false-positive or false-negatives for each disease stage. indeed, our proposed method recovers f consistently better than svm for the values 0, 1, and 2 when dealing with few training points ( table 1) . as expected, our kernel performs poorly on the stages 3 and 4, implying the lack of smoothness of f within these progressed stages. the transition from 0 to 2 seems to be steered by a smoother process, which is reflected by a smoother query function yielding better results than svm methods. [cit] contains 683 patterns with 9 attributes. we aim to predict quantitative attributes. in fact, we randomly select 200, 300, and 400 training points to learn the attribute ''clump thickness'' (ranging from 1 to 10) and aim to predict its values on the remaining data. we call it a hit when the prediction is within a radius of 1, i.e., if the measured size was 3, the predictions in the interval [cit] are counted as a hit. the excellent performance of our proposed method by means of sensitivity (number of hits divided by the cluster size) for few training data is shown in table 2 ."
"where (n x, n y, n z ) is the unknown normal for the specific pixel, (l x, i, l y, i, l z, i) is a light direction and l i is the value of the polynomial for that direction. the number of directions here used is 32. the set of directions is obtained by sampling the polar coordinates that identify a direction (elevation and azimuth angles) by step of 20 degree starting from 10 to 70, for the elevation, and subdividing the azimuth in 8 parts. in this way the sampling is not uniform (more samples at the pole), but the relatively high number of samples provides a good estimation of the real surface normal. the estimated normal differs from the real one as the material deviates by a lambertian reflector. the linear system of equation (5) can be solved by using a singular value decomposition (svd) approach which consists in decomposing the matrix a into three sub-matrix that can be combined to obtain a robust least square solution of the system."
"in the context of data-defined manifolds, we do not explicitly know any formulas for the local coordinate charts. therefore, it is not easy to define the notion of derivatives. nevertheless, we can redefine the notion of an infinitely differentiable function on our unknown manifold as membership in every smoothness class w s ðxþ. thus, the set of all infinitely differentiable functions is denoted by"
note that the cumulative product in (4) is essentially the lagrange coefficient. the correctness of (4) can be easily verified based on the definition of f(x).
"the visualization enhancement is possible thanks to the information provided by the per-pixel bi-quadratic polynomial. here, we propose an alternative way to help the experts in the artifacts' examination, i.e. not to try to enhance the rendering, but to extract specific information from data."
"this normal estimation, however, can be not accurate in certain cases, and tends to be oversmoothed [cit] . to reduce these problems we propose to compute normals using a typical photometric stereo [cit] approach that consists in assuming the material lambertian, i.e. a pure diffusive material, and setup a linear system in the following way:"
"proof if a bidder (weight publisher) make a bid, because that the evaluators work together to figure out the optimal result, and each participant is verified by other participants. if some weight publisher deny making the bid, the other evaluators can work together to trace all the internment mask result to verify whether the weight publisher is lie or not according to the optimal result."
"using the hp tool ptm files can be created from sets of images acquired with standard cameras, provided that the direction of the light is known. a trick to evaluate this direction directly from the acquired images is to include a reflective sphere near the object allowing the direction estimation (l u, l v ) from the specular highlight. for a correct creation of ptms of object of interest, therefore, it is not necessary to have an expensive setup: for the acquisition we made in the archaeological site in the holy land, we used simply:"
"there are two common approaches to solving this problem. in the first approach, one finds the extension f c as a solution of some minimization/regularization problem, for example,"
"in this paper, we presented a privacy-preserving combinatorial auction without an auctioneer scheme. in our scheme, the price is represented as the degree of a polynomial; thus, the degree of the sum/product of the two polynomials construct the maximum/sum of the degree of two polynomials. the bidders information is hidden, and the legitimacy of the evaluator is also verified based on secret sharing, which can resists collusion attacks."
"the outline is as follows. in section 2, we briefly discuss two approaches to reconstructing the query function from training data. in section 3, we present our local learning approach. the numerical implementation is discussed in section 4. in section 5, we outline our scheme for the special case in which the manifold is the sphere. we apply our methods to analyze several biomedical datasets in section 6."
"where (l u, l v ) is the light direction vector (normalized) projected on the image plane (see fig. 1 ). since this vector is normalized the third component is redun-"
"is the final color for the given pixel and the given light direction. this type of ptm is called lrgb ptm since the \"luminance\" term modulates the rgb color channels. in the case of an rgb ptm each channel has its own coefficients, for a total of 18 coefficients per pixel. we refer to l(l u, l v, x, y) as \"luminance\" but, more precisely, this term represents the reflectance functions with the self-shadowing effects embedded. other basis can be substituted for ptm to better approximate materials characterized by more complex reflectance behavior, such as gold or marble [cit] . hewlett-packard labs, where this technology has been developed, freely distribute software tools for the creation and the interactive visualization of ptm data as well as test data, available at the web site http://www.hpl.hp.com/research/ptm."
"section ii first presents a theoretical analysis of the power transferred to the load with a pair of magnetic coupled resonators. powering range distance and power efficiency are also analyzed. then, section iii presents the selected commercial coils. quality factors are measured for different frequencies and exposure regulations are verified through simulations. section iv presents the design of the primary and secondary networks. the primary network includes a class d power amplifier whereas the secondary network includes a rectifier stage and a voltage regulator in order to properly power the autonomous sensor. section v shows the measured performance. finally, section vi concludes the work. fig. 1 shows the equivalent circuit of a pair of magnetically coupled series resonators, being the left-hand and right-hand networks the primary and secondary, respectively. v 1 is a sinusoidal signal that models the output of the power amplifier that drives the primary network; i 1 and i 2 stand for the currents of the primary and secondary; l 1 and l 2 model the coils; c 1 and c 2 are the added capacitances to work at resonance; r s models the output resistance of the power amplifier, r l1 and r l2 model the losses of the coils, and r load models the load; and finally m models the mutual inductance between the coils, where being k the coupling factor between the coils. we neglect the losses of the capacitors as they usually are much lower than that of inductors. the series-resonant primary network maximizes the injected current across l 1 . for the secondary, a series-resonant tank was also chosen."
η 1 accounts for the percentage of the generated power transferred to the secondary whereas η 2 accounts for the percentage of the received power at the secondary that is further transferred to the load. both η 1 and η 2 must be high in order to have a high overall efficiency.
"vehicles can also benefit from inductive powering. in particular, some vans and minivans incorporate removable seats in order to flexibly arrange their internal space. wiring that seats in order to incorporate, for instance, seat belt detectors, can become unpractical. so, in some vehicles a passive detection is performed via magnetic coupling. the addition of wireless power could allow the incorporation of new devices that require some amount of power, such as a seat occupancy sensor or a microcontroller that adds intelligence to the removable seat. in these applications, the available or acceptable space for the coils is rather limited. this paper explores the feasibility of remotely powering, via magnetic coupling, autonomous sensors for occupancy and belt detection in removable vehicle seats. the primary and secondary networks may be placed respectively in the car floor and at the bottom of the removable seat. thus, the application is space-constrained and, consequently, small-size coils have to be used. in addition, the power demand of the load is considered between tens and hundreds of milliwatts, according to commercial radio devices used in autonomous sensors. in another paper, we propose seat occupancy and belt detection via a passive inductive link [cit] ."
we selected commercial devices from fastron (pist model) with a value of 1 mh. these coils use ferrite as magnetic core material. sizes can be shown in fig. 2 (values are in millimeters). published dc resistances are approximately 1.5 ω.
"in ancient tablets or stones, structures of interest are human artifacts and inscriptions that are usually composed by a set of lines. the classical image processing tools used to search lines in images are edge detectors, trying to create lines joining pixels with high values of image gradient. edges should highlight the important structures in the image rejecting noise and negligible information. we can do something similar for the ptm image data: instead of processing relighted images trying to find optimal visualization, we can simply extract edges from the ptm coefficients and use them to interpret more easily the data acquired."
"remote or wireless power transmission via inductive coupling has been around for a long time. high-power transfer includes battery recharging of electrical vehicles [cit] and a broad range of industrial applications [cit] whereas low-power transfer includes rfid systems [cit], biomedical implants [cit], or portable consumer electronic products [cit] ."
"reflection transform imaging (rti) techniques encode a per-pixel parametric approximation of the reflectance behavior of the object depicted, allowing the creation of rendered images with varying lighting of the scene of interest. rtis are becoming a popular tool to study archaeological sites, stones and artifacts. the analysis of the acquired data is usually done interactively by testing the effects of different light sources on the rendered image. automatic tools able, for example, fig. 7 . a: frontal relighting of a square lid of a tomb. b normal discontinuity map extracted using derivatives at two scales. c: edges extracted after hysteresis thresholding. to identify automatically regions of interests in the images could be extremely important to help archaeological study of large collections of data. in this paper we proposed a simple edge detector applied to ptm data (a type of rti) that can be used to enhance lines at different scales, possibly corresponding to human artifacts. computing 3d surface normals discontinuity and tracing discontinuity profiles on the 2d image plane using a classical edge detection approach, we found that is possible to capture relevant information on the acquired data. normals are computed using a photometric stereo approach on relighted images and not directly derived from the ptm coefficients, allowing a more precise result."
"necessarily, any such model cannot be expected to yield perfect reproduction of the actual target function. the subject of approximation theory deals with the intrinsic errors inherent in constructing different kinds of models for the target function. in traditional scenarios, the accuracy of approximation is closely related to the smoothness of the function. because of this history, many experts in approximation theory nowadays consider the accuracy of approximation itself to be a measurement of smoothness. this viewpoint is particularly useful in our setting, where the manifold is unknown and, therefore, it is impossible to define the smoothness in a classical manner."
"the last named author and his collaborators have developed approximation theory tools applicable in the current context in a series of papers [cit] . a particularly interesting aspect of this theory is a definition of pointwise smoothness of the target function. the research has also enabled us to devise specific algorithms, extending the theory developed for the understanding of geometry, with the property that the rate of convergence of these algorithms in neighborhoods of different points completely characterize local smoothness properties of the target function at those points."
"to study the asymptotics for n / n, we call for a sequence of training sets c n that induce quadrature formulas of strength an. we shall verify in appendix c that, for f 2 w"
"-put the camera on the tripod at a distance of approximately 5/3 of the side of the object from its center -measure aperture and shutter speed under the illumination of the central light. keep these values fixed for all the photos, in order to have a constant exposure."
"our future research will focus on the following direction: design more efficient approaches based on greedy algorithm to protect the privacy of combinatorial auction, which would be much more suitable for practical applications."
"a recent idea, originating in computational harmonic analysis, is to let the data speak for itself. in this approach, one deals typically with high-dimensional, unstructured data. in theoretical analysis, one assumes that the data represents a sample from some unknown low-dimensional manifold embedded in a highdimensional ambient euclidean space. the objective is then to understand the geometry of this manifold. thus, statistical techniques have been devised to estimate the dimension of this manifold [cit] . a simulation of brownian motion is expected to reveal the relative neighborhoods of different data points, as well as provide local coordinate systems for the manifold [cit] . see the special issue [cit] for an introduction to these ideas. [cit] ."
"we will demonstrate below that in the approximation approach, we can construct a linear operator with mathematical performance guarantees of equation (2). we do not need to solve any minimization problem, so that all the computational issues mentioned above are avoided altogether. moreover, we can design this operator in a manner that its performance guarantees are automatically better on regions of x where the target function is ''smoother.'' this does not involve a careful detection of edges and partitioning of x."
"the normal maps computed with the proposed technique appear sufficiently accurate to capture the fine structures of the ptm image. fig. 2 shows the results obtained on a sample ptm downloaded from the hp site representing an ancient tablet from the archaeological research collection of the university of southern california. fig. 2 a shows a single relighting of the data, fig. 2 b the normal map obtained with our photometric stereo approach and fig. 2 the normal discontinuity map i computed with two-scale derivatives. the result shows clearly the inscriptions. it is also interesting to compare the edge map obtained applying non maxima suppression and hysteresis thresholding on the previous map, compared with a similar edge detection performed on a single relighted image. it is possible to see that the classical edge detection as well as the gradient map does not highlight correctly the structures of interest, while the ptm based method does 3 similar results can be obtained also on our on-field acquisitions on the holy land site. fig. 4 shows an image created by relighting a ptm created with the acquired images. it represents a large stone wall in a tomb, and it is difficult to determine if something interesting can be found in it using the default frontal illumination used."
"recently, combinatorial auctions have become an interesting domain, which allow that multiple goods are sold simultaneously and any combination of goods can be bid. for example, fcc spectrum, network routing, and railroad segment can be auctioned."
"a.2. assumption on products of polynomials. there is a ‡ 2 such that, for all f ‚ g 2 p n, their product fg is contained in p an . in fact, we only need the weaker condition saying that, for"
"age-related macular degeneration is the most common cause of blindness among the elderly population in the western world [cit] . aging of the human retina is universally associated with microscopic changes within the retinal pigment epithelium (rpe), including increased number and volume of fluorescent lipofuscin granules [cit] . in a sensitivity (one minus false negative rate) for disease stages 0 to 4 using the dataset chdd in section 6.1.1. with 40, 100, and 200 training points, averaged over 50 instances for each method. our local kernel method performs better for few training data than svm on disease stages 0, 1, and 2, in which we expect the query function to be relatively smooth. chdd, cleveland heart disease database. majority of americans over the age of 60, the earliest clinical signs of rpe dysfunction are observed in color fundus photographs as drusen-bright highly reflective extracellular deposits between the rpe and its basement membrane. macular drusen increase in number and size with advancing age in epidemiological studies and larger, irregular-shaped, perifoveal drusen (''soft'') are considered to confer the greatest risk for progression to advanced amd. through many years of large-scale studies of the natural history of amd and controlled prevention trials, clinical observations of fundus photographs suggest that people with soft (larger than 150 microns and irregularly shaped) drusen are at high risk for progressing to advanced amd. currently, pathologists in reading centers classify drusen based on size and shape [cit] in reflection color fundus images. there is demand for automated analysis tools that allow for quantitative prediction of disease progression."
"the technique has been here applied to polynomial texture maps, but can be applied as well on different types of rti [cit] which provide a better approximation of the per-pixel reflectance function. we expect to obtain, in this case, improved results. we plan also to extend the idea of automatic processing of rti data, testing other types of computer vision/pattern recognition methods. for example unsupervised and supervised classification could be applied using the local reflectance coefficients as feature vectors in order to detect regions corresponding to different materials. pattern recognition tools could be applied as well to recognize characters from lines obtained with the proposed edge detector."
"where b d (y 0 ) & xdenotes a ball of radius d around y 0 . thus, when f is locally smooth in a neighborhood around y 0, then we can locally reconstruct f from the training data. the analogous result for functions that are globally smooth is contained in filbir and mhaskar (2010, 2011 ), maggioni and mhaskar (2008, [cit], which contains local estimates, but the approximand requires global knowledge of f and is not purely defined through the training data only."
"after the learning step, we mark a region of interest (roi) in six patients (including the three patients used for learning). to classify pixels from the roi into the four partitions, we merge the roi pixels of the patient under consideration with the roi pixels of the three learned patients to form the data on the manifold. the labeled regions are the training data, and since the pixel vectors lie on the sphere, we can follow the approach in section 5 to derive r n (f, x) (fig. 2) . the majority of the pixels within the roi would define the drusen class of the patient. it should be mentioned that the size of the roi influences the classification scheme, and we obtained good results with the center in the fovea and extending to 5 degrees, consistent with common image analysis in ophthalmology. sensitivity analysis for the data set wbcd in section 6.1.2, averaged over clump thickness and 50 instances for each method. our local kernel method yields high sensitivity compared to other methods when there are only few training data. wbcd, wisconsin breast cancer database."
the maximum/sum of two secrets to be locally determined as each evaluator e l can calculate its share of sum a + b / product a · b of two polynomials a and b by calculating the sum a(
"equation (3) measures the best error achievable if one wishes to use p n as the model for f, and wishes the error to be small at each point of x. it turns out that the rate at which the quantity e n (f) decreases to 0 as n / n is closely related to the smoothness of f. thus, if f is smooth enough so that d r f 2 c(x) for some integer r ‡ 1, then"
a possible improvement of the method could consist in storing local information about the normals and estimating the position of lights enhancing the lines of interest that could be combined creating an adaptively multiple source relighted image.
"looking at small details and changing the light direction something interesting can be, however found. fig. 5 a shows a central detail on the image of fig.4, where it is possible to find a written text. to find it automatically, without the necessity of searching manually the optimal light direction, it is possible to try an enhancement filter like those implemented in rti viewer. extracting normal discontinuities, however, the fact that an inscription is present in that region becomes clearer. fig. 6 a shows the normal discontinuity map compute at the finest scale, and fig. 6 b the extracted edges. it is possible to see lines belonging to a text. fig. 7 a a frontal relighting shows the lid of a tomb, where it is not easy to see where human artifacts are present. fig. 7 b shows the normal discontinuity map extracted using derivatives at two scales. fig. 7 c the edge extracted where it is possible to see squares and lines traced in the stone. fig. 8 show another part of the archaeological site where the normal discontinuity map and the edges reveals artifacts and text fragments."
"the rest of the paper is organized as follows. section 2 introduces related work. section 3 presents preliminaries. in section 4, we describe the main idea of the proposed scheme. in section 5, we analyze the security and performance of the scheme, followed by a conclusion in section 6."
"such local smoothness ideas are particularly useful in biomedicine, as disease progression underlies natural variations, medication leads to abrupt changes in disease progression, and environmental factors vary quickly, so that the query function might not be globally smooth. while late disease stages underlie large variations, the transition from healthy to early pathology can be smooth, leading to query functions that are locally smooth within such early disease transitions."
"simulations were carried out in order to assess whether we complied with the reference levels for general public exposure to time-varying electric and magnetic fields [cit] . an h-field strength lower than 5 a/m (reference level for frequencies lower than 150 khz) is achieved at distances higher than 0.5 mm from the coils, which is safe enough for the intended application. we used a suitable current density in accordance with the experimental results presented later in section v. fig. 6 shows the circuit schematic of the primary network. we used a class d power amplifier based on a low-cost commercial self-oscillating half-bridge driver (ir2153) and two external n-channel mosfets (bs108), m1 and m2. the driver, powered (v cc ) at 12 v dc (battery voltage in vehicles), alternatively activates the two mosfets, thus injecting a square wave signal into the series-resonant network. the oscillation frequency is selectable via an rc network (r b, c b ) up to 1 mhz. a potentiometer was used to fine tune the desired resonant frequency. following the manufacturer guidelines, a bootstrap capacitor (c c ) was used to properly activate m1. for moderate to high quality factors, only the first harmonic will generate a current through the network, being its amplitude of 7.64 v (2v cc /π) and its rms value of 5.4 v."
"in order to calculate a precise illumination function, a critical factor is that the digital camera must not move from one photo to the other. even a misalignment of a few pixel can produce a bad result, with visible aliasing."
"for the intended application, small-size coils are sought. at the same time, in order to comply with the reference levels for general public exposure to time-varying electric and magnetic fields [cit], and to reduce the power losses of the power amplifier, frequency resonance was limited to less than 150 khz. so, in order to increase the quality factor of the coils with these constraints, the use of magnetic-core material was considered as an appropriate solution."
"a class d power amplifier has been used for the primary network. experimental results have shown that a power of tens of milliwatts can be transferred to a load of 100 ω placed at the secondary network up to a distance of 2 cm, near seven times the radius of the coils (3 mm). the addition of a rectifier and a voltage regulator in order to properly power an autonomous sensor (3 v @ 30 ma) limits the powering range to 1 cm. overall efficiencies around 45 % and 20 % have been achieved at distances of 5 mm and 1 cm, respectively."
". if x 0 2 x, we will define the local smoothness of f at x 0 by the natural windowing construction. thus, we say that"
". this representation requires knowledge of f on the entire manifold x. to reconstruct f from the data only, we must replace the integral with a finite sum over data points and localize the kernel f so that f (x) is determined by its values in a neighborhood around x. in this section, we shall make these ideas mathematically precise and construct a linear operator based on the data to derive p 2 p n that essentially minimizes equation (3)."
"from the normal vector n we can estimate the normal discontinuity along u and v directions (at different scales). if we consider directional derivatives of normal vectors ∂n /∂u, ∂n /∂v we can obtain the intensity of the discontinuity as"
"to carry out a combinatorial auction, the winner determination problem has to be solved first. the problem can be cooperatively solved by multi-auction servers, which can calculate the maximum sum of combinations of bidding prices. it is a challenge problem to protect bidding prices. if the auctioneer is trust, it can solve the winner determination problem. however, it is not practical as the auctioneer may collude with a participant to reveal the bids' information during the auction. if a strategy-proof mechanism is utilized to resist collusion attacks. however, the auctioneer can create a fake bid to increase revenue."
"to facilitate the analysis of complex biomedical data, we developed the mathematical foundations for a numerical algorithm that enables global and local data analysis integratively. after we validated our the y-axis counts the correct classifications. note that we simply removed random pixels from the entire set of 3 · 2000 training pixels, so that each smaller training set is contained in the larger one. we repeated this process over 20 runs to avoid random anomalies, which led to the black curve. the most typical curve in a single run is depicted in red. for few training data, we only classify one single patient correctly. increasing the size of the training pixels enables consistently correct classifications of all but one patient (consistently the same one). it seems that the critical number of training pixels is between 600 and 800 pixels. approach on two standard datasets, we aimed to classify and predict disease progression in amd patients. drusen were classified in multispectral retinal image sets that enabled quantitative measurements of advanced pathological changes. clearly, our new mathematical approach to identifying new components controlling amd progression is preliminary and needs further validation to claim its usefulness in general terms. we anticipate improvements through synergies of multiple analysis schemes with multi modal data so that our proposed analysis could be part of an iterative process."
"in our implementation we computed normal derivatives using 5-pixel masks, and supported a multiscale estimation of the derivatives/discontinuity as follows: derivatives at different levels of detail are obtained using iterative smoothing and taking derivative masks with increased sampling steps. the final multiscale i and are obtained selecting for each pixel position the value corresponding to the scale maximizing i. results, as in the usual edge detectors, clearly depend on the parameters chosen (thresholds) and on a possible initial normal smoothing (not necessary however, being the normal maps derived by the ptm coefficients already smoothed). however, for our test a higher threshold of 0.2-0.3 and a smaller one of about 0.05 were able to provide reasonable results on the tested images."
"polynomial texture maps [cit] are an extremely useful tool for the documentation and the visual analysis of ancient coins, bas-reliefs, paintings and many other cultural heritage objects. they are relightable image, i.e. image where the user can modify interactively the lighting conditions. each pixel contains a bi-quadratic polynomial that encodes an approximate reflectance function of the scene allowing the possibility modify the image given the illumination direction. ptms of an object can be created from multiple images acquired under different incident light directions, without specific hardware (low-end digital camera provides enough resolution to produce good ptms, and almost any type of light source can be used). several applications of this technique have been proposed, mainly in the field of cultural heritage [cit], where this type of image and other similar ones [cit] have demonstrated to be very useful for analysis purposes."
"to tackle the above challenges, two problems have to be solved. first, multi-auction servers compute the maximum sum of combinations of bidding prices, while the information of bids and the part of the optimal solution should be kept secret. second, the collusion activity of multi-auction servers must be resisted. we employ verifiable secret sharing [cit] to protect privacy and data security in combinatorial auctions. the scheme allows multi-servers to randomly choose secret shares and verify the legitimacy of them to each other."
"1. evaluators (servers) select their secret keys by themselves, and the weight publishers (wp) (buyers and sellers) calculate and publish the weights for each share. 2. the legitimacy of evaluators is verified to each other, and then the evaluators cooperatively implement dynamic programming protocol to find the optimal solution, while each weight is kept secret."
". this leads to a linear system of equations whose solution is reasonably close to exact weights in practice, but only for small parameters d, n, and r. if any of these parameters is not small, then the problem becomes numerically unstable, and we need to follow the approach presented in appendix e."
"next, we explore the influence of the size of the training data. we only use a fraction of the pixels that were originally labeled by our grader. the individual pixels are selected by a random sampling. figure 3 shows that we require a critical number of training pixels and from there on, we obtain stable classification results."
"-a low-end digital camera (fuji finepix hs 20 exr) -two tripods, one for the camera and one for the black reflective ball -one lamp able to generate approximately uniform light on the size of the objects of interest."
we introduce the proposed privacy-preserving combinatorial auction without an auctioneer based on the longest path of a one-dimensional directed graph. an example is introduced in section 4.6.
"we noted some comparisons between the two approaches. first, we the optimization approach does not necessarily imply any performance guarantees of equation (2). moreover, the value of the regularization functional depends upon the data. there are no bounds to how large this value might get as more and more data are introduced. finally, there are common computational issues such as local minima, convergence of the algorithms, and convergence of the minimizers f c as the data becomes dense on the manifold. all of these issues are completely avoided in the approximation approach."
"we first compare our proposed approach to widely used classification methods on two standard biomedical datasets. after this verification, we use our scheme to analyze multispectral retinal images of agerelated macular degeneration (amd) patients. all eye-related data were collected by our collaborators at the national eye clinic at the national institutes of health (bethesda, nih; maryland)."
"at the terminal time t f, if euclidean norm of error vector e(t f ) can be reduced to zero, the system state x(t f ) is close to x dap . considering the following linear quadratic optimal control problem, the quadratic cost function is chosen to be"
"cross validation (cv) is used to determine how deep the decision tree will grow. this is known as the post-pruning rule. being used by the c4.5 algorithm [cit], it involves the following steps:"
"the error vector e(t) represents the difference between the system state and the expected output. in the scenario of time-coordinated attacking of uav swarm, the desired relative position and velocity between uav and target are only reached at the terminal time t f, not in the whole process. hence, the value of y d (t) at the terminal time is set to x dap :"
"ubicomp systems are often embedded in people's everyday environments, surrounded by other physical objects and social meanings that comprise the ecology of that place. inspired by research in context2 and location2awareness [cit], our next concept emphasizes the importance of interpreting the physical setting where an interaction takes place [cit] . in particular, people's relationships to fixed and semi fixed features (as defined by hall [cit] ) can be indicators for directing actions to a particular ubicomp system. in our media player [cit], the ubicomp system not only monitors a person's proxemic relationship towards a device, but also to that person's distance to other fixed and semi2fixed features in the ecology. if a person selects a video and then sits on the couch, that is interpreted as an indicator that she is ready to watch the currently selected video and thus video playback begin. however, if the person instead moves to the doorway, that is interpreted as an indicator that she is no longer interested, and the system shuts down. in both cases, the distance from the person to the screen is the same, but her location in the room's ecology is different."
"where f t (·) is the transformation function of target state. as discussed, the target trajectory is composed of a set of modes. in order to simulate the process of the target motion, cv, ca and ct models [cit] are adopted here, and the process noise is negligible. the three models above are convenient to describe the movement process of the target and they are consistent with the movement of vehicles. constant velocity (cv) mode:"
"after arriving at the daps, the attack modes are various from different types of uavs. for a bomb-dropping uav, the attack mode is to launch missiles or drop bombs. for a suicide uav, the attack mode is to guide itself to the target by the guidance of the seeker and to explode once hitting the target. this paper focuses on how to control uav swarm to reach their daps with the respective expected arrival time. attack guidance (green curves in fig. 2 ) intervals of each uav can be negligible. therefore, the interval time of attacking target can be approximately equal to the interval time of reaching dap."
decision system (faults diagnosis) fault tolerance system pilot &acc evaluations studying the cases presented before; and the last section concludes the paper and presents a preview of future works.
"a) the faults detection and discrimination remains easy for the system, even with a change of input conditions and tree structure. b) pca finds the principal components and the system can get well adapted to them and classify the aircraft's situations."
"through efficient coordination, uav swarm which is composed of many uavs connected by communication network, can emerge much better performance than several independent individuals [cit] . cooperative executing missions of uav swarm without constant supervision of human operators has attracted increasingly attention in both civil and military applications. for the case of attacking a ground target, uav swarm must execute a coordinated maneuver to arrive at the predefined positions over the target from multiple angles simultaneously [cit] . time-coordinated strategies in these missions are effective to achieve the maximum reward by saturation attack and improve the overall abilities of uav swarm. in general, time-coordinated control problems have been investigated in various applications, including"
"anthropologist edward hall introduced proxemics as a theory for studying the interpersonal spatial relationships between individuals [cit] . his theory -while emphasising social and cultural differences -generally describes how people perceive, interpret, and (often unconsciously) use the micro2space around them, and how this affects their interaction and communication with other nearby people."
"challenge 4. providing feedback, [cit] . appro2 priate feedback is a mainstay of traditional gui interaction design. yet as ubicomp interfaces move away from the traditional desktop computer setting, it becomes even more important to provide feedback about the current status of the application, its interpretation of user input, or the occurrence of errors. to complicate matters, ubicomp systems have to consider that people's attention in regards to the ubicomp technology might switch between foreground and background."
"explicit action to undo. ju [cit] presents an opposing explicit strategy to undo actions. her application runs on the interactive whiteboard, where it implicitly responds to people's actions. this can easily result in an unwanted action (for example: automatically moving a cluster of ink strokes to the side of the display to free up space). to correct this, the person moves closer to the screen (instead of stepping back, like in vogel's system) and grabs the cluster of ink strokes to keep it from moving."
"we will shortly explain techniques based on proxemic interaction that can mitigate problems inherent in these challenges. however, we will first set the scene by summarizing aspects of proxemic theories and use of personal space that we believe are relevant to ubicomp design."
"considering attention and orientation. instead of relying on only distance, the system can use other measures to infer a person's attention to it. this is the premise of attentive user interfaces (auis) that are designed to \"support users' attentional capacities\" [cit] . in one class of auis, the system reaction depends on whether a person is directing his or her attention to the device as detecting eye gaze [cit], which in turn can be considered a very fine2grained measure of orientation. our media player also exploits orientation as a measure of attention [cit] . when a person turns away from the video screen (to, say, read a magazine or talk to another person), the system pauses video playback, and resumes when they turn back towards it. wang's proxemic2aware presenter [cit] also uses orientation as an indication of attention. if the presenter is facing towards the audience and away from the large display, a standard slide deck is shown. however, when the presenter turns towards the display, small navigation controls and speaking notes become visible at the side of the screen closest to the presenter."
"in the scenario, n uavs are assigned to attack the ground-moving target cooperatively. time-coordinated strategies such as simultaneous arrival or interval arrival are effective for improving suddenness or persistence of the attack. simultaneous arrival strategy requires n uavs to reach their predetermined desired attack positions (dap) at their terminal arrival times respectively, as shown in fig. 2 . not only is there a certain requirement on uav's relative position to target in the terminal time, but the relative velocity and entry angle should also be a specific range when uav reaches the dap. denote the terminal relative position and velocity as"
"people constantly adjust their use of space to fit the presence of, and interactions with, others. this includes how people react to and try to overcome 'invasions' or 'violations' of their personal space. some theories describe people's adaption to given spatial circumstances, and how they try to maintain a certain comfort level or equilibrium in these situations [cit] . for example, the intimacy equilibrium model [cit] assumes that when people interact they always strive to maintain an overall balance towards a desired optimal proxemic distance. to achieve this balance, people might try to adapt proxemic variables such as distance, orientation, or eye contact, which the model describes as \"inverse relationship between mutual gaze, a nonverbal cue signaling intimacy, and interpersonal distance\" [cit] . for example, when a person stands too close to us, we might step back to maintain the equilibrium. if any of the variables cannot be changed in this particular situation (such as standing very close to others in an elevator), the change of another variable can be used to compensate (in the elevator example: changing orientation to face away while avoiding eye contact). another predictive model formalizes equilibrium as an optimal proxemic distance, where it adds proxemics variables including identity and familiarity of the other person, and the type of interaction [cit] . people also use personal space as a method to protect a certain level of privacy. altman [cit] reframes this use as a dynamic boundary regulation process that controls privacy."
"where (·) is time-coordinated function, t f,i is the arrival time of ith uav. when adopting the strategy of interval arrival, (·) can be expressed as"
"we now revisit each design challenge, where we speculate -with examples drawn from the literature -how knowledge of proxemics as gathered by the 5 dimensions can mitigate problems inherent in each challenge. our examples are merely a starting point, where their contributions are re2 framed within each challenge, and where they hint at the potential of future proxemic interaction designs."
"proxemic safeguards. as a safeguard mechanism, actions with a high impact (e.g., deleting information, or resetting the system) could be restricted to occur only when a person is in very close proximity to a device. for example, while a person can manipulate information on an interactive whiteboard from a large distance by using remote gestures, she would have to move directly in front of the screen to delete data by (say) direct touch. alternatively, such actions with high impact could even require a certain proxemic relationship in multiple dimensions. for example, the delete action could require a person to stand in close proximity to the screen and being oriented towards it and look at the screen simultaneously. the action could also be tentative and undoable as the person remains close by, but committed as they move away."
"in this paper, a faults diagnosis system of aircrafts using decision trees is presented. the learning speed of this intelligent system is much higher than the speed of other diagnosis systems, like neural networks or svms, which allows it to perform a quick relearning in the cases of data loss. pca is used to accurately reduce the system's input dimension. tests results of normal and abnormal flight conditions demonstrate the high abilities of the system. for the future work, we will focus deeply on how to prognosis the situations (failure prediction) of an aircraft with a decision system."
"in this system, all the input variables are not boolean but can take multiple values. therefore, the values of each attribute are sequently arrayed firstly, and then all the expected pair values which are neighboring but sorted in different classes are picked out. after that, all possible threshold values are calculated, as the halves of the sums of the pairs mentioned before. finally, the threshold value with maximum information gain is taken."
"to partially realize seamlessness, weiser and brown also proposed technology that \"engages both the center and periphery of our attention\" [cit] . it is these parts of weiser's vision -the seamless interaction, the disappearing technology, the seamless transitions between foreground engaging activity and background peripheral perceptionthat is still missing from people's everyday experience with ubicomp technology."
"the proxemic theories above describe many different factors and variables that people use to perceive and adjust their spatial relationships with others. we recognize that these theories describe people's relations to people, and not to devices. even so, our belief is that we can use these theories as a first2order approximation to apply proxemics to ubicomp design. as part of this approximation, we offer five device2oriented proxemic dimensions -inputs and states that devices can hold about proxemics relationshipsthat we believe are most relevant to operationalizing proxemics in ubicomp interaction [cit] . that is, they describe not only relationships between person2to2person, but with all entities in ubicomp ecologies: people, digital devices, non2digital objects, and the features of the surrounding environment. the list below describes each dimension, while figure 2 illustrates each as a graphic."
"faults diagnosis is of a high importance for modern aircrafts, with structures and systems becoming more and more complex. hundreds of sensors are being used to supervise an aircraft, but abnormal information and faults are still difficult to find. in order to deal with this problem, many faults diagnosis systems have been invented, with intensive studies of data mining. one can cite: the expert system [cit] that needs to establish the knowledge base and the rules by experts and has to face an intractable problem -the conflict of rules [cit] . also, neural networks [cit] and support vector machines (svms) [cit] which require much more time for the learning process."
"time-coordinated control problems for uav swarm cooperative attack are investigated in this paper, where the target is moving on the ground. relative motion between uav and ground-moving target are considered as a finite-time time-varying tracking system problem. proposed control law for a single uav is obtained by linear quadratic optimal control theory, which can guide the uav to the desired attack positions with the specified terminal time and relative velocity. time-coordinated function is proposed to model time-coordinated strategies, and different functions are constructed to implement various strategies. on the basis of proposed control law for a single uav, the control law for uav swarm is obtained by substituting the parameters of each uav. numerical simulations show that the proposed control law can steer the uav to arrive at the desired attack positions and effectively realize the time-coordinated strategies. but with the acceleration or turning of the target, the actual controls extreme fluctuate. there are still a number of issues need to be further investigated and coordinated control for uav swarm in various missions are currently under investigation. another thing needs to be addressed in the future is that other constraints such as control saturation, target's moving speed, measurement error, time delay and collision avoidance should be taken into consideration."
"where x t (k), y t (k) and v tx (k), v ty (k) are the cartesian coordinates of position and velocity of the target in the inertial reference frame, respectively. the equation of target's motion can be expressed as"
"in this simulation, the movement of target is based on the cv, ca and ct models given in (9) (10) (11) . assume that the target moves as: constant linear motion in 0 − 24s, uniformly decelerated motion with deceleration of (−2, −0.25) m s 2 in 24 − 26s, coordinate right turn motion within 26 − 35s, uniformly accelerated motion with acceleration of (0.25, −2) m s 2 in 35 − 37s, constant linear motion in 37 − 60s. the trajectory and velocity are shown in fig. 5(a) and fig. 5(b) ."
"adjusting feedback output. due to the embedded nature of many ubicomp systems, there is often no graphical display for showing feedback to the user. instead, output can be via visual lights, audible sounds, speech, or physically moving objects (like in many tangible user interfaces). assuming a system knows the physical orientation and distance of a person, it can adjust the provided output to the person that it is addressing. the listen reader [cit], for example, adjusts the volume of the audio output depending on a person's proximity to a digitally augmented book. similarly, in our media player [cit] a person sees large preview thumbnails of available videos when at a distance. the screen continuously shows more content as the person moves closer (and thus, can read more information)."
"in this section, we introduce six core challenges in the design of embodied and seamless interaction within ubicomp, with an emphasis on their relevance to proxemic interactions. our challenges are inspired by bellotti's [cit] important design considerations for sensing systems, augmented by issues raised in other analytical and reflective ubicomp discussions [cit] : norman [cit] appropriated gibson's [cit] notion of affordances to describe how an object's visuals can 'suggest' how it might be used. traditional guis exploited affordance to design interface elements that suggested their use and possible actions; they worked, because they could assume that they were in the foreground of a user's attention, i.e., the person was watching the screen. yet this cannot be directly applied to ubicomp, as ubicomp assumes that technology can be integrated into the everyday environment in a way that it 'disappears', or is present in the just2perceptible periphery of our attention, and that it is able to fluently move into the center of our attention as needed [cit] . this introduces the challenge: how can technology be designed to reveal the interaction possibilities appropriate when it is not only in the background of a person's attention, but during the transition of it moving into the foreground?"
"from awareness to interaction. in real life, people exploit proxemics cues as they greet and engage in social interaction. one may have peripheral awareness of the other while at a distance, become increasingly aware and engaged as the other turns towards and approaches them, and then begin to interact when within an appropriate proxemic region. some public ambient displays apply a similar mechanism to engage people, where they trigger actions to attract a passer2by's attention, and progressively show more information and interaction possibilities as the person approaches and attends the display, ideally leading to foreground interaction by direct touch [cit] . the idea is that the passer2by notices the public display as it implicitly reacts to their presence, where it captures their attention and interest (as discussed in challenge 1). their attention is realized by moving closer and facing the display; the system also detects and react to that interest [cit] . in the media player [cit], for example, the number of videos, their size and associated text is adjusted as the person approaches the display, where it reveals more video selections and more information about those videos. a system such as this exploits distance, orientation and movement to infer a person passing by at a larger distance, then turning towards the display, then approaching, and finally standing directly in front of it."
"our next design challenge addresses the question of how a person can correct errors, such as those that result from the system misinterpretation a person's action, or by the person performing an unintended action."
"challenge 6. managing privacy and security [cit] : a large issue in ubicomp is that as the number of potential interactions with technology increase, so too do the risks to privacy and the need for greater security. the question is how can the system protect privacy sensitive information and handle the access to information, while at the same time not get in the way of all the positive offerings of ubicomp mentioned in challenges 125?"
"altman's [cit] theory considers personal space as a protection mechanism for maintaining a certain level of privacy. this could be leveraged to design systems that respect people's expectations of personal space. that is, the ubicomp system can influence the simultaneous interaction of multiple people in a way that maintains such levels of privacy for everyone involved. to illustrate, let us revisit vogel's public ambient display [cit] . when people move closer to the display, they get more details about their own personal calendar visible on the screen. thus, people stand next to each other viewing their personal calendars. when considering altman's theory of balancing privacy through proxemics, the system could be designed to separate the large screen interaction areas of the two people. for instance, the areas for viewing personal calendars could be displayed where it depends upon a minimum distance between those people."
"decision tree learning is a method commonly used in data mining. its goal is to create a model that predicts a target value (that can be the class of an input example), based on several input values that describe the example. this method uses a hierarchical tree structure that progressively checks input variables and makes corresponding decisions like classification. like the system of expert, learned trees can also be re-represented as sets of if-then rules to improve human readability, but they don't need human experts to establish a knowledge and rules database, they build rules by themselves. an example is shown in figure 2 . data represented by three input variables can be classified into two classes. each node corresponds to one of the input variables -a or b or c, and inside each node, the variable is compared to a corresponding edge. the result of this comparison is translated by a leaf that leads to another node, or to the final classification decision. but in the presented example, one can ask several questions, like: why is it that a is in the root, not b or c? how was the threshold x found for a? so, in a decision tree algorithm, and in a more general manner, two questions are to be answered, in order to find the best possible classifier: 1) how to decide the sequencing of variables and corresponding nodes in the tree? 2) how to determine the used thresholds values?"
"to address this challenge, a system must offer possible actions [cit] that afford seamless transitions from background to foreground interaction [cit] . this concept is somewhat similar to how people approaching each other exchange greet and begin communicating through various signals (eye gaze, body language and talk), where signals and possible actions vary appropriately across this greeting phase. similarly, ubicomp should 'greet' other entities by revealing interaction possibilities that match what is possible at the moment. several strategies to accomplish this are described below."
"where a uxi, a uyi and a uzi are the certain control variables in the double-integrator model, which have the relationships with the actual control variables as follows"
"the test results will be presented in two forms corresponding to two different conditions: the condition of maintenance and the condition of mission on-line. 1) in the condition of maintenance, the system must distinguish clearly each fault and show the diagnosis results to an engineer. with the confusion matrices which are shown in the tables i -iv, three observations can be reached: a) performance of system with pca: although pca reduces the dimension of the data, the performance of the decision system is not reduced. the score of the system based on pca is nearly the same as that of the system without pca, especially for the systems with a pruned tree. for example, the score of table ii is 95.16% and the score of table iv table i and table ii (or table iii and table iv), as mentioned in section ii, the results confirm that a system with a pruned tree works better than a system with a tree without pruning. the pruned trees add about two to four percentage points to the scores of diagnosis. 2) in the condition of mission on-line, the pilot doesn't need to know exactly which type of fault of anti-icing system appeared, he only needs to know that the anti-icing system is broken or not. if it doesn't work, he must go back to acc or make a emergency landing. therefore, all the types of faults are grouped as one fault in this condition, the correct detection ratio, missed detection ratio and false alarm ratio will be studied in this part. as can be seen in table v and table vi, the system offers satisfying performances with high correct detection rates, and low missed detection and false alarm rates. the performances slightly decrease with decreasing input data dimensions, but the results remain highly accurate. this may be due to one of two hypotheses:"
"hall noticed that the layout of the fixed features as well as the arrangement of elements in the semi fixed feature space influence our use and perception of personal space, where particular layouts can be sociofugal (separating people) and sociopetal (bringing people together) [cit] . a simple example is how chairs in a living room can be brought together into a sociopetal small circle to encourage intimate chat."
"this paper discussed the application of proxemics in ubicomp interaction design. the intention was to inform ubicomp designers implementing proxemic2aware devices about important proxemic dimensions to consider for the design and review methods of how those can be applied to challenges in ubicomp interaction. by focusing in particular on how the knowledge in the five proxemic dimensions (distance, orientation, movement, identity, and location) can be applied to ubicomp interaction, we hope to open up a new perspective onto how proxemics can be considered when designing new ubicomp systems that react seamlessly and appropriately to people's expectations."
"in the physical world, we often know what is available simply by looking around us. ubicomp, however, participates in both the physical and virtual, so not all offerings are readily visible. one way around this is to spatially visualize otherwise hidden offerings on a device's screen(s). consider the problem of several people in a room, each with a mobile device, and how one knows when one device is within range of another, and what can subsequently be done with them. to solve this, gellersen's relate gateways [cit] provides a graphical map on each mobile device's screen showing the nearby devices that it senses. icons on the map indicate the position of devices that can be accessed. in an alternate visualization, icons at the border of the screen represent the type and location of surrounding devices relative to that device's position. in our media player example [cit], the display visualizes the spatial relationships to nearby personal devices by using a method akin to ray2casting: if a person points their device towards the large screen, a graphic appears as a ray2cast 'projection' on that screen indicating its position and orientation. as the mobile device approaches and is oriented towards the large display, increasing detail about that device, its contents and its interaction possibilities are revealed."
"motivated by above discussions, this paper focuses on how to control each uav in the uav swarm to reach the desired attack position at a specified terminal time when the target is moving. compared with reference [cit], the target in this paper is a moving in real time, and the motion of the target is uncertain. consequently, path pre-planning or coordinating the speed of each uav are not suitable for this problem. in order to take target's motion into consideration, relative motion between each uav and ground-moving target are modeled as a finite-time time-varying tracking system problem. its core idea is that tracking error converges to zero at the terminal time. compared with the existing time-coordinated control methods based on consistency theory, the proposed method can assign a specific arrival time for each uav rather than all uavs. significantly, by this newly proposed approach, the terminal time and arrival position of each uav are decoupled and various, which brings more possibilities for a variety of missions. the main contributions of this paper are summarized as follows: (i) considering relative motion between each uav and ground-moving target as a finite-time time-varying tracking system problem, the control law is obtained by optimal control theory. (ii) the desired attack position (dap) and time-coordinated function are defined to model the coordination of time and space. (iii) the time-coordinated strategies of simultaneous arrival within one group and interval arrival between groups are adopted to verify the validity and accuracy of the proposed control law."
"while challenge 1 concerns how a ubicomp system can reveal interaction possibilities to a person, challenge 2 addresses how a person can in fact direct their input actions to a particular device."
"remark 2: there is no formal difference of control law between a single uav and uav swarm, but the different parameters are adopted for each uav. the position r ui and velocity v ui of each uav are different in real time and the terminal arrival time t f,i may be various. the different terminal arrival time t f,i and desired arrival position y d (t f,i ) cause that the parameters p 2,i, p 3,i and g i are distinct for each uav."
"going straight towards another person -or instead quickly passing by -are also proxemic cues that we implicitly interpret in everyday interactions with others. similarly, ubicomp systems can interpret people's and device's motions for directing actions. for example, vogel's [cit] ambient display ignores people quickly passing by, but reacts to (and gathers input) from people walking straight towards it. motion cues can be quite fine2grained, where it can exploit distance, orientation and velocity as well as how each changes over time."
"the associate editor coordinating the review of this manuscript and approving it for publication was juan liu. multi-agents [cit], multiple underwater vehicles [cit], multiuavs [cit] and multi-missles [cit] . compared with robots, uavs move in three-dimensional space with positive speed restrictions and can not stop or back off. compared with missiles, uavs can change the velocity within its allowable range, or increase the flight time by hovering."
"selecting appropriate feedback modality. furthermore a system can select the most appropriate output modality to a person (e.g., visual vs. audible) based on their proxemic relationship. for example, when the person is facing away from a large screen, the system might use an audible signal as a notification. when the person is standing closer to the system facing the screen, visual output may be used instead."
"in this paper, a novel intelligent decision system for faults diagnosis of aircrafts is proposed. we decided to use a decision tree learning algorithm since it provides much easier data to interpret than other algorithms, such as neural networks and svms [cit] . decision tree learning is a method for approximating discrete-valued target functions [cit] . a decision tree algorithm automatically \"learns\" a decision tree by performing a search through the space of possible trees to find the one that best fits the training data. the particular algorithm used in this paper is known as c4.5 (see section ii). as figure 1 shows, the decision system uses the output information of sensors, and its diagnosis results are delivered to the fault tolerance system and the decision system which will demonstrate the faults and suggest decisions to the pilot and the acc (airport command center) (emergency landing, return to base or mission continuation if the fault tolerance system can deal with the fault, etc.). in accordance with the results of the decision system, the pilot or acc can decide their next action. aside the normal working conditions of the aircraft and thus of the fault diagnosis system, the real risk events in the system should be taken into consideration during a flight. such cases can be seen for example in the loss of some sensors' information in special situations, especially for the fighting aircrafts. and in these cases, the diagnosis system should be ensured of working continuously. the presented system is able to deal with these cases. this paper studies the case where 3 faults of the anti-icing system of the aircraft x (dassault aviation) need to be diagnosed. usually the hot air exhausted by the aircraft's engine is used in the anti-icing system to deice the aircraft wings. 33 sensors supervise the anti-icing system. they mainly provide values of the engine and air wings' temperatures, pressures and so on. so our diagnosis system has input vectors of a dimension 33 used to analyze and build the decision tree. note that the natures of the faults cannot be revealed in this paper because of confidentiality issues."
"but, there still exists a big problem: all the data is recorded per second during the mission of the aircraft, if there is 5% missed detection ratio or false alarm ratio, the pilot will receive the missed detection or false alarm 3 times per minute, which is unacceptable in the real applications. to solve this problem, statistics of the results of the diagnosis system during a period of time, as 15 or 30 seconds, are studied. that is to say, the system records the diagnosis results from a time; then compares the fault's ratio diagnosed in the results with pre-set threshold; at last the system will judge it as fault occurred if its value is above the threshold. for example, the pre-set threshold is 15%, and only two results are diagnosed as fault by the system during 15 seconds. since the frequency is below the threshold, the diagnosis of these 15 seconds is considered as \"no-fault\". this criterion is used to test the continuous data of 18000 seconds: the correct detection ratio equals 100%, the missed detection ratio equals 0, and the false alarm ratio equals 0.0013% (using 6 vectors of input data whose pca is about 95%)."
"note that each uav can only obtain the state of target in real time, and the motion in the whole process is unknown. a time-coordinated attack strategy, simultaneous arrival within a group and interval arrival between groups, is adopted here to verify the effectiveness of the proposed control law. in this scenario, the uav swarm including 6 uavs, which is divided into two subgroups, is assigned to attack a ground-moving target. the first subgroup is composed of uav1, uav2 and uav3, and the second is uav4, uav5 and uav6. a circle with a radius of 200 meters is set as an attack circle at a height of 300 meters above the target, and three points on the attack circle from different directions are chosen as three daps. the initial position, initial velocity, dap and arrival time of all uavs are given in table 1 ."
"coordinated turn (ct) mode: where ω is the turn rate of the target of which sign determines the turning direction. if the target turns clockwise, ω is negative. based on the above three models, the process of the ground target's motion can be completely described. note that the control law proposed in following sections is not only applicable to targets modeled by cv, ca and ct, but also to targets with any other motion models."
"challenge 2. directing actions [cit] : input to a single traditional device is straightforward, as it usually comes through a dedicated input device (e.g., a mouse, keyboard, touch surface). yet ubicomp can be different. input may be detached from a particular device. possible actions can be performed through speech, gestures, eye gaze and other alternative options. one problem is that the device has to somehow discern whether that action is actually a directive to the system, or whether it should be ignored [cit] because it is just part of a person's everyday actions (e.g., a voice command vs. social talk; a command gesture vs. a gesture or movement made in the course of doing other things). the problem of directing the actions to a particular device is even more problematic when there are large quantities of devices present in the local ecology, for the system has to discern which device (or set of devices) should respond to a person's directed action."
"uav's control input is calculated by substituting target's real-time position r t, velocity v t and acceleration a t into (37). in other words, uav's control input is only relative to target's current state, rather than past or future motion. therefore, the state of the target is changing and known in real time, but the future state of the target is uncertain."
"the rest of this paper is organized as follows. the problem formulation is illustrated in section ii. in section iii, the cooperative attack control laws for a single uav and uav swarm are designed respectively. simulation studies are given in section iv. finally, conclusions and future researches are presented in section v."
"as suggested by our last example, people need to somehow control how one device connects to another device within a potentially large ecology of devices in a way that seamlessly supports their interaction needs while still safeguarding privacy and maintaining security. we do this naturally -the way we greet and move closer to one another via proxemics is essentially a negotiation to establish connections for communication."
"for the sake of safety, plenty of the sensors installed in the aircraft are redundant. multiple sensors can do the same role, in order not to lose the information when one of them malfunctions. indeed, the sensors' redundancy is indispensable for the aircraft, especially with sensitive and important parts. but for the diagnosis system used here, using all the sensors' data directly in the input space at once is not very necessary and it affects the arithmetic speed of the diagnosis system. principal component analysis (pca) is employed, as being a tool that effectively reduces the input data dimension and improves the computational loads of the diagnosis process, and thus of the overall system. pca is mathematically defined [cit] as an orthogonal linear transformation that converts a set of observations of possibly correlated variables into a set of values of uncorrelated variables called principal components. the data is projected to a new coordinate system where the greatest variance of the data obtained by any projection comes to lie on the first coordinate (principal component). the second greatest variance lies on the second coordinate, and so on. the number of principal components is generally less than the number of original variables. so pca reduces the dimension of our aircraft diagnosis system input data. for example, in our case, figure 5 shows that the first principal component's variance constitutes more than 50% of the total sum of all variances over all components. this variance decreases as the order of the corresponding component increases. finally, 6 principal components were found to add up to 95% of the total sum of the variances in our case. note that these principal components are computed based on the 33 used anti-icing system sensors and thus a big dimension reduction is performed, while keeping most of the beneficial information. diagnosis systems using neural networks or svms, where the absence of signals or the zero-valued inputs lead to inactivity of certain parts of the networks. however, while the diagnosis tree cannot resolve this problem, it can perform a quick relearning based on the initial learning database and using only the data of the currently active sensors. for example, with a computer -intel pentium dual-core t4400 and 2g memory, it only needs 46 seconds for relearning with a 33x160000 dataset while with an svm or a neural network it takes more than one hour. figure 6 . relearning of the diagnosis tree figure 6 shows data provided by 33 sensors and m learning examples. if the aircraft loses n sensors or sensors' information, the diagnosis tree will automatically detect the loss and get relearned using new vectors of dimension 33-n. this ensures an appropriate online functioning of the system during a flight."
"where values(a) is the set of all possible values for the variable a, and s v is the subset of s for which the variable a has the value v. so, as said before, at first the entropies and the information gains of all the variables are calculated. the variable with the highest information gain is the root node from which leaves go out, each leaf corresponds to a value that its variable can take. following this, entropies and information gains corresponding to the other variables and to each of the leaves going out of the root nodes are computed, and new nodes are taken corresponding to the highest gains. and so on, until all variables are used and the final tree is formed. classification is done by using the values of the variables constituting an example and following the resulting tree leaves and nodes from the root to the final leaf which gives the class."
"adapt to number of nearby devices. a system's interpretation of a person's actions can also depend on the number of other nearby devices that it can sense. to illustrate, consider swindells [cit] gesturepen that allows device to device interaction from a distance when one device is pointed to the other device. we foresee extending this technique to a large number of devices by applying further distance2or identity2based filtering technique to limit the number of possible pointing targets. when pointing towards a large number of possible targets, the system could require the person to move closer to do a precise selection of the target device. in contrast, if only a single device is in that area, the system will recognize the target device and will not require the person to move closer."
2. convert the learned tree into an equivalent set of rules by creating one rule for each path from the root node to a leaf node.
"in order to answer the first question, and thus to determine the best sequencing of the variables, we resort to a measurement that is commonly used in information theory, called entropy. entropy characterizes the impurity of an arbitrary collection of examples [cit] . for example, given a collection s containing positive and negative examples of a target concept, the entropy of s relative to this boolean classification is:"
"where x a, y a, z a and v ax, v ay, v az are the cartesian coordinates of terminal relative position and velocity between uav and target in the inertial reference frame, respectively. in what follows, x dap is the terminal state constraint of the control system while designing time-coordinated attack control law. uav swarm is divided into several attack subgroups. according to the different roles in the mission, uavs in uav swarm can be divided into three types: commander uav, subgroup leader and subgroup members, as shown in fig. 3 . the process of initiating and assigning cooperative attack tasks is as follows. after the commander uav receives the attack task, the terminal arrival time t f and desired attack position x dap of each subgroup are coordinated on the basis of the attack modes, target characteristics, subgroup distribution, and so on. then, the commander uav assigns attack tasks to each subgroup, and sends attack parameters to the subgroup leaders through the communication network. finally, the subgroup leaders coordinate every subgroup member to set attack parameters and launch cooperative attack. by the way, the desired attack position is calculated by the commander uav, which is not preprogrammed in a rigid position. actually, there is some algorithm computing the optimal desired attack position, but this is not the focus of this paper."
"the rest of this paper is organized as follows. section ii demonstrates the classical decision tree fundamentals, and shows a study of the tree's over fitting and the pruning of unnecessary branches. section iii demonstrates the usage of pca in decreasing the initially high input data dimension. both the procedure of that application and its effect on the system are included in this section. section iv reveals the robustness and the fault-tolerance of the system even in the case where some sensors are lost. section v shows the results of dgcis, conseil ré gional d'ile de france, conseils gé né raux 77 et 78 (sponsors)."
"during the training, cv is used to avoid the over fitting problem. subsection b shows the pca's utility in reducing the data dimension, subsection c simulates a dangerous situation of sensors loss and presents corresponding results. in order to get a valid scientific and compelling evaluation scheme, a trail of one hundred repetitions of all the processes is studied as shown in figure 7 . the evaluations study the ratios of the following criteria: correct decision, missed detection, and false alarms as shown in the last two subsections."
"orientation generally describes how people face towards or away from each other, and this too affects proxemic relationships. sommer [cit] studied people's preference of spatial seating arrangements and relative orientation around a table depending on the task at hand. depending on the task, the majority of people tended to particular seating positions: face2to2seating face for competitive tasks, side2 by2side for cooperative tasks, and side2by2side or corner2to2 corner during conversations. others identified patterns, where people's orientation to one another depended on the type of conversion and social status [cit] . in later work, hall included orientation as an essential variable in describing proxemic relationships [cit] ."
"where t f is the simultaneous arrival time. similarly, more complex time-coordinated strategies can be expressed by constructing different time-coordinated function. such as a strategy of simultaneous arrival within one group and interval arrival between groups, (·) becomes the time-coordinated strategies mentioned above are applied to verify the effectiveness of proposed time-coordinated control law. on the basis of the control law proposed in this paper, the real-time communication in the swarm is not required, but the communication of coordinated variables is necessary at the initial time. before the attack, all uavs in uav swarm should coordinate their arrival time t f,i and the dap x dap,i according to the requirements of mission. for ith uav in the swarm, the parameters p 2,i, p 3,i and the adjoint vector g i in [t 0 : t f ] can be obtained based on the predefined t f,i and x dap,i . the time-coordinated control law for ith uav is"
"proxemics as people's cultural perception and use of personal space to mediate their social interactions with others in everyday situations [cit] . while proxemics emphasises distances between people, other attributes are also relevant, e.g., orientation and body language. yet, despite people's understanding of proxemics, only a handful of interactive systems within pervasive and ubiquitous computing (ubicomp) [cit] have applied proxemic relationships to interaction design in a holistic way e.g., [cit] . this is surprising, since one promise of ubicomp is to situate technology in people's environments, where it leverages, exploits, and becomes integrated into everyday practice [cit] ."
"in our own recent work, we proposed the idea of proxemic interactions [cit] . we described how devices could have fine2grained knowledge of nearby people and other devices, and -as others have also done -we illustrated various examples of how that knowledge could be exploited to design interaction techniques."
"1. use the cv to infer the decision tree from the training set, grow the tree until the training data is as fit as possible and allow over fitting to occur."
"substituting p[t 0 : t f ] into (33), g(t) in [t 0, t f ] can be calculated by rk4. substituting (29) and (32) into (26), the optimal control law in x-axis becomes"
"a database consisting of 160000 examples (160000 records, one record per second) is used to build the diagnosis system and test it. it is constituted of data vectors obtained with the pre-described 33 sensors of the anti-icing system of the aircraft x. data is measured during a flight and describes the faults and the normal aircraft state without faults. 75% of the data were chosen randomly for the training and the remaining 25% are used to test the tree."
"in this subsection, a state of emergency is described: a number of sensors are lost. to simulate this, at first the number of the sensors is randomly reduced, without pca. and then pca is used to pre-treat the data if a few sensors like 1 ~ 5 are lost. in this case, pca wasn't able to reduce the dimension of the input data if there were already many sensors lost. we run the system one hundred times, and at each time sensors are lost by random; at last we calculate the mean of the correct detection, the missed detection and the false alarm of the tests series. table vii and table viii show the stability of the system within several lost captors. while the missed detection and false alarm ratios rise with the loss of sensors information, the performances of the system remain sufficiently accurate."
"hall also identified two other factors that influence people's use of the micro2space around them [cit] . fixed features include the immobile properties of the space: the layout of buildings and rooms, the walls, doors, and windows. semi fixed features include the spatial layout of elements in the space that can be moved (like furniture, chairs, or tables)."
"challenge 3. establishing connections [cit] : device connectivity is a significant challenge in ubicomp. technical issues aside, ubicomp's ad2hoc nature means that people need to somehow control (albeit seamlessly) how one device connects to another device in a way that reflects their interaction needs while still safeguarding privacy and security (for example to transfer digital content from a personal smartphone to a large public screen). this challenge is compounded by the potential and perhaps unpredictable interplay between a large numbers of digital devices. some may be personal (a smart phone), others may belong to the inhabitants of a space (a home's picture frame), and others may be public (e.g., a public wall display). their form factor also affects their mobility, which in turn can suggest different factors affecting how they should establish connections."
"reacting to the presence and approach of people. at the most basic level, if a system can sense the presence and approach of people, it can use that information to reveal possible interactions."
"proxemic)dependent reveal of feedback. details presented to a person can vary depending on the distance and/or orientation of the person relative to the system. lean and zoom, for example, introduced a distance2dependent semantic zoom technique [cit], where more details of the displayed object are revealed when the person moves her head closer to the screen. this idea of semantic zoom can be applied to ubicomp environments. he [44, chapter 3] implemented an augmented reality energy viewer for the home, where feedback of energy use was adjusted based upon the viewer's proximity to rooms or appliances within a room (distance and orientation are detected through fiduciary tags). when holding the viewer outside a room's doorway, the energy use of that room as a whole is displayed. when the person moves into the room, the energy use of each appliance is seen as a coloured glow around it; as he moves closer to a particular appliance, details of that usage appear first as a text overlay and then as a graph."
"in this article, we take a step back. we focus on proxemic theory and show its potential to address five key design challenges of ubicomp interaction [cit] : revealing interaction possibilities, directing actions, establishing connections, providing feedback, preventing and correcting mistakes, and managing privacy and security. we then operationalize proxemics as knowledge that can be sensed or captured by devices via five essential dimensions -distance, orientation, movement, identity, and location -and discuss the nuances of their use. we then relate both the theory and dimensions to the design challenges, and situate a sampling of prior systems within that setting [e.g., 3, 4, 6] ."
"the first term in (20) is the terminal term, indicating the tracking error at t f, that is, the sum of squared errors between x(t f ) and y d (t f ). the second term in (20) is the process term that represents the magnitude of energy consumption during system control. the physical meaning of (20) is to optimize the energy consumption of the system during the control process and the system steady-state error at the terminal time. in other words, uav's fuel consumption in the process of reaching the dap and the error between uav's state and dap at the terminal time are comprehensive minimum."
"as shown in fig. 8(b), the accelerations of uavs change dramatically during 24−26s and 35−37s, which is influenced by the acceleration and deceleration of the target. furthermore, the control input of uavs converges to zero at and for the first subgroup and the second subgroup, respectively. fig. 9 illustrates the time history of thrust, g-load, banking angle and flight path angle, respectively. the first three figures in fig. 9 demonstrate the actual controls which are computed using (5) . obviously, all of the actual controls are within the prescribed constraints."
"in this section, the models of the uav and the groundmoving target are firstly described. a feedback linearization technique is then employed to simplify the uav model to a double-integrator model. the time-coordinated attacking problem will be investigated based on the reduced model."
"based on the feedback linearization, the complicated nonlinear uav model can be transformed into a linear time-invariant double-integrator model [cit] . specifically, we can differentiate the kinematic (1) once with respect to time, and then substitute the dynamic (2) to obtain"
"of course, both the above techniques can be combined to override the system. in fact, vogel used both in his system: a person can either use a set of simple hand gestures to trigger or stop certain system functions, or just step back from the screen to have the same effect."
"we concentrated on a few example systems and techniques to illustrate how our challenges can be addressed. these were chosen to inspire design thinking. they are not meant to be a complete review, nor as a catalog of solutions. we also recognize that a single technique can serve different purposes across these challenges. for example, the idea of progressive reveal of information as a person approaches a display reveals interaction possibilities (challenge 1), affords actions being directed to it (challenge 2), is used to establish a connection (challenge 3), provides feedback that it is responding to the person (challenge 4), can be used to prevent and correct mistakes by inverting actions (challenge 5), and helps people manage privacy and security simply by moving to adjust what information is visible (challenge 6). we believe this to be one of the strengths of proxemics: if techniques are developed with social expectations of proxemics in mind, they can likely be applied as a universal way to mediate many challenges in ubicomp."
"compared with attacking ground target with a single uav, the advantage of uav swarm is that there are massive emergences of attack effectiveness, with the time-coordinated capabilities. therefore, the arrival time of each uav should be limited, according to different time-coordinated strategies. the constrain of each arrival time has the form of"
"while the above systems are binary in nature, progressive connection processes are also possible. kray's group coordination negotiation [cit] introduced spatial regions around mobile phones to establish and break device connections or initiate data transfer. as a device moves across three discrete regions, a preview of a media transfer is first display, where transfer begins only after moving into a closer region. our proxemic media player is somewhat similar, but it uses a continuous rather than discrete progression over distance [cit] . when a person holds a handheld media player in her hand, a subtle notification on the large screen indicates the connection possibility. as he moves closer to the screen, he sees the two devices connect, where the large display progressively reveals more information about the handheld's video content as icons. as the two devices move within touch distance, a touch interface appears that allows the person to transfer digital media either through pick and drop or by touching the handheld to one of the icons revealed on the large display."
"where  p and  p are respectively the proportions of positive and negative examples in s. note that in calculations involving entropy we consider 0* log (0) to be equal to 0. in our case, a collection will be the set of values provided by a sensor, that is to say, a variable. such a variable can be boolean, like in the example, or can take multiple values. so at first, we calculate the entropies of all the variables. then, a measurement of the effectiveness of an attribute or value that a variable can take is needed. we use a measure called information gain that is simply the expected reduction in entropy caused by partitioning the according to the attributes and values that a variable can take. more precisely, the information gain, gain(s, a), of an attribute a, relative to a collection of examples s, is defined as:"
"challenge 5. avoiding and correcting mistakes [cit] : when mistakes or errors happen, the system should provide options for a person to correct these mistakes. as many ubicomp systems use some kind of sensing technology to monitor people's actions, such errors and misinterpretation of sensor data are even more likely to occur in ubicomp settings than with traditional computers."
"equation (30) is solved from t f to t 0, where t f is the initial time and t 0 is the terminal time in rk4, and the time step h is negative. p(t) in [t 0, t f ] can be calculated offline."
"these actual control variables above are sent to the autopilot in real time, which automatically calculates the engine thrust and the rotation angles of rudder, ailerons and elevator."
"almost twenty years ago, mark weiser proposed ubiquitous computing (ubicomp) as the next era for interacting with computers [cit] . in this proposed future, he foresaw that network connected digital technologies would be available in our everyday environments, in a variety of form factors and sizes that would suit the task at hand. given today's availability and use of such devicessmartphones, tablet computers, net2aware digital cameras, photo2frames, interactive whiteboards, digital tabletops and so on -it may seem that his vision has been realized. but weiser's vision went beyond devices. importantly, he predicted the move of computing technology into people's everyday surrounding, embedded in all kind of everyday objects and spaces, where it would be seamlessly accessible: \" the most profound technologies are those that disappear."
"proxemic)aware privacy mechanisms. while these approaches consider distance as a factor affecting access, the techniques could be further refined by considering other proxemic dimensions such as orientation, identity, or location. a person's body, face, or gaze orientation can affect the amount of information shared. for example, privacy2sensitive information shown on the display of a proxemic2aware mobile device could be visible as long as the person is looking at the screen, but hidden once looking away. alternatively, the information might disappear once the system notices another person looking at the display. by considering the identity dimension, a system would be able to use relaxed privacy and security settings when a person is alone, but switch to more restrictive privacy and security settings when it detects any other people or devices around them (e.g., in a crowded setting). by considering location, a mobile ubicomp device could adjust its security setting depending on the type of environment; using higher level settings in an open office (where strangers may come by and try to access the device), but lower security level when at home (which is usually a much more trusted setting)."
"so in summary, cv performs a search through multiple pruned tree configurations, and keeps the configuration that gives the best validation results. in this paper, the decision system is built with 33 sensors' information. as shown in figure 3, cv operated during a learning procedure allowed us to find the best size of the tree based on these 33 sensors, with 11 terminal nodes. this is obtained while the number of nodes without cv can actually grow much higher than 11. figure 4 shows the pruned tree, originally 59 nodes are obtained without pruning, the dotted lines are the leaves to prune, and the remaining nodes contain the rules that the system finally used. it is clearly that there is one root with 14 branches in the original, and only 6 branches left after optimization, which highly enhances the working efficiency."
"in general, the solution of riccati differential equation has no explicit expression, and can only be obtained by numerical algorithm. runge-kutta method is a high precision one-step algorithm widely used in engineering to solve this equation, including the famous euler method. euler method is the first-order form of runge-kutta method and its error is o(h), where h is the time step. due to the large accumulation of errors in the calculation process, euler method is not adopted in practical application. one of the various runge-kutta methods is so common that it is often called fourth-order runge-kutta (rk4). rk4 is a fourth-order method and the error of each step is o(h 5 ), and the total accumulated error is o(h 4 ). therefore, rk4 is great enough to meet the requirements of solving time and precision in practical applications."
"since the solution of p(t) is independent of y d (t f ), it is not necessary to obtain p(t) repeatedly when solving the optimal control a * y and a * z in the y-axis and z-axis. the solution of g(t) is related to y d (t f ) which is different in three directions so that g(t) has to be solved separately. the optimal control law in the y-axis and z-axis are solved by the same method, and the optimal control law is obtained as"
"in order to establish conveniently the mathematical model of the target, the target is regarded as an object moving in two-dimensional plane in section ii. however, in the process of deriving the control law, the target refers to an object moving in three-dimensional space for the sake of integrity of the theory. fig. 4 is given to illustrate the scenario, where the point u, t and a represent the uav, the target and the dap, respectively. all of these points are moving in three-dimensional space, and the relative position between a and t remains fixed. it is assumed that there is a target tracking system to locate the target's position and estimate its motion state. the motion equation of uav (4) and target (8-11) satisfies the property of second-order integrator. so the relative motion satisfiesṙ"
"since the equation of system state given in (16) (17) and the terminal constrains in (19) are decouple between the x-axis, y-axis and z-axis, optimal control law a * x, a * y and a * z can be independently obtained. therefore, the analysis and the solution of the optimal control law will be limited only to the x-axis. the system along x-axis is simplified to"
"one technique allowing a person to correct a mistake (and thus undo a system's action) is by performing the inverse/opposite action. the system implicitly responds by reverting to the prior state. for example, in vogel's ambient display setting [cit], when a person moves closer to the screen, personal calendar information is revealed. if the person didn't want this information made public, he just steps back (and thus performs the opposite action): the personal information disappears immediately. other proxemic dimensions can be exploited as well. for example, an action triggered by the person facing a screen can be stopped (or reverted) simply by turning away."
"where t 0 and t f represent the initial time and the terminal time, respectively. f is a non-negative symmetric constant matrix, which means the error weight matrix"
"the method we have developed and applied here provides a strategy for reconstructing and analyzing dynamical networks in biological systems. in addition to providing networks in the temporal context, our method provides the directionality and potential causality of molecular interactions. we note that we built our methodology based on the notion of granger causality, which is not meant to be equivalent to the true causality."
"3) s6 and rsk: ribosomal protein s6, which is involved in cell growth and regulation of cellular translation, is phosphorylated at several serine residues with mitogen stimulation by activation of one or more protein kinase cascades. it is well known that in mammalian cells, phosphorylation of ribosomal protein s6 in vitro and in vivo is regulated by the activation of rsk [cit], while our results indicate the existence of a bidirectional connection . rsk is involved in receptor-mediated signal transduction. phosphorylation of rsk, which promotes cell survival and proliferation, lies at the end of the signaling cascade mediated by erk and is regulated through the activation of erk subfamily of map kinases [cit] . we observed this relationship in the first and second stages. furthermore, our network suggests that rsk can be activated by p38 through the connection in stages 1 and 2 and in stage 3. in current literature there is some evidence confirming this interaction in hek293 cells [cit] . protein kinase c (pkc) is a family of fatty acid-activated protein kinase enzymes that is involved in regulating cell growth, learning and memory, transcription and mediating immune response. pkc which exists in various isoforms, is known to be involved in the activation of erk in hek293 cells [cit], which then results in the activation of rsk through the map kinase pathway [cit] . therefore it is anticipated that rsk and pkc have a hidden indirect relationship that was captured in our model where the connection is found in stage 3 and the underlying network and the connection is found in stage 2. our model still captured this connection by considering a faster time step (half a minute) in the model. in addition, pkc mediates the phosphorylation of s6 in vivo in hek 293 cells [cit] ."
"this paper has presented the design and control of lightweight 3-dof compliant perching arm for the astrobee robot. the arm can be used for future manipulation research in zero-gravity; astrobee robots will be able to grasp each other in flight, to simulate orbital capture scenarios. the arm also allows astrobee to perch on handrails, so it can dwell for extended periods with reduced power consumption and disturbance to crew. the arm is small and light enough to be accommodated completely within astrobee's payload bay, for collision safety. the under-actuated tendon-driven gripper is sized to grasp iss handrails as well as a variety of other objects. it uses springs to close and a motorized tendon to open, so the gripper passively stays closed when powered down, and can be opened manually by the crew. we hope that the modular end-effector design will provide an opportunity to future payload developers. co-located rgb camera and lidar sensors provide perception. developing advanced torque control for the arm and autonomous perching navigation are left as a further work."
"in contrast, most of the exposed \"walls\" of the iss interior are actually unique surfaces with different material properties, such as the front of a payload rack containing sensitive experiments, the front of a fabric bag, etc. furthermore, much of the wall surface is cluttered with items like switches, small avionics boxes, and wire runs. thus, grippers designed to adhere to flat surfaces (gecko, electro-adhesion, microspine, etc.), while highly promising, are also more complicated to use in terms of identifying open spaces to perch on, ensuring the gripper can adhere to a variety of surface materials, and obtaining permission to perch at each location."
"the intelligent robotics group at nasa ames research center is building the free-flying astrobee robots to operate inside the international space station (iss) [cit] . three astrobee robots will operate in the iss, and three will remain on the ground for support testing. the robots' primary function is to serve as a zero-gravity robotics research platform, replacing the free-flying spheres satellites that have been among the most frequently-used payloads on the iss over the past ten years. the robots will also serve operational needs: as free-flying cameras to observe crew activities, and as platforms to carry sensors to survey the iss interior. for example, the realm project [cit] will use an astrobee to carry an rfid reader and take inventory of rfid-tagged items."
"to attach a payload, a crew member slides the payload down a pair of rails until it makes contact at the bottom, then actuates two retention levers. these levers both close the electrical connection with the blind-mate connector and robustly lock the payload in place. this no-tool design allows quick payload swapping with minimal crew time. prototype arm shown in this paper does not yet incorporate the lever mechanism.) a \"1u\" payload shown in fig. 2 (a) stays completely within a single payload bay. a \"2u\" payload occupies both of the contiguous payload bays on the bottom. by this metric, the perching arm is a \"1.5u\" payload that stays mostly within the top aft payload bay, but also cheats into the top forward bay, while carefully avoiding interference with the top forward module, as shown in fig. 3(b) . a second identical perching arm could be mounted in either of the bottom bays, if desired."
"stage 1 [ fig. 5(a) ] shows the initiation of interactions among phosphoproteins. since this network captures the early phase of the response of the system to the ligands, there are very few interactions taking place in the network. extracellular signal-regulated kinase (erk) plays a crucial role in the regulation and phosphorylation of most of the proteins that are present in the first stage of the network including p38 map kinase (p38), p90 ribosomal s6 kinase (rsk), glycogen synthase kinase-3 (gsk), and protein kinase c (pkc) d. ribosomal protein s6 (s6) affects erk1 and erk2. there is also a regulatory interaction between nuclear factor kappa b (nf-p65) and p38. in addition, it is evident that moesin (moe) and ezrin/radixin (ezr) are part of the same pathway since a bidirectional link exists between them. as the network progresses to stage 2, several other interactions emerge. fig. 5(b) shows that protein kinase b (akt) arises in stage 2 and regulates the phosphorylation of . the signal transducer and activator of transcription 1 a and b (stat1a/b, also st1a/b for short) pairs are variants of the same protein and are expected to be activating one another. indeed, they show a bidirectional relationship. pkcd that was regulated by erk2 in stage 1, now promotes the phosphorylation of ezr and mother against decapentaplegic homolog 2 (smd2), as well as mutually regulating neutrophil cytosolic factor 4 (p40). in stage 2, pkcm also appears and plays role in the regulation of rsk, s6 and erk1/2, while being activated by p38. role of s6 almost stays unchanged; i.e., it continues to regulate erk1/2, except that as a result of the network progression from stage 1 to stage 2, we also see its interaction with rsk. this progression also brings about the phosphorylation of by rsk. in stage 1, p38 was activated by p65 and erk2, whereas in the second stage, p38 regulates erk1/2 along pkcm and gets involved in a mutual regulatory relationship with p65. p65 also affects erk1/2 as well as rsk. the evolution of the network to stage 3 provides not only most of the links that existed in stage 2, but also includes some new interactions. for instance, akt proceeds to phosphorylate, while other nodes such as p65, rsk and p38 start to have causal influences on the activation of . furthermore, in this phase, pkcd is regulated with the activation of pkcm, p40, smd2 and ezr. another interesting change is that p65 takes part in the activation of pkcm and erk1/2. moreover, akt, broadly known for the activation of gsk, gets involved in the activation of s6, while being activated by erk2."
"to enable crew to back-drive the arm, the controller compares the disturbance torque to a threshold α. when the threshold is exceeded, it turns off the motor torque for β seconds and signals an error message to the astrobee's mlp. in this way, the controller provides a simple form of impedance control."
"other advantages of the tendon-driven gripper are that it does not use high voltages (cf. electro-adhesion), does not require a bulky, vibrating vacuum pump (cf. suction, jamming), and does not require initial reaction forces that are challenging to provide given the robots' limited propulsion performance (cf. gecko, jamming)."
"since the tendons are tensioned by the gripper dc motor in the closed configuration, the gripper does not require a calibration when the controller board is power cycled. however, the tendons may stretch over time, requiring recalibration. during the calibration step, the control algorithm continues opening the gripper until it detects a hard stop (finger joint interference) using motor driver current feedback monitoring. during the normal operation, the gripper dc motor is controlled under velocity pd control at 1000 hz. nominal gripper motor velocity is 50 rpm."
"the astrobee arm is shown in fig. 1 . the arm's kinematic arrangement with two revolute joints is designed so that when the robot perches on a handrail, the arm can act as a pantilt unit, controlling pointing of the scicam camera on the opposite side of the robot. the arm length is sized to avoid interference between the robot and the wall over the required pan-tilt range."
"2) performance metrics: type i error, type ii error, and accuracy of the network is computed [cit] as follows using the false positives (fp), false negatives (fn), true positives (tp) and true negatives (tn) in the network identified:"
"to evaluate the consistency of the data across experiments involving different ligand combinations, we applied the var model to single ligand experiments (22 experiments). according to our results, the reconstructed network based on only single ligand experiments has higher type i and type ii error. we also used only the double ligand experiments to model the network, and as we anticipated, the performance does not change significantly. it can be noted that the double ligand combinations result in activation of the signaling pathway in ways that are functionally distinct from single ligand experiments. furthermore, as an estimate of the differences in the variability for different phosphoproteins across time and treatment, we computed the ratio of the standard deviation of the standard deviation (std) to the mean of the std of every phosphoprotein (std is computed at every time for every treatment, using the replicate data), and found that this measure is of the same order (about 1) for all phosphoproteins across experiments."
"as a research platform, the astrobee robots must enable guest scientists to add new payloads to support their research. astrobee provides four identical peripheral bays: top forward, top aft, bottom forward, and bottom aft ( fig. 2(a) ). the top forward bay is always occupied by a collection of sensors that are critical for navigation ( fig. 2(b) ). the top aft bay is normally occupied by the perching arm, but crew can easily detach the arm and replace it with a payload. thus, three of the four bays are available for guest scientists to add new payloads that support their research. the identical layout gives payload developers the flexibility to assign any available bay to their payload on orbit, making it more feasible for multiple payloads to ride along simultaneously."
"the results shown above are acquired through data-driven reconstruction of the network with no a priori information about the behavior of the underlying biological system. here, we inspect our results and compare them with the existing information in the biology literature. in table iii, every causal relationship between pairs of phosphoproteins is shown by a directed arrow, and each mutual interaction is shown by a bi-directed arrow."
"the correlation coefficients with their corresponding -values, along with the benjamini-hochberg fdr and -values based on the -test on the model coefficients for the connections retained in the underlying network (fig. 3) are listed in table ii . it can be noted, that the benjamini-hochberg fdr for all these connections/edges are less than 0.026. the distribution of the -values ( -test on the model coefficients) from all 17 17 possible connections for the underlying network is shown in fig. 4 (implicitly used to calculate fdr) ."
"the arm is small enough to fit completely within an astrobee payload bay when stowed. this avoids increasing the volume occupied by the robot, which is important when moving through confined spaces such as the hatchways between modules. it also keeps the stowed arm within the envelope of the robot's padded corner bumpers, which are designed to protect both the robot and the iss from high forces in case the robot collides with iss structure."
"another phosphoprotein involved in the regulation of gsk is p38. recent studies indicate that p38 induces gsk phosphorylation in brain, thymocytes and human breast cancer cells (mda-mb-231 cells) [cit] which is detected in the last two stages in our network. furthermore, erk activates gsk through phosphorylation in hep-g2 cells and myocardial tissue cells in mice [cit] . we detect this relationship in the first two stages. moreover, the existing knowledge illustrates that gsk is involved in the activation of p65 in hepatocytes from mice and hela cells [cit] while our model captures the reverse connection in stage 3. 2) ezr and moe: ezr and moe are part of the same pathway, called ezrin/radixin/moesin (erm) protein pathway. the erm proteins regulate actin cytoskeleton and are involved in signaling, transport, and structural functions of the cell [cit] . as we can see in fig. 2, the heat-map shows high correlation between these variables. in addition, the pairs erk1/2 and stat1a/b are variants of the same protein and are expected to be regulated similarly. thus, as expected, high correlations and bidirectional causal relationships are observed between the members of each pair in figs. 3 and 5 . despite the fact that the heat-map in fig. 2 shows very high correlation between and in all stages, we observe the connection only in stage 2. this is an interesting result confirming the fact that \"correlation does not imply causality\" in the sense that the two variables may be highly correlated but there is no information in the past of one of them that can be used to predict the future of the other. the same result was found for pkcd/m. the connection was found only in stage 3."
"our selected end-effector design is a 1-dof underactuated tendon-driven gripper. we use two fingers with two revolute joints per finger, all actuated by a single tendon. compared to a simpler and smaller parallel-jaw gripper, this type of kinematic design has the potential to perform more stable enveloping grasps on a range of objects of different sizes [cit] . the gripper is sized to robustly grasp iss handrails-these handrails are attractive targets because they are located throughout the interior for crew convenience, they have uniform shape and appearance, and their rugged aluminum structure is designed to handle high loads exerted by crew (so astrobee robots will not damage them)."
"we have reconstructed the phosphoprotein signaling network that represents the underlying network corresponding to the full time series data shown in fig. 3 . in this network, out of 17 17 possible connections, only 35 were significant, many of which have negative coefficients in matrix . connections with negative coefficients are considered as inhibitory relationships shown in fig. 3 . important inhibitory edges include [cit] . different edge-widths are used to indicate edges with low, medium or high correlation. to test the robustness of our model to the choice of and correlation threshold, we used different correlation thresholds and confidence intervals (for the two tailed -test) to reconstruct the underlying network. to evaluate the performance of each trial, we compared the significant connections identified for the underlying network to the true connections from the literature. table i implies that by increasing from 0.01 to 0.02 and 0.05, i.e., reducing the confidence interval from 99% to 98% and to 95%, the number of false positives increase and thus, type i error increases. we also tested the results for different correlation thresholds that result in further trimming of the parameters. the optimal correlation threshold for which type i and type ii errors are both minimized, is . we also studied the effect of more fine time-intervals. if we interpolate with steps of half a minute instead of one minute, the accuracy of the model does not change significantly. with a sample time of one minute, accuracy is 0.86, and with that of half a minute, accuracy is 0.87. we found that by using the cubic interpolation rather than linear interpolation, type ii error increases, justifying the use of linear interpolation."
"we also present the dynamic evolution of the network in three temporal stages shown in fig. 5 . the topology of the phosphoprotein network changes through time. fig. 5(a) corresponds to the reconstructed network in the first stage of the network development. fig. 5(b) and (c) correspond to the reconstructed phosphoprotein networks for the second and third stages of the network evolution, respectively. the inhibitory edges such as are shown in fig. 5 ."
"we applied this method to time-course data on the level of phosphorylation of proteins in raw 264.7 macrophages in response to stimuli, provided by the alliance for cellular signaling (afcs) [cit] . this data set consists of fold changes of 21 phosphoproteins at 4 time points; i.e., data at 1,3,10 and 30 minutes, in response to treatments with 22 single ligands and their double ligand combinations measured using the western blot method. the fold changes of the phosphoproteins are determined by dividing the volume of each phosphoprotein band for the ligand-treated samples by the average volume of the corresponding bands for the untreated samples (volume is the sum of the image pixel values within the area of the band). the replicates for the experiments with unique combination of ligand(s) for each phosphoprotein were averaged. out of 327 unique ligand combinations, the number of combinations with 1, 2, 3, 4 and more than 4 replicates was 68, 68, 123, 37 and 31, respectively. thus, most ligand combinations have three replicates, hence resulting in only a small bias due to the difference in the number of replicates."
"the perching arm is required to operate both in the zerogravity iss environment and in 1 g lab testing. thus, the arm joint torques must be specified so that the arm can support its own weight in 1 g, but joint brakes are not needed, because they will provide little power consumption benefit when the system faces low disturbance torques on orbit. the 2-dof arm uses two dynamixel xm430-w210 motors. each motor has an aluminum case, which helps with heat rejection and satisfy iss flammability requirements. the lengths of the proximal and distal links are optimized such that the 2-dof arm stows inside the astrobee payload volume, and provides a pan range of -90.0"
"part of astrobee's safety approach is to ensure each robot is light, soft, and slow enough that it is unlikely to damage the iss in case of a collision. this approach forces a lightweight design, making it impractical to build the arm to resist strong forces exerted by crew. we have chosen to turn this constraint into a feature by making the arm highly compliant and back-drivable. each arm joint can detect large astronaut-induced torques in real time and deactivate automatically to allow back-driving. the 1-dof gripper uses a torsional spring to close and an actuated tendon to open, so the gripper passively maintains its grip when unpowered, and astronauts can manually open the gripper by overcoming spring torques."
"due to the fact that the time intervals are not equal, we interpolated the data using linear interpolation with steps of one minute. other interpolation methods (e.g., cubic) may result in large deviations at the intermediate time points, and this may not be close to the real variation of the fold change of the phosphoproteins in the biological system. we excluded the last sample in the original data, since it was taken 20 minutes after the previous one, which is considered to be too large an interval for accurate interpolation. in these experiments, we had missing data for 4 of the 21 phosphoproteins, signal transducer and activator of transcription (stat) 3, stat5, c-jun n-terminal kinases (jnk) long (jnkl) and jnk short (jnks). therefore, we excluded these variables from further analysis. we assumed that at a given time, the underlying phosphoprotein network that represents the structure or the topology of the biological system is the same across all experiments, i.e., the topology of the phosphoprotein network representing the behavior of the biological system remains unchanged regardless of which ligand(s) is stimulating the system. thus, to deal with the problem of rank deficiency of matrix in (4), we stacked the data from multiple experiments for both the output data in matrix (data related to present) and the input data in matrix (data related to the past). this ensures that matrix will have full column rank and there will be a unique solution to the least squares problem. fig. 1 shows a schematic of how the input and output data from multiple experiments were stacked. before implementing the var model, the data in matrix was normalized and matrix was mean-centered for each variable. in addition to implementing the var model, the correlation between the past and present values for each pair of variables was studied and the correlation matrix between the input and output variables was computed. fig. 2 visualizes the correlation matrix as a heat-map, where the rows and columns of the heat-map are the input (at time ) and the output variables (at time ) for the whole time-series data, respectively."
"since the intracellular networks have a dynamic nature and their topology changes with time, in this work, our main goal was to investigate the temporal evolution of the phosphoprotein network. during the early stage, erk plays an important role in regulating p38, rsk, pkcd and gsk, while erk itself is regulated by s6. as the network evolves to the second and third stages, the well-known signaling pathways such as the mapk, stat1a/b, akt/gsk and nfpathways appear to play role in the network. these results have enhanced our knowledge about the important signaling pathways that activate macrophage cells and play an essential role in the secretion of cytokines during an inflammatory response, and may contribute to finding novel targets for inflammation-related diseases."
"integrative framework, taking into account the dynamics of signaling networks [cit] . during the last decade, the application of mathematical and statistical approaches to high-throughput biological data has been used extensively to decipher the relationship between different components in the cell to partially reconstruct intracellular networks. with the availability of large-scale omics data, computational systems biology has made substantial progress towards modeling and reconstruction of data-driven networks using (1) input/output-based models such as partial least squares (pls) [cit] and principal component regression (pcr) [cit], (2) probabilistic graphical models such as bayesian network-based models [cit], probabilistic boolean network models [cit], and (3) information theory-based methods such as integrated correlation and transfer entropy based approach [cit] and c3net [cit] . other approaches using differential equations [cit], structural equation methods [cit] and state-space models [cit] have also been proposed during the past few years."
"if a perched astrobee is blocking a crew member's escape path during an emergency, they can simply push it out of the way, easily overcoming the grip strength. they can also backdrive both arm and gripper to manually perch an astrobee, when convenient."
"some of these pathways such as p38 and nfregulate the transcription of the cytokine tumor necrosis factor ( ) which is a target for rheumatoid arthritis [cit] . nfis involved in the regulation of pro-inflammatory chemokines and cytokines in meningitis [cit] . furthermore, deviations in the levels of mapks from their normal cellular levels have been implicated in the development of cancer [cit] ."
"as a part of the astrobee robotic system, a compliant, detachable arm is being developed. this arm will both support manipulation research and allow an astrobee to perch on iss handrails during long duration tasks. perching allows the robot to minimize power consumption by idling propulsion and reducing computational load. it also keeps the robot quiet and out of the way of crew, an important advantage when it is used to capture video of crew activities. when designing the arm, we were faced with a great diversity of end effector options [cit] (including suction [cit], electro-adhesion [cit], microspine [cit], geckoadhesion [cit], and underactuated grippers [cit] . in order to preserve as much flexibility as possible for future researchers, we chose to: (1) make the end-effector module easily swappable on-orbit, giving researchers the option to add a new end-effector without needing to build an entirely new arm, and (2) build a baseline end-effector using mature technology that is highly reliable and can serve astrobee's operational need for perching, while also supporting as much research as possible."
"in this subsection, we discuss the dynamic nature of the phosphoprotein network evolving in three successive temporal stages. for the sake of simplicity in our discussions, we treat each phosphoprotein as a node and each regulatory interaction as an edge in the network analysis."
"note that the gripper is designed as a modular component. crew can easily swap in a new gripper, either as a research payload or a permanent upgrade. since the arm communication bus allows daisy chaining, a future gripper module could even provide extra arm degrees of freedom. the prototype shown in fig. 1 has been tested with an air bearing such that the astrobee robot is free to drift on the granite table. the prototype is able to grasp a handrail and exercise the pan motion successfully. this paper is organized as follows. section ii describes the design of 3-dof perching arm for astrobee including the astrobee payload interface, structure, and avionics. section iii explains the process of controlling 3-dof perching arm in real time. section iv presents the simulation results of gripper and the experimental results of perching arm on a micro-gravity simulating surface. concluding remarks follow in section v."
"the arm, shown in fig. 3(a), consists of arm base, arm proximal joint, arm distal joint, and gripper. the controller board is located in the arm base. the arm proximal joint and the arm distal joint are used to stow the gripper inside of the outer structure and to operate as a pan-tilt module. the gripper is designed to grasp iss handrails, and packaged as a replaceable modular component. all structural pieces are printed using ultem 9085, which is the preferred 3d print material on board the iss due to its offgassing and flammability properties. fig. 3(c) shows the deployed configuration. the arm deploys while in free space away from the handrail. during the perching approach, the robot transitions from generalpurpose localization (using forward-facing navcam monocular vision) to handrail-relative localization (aft-facing perchcam depth sensor that detects handrail geometry). once the gripper has nominally enveloped the handrail, the robot can verify a successful grasp by reversing thrust and checking for null motion."
"in flight, the robot will be able to use its propulsion system to autonomously approach a handrail and perch, but that functionality is not implemented and left as future work."
"t he understanding of cellular function at the molecular level involves the study of intracellular signaling, metabolic pathways and gene regulatory networks, through \"omics\" measurements on biological systems. protein phosphorylation is one of the main steps in intracellular signaling from the activated proteins located at the plasma membrane to the cytosolic space and nucleus. phosphorylation is one of the most studied post-translational modification of proteins since is it vital for many protein interactions that regulate cellular processes such as cell growth, cell differentiation and development to cell cycle control and metabolism [cit] . phosphorylation is a key reversible modification with the combined involvement of protein kinases and phosphatases to activate and deactivate proteins [cit] . phosphorylation mainly occurs on serine, threonine and tyrosine residues that can regulate enzymatic activity, subcellular localization, complex formation and degradation of proteins. activation of proteins through phosphorylation serves as the flux in the signaling pathways. several signaling pathways such as the nuclear factor kappa b (nf-), mitogen-activated protein kinases (mapk), and signal transducer and activator of transcription (stat) play essential roles in transmitting signals that trigger the release of cytokines, which are central to the processes of inflammation and modulation of immune function [cit] . the signaling pathways act as modules to regulate the transcription and release of various cytokines, some of which are involved in the pathogenesis of many diseases, e.g., chronic inflammatory diseases, autoimmunity and cancer. thus, reconstructing protein networks from \"omics\" measurements can help us not only understand and model cellular signaling pathways but also assist in uncovering the mechanisms of disease progression. since knowledge of protein-protein interaction is sparse, it is difficult to simultaneously analyze the dynamics of various proteins in vitro or in vivo. high-throughput technologies, such as nextgen sequencing, dna microarray expression profiling, phosphoproteomics, metabolomics and high-content imaging, have made it possible to make concurrent quantitative measurements of various components of the cell, including mrna levels, protein phosphorylation and metabolites, enabling the reconstruction of large-scale cellular networks. [cit] -4545 © 2014 ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"while the arm was commanded to deploy slowly and the gripper was commanded to open. (in flight, slow movement minimizes dynamic effects on the robot.) the test operator then manually moved the robot so the gripper contacted the handrail as it would after a successful perching approach. the gripper was commanded to close slowly, enveloping the handrail. the test operator checked for a solid grip by applying a gentle pulling force directly away from the wall, then released the robot. (in flight, both the propulsion system and compute-intensive navigation software would be turned off at this point to reduce power consumption.) the arm was then successfully commanded through a pan motion, repointing the scicam camera attached on the opposite side of robot. scicam images are shown as insets at the upper left."
"there are three distinct subfamilies of mapk pathway: erk1/2, jnk and p38 map kinases that have substantial impact on mediating various cellular signaling functions and physiological processes. these three enzymes are part of a phosphorylation system in which they regulate and phosphorylate one another [cit] . in this study we do not analyze the role of jnk in the signaling pathway, and we focus on the role of erk1/2 and p38 in regulation and phosphorylation of one another and other phosphoproteins. the activation or inhibition of p38 potentiates the activation of erk [cit] . unlike other pathways that appear only in the last two stages in our results, the crosstalk between erk and p38 is found in all three stages. the activation of nf-kappa b (p65) can be triggered by the phosphorylation of erk1/2 and recent research affirms the existence of cross-talk between erk and p65 and between p65 and p38 [cit] that can be seen in fig. 5 . p38 mapk plays a critical role as downstream effector of pkc enzymes in lncap human prostate cancer cells and sk-hep-1 hepatocellular carcinoma cells [cit] . our results indicate the connections in stage 3 and the underlying network, and in stage 2. furthermore, p38 modulates the phosphorylation of subfamilies of rsk such as 70 kda ribosomal s6 kinase (p70s6k) and ribosomal s6 kinase 1 (s6k1) [cit] . we also know that rsk's target substrate is s6 [cit] . this implies that p38 may indirectly play a role in the phosphorylation of s6. our findings indicate that the connection exists in stage 3 and the underlying network. there is no evidence in the existing literature confirming this relationship. the correlation coefficients for these edges are close to the correlation threshold. with a faster time step in the model, this connection is no longer significant. hence, this interaction can be considered as false positive in our results. moreover, phosphorylation of ribosomal protein s6 is known to be dependent upon the activation of erk in hela cells and in mouse dentate gyrus [cit] whereas our model captured the reverse connection."
"note that payloads could also extend outside the volume of the payload bays, but in that case they would no longer be able to rely on the robot's padded corner bumpers to protect them in a collision; they would need to develop their own collision safety plan (perhaps employing additional bumpers, or relying on crew tending for safety)."
"recent evidence implies that stimulation of pkc activates erk1 and erk2 in myocardial cells of rabbit, glomeruli of diabetic rats and glomerular mesangial cell cultures under high glucose conditions and in human neutrophil cells [cit] . in our results, this relationship arises in the last two stages."
"there are three main design drivers for astrobee perching arm structure-size, mass, and compliance. the perching arm must stow completely inside of astrobee payload volume so that it is not exposed to collision hazard during flight operations. the perching arm must be lightweight in order to preserve the robot's maneuverability given propulsion performance limitations (the lighter the better, target mass on the order of 500 g-1 kg). the design must allow an astronaut to manually perch the arm and gripper to a handrail without requiring power from actuators. in contrast, once the actuators are powered, arm joints must be fully back-drivable and gripper must be released automatically when it detects large astronaut-induced torques."
"biological systems evolve through time and it is important to study the dynamic behavior of the topology of the signaling pathways/networks themselves [cit] . thus, we allow the network topology (the set of connections/edges present in the network) to evolve with time. our objective in this study is to derive a time-varying model for the phosphoprotein network to understand the dynamics of signaling pathways using the notion of granger causality. causality can be determined by prior biological information. however, in many cases, no \"a priori\" knowledge is available to provide causal relationships in network reconstruction. furthermore, it is appealing to discover new causal relationships, rather than already known ones. in the present work, we have applied the notion of granger causality and statistical hypothesis testing to estimate causal relationships between different phosphoproteins using time-series data. according to granger's definition of causality, it is said that signal causes signal, if future values of can be better predicted using the past values of and than only using the past of itself [cit] ."
"as shown in fig. 5(b), the modular gripper design provides a guide pin to assist in alignment of gripper during installation of blind-mate connector, and locks into place with two captive screws located on the perpendicular plane. table i presents the pinout of the 10-pin blind-mate connector, which provides power and data line for one servo motor, one dc motor, and multiple arm motors."
"the gripper detachment force from a handrail is calibrated so that crew are unlikely to dislodge a perched astrobee through casually bumping it, yet it is easy to intentionally remove an astrobee that is blocking crew movement in an emergency. the ideal force range is approximately 1-5 lbf (4.45-22.2 n). we experimentally measured the detachment force with the prototype arm by pulling a perched astrobee directly away from a wall while grasping a handrail. the mean detachment force from 5 trials was 6.43 n, within the ideal range. we also note this value is five times greater than the astrobee's maximum thrust capability; there is little risk of astrobee detaching itself during its grasp check, assuming successful envelopment. fig. 7 shows the perching arm operating on astrobee prototype 4 [cit] . the air bearing was running such that astrobee was free to drift on the granite table, but the propulsion system was turned off for this test of the perching arm. the test operator initially held the astrobee robot steady pan -45° pan 90° pan 0° fig. 7 . snapshot of testing the pan motion on micro-gravity simulating surface."
"the arm controller board commands the 2-dof arm at 1000 hz. for operator situation awareness, it sends a feedback packet to the arm driver on the mlp at 1 hz. this packet includes arm motor current, velocity, position, and temperature."
"due to the fact that intracellular networks are not static, we use time series data in order to determine these dynamic changes in the network topology. in the present work, we use a vector autoregressive (var) model to infer relationships of granger causality among phosphoproteins by analyzing the time-varying fold changes of phosphoproteins in response to single and double ligand stimuli. the quantitative levels of phosphoproteins were measured through western blot experiments by the alliance for cellular signaling (afcs) [cit] in raw 264.7 macrophage cells. we infer the topology of the phosphoprotein networks in three distinct time intervals."
"the control algorithm of astrobee perching arm is presented in algorithm 1, which runs at 1000 hz to receive the command from the mlp, to control 2 arm motors and 1 gripper dc motor, and to send a feedback packet to the mlp at 1 hz. the boolean flag control loop time becomes true when the internal timer interrupt is reached at every 1000 hz in order to synchronize the control loop cycle. if the controller receives any command from the mlp, it sends the command to two arm motors and updates the gripper motor command. the user is capable of enabling/disabling motor torque, updating goal position and velocity, and updating control parameters including pid gains."
"the optimal order of the var model can be found through approaches such as minimum description length [cit] which requires many samples in time. in the present work, since there are only three original samples in time, we consider the following first order var model:"
"the astrobee perching arm is designed to support manipulation research. for example, it would be easy to add a grasping fixture to one robot and grasp it with the perching arm of another robot. the gripper is also designed as replaceable modular component-the existing 2-dof arm could accommodate a new end-effector, or even a multiple-dof \"extension arm\", controlled via the daisy chained rs485 bus."
"we have investigated various types of gripper [cit] -suction [cit], electro-adhesion [cit], microspine [cit], geckoadhesion [cit], and underactuated grippers [cit] . each type was compared using the following criteria: size, mass, compliance, grasping force, precision, and actuation time. after a trade study, suction, electro-adhesion, microspine, and gecko-adhesion gripper were rejected. the suction gripper requires a bulky vacuum pump, and the size of the state-of-art electro-adhesion gripper (grabit 1832 gripper) is not suitable to be attached at the astrobee perching arm. the microspine would not work on smooth surfaces like iss handrails, and the gecko-adhesion gripper requires an additional mechanism to attach/detach and is sensitive to temperature and humidity. in the end, we selected an open source tendon-driven gripper design [cit] and modified it slightly. this design offers high technical maturity, small form factor with a single actuator, compliant contact with the surface, and high grip strength through envelopment. fig. 5 shows the closed and opened configurations of the gripper, which consists of gripper tendons, torsional springs, and a 1-dof gripper (pololu dc) motor. the gripper uses torsional springs for joint flexion and an actuated tendon for extension. this allows grasping force to be maintained even with the motor turned off. it also allows external forces to open the gripper by overcoming spring torques, rather than having to back-drive the motor. furthermore, independent flexion torques at the gripper proximal and gripper distal joints provide passive compliance to the shape of the grasped object; the perching procedure is thus robust to positioning errors with respect to the handrail. silicone foam is attached on the inner surface of gripper to increase the contact friction between iss handrail and the gripper surface. a total of 3 torsional springs (2 at the gripper proximal joint and 1 at the gripper distal joint) are used at each gripper joint to produce a grasping force. the spring coefficients of the torsional springs are maximized based on the relationship between the maximum angle of deflection for each spring and the stall torque and spool of gripper motor, where a factor of safety of 1.2 is used to account for friction and avoid over-stress on the gripper motor. when the iss handrail is grasped as shown in fig. 5(a), the gripping forces at the gripper proximal joint and the gripper distal joint are 3.47 n and 2.87 n, respectively. when the gripper is fully opened as shown in fig. 5(b), the gripper proximal joint makes a 45.0"
"we have applied the notion of granger causality through the vector autoregressive model to develop a novel framework for reconstructing dynamic networks from large-scale multi-experiment multivariate high-throughput data sets. we used an approach based on a linear-model template and statistical hypothesis testing ( -test) of the coefficients of the model to find significant or potentially causal connections. we have applied this methodology to phosphoprotein time-course data generated by the alliance for cellular signaling (afcs) in raw 264.7 macrophage cells in single and double ligand experiments. we were able to predict connectivity, causality and dynamics of information flow in the progression of the phosphoprotein network. we also found that the reconstructed network based on only single ligand data has higher type i and type ii error as compared to using both single-and double-ligand data."
"var allows identification of granger causality for linear relationships. in order to find causal relationships, we analyze the elements of matrix . an important outcome of this approach is that the series is not the cause of if and only if the entry of matrix is zero, . therefore, it is sufficient to estimate the autoregressive coefficient matrix of the var model in order to identify the direction of granger causality."
"many of the connections found using our approach (underlying network, fig. 3) were also identified using a pls-based approach [cit] .there are some differences between our network and the network obtained using the pls approach. the connections, and are found in our network (fig. 3), but not in the pls-based network. however, the connections, and are found using the pls approach, but are absent in our network."
"it can be noted that since we are considering three separate time intervals to study the temporal evolution of the network, we expect that the information provided in the time series data may differ from stage to stage. therefore, a causal relationship that exists at an earlier stage need not exist at the following stage, i.e., the past value of a may no longer contribute to predicting the future value of b at the following stage. thus, according to granger's definition of causality, there will be no causal relationship at the following stage. this implies that the weights of edges (resulting in fluxes through connections) change though time. for example, if the weight of a connection decreases and the corresponding -value becomes more than the threshold of 0.01 (for a confidence interval of 99%), we no longer consider that connection to exist as a strong causal relationship even though we may observe the connection in the underlying network."
"in order to minimize the norm of r n, it is possible to find a polynomial p n which can minimize the equation (1)."
"the main concern of preconditioned krylov methods is the cost of per iteration, because of the global communication and synchronization overheads. in order to evaluate the performance of ucgle method on both cpu and gpu clusters, we evaluate its strong scalability comparing with the classic and preconditioned gmres by the average time cost per iteration. the test matrix is meg1. the average time cost for these methods is computed by a fixed number of iterations. time per iteration is suitable for demonstrating scaling behavior."
"we have selected four different matrices to evaluate ucgle method. the matrix matline is a ml type matrix, and matblock is a mb type matrix, they are both generated by the industrial matrix utm300 which can be downloaded from the matrix market. the distribution of eigenvalues in the complex plane has an important impact on the convergence of linear systems. we have selected two scientific matrices with known eigenvalues: meg1 and meg2. the eigenvalues of the matrix meg1 and meg2 have different eigenvalues distribution in the complex plane. the table 2 gives the details of these test matrices."
"in this section, we evaluate the acceleration of convergence and the scaling performance of ucgle method comparing with selected classic preconditioners using the four selected large-scale matrices on both cpu and gpu platforms."
"the main characteristic of ucgle method is its asynchronous communication. but the synchronous communication takes place inside of gmres and eram components. distributed and parallel communication involves different types of exchange data, such as vectors, scalar tables, and signals among different components. when the data are sent and received in a distributed way, it is essential to ensure the consistency of data. in our case, we choose to introduce an intermediate node as a proxy to carry out only several types of exchanges, and thus facilitate the implementation of asynchronous communication. this proxy is called manager process as in figure 2 . one process can fulfill all the data exchanges."
"there are three levels of parallelisms in ucgle method to explore the hierarchical computing architectures. the convergence acceleration of ucgle method is similar with a deflated preconditioner. the difference between them is that the improvement of the former one is intrinsic to the methods. it means that in the deflated preconditioning methods, for each time of preconditioning, the solving procedure should stop and wait for the temporary preconditioning procedure. asynchronous communication of the latter can cover the synchronous communication overhead."
"inspired by the unite and conquer approach, this paper introduces a recent development of unite and conquer method to solve large-scale non-hermitian sparse linear systems. this method comprises three computation components: eram component, gmres component and ls (least squares) component. gmres component is used to solve the systems, ls component and eram component serve as the preconditioning part. the key feature of this hybrid method is the asynchronous communication among these three components, which reduces the number of overall synchronization points and minimizes the global communication. this method is called unite and conquer gmres/ls-eram (ucgle) method."
"for the evaluation of ucgle method on the homogeneous cluster, the four computing components are all implemented on the cpus, the core number of gmres component is set respectively to the computing resource number of classic and preconditioned gmres keeps always the same with the core number of gmres component in ucgle method, thus it ranges from 1 to 256 for cpu performance evaluation, and from 2 to 128 for gpu performance evaluation."
"in the experimentations, firstly we can find that ucgle method is able to take advantages of the gpu accelerators which has almost 4 times speed up for the time cost per iteration comparing with the homogenous cluster without accelerators."
"we evaluate the convergence acceleration of four large-scale matrices matline, matblock, meg1, meg2 using different methods: 1) ucgle, 2) restarted gmres without preconditioning, 3) restarted gmres with sor preconditioner, 4) restarted gmres with jacobi preconditioner. we select the jacobi and sor preconditioners for the experimentations because they two are well implemented in parallel by petsc. the gmres restarted parameter for matline, matblock, meg1, meg2 are respectively 250, 280, 30 and 40. figure 5 compares the convergence curves of experimentation, and the table 3 gives the convergence steps of each method with 4 test matrices in details. we find that ucgle method has spectacular acceleration on the convergence rate comparing these conventional preconditioners. it has almost two times of acceleration for matline, matblock and meg1 matrices, and more than 10 times of acceleration for meg2 than the conventional preconditioner sor. the sor preconditioner is already much better than the jacobi preconditioner for the test matrices."
"ucgle method has been tested with different matrices, both industrial and generated. our purpose is to test this algorithm on large sparse linear systems. we have successfully evaluated ucgle with a number of sparse matrices from matrix market. however, these matrices are small compared to the desired sizes. thus we proposed a matrix generator to create several large-scale linear systems. additionally, the speedup of ucgle method depends on the spectrum figure 4 : global synchronous communication evaluation by parallel sum and dot product operations on romeo; x-axis refers respectively to the cpu core number from 1 to 256 and the gpu core number from 2 to 128; y-axis refers to the operation time; a base 10 logarithmic scale is used for y-axis and a base 2 logarithmic scale is used for x-axis."
"for the generation of matrix ml, several parallel lines with different values can be added to the off-diagonal. the good selection and meg2 by ucgle, classic gmres, jacobi preconditoned gmres, sor preconditioned gmres, ucgle_ft(g) and ucgle_ft(e); x-axis refers to the iteration step for each method; y-axis refers to the residual, a base 10 logarithmic scale is used for y-axis; gmres restarted parameters for matline, matblock, meg1, meg2 are respectively 250, 280, 30 and 40; eram fault points are respectively 500, 560, 60,30, and gmres fault points are 600, 700, 70 and 48. of added values can prevent the generated matrix to converge fast with the basic iterative solvers. the way to generate the mb type matrix is much easier, the original matrix is copied on the diagonal and the first block column matrix as shown in the figure 3 ."
"communication and synchronization points. the communication overhead makes the sor preconditioned gmres much more easily lose its good scalability with the augmentation of computing unit number. there isn't much difference between the time cost per iteration of classic gmres and ucgle. this phenomenon is caused by the asynchronous communication of ucgle method. since the resolving part and preconditioning part of ucgle work independently, the global communication and synchronize points of ucgle is similar with the classic gmres for each time of preconditioning. that's the benefits of ucgle and its asynchronous communication."
"obviously, the asynchronous communication among the different computation components improves the fault tolerance and the reusability of this method. the three computation components work independently from each other, when errors occur inside of eram component, gmres component or ls component, ucgle can continue to work as a normal restarted gmres method to solve the problems. in fact, the materials for accelerating the convergence are the eigenvalues. with the help of asynchronous communication, we can select to save the computed eigenvalues by eram method into a local file and reuse it for the other solving procedures with the same matrix."
"we developed this parallel sparse matrix generator based on mpi and petsc, which reads an industrial matrix of matrix market collection as an initial one to build larger ones. this generator allows building a new matrix a by performing several copies of a same small unsymmetrical matrix b onto the diagonal. in order to keep the generated matrices being unsymmetrical and especially non-block in diagonal, we propose two different strategies to add the values on the off-diagonal of a, as shown in the figure 3, the first one is called ml type matrix, and the second is called mb type matrix. the reason of adding different values on the off-diagonal is to ensure that the eigenvalues of newly generated matrix won't be the same as the original one, and the convergence rate won't be too fast."
"ucgle method is a combination of three different methods, there are a number of parameters, which have impacts on its convergence rate. we summarize these different related ones, and classify them according to their relations with different components. the algorithm 6 shows the implementation of ucgle's three components and their asynchronous communication in detail. eram component loads the parameters m a, v, r, ϵ a and the operator matrix a, then launches eram function. when it receives a new vector x _t mp from gmres component, this vector will be stored in eram component. this vector is updated with the continuous receiving of a new one from gmres component. if the r eigenvalues λ r are approximated by eram component, it will send them to ls component, at the same time, it is able to save the eigenvalues into the local file."
"various parameters have an impact on the convergence. thus, an auto-tuning is required in the future work, where the systems can select different krylov subspace dimensions, numbers of eigenvalues to be computed, degrees of least squares polynomial according to different linear systems and cluster architectures."
"nowadays, the hpc cluster systems continue not only to scale up in compute node and central processing unit (cpu) core count, but also the increase of components heterogeneity with the introduction of the graphics processing unit (gpu) and other accelerators. this trend causes the transition to multi-and many cores inside of computing nodes which communicate explicitly through fast interconnection networks. these hierarchical supercomputers can be seen as the intersection of distributed and parallel computing. indeed, with a large number of cores, the communication of overall reduction and global synchronization of applications are the bottleneck. when solving a large-scale problem on parallel architectures with preconditioned krylov methods, the cost per iteration of the method becomes the most significant concern, typically because of communication and synchronization overheads [cit] . consequently, large scalar products, overall synchronization, and other operations involving communication among all cores have to be avoided. the numerical applications should be optimized for more local communication and less global communication. to benefit the full computational power of such hierarchical systems, it is central to explore novel parallel methods and models for the solving of linear systems. these methods should not only accelerate the convergence but also have the abilities to adapt to multi-grain, multi-level memory, to improve the fault tolerance, reduce synchronization and promote asynchronization. the conventional preconditioners have much additional global communication and sparse matrixvector products. they will lose their advantages on the large-scale hierarchical platforms."
"the failure of gmres component is simulated by setting the allowed iteration number of gmres algorithm to be much smaller than the needed iteration number for convergence. the values of these four cases are respectively 600, 700, 70 and 48. they are also marked in figure 5 . in this figure, after the quitting of gmres component without the finish of its task, eram computing units will automatically take over the position of gmres component. the new gmres resolving procedure will use the temporary solution x m as a new restarted initial vector received asynchronously from the previous restart procedure of gmres component before its failure. in this case, eram component no longer exists, thus the resolving task can be continued as the classic gmres without preconditioning. in figure 5, the ucgle_ft(e) curves of four experimentations give the simulation of this case. we can find there's the difference between ucgle_ft(e) and ucgle_ft(g). in ucgle_ft(g), the new gmres component takes x m of previous restart procedure, thus it will repeat the iteration steps of previous restart iterations until the failure of gmres. another fact of ucgle_ft(g) which cannot be concluded from figure 5, but can be easily obtained, is that the resolving time will be different if the computing units numbers of previous gmres and eram components are different."
"in this paper, we have presented a distributed and parallel method ucgle for solving large-scale non-hermitian linear systems. this method has been implemented with asynchronous communication among different computation components. in the experimentation, we observed that ucgle method has following features: 1) it has significant acceleration for the convergence than the conventional preconditioners as sor and jacobi; 2) the spectrum of different linear systems has influence on its improvement of convergence rate; 3) it has better scalability for the very large-scale linear systems; 4) it is able to speed up using gpus; 5) it has the fault tolerance mechanism facing the failure of different computation components. we conclude that ucgle method is a good candidate for emerging large-scale computational systems because of its asynchronous communication scheme, its multi-level parallelism, its reusability and fault tolerance and its potential load balancing. the coarse grain parallelism among different computation components and the medium/fine grain parallelism inside each component can be flexibly mapped to large-scale distributed hierarchical platforms."
"asynchronous communication allows each computation component to conduct independently the work assigned to it without waiting for the input data. the asynchronous data sending and receiving operations are implemented by the non-blocking communication of message passing interface (mpi). sending takes place after the sender has completed the task assigned to it. before any prior shipment, the component checks whether several transactions are now on the way. if yes, this task will be canceled to avoid the competition of different types of sending tasks. sent data are copied into a buffer to prevent them from being modified while sending. for the asynchronous data receiving, before starting this task, the component will check if data is expected to be received. once the receiving buffer is allocated, the component performs the receiving of data while respecting the distribution of data globally according to the rank of sending processes. it is also important to validate the consistency of receiving data before any use of them by the tasks assigned to the components."
"the failure of eram component is simulated by fixing the execution loop number of eram algorithm, in this case, eram exits after a fixed number of solving procedures. we mark the eram fault points of four matrices in figure 5 : respectively 500, 560, 60 and 30 iteration step for each case. the ucgle_ft(e) curves of four experimentations show that gmres component will continue to resolve the systems without ls acceleration. the table 3 shows that the iteration number is greater than the normal ucgle method but less than the gmres method without preconditioning."
"the fault tolerance of ucgle method is also studied by the simulation of loss of either gmres or eram components. ucgle_ft(g) in figure 5 represents the fault tolerance simulation of gmres component, and ucgle_ft(e) implies the fault tolerance simulation of eram component."
"the krylov iterative methods are often used to solve large-scale linear systems and eigenvalue problems. in this section, we present in detail the three basic numerical algorithms used by ucgle."
"to be more precise: 1) if eram computing units are in fault, gmres component can continue to run as a classic gmres method without receiving 2) if gmres computing units are in fault, the fault tolerance mechanism will be more complex. in this situation, firstly the tasks of eram component will be canceled, secondly, these released computing units will be reset as a gmres component to continue the resolving procedure without acceleration. the feasibility of replacing eram by gmres is guaranteed. the required materials to retake the resolving task is the operator matrix a, the right-hand side b and the temporary solution x m . the two former ones have been loaded along with the set-up of ucgle method. the x m is ensured to be on the former eram computing units since it can be sent and received by the asynchronous communication between gmres and eram components."
"the average time cost per iteration is also used to evaluate their performance. for the performance comparison, it is necessary to keep the total computing resource number of ucgle and other methods the same. we have tested the classic and conventional preconditioned gmres with the cpu core number fixed respectively as 4, 5, 7, 11, 22, 38, 70, 140, 274 and the gpu number fixed respectively as 3, 5, 9, 20, 36, 68, 136, referring to the previous evaluation of ucgle method in section 4.6. in the evaluation on gpu cluster, the two cpus for ls component and manager component have been ignored because they have a minor influence. the performance comparison is given in figure 7 . we can find that if the computing resource number is small, the performance of classic and conventional preconditioned gmres is much better total cpu core or gpu count no preconditioner (gpu) ucgle (gpu) figure 7 : performance comparison of solve time per iteration for ucgle, gmres without preconditioner, jacobi and sor preconditioned gmres using matrix meg1 on cpu and gpu; x-axis refers respectively to the total cpu cores number or gpu number for these four methods; y-axis refers to the average execution time per iteration. a base 2 logarithmic scale is used for x-axis, and a base 10 logarithmic scale is used for y-axis."
"one important property of the asynchronous ucgle algorithm is its fault tolerance. that means, the loss of either gmres component or eram component at run time doesn't impede the whole computation."
"in section 2, we present the three basic numerical algorithms in detail which construct the computation components of ucgle method. the implementation of different levels parallelism and communication are shown in section 3. in section 4, we evaluate the convergence and performance of ucgle method with our scientific large-scale sparse matrices on top of hierarchical cpu/gpu clusters. we give the conclusions and perspectives in section 5."
we implement ucgle method based on the scientific libraries petsc and slepc for both cpu and gpu versions. we make use of these mature libraries in order to focus on the prototype of the asynchronous model instead of exploiting the optimization of codes performance inside of each component. petsc provides also different kinds of preconditioners which can be easily used to the performance comparison with ucgle method.
"p t is thus divided into several mpi groups according to a color code. the minimum number of processes that our program requires is 4. we utilize the mechanism of mpi standard to fully support the communication of our application. the communication layer that does not depend on the application, this allows the replacement and scalability of various components provided."
"in figure 6, we can find that these methods have good scalability both on cpu and gpu with the augmentation of computing units except the sor preconditioned gmres. the classic gmres has smallest time cost per iteration. the jacobi preconditioner is the simplest preconditioning form for gmres. this time cost gap between jacobi preconditioned gmres and classic gmres is not enormous. the gmres with sor preconditioner has the largest time cost per iteration since sor preconditioned gmres has the additional matrix-vector and matrix-matrix multiplication operations in each step of the iteration. these operations have global no preconditioner (gpu) ucgle (gpu) figure 6 : strong scalability test of solve time per iteration for ucgle, gmres without preconditioner, jacobi and sor preconditioned gmres using matrix meg1 on cpu and gpu; x-axis refers respectively to cpu cores of gmres from 1 to 256 and gpu number of gmres from 2 to 128; y-axis refers to the average execution time per iteration. a base 2 logarithmic scale is used for x-axis, and a base 10 logarithmic scale is used for y-axis."
"in this section, we study the potential reusability of ucgle method, which is assured by its asynchronous communication. indeed, the eigenvalues are used to improve the convergence rate of linear systems by gmres method. these eigenvalues approximated by eram component can be saved into a local file. for the next time of a different linear system with the same operator matrix, these eigenvalues can be directly reloaded from the local file by ls component and execute the preconditioning procedure. this reusability proposes also a new strategy of resolving a series of linear systems in sequence with the same matrix and different right-hand sides. this type of multiple resolving linear systems is well needed in various scientific fields. this strategy is similar to a traditional gmres method using the least squares polynomial method as a deflated preconditioner. but the ls component and gmres component communication keeps asynchronous, thus the preconditioning on the restarted gmres can be flexible, we can control the frequency of preconditioning and restart (the parameter l). in the experiments, we can propose an autotuning strategy to get an optimized l for specific linear systems. it seems if we compute a specific number of eigenvalues before the first time computation and then load them for all the resolving procedures with the same matrix, eram component will be not needed, and the existence of ucgle method will be questioned. in fact, the speedup of ucgle method depends on the quality and quantity of approximated eigenvalues which cannot always be quickly approximated. the more eigenvalues are calculated, the more accurate these eigenvalues are, the more significant the acceleration of ls preconditioning will be. the multiple solving different linear systems with the same matrix by ucgle allows the augmentation of eigenvalues number and the amelioration of these values. the reusability of ucgle method will be presented in future as the page limitation of this article."
"), which will be used for constitution of a new gmres initial vector. a, c, d are the required parameters to fix an ellipse in the plan, with a the distance between the vertex and centre, c the centre position and d the focal distance. for more details of least squares iterative method, see [cit] ."
"in experiments, we implement ucgle method on a cluster romeo. romeo is located at university of reims champagne-ardenne of france. it is a heterogeneous system made of xeon cpus and nvidia gpus, with 130 bullx r421 nodes, each node composes 2 processors intel ivy bridge 8 cores @ 2.6 ghz, 2 nvidia tesla k20x accelerators, and 32 gb ddr memory. the exact information of romeo is given in table 1 ."
"end for 9: end function arnoldi algorithm is a well-known method to approximate the eigenvalues of large sparse matrices, which was firstly proposed by w. e. [cit] . the kernel of arnoldi algorithm is the arnoldi reduction, which gives an orthonormal basis"
"from the two case studies, it is observed that optimization of gc and emission simultaneously is leading to a pareto-optimal set of solutions. the best solution from the set is found by using the reliability indices of generating stations. the final solution is considered as the reliable solution of the proposed multiobjective bilevel optimization."
"assuming that the probability in model j at time k à 1 is u j (k à 1) and that at time k is u j (k), the probability differential value of the same model at adjacent times reflects the change in the degree of matching between model j and the true motion model. the rate of change of the posterior probability in model j can be defined as"
"characteristic feature of forest resources and their site environment at different spatial and temporal scales are different, especially at spatial scale, which determines the two different classification scales have different semantics [cit] . using visual interpretation, image processing, analysis techniques and according to corresponding correlation between remote image features and land surface features, relationship of interpretation factors with their features characters like as color, shape, structure, geographical distribution and site conditions were built with the supporting information. by field survey and indoor analysis, unified understandings and interpretation standards were built by charactering each class features for remote sensing data. thus, interpretation keys of the two classification system were built as followed:"
the branch connecting buses 15 and 23 is considered to be out of service and the solution of generation rescheduling is found by considering the equality and inequality constraints. the results of considering a single table 6 . best solution from the pareto-optimal front based on afor in case-1. tables 7 and 8 . the pareto-optimal front obtained by considering both objectives at a time is shown in figure 3 . the feasible solutions of the pareto-optimal front based on the limits of gc and emission are tabulated in table 9 and the best solution of the pareto-optimal front based on afor is presented in table 10 .
"the algorithm used for optimization is shown in figure 1 . the decision variables are coded in binary form and the parameters of the genetic algorithm are presented in table 1 . in the step of checking the equality constraints, power flow equations are solved using the newton-raphson method and populations with converged solutions have higher probability to participate in crossover. later inequality constraints are checked and the penalty function is implemented for the population that violates the constraints. the pareto-optimal solutions within the limits (gc: 22600 $/h, emission: 2000 lb/h) are separated and afor is calculated for each solution."
"(1) maximum likelihood classifier maximum likelihood classifier, also known as bayesian classifier, is a commonly statistical method for supervised classification [cit] . by vesting training samples in the categories with largest index value of belonging probability, classification is carried out [cit] . when pixel i exist, its belonging probability is as followed:"
"a. pre-processing (1)geometric correction geometric correction was done based on terrain features since there seldom road turning points in mountain area [cit] . first, ridge lines and valley lines are extracted from 30 meters dem. secondly, find apparent turning points from those feature lines and the remote sensing images. thirdly, construct polynomial function to adjust images. the errors were controlled within one and a half pixels."
"with little land use information, hj-1a satellite ccd image was segmented based on color polygons, texture indexes, and distance measurement. in the view of visual judgment, the land use patches could reflect laws of land features distribution and were well conformed to human drawing result. when each forestry land use distribution was documented and mapped based on forest inventory database, the forest categories were interpreted by region grouping and classing to polygons (figure 8 ). the confusion matrix (table . ⅴ ) shows that the classification could meet the demands of forest application with the overall accuracy of 87.18%."
"finally, finger numeral representations also have an impact on arithmetic. in a recent study [cit], participants provided a verbal response to simple additions, which triggered the presentation of a correct or incorrect result displayed either as canonical configurations of fingers or as a series of rods. they answered more quickly with finger configurations than with rods, but only when the finger configurations showed the correct response. this supports the idea that, even in adults, simple arithmetic operations are still unconsciously underpinned by finger numeral representations."
"the 6s atmospheric correction model corrected by dem was a good algorithm to restore hj-1a ccd1 spectral information, which is suitable for professional application that have high visualization demanding."
"where α k, β k, γ k, η k, δ k are the emission coefficients of the k th generating station."
"this paper also classified hj-1a ccd1 and tm images using parallelepiped classification algorithm and neural network classification algorithm. however, the two classifiers took a long time, especially neural network algorithm, and both classifiers were unable to identify each surface class at the two scales."
"experiment demonstrated that hj-1a satellite ccd camera data could perform well operation on identifying information of conifer, mixed forest, broadleaf, shrubby, and others. with the supporting of distributing knowledge of those land classes, the accuracy could meet users' demands. using advanced k-nn classified method, computer can simulate well with human operation in polygon drawing, which supplies good basic vector patches for the further identification of polygons and class discrimination using hj-1a satellite ccd camera data. the laws of forest land distribution play an important role in the above procedures."
"the original k value equaled one and some texture indexes were selected as feature vectors. according with the predefinition and the region-group setup, color patches were merged into land use patches, just as the followed figure 7 :"
"by comparing and analyzing land use features at different scales on hj-1a ccd1 image and tm image with multiple classifiers, some conclusions can be attained: a) tm image has stronger resistance to noise than hj-1a ccd1 data, but the overall qualities in blue band are worse. b) both hj-1a ccd1 data and tm data have good effect in automatic classification of land use information. tm images has advantages with good spectrum information in sparse forest land and non-forest land area and is suitable for doing those surface features classification, which classification accuracy is higher than hj-1a ccd1 data. c) hj-1a ccd1 images have more abundant spectral information over vegetation area than tm images and have a higher classification accuracy, which meaning that they are more suitable to support detailed land types extraction for forest inventory. d) at first classification scale, supervised classification results for both hj-1a ccd1 data and tm data are better than unsupervised classification results. e) above all, in the situation that hj-1a ccd1 data is applied in forest information extraction, when land type area is too small and they have little difference of spectral information, supervised classification method should be used since unsupervised classification method has a low accuracy for surface features identification at a finer scale."
"this paper presented land use and forest classification with segment polygons and mathematics accuracy of hj-1 remote sensed data. 1) hj-1a satellite ccd data and tm were preprocessed using the same well algorithms of geometry correction and spectrum recover. 2) identification of land use information in different scales with supervised method and non-supervised method. 3) by comparing and analyzing classified results, the conclusions were illustrated about suitability of hj-1a ccd data for land use categories with the best scale. 4) with the supporting of forest resource distributing knowledge, classification in forest land area used hj-1a ccd1 data was done, which combined a cluster algorithm and k-nn classified method."
"considering forced outage rate (for) [cit] of generating stations is proposed in this paper, which can be used by an iso for day-ahead scheduling and preventive maintenance. a new reliability index, aggregate forced outage rate (afor), has been introduced to find the most reliable solution among the available solutions on the pareto-optimal front [cit] of the optimization curve. the advantage of using this algorithm is that the system operator can reduce the chance of outage, as more power is scheduled on a generator unit with higher reliability."
"the study area located in the hinterland area of east ni leke, where is in the northern tian shan mountain, xinjiang province. ni le-ke crosses 81°58'-84°58' of longitude and 43°25'-44°17' of latitude, which contains a state-owned forest farm named ni le-ke forest farm. from east area to west area, it is elongated shape with surrounding high mountains that high in northeast, low in southwest, valleys and terraces exist in the middle. the main land type of this area is coniferous forest with shrub as supplement. the tree species is main spruce characterized by slow growth rate and living in shady slope below 2800 meters elevation. [cit] supplied by xinjiang autonomous region forestry bureau, land types are composed by bush forest land, coniferous woodland, mixed forest land, waste mountains and land suitable for tree planting, and broad-leaf forest land, just as the graph over ni le-ke forest farm in figure 1 . the small image in fig.1 comparison blue, green, red, and near infrared bands of hj-1a ccd camera with landsat/tm, just as the detailed parameters in tableⅰ [cit] . they have three characteristics: same spatial resolution, similar wavelength range and higher time resolution. the blue band that can distinguish soil, vegetation and land use information were selected to quantitatively judge qualities of the two type remote sensing data through information, signal to noise ratio, mean value and other index, as the following tableⅱ. comparing from index values, information expression quality of hj-1a satellite ccd1 experimental data in band blue is better than tm experimental data in band blue, but its resistance to noise is lower than tm data. ⅲ. methods"
"2) minimum distance classifier according with the average distance from each pixel to training samples, minimum distance classifier, known as the spectral distance criterion rules, uses mean pixel value of each class in each band of training samples to determine which class they should belong. this classifier is suitable when each class has one representing vector."
"where s kl is the apparent power flowing in the line connecting buses k and l, and s max kl is the maximum limit of apparent power flow in the line connecting buses k and l ."
"from the literature survey, it is observed that risk evaluation with bilevel optimization is seldom used in power system operation and this paper partially fills the gap. a novel multiobjective genetic algorithm * correspondence: research.vkb@gmail.com this work is licensed under a creative commons attribution 4.0 international license."
"finally, compared from visual interpretation and measured data, result quality of the two type images at the second scale were lower than results at first scale that there existed the phenomenon of wrong identifying polygons."
"next, finger numeral representations exert their influence even when no motor outputs are required. for example, just like children (noël, 2005), adults name finger configurations faster when they conform to their own finger-counting habits than when they do not . this facilitation in the naming of canonical configurations is not a mere perceptual effect but truly reflects semantic access. indeed, numeral finger configurations used as unconsciously presented primes influence comparative judgments of arabic numeral targets: the participants respond faster to and make fewer errors with numerical than with non-numerical primes, and when the primes and targets are congruent (i.e., lead to the same response), but this priming effect generalizes to new, never-consciously seen, numerosities for canonical configurations only, not to non-canonical ones. furthermore, mere visuo-perceptual differences are not the source of the better identification of and semantic access to canonical numerical competencies may simply rely on some perceptual object-tracking system [cit], the association between numbers and space leading to a linear representation being constructed by exposure to cultural conventions [cit], such as reading-writing direction. by contrast, the very act of using fingers to represent numerosities seems quite spontaneous -what is culturally determined is the sequence in which fingers are raised -and can guide early numerical learning. in other words, a number line is most probably the best conceptual representation induced by the cultural environment, whereas finger numeral representations are the best empirical representation, which can be deduced from personal sensorymotor experience."
"supervised classification method and unsupervised classification method are used to do land cover classification over none-urban area for hj-1a ccd1 and tm datum. supervised classification used maximum likelihood classifier; mahalanobis distance classifier and minimum distance classifier, respectively. non-supervised classification used kmeans classifier. each result with these classifiers was tested by system evaluation indexes of overall accuracy and kappa index."
"an expression for the state estimation error after input interaction 16 can be obtained as from the true motion model, the state estimation error after input interaction, the expression in equation (13) can be obtained"
"firstly, at the first scale classification, classification accuracy of hj-1a ccd1 data is lower than tm data, which means tm data is more suitable for classification at larger scale under the same pre-processing. at the second classification scale, classification accuracy of hj-1a ccd1 data is higher than tm data. which means hj-1a ccd1 data is more suitable for classification at small scale."
"let the probability that the target motion model i transitions from model j at time k à 1 be p ij (k à 1), and update the markov model transition probability with du j (k). then the expression in equation (21) is obtained"
"(3) mahalanobis distance classifier difference of this classifier with minimum distance classifier is their covariance. assuming that input bands follow normal distributions, this classifier considers sample correlation and is applicable for classification that has to care about statistical indicators. the main formula is as followed:"
"supervised classification and unsupervised classification base on spectral information, and make land use type discrimination using different gray clustering algorithm. so, both algorithms are difficult to directly make accurate classification results for the reason of error accumulation combined by the algorithms' limitation and pre-processing operation errors. how to further improve the ability to identify surface features of hj-1a ccd1 data over non-forest land area, and enhance the accuracy for classification of hj-1a ccd1 images at small spatial scale to meet the demands of visual interpretation and quantitative analysis is a future work."
"the wimm algorithm improves the imm algorithm in two ways. first, a weighted sum is performed on the means of the residual errors, and the model probabilistic likelihood function is reconstructed, thus increasing the probability of identifying the true motion model. second, the model transition probability is updated for self-adaptation using the model posterior probability, thus accelerating model switching as well as increasing the model identification rate. figure 3 shows a schematic diagram of the wimm algorithm. this algorithm consists of the following five steps: input interaction, kalman filtering, model probability update, model transition probability self-adaptation, and output fusion."
"the simulation parameters are selected as follows: the noise covariance in each model during the estima- simulation results and analysis during the perceptible period figure 7 shows that during the period when the aircraft is perceptible, the maximum position error using the imm algorithm is 2.13 m and that using the rmimm algorithm is 1.30 m, but when using the proposed algorithm, the maximum position error is only 0.42 m. figure 8 shows that the maximum rmse error using the imm algorithm is 0.059 m/s and that using the rmimm algorithm is 0.356 m/s, but when using the wimm algorithm, the maximum rmse error is only 0.023 m/s. as can be seen from these results, tracking precision is greatly improved when using the wimm algorithm. figure 9 (a) shows the imm algorithm, figure 9 (b) shows the rmimm algorithm, and figure 9 (c) shows the wimm algorithm and gives the selection probability curves of the cv, ca, and ct models. figure 9 demonstrates that the imm algorithm cannot identify each motion model clearly and that the intersection points of the three selection probability curves are close together. moreover, switching among motion models is slowed down, and flexibility is also reduced. the rmimm algorithm is more advantageous than the imm algorithm, yet still needs to be improved. the wimm algorithm, however, can solve the problems encountered by the first two algorithms, not only facilitating estimation of aircraft motion state using observed data but also effectively increasing the degree of model identification. table 1 shows a comparison of the highest identification rate of each motion model and the model switching time among the three algorithms."
"because of the uncertainty of the true motion model of a surface target, quantization of f t cannot be performed, and therefore the mean of the residual errors cannot be obtained. in this article, each model in the motion model set is regarded as the real motion model of the moving object, and a weighted sum is calculated using the posterior probability of each model. the expression in equation (18) can be obtained by summing the means of the residual errors"
"we believe and argue that finger numeral representations are more than just another way of mentally representing numerosities. firstly, they possess almost all the properties presented separately by the other representations (i.e., visual, verbal, and analog). although they are optimal only for small numerosities and they are not linked to a written notation, they possess simultaneously iconic (i.e., features shared with the referent), symbolic (i.e., conventional meaning shared with other individuals), computational (i.e., used to support calculation procedures), and communicative (i.e., used to communicate numerosities through gestures with other individuals whatever their language) properties. secondly, and most importantly, all these properties rely on perceptual and sensory-motor processes that provide a non-arbitrary link between the symbols (here, finger configurations) and reality (here, numerosity), and that can be spontaneously self-experienced by every human child and adult. in contrast, other representations only possess some of these properties and they cannot be inferred and acquired without external influence. visual and verbal representations serve a communicative purpose because they are shared among individuals, but they possess no numerical meaning and very little can be inferred from them by the cognitive system as they stand for symbolic notations (respectively, verbal and arabic numerals) composed of totally arbitrary symbols. for example, \"6\" and \"six\" can unambiguously be communicated and understood, but no numerical meaning can be inferred from either their physical traits, or their mental representation. an analog number line can easily represent continuous and large numerical quantities and their ratio, but it cannot easily serve the purpose of accurate communication. moreover, except in very few people who explicitly develop a spatio-linear representation of numbers (galton, 1880; [cit] ), there is no evidence of a spontaneously self-practiced linear medium underlying and guiding early numerical learning. rather, early configurations. when participants have to decide whether a canonical configuration is present among a set of distractors expressing the same numerosity in a non-canonical way, the time to detect the presence of the target grows linearly with the number of distractors showing that canonical targets enjoy no perceptual saliency (i.e., no pop-out effect; [cit] ). most interestingly, a recent study shows that canonical configurations are processed in the same way as other symbolic notations . when participants named arabic and verbal numerals primed by canonical and non-canonical finger numeral configurations, canonical configurations primed target numbers to which they were close, whether they were smaller or larger than the target, with the extent of activation being inversely proportional to the distance between the prime and the target. this results in a v-shaped pattern of priming, supporting the idea that canonical configurations, although not supported by a written system, activate representations with the same properties as those activated by verbal or arabic numerals (i.e., a placecoding representation; [cit] ) ."
"in general, moving targets in airport scenes include vehicles and aircraft, which are ferromagnetic objects that detect the position of non-cooperative targets based on magnetically induced anisotropic magnetoresistance geomagnetic sensors. 20 by using a wireless geomagnetic sensor network, non-cooperative motion targets can be effectively detected with high precision, small volume, low cost, no wiring, and deployment flexibility. due to the large surveillance area, users of the geomagnetic technology-based surveillance scheme must consider the method of deployment and the number of geomagnetic detection nodes to reduce tracking cost and communication redundancy. it is known that with a given set of restrictions on a moving target, the target motion characteristics can be predicted in various airport areas. a new deployment mode for nodes with more space between them has been designed in combination with the characteristics of the moving target and prior information on the airport surface, as shown in figure 1 (taking a taxiway section as an example)."
"the results of only gc minimization are presented in table 3 . comparing with the emission minimization results shown in table 4, it is observed that minimizing gc results in cost of 21639 $/h but emission is 2235.7 lb/ [cit] lb/h. similarly, minimizing emission results in emission of 1649.3 lb/h and cost of 22750 $/h, which is higher than the considered limit of 22600 $/h. thus, the optimization of a single objective without considering the other is leading to a nonoptimal solution."
"after extracting categories from the first scale system and selecting certain samples, supervised classification and unsupervised classification were carried over all areas of hj-1a ccd1 data and tm data. based on the survey results of forest land type distribution of forest inventory data, forest land area were clipped out and the samples were selected form each type polygon including forest land, sparse forest land, shrubbery land, barren hills and wasteland suitable for planting. then, the second scale classification was done with each classifier. in this paper, the first level classification results of maximum likelihood classifier, mahalanobis distance classifier; minimum distance classifier and k-means classifier are showed in figure 4 and figure 5 . on the one hand, fragmentation of classification results for ccd image is greater than tm image, and the ability to identify surface features of the former is stronger than the latter over forest distribution area. on the other hand, 1) aggregation degree of tm image recognition is higher. 2) results of minimum distance classifier for spruce were better. 3) each classifier to tm data had phenomenon of over-classification that spruce forest spread to sunny slope from shady slope. 4) the maximum classifier was better for spruce, linear land features, and snow covering land. after remote sensing image classification, results must be checked by quantitative accuracy analysis. through analysis of accuracy, classification models effectiveness and qualities can be determined. this paper calculated the overall accuracies and kappa coefficients by the error matrix to evaluate the different sensor images' results with different classifier. overall accuracy, representing the total accuracy level of classification results, was computed by the division that correctly classified pixel number to sum pixel of surface real classes. kappa coefficient was calculated by kappa coefficient function by the whole error matrix and assessed classification accuracy from a more comprehensive perspective [cit] . figure 6 compared different remote sensing images' results by classification methods at different scales using overall accuracy and kappa coefficient. for attaining directly comparison with hj-1a ccd1 data and tm data, no more merge, eliminate and dissolve operation were done."
"unsupervised classification is more automated than supervised classification. the application of this method does not require users' knowledge for study area, but only few parameters setup by users to carry out auto-recognition. unsupervised classification reduces probability of errors and influence made by subjective factors. by default, the unsupervised classification judge pixels with the same or similar spectral characteristics which should belong to one feature class. besides, different surface features may have different spectral information. these limitations determine that the results of classification may not satisfy some users' needs. that is, same land covering types may be categorized into different classes in different images at different periods. and, this method usually results in poor continuity of segmented polygons [cit] ."
"ⅰ. build training sample dataset x ⅱ. set up the original value of k ⅲ. select k samples from training samples, which are near by target sample. supposed that the sample point x belongs to space r n with n dimensions, and then the nearest samples are measured by euclidian distance. set the ith sample is, which represents the lth feature value of the ith sample. the euclidian distance between and is defined by:"
"simulation results of trajectory prediction during the period when the aircraft is not perceptible are displayed in figures 10 and 11 . figure 11 . position error curve. figure 10 illustrates that as aircraft operation time increases, the error between the positions predicted using the imm, rmimm, and wimm algorithms and the actual position increases. however, the predicted position is closest to the actual position when using the wimm algorithm. figure 11 illustrates that at the last moment of position prediction, the prediction error has accumulated to 10.32 m when the imm algorithm is used and 5.72 m when rmimm is used, but only 2.16 m with the wimm algorithm. it is apparent that the wimm algorithm outperforms the imm and rmimm algorithms in terms of extrapolated trajectory prediction, particularly during the period when the aircraft is not perceptible."
"assuming that ds i is the offset of the ith corresponding feature point of signals 1 and 2 in the sampling sequence and that the sampling frequency of the two sensors is f s, the estimation formula for the instantaneous velocity v(t i ) can be given as"
"the remainder of the paper is organized as follows. section 2 focuses on the problem formulation. section 3 addresses the constraints that should be considered while solving the optimization problem. section 4 describes the proposed methodology to solve the multiobjective optimization problem. section 5 discusses the results of simulation and, finally, section 6 presents the conclusions and contributions of the paper."
the k nearest neighbors classified method bases one clustering algorithm to select learning sample from the result dataset of cluster and then constructs classifier to classify the whole dataset [cit] . it is mainly concluded the following procedures:
"where f k is the forced outage rate of the k th generating station. the qualitative meaning of eq. (3) is to translate the unavailability of each mw of power scheduled on a particular generating station to a per unit value with respect to total power generated. it is formulated with the assumption that each generating station has one unit. if a generating station has multiple units, the probability of outage should be taken from the capacity outage probability table."
"where k à 1, k represent sampling times, x is the state vector, f is the state transition matrix, z is the observed values, h is the measurement matrix, w is motion process noise that obeys a gaussian distribution with mean 0 and variance q, and v is observation noise that obeys a gaussian distribution with mean 0 and variance r."
"recent findings show that finger numeral representations possess many characteristics of the other numerical representations postulated in classical cognitive number architectures. among others, they, like other symbolic notations, are shared by individuals of the same cultural group, they can be used to communicate numerosities and to calculate, they possess iconic properties preserving cardinality, and place-coding properties. most importantly, they have specific sensorymotor properties preserving numerical properties and allowing mathematical principles to be inferred and experienced. thus, they are not just a way of mentally representing (in the sense of \"standing for\") numerosities as other representations do; they represent and, at the same time, can help to build or, at least, improve the concept of number. we do not intend to claim that finger numeral representations replace all other representations, or that without finger-counting activities, human beings could not develop an accurate concept of number. but fingercounting/montring activities, especially if practiced at an early age, can contribute to a fast and deep understanding of number concepts, which has an impact during the entire cycle of life by providing the sensory-motor roots onto which the number concept grows."
"the paper proposes a novel multiobjective bilevel generation rescheduling algorithm considering the reliability of generating stations. the primary objectives considered are generation cost minimization and emission minimization. as the two primary objectives conflict with each other, a set of pareto-optimal solutions are obtained by modified spea. the best solution from the pareto-optimal set is found by evaluating afor for each solution. the proposed method has been validated on the ieee 30-bus test system and the results obtained for multiobjective bilevel optimization are globally optimal when compared with the results of single-objective optimization. this method can be used by an iso for finding the generation schedule for day-ahead scheduling table 10 . best solution from the pareto-optimal front based on afor in case-2. time of computation is 32.28 min and preventive maintenance. the time of computation can be reduced by considering a smaller population, but it may lead to a local optimum. the proposed algorithm can be used for transmission congestion management by reducing the time of computation when the decision needs to be made in less time."
"hj-1a satellite ccd data have affluent gradations and good readability over mountain area. the geometry correction method using ridge and gorge polylines is suitable for hj-1a images with low errors in mosaic obvious landmarks. the procedure of this paper was proved to be efficient in interpretation of land use information, which uses spectrum and its texture information as the first step and history inventory database as the auxiliary data for the follow-up steps."
"markov model transition probability self-adaptation. in the imm algorithm, the input interaction process is generally a markov process, and the initial transition probability of the model is determined by humans and does not change with actual motion conditions. when the deviation between the actual motion of the target and the initial transition probability is large, the artificially determined markov matrix cannot accurately reflect the actual motion mode transition of the target due to the distortion of the situation. in view of this, the article uses the runway and taxiway in the airport. a priori information such as the probability of running is corrected in real time to make it fit the actual motion state of the target. therefore, the markov model transition probability p ij is updated using the posterior information in the wimm algorithm to solve this problem."
the target's motion model is generally expressed as a dynamic model and a measurement model. 21 the dynamic and measurement models can be expressed as
"owing to the necessity of finding an optimal solution with respect to both the objectives, the nondominated set [cit] obtained using the flow chart ( figure 1 ) is shown in figure 2 . instead of optimizing all three objectives at a time, reliability is given preference next to financial feasibility and environmental concern. after obtaining the pareto-optimal front, afor is computed for each solution of the pareto-optimal front and the solution with the least afor is taken as the final solution. the solutions on the pareto-optimal front within limits of cost and emission are presented in table 5 with scheduled powers in mw, cost in $/h, and emission in lb/h. the best solution based on reliability is shown in table 6 . the obtained cost is 22586.02 $/h and the emission is 1673.89 lb/h, which are within the limits of 22600 $/ [cit] lb/h, respectively."
"the interpretation keys used by supervised classification were established at two scales that the first and the second classification scale system. according to \"land use classification\" issued jointly by general administration of quality supervision, inspection and quarantine of the people's republic of china and standardization administration of the people's republic of china, and combined with nileke forest survey contents, the first scale classification types include forest land, grassland, and arable land. in addition, as there are some snow areas in remote satellite images and fully identification can not be done. so, the first classification system also includes snow covering land. the second scale classification system contains forest land, barren hills and wasteland but suitable for planting, spares forest land, and shrub forest land according to identifiable classes and type settings of forest resources survey project on the land type investigation."
"the considered primary objectives of generation rescheduling are minimization of generation cost ($/h) and minimization of emission (lb/h). as these two objectives conflict with each other, simultaneous optimization of both the objectives leads to a pareto-optimal set of solutions and one among them is chosen with higher knowledge. the different objectives and constraints that are considered for multiobjective optimization are presented as follows."
"this article proposes a distributed non-cooperative scene target monitoring scheme based on geomagnetic sensing technology. in addition, based on the wimm algorithm, the trajectory tracking and prediction algorithm for moving objects in airport scenes is derived. the algorithm improves the imm algorithm by calculating the weighted sum based on the mean of the residuals and reconstructing the model probability likelihood function. the model transition probability is updated for self-adaptation with posterior information, thus accelerating model switching as well as increasing the model identification rate. finally, this article presents simulation results for target tracking and prediction both when a target is perceptible and when it is not perceptible using two algorithms. from simulation results, both from periods when the aircraft is perceptible and when it is imperceptible, it is apparent that the wimm algorithm is superior to the conventional imm and rmimm algorithms in terms of trajectory tracking and prediction, especially when the aircraft is imperceptible."
"k, e k are the cost coefficients of the k th generating station and p k is the scheduled power of the k th generating station."
"where p min gk is the minimum active power limit and p max gk is the maximum active power limit of the k th generating station, and q min gk is the minimum reactive power limit and q max gk is the maximum reactive power limit of the k th generating station."
"besides these developmental and cultural pieces of evidence, recent findings in adults show that finger counting shapes number processing and calculation throughout life, triple code framework. but is that all? are finger numeral representations nothing but another way of representing numbers, mainly small ones, mentally? what, after all, makes them so special for numerical cognition?"
"as an advanced land observation system equipped with kinds of remote sensor, such as wide coverage ccd camera, infrared camera, high spectral photographer, synthetic aperture radar (sar), hj-1 constellation is composed of those small remote sensors for monitoring environment or forecasting disaster. currently, it is the almost complicated and advanced system among domestic civil satellite systems. since hj-1 optical satellites were launched, they have been playing important roles with scientific foundation in forecasting change trends of ecological environment and disaster, fast disasters estimate, emergency rescue, postdisaster help and reconstruction. as a new satellite system of our country with middle and high resolution, hj-1 remote sensing images can offset some limits of ground measurement and have great potentiality in water, atmosphere and ecological environment [cit] owing to higher spectral resolution and time resolution than landsat tm. basing on hj-1 satellites can observe dynamic forest resource with high speed and wide coverage, the land use classification is fundamental for construction advanced environmental and disaster monitoring system and forecasting system, and also for improving the abilities of environmental monitoring and comprehensive disaster reduction."
"secondly, among classifiers of supervised classification method, maximum likelihood classifier was better than other classifiers. experiment showed that overall accuracy of tm data with this classifier at first classification system was 85.1% and kappa coefficient is 0.8. this result was considered to be well. although hj-1a ccd1 data has a lower accuracy than tm data, but comparison from each classifier, maximum likelihood classifier still the best suitable method to this type of remote sensing data. at second scale classification, the maximum likelihood classifier has the best result for hj-1a ccd1 data with overall accuracy of 85.4% and kappa coefficient of 0.74。 thirdly, comparison supervised classification method with unsupervised classification method at a certain scale, supervised classification is better than unsupervised classification. unsupervised classification' kappa coefficients of the first scale were between 0 and 0.2, which indicated there was low coincidence degree of the two type images. the second scale classification had negative results, which means classification quality is low."
"(2) spectrum recovery as the phenomenon that similar land features have different spectrum and different land features have similar spectrum, spectrum recovery must be done to get real landscape gray information. advanced 6s atmospheric correction model by dem is used to get rid of problem of dn differences in shady and sunny slope with the same vegetation [cit] . with dem, the flat landscape can be transformed into the real terrain landscape and resolve that phenomenon. origin tm image is showed as figure 2 . the spectrum recovery result is showed by hj-1a ccd1 image in figure 3 . this algorithm turned anti-visual images into normal visual images and the results was considered to be good."
"the residual sequence obeys a gaussian distribution with mean 0 and variance s j (k), and the motion model set can contain all motion models during the operation. however, due to factors like target movement uncertainty, surface restrictions, and maneuverings, the target motion model may exceed the model set in the algorithm. in addition, motion models cannot be added endlessly to the conventional imm algorithm for convenience in computation. multiple models will also tend to compete with each other, thus reducing trajectory tracking accuracy. therefore, if a motion model is not added, the model probabilistic likelihood function is reconstructed and is no longer assumed to obey a gaussian distribution with mean 0 and variance s j (k)."
"the minimization of gc resulted in a cost of 21643 $/h and an emission of 2236.8 lb/h. [cit] lb/h. similarly, emission minimization resulted in emission of 1649.6 lb/h and cost of 22756 $/h, which exceeds the limit of 22600 $/h. the cost and emission obtained by the proposed algorithm are 22591 $/h and 1674.3 lb/h, respectively, which are within the considered limits."
"ⅳ. given that a sample is to be classified, and means k samples which are relatively nearest to sample . a discrete target function (that is classifying problems) is, which is the feature keys of the ith class and label set is defined as ."
"in the system proposed here, a geomagnetic detection node is equipped with dual sensors to obtain the instantaneous speed of the target. let the distance between the two geomagnetic sensors be d. when a target passes the geomagnetic detection node, the two geomagnetic sensors have asynchronous access to the geomagnetic induction signal, the offset of which depends on the target's real-time velocity. the geomagnetic induction signal of a certain surface target obtained from a geomagnetic detection node is assumed to be as shown in figure 2 ."
"as can be seen in figure 2, induction signals 1 and 2 of the two geomagnetic sensors on the same detection node share a few common feature points, including the detection starting point onset, peak, and trough, the extreme point, and the detection ending point. assuming that the two sensors have the same parameter index, the corresponding feature points of the two induction signals can be regarded as the geomagnetic induction signal acquired when one target cross section passes the two geomagnetic sensors. the instantaneous velocity sequence of several sections can be obtained from the sampling offset of corresponding feature points, and the velocity sequence can be used as an instantaneous velocity sequence when a target passes a geomagnetic detection node."
"for this experiment, land type information and forest land type extraction results showed that tm data have a good result at large scale and hj-1a ccd1 data have a good result at small scale. supervised classification has a higher overall accuracy than non-supervised classification method. in the case of absence of prior knowledge, by input few parameters, unsupervised classification is more suitable for a practical operation. if there is a priori knowledge, analyst can manually select sample area, maximum likelihood classifier for supervised classification method is relatively a kind of better classification method."
"an aircraft passing a certain geomagnetic detection node on the taxiway is taken as an example in this research and compared with the imm, residual-mean interacting multiple model (rmimm), and wimm algorithms with regard to trajectory tracking and aircraft prediction performance in both perceptible and imperceptible states using monte carlo simulation. assume that the process of aircraft operation is as follows. first, when the aircraft is in the perceptible period: (1) 0-4.5 s for the ct model; (2) constant acceleration (ca) is performed at 0.45 m/s 2 from 4.8 to 12 s;(3) constant velocity (cv) is performed at the velocity obtained from the first two steps from 12.3 to 15 s. second, when the aircraft is in the imperceptible period: (4) the aircraft maintains cv for about 30 s at the velocity obtained from step 2 and then operates to the next detection node. figure 4 shows the actual position of the aircraft according to the operating procedure."
the secondary objective is used to filter the pareto-optimal set of solutions obtained by simultaneous optimization of both the primary objectives. the new reliability index (afor) introduced to address the need of finding the most reliable solution from the pareto-optimal set based on the for of individual generating stations is defined as:
"this paper selected k-means classifier as the class clustering algorithm of unsupervised classification. k-means classifier algorithm is an indirect clustering method based on similarity of samples. by iteration, class centers are moved successively until the best clustering results were obtained."
"after substituting the predicted state valuê x j (k + 1jk) and the predicted model probability u j (k + 1jk) of each model into equations (25) and (26), the predicted state value and the prediction filtering covariance matrix when the target is imperceptible can be defined aŝ"
"this article is organized as follows: the ''geomagnetic sensing-based surface target surveillance scheme'' section proposes a geomagnetic sensing-based surface target surveillance scheme and target instantaneous velocity acquisition method; in the ''target tracking and prediction based on the wimm algorithm'' section, based on a general target motion model, target tracking and prediction based on the wimm algorithm is proposed; in the ''simulation and analysis'' section, monte carlo simulation is employed to compare the imm with wimm algorithms based on trajectory tracking and aircraft prediction performance in both perceptible and imperceptible states."
3) select the representative data as training samples from each class by one method taking for random selection as example. 4) construct classifying algorithm using training samples. the details of this procedure are as the followed:
"given these findings in children and adults, finger numeral representations (whether they come from finger counting, finger montring, or other personal ways of using fingers to represent numerosities) [cit] and that finger numeral representations do not disappear when symbolic numerical representations develop. on the contrary, their critical impact is still observed in educated adults."
1. genomic region identification: for genes where at least one transcript was known we used genomic regions starting 50000 bases upstream of the transcript start to 50000 based downstream of the transcript end to account for potentially significantly longer transcript. we then trimmed the regions if we found a gap in read coverage of more than 100 bases that was also not overlapped by spliced reads.
"we implemented the user interface with modern web technologies, such as angular, typescript, and sass. the qa-frontend service was implemented in python with flask. it is fully configurable and allows multiple candidate ranking services to be used at the same time."
penalty term and (4) the exonic segment read count fit. we did not include parameters η1 and η2 into the model selection. we found that choices of 1.2 and 0.0 worked consistently well in all experiments.
"the central part of our proposed system is the qafrontend. this component coordinates the other services and combines them into a fully functional question answering system. since our primary goal was to provide a way to explore and compare attention-based models, we especially focused on the user interface. our ui fulfills the following requirements:"
"we simulated sequencing errors by estimating an error model based on an illumina sequencing run (hepg2 [cit] ) the error model computes a mutation probability based on read quality scores, while error positions are assumed to be independent. a set of read quality strings was sampled from the same illumina run and randomly assigned to a read. thus, we obtain a read error distribution similar to a given illumina run. this strategy is implemented in the palmapper package [cit] ."
"current trust management research in wireless networks, usually views the neighbourhood from three levels [cit] . for the neighbourhood of one node, according to the link conditions, we can classify as direct, indirect, and recommendation relationships, on behalf of node a's different neighbours' trust opinions."
"for each of the five samples we obtain approximately 2.85 million fragments for 1000 genes, corresponding to about 57 million read for the whole human genome (assuming 20,000 expressed genes). we note that this is in the same order of magnitude as the d. melanogaster data set comprising 550 million reads in total. the gene structures and simulated reads are available from the mitie website (www.bioweb. me/mitie)."
"direct trust is established through observations on whether the previous interactions between the nodes have been successful [cit] . for example, node a wants to know node b's information, for which observations from a to b are for direct trust. indirect trust can be transited through the third entities. for example, node e and f are the indirect trust nodes, which have interactions with b, but not with a. recommendation trust is a special type of trust relationships. we assume the nodes have a common node to communicate. this common node is denoted as the recommendation node. for example, nodes a and b have a common node c. if a wants to know the trust records of b from c, c will calculate the trust value of b based on the observations of interactions between b and c."
"the regions in figure 3a summarize genomic positions sharing the same composition of overlapping transcripts. if we assume to know the number of expressed transcripts in advance (3 transcripts in this case), then we know that at least five of the unknowns are equal to zero. if we randomly select 5 unknowns and set them to zero, then this results in a system of four equations and three unknowns which is either infeasible or has exactly one solution. if it is infeasible, we know that the three remaining transcripts cannot explain the read coverage. otherwise, we found one possible solution. we iterate this by setting all possible permutations of transcripts to zero and count the number of cases the corresponding system of equations has a solution."
"from the figure, the results show that taking different relationship factors will affect the total value. t 10-average is higher than t 10-3nodes and t 10-5nodes, due to not including the relationship weights. if the simulation just considers the opinions of nodes 0, 2, 3, the result will be lower than that including nodes 0, 2, 3, 4, 5 because t 15 is higher than t 13, and t 14 is higher than t 12 . figure 4 shows the trust values calculated by grey theory, and plr (packet loss rate) which is often the single metric selected by current tmf approaches such as otmf (objective trust management framework) [cit] . existing trust management schemes like otmf, often choose the probability of successful interactions as their input parameter in order to calculate their trust values. for nodes 0, 1, the simulation takes a 10-second cbr/udp traffic from node 0 to node 1. every second as an interval, the simulator calculates and records the grey trust value of node 1 from node 0, compared with the good-put trust values obtained from the packet delivery rate (pdr) of node 1. the good-put trust value obtained from the packet delivery rate is equal to 1-plr . t 10 in figure 3 is the grey trust value of node 1 from node 0 for the period of 10 seconds, while the good-put trust value of node 1 for the period of 10 seconds is 0.463517. the result clearly shows that the individual good-put sample values are very close to the good-put value for the period of 10 seconds, while the grey values have significant variation from t 10 . that means when plrs are similar, trust values also tend to be similar, by using existed trust management schemes which choose plr as the main parameter, though these schemes may process plr with various formulas or algorithms, i.e., bayesian approach. however, the new tmf described uses multiple parameters with grey theory to measure trust values. this is based on the assertion that any one node's behaviour, whether that interaction is successful or not, is affected by various factors, for example the node's signal strength, data rate, throughout, and delay. therefore, the judgement on whether any node should be trusted, is not only determined by the probability of successful interactions, but also from various parameters in the physical and mac layers.in fact, the packet loss rates of nodes 1, 2, 3 have little variance over the period, while the throughput and delay times of node 1 are changing with time, compared with those of nodes 2 and 3; this is shown in figure 5 . these changing parameters have a significant impact on the grey trust values."
"simulation results are presented that reveal the proposed framework can show clearly the difference in the trust values between a normal and selfish node on a specific parameter by setting an appropriate weight vector. in addition, the total trust value is calculated by using relation factors and weights of neighbour nodes, not just by simply taking an average value. further research will test the proposed framework in more comprehensive environments with more network alternatives and selection criteria."
"in figure 6, the tmf sets different weight values for the signal strength and other parameters. the weight of signal strength is 0.2 figure 6 (a), 0.6 for figure 6 (c), while 0.1 for figure 6 (b), (d), (e) and (f). generally, the system can find the difference in trust values between a normal node and a selfish one, when the five parameters have equal weight values, shown in figure 6(a) . then, by using other weight vector groups, it can be clearly seen that there is a very large gap between normal and selfish trust values whenever the signal strength is set as the most important factor (weight value 0.6), in figure 6 (c). this reveals that the observed node is likely to be behaving selfishly on the aspect of signal strength, due to the abnormal value in figure 6 (c)."
"analysis enabling researchers to directly relate the results of our question answering system to the answer selection component requires the absence of major negative influences from the answer retrieval component. to analyze the potential influence, we evaluated the list of retrieved candidates (size 500) for existing questions of insuranceqa and of different stackexchange dumps. questions in these datasets have associated correct answers, 3 which we treat as the ground-truth that should be included in the retrieved list of candidates. otherwise it would be impossible for the answer selection model to find the correct answer, and the results would be negatively affected. table 1 shows the number of questions with candidate lists that include at least one ground-truth answer. since the ratio is sufficiently high for all analyzed datasets (83% to 88%), we conclude that the chosen retrieval approach is a valid choice for our end-to-end question answering system."
"the efficient retrieval of answer candidates is a key component in our question answering approach. it allows us to narrow down the search space for more sophisticated, computationally expensive attentionbased answer selection approaches in the subsequent step, and enables us to retrieve answers within seconds. we index all existing candidates of the target dataset with elasticsearch, an opensource high-performance search engine. our service provides a unified interface for the retrieval of answer candidates, where we query the index with the question text using bm25 as a similarity measure."
"for example, an attacker/selfish node may make use of the knowledge that the packet loss rate is the main parameter used in trust calculation. due to this limitation, the attacker can obtain a very high trust value by just interacting with close neighbours, while dropping or abandoning communications with nodes far away. in comparison, normal behaving nodes will communicate with all neighbours (near and far). however when a normal node interacts with a far away neighbour, the packet loss rate may be higher than that of the attacker which communicates only with near neighbours. thus, if only the packet loss rate is used as the deciding parameter, it will lead to a normal node's trust value being lower than the attacker's who intends to choose partners. this leads to the conclusion that any tmf should consider multiple parameters, including those involved in the communications processes in order to avoid such duplicity."
"the candidate ranking service provides an interface to the attention-based neural network, which the researcher chose to analyze. it provides a method to rank a list of candidate answers according to a given question text. an important property is the retrieval of attention vectors from the model. these values are bundled with the top-ranked answers and are returned as a result of the service call. since our primary objective was to enable researchers to explore different attention-based approaches, we created a fully configurable and modular framework that includes different modules to train and evaluate answer selection models. the key properties of this framework are:"
we precompute the coefficients of l for values of v ranging from 1 to 30000. between the positions of this grid we linearly interpolate the coefficients.
"the success of these approaches clearly shows the importance of sophisticated attention mechanisms for effective answer selection models. however, it has also been shown that attention mechanisms can introduce certain biases that negatively influence the results [cit] . as a consequence, the creation of better attention mechanisms can improve the overall answer selection performance. to achieve this goal, researchers are required to perform in-depth analyses and comparisons of different approaches to understand what the individual models learn and how they can be improved. due to the lack of existing tool-support to aid this process, such analyses are complex and require substantial development effort. this important issue led us to creating an integrated solution that helps researchers to better understand the capabilities of different attention-based models and can aid qualitative analyses."
"a high-level view on the framework structure is shown in figure 3 . a particularly important property is the dynamic instantiation and combination of module implementations. a central configuration file is used to define all necessary options that enable to train and evaluate neural networks within our framework. an excerpt of such configuration is shown in listing 1. the first four lines describe the module import paths of the desired implementations. our framework dynamically loads and instantiates the configured modules and uses them to perform the training procedure. the remaining lines define specific configuration options to reference resource paths or to set specific neural network settings. this modular structure enables a high flexibility and provides a way to freely combine different models, training procedures, and data readers."
"other parts of the genome were segmented into regions based on a map adding per position coverage, number of spliced reads spanning a position and number of read-pairs spanning a position. whenever the value of this map exceeds a user defined threshold (default 2) relative abundance of transcripts sorted by expression. we measure the average relative abundance of transcripts in genes with at least 3 transcripts using cufflinks for quantification on modencode data (blue). in red we show the average relative transcript abundance obtained with a uniform stick breaking process."
"additionally, our framework is capable of starting a seamlessly integrated webserver that uses a configured model to rank candidate answers. since model states can be saved, it is possible to load pretrained models to avoid a lengthy training process."
"suppl. section a.1 read simulation based on the genes structure generated as described in suppl. section c, we randomly selected 1000 genes from chromosome ii with more than three transcripts. the reads were simulated using the flux simulator [cit] . the gene expression values cg (sum over transcript mrna copy numbers) for gene g was given by"
"our optimization formulation is restricted to polynomials of degree two. thus, we cannot directly employ the loss function l. instead, we utilize a approximation of by fitting the this function with two polynomial of degree two. we fit l for expected values smaller than the observed value v separately from expected values larger than v . we minimize the squared error between the quadratic proxy function and the negative log likelihood function on a grid ranging from −10 to +10 standard deviations of the negative binomial with a step size of v /500 + 0.002. we obtain four coefficients llv (left linear), lqv (left quadratic), rlv (right linear) and rqv (right quadratic). the resulting loss function can be written as:"
"attention-based neural networks are especially successful in answer selection for non-factoid ques-tions, where approaches have to deal with complex multi-sentence texts. the objective of this task is to re-rank a list of candidate answers according to a non-factoid question, where the best-ranked candidate is selected as an answer. models usually learn to generate dense vector representations for questions and candidates, where representations of a question and an associated correct answer should lie closely together within the vector space [cit] . accordingly, the ranking score can be determined with a simple similarity metric. attention in this scenario works by calculating weights for each individual segment in the input (attention vector), where segments with a higher weight should have a stronger impact on the resulting representation. [cit] ) ."
"wireless network technologies have greatly changed our daily lives by offering access to the internet anywhere and anytime. however attacks and intrusions against wireless networks, internet fraud and high-tech crimes have kept raising in recent years, and security has become a major concern for those who intend to use wireless technologies and internet services. substantial resources have been deployed to tackle this issue; firewalls, anti-virus software, encryption algorithms, and intrusion detection and prevention systems are examples of the tools to secure network applications. a networked application involves interactions among many entities, such as networking protocols from physical layer to application layer, middleware, various algorithms, etc. an essential challenge when designing a secure application is therefore to determine how one network entity can trust another network entity."
"the input parameters include: packet loss rate, signal strength, data rate, end-toend delay, and throughput. these parameters are chosen as the basic minimum set of parameters required to cover all types of attacks against lower level protocols as they can easily be obtained from mac, data link and network layer protocols."
"we keep segments that a) have more than 5% of their nucleotides covered, b) are part of annotated transcripts, or c) if the removal of segment s does not leave any path between two segments connected by paired-end reads (if available)."
1 d a t a −module: d a t a . i n s u r a n c e q a . v2 2 model−module: model . a p l s t m 3 t r a i n i n g −module: t r a i n i n g . dynamic 4 e v a l u a t i o n −module: e v a l u a t i o n . d e f a u l t 5 6 d a t a : 7 map oov: t r u e 8 e m b e d d i n g s : d a t a / g l o v e . 6 b. 1 0 0 d . t x t 9 i n s u r a n c e q a : d a t a / i n s u r a n c e q a 10
"this section presents a tmf for a pure mobile ad hoc network environment, using grey theory. the tmf is designed to be robust against attacks that are aimed to deceive the trust relationships, such as selective misbehaviour attack, on-off attack, conflicting attack, and bad mouthing attack [cit] . in wireless ad-hoc and mesh, the links between communicating nodes can be one-hop, and multi-hop, only single-hop links are considered in this paper."
"for the d. melanogaster data set we performed cufflinks predictions with default parameters. the parameter values optimized for the human artificial data are not likely to perform better in this setting, since data as well as the evaluation criterion have changed. performing the same model selection on genome wide data was computationally prohibitive."
"the trust models currently used seldom consider the influence of different nodes' viewings; moreover, they also set the weights of the opinions as fixed values, usually average values. this means the important degrees of opinions about trust information from a normal node and a selfish node (or an attacker) are equal. therefore, our approach is to set the weights as changeable parameters in order to express the degree of trust of a node or nodes, based on their historical behaviour."
"the simulations were modified so that one node behaved selfishly. in operation, the node tends to be normal during the initial time, and then behaves selfishly, by only communicating with its nearby neighbours. using grey theory, the trust values are affected by the change in received signal strength, although the packet loss rate maintains the same value as with normal behaviours, this is shown in figure 4 . the main reason why the selfish node's trust value decreases is that its signal strength observed by the neighbour is increasing, leading to the drop in its signal strength grey value."
"in today's business communications systems, trust plays an important role in virtual organizations, where it is used to counter uncertainty caused by the business requirement for openness. the requirement seeks to make marketable services openly available to all potential, highly autonomous clients, which increases a service provider's vulnerability to an attack [cit] . especially in distributed environments, trust management can provide a basis for more detailed and better-informed authorization decisions, while allowing for a high level of automation. researchers want to design trust management systems in order to establish trust relationships, dynamically monitor, and adjust any existing trust relationships [cit] . in recent years, various models and algorithms for describing trust and designing trust management in distributed systems or wireless networks have been considered, such as policy language, public-key cryptography, the resurrecting duckling model, and the distributed trust model [cit] . the distributed trust models are usually applied in peer-to-peer (p2p) systems and wireless ad hoc networks; these networks rely on all participants actively contributing to network activities such as routing and packet forwarding. the particular characteristics of a wireless network's nodes, such as limited memory, battery power, and bandwidth, can provide incentives for them to act selfishly (refuse to participate in routing and provide services to other nodes, for example). trust management can help mitigate nodes' selfish behaviors and advantage the efficient utilization of network resources. recent research has considered how to evaluate the trust of communication entities in wireless networks, and various theories such as probabilistic estimation [cit], information theory [cit], fuzzy theory, and game theory have been used for designing the trust metrics [cit] . for evaluating trust values from more aspects, some researchers introduce grey theory to improve existing trust management or network performance [cit] ."
"we call a region. we join neighboring regions with a distance of less than 50 bases and finally, we discard regions with fewer than a user defined number of reads (default 50)."
"by setting the weight vectors, the trust management framework can detect more covert (intelligent) selfish behavior. for example, a node may maintain the packet loss rate at a normal level, but just cooperate with other nodes less frequently compared with normal nodes. by using the new framework, the results in figure 7"
-l 10 -l 20 -k 12 -c 30 - [cit] 00 -ni 2 -sa 100 -ct 50 -a -s -seed-hit-cancel-threshold 1000 -report-splice-sites 0.95 -filter-splice-region 5 - [cit] -qpalma-prb-offset-fix -min-spliced-segment-len 8 -report-splice-sites-top-perc 0.01
"the rationale behind this strategy is to explore the space and choose new parameter vectors that might lead to good performance (vectors with high predicted mean and high variance), but finally to select a vector with a high mean and low variance to achieve good performance with high confidence. we optimized the regularization parameters for (1) the number of transcripts (2) figure 4a in the main manuscript, but including cufflinks quantification results. we computed the pearson correlation based on the lower confidence interval reported by cufflinks, which has higher correlation to the true abundance than the estimated abundance value itself."
"we parsed the graphs and read counts for all components from files \"comp*.out\" and collapsed linear portions of the graph. we then resolved cycles by removing self loops and cutting each larger cycle at the first node on the path from an initial segment which is also part of the cycle. we then defined a total order of nodes ran the mitie optimization. for simple cases with less than 9 paths we did not run the mitie, but reported all possible paths instead."
"to transform attention-based answer selection models into end-to-end question answering systems, we rely on a service orchestration that integrates multiple independent webservices with separate responsibilities. since all services communicate using a well-defined http rest api, our system achieves strong extensibility properties. this makes it simple to replace individual services with own implementations. a high-level view on our system architecture is shown in figure 1 . for each question, we retrieve a list of candidate answers from a given dataset (candidate retrieval). we then rank these candidates with the answer selection component (candidate ranking), which integrates the attention-based neural network model that should be explored. the result contains the topranked answers and all associated attention weights, which enables us to interactively visualize the attention vectors in the user interface. our architecture is similar to the pipelined structures of earlier work in question answering that rely on a retrieval step followed by a more expensive supervised ranking approach [cit] . we primarily chose this architecture because it allows the user to directly relate the results of the system to the answer selection model. the use of more advanced components (e.g. query expansion or answer merging) would negate this possibility due to the added complexity."
"all components of our system are highly modular which allows it to be easily extended with additional functionality. for example, our modular answer retrieval component makes it simple to integrate new datasets, and our answer ranking framework allows researchers to add new models without requiring to change any other part of the application."
"in figure 3, there are the trust values of node 1 from node 0, 2, 3, 4, 5, and the total values with 3 nodes and 5 nodes, the average value t 10-average of t 10~t15 ."
"because all components in our extensible service architecture are loosely coupled, it is possible to use multiple candidate ranking services with different attention mechanisms at the same time. the user interface exploits this ability and allows researchers to interactively compare two models side-by-side within the same view. a screenshot of our ui is shown in figure 2, and an example of a side-by-side comparison is available in figure 4 ."
the service implementation is based on scala and the play framework. our implementation contains data readers that allow to index insuranceqa [cit] and all publicly available dumps of the stackexchange platform. 2 researchers can easily add new datasets by implementing a single data reader class.
"in this paper, a new trust management framework for ad-hoc wireless networks is presented. the new tmf employs multiple metrics to calculate a node's trust values rather than current approaches such as otmf that consider only one parameter. the approach also uses grey theory and fuzzy sets to improve the trust value generation algorithms. unlike other trust management frameworks, the tmf described in this paper sets a weight vector for each of the input parameters. this provides a significant new benefit for the tmf as it can detect not only selfish or anomalous behaviour, but can also help identify the type of parameters used in the strategy of the attacker or selfish node."
"the rest of this article is organized as follows: first, the classification of trust relationships is introduced, then the application of grey theory to the design of a new trust management framework is described. several simulation cases are then described that use the proposed algorithms and their performance examined. conclusions and further research are then detailed."
"we iterate by assigning a fraction of δi of the remaining mass to the i-th transcript. read length was set to 75bp, library preparation simulation parameters were chosen to be \"random priming\" and \"chemical fragmentation\"."
"in this work, we present an extensible service architecture that can transform models for non- figure 1 : a high-level view on our service architecture."
"given a genomic region, we construct splicing graphs by generating a list of segment boundaries. boundaries are either splice sites (ss), potential transcription start sites (tss) and termination sites (tts). potential ss positions can originate from spliced reads or annotated transcripts. analogously, tss and tts sites can stem from annotated transcripts or from potential transcript start and end positions inferred from rna-seq coverage. we identify possible start and end positions as a) drop of the read coverage to zero or b) steep drops in read coverage. the latter we find by applying a statistical test as follows. for each segment, we use a sliding window of length 60 and compare the number of read starts (ends) in the first half of the window to the corresponding number in the second half of the window in case of tss (tts). we apply a binomial test on the obtained counts and call a tss/tts site, if the p-value is smaller than 10 −4 ."
we merged the four human genome annotations [cit] by first creating the union of all transcripts for each gene. we then merged transcripts having identical splice structure. we consolidated minor deviations in transcript ends by using the mean on the original transcript ends in the resulting transcript. we generated a splicing graph and obtained additional transcript features by integrating evidence from two rna-seq libraries for cell lines hepg2 (wgencodecshllongrnaseqhepg2celllongnonpolyaalnrep2.bam) and k562 (wgencodecshllongrnaseqk562cellpapalnrep1.bam) from http://hgdownload.cse.ucsc.edu/goldenpath/hg19/ encodedcc/wgencodecshllongrnaseq/ [cit] . the integration was performed using the splice graph generation strategy described in section 3.1.
"in this work, we presented a highly extensible service architecture that can transform non-factoid answer selection models into fully featured end-toend question answering systems. our key contribution is the simplification of in-depth analyses of attention-based models to non-factoid answer selection. we enable researchers to interactively explore and understand their models qualitatively. this can help to create more advanced attention mechanisms that achieve better answer selection results. besides enabling the exploration of individual models, our user interface also allows researchers to compare different attention mechanisms side-by-side within the same view."
"the experiment scenario used ns-2 to create a wireless environment, using 802.11 standards, to simulate 6 wireless nodes in a distributed manets like structure shown in figure 2 . node 0 wants to get the trust value of node 1 based on trust opinions from node 0 and its neighbouring nodes 2, 3, 4 & 5.the dsdv routing protocol is used. fig. 2 . topology of the 6 wireless nodes"
"to estimate the variability of read counts falling into segments we investigated human annotated single transcript genes using one rna-seq library from the encode project (library for cell line k562 (wgencodecshllongrnaseqk562cellpapalnrep1.bam) from http://hgdownload.cse.ucsc.edu/goldenpath/hg19/encodedcc/wgencodecshllongrnaseq/) [cit] . we counted read starts falling into 20 randomly selected segments for each transcript and computed mean and variance. figure c shows a scatter plot of mean versus variance. as discussed in the main manuscript we model the relationship of mean and relationship of mean and variance of read starts for randomly selected segments in 10,820 single transcript genes (human hg19). we counted read starts for up to 20 non-overlapping regions of length 30nt and computed mean and variance of read start counts. in red we show the weighted least squares fit."
"for a node in a distributed environment, such as in a wireless ad hoc network, the trust management of the network views the node as an agent for obtaining the trust information. the functional blocks of the framework are shown in figure 1 . in figure 1, the nodes in the tmf firstly collect the input information for subsequent computation of trust. many existing trust models for distributed environments choose the probability of successful interactions, which is generally viewed as corresponding to the packet loss rate, as the main parameter in calculation of the trust value. however, in fact the probability of one node cooperating with other nodes is influenced not only by the packet loss rate, but also signal strength, data rate, and other physical factors that are not considered in current trust models."
"a screenshot of our user interface is shown in figure 2 . in the top row, we include an input field that allows users to enter the question text. this input field also contains a dropdown menu to select the target model that should be used for the candidate ranking. this makes it possible to ask the same question for multiple models and compare the outputs to gain a better understanding of the key differences. below this input field we offer multiple ways to interactively change the attention visualization. in particular, we allow to change the sensitivity s and the threshold t of the visualization component. we calculate the opacity of an attention highlight o i that corresponds to the weight w i in position i as follows:"
"factoid answer selection into fully featured end-toend question answering systems. our sophisticated user interface allows researchers to ask arbitrary questions while visualizing the associated attention vectors with support for both, one-way and twoway attention mechanisms. users can explore different attention-based models at the same time and compare two attention mechanisms side-by-side within the same view. due to the loose coupling and the strictly separated responsibilities of the components in our service architecture, our system is highly modular and can be easily extended with new datasets and new models."
"model selection for mitie optimized the f-score on transcript level. doing the same for cufflinks results in sub-optimal predictions when samples are merged with cuffmerge. thus, the main text shows the cufflinks+cuffmerge combination, where the mean of sensitivity and specificity was optimized for cufflinks. figure d shows the sensitivity and specificity for all predictions also shown in figure 5a and in addition the result for the f-score optimized version. we note that if we optimize the same criterion for cufflinks and mitie then mitie predictions significantly outperform cufflinks predictions in sensitivity as well as in specificity. furthermore, we note that ranking methods based on the mean of sensitivity and specificity favors trivial and meaningless solutions. one example for such a trivial solution is to select only a single high confidence transcript prediction for the whole genome which if correct results in an mean(sn, sp) of 50%, but a fscore close to zero. for each of the eight optimized cufflinks parameters we tried seven values. thus we performed 56 predictions for each optimization criterion (112 in total). tested values were equally distributed in log-space from default value divided by five to default value times five. all optimized parameters are shown in table a we combined the different cufflinks predictions using cuffmerge and also optimized the hyper-parameter \"-min-isoform-fraction\" of the cuffmerge tool. sensitivity and specificity of the predictions are shown in (suppl. figure d) ."
"for multiple input parameters, we can use grey theory to proceed and calculate trust value. from grey theory, let x be a grey relational set which is used as the evaluation index set,"
"we evaluate against a set of annotated genes by first finding all transcripts overlapping with a given gene. we then compute a binary match score for each pair of transcripts according to the criteria described in section 4.2.3. we then computed a maximal matching to find pairs of annotated and predicted transcripts such that the total number of correct matches is maximal. the number of matching pairs is taken as the number of true positive predictions, when computing sensitivity and specificity of the methods. clearly, no annotated and no predicted transcript is allowed to be part of more than one such pair. all predicted transcripts not being part of a matching pair are counted as false positives and all annotated transcripts not being part of a matching pair are counted as false negatives."
"attention-based neural networks are increasingly popular because of their ability to focus on the most important segments of a given input. these models have proven to be extremely effective in many different tasks, for example neural machine translation [cit], neural image caption generation [cit], and multiple sub-tasks in question answering [cit] ."
for cufflinks we recorded the cpu-time spend in the assemble bundle method in the cufflinks.cpp file using the boost::chrono library. we recorded the time for each bundle and stored it with bundle start and stop coordinates in a separate output file.
(1) the mathematical model of the emsc under synchronously rotating coordinate system was obtained and the integrated model of emsc and pwm rectifier was initiatively deduced. the consistency of simulation and experiment results indicates that mathematical model of the emsc has high accuracy.
"field-oriented control has been widely applied in threephase motors [cit] . based on field-oriented control theory [cit], direct-axis component of the induced three-phase current tracks the target value of zero to realize synchronization between input current and input voltage of the pwm rectifier. for modulation of the pwm rectifier, space vector pulse width modulation (svpwm) is superior to sinusoidal pulse width modulation (spwm) in some aspects, such as high voltage utilization ratio, low switching frequency and easy implementation [cit], so svpwm technique is applied to ensure that input current of the pwm rectifier is sinusoidal, thus improve dynamical performance. after rectification, the slip energy in the form of direct current is recycled by the super capacitor that is widely used in energy storage system [cit] because of its high power density and longer cycle lifetime [cit] ."
"where, ψ d is direct-axis component of flux linkage of three-phase windings, ψ q is quadrature-axis component of flux linkage of three-phase windings, l d is direct-axis component of self-inductance of three-phase windings, l q is quadrature-axis component of self-inductance of three-phase windings, l f is self-inductance of field winding, m sf is mutual inductance between three-phase windings and field winding, i d is direct-axis component of current in three-phase windings, i q is quadrature-axis component of current in three-phase windings, u d is direct-axis component of terminal voltage of three-phase windings, u q is quadrature-axis component of terminal voltage of three-phase windings, i f is excitation current of field winding, u f is terminal voltage of field winding, r s is resistance of each phase winding in outer rotor, r f is resistance of field winding in inner rotor, ω is electrical angular velocity difference between outer rotor and inner rotor, n p is pole pairs of inner rotor, t i is drive torque on outer rotor, t e is electromagnetic torque, t l is load torque on inner rotor, j 1 is rotational inertia of outer rotor, j 2 is rotational inertia of inner rotor, b 1 is viscous damping coefficient of outer rotor, b 2 is viscous damping coefficient of inner rotor, 1 is mechanical angular velocity of outer rotor, 2 is mechanical angular velocity of inner rotor."
"in table 1 we show the number of discards in playouts. this number of discards is listed per each game age and for 4 different values of the hard limit l on branch exploration (this is done for 3 players and 15000 simulations per game turn and there is some variance due to the game randomness). some hands of cards really do not contain any playable card, thus a discard is compulsory by the game rules, but in other cases there are playable cards that are not seen if l is too low: the exploration of trading choices is ended too early. indeed we see in table 1 that the number of discards increases when putting too strict limits on trade exploration, meaning that we drop cards that could have been played by allowing a longer search."
"where, k p3 is the proportional coefficient of the pi controller to control u dc, k i3 is the integral coefficient of the pi controller to control u dc ."
"tuning of the bandit formula first table 3 shows a comparison of success rate for several values of the exploration constant k u ct, on 1000 games, with 1000 simulations per game turn for each mcts player. the success rates of the rbai player are very low when using this number of simulations for mcts and are not reported here (thus the success rates displayed do not sum up to 100%). the experiment show that good k u ct values can be obtained in the range [0.3, 1.0], although values strictly greater than 0.3 do not yield no much significant improvement. this good 0.3 value is slightly superior to the standard 0.2 found in the literature. this might be explained by the fact that some moves may appear very attractive in a few simulations (e.g. playing a military card) while some other good moves need more simulations to show their robustness (e.g. playing a science card that also allows some good later cards for free)."
"emsc transmits power through the interaction between excitation field generated by excitation current and induction field generated by eddy current [cit] . due to the eddy current, the slip energy, that is energy difference between the input and output of emsc, dissipates as heat, which results in low efficiency especially under largeslip conditions [cit] . to improve transmission efficiency of emsc the slip energy should be recycled. one of feasible approaches is to conduct eddy current out of the armature by three-phase windings. the induced current in three-phase windings should be converted to direct current (dc) before slip energy recovery. conventional rectifiers that convert alternating current into direct current are diode rectifier and silicon controlled rectifier, respectively constituted of diodes and thyristors [cit] . for silicon controlled rectifier, phase of the input current lags behind phase of the input voltage and the phase hysteresis increases with the trigger angle of thyristor. so power factor of silicon controlled rectifier is relatively low, which indicates low efficiency [cit] . for diode rectifier, although the phase hysteresis between input voltage and input current is nearly zero, input current contains large numbers of high-frequency harmonic components which also result in low power factor [cit] . pwm rectifier, whose input current is sinusoidal and in-phase with the input voltage, can realize unity power factor [cit] ."
"where, e is recycled energy in super capacitor, u sc is terminal voltage of super capacitor, u sc0 is initial terminal voltage of super capacitor."
"the energy-regenerative emsc is a dual mechanical ports machine, which consists of outer rotor, inner rotor, and energy-regenerative apparatus, as shown in figure 1. outer rotor and inner rotor are embedded with three-phase windings and field winding respectively. outer rotor connects to prime mover by the input shaft, and inner rotor connects to mechanical load by the output shaft. the energy-regenerative apparatus including pwm rectifier, super capacitor and controller recycles slip energy in three-phase windings. the controller regulates current of the field winding and modulates pwm rectifier. when the outer rotor is driven by the prime mover, the three-phase windings induce three-phase alternating current under the excitation of magnetic field generated by field winding. due to the interaction between induction field and excitation field, the inner rotor rotates along with outer rotor in asynchronous speed. the pwm rectifier converts induced alternating current into direct current to charge the super capacitor for slip energy recovery."
"-a card providing 2 or more resource types is always played; -a card providing a single resource type that is lacking to the rbai is always played; -a military card is always played if rbai is not the only leader in military, and the card allows rbai to become the (or one of the) leading military player(s); -the civil card with the greatest vp award is always played; -a science card is always played; -a random remaining card is played if possible, else a random card is discarded."
"future works consist in implementing the use of the rapid action value estimate (rave) enhancement [cit], which is one the most powerful improvement for several games [cit] . another interesting work should be to analyze and improve the enhancements tried in this paper. in particular, having one progressive widening per age seems to be a good idea. the real score could also be incorporated in a similar way, as the relevance of its impact is probably bigger in the last stages of the game. parallelization of the playouts could have both advantages of increasing the level of play by using more simulations, and allowing enough time to explore all trading moves in playouts. last but not least, we plan to interface our ai with a gaming website in order to obtain a better assessment of its game level through the confrontation with more human players."
"coping with weak decisions a 7w player has always the choice of putting a card on the discard pile to earn 3 coins of game money. this is the sole option available when the resources needed to play any card are not available, but it can also be a tactical choice, e.g. in order to have enough money to buy resources from neighbors on a later turn. however, most of the time discarding is a worse than average move: notably 3 coins amount to 1 vp, while played cards should bring an average reward of about 3 vps per card. as any one card can be discarded for money, while not all cards can usually be played, depending on available resources, thus discards are the most common moves and would be the dominant moves explored by a fully random playout procedure. this results in non-informative playouts and a weak mcts player. the presence of such a class of weak moves is not uncommon in other games too, for instance in go, all programs discard the \"empty triangle\" move from their playouts. we act similarly in the playouts, allowing discards only when no other move is possible. on the contrary we keep all moves in the mcts subtree construction, in order to allow for tactical discards."
"after playing their card, there is a so-called drafting phase, where all players give their hand of remaining cards to their left (age 1 and 3) or to their right (age 2) neighbor. thus the cards circulate from player to player, reducing the hidden information. when there are less than 6 players, some cards from his original hand will eventually come back to every player . on the 6th turn, when the players receive only two cards in hand, they play one of the two and discard the other."
"to investigate dynamic performance and energy recovery performance of the energy-regenerative emsc, simulations are carried out. simulation parameters are shown in table. 2. the simulation conditions are that input speed of emsc is 1200 r/min, desired output speed is set as 600 r/min, and load torque on output shaft is 10 n·m. simulation results are presented in figure 8 to figure 12. in figure 8, output speed of the emsc is quickly convergent to the desired value of 600 r/min after a slight overshoot. from figure 9, it can be seen that voltage of dc side of pwm rectifier reaches steady state in a very short time. simulation results of output speed and dc side voltage both demonstrate excellent dynamic performance of energy-regenerative emsc. in figure 10, voltage and current of ac side of pwm rectifier are almost in same phase, which indicates approximate unity power factor of the energy-regenerative apparatus. simulation of net recycled energy that is the difference between recycled energy in super capacitor and excitation energy is implemented. the result as shown in figure 11 indicates that recycled energy is less than excitation energy at the beginning, and then it increases fast as time goes on. the effect of initial terminal voltage of the super capacitor on net recycled energy is also investigated. the simulation result is presented in figure 12. it shows that net recycled energy increases observably with initial terminal voltage of super capacitor, which demonstrates that the energy-regenerative apparatus can substantially recycle slip energy of emsc."
"we can give an approximation of the state space size for 3 players, by considering that there are 68 possible different cards, from those each player will usually play 18 cards. we thus obtain"
"let us first define two functions: mc(s) which plays a uniform random decision (move) from the situation s and returns the new position, and result(s) which returns the score of the final situation s. the idea is to build an imbalanced partial subtree by performing many random simulations from the current state, and simulation after simulation biasing these simulations toward those that give good results. the construction is then done incrementally and consists in three different parts : descent, evaluation and growth, illustrated in fig. 2 ."
"-it has a complex scoring scheme combining linear and non linear features: blue cards provide directly vps to their owner, red cards offer points only to the owner of the majority of red cards symbols, yellow ones allow to save or earn game money, green ones give their owner the number of identical symbols to the square, with extra points for each pack of three different symbols. -resource cards have delayed effect : they mainly allow a player to put vps awarding cards on later turns; this is also the case of green cards that, apart from the scoring of symbols, allow some other cards to be played for free later on."
"all experiments are composed of 1000 runs, and the number of mcts simulations per game turn is given for the 1st of the 3 ages of the game. this number is multiplied respectively by 1.5 and 2 in the 2nd and 3rd ages, since the shorter playouts allow more simulations in the same time (so a \"1000 simulations\" means respectively 1000, 1500 [cit] simulations per turn in ages respectively 1, 2 and 3)."
"we dealt with that issue in playouts by imposing a random order on the branches of the tree of possible trade choices, and fixing a hard limit l on the number of branches explored, depending on the game age: 3rd age cards generally award more vps and require more resources, so it is sensible to spend more exploration time before deciding if they are playable or not in playouts. if we cannot find a suitable branch in the first l branches explored, then we consider that the card cannot be played (either on the table or under the board), thus it must be discarded."
"in the mcts vs rbai test, we use one instance of mcts versus 2 instances of the rbai. iteratively, 20 sets of personal boards are drawn, and 50 independent random cards distributions are played on every board set."
"the monte-carlo tree search algorithm (mcts) has been recently proposed for decisionmaking problems [cit] . applications are numerous and varied, and encompass notably games [cit] . in games mcts outperforms alpha-beta techniques when evaluation functions are hard to design. the most known implementation of mcts is upper confidence bound (uct), that is presented below. enhancements have also been proposed, such as progressive widening [cit], that is described at the end of this section."
"the goal of the game is to score the maximum victory points (vp), which are awarded to the players at the end of the game, depending on the cards played on the table, under the boards and the respective amounts of game money. the cards are almost all different, but come in families distinguished by color : resources (brown and grey), military (red), administrative (blue), trade (yellow), sciences (green) and guilds (purple). the green family is itself sub-divided between three symbols used for vp count. see figure 1 for an illustration of a player situation. this game presents several interesting traits for ai research, that also probably explain its success among gamers:"
"the novel energy-regenerative emsc was developed for slip energy recovery. in this paper, the configuration of the emsc was constructed; the mathematical models were built; the field-oriented control strategy was designed; simulations and experiments were carried out for the validation. based on the investigated results, the following conclusions can be drawn."
"where, * 2 is desired mechanical angular velocity of inner rotor, k p is the proportional coefficient of the pid controller to control 2, k i is the integral coefficient of the pid controller to control 2, k d is the differential coefficient of the pid controller to control 2 ."
"in the mcts parameter tuning experiments, we use one instance of the rbai versus 2 instances of mcts with different parameters/enhancements, called mcts-1 and mcts-2. we draw iteratively 5 board sets where the same board is duplicated for the two mcts (this duplication of boards is not allowed in the original game rules but is handy to ensure that the mcts comparison is not biased by the strength of the different boards). for each board setting, 100 independent random card distributions are played, then the two different mcts instances swap position and the same 100 card distributions are played again. this is to remove any bias that could be generated by the position of each mcts relatively to the rbai player."
"in this section we compare the success rate of one mcts player against two instances of the simple rule-based ai. table 2 shows that mcts is clearly superior. adding more simulations improves the mcts success rate, although with diminishing returns as the number of simulations increases, as expected by the theory."
"in this paper we have shown the interest of the monte carlo tree search algorithm in the case of the 7w complex boardgame, using full rules. the game of 7w presents several hard features: multiplayer, hidden information, random elements and a complex scoring mechanism, that renders difficult to handcraft an evaluation function. in this context, the mcts method obtains convincing results, both against a human designed rule-based ai and against experienced human players. however, the implementation is not straightforward: we use determinization to handle the hidden information element, and we refine the playouts by suppressing a class of weak moves. moreover the computing cost of exploiting all possible trading decisions in the playouts is too large to allow enough simulations for real-time play against humans. we solve this problem by approximating the set of allowed trading decisions. thus the gap between mcts theory and practice is not negligible."
"where, l is inductance of filter inductor, r is equivalent resistance of filter inductor and mosfets, s a, s b and s c are switch functions that indicate the states of rectifier bridge arms, i a, i b, i c are input current of rectifier bridge, u a, u b, u c are input voltage of rectifier bridge, u dc is voltage of dc side after rectification, c sc is capacitance of super capacitor, r sc is internal resistance of super capacitor."
"we notice that the various parameter effects (k u ct, scalability, progressive widening, and adding a score information) are quite similar to what is observed in classic abstract games, despite the fact that this game seems substantially different."
"the developed emsc prototype includes thirty-six slots and three pairs of magnetic poles. due to time-variant self-inductance and mutual inductance of three-phase windings and field winding under fixed coordinate system (a-b-c coordinate) [cit], mathematical model of the emsc is constructed under synchronously rotating coordinate system (d-q coordinate). taking no account of flux saturation, harmonic wave and armature reaction, flux linkage equations, terminal voltage equations, electromagnetic torque equation and kinematic equations are established, shown as follows."
"the rule-based ai (rbai) is deterministic and is managed, in age 1 and 2, along the principles listed below by priority order (when a card is \"always played\", it means of course if it is affordable):"
"in the table 1 we see that discards are more common in the first age and are not affected by our limit thresholds. this is explained by the fact that not many resources have usually been played by random playouts in the first age, thus discards are often the sole possible decisions. on the contrary, the l threshold impacts the successful discovery of playable moves in age 2 and 3. discards remain more common in age 3 than in age 2, since age 3 cards require more resources. by setting a limit of 8, 24 and 32 branches on respectively age 1, 2 and 3, we gained a more than 5 times speedup against the exploration of the whole trading tree. this allows for about 1000 simulations per second in age 1. the loss of precision in playouts (discards that could have been avoided with a longer search) is more than compensated for by the greater number of simulations allowed in the same time. note that humans are also confronted to the same problem, and it is not uncommon that a player thinks a card is not playable, while it really is."
-puts it on the table in his personal space; -or puts it under his personal board to unlock some specific power; -or can discard it for 3 units of the game money.
"the descent is done by using a bandit formula, i.e. by choosing the node j among all possible nodes c s which gives the best reward according to the formula :"
"just like the simulation, load torque of emsc is stepped from 10 n·m to 15 n·m in the experiment. test result in figure 18 shows that output speed of emsc decends to 450 r/min at the time of one second, then it increases rapidly and returns to the initial value after a fluctuation about 100 r/min in a short time, which demonstrates the robust anti-disturbance capability of the energy-regenerative emsc with the field-oriented control strategy."
"we have chosen a simple ordering heuristic based on human play. this ranking consists in having first alternative resource type cards, then single resource type cards, then military cards, followed by science cards. other cards are left unordered since for trade cards it is difficult to assess an a priori order, and for civil cards we can expect that they are easier to evaluate by mcts, since their reward is mostly independant of other cards."
"from above expressions, it can be seen that direct-axis component and quadrature-axis component of current in three-phase windings are coupled. to achieve current decoupling, feed forward control approach is proposed as follows."
"using the real score of the game it has been shown that using an evaluation function instead of a monte-carlo policy can improve the global strength of the mcts algorithm [cit] . however, building such a function is not always possible and sometimes monte-carlo evaluations is the only choice. sometimes, an intermediate solution consists in taking the real score of the game in order to bias the bandit formula. this is only possible when such a score exists and results are moderate. for instance, in the game of go, there is only a very small (but significant) improvement [cit] . one emphasis reason is that it becomes too greedy to win by more points and takes risks. we try to use the real score to bias the bandit formula as this score exists in the original game. following [cit], the bandit formula becomes then"
"the control aim is to recycle slip energy efficiently and improve dynamical performance of the emsc. from the kinematic equations of the emsc, it can be seen that dynamical performance of the emsc is influenced by electromagnetic torque which depends on induced current in three-phase windings and excitation current in field winding. according to mathematical model of the pwm rectifier, current in threephase windings is related to the modulation of the pwm rectifier. so output speed of emsc can be regulated by excitation current in field winding and pwm rectifier modulation. based on above analysis, efficient slip energy recovery of emsc can be realized by pwm rectifier modulation. the principles of modulation are that input voltage and input current of pwm rectifier is in-phase and input current is sinusoidal to eliminate high-frequency harmonic components."
"from the structure of the energy-regenerative emsc, it can be seen that three-phase windings in outer rotor connect with pwm rectifier, thus current in three-phase windings is also the input current of pwm rectifier."
"during the simulation, load torque of the emsc is stepped from 10 n·m to 15 n·m to validate anti-disturbance performance of the proposed control strategy. the simulation result in figure 13 shows that output speed of emsc decreases by 80 r/min at the time of 0.5 seconds and then returns to 600 r/min in a short time."
"where, i * d is set as zero to realize phase synchronization of current and voltage in three-phase windings. i * q is presented in the following expression to ensure that voltage of dc side after rectification is steady."
"in modern industries, variable frequency motors have been widely used in drive and transmission applications [cit] . nevertheless, they are not applied in the area of unavailable electric power, such as automotive transmission, ship propulsion and construction machinery, where mechanical load can be adjusted by the coupling equipped between prime mover and mechanical load. there are generally three types of couplings, respectively, mechanical coupling [cit], hydraulic coupling [cit], and magnetic coupling [cit] among which magnetic coupling is more reliable and feasible because of no mechanical wear and oil pollution. magnetic couplings are classified as electromagnetic slip coupling (emsc) [cit] and permanent magnetic coupling [cit] according to different ways of excitation. in terms of variable speed transmission, emsc varies speed of mechanical load by controlling excitation current, while permanent magnetic coupling adjusts speed of mechanical load by varying length or area of the air gap via a mechanism [cit] which is more complicated."
"note that our mcts ai was also successfully opposed to some experienced human players, even if the number of plays (and players) cannot yet be considered significant and reported here."
"the number of different cards (68 for 3 players, from those 5 are removed randomly) and the 7 specific boards, together with the delayed effect of many cards and the non linear scoring, make it difficult to handcraft an evaluation function. notably, the number of vps gained in the first game age is a bad predictor of the final score, since scoring points at this stage of the game usually precludes playing resource cards that will be needed later on."
"argument node s, mcts subtreet while there is some time left do s ← s initialization: game ← ∅ // descent while s int and s not terminal do s ← arg max"
it can be seen that output speed of emsc is closely relative to quadrature-axis component of current in three-phase windings and excitation current in field winding from above expression. quadrature-axis component of current can be resolved by expression (10) and (11) . excitation current is expressed by following equation to ensure that output speed of emsc tracks the desired value in the case of load disturbance.
"the simulation model of emsc in matlab/simulink is built, and its parameters are shown in table 1. the dynamic characteristic, mechanical characteristic, output speed versus input speed characteristic, and output speed versus excitation current characteristic are simulated. the simulation results are shown in figure 2 to figure 5. in figure 2, as the rotational speed difference increases, the electromagnetic torque first increases linearly and then decreases dramatically, which demonstrates that mechanical characteristic of the uncontrolled emsc is soft. in figure 3, the mechanical characteristic presents the linearity at the rotational speed range from 0 to 600 r/min, which is consistent with the dynamic characteristic. in figure 4, output speed of the emsc varies linearly with input speed. in figure 5, output speed of the emsc varies dramatically with excitation current, which also indicates that mechanical characteristic of the uncontrolled emsc is flexible. the experiments about mechanical characteristic, output speed versus input speed characteristic and output speed versus excitation current characteristic were carried out to verify mathematical model of the emsc. the comparisons of simulation and experiment results are implemented as shown in figure 3 to figure 5. it can be seen that simulation results are consistent with the experiment results, which demonstrates that the model of emsc is correct and precise."
"the last decision (or move) is always possible, while the first two possible moves depend on the player ability to gather enough resources from his board or from the production cards he already played in his personal space. he can also buy, with game money, resources from cards previously played by his left and right neighbors. this trading decision cannot be opposed by the opponent player(s) and the price is determined by the cards already played."
"the test bench and test system of energy-regenerative emsc are developed to verify the proposed control strategy by experiment. the test bench consists of variable frequency motor, torque-speed sensors, emsc, magnetic powder dynamometer and control cabinet as shown in figure 14. test system consists of controller, pwm rectifier, super capacitor, current sensor, acquisition instrument and oscilloscope as shown in figure 15 . the test conditions are that output speed of variable frequency motor is set as 1200 r/min, load torque of the emsc is set as 10 n·m, desired output speed of the emsc is set as 600 r/min. in the experiment voltage and current of three-phase windings, field winding and super capacitor are collected by the oscilloscope; output speed of emsc is acquired by the acquisition instrument. test results are presented in figure 16, figure 17 and table. 3. it can be seen from figure 16 that input voltage and input current of pwm rectifier are almost in-phase, and input current contains few of high-frequency harmonic components, which demonstrates that the control strategy functions effectively and efficiently. in figure 17, dc side voltage of pwm rectifier reaches the steady value of 38.5 v at the time of 0.6 seconds, which is consistent with the one in the simulation. the stabilization time of dc side voltage in the experiment is a bit longer than the one in the simulation since factors such as hysteresis and friction are not based on input power and output power, slip power of emsc can be calculated by following equation."
"uct algorithms are efficient methods for balancing exploration and exploitation, however, only few information are provided for decisions loosely explored. several enhancements have been provided to tackle this problem. the most famous are progressive widening [cit] and rapid action value estimate [cit] ."
"in the first section we introduce the \"7 wonders\" game and its rules, before presenting the mcts heuristic. then we focus on specific issues that arose during implementation, before presenting a set of experiments and their results."
"(2) from the mathematical model of pwm rectifier, it can be seen that direct-axis component and quadrature-axis component of current in three-phase windings are coupled. to achieve the current decoupling, feed forward control approach was proposed."
"(4) based on the proposed control strategy, unity power factor of the energy recovery apparatus was realized. test results showed that energy recovery efficiency of the energy-regenerative emsc reached 82.5%, which demonstrated that transmission efficiency of emsc was highly improved."
"in order to play a card in 7w, a player may choose to buy resources either from the right, left or from both of his two neighbors, at possibly different costs, depending on the resource type. moreover some of the resource producing cards provide alternate choices: e.g. the \"caravanserai\" card provides one unit of either ore, wood, brick or stone. these two game rules induce a combinatorial tree of possible resource trade choices, and it is not uncommon that this tree has several hundred branches, especially during the 3rd game age. some of the branches are dead end, that is after setting some trade choices, it appears that some required resources cannot be gathered or are now unaffordable to the player. other branches may offer valid and affordable trade choices, that constitute as many possible game decisions. note that there is such a tree of possible trade decisions for every card in hand."
"-there is hidden information when the players receive their hand of cards at the beginning of each age of the game. -there is a great interactivity between players as they can buy resources from each others to achieve the playing of their own cards. some cards also give benefits or vps depending on which cards have been played by the neighbors. moreover the drafting phase confronts players with the difficult choice of either playing a card that gives them an advantage, or another card that would give a possibly greater advantage to one of the neighbors that would receive it after the drafting phase. -the game is strongly asymmetric relatively to the players, since all player boards are different and provide specific powers (such as resources, or military symbols). most of these benefits are available when playing a card under the personal board at a given cost in resources. thus some boards are oriented towards specific strategies, such as maximizing the number of military symbols, or increasing the bonuses of collecting green cards symbols, for example."
"where, s d is direct-axis component of switch value of rectifier bridge arms, s q is quadrature-axis component of switch value of rectifier bridge arms."
"to realize the phase synchronization between input current and input voltage of pwm rectifier, direct-axis component of current in three-phase windings should be adjusted to zero. mathematical model of the pwm rectifier is simplified as follows."
"the objective of the experiments is to study the level of efficiency of mcts to play successfully at 7w. this is done in comparison with a simple handcrafted rule-based ai presented below, and also by studying several values of mcts parameters and enhancements, such as progressive widening."
"in a 7w game, from 3 to 7 players 1 are first given a random personal board among the 7 available, before playing the so-called 3 ages (phases) of the game. at the beginning of each game age, each player gets a hidden hand of 7 cards. then there are 6 playing card rounds, where every player simultaneously selects a card from his hand and either:"
"while the exploration of the whole tree for every card is recommended for building the mcts subtree, since one does not want to forget a possible card playing decision, this would greatly impact the speed of play when it is done in playouts. nonetheless it is not possible to ignore the trade decision tree, since truly random decisions would make nonsense in most cases (such as trying to buy resources from a player that do not own them, or that own them in insufficient number). thus some sort of exploration of the trade tree must be done."
"a larger ξ 1 signifies that the cancer cell is damaged substantially. here we set the ξ 1 as 0.6n. then, for the normal cell, we define the objective function as i γ i · r i. it should be noted that for this problem it is possible that there does not exist any singleton attractor for n 2 since the control set must satisfy the inequality (1) ensuring significant damage to the cancer cells. in terms of ilp-a', we aim at finding the maximum of the minimum score of n 2 for the case of having multiple singleton attractors. since we consider the multiple singleton attractors for n 2, we do not need to include constraints about n 1, so we can replace r i with"
"instance: 2 bns, a score function g, the number of control node m, and thresholds θ 2 and ξ 2, problem: find m nodes and a 0-1 assignment of these control nodes for which the minimum score of singleton attractors for n 1 is no less than θ 2 and the score of singleton attractors for n 2 is greater than ξ 2, respectively. if such nodes do not exist, then \"null\" is the output."
"each gene v i is updated by a regulatory function f i . the corresponding truth table of a bn is given in table 1 . the dynamics of a bn and state transition diagram are shown in figure 1 . for example, the second row of the table means that if the state of bn is [cit] at time t, then the state at time t + 1 will be [cit] . similarly, the arc from 001 to 110 signifies that if the state of bn is [cit] at time t, then the state will be [cit] at time t + 1. it is easily seen that a bn with n nodes has in total 2 n possible states. thus, the state transition table and the state transition diagram have 2 n rows and 2 n vertices respectively."
"1. repeat steps 2 -3. 2. find (v', b') yielding the maximum score of a singleton attractor using ilp-b where (v', b') is not the same as any of the already examined nodes/values pairs. \"null\" should be output and halt, if the maximum score is less than θ. 3. for (v', b'), calculate the singleton attractors with the minimum score by ilp-a'. output (v', b') and halt, if the minimum score is no less than θ."
"we set the ξ 2 to be 0.6n, and we define the objective function as i α i · u i. the singleton attractor for n 1 is difficult to find for this problem, because the control set must satisfy the inequality (2) (not damaging the normal cell substantially). considering that there may exist multiple singleton attractors for the cancer cell, and the worst case, it is necessary to maximize the minimum score of singleton attractors."
"it is also of interest to consider finding a singleton attractor for cancer cells with limited damage to normal cells, to determine if there exists a control set ensuring no damage to normal cells, and under such a control set, to search for a singleton attractor for cancer cells."
"thus for ilp-a', we do not need to add any constraints about n 2 for ilp-a' so we replace u i by ', b') and halt, if the minimum score is not less than θ 2 ."
"problem: find m control nodes and 0-1 assignment for them where the minimum score of singleton attractors is greater than θ. if no such nodes exist, then \"null\" is the output."
"though attractor control problems are σp-hard, the experimental results have shown the efficiency of our proposed method. furthermore, this method was seen to be useful for solving medium-size instances of these problems. the method we proposed might not be the fastest, but it is easy and simple to implement and, furthermore, it has rooms for modifications and extensions. in particular, the use of non-linear costs for the scoring functions is of interest for future work."
"in this section, we first give a brief introduction to bns. since the attractor control problem is based on the attractor detection problem (attractor detec-tion), we begin with a brief introduction to that problem. we then define three variants of the attractor control problem: simultaneous attractor control for two bns (cancer cell vs normal cell), attractor control for normal cells under the assumption of significant damaging cancer cells, attractor control for cancer cells under the assumption that normal cells are not damaged."
"definition 3: [attractor control under damaging cancer cells substantially] (acdc) instance: 2 bns, a score function g, the number of control node m, and thresholds θ 1 and ξ 1, problem: find 0-1 assignment to m control nodes where the minimum score of singleton attractors of n 2 is no less than θ 1, and the score of the singleton attractor for n 1 is greater than ξ 1, respectively. if such nodes do not exist, then \"null\" is the output."
"integer programming, in particular, integer linear programming (ilp) is to maximize (or minimize) a linear objective function with linear constraints (i.e., linear inequalities and linear equations) with all the variables taking integer values. from here on, either 0 or 1 is assigned to each variable, representing the boolean values. furthermore, we introduce x i and si to represent a 0-1 variable that corresponds to v i for n 1 and n 2, respectively."
"note that in the worst case, it may repeat this procedure exponentially many times, but we expect that the procedure will not be repeated so many times, since the expected number of singleton attractors (i.e, per (v', b')) is small, regardless of the total number of nodes (n). how to select the thresholds θ is an important issue in this program. if we know an appropriate threshold in advance, we can simply use such a θ. here, we let θ be 1.2n, because the desired attractors almost always exist if θ ≪ 1.2n, and it often occurs in our preliminary computational experiments that the algorithm cannot find the desired attractors if θ ≫ 1.2n."
"for this problem, we first guarantee that this control damages the cancer cell substantially and add the additional constraint (1) . it is possible that no singleton attractor exists for the normal cell under the condition that the control set guarantees significant damage to the cancer cells. as shown in table 3, our proposed method is useful also for this problem."
"we consider another attractor control variant for multiple networks. we try to investigate whether there exists a control that can guarantee significant damage to cancer cells (n 1 ) and a singleton attractor for normal cells (n 2 ). to ensure that the control can cause significant damage to the cancer cells, we introduce a threshold ξ 1 for n 1 to be given later. the score function g(v) becomes g(v). it is possible that there exist multiple singleton attractors for n 2, so we maximize the minimum score of the singleton attractors and give the threshold θ 1 for the minimum score. the problem is formulated as follows:"
"we can solve this problem using the following algorithm. here, we set θ 1 to be 0.6n, because if θ 1 ≪ 0.6n, then the desired attractor almost always exists, whereas if θ 1 ≫ 0.6n, then the desired attractor seldom exists."
"definition 1: [attractor detection] instance: a bn and p max, the maximum period. problem: output an attractor whose period is at most p max . if such an attractor does not exist, \"null\" should be the output."
"as another variant of attractor control, we find singleton attractors for normal cells with a guaranteed damaging level for cancer cells. in other words, we try to investigate whether there exists a control set that can damage cancer cells, while at the same time looking for a singleton attractor for normal cells under this control."
the results also suggest that our proposed approach is efficient and effective for solving the problem of attractor control of the cancer cell with no damage to the normal cell guaranteed.
"is called the gene activity profile (gap). x ∧ y, x ∨ y, x ⊕ y,x is used to represent \"and\" of x and y, \"or\" of x and y, exclusive or of x and y, and \"not\" of x, respectively. we denote in (v i ) as the set of relevant input nodes v i1, ..., v ik to v i . the number of relevant nodes incoming to v i is called as indegree of v i . k is used to represent the maximum indegree of a bn."
"consider that each variable vi has two possibilities, i.e, (i) v i is not chosen for a control node (i.e., v i corresponds to an internal node), (ii) v i is chosen for a control node (i.e., v i becomes an external node)."
define v i (t) to be the state (0 or 1) of vertex v i at time t. the rules for the regulatory interactions among the genes are then expressed by
"as the final score function for these two networks. since the singleton attractors may not be uniquely determined, considering the worst case, the minimum score of singleton attractors is necessary to be maximized. because it is difficult to give a direct formulation of this problem, the problem of simultaneous control of attractors is defined with θ, which is a threshold of the minimum score, as follows."
"the resulting ilp is denoted by ilp-a'. from the perspective of avoiding examination of the examined (v′, b′), we will modify ilp-a. in other words, we try to find node-value pairs that are not the same as the previously obtained solutions. this can be tackled by some additional linear inequalities which ensures that the following node-value pairs differ from the previous ones. note that these two networks have the same control, so if we can avoid obtaining the previously examined (v′, b′) for n 1, then we can also get different node-value pairs for n 2 . thus, we shall consider one of the networks, i.e.,"
"thus, this boolean function can be converted into the following inequalities: it is easily seen from this example that the transformation is correct. integrating all the constraints, we can formulate the integer programming for singleton attractor detection as below."
"in this paper, we formulated three novel problems, simultaneous attractor control for multiple networks, attractor control for normal cells with guaranteed damage to cancer cells, and attractor control for cancer cells guaranteed not to damage normal cells, and we applied an ilp-based approach for solving these problems in a unified manner. we further investigated the attractor control problem for multiple bns and validated our proposed method for realistic networks."
"to utilize ilp, all instances of the original problem are converted into ilp, to be able to apply an existing solver. these problems are transformed into ilp in a similar and systematic way to be shown later. the experimental results, using artificially generated bns and realistic bns, have demonstrated both the efficiency and the effectiveness of our proposed method."
"in the post-genome era, we observe rapid development in systems biology, a field focusing on interactions among the components of biological systems. gene regulatory networks (genetic networks, in short) have been proposed to better understand the interaction of various kinds of genes, proteins and molecules. many formalisms have been developed as models of genetic regulation processes, in particular, boolean network (bn) has received substantial attention owing to its capacity to capture the switching behavior of genetic processes. furthermore, its dynamic process is rich and complex and can provide insight into the global behavior of large genetic regulatory networks. a bn is a simple mathematical network: each gene (node) takes either 1 (active) or 0 (inactive), and the state of a gene is regulated by several genes called its input genes via its boolean functions. it is to be noted that the states of genes can be updated synchronously and asynchronously. thus synchronous bns and asynchronous bns have been proposed to model the two different behaviors. in this paper, synchronous bns are considered since existing control studies of bns are based on synchronous bns, and detection of singleton attractor is equivalent for both models. though synchronous bns may be too simple compared with asynchronous bns, they are effective to model and analyze some properties of real gene networks. indeed, they have been used to analyze d. melanogaster embryo development, and the robustness of the genetic networks of s. cerevisiae, e. coli, b. subtilis, d. melanogaster and a. thaliana [cit] ."
"for this problem, we set θ 2 as 0.6n because it often occurs that the algorithm can find the desired attractor if θ 2 ≪ 0.6n, and that it hardly finds the desired attractor if θ 2 ≫ 0.6n."
"the similarity between two documents is measured by the cosine of the angle between their corresponding vectors of terms' weight. cosine values are in [−1, 1] but negative values are discarded and a link has thus a value in ]0, 1], because similarity cannot be negative between two documents and 0 similarity means that two documents would not share any textual information. different ir techniques [cit] can be used to compute the similarity between bug reports and source code files. in this paper, we use the ir techniques lsi and vsm. thus, libcroos ir engine generates a set l."
"step 1: padl model creation (padl model creator): we use the ptidej tool suite [cit] and its padl meta-model (patterns and abstract-level description language) to build padl models of object-oriented programs. a padl model is a representation of a program compliant with the padl meta-model. such model includes the main constituents that represent the structure and part of the behaviour of a program, i.e.,ie classes, interfaces, member classes and interfaces, methods, attributes, inheritance."
"this section presents the results of our experiments. figure 2 shows the box plots of the accuracy measure, in terms of ranking, of libcroos, lsi, and vsm applied on the corpora. we use manually built oracles to analyse the rank of actual culprit class in the ranked list generated by libcroos, lsi, and vsm. for all the corpora, libcroos assigns a lower rank to the culprit classes in terms of lower quartile, mean, median, standard deviation, and upper quartile. the results illustrated in figure 2 provide a high-level view of the accuracy of libcroos. the smaller boxes represent the decreases in rankings, i.e., the culprit classes at the top in the ranked lists. table i reports results for libcroos using all the bcrs and using only one bcr at a time. results show that libcroos statistically decreases the ranks of culprit classes up to 67%. for example, in the case of mucommander (vsm), on average across bug reports, libcroos decreases the rank of culprit from 39.88 to 13.23. standard deviation values show that all the ranks tend to be very close to the mean for libcroos, whereas for lsi and vsm, they are spread out over a large range of values."
"while working on the design and implementation of the multi-mano scenario that we are going to describe in this paper, we were faced with a fundamental question about slicing. how is it possible to connect the various segments, including the mobile edge, the core networks, and the data centers, as well as what layer of the existing mano architecture should implement it. in particular, we identified two possible answers to that question: namely, (i) changing the existing orchestration and management software elements to deal with slices, or (ii) considering slicing at a lower layer and presenting a slice to the upper layers. if we choose the former approach, then all the main software elements would need to be updated and adjusted to know about slices, together with all their related apis. we realised that if slicing is implemented at a lower layer, then the upper layers, such as vims and orchestrators, would not need to know anything about slicing [cit] ."
"we perform an empirical study to compare libcroos with two ir techniques alone: lsi and vsm. we evaluate the effectiveness of our proposed approach on four programs-jabref, lucene, mucommander, and rhino. our results show that libcroos can statistically improve the accuracy of the two ir techniques. furthermore, we analyse each bcr separately; we observe that the inheritance and aggregation relationships help to improve the accuracy of the two ir techniques more than the other relationships. results of our empirical study suggests that developers must use more relationships among classes contributing to a same feature."
"we discussed how the end-to-end slicing approach can provide a more effective resource management for the deployment of customer services. we demonstrated why having a multi-mano hierarchical orchestration system with slicing support can be a viable approach to address the requirements of that scenario, and we presented this unique and new configuration where services are deployed across distributed domains."
"rhino is an open-source javascript engine entirely developed in java. rhino converts javascript scripts into objects before interpreting them. it works in compiled as well as in interpreted modes. it is intended to be used in server-side systems. rhino can be used as a debugger by making use of the rhino shell. rhino version 1.5r4.1 has 111 classes, 94, 078 loc, and 41 bug reports. there are 92 manually built links between bug reports and classes."
"to test our conjecture, we propose libcroos, an approach that uses linguistic (textual) and bcrs of object oriented systems, to improve the accuracy of ir techniques. in libcroos, an ir technique creates a set of so-called baseline links between a bug report and the classes of a program. then, each bcr among the classes acts like an expert to vote on the baseline links. the higher the number of experts voting [cit] for a baseline link, the higher the confidence in the link, and the higher libcroos puts that class in its ranked list. thus, libcroos is a complementary approach to any ir technique, which is, to the best of our knowledge, the first approach to use bcrs as experts to vote on the links between bug reports and classes."
"as we have seen that developers/testers, and in particular those that are new to call centre domain and telephony, have experienced difficulties while testing ivr applications especially for above mentioned functionality of ivr applications. there should be some deciding factors while deciding between automated and manual testing such as extent of human interfere, nature of application, knowledge level of testing team, time constraint, environment etc. it is good to automate the test execution but not on the expense of customer experience, complexity and cost. customer satisfaction is a key goal of every organization. the real challenge comes in maintaining balance between customer expectations and investment in business to meet expectations. this is completely true for ivrs. by using the right testing strategies and proven best practices as illustrated in the paper, organizations can avoid difficulties that can result in financial losses and customer dissatisfaction."
"lack of inconsistent standards proprietary hardware, and third-party components testing b iometr ic p attern: -automation tools are not designed to store biometric pattern for the called for speaker verification. speaker verification means an ivr caller should be able to use the ivr system, even if he doesn't remember his account number. as ivr can automatically verify the customer based on matching the biometric pattern. this is unique like the finger prints. investing on features like speaker recognition."
"if, at any point, the monitoring team does not get the right response from the ivr or the response time exceeds a predefined limit, they can immediately call/page to technical support resources to correct the problem. the page can specify which ivr system/application the problem was encountered in and where in the call flow the problem occurred. this can mean significant time savings in finding and correcting the source of the problem and getting ivr applications back in service which would in-turn, lead to improved customer satisfaction and cost savings ."
"we perform an empirical study on four programs and with two state-of-the-art ir techniques to assess the accuracy of our proposed approach for bug location. this study provides data to assess the accuracy, in terms of ranking, of the improvement brought by libcroos over two \"traditional\" ir techniques, using lsi and vsm alone."
"have you ever observed ivr problem is not due to ivr application failure but due to external problem like system and network problems like database down, updated patch applied on some operating/software system, backend interface down or due to some application dependencies etc. the most common issue that has encountered with most of the organization is backend database failure. in this case ivr applications will not be able to fetch caller data from database and result in call drop/call hang-up/call transfer to call representative, all result in loss of customer/revenue. from customer point of view, ivr application is not working properly but this is not the case."
"a slice is an aggregated set of resources that can be used in the context of an end-to-end networked service comprised of vnfs. slices are composed of multiple resources which are isolated from other slices and allows logically isolated network partitions, with a slice being considered the basic unit of programmability using network, computation and storage. when considering the wide variety of applications to be supported by 5g networks, it is necessary to extend the concept of slicing to cover a wider range of use-cases than those targeted by the current sdn/nfv technologies."
"if we have slicing everywhere, including networks and dcs, we observe the following attributes: (i) there is a separation of physical resources; (ii) there is isolation of services as no customers share physical resources; and (iii) it is secure as only specified customer can access host, no sharing or cross vm issues."
"vi. conclusions this paper considered emerging 5g scenarios where different segments of a service provider's infrastructure -from the edge cloud to the central dc -are administered by separate organisational divisions / departments, each possibly relying on a particular mano systems for the deployment of the end-toend services on the distributed resource infrastructure. whilst each domain has its own mano, the manos are configured in a north-south way creating a hierarchy of service provision capabilities, called the hierarchically structured service provider (hsp). this approach works particularly well where each domain, from the mobile edge, to the core dc, can be managed independently of the others, but needs to be combined to form slices."
"another potential problem could be line on which these ivr applications are running goes into down state. here caller will hear some music or error message like \"all of our customer care representatives are busy in handling another customer. please call back late \"and call will get disconnected."
"if slicing is done in the orchestrator, which uses an inter-domain orchestrator api interaction and / or a peer to peer approach, a slice is closer to a small abstraction in the orchestrator, rather than an infrastructure partition. there are various projects and initiatives that are doing slicing at the orchestrator level. these include 5g-transformer [cit], 5g pagoda [cit], slicenet [cit] . their approach has the easiest entry position, but all the main software elements need to be updated and adjusted to know about slices; and all of the apis, the modules, and internal function paths, and the data structures need to be adjusted and adapted to factor in slices."
"step 2: bcrs among classes recovering (bcrs recovery): using padl and based on an extensive literature review [cit], we implemented analyses to uncover bcrs in the source code of programs and make them explicit in their padl models."
"each of these slices will be allocated and de-allocated in an on-demand fashion under software control via interacting with the slice controller. each slice will get its own vim and not have management as part of shared one. as each vim instance is independent of the others, the best type of vim can be selected for deployment according to the type of resources available in each part of the segmented end-to-end infrastructure. figure 1 presents how the resources of a dc are isolated from each other, and how a slice controller is involved in such a process."
"a. 5gex-sonata multi-mano system figure 4 depicts the architectural view of the considered inter-mano scenario. the implementation is based on the 5gex mano system at the top level of the hierarchy, playing the role of the highest level orchestrator. this choice represents the straightforward design approach as the 5gex system was originally conceived and built to be a multi-domain orchestrator. at the lower level of the orchestration hierarchy, an instance of the sonata service platform is deployed to operate on the resources of its own segment of the nfvi, and behaving as a local domain orchestrator, connected to the top 5gex mano via the sonata domain adapter."
"the literature [cit] showed that information retrieval (ir) techniques are useful to link features, e.g., bug reports, and source code, when a bug is defined as a feature that deviates from the program specification [cit] . however, ir techniques lack accuracy in terms of ranking [cit] . in this paper, we conjectured that whenever developers implement some features, they use some bcrs among the different classes implementing the feature. thus, combining bcrs with ir techniques could increase their accuracy in terms of ranking, to help developers with their bug location tasks. to verify our conjecture, we proposed a new approach, libcroos, that uses bcrs and textual information extracted from source code to link classes and bug reports. to the best of our knowledge, libcroos is the first approach to use bcrs as experts to vote on the links recovered by an ir technique. in this paper, we considered four commonlyused relationships, i.e., use, association, aggregation, and inheritance, and two ir techniques, i.e., lsi and vsm, to link classes and bug reports."
"in the 5g landscape, a provider or operator can deploy network services whose elements could potentially span the whole infrastructure from the mobile edge to the core. moreover, the resources utilised in those segments of the infrastructure could be managed by a particular mano system that an organisational unit of that provider is not willing to replace or update. the instantiation of full end-to-end services can be achieved via an inter-mano interaction, based e.g., on a north-south interface interaction between the involved orchestration systems. each north-bound mano system interacts with its south-bound counterpart as it would do with a vim (virtual infrastructure manager)."
 state recall allows application to recall the last status and avoid user repeating the already completed steps of the incomplete transaction. this feature is very useful in application where a transaction spans across many steps such as payment/billing processing applications.
"after the slice creation process in each part of the distributed nfvi is completed, the sonata domain adaptor interacts with the sonata gatekeeper api (step 2c on figure 4 ) to gather both resources available from the allocated slices and a list of services and functions from the sonata catalogue."
"static blts often use information retrieval (ir) techniques [cit] to link bug reports to classes. ir-based techniques, in particular the commonly-used latent semantic indexing (lsi) [cit] and vector space model (vsm) [cit], have proven useful for bug location [cit] . an ir technique takes as input some text extracted from a program class through static analyses. then, it computes the textual similarity between the class and the bug report. a high textual similarity means that the class and bug report share several concepts [cit], i.e., they are likely to be related to one another. developers should consider classes highly-similar to a bug report first during their bug location task because they are more likely related to the bug and, hence, cause of the bug."
we therefore present the 5gex and sonata manos to highlight their relevant functions and apis that are of interest from a design and implementation perspective of the considered scenario.
"did anyone think why ivr monitoring is needed to get the benefits of ivr? in today's business development most of the organizations rely on their ivr applications to resolve customer queries, handle various transactions etc. but the question is how they would get to know if their customers are having some issues using their ivr system?"
"personalization testing: -there is no industry standard tool available in market to test \"personalization\". by way of personalization caller shall have the ability to create/modify/delete his own profile. profile may include information relating to language option, ability to set various billing thresholds, feedback alert mode (sms, email, call back) etc. small things like personalized greeting for an authenticated customer or specialized greeting in case it happens to be callers birthday/wedding. here tester has to setup/feed the personalized data and test the ivr application based on that."
"for rq1, we consider the four bcrs, i.e., use, association, aggregation, and inheritance, to analyse how much libcroos can decrease the rank of classes to put culprit classes closer to the top of the list and, hence, can decrease a developer's effort. consequently, we apply libcroos, lsi, and vsm approaches on the four programs seeking to reject the two null hypotheses: h 01 : there is no statistical difference in terms of ranking between libcroos and vsm. h 02 : there is no statistical difference in terms of ranking between libcroos and lsi. for rq2, we use libcroos with only one bcr at a time to observe which relationship helps more than the other to put the culprit classes at the top. consequently, we apply libcroos, lsi, and vsm approaches on the four programs seeking to reject the two null hypotheses: h 03 : all the bcrs equally improve accuracy, in terms of ranking, over vsm. h 04 : all the bcrs equally improve accuracy, in terms of ranking, over lsi. we use the approaches, either a libcroos, lsi, or vsm, as independent variables and the rankings of the approaches as dependent variables to empirically attempt rejecting the null hypotheses."
"root c ause an alysis: -even after you discover that you have a problem with one or more of your ivrs, finding exactly where the problem is and correcting it can pose a significant challenge. it can take hours just to identify the source and cause of an ivr problem. with manual testing it is very easy to locate the bottleneck using some trace mechanism or following the call flow."
customer profile database or other customer data sources (for performing the database lookup) to get the customer personal as well as other information like billing information.
"mucommander is a lightweight, cross-platform file manager with a dual-pane interface. it runs on any operating system with java support. it supports 23 international languages. mucommander supports virtual filesystem as well as local volumes and the ftp, sftp, smb, nfs, http, amazon s3, hadoop hdfs, and bonjour protocols. mucommander version 0.8.5 has 1, 069 classes, 124, 944 loc, and 81 bug reports. there are 231 manually built links between bug reports and classes."
"libcroos uses some ir techniques as an engine to create links between bug reports b and classes c. libcroos is not dependent on a particular ir technique, any ir technique could be used with libcroos. ir techniques consider both bug reports and classes as textual documents. for source code, we use a parser, e.g., a java parser, to extract all source code identifiers. the parser discard extra information, e.g., data types, from the source code [cit] . ir techniques extract all the terms from the documents and compute the similarity between two documents based on the similarity of their terms and-or the distributions thereof."
"theoretically, we assume that a bcr exists between two classes if any method of one of the two classes invokes at least one method of the other. then, we define four properties of any potential bcr but inheritance: we exclude inheritance because it is explicit in the source code of programs in c++ (through the : syntax) and java (through the extends keyword). we need dedicated analyses to recover all bcrs but inheritance because, in mainstream programming languages, such as c++ and java, these relationships are not explicit in the source code but implemented by developers from the design documents using various idioms."
"generating corpora: we download the source code of jabref v2.6, lucene v3.1, mucommander v0.8.5, and rhino v1.5r4.1 from their respective cvs/svn repositories. we generate corpora of all the programs for ir techniques to link bug reports to classes. to generate corpora, we extract source code identifiers using a dedicated java parser [cit] to extract all source-code identifiers. the java parser builds an abstract syntax tree (ast) of the source code that can be queried to extract required identifiers, e.g., class, method names, and so on. only for lucene, we download bug reports from the jira bug repository. for all the other programs, we used the bug reports provided by the other researchers [cit] . we only use long description of bug reports."
testing state rec all: -automation tools are not designed for advance features like state recall which can help in the event when the transaction was incomplete because of any reason such as user hanged up. (e.g. caller received a call on his other phone and had to drop the call in between a transaction) system issues (call drops because of an issue with the ivr itself)
"the following section provides further details on the considered multi-mano interaction, highlighting the detailed interworking of the systems, based on the data models, abstractions and interfaces that have been used for its implementation. figure 4 shows the different steps of the interaction between the relevant components of the 5gex and sonata mano frameworks and how have been used to build the hsp scenario."
"yet, to the best of our knowledge, no previous authors considered binary-class relationships (bcrs). bcrs include use, association, aggregation, composition, and inheritance. we conjecture that the existence of bcrs among classes is a useful information to re-rank the list of culprit classes. our conjecture stems from the observations that developers express the design and implementation of their programs in terms of classes and the bcrs among these. for example, the classes playing a role in implementing the \"find/replace\" feature in a program would probably be related through some bcrs. we therefore study the impact of using bcrs to improve the ranking of classes to help developers during their bug-location tasks. bcrs are extracted from source code using static analyses. we exclude from our study composition bcr that, in general, require dynamic analyses."
"preliminary to any software maintenance task, a developer must understand the source code. due to incomplete/missing documentation, software evolution, and software aging, developers may have difficulties to comprehend the source code. program comprehension-related activities are involved in 50% to 90% of all software maintenance activities [cit] . thus, these difficulties to understand source code increase maintenance costs."
"the professional monitoring/testing tools can be used as an automated testing tool to test/monitor ivr/speech applications by making calls proactively to identify the issue. these tools take end to end call flow in terms of recorded scripts along with duration, default speaks, duration as an input. tool then periodically makes the call to production system (execute scripts) and compares recorded behavior to call behavior. in case there is any mismatch, tool sends the alarm to applicable support teams. based on alarm, support team can analyze the alarm and fix errors .tools also take call in number, call frequency, support team information, server information (where the call will land) as an input."
"consequently, there are inherent trade-offs when selecting one or the other slicing approach. the actual decision on which slicing approach will depend on various key aspects of the service requirements under consideration, and can be focussed on the technical desires of the provider, together with the technical abilities and technological choices of the tenants."
"we do not use precision and recall, because, on the one hand, the ir techniques would link all the bug reports to all the classes and recall would always be equals to 100% and because, on the other hand, using a high threshold value [cit] based on textual similarity and linking only some bug reports to classes would yield a high precision. thus, to compare libcroos, lsi, and vsm, we use the rank of the first related class to a bug report as a measure of accuracy [cit] . to answer rq1, we compare libcroos generated ranked list with lsi and vsm generated ranked lists of each bug. to answer rq2, we use libcroos with only one bcr at a time to compare its ranked list with lsi and vsm's ranked lists. each approach used the same bug reports and classes. all null hypotheses have been tested using a paired, nonparametric test mann-whitney test. we use a significance level of 95% for all the statistical test. we use the mann-whitney test to assess whether the differences in accuracy, in terms of ranking, are statistically significant between the libcroos, lsi, and vsm. mann-whitney is a nonparametric test and, therefore, it does not make any assumption about the distribution of the data."
"thus, we answer rq1 as follow: libcroos helps to decrease the rank of culprit classes and put culprit classes higher in the ranked lists when compared to \"traditional\" ir techniques alone."
each of these slices will be allocated and de-allocated in an on-demand fashion via interacting with the slice controller. as each slice will be separate from the others and will include its own vim instance the dc resources will also be isolated from each other.
"it is very also very difficult to test speech applications using automation tools due to following factors:  lack of in tegr ation -voicexml engines, development tools, call processing framework, telephony hardware, web services and object libraries all come from different vendors. automate testing of speech applications in this integrated environment is not practical as these are not designed to work easily together.  interacting with the caller through natural language as clear and unambiguous wording of a prompt is a key contributor to application success. to automate the test cases while side speechuser's tendency to talk to others while on the call is very complex and required lots and lots of thinking. in these cases, the application will not be able to recognize the user input and goes into a rejection or error state and may take users back to the main menu  designing, documenting and then incorporating test cases in automation tool is very time consuming and cumbersome task as the user input is not limited to some digits. this will put extra burden on application testing and lots of effort has to put even if for simple change request."
"even after you realize that you have a problem with one or more of your ivr system, finding exactly where the problem is and correcting it can pose a significant challenge identify the serious customer service problems and higher expenses caused by \"out of order\" ivr applications, some organizations have started manually monitoring their call centre ivr applications with the goal of reducing ivr downtime. a group of people with knowledge of ivr functionality monitors ivr systems by dialing into each ivr application on a planned basis to ensure that everything is working properly and customers are not encountering problems. this approach, though time consuming and expensive, has helped to reduce the impact of undiscovered ivr outages. by finding and fixing ivr problems more quickly, fewer calls overflow from out-of-service ivrs to call center csrs [cit] . keeping more calls in the ivrs reduces overall transaction costs and more than pays for the cost of physically monitoring the ivr applications."
"the relation model is the part of libcroos that provides the bcrs between classes. in libcroos, each bcr is treated as an expert that votes on the links recovered by the ir engine. the relation model takes as input the set l provided by the ir engine and the source code or binary code of the program. it produces as output the set q using analyses based on models of the source code [cit] ."
"object oriented programming is a method of implementation in which programs are organised as a collection of collaborative objects [cit] . as the real world entities, classes in object oriented programs do not exist in isolation; they cooperate through the bcrs between them. the main bcrs are inheritance, association, aggregation, and using (use relation) [cit] . in our study, we will consider these four bcrs. in object oriented programs, relationships are as important as the objects themselves [cit] . class relationships allow classes to share data, to define more complex structures or to participate in the implementation of a program feature [cit] . therefore, classes involved in the implementation of a feature (program behaviour) are probably linked by class relationships. program behaviour that deviates from its specification is called a bug [cit] . thus, to locate relevant classes involved in the occurrence of a bug is similar to locating classes involved in the implementation of the feature that has not been correctly implemented. based on this observation, we believe that using information about bcrs to locate a bug can be helpful."
"to manifest this slice approach, we have designed and built a dc slice controller which is able to allocate a slice of a dc and create a per-slice vim (virtual infrastructure manager) in an on-demand fashion. each slice and its associated vim are independent of the other slices and vims. in this way, customers will never share servers, and the worry of vms of one customer interacting or spying on another customer will be eliminated. also, the issue of one customer's vm consuming all the resources and starving other customer's vms is ameliorated to some extent."
"information retrieval (ir) techniques, bug, concept, feature location as well as binary class relationships are related to our research work. bug or feature location is a search activity, whether a developer searches the source code to find the classes that are playing a role to implement a feature or cause a bug."
"the ptidej tool takes as input the c++ or java source code or binaries of programs and generates padl models of these programs. the ptidej tool suite essentially divides into a set of parsers, an implementation of the padl meta-model, and language-dependent extensions to the meta-model to integrate, within a padl model, constituents particular to a programming language. for example, the padl meta-model does not define a constituent to describe c++ destructors, this constituent is provided along with the c++ parser to allow the modelling of c++ programs with possible highest precision. the c++ and java source code parsers are based on the parsers provided by the cdt and jdt eclipse plugins. the java binary code parser uses the bcel library."
"we select the four open-source programs, jabref, lucene, mucommander, and rhino because they satisfy several criteria. first, we select open-source programs, so that other researchers can replicate our experiments. second, we avoid small programs that do not represent programs handled by most developers. third, three of the programs have been used in previous studies by other researchers [cit] for possible comparisons. finally, three programs come with independent, manually-built oracles, which mitigates some of the threats to the validity of our results. only for lucene, we recovered links between bugs and source code."
"we have statistically significant evidence to reject the h 01 and h 02 hypotheses. the p−values, for all the comparison of libcroos vs. lsi and libcroos vs. vsm, are below the standard significant value, i.e., 0.05. results of rq1 support our conjecture that adding bcrs with ir techniques helps to improve the accuracy, in terms of ranking, of ir techniques. thus, we conclude that using bcrs among different classes that implement a same feature (or participate to a same bug) with ir techniques yields better accuracy than \"traditional\" ir techniques alone in bug location."
here we describe the dc slice controller which supports data center slicing and the vim on-demand model. we present each of the main components and describe their functions. the slice controller has the following elements for its operation:
"in the search results, developers tend to look at the top few results only [cit] . many researchers have proposed automated and semi-automated approaches to facilitate developers to locate a feature or a bug. all the proposed approaches could be divided into three categories, dynamic, static, and hybrid. static approaches have a benefit over dynamic and hybrid approaches that they do not require compilable program. static analyses [cit], execution traces [cit], and ir techniques [cit] have been used by researchers since the early works on feature location [cit] . often, ir techniques [cit], use vector space models, probabilistic rankings, or a vector space model transformed using latent semantic indexing. whenever available dynamic data [cit] proved to be complementary and useful for traceability recovery by reducing the search space. recently, high-level documentation was mapped into source code features using a variety of information sources [cit] ."
"the ivr is connected to: cti server (for attaching customer entered digits and data dips, getting dnis [cit], performing transfers). it is used for collecting useful information from a caller before the call is transferred to an agent."
"in order to support service provisioning over these slices, it is necessary to have mechanisms that implement the slicing of the network resources and the data center compute and storage resources across the different segments of a provider's infrastructure. to manifest this slice approach, we have designed a built a dc slice controller which is able to allocate a slice of a dc and create a per-slice vim in an on-demand fashion. the dc slice and the vim are provisioned solely for use with the service. each slice and its associated vim are independent of the other slices and vims. in this way, customers will never share servers, and the worry of vms of one customer interacting or spying on another customer will be eliminated. also, the issue of one customer's vm consuming all the resources and starving other customer's vms is also ameliorated to some extent."
"the key point of this specific multi-mano interworking solution is in the interaction between the sonata domain adapter in the 5gex framework (left-hand side of figure 4 ) and the gatekeeper in the sonata sp (right-hand side of figure 4) . the sonata domain adapter utilises the rest api exposed by the gatekeeper, and provides the 5gex resource orchestrator with an abstract view of the resources and services of the underlying slices in the sonata domain, via the i3-rc interface introduced in section iv-a. more specifically, the services and the vnfs available from the sonata service platform are exposed by the sonata domain adapter as domain capabilities to the 5gex resource orchestrator, together with an abstracted view of the resources available from the allocated dc slice."
these tools can be used to performance/load testing. it requires peak amount of load to be put on production servers but amount of t1 lines/ports are limited. the number of simultaneous calls at any single point of time is directly proportional to number of ports. moreover blocking all ports at a given time is not viable. here we can use the load gene ation tool. it can be used both for dtmf as well as speech applications.
"an end-to-end service instantiation request can now be issued to the 5gex resource orchestrator, which will split the required end-to-end service elements on the lower-layer resource domains according to the capabilities and resource availability reported by each of them (step 3b on figure 4) . next, embedding occurs where lower level service instantiation requests are sent to the individual domain adapters according to the embedding decisions (step 4 on figure 4) ."
"oracles: we use previously built oracles to verify if a class linked to a bug is true positive or false positive link. some researchers [cit] manually and-or semiautomatically created links between bug reports and methods to create oracles. in this paper, we link bug reports to classes, thus we converted the oracles at class level. in the case of lucene, we download bug reports and svn logs from jira repository. developers usually write bug ids in svn logs [cit] when they fix a bug. we automatically extract bug ids from svn logs to link bug reports to the classes."
"we also observed that, as the size of the source code increases, ir techniques produce larger ranked lists, which increase the difficulty of developers' bug-location tasks, because they must manually iterate through more potential culprit classes. libcroos automatically increases the rank of non-culprit classes and brings actual culprit classes closer to the beginning of the ranked lists. our experiments showed that the size of the source code does not impact the accuracy of libcroos. in the contrary, the more relationships among classes, the greater the accuracy of libcroos. our empirical study helps developers and practitioners to understand how bcrs could be used for bug location. in addition, it suggests that developers must use more relationships among classes contributing to a same feature."
"in libcroos, we take the processed corpora as input to link the bug reports with classes. any ir technique could be used in libcroos to link bugs to classes, as base line to start the process. in this paper, we use the lsi and vsm ir techniques. for each bug report, lsi and vsm generate a ranked list of classes. each ranked list contains potential culprit classes in descending order. we use our relation model to analyse the relationships between the classes of each bug's ranked list. we put the classes in four sets, i.e., 3, and q i,4 (see section iii-c), depending on the bcr by which they are linked to other classes in the ranked list. if a class does not have any bcr with other classes, we do not keep that class in any of the sets. thus, each set is treated as a different ranked list. indeed, we have four ranked lists for each bug. we combine all ranked list as described in iii-d using equation 1."
"this paper is organised as follows. section ii provides a brief description of the state-of-the-art blts. section iii describes proposed approach in details and sketches our implementation. section iv provides details on our empirical study. section v reports the results, discussions, and threats to the validity of our findings. finally, section vi concludes with future work."
"although class relationships are essential in the implementation of features and for program comprehension tasks, they are not all explicit in the source code [cit] . it is not an obvious task to recover class relationships in source code. indeed, many researchers propose various approaches [cit] to extract class relationships in the source code. part of our approach is based on the approach proposed by guéhéneuc [cit] . in this approach, authors formalised bcrs based on 4 independent-language properties: exclusivity, receiver type, life-time and the number of instances. using these properties and specific algorithms, they were able to recover class relationships in the source code. they provided this technique in the ptidej tool suite 1 [cit] . to the best of our knowledge, none of previous work performed experiment to analyse what are important bcrs, in particular at class-level, for bug location. in addition, how bcrs could be combined with ir techniques to improve the accuracy, in terms of ranking, for bug location. the work presented in this paper is complementary to the existing ir blts, because it exploits the bcrs of object-oriented programs to improve the accuracy of ir techniques."
"the 5gex i3 interface (from management to domain orchestrator) was designed and implemented to allow the interaction between a mdo and the underlying domain orchestrators. in particular, the sub-interface i3-rc is responsible for retrieving the local resource availability from the technological domains and for enabling their programmability. i3-rc (resource control) is based on the 5gex common domain abstraction api [cit] . a model called the bis-bis node model is considered to represent the resources and capabilities of each domain. the i3-rc interface and bis-bis abstractions play a key role in the implementation of the north-south interface interconnecting the mano systems across domains, which are considered in this hsp scenario (as further described in section v)."
"can't check other required processes/utilities that should run for proper functionality of ivr applications e.g. sqlclnt is a process which should run on unix servers so that ivr applications can directly interact with oracle database but in case this process is not running or in down status then automation tool will simply fail that test case saying data not found. again effort has to be put to analyze why test case had fail. most of the problems are not due to ivr applications/ivr components but they are due to system and network problems external to the ivr like cti, middleware's, mainframe database down etc. in these scenarios, automation tool while testing ivr applications mark test cases failed. in these scenario's, developer have to have check whether cti is working or not, mainframe database is up (mainframe slowdowns and outages impact database lookups such as authenticate account, bill enquiries, bill payment etc.). another scenario might be lines on which ivr applications are attached are not in up (running) or in busy state or whole t1 is out of order. here also, developer/tester needs to manually check line status. another situation is cti is down. in this case customers will experience hold times, hang up, and try again later errors. various network and system management tool will not be able to detect/indicate problems of these types."
"we do not recall here the sets of values for each properties for lack of space and because the reader may find all details in a previous work [cit] . also, we do not further consider composition because it requires dynamic information that would either be gathered through dynamic analyses or through incomplete static analyses. these properties and their values essentially allow identifying the various idioms used by developers to implement bcrs."
"external validity: the external validity of a study relates to the extent to which we can generalise its results. our empirical study is limited to four programs, jabref, lucene, mucommander, and rhino. yet, our approach is applicable to any other object-oriented programs. however, we cannot claim that the same results would be achieved with other programs. different programs with different usage patterns of bcrs, source code structure, and identifiers may lead to different results. however, the four selected programs have different source code size, different number of bcrs, and identifiers. our choice reduces the threat to the external validity. the results suggest that inheritance helps better to put the culprit classes at the top. we explain this observation by the fact that inheritance is the most accurate bcr in the model because inheritance is explicit in the source code and actually expresses exactly what the developers want. for other bcrs that are not explicit in source code and recovered with the help of some algorithms, it may not always be the developers' intent to have these bcrs. also, the analyses may miss some relationships or add more relationships than expressed in the design. thus, more case studies are required to generalise our results. conclusion validity: conclusion validity threats deals with the relation between the treatment and the outcome. the appropriate non-parametric test, mann-whitney, was performed to statistically reject the null-hypotheses, which does not make any assumption on the data distribution."
"section ii reports about the existing slicing approaches based on the analysis of current the state-of-the-art. section iii presents the concepts and the design characteristics of the components implementing the slicing functionalities in the considered multi-mano scenario. sections iv and v describe the architectural elements, interactions and initial results that are relevant for the considered multi-mano scenario."
this section provides an overview of how the different mano architectural elements described in section iv have been combined in order to build a novel hierarchical sliceenabled multi-mano environment for the deployment and management of end-to-end 5g network services.
"we performed the following analysis on the libcroos, lsi, and vsm ranked lists to answer our research questions by attempting to reject our null hypotheses."
"-interactive voice response applications allows customers to call and navigate through various instructions to check on account, pay bills, order service etc. ivr applications interacts with the caller to determine (via collection of customer entered dtmf digits) who is calling and the reason for the call. the ivr can also provide automated services to the customer e.g. automated bill inquiry. the ivr acts as a play-and-collect box where the only call routing logic exists to provide fall back routing capability. the ivr performs a database lookup based upon the collected digits and passes the customer data to the cti [cit] infrastructure associated with the call."
"to evaluate the effectiveness of our proposed approach, we performed an empirical study on four software programs, i.e., jabref, lucene, mucommander, and rhino. we compared libcroos-generated ranked lists of classes with the ranked lists produced by lsi and vsm alone. the results achieved in the reported empirical study showed that, in general, libcroos improves the accuracy of lsi and vsm. in all experiments, libcroos improved the accuracy of the ir-based technique and could reduce developers' efforts by putting actual culprit classes at the top in the ranked lists. we also used libcroos with only one bcr at a time to analyse which bcr helps more to improve the accuracy of the ir techniques. in the majority of the programs, we observed that inheritance is a more important relation than aggregation, use, and association."
"slicing is a move towards segmentation of resources and deployment of nfv for the purpose of enhanced services and applications on a global shared infrastructure. in order to support service provisioning over a slice enabled distributed nfvi (for nfv infrastructure), mechanisms that enable the slicing of the whole end-to-end infrastructure -from the mobile edge to the core dc -including network, compute and storage resources are required. in this paper we consider the extension of the above mano-to-mano north-south interaction to support the concept of slicing and we present an implementation of this scenario based on existing mano systems that have been used in the context of 5g."
the following sections describe the slicing model based on the second approach and making use of the new concept of data center slicing and vim on-demand. we present how a data center can be sliced and how a vim on-demand can be allocated for the slice.
"construct validity: construct validity concerns the relation between theory and observations. the degree of imprecision of automatic bug reports to class traceability link was quantified by means of a manual validation of the ranked lists generated by the approaches, using manually-built oracles. we did not build the oracles for three programs. thus, there is no bias in the links between bug reports and classes. all three corpora have been used by various researchers and manual oracles have been verified to avoid imprecision in the measurement. only for lucene, we automatically built the oracle by mining jira software repository. we used lucene to mitigate the threat of imprecision of manually built oracles."
"we have statistically-significant evidence to reject h 03 and h 04 . the p−values, for all the comparison of libcroos using one bcr vs. lsi and vs. vsm, are below the standard significant value. the results show that using only one bcr also improves the accuracy, in terms of ranking, of ir techniques for bug location. libcroos using only one bcr helps to decrease the ranks of culprit classes and put culprit classes higher in the ranked lists, when compared to \"traditional\" ir techniques alone. however, all the bcrs have different importance for bug location."
providers can explore new business models by federating their resources and service offerings in order to provide the customers with the ability of instantiating end-to-end services across multiple technological domains. these services can be located either in separate geographical locations of one provider or outside of its own administrative borders.
"on initialization, in the branch that makes use of the 5gex mano system, the vlsp domain adaptor creates a slice and then collects local information (step 2a on on figure 4 ). also, in the branch using the sonata mano, the slice creation process is driven by the infrastructure abstraction (ia) interacting with the slice controller (step 2b on on figure 4 ). on-demand instances of the lightweight vlsp vim are allocated as part of the dc slice and reference to them are returned to the ia."
"the end-to-end service request includes service elements available from the sonata domain, and so the sonata platform will receive a service instantiation request from the sonata domain adapter on the relevant endpoint of the gatekeeper api (step 5 on figure 4 ). from the sonata sp perspective, the request will look like a normal service request, such that the normal sonata service deployment flow will follow the request, instantiating the entities as described in the package on-boarded by the service developer in step 1."
"tool stand ar d: -automation tools have been built based on some guidelines that make it difficult to test each different scenario's. here in case of ivr testing, we cannot left out any untested scenario and put the code in production which could result in backout's."
"the 5gex mano operates over its own segment of the nvfi using the data center slicing approach discussed in section iii: the on-demand allocation of slices including lightweight vlsp (very lightweight network & service platform [cit] ) vim instances is triggered by the vlsp domain adapter interacting with the local slice controller. the vlsp domain adapter in this scenario plays the same role played by the slice orchestrator component in figure 1 . in a similar way, the sonata sp operates on top of another segment of the nfvi, again using the data center slicing capabilities and slices including on-demand instances of the vlsp vim. in this case, the sonata's own infrastructure adapter drives the creation of those slices by interacting with the local slice controller and offers to the sonata mano the interface to interact with the on-demand allocated vim in each slice."
"a particular task during which these difficulties slow down developers is that of bug location. to fix a bug, developers must locate the bug in the source code of a program: they must identify the classes (and-or parts thereof) describing the unexpected behavior of the program. therefore, they must understand the source code enough to identify the culprit classes and modify them to fix the bug. effectively automating this task could reduce maintenance costs by reducing developers' effort [cit] ."
"lucene is an open-source high-performance text-based search-engine library written entirely in java. it is a technology suitable for nearly any application that requires full-text search, especially cross-platform. lucene version 3.1 has 434 classes, 111, 117 loc, and 89 bug reports. we select lucene v3.1 because it contains more closed bugs than other versions and these were linked to its svn repository. there are 235 manually built links between bug reports and classes."
the ranker assigns weights to different bcrs and similarity computed by ir engine to re-rank the classes linked to a bug. the ranker takes the set l generated by the ir engine and the set q generated by the relation model as input.
"international journal of computer volume 4 -no.6, [cit] for a customer call to be successful, all system components like telecom switch, ivr application, database, network, and cti must work together seamlessly. to ensure seamless performance, most organizations test their system in the preproduction environment before deploying to production environment and production environment prior to large scale deployment. yet, for multiple reasons, including time or expertise constraints to poor planning or execution, these tests fail to capture all the problems and truly prove that the voice system can meet the business objectives it was designed to support. as a result, costs go up, and customer quality of experience suffers. problems are left for the customers to detect and for technical teams to scramble to resolve. proper testing techniques and execution can help improve the performance of any step in the call flow of ivr applications. note, however, that ensuring quality does not end with successful pre-production and production testing. because a contact system will undoubtedly undergo software and hardware changes during production, ongoing monitoring is also very important. it will immediately alert contact center operations personals when problems occur, providing them with real-time performance data they can use to resolve these problems quickly basis. this document aims to provide some key inputs to assess the current testing practices and recommend industry best practices to improve effectiveness of ivr applications and enhance end-user experience as well-tuned applications can result in savings of millions of dollars per year. automated test tools are powerful support to improving the return on the testing investment when used wisely but for ivr applications there is a need of human judgment to assess the accuracy of the result or extensive, ongoing human intervention to keep the test running. automated testing is an enormous investment, one of the biggest that organizations make in testing. tool licenses can easily go upto six to seven figures with training, consulting, and expert contractors can cost more than the tools themselves. then there's maintenance of the test scripts, which generally is more difficult and time consuming than maintaining manual test cases. moreover, the organizations do enable isn't compatible with other vendors' products. automation solutions doesn't allows to leverage already developed some of already existing scripting work, difficult to enabling those scripts and programs to be run through and stored in automation framework as well.  testing dtmf applications interacting with speech based applications: sometimes dtmf/legacy applications has to interact with speech applications as there could be some possible scenario's wherein organization is transforming their dtmf/legacy technology into enterprise structure and some of the functionality has not been migrated."
"we now present the details of our approach for bug location using bcrs, which uses textual information and bcrs extracted from the source code to link classes to bug reports. figure 1 shows the high-level view of libcroos. it has three main modules, i.e., an ir engine, a relation model, and a ranker. we give some details about the abstract model of libcroos and then explain each module."
"we also define ψ i,j, a function that computes the final similarity between a class c j and a bug b i by combining the vote of each bcr, i.e., inheritance, association, aggregation, and use relation, and ir technique, as:"
"this new slice-based orchestration approach on the one hand will provide a more effective resource management thanks to the isolation introduced at the control and data planes; on the other hand, slicing will facilitate the implementation of new emerging 5g scenarios where customers look for seamless mobility across the whole infrastructure: the implementation of end-to-end slices that include different locations of the edge infrastructure can help supporting physical / virtual mobility of devices, network and services."
"this section describes the architectural elements that have been utilised for the design of a multi-mano hierarchically structured service provider (hsp) scenario on a sliceable nfvi. this scenario was devised during a 5g-ppp crossprojects activity, which led to the implementation of a proofof-concept demonstrator able to instantiate end-to-end network services via a 5gex [cit] based mano and one or more instances of a sonata [cit] mano (i.e., the sonata service platform) on multi-domain end-to-end slices."
"before deciding any strategy on monitoring ivr performances, it's very important to note some key attributes:- there are tools available in market which can be used for continuous quality monitoring of ivr applications. these tools can be scheduled to dial into ivr application on a planned basis to verify everything is working accurately. after the call gets over, someone from qa team can verify if call was successful or not. they can verify it through log generated by ivr applications. these tools need ivr call flow as an input and some threshold limit so that if the ivr application exceeds the threshold limit, the monitoring team can immediately page to technical support resource to correct the problem."
"the research questions that our empirical addresses are: rq1: does libcroos provide better accuracy, in terms of ranking, than ir techniques? rq2: what are the important bcrs that help to improve ir techniques accuracy more than the others? to answer our research questions, we perform four experiments on jabref, lucene, mucommander, and rhino using libcroos, lsi, and vsm. we use a measure of ranking [cit] to measure the accuracy of proposed and ir techniques to answer our research questions. lsi and vsm return ranked lists of classes for each bug in descending order of the textual similarities between the bug report and the classes. libcroos returns a similar ranked list in descending order of the similarities computed using equation 1. if a culprit class is lower (has a higher rank) in the ranked list then a developer must spend more effort to reach this actual culprit class because she must assess more candidate classes to solve the bug. thus, the higher is a culprit class in a ranked list (decreased rank), the less is the developers' effort and the more accurate is an approach."
"jabref is an open-source bibliography reference manager. the native file format used by jabref is bibtex, the standard latex bibliography format. jabref runs on the java vm (version 1.5 or newer), and should work equally well on windows, linux and mac os x. jabref version 2.6 has 579 classes, 287, 791 loc, and 36 bug reports. there are 108 manually built links between bug reports and classes."
"when a request for instantiating an end-to-end service is received at the top-layer mano, the 5gex resource orchestrator will deploy the requested service elements in the different allocated dc slice parts according to the resources and capabilities reported by the domain adapters. virtual links will be created between the allocated service elements (e.g., vnf1, vnf2 and vnf3 in figure 4 ) for the implementation of the requested end-to-end service function chain."
"we believe that our hsp experiment is still exploratory work on the very wide subject of mano platforms interworking. in this paper, we created a first design and implementation solution to the problem, and provided a possible solution to a scenario that includes the 5gex and sonata mano frameworks. we demonstrated how the proper combined use of data models, abstractions, and apis provided by those systems could be considered as reference, in future works, for the design and implementation of more generic hsp mano-to-mano scenarios."
"however, due to the often low similarity between the textual information extracted from source code and bug reports, ir techniques may have poor accuracy, in particular ir techniques may put the classes most related to a bugfrom the developers' point of view-very low in the ranked list. consequently, many researchers proposed to use additional information to improve the accuracy of ir techniques in general [cit], and for bug location in particular [cit] . they conjectured that other information combined with ir techniques could be used to re-rank the classes for each bug to bring the most relevant classes higher in the ranked list."
"the goal of our empirical study is to evaluate the effectiveness of our novel approach for bug location against traditional lsi and vsm-based approaches. in addition, we analyse which bcr helps to improve ir technique accuracy the most. the quality focus is the ability of libcroos to link a bug report to appropriate classes in the source code in terms of ranking [cit] . the perspective is that of practitioners and researchers interested in locating bugs in source code with greater accuracy than that of currently-available buglocation approaches based on ir techniques. the objects of our empirical study are four open-source programs, jabref 2, lucene 3, mucommander 4, and rhino 5 ."
"these properties are: (1) exclusivity of the participation of the instances of the classes involved in the bcr, (2) the types of the receivers of the messages exchanged, (3) the life-time of the instances of the classes involved in the bcr, and (4) the multiplicity of the instances of the classes involved in the bcr. we use these properties to define uniquely each bcr, from the least constraining in terms of the values of the properties to the most constraining: use, association, aggregation, and composition."
"preprocessing the corpora: we remove nonalphabetical characters from the terms gathered from the source code and the bugs and then use the classic camel case and under-score algorithms to split identifiers into terms. then, we perform the following steps to normalise bug reports and source code: (1) convert all upper-case letters into lower-case and remove punctuation; (2) remove all stop words (such as articles, numbers, and so on); and, (3) perform word stemming using the porter stemmer bringing back inflected forms to their morphemes."
"the gatekeeper plays an important and active role in the implementation of the communication interface in our inter-mano hsp scenario, as it provides the api methods that are used by the higher level 5gex mano to interact with the lower level sonata mano. further details are reported in the following section where we show how these elements are composed to form the full hierarchy."
"when defining the properties of any bcr (but inheritance and composition), we make sure that we can identify these values of the properties using padl constituents, in particular: classes, methods, and fields, and method invocations between methods. thus, our analyses mainly consist in identifying potential bcr among classes and then refining these candidates using the values of their properties. these analyses are source code analyses, because they use essentially information extracted from the source code. however, we abstract these analyses to make them operate on padl models so that we can recover bcrs from various programming languages."
the only way to know anything about malfunctioning of ivr is if a customer comes to complaint. this represents a significant potential business risk. organizations could loose as much as 50-70% their customer or revenues due to undetected ivr transaction problems. a single 30-minute outage or slowdown of ivr applications specially billing applications in larger organizations can impact thousands of calls at a potential cost in the tens of thousands of dollars. this had occurred with lots of telecom service providers around the world.
research has shown that using automated testing of ivr applications; many organizations have been able to reduce the time it takes to find and solve ivr problems from hours down to minutes result in end-user satisfaction and cost benefits to companies.
"integrated reporting server (for storing data related to reporting) for analysts to analyze the call patterns like average duration of calls, type of callers, call hang-ups etc."
"the 5gex resource orchestrator retrieves the available dc slice resources, functions, and services from the connected underlying domains via the i3-rc interface (step 3a on figure 4 ). the domain adapters expose a bis-bis [cit] abstracted view of the resources coming from both the slices vlsp and sonata domains: the available collected dc slice resources are added to the bis-bis view as computational characteristics (cpu, memory and storage); the available services and functions are included as domain capabilities. the end-points of the on-demand vims instantiated in the sliced nfvi are also collected and mapped as service access points, [cit] of that sliced resource domain."
"having different types of vims in different segments of the infrastructure will allow creating end-to-end slices according to the type of resources that will host that particular vim: lightweight vim versions might be instantiated at the mobile edge, whereas the core dc might host more traditional vim implementations, such as openstack."
"we successfully demonstrated an end-to-end service instantiation, using multiple manos, which had service elements executing on the different resource domains, that were all interconnected to create a single end-to-end service function chain. moreover, the service elements were running on a slice of each domain, where each slice was allocated on-demand."
"when applying these analyses on a padl model, we obtain a new padl model that contains all the constituents from the original model plus constituents representing explicitly the bcrs: instances of the use, association, aggregation, and implementation constituents of an extension to the padl meta-model."
"the top of the figure shows the key 5gex mano component -the multi-provider multi-domain orchestrator, which we refer to as multi-domain orchestrator (mdo) for simplicity. the middle part shows the domain orchestrators, responsible for performing virtualization service orchestration and / or resource orchestration exploiting the abstractions exposed by the lower resource domains. the bottom part of the figure shows various resource domains, hosting the actual resources. the mdo handles the orchestration of resources and services from different providers, coordinating resources and service orchestration at the multi-domain level. multi-domain refers to multi-technology (orchestrating resources and services using multiple domain orchestrators); or multi-provider (orchestrating resources and services using domain orchestrators belonging to multiple administrative domains)."
"imagine a service developer wishes to deploy a service on a 5g provider infrastructure, where one branch of the provider is managed by the sonata mano and the another branch uses the 5gex mano system. it is important that all service elements and vnfs are on-boarded before any deployment can occur. for 5gex, a standard set of vnfs and their descriptors are on-boarded when the mano starts, and services get composed from those vnfs. on the sonata mano, a packaged service has to explicitly be on-boarded via the gatekeeper (step 1 on figure 4 )."
"in order to facilitate the allocation of resources for the end-to-end services, each 5g provider / operator may take advantage from using complex software systems that perform the management, control and orchestration of all the resources to be allocated for the deployment of 5g service function chains. these systems are usually referred to as mano(s) (vnf management and orchestration [cit] ) as they are able to deal with the tasks required to build network services by combining multiple virtual network functions (vnfs)."
"we evaluated this setup on the ucl (university college london) testbed implementing a test service as an orchestrated deployment of vlsp lightweight vnfs across the sliced nvfi. after successful instantiation of the requested end-toend service, network traffic was generated and injected in the service function chain (which only included forwarding vnfs) using the iperf network bandwidth measurement tool. more specifically, an iperf server was attached to the sapout2 of figure 4 and was able to receive the packets generated by an iperf client instance attached to sapin1. the description of the mechanisms utilised for the interconnection of the different slice parts is considered out of the scope of this paper."
"each primary server is mapped to exactly one recovery group. all data hosted by that primary server is replicated across its corresponding recovery group. this leads to a simple failure recovery strategy: if a primary server fails, the file system activates the corresponding recovery group. because all of the server's secondary replicas reside in that recovery group, this is sufficient to restore availability to the data stored on the failed primary."
the support vector machine (svm) and the traditional self-advising support vector machine (sa-svm) is used to classify the extracted features. the brief descriptions of these two learning machines are expressed as follows.
"'lvjxvw )hdu 6plolqj 1hxwudo 6dgqhvv 6xsulvh figure 1 . example facial expressions. the face must be detected, aligned and cropped before facial expression recognition is performed."
"rabbit borrows write offloading from the everest system [cit] b ] to solve this problem. when primary servers become the write performance bottleneck, rabbit simply offloads writes that would go to heavily loaded servers across all active servers. while such write offloading allows rabbit to achieve good peak write performance comparable to unmodified hdfs due to balanced load, it significantly impairs system agility by spreading primary replicas across all active servers, as depicted in figure 3 . consequently, before rabbit shrinks the system size, cleanup work is required to migrate some primary replicas to the remaining active servers so that at least one complete copy of data is still available after the resizing action. as a result, the improved performance from everest-style write offloading comes at a high cost in system agility. figure 4 illustrates the very different design points represented by sierra and rabbit, in terms of the trade-offs among agility, elasticity, and peak write performance. read performance is the same for all of these systems, given the same number of active servers. the minimum number of active servers that store primary replicas fig. 4 . elastic storage system comparison in terms of agility and performance. n is the total size of the cluster. p is the number of primary servers in the equal-work data layout. servers with at least some primary replicas cannot be deactivated without first moving those primary replicas. springfs provides a continuum between sierra's and rabbit's (when no offload) single points in this trade-off space. when rabbit requires offload, springfs is superior at all points. note that the y-axis is discontinuous."
"some recent distributed storage designs (e.g., sierra ], rabbit [cit] ) provide for elastic sizing, originally targeted for energy savings, by distributing replicas among servers such that subsets of them can be powered down when the workload is low without affecting data availability; any server with the primary replica of data will remain active. these systems are designed mainly for performance or elasticity (how small the system size can shrink to) goals, while overlooking the importance of agility (how quickly the system can resize its footprint in response to workload variations), which we find has a significant impact on the machine-hour savings (and so the operating cost savings) one can potentially achieve. as a result, state-of-the-art elastic storage systems must make painful tradeoffs among these goals, unable to fulfill them at the same time. for example, sierra balances load across all active servers and thus provides good performance. however, this even data layout limits elasticity-at least one third of the servers must always be active (assuming 3-way replication), wasting machine hours that could be used for other purposes when the workload is very low. further, rebalancing the data layout when turning servers back on induces significant migration overhead, impairing system agility."
"stage 1 is designed to extract features at different orientations. it consists of a set of nonlinear filters that are based on a biological mechanism known as shunting inhibition. this mechanism, found in the early visual system [cit], has been applied to improve image contrast [cit] . the output response of the proposed directional nonlinear filter is computed as"
"this section motivates our work. first, it describes the related work on elastic distributed storage, which provides different mechanisms and data layouts to allow servers to be extracted while maintaining data availability. second, it gives additional details on the data layout of one specific instance (rabbit). third, it demonstrates the significant impact of agility on aggregate machine-hour usage of elastic storage. at last, it describes the limitations of state-of-the-art elastic storage systems and how springfs fills the significant gap between agility and performance."
"rabbit [cit] ] is a distributed file system designed to provide power proportionality to workloads that vary over time. it is based on hdfs [cit] ] but uses alternate data layouts that allow it to extract (e.g., power off) large subsets of servers without reducing data availability. to do so, rabbit exploits the data replicas (originally for fault tolerance) to ensure that all blocks are available at any power setting. in addition to maintaining availability, rabbit tries to be power proportional with any number of active servers, that is, performance scales linearly with the number of active servers. this section describes in detail the equal-work data layout that rabbit relies on to achieve power proportionality."
"we can derive a lower bound on the spread based on the observation that the number of blocks stored on the servers in the range [ p + 1, s] must correspond to (r − 1) replicas of the dataset. hence,"
"there are several directions for interesting future work. for example, the springfs data layout assumes that servers are approximately homogeneous, like hdfs does, but some real-world deployments end up with heterogeneous servers (in terms of i/o throughput and capacity) as servers are added and replaced over time. the data layout could be refined to exploit such heterogeneity, such as by using more powerful servers as primaries. second, springfs's design assumes a relatively even popularity of data within a given dataset, as exists for hadoop jobs processing that dataset, so it will be interesting to explore what aspects change when addressing the unbalanced access patterns (e.g., zipf distribution) common in servers hosting large numbers of relatively independent files."
"indicates the minimal system footprint one can shrink to without any cleanup work. the maximum write performance shown on the x-axis is normalized to the throughput of a single server. as described in the preceding text, state-of-the art elastic storage systems such as sierra and rabbit suffer from the painful trade-off between agility and performance due to the use of a rigid data layout. springfs provides a more flexible design that provides the best-case elasticity of rabbit, the best-case write performance of sierra, and much better agility than either. to achieve the range of options shown, springfs uses an explicit bound on the offload set, where writes of primary replicas to overloaded servers are offloaded to only the minimum set of servers (instead of all active servers) that can satisfy the current write throughput requirement. this additional degree of freedom allows springfs to adapt dynamically to workload changes, providing the desired performance while maintaining system agility."
"most of this discussion focuses on availability. restoring durability and layout after a primary failure uses the same mechanisms as writing new data to the file system, described in section 3.1. the equal-work data layout cannot remain power proportional in the event that a primary server fails, because blocks from each primary server are scattered across all secondary servers. when a primary server fails, all secondary servers must be activated to restore availability. therefore, the non-fault-tolerant version of the equalwork layout cannot achieve its target minimum power setting of p when there is a primary server failure. instead, it has a minimum power setting of ep − 1."
"the structuring of automatic damage detection system in civil structures has been proposed and developed. the experimental study is performed to identify cracks in a concrete beam under static loading in laboratory condition. the piezoelectric-based transducers have been used to present the discontinuities inside journal of signal and information processing the structure. the hybrid statistical feature analysis algorithm based on the combination of three damage indexes which are the differences of gradient, mean, mean square error between the healthy and non-healthy state of the structure along with the self-advising support vector machine is carried out on the obtained signals to determine the health state of the structure. the result shows that the proposed algorithm is able to determine the state of the structure more accurately when compared with traditional algorithms. this algorithm can be applicable in other area of research. further improvements on accuracy can be achieved through utilizing the more features in the feature set vector."
"facial expression, controlled by a complex mesh of nerves and muscles beneath the face skin, enables people to convey emotions and perform nonverbal communications. accurate recognition of facial expression is essential in many fields, including human-machine interaction, affective computing, robotics, computer games and psychology studies. there are seven basic facial expressions that reflect distinctive psychological activities: anger, disgust, fear, happiness, neutral, sadness and surprise. examples of these facial expressions are shown in fig. 1 . this paper focuses on the recognition of two major facial expressions: neutral and smiling. the contribution of the paper is two-fold. first, we propose a method to address localisation error in existing face detection methods. after preliminary face detection, our method applies eye detection, a geometric face model and template-based face verification to precisely locate the face and correct rotation (inplane and some out-of-plane rotation). second, we propose a novel neural architecture for image pattern classification, which consists of fixed and adaptive nonlinear 2-d filters in a hierarchical structure. the fixed filters are used to extract primitive features such as edges, whereas the adaptive filters are trained to extract more complex facial features."
"the green curve which represents the mean square error reaches the highest value more than 0.02, while the other two curves values are less than 0.02. it shows that the mean square error shows the differences between the healthy and non-healthy concrete beam more accurately. based on the results, average accuracy and f-score of \"our proposed approach\" are 83.54377 and 0.54242, respectively. for the purpose of comparison, the svm is also trained with the same set of features employed for our proposed algorithm results in table 1 . the results of \"gmm/svm classification\" are shown in table 2 the results show the better performance of our proposed algorithm of \"gmm/sa-svm classification\" than another algorithm. our achieved results are confirmed by the results achieved by the reference paper we used for sa-svm [cit] . both the reference paper and our results show a similar pattern in performance of svm and sa-svm. the t-test is also carried out as another analysis factor to show the effectiveness of the proposed algorithm rather than the traditional one. table 3 shows the superiority of our proposed algorithm rather than the traditional one through the t-test."
"when a new dataset is to be written into the dfs, nodes are chosen greedily starting with the node with the minimum score, s i . the score's are updated once the blocks of the new dataset are stored. figure 7 shows the layout policy for three datasets. each of the datasets has a spread that is equal to the size of the entire cluster, but the order of nodes in the expansion-chains of the datasets is unique for each dataset."
"on the other hand, a large gear size does not allow fine-grained power adjustments, but can run at very low power even when recovering from a failure. this relationship is complicated by the fact that the number of secondary servers depends on the gear size."
"this section describes modifications to the equal work layout that allow the file system to remain power proportional when a primary server fails. we only consider crash failures instead of arbitrary byzantine failures. the failure recovery process is composed of three parts, though they are not necessarily separate activities. each involves restoring some property of the file system."
"existing approaches for facial expression recognition can be divided into three categories, based on how features are extracted from an image for classification. the categories are geometric-based, appearance-based, and hybrid-based."
"the output of the eye filter is shown in fig. 3a . then, the connected components are extracted. image erosion and dilation are performed to remove noise regions. figure 3d shows eye candidates, after these steps. note that it is essential to eliminate the false candidates."
"in this section, the results of the proposed algorithm are investigated. figure 1 shows the de-noised signal through wavelet de-noiser. the red signal is the signal obtained through the sensor on concrete beam and the blue signal shows the de-noised signal through the wavelet de-noiser."
"our experiments are focused primarily on the relative performance changes as agility-specific parameters and policies are modified. because the original hadoop implementation is unable to deliver the full performance of the underlying hardware, our system can only be compared reasonably with it and not the capability of the raw storage devices."
"to maintain control over the i/o bandwidth allocated to a dataset, a given node is assigned to exactly one dataset, which means that the i/o bandwidth of that node is allocated solely to that dataset. we choose this approach for two reasons. first, in a large cluster, the node will often be an acceptable level of granularity. second, performance insulation in storage clusters remains an open problem, and sharing nodes relies on it to be solved."
"we focus on elastic storage systems that ensure data availability at all times. when servers are extracted from the system, at least one copy of all data must remain active to serve read requests. like rabbit, sierra also exploits data replicas to ensure that all blocks are available at any power setting. with 3-way replication 4, sierra stores the first replica of every block (termed primary replica) in one third of servers, and writes the other two replicas to the other two thirds of servers. this data layout allows sierra to achieve full peak performance due to balanced load across all active servers, but it limits the elasticity of the system by not allowing the system footprint to go below one third of the cluster size. we show in section 5.2 that such limitation can have a significant impact on the machine-hour savings that sierra can potentially achieve, especially during periods of low demand."
"one of the effective monitoring techniques in determining the health state of structures is an automatic diagnosis system using sensing data which is based on the principle that damage in a structure, e.g. crack or a loosened connection, can modifies the dynamic structural responses [cit] . the automated condition assessment systems can interpret the large volumes of inspection data to detect and prevent potential structural failure in early stages by minimizing errors to ensure effective risk management, while reducing the asset management costs."
"note that in addition to the capability of i/o scheduling, the multi-dataset mechanism also improves the capacity utilization of the systems, allowing small values of p without limiting storage capacity utilization to 3p/n."
"springfs is implemented as a modified instance of the hadoop distributed file system (hdfs), version 0.19.1. 7 we build on a scriptable hadoop interface that we built into hadoop to allow experimenters to implement policies in external programs that are called by the modified hadoop. this enables rapid prototyping of new policies for data placement, read load balancing, task scheduling, and rebalancing. it also enables us to emulate both rabbit and springfs in the same system, for better comparison. springfs mainly consists of four components: data placement agent, load balancer, resizing agent, and migration agent-all implemented as python programs called by the scriptable hadoop interface."
"the smiling and neutral facial expressions are differentiated using a new neural architecture, which consists of three processing stages as shown in fig. 7 . the first and second stages consist of nonlinear filters, which are used for extracting visual features. the third stage performs classification."
"everest [cit] b ] is a distributed storage design that used write performance offloading 3 in the context of enterprise storage. in everest, disks are grouped into distinct volumes, and each write is directed to a particular volume. when a volume becomes overloaded, writes can be temporarily redirected to other volumes that have spare bandwidth, leaving the overloaded volume to only handle reads. rabbit applies this same approach, when necessary, to address overload of the primaries."
"in this paper, svm is selected as the method of choice as it linearly classifies data in a high dimensional feature space which is related nonlinearly to the input space using specific radial basis (rbf) kernel."
"stage 2 aims to detect more complex features for classification. the output maps produced by each filter in stage 1 are processed by exactly two filters in stage 2: one filter for the on-response and the other filter for the off-response. hence, the number of filters, n 2, in stage 2 is twice the number of filters in stage 1:"
"where i is a 2-d input face pattern, z 1,i is the output of the i-th filter, d i and g are the filter coefficients, \" * \" denotes 2-d convolution, and the division is done pixel-wise. in this paper, the subscripts 1 and 2 in z 1,i and z 2,i indicate the outputs of the first and second processing steps, respectively. the kernel g is chosen as an isotropic gaussian kernel:"
"stage 2 is also based on the shunting inhibition mechanism. consider an input map z 4,i to stage 2. suppose that p k and q k are two adaptive kernels for the filter that corresponds to this input map. the filter output is calculated as"
the gabor filter is the product of a harmonic function and a gaussian function. the real part of a 2-d gabor filter is defined as
"springfs borrows the ideas of write availability and performance offloading from prior elastic storage systems. specifically, it builds on the rabbit design but develops new offloading and migration schemes that effectively eliminate the painful trade-off between agility and write performance. these techniques apply generally to elastic storage designs with an uneven data layout."
"to extract elementary facial features at different directions, the kernel d i is formulated as the m -th order derivative gaussian. its coefficients is defined as"
"this article makes three main contributions: first, it shows the importance of agility in elastic distributed storage, highlighting the need to resize quickly (at times) rather than just hourly as in previous designs. second, springfs introduces a novel write offloading policy that bounds the set of servers to which writes to over-loaded primary servers are redirected. bounded write offloading, together with read offloading and passive migration, significantly improves the system's agility by reducing the cleanup work during elastic resizing. these techniques apply generally to elastic storage with an uneven data layout. third, we demonstrate the significant machine-hour savings that can be achieved with elastic resizing, using six real-world hdfs traces, and the effectiveness of springfs's policies at achieving a \"close-to-ideal\" machine-hour usage."
"by \"agility,\" we mean how quickly one can change the number of servers effectively contributing to a service. for most non-storage services, such changes can often be completed quickly, as the amount of state involved is small. for distributed storage, however, the state involved may be substantial. a storage server can service reads only for data that it stores, which affects the speed of both removing and reintegrating a server. removing a server requires first ensuring that all data is available on other servers, and reintegrating a server involves replacing data overwritten (or discarded) while it was inactive."
"that is we pick the live dataset that contributes the most to the score of the node. rabbit and springfs can control the i/o bandwidth available to a dataset d by controlling the size of the set a(d). since all requests for blocks belonging to that dataset are serviced by the nodes in the set a(d), and no others, this sets the total amount of bandwidth available to the dataset. fig. 8 . gear groups and recovery groups. all data from a single primary exists on a single recovery group, such as the grey box. when increasing the power setting, the file system turns on gear groups in an all-ornothing fashion."
"when springfs tries to write a replica according to its target data layout but the chosen server happens to be inactive, it must still maintain the specified replication factor for the block. to do this, another host must be selected to receive the write. availability offloading is used to redirect writes that would have gone to inactive servers (which are unavailable to receive requests) to the active servers. as illustrated in figure 6, springfs load balances availability offloaded writes together with the other writes to the system. this results in the availability offloaded writes going to the less-loaded active servers rather than adding to existing write bottlenecks on other servers."
"despite its simplicity, read offloading allows springfs to increase write throughput without changing the offload set by taking read work away from the low numbered servers (which are the bottleneck for writes). when a read occurs, instead of randomly picking one among the servers storing the replicas, springfs chooses the server that has received the least number of total requests recently. (the one exception is when the client requesting the read has a local copy of the data. in this case, springfs reads the replica directly from that server to exploit machine locality.) as a result, lower-numbered servers receive more writes, while higher-numbered servers handle more reads. such read/write distribution balances the overall load across all the active servers while reducing the need for write offloading."
"the result shows the effectiveness of this de-nosing tool on the experiment obtained data. therefore, in this paper to take the advantage of all three damage indexes, the combination of three damage indexes has been taken into account to provide the more affective damage recognition. in this regard, the extracted features through equation (1) and equation (4) and equation (7) which computes these damage indexes between the healthy and non-healthy state of the concrete beam are considered as the feature vector. this feature vector is sensitive to any change that may happen in the structure. shows that the mean square error difference has the highest value than the other two mean and gradient differences."
"we analyse the performance of the proposed method on the standard japanese female facial expression (jaffe) database [cit], which is commonly used in research on facial expression recognition. this database consists of 213 images from 10 japanese actresses. they were instructed to produce seven types of facial expressions (see fig. 1 ). for each person, two to four images were recorded for each facial expression."
"distributed storage can and should be elastic, just like other aspects of cloud computing. when storage is provided via single-purpose storage devices or servers, separated from compute activities, elasticity is useful for reducing energy usage, allowing temporarily unneeded storage components to be powered down. however, for storage provided via multi-purpose servers (e.g., when a server operates as both a storage node in a distributed filesystem and a compute node), such elasticity is even more valuable-providing cloud infrastructures with the freedom to use such servers for other purposes, as tenant demands and priorities dictate. this freedom may be particularly important for increasingly prevalent data-intensive computing activities (e.g., data analytics)."
"resizing agent. the resizing agent changes springfs's footprint by setting an activity state for each datanode. on every read and write, the data placement agent and load balancer will check these states and remove all \"inactive\" datanodes from the candidate list. only \"active\" datanodes are able to service reads or writes. by setting the activity state for datanodes, we allow the resources (e.g., cpu and network) of inactive nodes to be used for other activities with no interference from springfs activities. we also modified the hdfs mechanisms for detecting and repairing underreplication to assume that inactive nodes are not failed, so as to avoid undesired re-replication."
"patterns. the correlation coefficient between the face candidate and the face template is calculated. among the overlapping candidates, the one with the maximum correlation score is considered as the true face. figure 6b shows the corrected upright face image after rotation by the angle of α."
"the dfs is an ideal location for the implementation of mechanisms to control the amount of bandwidth provisioned to applications. rabbit and springfs can manage i/o resources between datasets stored in the cluster. it is possible to allocate i/o bandwidth to a particular dataset that would then be shared by the applications using that dataset. this section describes how we use the mechanisms used to provide powerproportionality to perform i/o scheduling for datasets in rabbit or springfs. recall that a dataset is defined to be an arbitrary set of files stored in rabbit. we assume that all data entering the system is tagged with metadata specifying the dataset that the data belongs to. one way to do this is to define datasets based on file system hierarchy, with subtrees explicitly associated with datasets, as with volumes in afs [cit] ]."
"our focus is on data-intensive jobs whose performance significantly depends on i/o bandwidth, such as most jobs run on hadoop. hadoop has its own fair scheduler that indirectly manages i/o resources by controlling the compute scheduling, but this approach only guarantees fairness for map-reduce jobs using the particular instance of the hadoop library. in a data center environment, there can exist multiple different applications, such as bigtable [cit], that use the services offered by the dfs. in such scenarios, indirectly managing the i/o resources through compute scheduling becomes impossible. our solution enables scheduling of i/o resources at the level of the dfs and allows the i/o bandwidth of the cluster to be shared among the datasets in an explicit manner. section 2.2 explains the equal-work data layout policy. to handle multiple datasets, we use the same policy but overlay the datasets over one another using a greedy strategy to choose the nodes. we define a score, s i for a node i that depends on where that node figures in the expansion-chains of the different datasets. let d i be the set of datasets that have blocks stored on node i, let s i (d) be the contribution of dataset"
"to simulate periods of high i/o activity, and effectively evaluate springfs under different mixes of i/o operations, we used a modified version of the standard hadoop testdfsio storage system benchmark called testdfsio2. our modifications allow for each node to generate a mix of block-size (128 mb) reads and writes, distributed randomly across the block id space, with a user-specified write ratio."
"rabbit, on the other hand, is able to reduce its system footprint to a much smaller size (down to p) without any cleanup work in virtue of the equal-work data layout. however, it can create bottlenecks for writes. since the primary servers must store the primary replicas for all blocks, the maximum write throughput of rabbit is limited by the maximum aggregate write throughout of the p primary servers, even when all servers are active. in contrast, sierra is able to achieve the same maximum write throughput as that of hdfs, that is, the aggregate write throughput of n/3 servers (recall: n servers write three replicas for every data block)."
"therefore, the main objective of this paper is to propose a more robust scheme of automated condition assessment system to monitor and evaluate the health state of the structure. for the purpose of this detection system, non-destructive testing (ndt) method which has attracted lots of attention in structural health monitoring for characterizing and assessing the materials and structures is used to characterize the concrete members. thereafter, the signal processing and artificial intelligent approaches are employed to analyze the data and enhance the accuracy of detection. however, the critical challenge here is the huge number of data which are obtained. the high priority is proposing the potential algorithm to reduce the loss of information while reducing the noise effect. the second concern is the proper extraction of features which can effectively determine the health state of the structure. these features need to be sensitive enough to detect any discontinuity happens in the structure. these features are directly affecting the performance of the classifier. the third challenge is choosing the appropriate classifier that identifies the crack as happened. the question raised here is what features and which classification technique are appropriate to determine the structural condition; what can be measured that correlates to damage, and how to measure and employ the raw measurements to determine the structural condition."
"data layout. regardless of write performance, the equal-work data layout proposed in rabbit enables the smallest number of primary servers and thus provides the best elasticity in state-of-the-art designs. 6 springfs retains such elasticity using a variant of the equal-work data layout, but addresses the agility issue incurred by everest-style offloading when write performance bottlenecks arise. the key idea is to bound the distribution of primary replicas to a minimal set of servers (instead of offloading them to all active servers), given a target maximum write performance, so that the cleanup work during server extraction can be minimized. this bounded write offloading technique introduces a parameter called the offload set: the set of servers to which primary replicas are offloaded (and as a consequence receive the most write requests). the offload set provides an adjustable trade-off between maximum write performance and cleanup work. with a small offload set, few writes will be offloaded, and little cleanup work will be subsequently required, but the maximum write performance will be limited. conversely, a larger offload set will offload more writes, enabling higher maximum write performance at the cost of more cleanup work to be done later. figure 5 shows the springfs data layout and its relationship with the state-of-the-art elastic data layout designs. we denote the size of the offload set as m, the number of primary servers in the equal-work layout as p, and the total size of the cluster as n. when m equals p, springfs behaves like rabbit and writes all data according to the equalwork layout (no offload); when m equals n/3, springfs behaves like sierra and load balances all writes (maximum performance). as illustrated in figure 4, the use of the tunable offload set allows springfs to achieve both end points and points in between."
"a dataset is an arbitrary user-defined set of files stored in the distributed file system (dfs). for each dataset, we define an ordered list of nodes, called the expansion-chain, which denotes the order in which nodes must be turned on or off to scale performance up or down, respectively. the nodes of the expansion-chain that are powered on are called the active nodes, a(d), for dataset d. for the rest of section 2.2, we do not consider multiple datasets, which will be discussed further in section 3.3."
"this experiment uses a smaller setup, with only 7 datanodes, 2 primaries, 3 in the offload set, and 2-way replication. the workload consists of 3 consecutive benchmarks. the first benchmark is a testdfsio2 benchmark that writes 7 files, each 2gb in size for a total of 14gb written. the second benchmark is one swim job [cit] ] randomly picked from a series of swim jobs synthesized from a facebook trace which reads 4.2gb and writes 8.4gb of data. the third benchmark is also a testdfsio2 benchmark, but with a write ratio of 20%. the testdfsio2 benchmarks are i/o intensive, whereas the swim job consumes only a small amount of the full i/o throughput. for the resizing case, 4 servers are extracted after the first write-only testdfsio2 benchmark finishes (shrinking the active set to 3), and those servers are reintegrated when the second testdfsio2 job starts. in this experiment, the resizing points are manually set when phase switch happens. automatic resizing can be done based on previous work on workload prediction [cit] ."
"our experiments demonstrate that the springfs design enables significant reductions in both the fraction of servers that need to be active and the amount of migration work required. indeed, its design for where and when to offload writes enables springfs to resize elastically without performing any data migration at all in most cases. analysis of traces from six real hadoop deployments at facebook and various cloudera customers show the oft-noted workload variation and the potential of springfs to exploit it-springfs reduces the amount of data migrated for elastic resizing by up to two orders of magnitude and cuts the percentage of active servers required by 67-82%, outdoing state-of-the-art designs like sierra and rabbit by 6-120%."
"because of availability offloading, reintegrating a previously deactivated server is more than simply restarting its software. while the server can begin servicing its share of the write workload immediately, it can only service reads for blocks that it stores. thus, filling it according to its place in the target equal-work layout is part of full reintegration."
"migration agent. the migration agent crawls the entire hdfs block distribution (once) when the namenode starts, and it keeps this information up-to-date by modifying hdfs to provide an interface to get and change the current data layout. it exports two metadata tables from the namenode, mapping file names to block lists and blocks to datanode lists, and loads them into a sqlite database. any changes to the metadata (e.g., creating a file, creating or migrating a block) are then reflected in the database on the fly. when data migration is scheduled, the springfs migration agent executes a series of sql queries to detect layout problems, such as blocks with no primary replica or hosts storing too little data. it then constructs a list of migration actions to repair these problems. after constructing the full list of actions, the migration agent executes them in the background. to allow block-level migration, we modified the hdfs client utility to have a \"relocate\" operation that copies a block to a new server. the migration agent uses gnu parallel to execute many relocates simultaneously."
"this section describes springfs's data layout as well as the bounded write offloading and read offloading policies that minimize the cleanup work needed before deactivation of servers. it then describes the passive migration policy used during a server's reintegration to address data that was written during the server's absence. lastly, it presents the i/o scheduling and fault tolerance mechanisms used in both rabbit and springfs. 5 fig. 5 . springfs data layout and its relationship with previous designs. the offload set allows springfs to achieve a dynamic trade-off between the maximum write performance and the cleanup work needed before extracting servers. in springfs, all primary replicas are stored in the m servers of the offload set. the shaded regions indicate writes of non-primary replicas that would have gone to the offload set (in springfs) are instead redirected and load balanced outside the set."
"consider a cluster with n nodes, where tput n is the i/o throughput obtained and pow n the power consumed when n nodes are active (powered on). the requirements of a power-proportional distributed storage system is formally stated here."
"-availability. all data may be accessed immediately. in the case of a power proportional dfs, this means ensuring that every block is replicated on at least one active node. -durability. the file system's fault tolerance configuration is met. for rabbit and springfs, this means that each block is replicated r times. -layout. the file system's target layout is achieved. for the equal-work layout policy, non-primary node i has approximately b/i blocks on it."
"where i(x, y) is an image pixel, n(x, y) is the normalized pixel, and μ(x, y) is the mean intensity of the neighboring pixels. third, a contrast-normalization is applied"
"we presented an approach for automatic recognition of smiling and neutral faces. in our approach, eye detection and face alignment are used to correct localisation errors in the existing face detectors. the classification between smiling and neutral faces is done via a novel neural architecture that combines fixed, directional filters and adaptive filters in cascade. the fixed, directional filters extract primitive edge features, whereas the adaptive filters are trained to extract more complex features, which is classified by the linear classifier. the eye detection and face alignment method can correctly process all images in the jaffe database. on this benchmark database, our system achieves a classification rate of 99.0% for smiling and neutral faces."
"the remainder of this article is organized as follows. section 2 describes elastic distributed storage generally, the rabbit design specifically, the importance of agility in elastic storage, and the limitations of the state-of-the-art data layout designs in fulfilling elasticity, agility, and performance goals at the same time. section 3 describes springfs, including its key techniques for increasing agility of elasticity, support for per-dataset bandwidth control, and fault tolerance. section 4 overviews the springfs implementation. section 5 evaluates the springfs design."
"read offloading. one way to reduce the amount of cleanup work is to simply reduce the amount of write offloading that needs to be done to achieve the system's performance targets. when applications simultaneously read and write data, springfs can coordinate the read and write requests so that reads are preferentially sent to higher numbered servers that naturally handle fewer write requests. we call this technique read offloading."
"as one concrete example, figure 2 shows the number of active servers needed, as a function of time in the trace, to provide the required throughput in a randomly chosen 4-hour period from the facebook trace described in section 5. the dashed and solid curves bounding the shaded region represent the minimum number of active servers needed if using 1-hour and 1-minute resizing intervals, respectively. for each such period, the number of active servers corresponds to the number needed to provide the peak throughput in that period, as is done in sierra to avoid significant latency increases. the area under each curve represents the machine time used for that resizing interval, and the shaded region represents the increased server usage (more than double) for the 1-hour interval. we observe similar burstiness and consequences of it across all of the traces."
"experiments were conducted to evaluate the proposed method for differentiating smiling and neutral facial expressions. results in table 2 indicate that recognition performance increases when the face pattern is better aligned and cropped and in-plane rotation is corrected. table 3 shows the confusion matrix for smiling and neutral recognition, when the face image is aligned. the overall recognition accuracy is 99.00%. this method can also cope with images containing multiple faces. for each face detected by opencv, we apply the proposed face alignment method. the corresponding facial expression is then determined by passing the upright face image through the fer system. figure 9 gives a visual example of the overall automatic fer system. figure 9 . a visual result of the automatic system: (a) eye detection, (b) expression recognition."
"effect of offloading policies. our evaluation focuses on how springfs's offloading policies affect performance and agility. we also measure the cleanup work created by offloading and demonstrate that springfs's number of active servers can be adapted agilely to changes in workload intensity, allowing machines to be extracted and used for other activities. figure 10 presents the peak sustained i/o bandwidth measured for hdfs, rabbit, and springfs at different offload settings. (rabbit and springfs are identical when no offloading is used.) in this experiment, the write ratio is varied to demonstrate different mixes of read and write requests. springfs, rabbit, and hdfs achieve similar performance for a read-only workload, because in all cases, there is a good distribution of blocks and replicas across the cluster over which to balance the load. the read performance of springfs slightly outperforms the original hdfs due to its explicit load tracking for balancing."
"this section evaluates springfs and its offloading policies. measurements of the springfs implementation show that it provide performance comparable to unmodified hdfs, that its policies improve agility by reducing the cleanup required, and that it can agilely adapt its number of active servers to provide required performance levels. in addition, analysis of six traces from real hadoop deployments shows that springfs's agility enables significantly reduced commitment of active servers for the highly dynamic demands commonly seen in practice."
"load balancer. the load balancer implements the read offloading policy and preferentially sends reads to higher numbered servers that handle fewer write requests whenever possible. it keeps an estimate of the load on each server by counting the number of requests sent to each server recently. every time springfs assigns a block to a server, it increments a counter for the server. to ensure that recent activity has precedence, these counters are periodically decayed by 0.95 every 5 seconds. while this does not give the exact load on each server, we find its estimates good enough (within 3% off optimal) for load balancing among relatively homogeneous servers."
"the main concern in civil infrastructures is the condition-based maintenance which requires the consequent monitoring of critical structures in order to minimize the probability of failure leading major effects on national assets and public safety in every country [cit] . therefore, the structural health monitoring is absolutely an essential concept which can potentially provide effective solutions to assess the health state of infrastructure. it can reduce the asset management costs, effectively prolongs operational lifetime and ensures the public safety. in this regard, getting access to a robust paradigm to deal with aforementioned concerns is of high priority."
"springfs is a new elastic storage system that fills the space between state-of-the-art designs in the trade-off among agility, elasticity, and performance. springfs's data layout and offloading/migration policies adapt to workload demands and minimize the data redistribution cleanup work needed for elastic resizing, greatly increasing agility relative to the best previous elastic storage designs. as a result, springfs can satisfy the time-varying performance demands of real environments with many fewer machine hours. such agility provides an important building block for resource-efficient data-intensive computing (a.k.a. big data) in multipurpose clouds with competing demands for server resources."
"face detection aims to determine the presence and the positions of faces in an image. many algorithms have been proposed for face detection, e.g. the viola-jones face detector [cit] . even with the state-of-the-art face detectors, the boundary for the facial region is sometimes incorrectly calculated. the localisation error is more common when there is rotation (in-plane or out-of-plane). to improve accuracy in facial image analysis tasks such as face, gender and facial expression recognition, it is essential to correctly align the face image. after initial face detection, we propose a method for face alignment that consists of two main steps. first, for each detected face, candidates for eye points are found using a combination of a gabor filter and a circular filter. second, for each eye pair, a face candidate is constructed using a geometric face model, and each face candidate is compared with a face template to remove false detection. figure 2 . an example of the opencv face detector. even the face is found, its coordinates and rotation angle can be better estimated."
"setting. each node stores no more than the minimum required number of blocks, which allows the blocks to be distributed across a larger number of nodes while holding the number of replicas fixed so energy is not wasted writing an unnecessarily high number of data copies. we define a dataset's spread to be the number of nodes over which the blocks of that dataset are stored. a dataset's spread is equal to the length of its expansion-chain. for the equal-work policy, the spread depends on the number of replicas used."
"data-intensive computing over big datasets is quickly becoming important in most domains and will be a major consumer of future cloud computing resources [cit] . many of the frameworks for such computing (e.g., hadoop [cit] and google's mapreduce ]) achieve efficiency by distributing and storing the data on the same servers used for processing it. usually, the data is replicated and spread evenly (via randomness) across the servers, and the entire set of servers is assumed to always be part of the data analytics cluster. little-to-no support is provided for elastic sizing 1 of the portion of the cluster that hosts storage-only nodes that host no storage can be removed without significant effort, meaning that the storage service size can only grow."
"we also used jaffe database to evaluate the proposed eye detection and face alignment method. as shown in table 1, the detection rate of the proposed method is comparable to other existing methods, tested on the same database."
"recall that the amount of data on each secondary server in gear j is (p+gj), meaning that choosing a larger gear group size causes less data to be stored on each server, thus requiring more secondary servers overall. table i shows the relationship between the gear size and recovery group for an example file system with 100 primary servers. the size of the recovery group, as a function of the number of primaries p and the gear size g, is"
"except where otherwise noted, we specify a file size of 2gb per node in our experiments, such that the single hadoop map task per node reads or writes 16 blocks. the total time taken to transfer all blocks is aggregated and used to determine a global throughput. in some cases, we break down the throughput results into the average aggregate throughput of just the block reads or just the block writes. this enables comparison of springfs's performance to the unmodified hdfs setup with the same resources."
"previous elastic storage systems overlook the importance of agility, focusing on performance and elasticity. this section describes the limitation of the data layouts of state-of-the-art elastic storage systems, specifically sierra and rabbit, and how their layouts represent two specific points in the trade-off space among elasticity, agility, and performance. doing so highlights the need for a more flexible elastic storage design that fills the void between them, providing greater agility and matching the best of each."
"for eye detection, several image preprocessing steps are performed to reduce the effect of the lighting condition. first, the image is histogram equalised. second, a normalised image is calculated as follows"
"to avoid this outcome, we impose further constraints on the secondary replicas of each block. the secondary servers are grouped into gear groups and recovery groups, with each server belonging to exactly one gear group and one recovery group. to visualize this, imagine arranging the secondary servers in a grid configuration depicted in figure 8 . the rows of this rectangle are the gear groups, and the columns are the recovery groups. the number of servers in these groups, that is, length of the rows and columns of the grid, are respectively known as the gear group size and recovery group size. fig. 9 . simulated performance of fault-tolerant layout with 10 primary servers and a gear size of 5. the geared layout achieves very nearly ideal performance when a full gear is turned on, but less than ideal when a gear is only partially enabled."
"after de-noising the signal through the wavelet signal denoiser tool, the following features are extracted from the signal in time-domain. 1) difference of gradient between healthy and non-healthy state. in gradient, the central difference for interior data points is calculated."
we use a second-order training method called the levenberg-marquardt (lm) algorithm [cit] to optimize the parameters of the adaptive filters in stage 2 and the classifier in stage 3.
"in this layout scheme, gear groups are the basic unit of power scaling. it is not helpful to turn on extra replicas for some primary server's data and not others: work can never be shared equally if some primaries have to read all of their blocks and others have to read only some of their blocks. therefore, when turning on servers to increase the power mode, the file system must turn on a set of servers that will contain data from all primaries, that is, a gear group."
"as expected, springfs exhibits better agility than rabbit, especially when shrinking the size of the cluster, since it needs no cleanup work until resizing down to the offload set. such agility difference between springfs and rabbit is shown in figure 13 at various points of time (e.g., at minute 110, 140, and 160). the gap between the two lines indicates the number of machine hours saved due to the agility-aware read and bounded write policies used in springfs. springfs also achieves lower machine-hour usage than sierra, as confirmed in all the analysis graphs. while a sierra cluster can shrink down to 1/3 of its total size without any cleanup work, it is not able to further decrease the cluster size. in contrast, springfs can shrink the cluster size down to approximately 10% of the original footprint. when i/o activity is low, the difference in minimal system footprint can have a significant impact on the machine-hour usage (e.g., as illustrated in figure 14 (b), figure 14 (c), and figure 14(e) ). in addition, when expanding cluster size, sierra incurs more cleaning overhead than springfs, because deactivated servers need to migrate more data to restore its even data layout. these results are summarized in figure 15, which shows the extra number of machine hours used by each storage system compared and normalized to the ideal system. in these traces, springfs outperforms the other systems by 6% to 120%. for the traces with a relatively high write ratio, such as the fb, cc-d, and cc-e traces, springfs is able to achieve a \"close-to-ideal\" (within 5%) machine-hour usage. springfs is less close to ideal for the other three traces because they frequently need even less than the 13% primary servers that springfs cannot deactivate. figure 16 summarizes the total amount of data migrated by rabbit, sierra, and springfs while running each trace. with bounded write offloading and read offloading, springfs is able to reduce the amount of data migration by a factor of 9-208, as compared to rabbit. springfs migrates significantly less data than sierra as well, because data migrated to restore the equal-work data layout is much less than that to restore an even data layout."
"to eliminate false eye candidates, we construct an image region based on each eye pair and verify if the region is a face pattern. we construct a geometric face model that reflects the relative anthropometric distances between the facial landmarks, as shown in fig. 4 . this face model is adjustable to out-of-plane rotation [cit] . based on the two eye points e 1 and e 2, the four corners of the face region are determined as follows."
"the extracted features are sent to stage 3 for classification. stage 3 may use any type of classifiers. previously, we used a linear classifier whose output y j is given as"
"this article describes a new elastic distributed storage system, called springfs, that provides the elasticity of rabbit and the peak write bandwidth characteristic of sierra, while maximizing agility at each point along a continuum between their respective best cases. the key idea is to employ a small set of servers to store all primary replicas nominally, but (when needed) offload writes that would go to overloaded servers to only the minimum set of servers that can satisfy the write throughput requirement (instead of all active servers). this technique, termed bounded write offloading, effectively restricts the distribution of primary replicas during offloading and enables springfs to adapt dynamically to workload variations while meeting performance targets with a minimum loss of agility-most of the servers can be extracted without needing any pre-removal cleanup. springfs further improves agility by minimizing the cleanup work involved in resizing with two more techniques: read offloading offloads reads from write-heavy servers to reduce the amount of write offloading needed to achieve the system's performance targets; passive migration delays migration work by a certain time threshold during server re-integration to reduce the overall amount of data migrated. with these techniques, springfs achieves agile elasticity while providing performance comparable to a non-elastic storage system."
the paper is organized as follows. section 2 reviews related work on facial expression recognition. section 3 describes the proposed face detection and alignment method. section 4 presents the proposed method for recognising smiling and neutral facial expressions. section 5 analyses the performance of the proposed method on a standard database and compares it with several existing techniques. section 6 gives the concluding remarks.
"when a server is reintegrated to address a workload increase, the system needs to make sure that the active servers will be able to satisfy the read performance requirement. one option is to aggressively restore the equal work data layout before reintegrated servers begin servicing reads. we call this approach aggressive migration. before anticipated workload increases, the migration agent would activate the right number of servers and migrate some data to the newly activated servers so that they store enough data to contribute their full share of read performance. the migration time is determined by the number of blocks that need to be migrated, the number of servers that are newly activated, and the i/o throughput of a single server. with aggressive migration, cleanup work is never delayed. whenever a resizing action takes place, the property of the equal-work layout is obeyed-server x stores no less than b x blocks."
"the partial derivative of the gaussian with respect to dimension x or y can be computed as the product of the hermite polynomial and the gaussian function,"
"this section evaluates springfs in terms of machine-hour usage with real-world traces from six industry hadoop deployments and compares it against three other storage systems: rabbit, sierra, and the default hdfs. we evaluate each system's layout policies with each trace, calculate the amount of cleanup work and the estimated cleaning time for each resizing action, and summarize the aggregated machine-hour usage consumed by each system for each trace. the results show that springfs significantly reduces machine-hour usage even compared to the state-of-the-art elastic storage systems, especially for write-intensive workloads."
"in contrast, rabbit can shrink its active footprint to a much smaller size (≈10% of the cluster size), but its reliance on everest-style write offloading [cit] b ] induces significant cleanup overhead when shrinking the active server set, resulting in poor agility."
"most distributed storage is not elastic. for example, the cluster-based storage systems commonly used in support of cloud and data-intensive computing environments, such as the google file system(gfs) [cit] ] or the hadoop distributed filesystem [cit] ], use data layouts that are not amenable to elasticity. the hadoop distributed file system (hdfs), for example, uses a replication and data-layout policy wherein the first replica is placed on a node in the same rack as the writing node (preferably the writing node, if it contributes to dfs storage), the second and third on random nodes in a randomly chosen different rack than the writing node. in addition to load balancing, this data layout provides excellent availability properties-if the node with the primary replica fails, the other replicas maintain data availability; if an entire rack fails (e.g., through the failure of a communication link), data availability is maintained via the replica(s) in another rack. but, such a data layout prevents elasticity by requiring that almost all nodes be active-no more than one node per rack can be turned off without a high likelihood of making some data unavailable."
"note that the spread increases exponentially with the number of replicas while maintaining ideal power-proportionality. since the maximum throughput obtainable depends on the spread, this allows the equal-work policy to obtain a high value for the same. we note that, since the spread also depends on p, a spread spanning the entire cluster can be obtained with any number of replicas r by adjusting the value of p."
"in this paper, the mounted sa based approach is utilized to detect and monitor the cracks in concrete members under loading for simple concrete beams. the results will be analyzed and compared with other traditional algorithms to evaluate the accuracy and effectiveness of the system."
"replica placement. when a block write occurs, springfs chooses target servers for the three replicas in the following steps: the primary replica is load balanced across (and thus bounded in) the m servers in the current offload set. (the one exception is when the client requesting the write is in the offload set. in this case, springfs writes the primary copy to that server, instead of the server with the least load in the offload set, to exploit machine locality.) for non-primary replicas, springfs first determines their target servers according to the equal-work layout. for example, the target server for the secondary replica would be a server numbered between p+1 and ep, and that for the tertiary replica would be a server numbered between ep + 1 and e 2 p, both following the probability distribution as indicated by the equal-work layout (lower numbered servers have higher probability to write the non-primary replicas). if the target server number is higher than m, the replica is written to that server. however, if the target server number is between p+1 and m (a subset of the offload set), the replica is instead redirected and load balanced across servers outside the offload set, as shown in the shaded regions in figure 5 . such redirection of non-primary replicas reduces the write requests going to the servers in the offload set and ensures that these servers store only the primary replicas."
"the results in figure 12 are an average of 10 runs for both cases, shown with a moving average of 3 seconds. the i/o throughput is calculated by summing read throughput and write throughput multiplied by the replication factor. decreasing the number of active springfs servers from 7 to 3 does not have an impact on its performance, since no cleanup work is needed. as expected, resizing the cluster from 3 nodes to 7 imposes a small performance overhead due to background block migration, but the number of blocks to be migrated is very small-about 200 blocks are written to springfs with only 3 active servers, but only 4 blocks need to be migrated to restore the equal-work layout. springfs's offloading policies keep the cleanup work small, for both directions. as a result, springfs extracts and reintegrates servers very quickly."
"data placement agent. the data placement agent determines where to place blocks according to the springfs data layout. ordinarily, when a hdfs client wishes to write a block, it contacts the hdfs namenode and asks where the block should be placed. the namenode returns a list of pseudorandomly chosen datanodes to the client, and the client writes the data directly to these datanodes. the data placement agent starts together with the namenode and communicates with the namenode using a simple text-based protocol over stdin and stdout. to obtain a placement decision for the r replicas of a block, the namenode writes the name of the client machine as well as a list of candidate datanodes to the placement agent's stdin. the placement agent can then filter and reorder the candidates, returning a prioritized list of targets for the write operation. the namenode then instructs the client to write to the first r candidates returned."
"in the equal-work data-layout policy, the first p nodes of the expansion-chain are called the primary nodes. one replica of the dataset, called the primary replica, is distributed evenly over the primary nodes as shown in figure 1 . keeping only these p nodes, on is sufficient for guaranteeing the availability of all data. because p n, this gives rabbit a low minimum power setting."
"to share work equally, each gear group should contain approximately the same amount of data from each primary. the amount of data stored on each server in a gear group depends on where that gear group falls in the expansion chain. servers belonging to low-numbered gear groups must store more data than those in high numbered gear groups, because they may be activated at lower-power modes. if the last server in a gear group is server number i, then every server in the gear group stores b i blocks. equivalently, with a group size of g, each server in gear group j stores b (p+gj) blocks. figure 9 shows the results of a simulation of the fault-tolerant layout in a failure-free case and with a single primary server failure. the performance is measured relative to the performance of a single server. the steps in the solid line show the effect of gearing: increasing the power setting causes no improvement in performance until a gear is completely activated, at which point the performance jumps up to the next level. the dotted line represents the power and performance curve in the case of a single primary failure. the file system can achieve the same performance with only a moderate increase in power."
"springfs's read offloading policy is simple and reduces the cleanup work resulting from write offloading. to ensure that its simplicity does not result in lost opportunity, we compare it to the optimal, oracular scheduling policy with claircognizance of the hdfs layout. we use an integer linear programming (ilp) model that minimizes the number of reads sent to primary servers from which primary replica writes are offloaded. the springfs read offloading policy, despite its simple realization, compares favorably and falls within 3% from optimal on average. figure 12 illustrates springfs's ability to resize quickly and deliver required performance levels. it uses a sequence of three benchmarks to create phases of workload intensity and measures performance for two cases: \"springfs (no resizing)\" where the full cluster stays active throughout the experiment and \"springfs (resizing)\" where the system size is changed with workload intensity. as expected, the performance is essentially the same for the two cases, with a small delay observed when springfs reintegrates servers for the third phase. however, the number of machine hours used is very different, as springfs extracts machines during the middle phase."
"the ability to allocate i/o bandwidth available through the dfs to specific applications that run on it would have significant benefits. previous results [cit] show that almost 5% of the jobs observed in a large-scale data center run for more than five hours and some jobs run for more than a day. in the presence of such long-running jobs, it is imperative to be able to guarantee some notion of fair sharing of the resources of the cluster. there should be capabilities, for example, to temporarily decrease the performance of long jobs during times of high load or when there are higher-priority, shorter-running jobs to be processed. although hadoop or an equivalent implementation of the map-reduce paradigm has its own scheduler, the underlying dfs will most likely support multiple kinds of applications in the data center. for example, google's bigtable [cit] ] and hadoop's hbase are designed to work directly on top of the dfs. it is not possible, with current solutions, to guarantee i/o performance for each of these jobs. in other words, there is no check on a single job monopolizing the i/o resources of the cluster. this problem is often exacerbated by the fact that jobs are increasingly data-intensive, such that their overall performance depends significantly on the amount of i/o bandwidth that they receive."
"this data layout creates a trade-off between gear size and recovery group size. a smaller gear size implies a larger recovery group size. by setting the gear size very small, we can achieve the goal of fine-grained power settings, but the large recovery group size means that in the event of a failure the minimum power setting will be high. note: even with a gear size of 5, allowing very fine grained scaling, the difference in minimum power setting is only 37%."
"choosing the offload set. the offload set is not a rigid setting but determined on the fly to adapt to workload changes. essentially, it is chosen according to the target maximum write performance identified for each resizing interval. because servers in the offload set write one complete copy of the primary replicas, the size of the offload set is simply the maximum write throughput in the workload divided by the write throughput a single server can provide. section 5.2 gives a more detailed description of how springfs chooses the offload set (and the number of active servers) given the target workload performance."
". as an example from this table, if the gear size is 10% of the number of primary servers, the recovery group size will be about 20% of the primary size. this means that the minimum power setting during failure recovery is only 20% higher than the minimum power setting with no failures. the ideal setting of these parameters depends on the workload of the file system and the rate of failure, but these results show that there is a wide range of reasonable settings for these parameters."
"the time required for such migrations has a direct impact on the machine hours consumed by elastic storage systems. systems with better agility are able to more effectively exploit the potential of workload variation by more closely tracking workload changes. previous elastic storage systems rely on very infrequent changes (e.g., hourly resizing in sierra ), but we find that over half of the potential savings is lost with such an approach due to the burstiness of real workloads."
"to achieve the theoretical ber of the proposed system, it is important to analyze the received signal exhaustively. it should be noted that, since virtual-four-antenna stcdtd diversity is considered, the following discussion is based on both even and odd time intervals, which is different from traditional singleantenna case."
"a performance comparison is made between 2-d blockspread cdma using relay virtual-four antennas, and the source user directly adopts 2-d block-spread cdma with fourantenna sttd/cdtd without any relay scheme, as shown in figs. 10 and 11. relay-virtual-four-antenna stcdtd has a much better throughput than four-antenna sttd. here, the total throughput s is defined as"
"in this paper, only two source antennas and two relay antennas are considered, which form a virtual-four-antenna stcdtd system. of course, more than two transmit antennas can be used; however, it is quite difficult if not impossible to theoretically analyze the achievable ber performance for the case of more than four antennas. a ber analysis of 2-d blockspread sc-cdma using virtual stcdtd with more than four antennas is left as a future study."
"the remainder of this paper is organized as follows. section ii describes the uplink cooperative relay network model. in section iii, the proposed 2-d block-spread cdma relay using virtual-four-antenna stcdtd is described. section iv presents the theoretical ber of the proposed scheme. the achievable ber performances of the proposed relay system are evaluated by computer simulation in section v. finally, section vi offers some concluding remarks."
"as a result of aiming to reveal the big data emerging technology, the wide range of percentages among the teachers needs to understand the way of using technology as a dynamic tool to apply it well. in terms of olr in assisting the framework of big data integration with cloud computing basis, the need to get improve the skills development should be considered as the task in supporting adaptive teaching performance and competencies. among interpersonal skills in enhancing their teaching skills, students learning experience can be yielded into the myriad of useful resources together with the sustainable learning culture [cit] . in terms of the dynamics of learning with ethical consideration, they can understand the good way to the learning process to enable them be aware of circumstance sphere [cit] . in particular, sharing the knowledge together with pointing out the result about the academic performance can be carried out the engaged basis of integrating the video presentation, internet sources use, and so on. this opportunity refers to maximise the use of resources from such divergent platforms throughout inevitably learning to achieve teaching and learning quality. moreover, innovative environment refers to accommodate in realising the benefits of olr among the teachers and learners. as an entire opportunity which can be adopted among the higher education (he) context, the component of learning and teaching including interactive online video presentation for instance should be adopted in underlying the learning styles to meet the needs and demands among them as shown in figure 2 ."
"where e c is the average chip energy, and t c is the chip duration. during the first time slot, antennas #0 and #1 of the source user transmitŝ u, 0, 0 (t) andŝ u, 1, 0 (t), respectively. before transmission, an n g -chip guard interval (gi) is inserted at every n c -chip block to avoid the interblock interference. the gi-inserted signal is transmitted to the relay user through the frequency-selective fading channel."
"in particular, perfecting the use of whole load of information modified to check in enabling the students to have chance in creating their interest into the particular subject about the certain topic gathered through online sites when connecting technology that acts as a medium in the internet basis. the form of collecting such different structured data needs to expand worldwide of information within the development concept of pedagogy in the digital era [cit] . moreover, information pattern referring to data elucidation in sns basis offers sites to let the users to have such different experiences in searching for materials collection. in terms of storing the uploaded materials used within wide range of programmes, big data emerging technology refers to have the skills particularly in using the digital devices required into the good internet connection [cit] . towards the highly advanced modern technology, the completion of setting the skills with allowing the technology connection among the users is reserved to entering the particular web address. as a result, the internet basis for postings and uploads of webpage or websites in addressing the process being made in the way which is formed into the basis of numerical format and words through online would automatically be saved in the big data. this initiative would be viewed in the basis of getting information data massively. moreover, taking roughly about the set up for the specific web address would show the screen computer which can be managed among the users."
"the theoretical and simulated average ber performances of 2-d block-spread sc-cdma using virtual-four-antenna stcdtd are plotted in fig. 6 . the theoretical average ber was obtained using the monte carlo numerical method. first, the conditional average ber for the multipath channel gains was computed using (6), (15), and (26) . then, the average ber was obtained by repeating the above computation a sufficient number of times by changing the multipath channel gains. to obtain the simulated average ber, the average ber was measured by transmitting a large number of chip blocks. owing to compare the transmitted and received digital data, the simulated results could be achieved. it is found that the theoretical results agree fairly well with the simulated results. hence, for convenience, we only show the results obtained by the simulation method."
"the basis of innovative environment for olr provides the learning sources with its distinctive pattern specifically in enhancing the learning performance. it rises to advance the way more convenient and active to contribute in supporting the learning performance [cit] . the pattern with innovative basis in assessing the learning resources refers to engage in assessing learning material resources. as a result, the convenient and innovative basis in the approach for adopting intervention, customization and personalization should be engaged with adopting the way in absorbing the massive data amounts. moreover, attempts to adopt data sources such as surveys, social network sites, newspaper and other sources would be achieved in the practical basis to cover the assortment and volume [cit] . in terms of supporting orl in the way to configure the self-regulated inquiry, attempts with interactive essence configured into the innovative style may become an entire element to commit the learning style by maximizing the resources of learning material assessed in the way to strengthen the learning environment basis [cit] . empowering the olr with big data approach can be seen in assessing material resources more flexible and to maximize the way in operating time and place. in particular, contributing the significant enhancement in terms exploring online resources has an insight to do with the big data emerging technology initiative in supporting the learning process. determining the guide way in gaining multitude resources refers to attempt to enable them in getting more benefits with encouraging controlling particular technology basis."
"noise, e(o) (n))/3) is the average variance that is used, instead of the common variance. equation (47) is only an approximate expression because of the approximate calculation of the average variance. we can use the feature of chi-square random variables that their distributions approach that of gaussian random variables when the number of degrees of freedom becomes large. thus, to simplify the processing of calculations, we viewed x u, e(o) as complex gaussian random variables instead of chi-square random variables, and the variance of x u, e(o) is equal to"
"in the last decade, the shift paradigm of human life has been widely emerged from face to face interaction basis to virtual one. especially in the information age to aid the life sphere including all aspects of our life, we are relying on information and communication technology (ict) directly or indirectly. as a result, ict use in all dimensions of human life can be generated into the data volume set into the pattern followed by the needs and demands. these distributions should be involved with the analytics patterns with big data approach. the data can come from online activities where the users usually upload, retrieve and also store the information shared through the internet platform [cit] . this will lead to fulfil in forming the model pattern of big data which can directly be engaged in supporting the human daily needs."
"as a result of contributing the insights for the research agenda, the idea initially in focusing on multi sources data provides the recommendation to propose the model framework in delivering the material resources in the learning process. with big data emerging technology, adopting the network data massively from the social media would enhance in providing the particular means in extracting the value from information space such as message, conversation, transaction and others [cit], where the sources of structured data come from enterprise resources data and sources of unstructured data come from audio and video [cit] . expanding the process of extracting the value from social network to pattern the data sources to fulfil the organisation goal aims to reveal the way of big data approach in extracting data value from data complexity involving variety and velocity into the volume. it can be achieved to develop prototype using data analytics associated from the topic, users and time analytics."
"along with information divergences deriving from a variety of types such as facebook contents and email as unstructured data and cash transaction as structured data, the variability of massive data could be generated into the trends of social media data. with this regard, the flow of data gathered from the variability refers to daily and seasonal and other reasons using big data methods in addressing interrelation with pointing out practical indications of relations among the variables [cit] . considering such events with the behavioural basis in high rate data flow, the categorizations which can be possibly linked to the data complexity should not be confused with data variety [cit] . moreover, the divergent types of data which comes from conversation, electronic messages, and photos or videos enable the initiative of data variety in finding the corresponding and connection among them which might later be organised using the analytic programs to transform the data from other sources like blogs, webpage and social network site (sns). as a result, operational and analytical systems can be adapted into different type of data formats as an attempt to recognise the pattern of their connection in corresponding to organize data sources from other parts in transforming across the systems."
"in the first time slot, the source user broadcasts its signal. the relay user receives and recovers the source user's transmission signal using the df strategy."
"in this paper, the chip-spaced discrete-time signal representation is used. a is the largest integer smaller than or equal to the real-valued variable a and a is the smallest integer larger than or equal to a. antenna #0 and antenna #1 are the source user's antennas, whereas antenna #2 and antenna #3 are the relay user's antennas. the 2-d block-spread cdma consists of two-level spreading: chip-level spreading and blocklevel spreading. the transmitter/receiver structure is shown in fig. 3 ."
"is the average interference-plus-noise power. we can directly substituteγ u, o (n) andγ u, e (n) into (37), and then calculate the average ber. if we can confirm the distributions of the above random variables, the pdfs can be obtained."
"roadband data services with a peak data rate of around 1 gb/s will be demanded of future wireless communications [cit] . to support such a high data rate, two important technical issues should be addressed: transmission power and spectrum efficiency."
"in particular, technology usage demand on internet basis would make the model of learning style adopted specifically among the learners in their own capacity basis. enhancing the serious effort with the models of big data integration stage attempts to get access for online resources using electronic devices convert to shift from the traditional basis to virtual approach in the way with a variety of innovative learning interactions through high-impact educational experiences [cit] . big data emerging technology offers an entire enhancement with its data analytics to generate the massive data patterned into the need and demand referring to the online resources for teaching and learning. as an attempt to obtain the initiative in improving the students' learning process and achievement, the extent to propose the innovative learning design configures the empowerment by expanding pedagogy and technology skill. with this regard, the basis of big data emerging technology analytic process attempts to support the initiative way of teaching and learning process. big data approach integrated into online learning in the way to addressing the learning behaviour is supposed to give insights in contributing the reference model of big data emerging technology for olr initiative basis."
"the uplink cooperative relay network mode is shown in fig. 1, where user 1 transmits its information to the bs with cooperation from user 2, and other users are an interference to user 1."
"the sttd is applied at the first time slot for cooperative relaying. in the second time slot, virtual-four-antenna stcdtd is simultaneously explored at both the source user and the relay user to increase the diversity order."
"big data is a platform to generate massive data gathered from the user's communication using digital devices such as tablets, smart phones, and laptop. with the numeric figures of data gathered particularly from the huge data amount, facts of statistics are being generated among the digital devices such as mobile phone, computer or laptop to extract the value of social network [cit] . towards the platform of big data emerging technology, data collection from the sources of traditional basis and digital one can be more complex and larger to manage with the computing technology use [cit] . in the way to basically manage and analyse massive data amount, extra tools in generating big data analysis are required to settle such kind of several difficulties on the complexity of big data in both unstructured data referring to the unorganised information interpreted easily like pictures, documents, videos, and also social network site like twitter, facebook and whatsapp, and structured data deriving from data obtained from the interaction among the users with machines such as web application use. interpreted in many different ways in the way to describe massive data, both interaction and application use can be enhanced using volume, velocity and variety [cit] . all these refer to assist in adopting the generated data to be delivered into the pattern which would benefit to the organization in terms of demands and needs. moreover, the supplying system was also assisted by introducing the two characteristics including complexity and variability. as a result of handling the massive data generated from the internet platform, these components would be adopted to analyse data appropriately in order to give insights of big data emerging technology which may later be fitted to the beneficial purpose as shown in figure 1 ."
"addressing the basis of online learning in covering the system of information is configured into technology where this is involved between the users and their performance way including the strategies and capabilities. enhancing online learning re-sources is widely addressed into supporting the learning process. this is followed in sharing olr based innovative environment to adopt the strategies which can be employed in facilitating the processes and stores, disseminating educational material, and supporting communication and administration configured into teaching and learning [cit] . attempts to run online learning process can be initiated to signify the course materials with pointing out management, distribution, and retrieval [cit] . olr is signified into the information systems where the process itself can be recorded well in convincing the efficient and efficient teaching basis [cit] . with this regard, establishing the basis in supporting the teaching and learning process should accommodate dynamic interactive and alternative learning experiences among the students. attempts to incorporate olr with knowledge management from divergences of social networks and multi-channels could be generated into the process in providing learning experiences with comprehensive requirements."
"in this paper, it is assumed that two transmit/receive antennas are used at both source and relay terminals. we present a scheme, which combines the source user's two antennas and the relay user's two antennas to form a virtual-four-antenna space-time cyclic delay transmit diversity (stcdtd), exploring a 2-d block-spread cdma (both single-carrier cdma (sc-cdma) and mc-cdma are considered) relay using the decode-and-forward (df) strategy to avoid mai since 2-d block-spread cdma [cit] achieves good ber performance with low-complexity single-user detection. another important contribution of this paper is that the theoretical ber of the proposed scheme is described, when the signals are transmitted through a multiple-access frequency-selective rayleigh fading channel."
"in line with determining the way supported in learning instruction, the subsequent enhancement in the way to transform the performance basis to raise the achievement can be taken in benefiting the students' development to strengthen the entire interaction within the process of interaction. attempt to achieve in encouraging the learning style more actively engaged into getting the way entirely with instructional process to improve thinking skills has to do consequently with boosting their confidence to raise the skills of learning enhancement [cit] . adopting big data emerging technology with actively presenting the knowledge basis and skills is fundamental with powerful skill and adaptive technology in achieving flexibility and connectedness. moreover, the committed mission in giving insights into the online learning basis undermines the control basis within integrating high-quality information [cit] . the effort which students may employ in interacting with the learning environment can be engaged into the following extent of engagement including meaningful learning with its interaction. allowing them to interact with such distinct kinds is significant to assist learning process attentiveness. designing the innovative learning referring to integrate knowledge and skills generates the valuable insights of skills integrated into the approach of innovative basis on learning enhancement. this regard would be adaptively strengthened with big data emerging technology. figure 4 shows big data emerging technology in supporting resources for online learning. the way of big data emerging technology to support olr refers to progressively strengthen the subsequent extent of introducing, preparing and managing the arrangements of learning within online basis [cit] . as a result, considering the learning approach to improve necessary implementation basis will lead to enhance the convenient way of learning style. with more upgrade to cover such usability on the technology platform, successfully implementing the way of learning process should be prepared in providing the better facilitation with the support of technological tool within the activities of learning process with borderless space. through utilizing extraction of massive data amounts created every second across the social network, innovative based learning model considered into the kind of learning activities is subsequently regulated in providing the entire extent of beneficial values in strengthening pervasive knowledge based learning enhancement [cit] . with regard to the technological empowerment in enhancing the learning basis within the perspective of anywhere and anytime context, technological tool with big data-based learning sources points out the academic skills on the availability of influencing the effectiveness of learning with a sufficient support in the way to corporate the collaboration basis. an attempt to empower the orientation in enhancing the practical strength would enhance to make easily in gaining the learning sources with the big data analytic approach [cit] . emphasizing the voluntary data complemented particularly into the big data analytics with the learning process should be engaged in making a whole context to underlie online learning sources indicators in the way of assessing the practice by widening the data complexities."
"as all signal modulation points are equiprobable during odd and even time intervals, considering that binary phase-shift keying (psk) is used, the conditional ber p u, b (γ u, o (n), γ u, e (n)) is achieved as"
"according to fig. 7, it is seen that the ber floor exists when no antenna diversity is used; however, the use of antenna diversity can sufficiently suppress the floor and achieve a good ber performance. this is because antenna diversity takes full advantages of the frequency-selective rayleigh fading channel by using fde."
"furthermore, the result in utilizing such massive data should be enhanced in organizing and finding connection among diverse data sources like governments and corporations. adopting data in helping to discover the issues, the way of operational systems in providing interactive workloads can be offered with a particular enhancement in offering the capabilities with an actual time primarily set out keeping the operational system together with analytical systems [cit] . this initiative refers to enhance the basis of analysis on complex data bringing into the action through emerging technologies of big data approach. as a result, attempts to enhance the massive data generated to select their value can be combined into the operational big data system. it can be generated into databases for instance in the way to address a broad set of applications deployed in optimizing specific applications like database of graph [cit] . towards new cloud computing architectures to effectively use efficiently and inexpensively, operational big data can be made with easier attempts together with managing to address the limitations of traditional databases system. in the attempts to make faster in applying the analytical big data, these analytical workloads give insights in providing valuable information with the particular systems used to measure the resources by increasing data volume [cit] . the ability to operate beyond the single server with operational and analytical big data system can generate the precious information to fit for the good purpose."
"the first (μ u, e andμ u, o ), second (μ mai, e and μ mai, o ), and third (μ noise, e and μ noise, o ) terms in (21) are the desired signal; the mai from other users and noise, respectively, as in (22) and (23), shown at the bottom of the next page; and"
"however, which cdma is selected by system designers should be determined not only by the ber performances but also by other factors. for example, for the downlink transmission (from the bs to the user terminal), mc-cdma is more flexible in frequency-time resource-block allocation than sc-cdma. however, for the uplink transmission (from user terminal to bs), sc-cdma provides more advantages due to its lower peak-to-average power ratio."
"in line with the scales of data explored into the big data volume, attempts to manage the wide range of sources including structured and unstructured data should concisely proceed into the velocity with the way in streaming data to be analysed in further. these like text messages, videos, pictures and many others need to gather the analytic process in order to generate the sources [cit] . in terms of gathering the various sources which need the process with analysing the variety together with the velocity, the disparity of data flow depends on certain developmental indicators usually assumed to make decision and act quickly without wasting time. with this regard, velocity becomes more significant to let data information interpreted in the basis of big data approach on emerging technology. thus, the companies or organisation have chance to transmit such data to be interpreted referring to the needs and demands for instance health care purpose, teaching and learning materials."
"revealing the theoretical basis on big data emerging technology to give insights into the innovative environment of olr, this paper provides a model of framework designed to help teaching and learning through the facility on learning resources. designing the interactivity of big data on olr construction refers to enhance in giving experiences in distinct educational exploration to assist the students' learning environment. big data emerging technology with analytic process provides particular advantages to transform the pattern of information fitted into the innovative learning environment to enhance in developing the learning resources. both prototype and model of data extraction value could be enhanced to facilitate the learning environment in supporting implementations with ease and convenience. this study is expected to contribute to improve the learning environment and outcomes with performance and achievement by enhancing students' learning process development to provide online resources in higher education context."
"equations (44) and (45) show the variances of noise signals. since the mai, mani, and noise can be viewed as being independent and identically distributed, the average power of interference plus noise can be given as"
"what should be noted is that (21) reduces to (11) when the fading is slow enough. in this case, since the path gains stay constant over at least consecutive blocks, the mai can be perfectly removed by the block-level despreading. it should be noted that (21) is developed for the theoretical analysis of ber; (11) is directly used for block-level dispreading at the receiver."
"in line with addressing the wide ranges of needs for teaching and learning purpose, online resources such as data sets or podcasts where the teachers show the video in class would emerge in providing the gateway with a whole level of learning. in this view, addressing the range of learning styles to point out individual online interaction needs to support technology resources including computers, smartphones, tablets, digital cameras, networks and social media platforms, etc. [cit] . as a result, olr can be initiated to integrate among the lesson plans, curriculum, and subjects exposed easily available among the learning activities to make it successful. with this initiative to all sorts of learning activities integrated through themed activities in daily classroom practices, the skills development in the management of accommodating the frequent online resources types such as online journals, online books, or databases can be more manageable through more unique approaches with the plenty of benefits for the online resources [cit] . it can be accessed with progressing the professional opinions combined into the teaching management covering space, time and frequency in the way to get information data using medium of online electronic message such as e-mail [cit] . both availability and accessibility of resources in pointing out convenient basis in the adoption within anytime and anywhere need to arrange the knowledge construction which may possibly be integrated into technology tool. in particular, the massive pervasive knowledge could be accommodated through the ict advancement in learning and teaching activities."
"in terms of providing the opportunities in the way to maximize the potential data value collected within big data approach, attempts to obtain the distinct feature in online learning systems refer to the pattern of integrated learning innovatively to transform the interactive practice configured with self-regulated inquiry [cit] . with this regard, the inquiry basis regulated into the customized and personalized services into the learning process offers the knowledge delivered to explore the potential data with an entire initiative to see the behaviour performance [cit] . as a result of viewing the students' learning way, the ability with more precision in managing the materials prepared in such extents of the system needs to take in particular to supply the resources of link which is fitted to the relevant needs and interest. moreover, developing the structure of learning course through understanding the learning quality in the way to determine the activities is entirely a pivotal essence to generate learning activities with enabling them to get access among the wide ranges of webpage [cit] . figure 3 shows big data emerging technology for olr based innovative environment. performing the detailed activities frequently refers to an entire effort to undertake innovative environment in big data emerging technology for olr. as a result of understanding the initiative to provide the topic content, it can be achieved from the pattern by adopting their activities to the system related to the needs of personal learning appropriately. in terms of the initiative to raise the platform in attracting increasingly to the attention basis, big data emerging technology for effective learning integrated into the approach with an essential competence could be engaged into the operational system design."
"equation (37) can be viewed as conditional error probability, where the condition is that γ u, o (n) and γ u, e (n) are constant. to obtain the error probability when γ u, o (n) and γ u, e (n) are random, the average ber p u, b is given as (38) where p(γ u, o (n), γ u, e (n)) is the joint probability density function (pdf) of γ u, o (n) and γ u, e (n). if we can obtain the pdf of p(γ u, o (n), γ u, e (n)), it is possible to derive the accurate theoretical ber expression. however, it is extremely difficult to achieve the distributions of sinr due to the complexity equation. a reasonable simplification is to assume that the pdf of p(γ u, o (n), γ u, e (n)) is not random but constant to simplify the complexity of the calculations. thus, we define the average"
", which can be viewed as a chi-square distribution (also chisquare or χ 2 -distribution) with three degrees of freedom. regarding the chi-square distributions, it is always assumed that"
"the innovative environment basis which is distinctively engaged into big data emerging technology to support the learning interaction refers to enhance the way in transmitting the platform to transfer the data value. with an entire process to extract the social network in engaging the learning sources, the pattern here refers to structured and unstructured data [cit] . extracting the potential content value aims to improve the experiential learning basis with regard to adopt the massive open basis which enables the learners to get access on online sources freely [cit] . in terms of enhancing the number of participants, the convenience in underlying the learning process can be managed in pointing out the competence of technological basis. as a result, working together with managing the learning style in the way to assist the learners with the strategies configured into the innovation with big data analytic process would enhance information in enabling to effectively reach their interest and needs [cit] . in particular, online activities can be deployed with technology adaptive skills which may help accommodating the learning material resources. in this view, the raise of the potential enhancement in finding out unlimited number on online activities is entirely configured using any subject aligned to assist the learning performance. in terms of the way to achieve the effective online learning with the easy access, the usability extent towards the technology-based learning refers to an initial requirement with a particular achievement in the way of human awareness to drive it appropriately [cit] . both process and management skills should be engaged into the application guideline to support assessing the multi-channels of sources of knowledge [cit] . with regard to attempting to obtain the computer interface design in determining the benefit about the big data application, making useful about the potential data refers to get involved into the learning style designed with the basis of innovative approach. it refers to transform the reflective essence in empowering the learning environment innovatively in providing olr platform basis."
"the network protocol used in this paper includes two time slots, as shown in fig. 2 . during the first time slot, the source user broadcasts its signal with small transmit power. the relay user who is located nearby the source user is able to receive the transmission signal from the source user's signal during the first time slot."
"in this paper, a 2-d block-spread cdma using relay-virtualfour-antenna stcdtd has been proposed, and the theoretical ber has been discussed in detail. computer simulations showed that the proposed scheme is capable of removing the mai when the channel is slow fading, meanwhile obtaining both the frequency diversity gain and antenna diversity gain, and therefore significantly improving the ber performances. while the decoding error at the relay is a critical and important issue for the proposed scheme, investigating a realistic ber performance with harq in cooperative communications is left for future studies."
"in line with supporting learning management aiming to successfully implement the learning process, providing online learning basis can be set up with the number of managing the process of automating the record-keeping with employing registration. although many studies were conducted on big data in enhancing teaching and learning process [cit], there are few scholarly attentions to address the approach of emerging technology with big data to generating to online learning sources. thus, this study aims to fulfil this gap by reviewing literature on the big data emerging technology in giving insights into the innovative environment for online learning resources (olr). extracting the social network value is followed by the critical analysis to propose the model framework in delivering the material resources for the learning process."
"in terms of getting and collecting information with big data approach, this can be proposed to enhance the innovative learning model design [cit] . moreover, attempts to design and manage big data model for innovative learning refer to engage with managing and storing such kind of intangible assets such as report and documents referring to knowledge and information pattern with the purpose of administration basis [cit] . as a result of providing electronic learning with pointing out the application software which is capable and designed to basically yield an integrated platform in delivering the content configured into supplying the accessibility with a wide range of users involving students, content creators, lecturers as well as administrators [cit], such kind of benefit can be achieved with the attempts to ease the learning activity [cit] . with pointing out the beneficial tool as provided in the online learning basis, academic institutions like schools and universities should facilitate in managing the expertise among the users by providing the number of corporations of training programs."
"step 3 ranks four objects w1 to w4 in order of performance value obtained by topsis. based on eqs. (9) and (10) and estimated weights in step 2, a normalized object matrix va and normalized standard matrix vs can be written as with matrices va and vs and eqs. (11) for w4. levels h 1 to h 4 can also be transformed to typical levels iii, v, iii and iii, respectively. the resultant typical levels are consistent with those obtained by an ahp-based fuzzy method [cit] ."
"a comparison with gsm, gmm and the gaem is illustrated in fig. 2. fig. 2(b) displays the modeled error distribution resulted from gsm. the results from standard gmm with 3 components are shown in fig. 2(c) and fig. 2(d) . because standard gmm cannot determine the number of the components automatically, it bring inherent distribution modeling error. the gaem are employed to model the error distribution, with results shown in fig. 2(e) and fig. 2(f) . gaem can automatically determine the number of the components and reduce modeling error in this process."
"compared with bus injection model, branch power flow provides better usability and stable computation results for distribution network. for a three-phase power system, the branch power flow s"
"according to the above discussion, the proposed gaem is capable to accurately model aggregated error distributions and determine the number of components at each feeder, which can effectively convert the chance constraints of the forecasting error model in (5h) (5i) into deterministic problems."
step 1 of the tiwr approach obtains the following normalized object matrix na and normalized standard matrix ns based on eqs. (3) and (4) and matrices a and s.
"in this study, the micro wind turbines and pv panels in the system operate at approximately 97.5% maximum available power to create power margins for optimal power flow. the distribution line loss corresponding to fig. 4(a) is shown in fig. 8(a) . the magenta curve indicates the total distribution line loss without opf, and the blue curve indicates the total distribution line loss with opf. it is clear that the opf can reduce the total distribution line loss while meeting the hourly scheduling requirements. specifically, as in (40) the g r2 is defined as 2.5% of the g r, which is the upper bound of the injection power control for the opf. at the peak generation of the renewable energies, the cost of system loss with opf decreases more dramatically than others."
", or ns c j, stands for worse water quality. note that eq. (3) has elements of matrices a and s while eq. (4) has only elements of matrix s. this is a one-way parameter propagation from water quality standard to indicators, marked with ① in fig. 1 ."
"is known as the observed data and incomplete, ξ m is the component identity of x m . the algorithm will be ended when the log likelihood function in (7) reaches the convergence."
panel (a) of table 1 can be regarded as object matrix a of four objects with eight indicators and panel (b) as standard matrix s. note that do is a benefit indicator and the others are cost indicators.
"for further analysis the original sample was divided into two parts: the \"learning\" part (1q2003-2q2011, 34 values for each variable) was utilized for index construction and the second part was used to verify the forecast accuracy for each index and compare different ifsis. to measure the quality models pearson and spearman correlation coefficients were explored"
"when all 3 groups of methods are considered then the pairwise regression with \"commercial real estate loans to total loans\" as independent variable gives the best approximation. this allows us to assume that this factor was used (explicitly or implicitly) in economic resilience index construction."
"it's clear that the ifsis obtained with the first group of methods can reflect only the dynamics of financial stability, not the absolute values. i.e. generally speaking the ifsis' values not necessary coincide with the values yl or ys. in order to simplify comparative analysis indexes were transformed using the coefficients obtained from the regression:"
"step 4 identifies level h i associated with water quality on the basis of the weighted average principle and fuzzy theory. water quality indicators and associated standards are coupled by eq. (3) in step 1, eqs. (10), (11) (14) in step 3 and eqs. (21)(23) in step 4. the tiwr approach is applied to two case studies. case 1 in section 3.1 presents a detailed process of applying the tiwr approach to evaluate water quality for the shitoukoumen reservoir. the tiwr and ahp-based fuzzy approaches give the same typical level for all objects. in addition, our approach offers more delicate and helpful level h i accounting for water quality than typical levels of i to v. case 2 in section 3.2 presents water quality evaluation for lake tai. it can be reasoned that the tiwr approach shows high consistency with official reports and produces more reasonable results than traditional fse approach. these findings indicate that the tiwr approach have important implications for decision makers and researchers in water environment protection and management."
"as shown in fig. 6(a), the proposed method is applied in the university of denver campus grid model as a real power system to demonstrate the effectiveness. in the campus grid model, there are 57 buildings and 6 buildings are installed with renewable generators, which are olin hall, daniels college, sturm hall, ritchie center, law center and newman center. the total rated power of the renewable energy is around 8 mw. [cit] of du campus were realized in this study. in fig. 7(a), the results show the total operation cost can be effectively reduced with the proposed approach. the pink line indicated the system operation cost can be furthermore decreased with the corrective action in (2) at some time of the day, which indicates the corrective action is used to resell the redundant power to electricity market. the results of the du campus demonstrate the proposed method can be operated well on the real-world power system models."
"consider m water quality indicators monitored at a site as elements of a vector called an \"object\". an object matrix a of n objects can be written as:"
"in real applications, the fast-developing technologies such as energy storage, gas, electrical vehicle, and water can benefit a lot for distribution systems, which also bring more challenges for distribution systems integration with gas systems, water systems, transportation systems. in the next step, these factors will be taken into considerations for future research."
step 3: ranking objects topsis is a powerful ranking technique for multiple attribute decision making problems [cit] . it can be applied to rank objects in order of performance value as follows. construct a normalized object matrix with weighted elements expressed as
"where vs c j, is defined in eq. (10). performance values c i and cs c depend on the elements of both matrices va and vs. this is a two-way parameter propagation between water quality indicators and their standards, marked with ③ in fig. 1 ."
"is defined, which is limited by the renewable generation in (40) . based on the schedule results of first system layer, g r2 is defined as 2.5% of the total renewable generation and used at feeder level."
"on the other side, the ratio of the residual deviation η is used to evaluate the performance of different approaches, where ∇ org is the envelope of the original forecasting error distribution, and ∇ d is the envelope of the forecasting error distribution attained from gsm, general gmm and gaem. according to (14), the ratio of the residual deviation of gsm is 2.17%, gmm is 1.27%, and gaem is 0.13%. it is clear that the proposed approach has the best performance for error distribution modeling."
"the hourly renewable generation with the total operation cost is simulated in fig. 4(a), where the blue (yellow) bars indicates the wind (solar) power generation hourly in a day. in this case, the lower limit of the forecasting error model in chance-constraint is set to 97.0%. fig. 4(a) describes the total operation cost with and without ca, which indicate with red and green curves, respectively. as shown in fig. 4(a), in golden, colorado, a typical day with a windy night and sunny daytime is selected with 24 hours data. the peak generation of the wind turbines (blue bars) and pvs (yellow bars) occurs at midnight and 14:00, respectively. the red line represents the total operation cost with ca, which indicates the system can resell redundant power at a lower price s t to the electricity market. then, the total operation cost can be reduced with benefits from reselling. the total operation costs at 13:00 and 14:00 for both scenarios (with and without ca) are the same, which indicates there are no redundant energy to resell and the ca does not occur. the similar scenarios also occur at 7:00 and 18:00. for the rest of the time, the redundant energy is resold to the market with a lower price to reduce the total operation cost."
different approaches to the integral financial stability index are presented. 16 macroeconomic variables collected by bank of israel were used as independent factors and economic resilience index was explored as the benchmark. the methods used can be attributed to one of three following groups: the principal components (pc) method and its modifications; regression models; hybrid methods. before the calculation all factors transformed with the standardization procedure. ifsi constructed in such a way that its larger value corresponds to greater financial stability. visual and statistical comparisons of ifsis then made including pearson and spearman correlation.
"for each of methods in the first group the set of factors having the cumulative weight not less than 50% is defined. only these sets are then used in multiple regression model. if some factor is not significant (at 5% level) it is removed from the model (backward elimination). the results of this method will be marked as \"reg(**)\" where ** in brackets indicates the model from the first group which defines the \"short list\" of factors."
"step 4: identifying water quality level fuzzy theory can be applied to identify water quality level for an object [cit] . a set of membership functions describing the membership of the ith object to levels i to v are written as . this is a two-way parameter propagation between water quality indicators and their standard, marked with ④ in fig. 1, because mf i c, relies on c i and cs c obtained in step 3. on the basis of weighted average principle [cit], level h i of water quality for the ith object is expressed as"
"quarterly data ranging from 1q2003 to 3q2013 (42 periods) for israel is employed. the (dependent) variables used to build the index are 16 (out of 39) financial soundness indicators being collected by imf on regular basis. the dependent variable is economic resilience (er) indicator collected by international institute for management development (imd). [cit] on a yearly basis; its values are scores from 0 to 10 where 0 corresponds to the lowest financial soundness and 10 -to the highest. the quarterly values of er were obtained through the linear (yl) or spline (ys) interpolation procedure. table 1 presents a summary of the data. the correlation matrix for independent variables' time series presented in table 2, where red indicates significance at 1% level, blue -at 5% level. id x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11 x12 x13 x14 x15 x1 the methods proposed can be divided into three groups:"
"as demonstrated in table iv, compared with other approaches such as interior-point and particle swam optimization [cit], it is obviously that the proposed approach has the minimum time consumption in du campus, ieee 123-, and 8500-bus systems. the convergence curves of the proposed approach in ieee-123 bus system are shown in fig. 8(b) . the curves of primal residual and dual residual decrease below 10 −4 after 35 iterations, which also demonstrates the high speed of the proposed approach. meanwhile, the results of operation costs with and without opf are simulated using testing data from four seasons in table v, which demonstrates the proposed approach can efficiently reduce the operation cost at the feeder level."
"one of the main result characteristics obtained with the principal component methods is the proportion (r) of baseline factors' total variance (equal to the sum of their dispersions) explained by the first principal component. to make a comparison the initial (before normalizing procedure) coefficient vectors are used. as mpc estimates were gained implicitly i.e. not at the computational step but afterwards at the normalization step, before comparison they must be transformed: the coefficients' estimates are presented in table 3 . cumulative weight is the cumulative sum of the individual factors' weights. the negative weight indicates that the factor included in ifsi calculation with negative sign. thus, the best in terms of maintaining maximum information from the initial factors is pc. however, this method gives a negative weight to many factors that complicates the interpretation of the ifsi as the initial factors' standardization procedure implies positive weights to all factors. mpc solves this problem, but it uses only a small part of the information contained in the individual indicators' series (2.7 %). above-mentioned problems can be avoided by using pcwc. table 4 presents the results of applying the second group of methods to the \"learning\" sample and their main characteristics. regression were estimated for one, two or three indicators. table 5 presents the results of applying the third group of methods to the \"learning\" sample and their main characteristics finally the main results of different ifsis comparative analysis presented in table 6 . the pearson correlation coefficient calculated separately for both \"learning\" sample and the sample for verification. spearman correlation coefficients calculated only for the second sample. also the significance levels for correlation coefficients are presented."
"based on what's discussed above, two problems are still unsolved in applying topsis-based approach to water quality evaluation. one is that most studies assumed indicators to be independent. the other is that few studies so far have attempted to give a guidance on how to couple water quality indicators and associated standards. solution to such problems will not only have important administrative implications, but also shed light on more accurate water quality evaluation. this paper develops an improved topsis-based approach, called the informative weighting and ranking (tiwr) approach. the approach separately starts from water quality indicators and their standards, and couples them for four times during the entire process. the effect of correlation among indicators on weights is considered using criteria importance through inter-criteria correlation (critic) approach. weighted average principle based on fuzzy theory is used to produce a delicate level associated with water quality. this approach is applied to water quality evaluations of the shitoumenkou reservoir and the lake tai. advances in the tiwr approach are discussed by comparison with other traditional approaches to water quality evaluation."
"the problem of integral index construction have already been solved for some countries 3, 4, 5 as well as for the global world economy 6 . for individual countries' indexes construction weighted average 3,4 /blocked weighted average 5, 7 or principal components 4, 8 methods were often used. columbia's index was also build using the dependent variable -number of bankruptcies 4 . however there is generally no forecasting power analysis in these studies. the comparison of different methods in terms of their forecasting power also was not made."
the active and reactive power of the distribution line at bus i. s i the injection power at bus i. g w t total renewable generation delivered to buses in time t.
"3.2. case 2: lake tai 3.2.1. study area and indicators lake tai, the third largest freshwater lake in china, is located in the yangtze river delta region. it has 1.9 m average depth and 2427.8 km 2 area, sustains 59.71 million population and contributes 10.2% of the national gdp. water environment of lake tai has significantly deteriorated over the last decades. the northern part of lake tai is impacted by algal bloom and the southeastern part undergoes remarkable degradation of aquatic vegetation . six indicators of nh 3 -n, tp, tn, ppi, do and bod 5 are monitored at sites w1 to w4 in area 1, w5 to w6 in area 2 and w7 to w20 in area 3 shown in fig. 2 . areas 1-3 cover 19.1%, 7.0% and 73.9% of the total lake area, respectively. monitoring data at the 20 [cit] are collected by the taihu basin authority [cit] . table 3 displays the annual mean concentration of the six indicators. table 4 presents the water quality standards of levels i to v for the six indicators."
"specifically, in the left part of fig. 1, the forecasting error distribution is derived from the historical data. combined with renewable energies, a day-ahead hourly scheduling objective function can be formulated with the chance constraints. according to gaem, the given aggregated error distribution model of the renewable energies can be accurately modeled with a combination of several gaussian components. with probability analysis, the chance constraints can be formulated into deterministic forms for stochastic optimization. finally, the optimal hourly schedule is determined and the optimal operation cost is computed at substation level."
"the main contributions of this paper are as follows: 1) from perspective of a distribution system operator, a multi-timescale analysis approach is proposed for a three-phase unbalanced distribution system. 2) considering day-ahead dispatch with the substation level, the objective is to minimize the system cost with renewable energy, and the corresponding chance constraint is build based on gmm and gaem. 3) at the feeder level, the objective also aims to minimize system loss, an opf problem is formulated for the threephase unbalanced system, relaxed by sdp and solved by admm. in the proposed multi-timescales model, feeder scheduling is designed with higher time resolutions and update frequencies, which can achieve near real-time (rt) calculation. 4) in numerical results, the ieee-123 bus distribution system, the ieee-8500 distribution system, and the university of denver distribution system are employed with various scenarios to demonstrate the feasibility and effectiveness of the proposed approach."
"step 1: normalizing object matrix and standard matrix two reasons bring us to normalize object matrix a and standard matrix s. one is that indicators often have different scales and units. the other is that indicators can be classified into two kinds of \"cost\" indicator and \"benefit\" indicator. the former such as chemical oxygen demand (cod) increases as deteriorating water quality. in contrast, the latter such as dissolved oxygen (do) increases as improving water quality. we design a technique to normalize matrices a and s. each of elements in object matrix a is rescaled as:"
"note that derivation of weight w j in step 2 is independent of normalized standard matrix ns. the product of weight and element of matrix ns in eq. (10) is therefore a one-way parameter propagation from water quality indicators to their standards, as marked with ② in fig. 1 . based on matrices va and vs, negative-ideal and ideal objects can be defined, respectively, as:"
"based on the discussion above, one year data with four seasons are employed to validate the proposed approach. specifically, for each season, 30 days data are selected, and we also employed the gmm and gsm for comparison. as shown in table iii, the proposed approach achieves the minimum cost for spring, summer, autumn, and winter. it is demonstrated that with the high accuracy in modeling, the corresponding operation cost can be reduced significantly."
"this section presents the application of the proposed tiwr approach to water quality evaluation of the shitoukoumen reservoir in section 3.1 and lake tai in section 3.2. in both case studies, all monitoring sites are set away from near-shore region for collecting representative water samples. the collection and preservation of water samples follows the standard of \"water quality sampling -technical regulation of the preservation and handling of samples\" issued by the ministry of environmental protection of the people's republic of china. laboratory analysis of water samples for measuring concentrations of water quality indicators also follows associated standards issued by the chinese government. all indicators inferior to level i are considered for water quality evaluation. [cit] . table 1 [cit] and water quality standard of levels i to v."
"in fig. 1, at the feeder level (the right side), a three-phase unbalanced optimal power flow analysis is used to model and compute distribution system loss. in this paper, 2.5% of the renewable generation in the system are reserved for opf regulation at feeder level. considering the nonconvexity of the three-phase unbalanced opf problem, sdp is used to relax the opf problem. compared with the centralized methods, admm is employed to solve this problem in a distributed manner to reduce computational time. finally, the three-phase unbalanced opf can be solved to minimize the system loss at feeder level."
"the em algorithm is typically used as a standard approach to calculate the parameters of the mixture model, which consists of an expectation-step (e-step) and a maximization-step (m-step)."
the analysis conducted allows to make a conclusion that in first group of methods pcpw is the best in terms of its ability to retain the information containing in initial factors and their weights' interpretation.
"officially-defined water quality standard classifies concentration of an indicator into one of several grades such as levels i to v. it is often regarded as a reference to identifying a level for water quality. some topsis-based approaches consider water quality indicators as inputs and exclude associated standards [cit] . this fails to identify water quality level to each of objects/ monitoring sites. on the other hand, some approaches consider both indicators and associated standards as the elements of a single matrix [cit], which may significantly affect estimated weights of indicators and diminish reliability of topsis results [cit] stated that estimated weights should depend on monitored indicators without any artificial perception. therefore, a more appropriate method for coupling indicators and associated standards may be needed."
"as in fig. 1, the proposed approach consists of two layers, the stochastic optimization for hourly scheduling at the substation level, and the three-phase unbalanced opf for minutes operation at feeder level. in the proposed two-layer framework, the optimal day-ahead scheduling power, purchased from the utilities for the next 24 hours, is computed in the first layer. in the second layer, using the results from the first layer, the opf is computed to reduce system loss and the total system cost within an hour."
"the numerical results for evaluating the proposed method are tested based on the ieee 123-bus system in fig. 3 . four wind turbines (100 kw for each) are connected at bus 25, 35, 76 and 105, respectively. the pv panels (55 kw for each) are installed at bus 28, 47, 49, 57, 64, 93 and 97. this system is used to demonstrate that the hybrid power system can work efficiently and reliably with the proposed approach."
"the dynamics of ifsis obtained by different groups of methods in comparison with yl dynamics shown in fig.1-3 . comparing the behavior of ifsis on the learning and examinee samples as well as from the graphs we can conclude that the best approximation quality for yl is achieved with the second group of methods. namely \"commercial real estate loans to total loans\" is the best predictor for economic resilience index. this allows us to assume that this factor was used (explicitly or implicitly) in economic resilience index construction."
the principal components (pc) method and its modifications. these methods don't need the dependent variable. the ifsi for this group is simply the first principal component. besides the pc method itself two its modifications -the modified principal components method (mpc) and the principal components method with positive weights (pcpw) are used regression models hybrid methods. these methods also based on (multiple) regression but only variables having a great contribution to the ifsis in the first group are used
at least 2 ifsis can be constructed with pc: if a is principal component then a is principal component too. the choice between a and a will be made in favor of that vector which have a positive correlation with yl (ys).
"the complex power flow. g err1, g err2 renewable energy forecast error, load forecast error. γ a probability is used to limit the power unbalanced error."
"typically, a forecasting error model represented by a normal distribution is used to incorporate the uncertainty of renewable generation [cit] . in this paper, the load forecasting error model is represented as normal distribution. the hourly renewable generation forecast and the hourly load forecast are described as:"
"the applications of topsis-based approaches to water quality evaluation have increasingly become popular. few researches, however, consider correlation among water quality indicators or couple them with water quality standard in a reasonable manner. to solve the two problems, this study proposes an improved topsis-based approach called the tiwr approach for water quality evaluation. it separately starts from an object matrix a defined in eq. (1) and a standard matrix s defined in eq. (2) as well as follows four steps described in the flowchart of fig. 1. step 1 normalizes both object and standard matrices. note: \"fse level\" stands for typical level obtained by fse approach. the subscript i represents the ith object. objects of unreasonable fse level are marked in bold."
"the remaining of the paper is organized as follows. the flowchart of the proposed approach is described in section ii. in section iii, the substation scheduling model is built with renewable generation, which are formulated as a deterministic form with gaem. in section iv, with a feeder scheduling model, the opf problem is solved with admm for the three-phase unbalanced distribution system. the numerical simulation and results are presented and analyzed in section v. the proposed approach is concluded in section vi."
"the proposed approach is compared with the gsm based approach in fig. 4(b) for 24 hours. for each hour, the left bar describes the ratio of g da t + g r1 t (light blue part) and g rt t (yellow part), which is fitted by gsm model. the corresponding total cost is displayed as a green dashed line. similarly, the right bar describes the ratio of g da t + g r t (dark blue part) and g rt t (orange part), which is fitted by gaem model. the corresponding total cost is displayed as a red dashed line. it is clearly that the orange part is shorter than the yellow one, which indicates the gaem model achieves less errors than the gsm model, and the system requires less energy from the rt market. the difference between two dashed lines demonstrates the proposed approach achieves a lower operation cost than gsm based approach."
"where ρ j k, is the correlation coefficient between the j th and k th indicators. the magnitude of ρ j k, falls in the range of −1 to 1 and a greater value indicates higher positive correlation between two indicators. a greater r j therefore indicates lower positive correlation between the j th indicator and the others. finally, the weight of the j th indicator can be defined as"
"is the corrective action (ca), which indicates the redundant energy will be resold to the market in a lower price. it is notice that, the total renewable power generation is divided into two parts, 97.5% of the total renewable power generation is used to minimize the operation cost at the substation level (g r1 ), 2.5% is used to optimize the opf problem at the feeder level (g r2 )."
"in table i, the skewness and kurtosis [cit] are calculated as (12) and (13) . skewness is defined to measure the data asymmetry around a sample mean. and kurtosis is used to describe the outlier-prone of a distribution [cit] ."
"as can be seen the pc procedure seeks a point(s) on an n-dimensional hypersphere of radius equals to one. the coordinates of this (these) point(s) when used as factors' weights maximize the total variance of initial standardized factors. the mpc then shifts the obtained point(s) to the \"positive\" side of the hypersphere and pcpw restricts the searching aria allowing coordinates only be non-negative."
"level h i has the following three advantages compared with typical levels i to v. first, the water qualities of objects w1, w3 and w4 can be distinguished by different h 1, h 3 and h 4 but are classified into the same typical level iii. second, ranking four objects w1 to w4 according to note: the unit of concentration is mg/l."
"blending the decomposability of dual decomposition, admm shows the superior convergence property of augmented lagrangians [cit] . for a general admm problem, the optimization problem is formulated as"
"to reduce the uncertainty impact of the renewable generation in a three-phase unbalanced distributed power system, a multi-timescale approach is proposed for three-phase unbalanced distribution systems. considering day-ahead dispatch at the substation level, the objective is to minimize the system cost with renewable generation, and the chance constraint is incorporated based on gaem. for the feeder level, the objective also aims to minimize the system loss, an opf problem is formulated for the three-phase unbalanced system, relaxed with sdp, and solved with admm. in the proposed multitimescale approach, the feeder scheduling is designed with higher time resolutions and update frequencies, which can obtain near rt calculation. the numerical results demonstrate the effectiveness and validity of the proposed approach in a real-world system model (du campus grid), a medium-scale power system (ieee 123-bus distribution system) and a largescale power system (ieee 8500-bus distribution system). for all four seasons, system operation cost can be reduced by the proposed approach."
"as shown in fig. 6 (b) [cit], the proposed method is implemented in an ieee 8500 bus system, which demonstrate a case study in a large-scale power distribution system. the installed power capacity of renewable energy is 20 mw. in fig. 7(b), a windy day in winter is selected, with persistent wind in 24 hours and a short sunshine duration for the study. compared with the operation cost of gsm, gaem reduces the operation cost effectively. furthermore, the corrective action is further reduces the operation cost when redundant power appears."
"some attempts have been made to develop approaches based on the technique for order preference by similarity to ideal solution (topsis), a powerful non-linear ranking approach (e.g., [cit] . estimating weights to indicators is a crucial step of topsis-based approach. most approaches assume indicators to be independent with each other, which is however not true for the case of water quality indicators that are often found to be highly inter-correlated [cit] . [cit] stated that 20% variation in weights significantly affects topsis results. topsis-based approach may give improper weights due to excluding the effect of correlation among indicators."
"a brief description of finite gaem is given below, which is used to model the forecasting error distribution of the renewable generation as in (6) ."
"firstly the number of independent variables (except intercept) and the significance level are set. then given these restrictions all possible regressions are evaluated. finally the regression having maximum value of r-squared and all parameters significant at given level is selected. hereinafter the significance level will be set at 5% and the results of this method will be denoted as \"reg(*)\" where * in brackets will be replaced by the number of independent variables in regression."
official water quality level of lake tai reported by the chinese government. step 2 estimates weights to indicators using the critic approach that can consider correlation among indicators.
"in order to avoid local minima and premature convergence, this paper adopts a disturbance optimization strategy. first, a dynamic evolutionary monitoring mechanism is used through analyzing the fitness variance of the population during the optimization process. once a convergence criterion is satisfied, the disturbance optimization strategy is implemented. concretely speaking, a disturbance is added by re-initializing a certain percentage of individuals that are randomly selected. the used convergence criterion is 22 1 thr1 thr2"
"with the increasingly serious energy crisis and environmental problems, it is has become a broad consensus to research and leverage renewable generations and electric vehicles [cit] . at the same time, as an effective carrier of distributed renewable generations, a microgrid (mg) can fully promote the integration of renewable generations and has become an important part of smart grids [cit] . compared with the grid-connected microgrids, isolated microgrids (imgs) have obvious advantages in areas which are inaccessible to the main grid, such as islands, remote areas, deserts, etc. [cit] . however, the inherent uncertainty of renewable generations is difficult to guarantee operational reliability and power supply security, especially for imgs. consequently, it is a tricky problem to address renewable uncertainty in imgs. recent studies have shown that the integration of electric vehicles (evs) into mgs via vehicle-to-grid (v2g) is beneficial to implement energy conservation and emission reduction [cit], but the ev charging behaviors have strong time and space uncertainties [cit] . therefore, the integration of evs will further increase the difficulty of mg scheduling. as a new type of demand-side management strategy, price-based demand response (pbdr) mainly includes real-time prices and time-of-use tariffs, which is able to maintain supply-demand balance by flexibly adjusting the load. in addition, demand response can effectively consume uncertain renewable generations including pv and wt. therefore, how to comprehensively consider ev demand response and uncertain renewable generations is an urgent and challenging problem."
"-superpixelization. numerous approaches have been adopted for superpixels generation [cit] . our hypergraph reduction algorithm is applied to the resulting inh. the algorithm can be further applied to the so-obtained reduced hypergraph (rinh), and so on. the iterations are stopped when the ratio between the size of two successive coarser hypergraphs, i.e."
"the sequence operation theory (sot) is here utilized to obtain probabilistic sequences of renewable generations, and then transform a chance constraint into its deterministic equivalent class, which avoids tedious and time-consuming monte carlo simulations in the solution process. in this study, all renewable outputs are modelled by probabilistic sequences obtained through discretizing continuous probability distributions."
"in this paper, we proposed a hypergraph reduction algorithm and we evaluate it in a supervised superpixelwise image classification. the effectiveness of the proposed method was demonstrated with experimental results using various generic and satellite images. our approach is an open system and several solutions can be made to improve the proposed framework such as reducing the hypergraph h(v, e) without imposing e to be ordered. in future work, we will add more information in superpixels and more particularly neighborhood information, as well as other visual features like shape or texture information."
"electric vehicles have been recently receiving increasing attentions since they play a critical role in energy conservation and emission reduction. in this work, the lower level seeks to minimize the ev charging cost."
"for purpose of properly evaluating the performances of the pbdr strategy, the following two cases are designed: case 1--without considering demand response: the charging price of evs is set to the price of the main grid."
"step 4: set the electricity price of the main grid as the basic price of the mg before optimization. the price is known in advance on the basis of historical data, the load demand of previous day/hour and the expected load demand of next hour or day."
"f are respectively the operation cost of the img and the ev charging cost under consideration of demand response. when this objective function takes the minimum value during iterations, the scheduling scheme corresponding to this iteration is chosen as the optimal scheme [cit] . in this case, the resulting optimal scheduling scheme is capable of balancing the interests of the microgrid and electric vehicle users, achieving a win-win situation for both. fig. 1 shows the procedure of the proposed solution method, and the specific steps are as follows:"
"the ev charging powers without and with consideration of demand response are illustrated in from fig. 10, it can be observed that demand response plays a key role in guiding the charging behaviors of ev users. concretely speaking, evs users decrease the charging powers in the peak-load periods, while they increase the charging powers in off-peak periods. by this means, the peak load shaving is achieved by leveraging ev flexibility on the load side while maintaining the source-load balance, thereby promoting ev users to actively participate in the mg scheduling. by this means, it provides a fundamental way to balance the interests of both mg and users."
"the main contributions of this paper are as follows: (1) a bi-level programming-based mg scheduling model under real-time pricing environments is proposed, which is the first attempt to coordinate renewable generation uncertainty and ev demand response; (2) a new hybrid solution algorithm called jaya-ipm is developed to solve the model with sufficient optimality and high computation efficiency; (3) and finally, the simulation results indicate that demand response of electric vehicles is able to guide ev users to actively participate in mg scheduling and achieve the peak load shaving, which provides a fundamental way to balance the interests of both mg and users. the novelty of this study lies primarily in our attempt to propose a new method that can coordinate the ev demand response and renewable generation uncertainty in mg scheduling."
"in this paper, it is necessary to introduce a joint optimization function f jo to screen out the optimal joint operation schemes from the yielded candidate ones during iterations [cit] ."
"case 2--considering demand response: the charging price of ev adopts real-time electricity prices. it can be seen from figs. 8 and 9 that demand response manages to promote the ess and ev users actively participating in economic operations of the microgrid. (1) regarding the ess, the ess charging-discharging frequencies in case 2 are obviously greater than those in case 1. this is because the ess will absorb more electrical energy to suppress load fluctuations when its discharging price is higher than the charging price. (2) in terms of evs, due to the introduction of pbdr, the charging behaviors of evs can flexibly response to the changes of electricity prices. therefore, ones can conclude that pbdr is an effective way for guiding the ess and ev users actively participate in the microgrid optimal scheduling."
"the upper level seeks to minimize the mg net operating cost, which is calculated by the difference between the operating cost and the revenue of the mg. herein, the mg operating cost is the sum of both the fuel cost of microturbines (mts) and the cost of spinning reserves provided by mts and energy storage systems (esss) [cit] . in this study, zn-br battery is chosen as the ess since, compared with other batteries for grid-scale energy storage, it has many significant benefits, like lower costs, higher energy density, and longer service life. the mg net operating cost can be expressed as follows:"
"the rest of this paper is organized as follows. section ii gives the modeling of mg and ev. in section iii, the problem formulations are introduced. next, section iv depicts the proposed solution methodology. section v analyzes the experimental results, and conclusions are drawn in section vi."
"for purpose of examining the economy of the joint optimization between img and ev, three different strategies are designed in this paper. cost ($) figure 5. img net operating cost and the charging cost of evs in different strategies fig. 5 illustrates that the strategy 2 is the best one among the three strategies since it can balance the interests of mg and ev users; while the other strategies only consider the interests of mg or ev users, separately. in particular, regarding the ev charging cost of the img, the result in strategy 2 is superior to that in strategy 1; while as far as the net operating cost of the img is concerned, strategy 2 outperforms strategy 3. on the basis of this phenomenon, it can be concluded that demand response of electric vehicles is able to improve the economy of img and ev users, which plays an important role in the scheduling of isolated microgrids with renewables. the electricity prices of the main grid in this work are listed in table 3 . fig. 6 shows that the proposed real-time electricity prices (including the initial and optimal real-time prices) significantly outperform the electricity price of the main grid, since the latter is unavailable to adjust the electricity prices according to the dynamic supply-demand relationships. furthermore, the optimal real-time prices are superior to the initial real-time prices, since the former is able to reduce the peak-to-valley difference of load powers."
"the interior point method originally proposed by john von neumann is a classical optimization method for addressing linear programming [cit], and its key principle is to gradually approximate the optimal solution of the original problem in the feasible domain. in view of the ipm's advantages of high efficiency and excellent accuracies, it is used to solve the lower-level model."
"in the current section we will discuss possible use of hypergraph reduction algorithm in image analysis domain and more particularly in a supervised image content classification. the proposed application can be summarized as follows: (i) from image we compute the inh representation,(ii) we reduce the inh representation and we obtain the rinh hypergraph, (iii) we generate a set of superpixels from rinh using proposition 2, (iv) generate a set of features for each superpixel and classify them using a svm framework."
"this section discusses the spinning reserve capacities under different confidence levels. spinning reserve is an important auxiliary service for balancing source-load difference [cit], and confidence levels γ determine the reserve capacities of mgs. the relationship between reserve capacity and confidence level is shown in fig. 7 . fig. 7 indicates that the mg reserve capacities are dependent on confidence levels. it's known that reserve capacities are closely related to the economy and reliability of the mg operation. for one thing, a higher confidence level can increase the system reliability at the cost of the economy; for another, a less confidence level will bring out a decreased reliability and a better economy. as a result, a suitable confidence level is crucial to trade off the reliability and economy of the mg operation."
"the proposed approach has been examined on an improved mg testing system, which is illustrated in fig. 2 . this system consists of one pv control board, three mt units, one wt unit, an ev charging station, and the original load. among them, pcc denotes a common coupling point. table 2 gives the used parameters of mt units in this work. in the above table,"
"the key idea of the jaya algorithm is that a solution must move away from the worst solution and move to the best solution [cit] . if xj,k,i is the value of variable j for candidate k at iteration i, then xj,k,i is calculated as"
"it is assumed that there is no a parking space in charging stations in this study, and the default ev charging mode is that an ev leaves the charging station immediately once it is charged to its expected capacity. in this mode, the travel demand and usage habits mainly reflect in the ev arrival time, daily travel miles. these factors determine the total charging amount and the charging time for ev users. [cit] national traffic survey of vehicles in the united states, the pdfs of the daily travel time and daily travel miles of evs are obtained [cit] . the ev arrival time obeys the normal distribution, and its pdf is"
"jaya is a powerful algorithm to handle complex optimization issues, which is proposed by r. venkata rao. since it requires no algorithm-specific parameter expect two common control parameters namely the population size and maximum number of iterations, its results are more stable than other intelligent optimization algorithms [cit] ."
"in microgrid scheduling, a probabilistic model should be given priority because of the small capacity and inherent renewable uncertainties in mgs. among them, the pv output follows the beta distribution, the wt output obeys the weibull distribution, and the original load power obeys the normal distribution. their respective probability models and probability density functions (pdfs) are detailedly given in the literature [cit] ."
"electric vehicle charging model is the basis for investigating the orderly charging of evs [cit] . in order to simplify the problem, this study only considers ev charging modes. and the main factors affecting the ev charging include the following two aspects:"
"this paper proposes a bi-level programming model for scheduling of isolated microgrids with renewable generations by incorporating demand response of electric vehicles. and thereby, a hybrid solution algorithm jaya-ipm is developed to solve the model. the simulation results demonstrate that demand response of electric vehicles is able to guide ev users to actively participate in mg scheduling and achieve the peak load shaving, which offers a \"win-win\" solution to balance the interests between microgrid and ev users. future work will focus on extending the proposed approach to scheduling of heat and electricity integrated energy system. note that it is assumed in this paper that only charging modes are available for evs, while a more realistic scenario shall consider ev discharging modes. besides, the charging waiting time of ev users in this paper is ignored, while it is necessary for real-world applications due to limited charging piles."
"there are many batteries can be used as candidates of ev batteries, such as lead-acid batteries, nickel-hydrogen batteries, lithium-ion batteries, and so on [cit] . each kind of battery has its technical and economic characteristics. compared with other batteries, a lithium-ion battery has some obvious advantages, such as relatively longer service life, higher charging efficiency and greater depth of discharge [cit] . consequently, the lithium-ion battery is chosen as the ev battery in this work. in addition, the charging modes of lithium-ion batteries consist of slow charging and fast charging. taking into account that fast charging modes are crucial for public acceptance of evs, this study focuses on fast charging of lithium-ion batteries."
"this section give the outputs of renewable generations, and the ev arrival and departure times. all of them are used as the basic data for the subsequent analysis. the original load power and renewable outputs are illustrated in fig. 3 ."
"step 12: judge whether the termination criteria is met. if met, end the optimization process and proceed to the next step, otherwise, return to step 5. here, the used termination criterion is whether the current iteration exceeds the pre-defined maximum number of iterations."
"in this study, the used ev parameters are as follows: the rated capacity, the ev expected capacity is 90%; the investment cost and the service life of a charging pile are respectively $ 3000 usd and 10 years."
the covering of the set of intersecting hyperedges; proof. we can build a graph γ in the following way: (1) the set of vertices is rv .
"eqs. (9) and (10) (16) denotes the starting and ending constraint. in order to balance the energy stored in the ess and prolong the battery life, the initial energy stored and the remaining energy at the end of a scheduling cycle should be equal [cit] . soc  represents the initially stored energy limit of the ess, 0 soc represents the initial energy in the ess, tend denotes the end of the total scheduling cycle (it is set to 24h here). eq. (17) - (19) represents the spinning reserve constraint of the microgrid. since the main power grid does not supply power to the img, the spinning reserve is a significant resource for balancing both supply and demand sides [cit] . eqs. (17) and (18) are the spinning reserve constraints for mt and ess, respectively, where, ress t p represents the reserve capacities of the ess in period t. considering that the joint output of renewable generations may be zero, in this small probability situation, the adequate spinning reserve must be provided to maintain the reliability of the system, but it will incur additional costs. eq. (19) illustrates the probabilistic spinning reserve requirement, in which γ denotes the confidence level."
"considering that a bi-level programming model is nondeterministic polynomial-time hard (np-hard), a hybrid solution algorithm called jaya-ipm is developed to ensure sufficient optimality and high computation efficiency. the algorithm solves the model through an iterative process between levels, and the optimal scheduling strategy is finally determined."
"in the proposed bi-level scheduling model, the upper level aims to minimize the net cost of the microgrid; the lower level seeks to the minimization of the ev charging cost. focusing on coordinating renewable generation uncertainties and demand response and maintaining the dynamic supply-demand balance of power, a real-time pricing mechanism that acts as a bridge between the two levels is proposed in this work."
"to explore the reproducibility of the results, this experiment was repeated five times on the same phantom with the same force 3 n and the ramp speed of 18 n/s. there was a 5-minute resting time between each trial and the sample was not removed or repositioned for these five trials. no sign of damage was observed in phantom while performing the test."
"the results showed strong agreements between the two devices, the automated compression device and the bose system, with an average relative error of 2.9% and 12.4%, for t 1 and t 2, respectively, based on the surface displacement measured by these two devices. it is understood that, in general, the strain on the surface and inside the phantom are not necessarily the same. however, our investigation to measure retardation times for each point inside the same phantom by ultrasound reveal a good agreement between the surface and internal computed retardation time constants, where the error was less than 15%."
"an alphabet is a finite non-empty set of symbols and as usual, given an alphabet x, we denote x * the set of all finite sequences (words) of symbols from x including the empty word . the number of symbols in a sequence we shall call length of this sequence; by definition, length of the empty word is zero. a subset l  x * is a language over alphabet x."
"in this paper we propose definition of parallel composition for two tfsms. this definition relies on the definition of fsm parallel composition and the latter is defined in terms of parallel composition of corresponding automata. for that reason we also describe the conversion procedure [cit] of a tfsm into an fsm, which then is used for the parallel composition construction. we also prove, that built fsm correctly reflects the language of a given tfsm."
"we also showed that the measured retardation times t 1 and t 2 across a uniform phantom resulted in 2-d maps, as shown in fig. 9 . these results suggest that t 1 and t 2 maps are less sensitive to the geometry and boundary condition variations than the strain map (see fig. 8(a) ). there are some large regional variation in t 1 and t 2 maps. however, the area of these regions is small compared to the total area of the image. this can be verified by calculating the mean and standard deviation of the maps, which are 3.4 ± 0.13 s for t 1 map and 33.6 ± 1.25 s for t 2 map."
"in this paper, we are interested in the viscoelastic response of the tissue, for which we need to study the temporal response of the tissue under a constant stress."
"in this equation, (t d ) is the retardation time, t 1 or t 2 measured by the compression device and (t b ) is corresponding values measured by the bose machine, which is regarded as the gold standard. based on the average results of five trials, the error percentage e(%) for t 1 and t 2 were -2.9% and −12.4%, respectively. these results indicate a strong agreement between the creep device and bose instrument. we used t-test to compare the results of the compression device and the bose machine. the t-test showed a p-value of 0.4 which proves that there is no significance difference between two devices."
"to make phantom a(softer), we used 32.3-g gelatin (sigmaaldrich, st. louis, mo, usa), 30-ml vanicream lite (pharmaceutical specialties, inc., rochester, mn, usa), 6-g cellulose (sigma-aldrich) for ultrasound scattering, and 6-g potassium sorbate (sigma-aldrich) as a preservative, dissolved in enough distilled water to make the total solution volume equal 600 ml. phantom b (stiffer) was made of 25.14 g of gelatin (sigmaaldrich; st. louis, mo, usa); 60-ml propylene glycol (sigmaaldrich, st. louis, mo); and 4-g cellulose (sigma-aldrich) for ultrasound scattering, dissolved in enough distilled water to make the total solution volume equal 300 ml."
"the relative error of phantom b, stiffer uniform phantom, and inclusion part of inclusion phantom is 2.7% and for its background and soft uniform phantom, phantom a, 4%."
"in this paper, we described the design of a device for applying an approximate step force on a tissue-mimicking phantom and measuring the creep response using an ultrasound probe. retardation time was calculated by fitting a second-order kelvinvoigt model to the displacement and strain profiles obtained from the device. the performance of the device was validated through a series of creep tests on a phantom and comparing the resulting retardation times with those from a standard mechanical testing instrument. beside that its performance was also checked on phantoms with different size and stiffness by applying the first-order kelvin-voigt model. the overall results of this study justify the suitability of this device for performing creep tests on tissue-like materials."
"finite state machine before we propose how to construct the parallel composition of timed finite state machines, we introduce the transformation procedure of a tfsm into an fsm and back, and then prove, that obtained fsm correctly describes f-language of the tfsm. consider an example in figure 2 . state q of tfsm has timeout 2 and therefore, we add one copy of q, 1 (denoted \"q1\") which is 1/n-successor of the state q while its 1/n-successor is s. the sets of successors of q and q, 1 for all other i/o pairs coincide."
"following [cit], we define the parallel composition of two fsms (fig. 1) based on their corresponding automata. however, the language of the parallel composition of two automata is not necessary an fsm language. for this reason, the obtained language should be intersected with the language (io) *, where i and o are external input and output alphabets of composition, to ensure that each input is followed by some output."
"to measure the dynamic response of the medium, we used the compression device to apply 8-n force with 16-n/s ramp on each of the phantoms. the ultrasound frame rate was 20 hz during the acquisition time of 20 s. thus, 400 frames were acquired for each experiment. initial 2 s of data were removed."
"one of the limitations of the compression device, and generally this method, is restrictions in accessing internal organs for measuring the creep response. due to that limitation, organs with easy access, such as the breast, thyroid, and possibly prostate, are the best candidates for this method. another limitation of this method is in the way the data are recorded. although particle displacement measurement by ultrasound is very accurate, it only measures the axial displacement. displacement measurement in other dimensions is not as accurate with the conventional ultrasound imaging."
"so far, we have shown how the compression device can be used to measure the retardation time of the material based on surface displacement. fig. 5 and table i show the results of overall retardation time measurements."
"let us illustrate our approach by constructing the parallel composition of tfsms. the parallel composition of fsms that corresponds to the parallel composition of tfsms is shown in figure 3 . in this case, port 1 is a common port for both machines as it corresponds to a counter of ticks and this accepts the designated input 1 that is an input for both component fsms and can be considered as an input that synchronizes time behaviour of component fsms. the designated output n is observed, when there are no outputs at ports o 1 and o 2 (it is observed at both of the ports). each component fsm has its own time variable, which increments every moment when component gets the designated input 1, and since this signal is applied via a common port for both components the global time is used, and thus, we can say that it synchronizes the behaviour of component fsms."
"it is common to use a rheological model to evaluate the viscoelastic response of the medium. these models are used to show the relationship between the stress and strain that models the viscoelastic response of tissue and its structure. the standard rheological models, like maxwell [cit], kelvin-voigt model [cit], and standard linear solid models [cit], are the most applicable ones. sloninsky [cit] described a model called fractional derivative to model the behavior of biological tissue with lower number of fit parameters [cit] ."
"the remainder of this paper is organized as follows. in section ii, we provide an overview of the rheological techniques for modeling tissue deformations under creep-like tests and highlight the hardware requirements for performing such tests. in section iii, we explain the details of the compression device that is specifically designed to apply step-like stress on tissue. in section iv, the function of this device is validated by comparing its results with those of a standard mechanical testing instrument [cit], using tissue-mimicking samples [cit] . section iv is devoted to integration of the compression device with a programmable ultrasound machine for viscoelasticity imaging based on ultrasound strain estimation. we discuss the results of the validation tests as well as viscoelasticity imaging in section v. this paper concludes with a brief summary of the results and future applications of the device."
"under an external stress, material deformation can be modeled using the constitutive equations from continuum mechanics. simplified models such as the generalized kelvin-voigt model are shown to be suitable in the case of uniaxial constant stress on hydrogels and their creep responses are shown to be close to those of soft tissue [cit] ."
"creep test is a dynamic viscoelasticity imaging [cit], in which a step force is applied on the object for a relatively long time, and the strain behavior of the tissue is recorded during the impression period."
"first, the uniform phantom a and the background part of the inclusion phantom were made and then the day after phantom b and inclusion part of inclusion phantom were made. the phantoms were kept at room temperature for a day before using them in the experiments."
"another direction of research with proposed approach, which we want to designate, is solving the tfsm equations. this line of researches is not covered enough in works on timed finite state machines and we believe that known methods for solving the fsm equations can be adapted to tfsms easily enough."
"to validate the compression device performance, we measured the retardation times of a test phantom by the both compression device and the bose instrument, and the outcomes were compared. for this purpose, we measured the surface displacement of the phantom."
"there exists a special set of languages which can be described by the use of finite automata; those are regular languages, which are closed under union, concatenation, complementation, intersection and also under restriction and expansion."
"the set l s of all functional traces of the tfsm s is the f-language of the tfsm s. here we again assume, that the pair / and the sequence  are the equivalent notions, when speaking about f-language of a tfsm."
we tested the same phantom with our compression device and with the bose system at identical force and ramp functions. the surface displacement data resulting from the bose system and the compression device outputs were recorded for 85 s.
"employing the creep test in different imaging modalities like ultrasound [cit], mri [cit] to evaluate the viscoelastic properties of the medium has been proposed by a number of investigators. for this purpose, compression devices have been designed for such imaging modalities [cit] . other investigators [cit] have proposed various compression devices for elasticity imaging, where such devices are mainly designed for testing tissue at its rest position after the compression. tissue compression has also been investigated in the context of poroelasticity [cit] . however, these studies aimed at measuring stress relaxation response of the medium."
"notice that transformation from a given tfsm to an fsm according to the above rules is unique whereas the back transformation from an fsm to a tfsm could be made in different ways; however all such tfsms are pairwise equivalent, i.e. their f-languages are the same (see the corollary 2 to proposition 1)."
"the creep test method described in this paper is particularly suitable for evaluation of breast masses. elastography methods that are currently used in clinics for breast evaluation are primarily designed to measure the stiffness of breast masses. the creep test method described here provides information on viscosity as well as elasticity. the additional information would improve differentiation of mass pathology, and, thus, may have a significant impact on breast cancer diagnosis."
"the goal of designing the automated compression device was to apply a prescribed amount of force for a predetermined time on a phantom or tissue to study its creep response during a period of time. a potential future application is to use this device for imaging the viscoelasticity of breast tissue or other organs in a group of patients. one of the important elements in this device is the combination of back plate that includes four sensors, located symmetrically at four corners of this plate, and pressure plate that is in contact with the other side of the sensors. outputs of these sensors are summed, thus these sensors collectively measure the total applied force that is applied to the object by the pressure plate. therefore, even in cases where the compression plate is not able to make complete contact with the surface of the object, for example, when the object's surface is not completely flat, the sensors can still measure the total force applied to the contact surface. this feature also increases the flexibility of the device application on tissues with curved surface like breast because in such cases, the pressure may not be evenly distributed. the device ability to measure the surface displacement is another important feature. the surface displacement profile helps to validate this compression device when comparing to a standard mechanical testing machine. to validate the compression device, the bose instrument was used for comparison."
"in strain elastography, the strain map is created by compressing the tissue and recording the pre-and postcompression displacement profiles [cit] . the strain is lower in a stiffer material than in a softer material. because it is not possible to measure a local distribution of stress, one cannot measure the quantitative values of stiffness or the young's modulus in this method [cit] . therefore, relative changes in the strain within the image have been used to detect various anomalies in different organs like breast [cit], prostate [cit], and thyroid [cit] ."
"as it was explained, the calculated t 1 value for uniform phantom b or stiffer one was 7.3 ± 0.5 s and for the inclusion part of the inclusion phantom was 7.1 ± 0.8 s. in the same way for uniform phantom a or softer one, the calculated t 1 value is 6.2 ± 0.4 s and for the background part of the inclusion phantom was 5.9 ± 0.3 s."
"gelatin is an appropriate model to study the viscoelastic properties of the breast tissue because of its similarity to breast stroma [cit] . in both cases, the mechanical properties are established by a high molecular weight, type i collagen matrix that is saturated in water [cit] . in this paper, we used three different phantoms and one with inclusion. all phantoms are based on gelatin."
"let us consider a word   a * . automaton s recognizes or accepts  if there exists an accepting state q  q such that q is a -successor of the initial state, i.e. q  suc s (s 0, ). the set l s of all sequences, which are accepted by s, is the language accepted by the automaton or simply the language of the automaton s. the language of a finite automaton is a regular language [cit] ."
"proof. the property to be deterministic, observable and complete is specified by the cardinality of sets of i/o-and i-successors. fsm a s has one and only one transition with pair 1/n at each state, that is why properties of fsm a s to be deterministic, observable and complete depend on transitions with other i/o pairs."
"based on the excitation source, elastography techniques can be divided in two main subgroups: dynamic elastography which is based on dynamic force stimulus [cit] and quasistatic strain elastography in which a compressive force is applied [cit] . both the dynamic and quasi-static elastography techniques have been implemented on ultrasound imaging systems. shear wave elastography is a dynamic elastography method in which the elasticity is estimated based on the shear wave induced by an excitation. the excitation may be from an external source [cit], or an internal source, such as acoustic radiation force. several techniques have been developed based on the acoustic radiation force, including supersonic shear imaging (ssi) methods [cit], acoustic radiation force impulse (arfi) [cit], and shear wave dispersion ultrasonic vibrometry (sduv) [cit] . these shear wave elastography methods have been applied to conduct in vivo studies in liver [cit], thyroid [cit], prostate [cit], and breast [cit] . among these shear wave elastography methods, ssi, sduv, and arfi have been used to quantify the viscoelastic properties of the soft tissue [cit] ."
the authors would like to thank l. berglund from mayo clinic biomechanics laboratory for experimental assistance with the bose instrument. the content is solely the responsibility of the authors and does not necessarily represent the official views of the national cancer institute or the national institutes of health.
"as we mentioned before, creep test has been done on a small group of patients with nonpalpable breast masses to differentiate between the benign and malignant masses [cit] . however, that study was performed with manual compression of the probe onto the breast. the present compression device provides a means for conducting the creep test in a more objective way with improved control of the applied compression compared to manual compression."
"as an example, consider the composition of tfsm s in fig. 2 and p in fig. 4 where corresponding fsms are shown as bottom figures. consider symbols a and o to be external input and output respectively, x and b are internal symbols. to derive the parallel composition of fsms, we firstly construct the related automata which are shown in figure 5 . double lines denote accepting states."
a 2-d autocorrelation method was used to calculate the particle velocity from adjacent frames and then the displacements were estimated by integrating the particle velocity in time [cit] .
"shear wave elastography and creep test method measure the mechanical properties of tissue at different time and frequency scales, which may affect their outcome. shear wave elastography measures the elasticity at high-frequency range in the order of hundreds of hertz and during a short time period. the creep test, on the other hand, measures the viscoelasticity of tissue over a long time period, in the order of seconds, which corresponds to the frequency range of less than a hertz [cit] ."
"a schematic of the device is shown in fig. 1 . this device is equipped with an ultrasound probe which can record the local deformation of tissue under stress. this device consists of a lightweight miniature linear actuator (mr20ls with 2-mm lead screw, pbc linear, roscoe, il, usa), driven by a brushless dc (bldc) servomotor (rp17m bldc servomotor with encoder, electrocraft, dover, nh, usa) which moves a commercial ultrasound probe together with a back and pressure plate rapidly onto the material or tissue until a preset force level on the pressure plate is reached. the encoder associated with the motor measures the displacement of the pressure plate. the actuator control system uses four small load sensors (fss015, 15 newton range, honeywell, bloomington, mn, usa), embedded in a back plate, to measure the applied force. the control system maintains the preset force level constant for a predetermined period of time, typically about 10-100 s. when the creep response measurement is complete, the actuator automatically retracts the probe."
"in this section, we present the application of the compression device combined with ultrasound strain imaging for viscoelasticity evaluation of a medium. the first step in retardation time imaging is acquiring sequential iq data of the phantom that is being compressed by the compression device. next, these data are used to calculate the strain at every pixel. then, the retardation time at each pixel is estimated to produce an image depicting the retardation time distribution across the phantom. fig. 6 shows the block diagram of the ultrasound probe pressure plate and the location of the phantom the phantom used for this experiment is the same one that was used in section iii-a. we used the compression device to apply a 3-n force with 18-n/s ramp. an ultrasound system (verasonics, inc., kirkland, wa, usa) with a linear array transducer (l11-4v, verasonics, inc., kirkland, wa, usa) was used to monitor the phantom response. for this purpose, we used plane-wave mode [cit] to acquire a series of rf data of the phantom during the compression. fig. 7 shows the first b-mode image from the acquisition sequence. in this experiment, the ultrasound center frequency was 6.43 mhz and the frame rate was 20 hz during the acquisition time of 85 s; thus, 1700 frames were acquired."
"the b-mode image of the stiff phantom b is shown in fig. 11(a) . the strain profiles of four points specified in fig. 11(a) is demonstrated in fig. 11(b) . these strain profiles are normalized and shown in part (c) of this figure, which shows the material has essentially the same behavior at these points. in fig. 11(d), the strain profiles of point 1 and point 2 accompanied with standard deviation are shown."
"in our paper we consider a problem of synthesis, namely the problem of parallel composition construction of two tfsms. this procedure gives an instrument to build complex systems from simple ones, each described by a tfsm. also, the approach we used in this paper to describe a parallel composition construction procedure opens the way for solving various problems of tfsms."
"to describe behaviour of a system, which transforms sequences over one (input) alphabet into sequences over another (output) alphabet, special kind of automata, called finite state machine, is usually used [cit] . a special timed or clock variable can be associated with a tfsm; this variable counts time ticks passed from the moment when the last transition has been executed and is reset to 0 after each transition (input-output or time-out). in this paper, for the sake of simplicity, we assume that the output is produced immediately after a machine gets an input, i.e., we do not consider delays when executing transitions."
a potential source for different appearances of the retardation time map in figs. 12 and 13 compared to that in fig. 9 is that we used different recipes for the phantoms used in those two sets of experiments. another potential source could be damage to the phantoms due to multiple compressions applied to the phantoms during the experiments. similar experimental variables could be responsible for the different appearances of the strain profiles in figs. 8 and 14 . these variations are some of the issues that we plan to explore in more details in our future studies.
"because the applied force is not a pure step function, the initial part of the response is a complicated mixture of the elastic and viscoelastic responses to the ramp excitation. however, after the force reaches its final value, it is safe to assume that the time varying part of the strain profile is only due to viscoelastic response. for this reason, the initial 1 s part of the response is excluded from the strain data analysis and the model is reduced to a second-order kelvin-voigt model (5) [cit] . 5 illustrates the surface displacement profile resulted from the bose instrument and our compression device before and after fitting a curve according to (5) . a nonlinear least squares optimization method was used to find the closest fit to measure the two retardation times t 1 and t 2 ."
"to demonstrate the performance of the device on media with different sizes and stiffness values, we built two uniform cubic phantoms: a (softer) and b (stiffer). we created a third inclusion phantom in which the background material was similar to the phantom a, and contained a cylindrical inclusion that was made from the same material as the one used in phantom b."
"for the repeatability check, the creep test was repeated five times on the phantom. the resulting relative errors for t 1 and t 2 are 2.9% and 12.4%, respectively, showing a reasonable repeatability of the test."
"we did not repeat the test on phantoms more than ten times, five times for bose machine and five times for compression device, because there was a risk of damaging the phantom."
"as it was shown in the previous part of this paper, the t 2 value for gelatin phantom is more than 30 s. since, for this part, we recorded only 20 s of data, therefore a single exponential, (9), is used to fit to the strain profiles at each spatial location to construct the t 1 value map in fig. 12(b), the measured t 1 value is 7.3 ± 0.5 s, while the fitting error map for this area is less than 2% as shown in fig. 12(c) .the same procedure was done for phantom a and the inclusion phantom. fig. 13 shows the resulting maps. the t 1 value measured for phantom a shown in fig. 13(b"
"in order to reduce memory needs and processing time for the inclusion phantom experiment, we processed approximately 3 cm of b-mode images in the axial and lateral direction, and for the uniform cubic phantoms, we processed only 1.5 cm of b-mode images in the axial direction. all of these experiments were done at room temperature."
"we tested the performance of the compression device also on the inclusion phantom with different size and structure, nonuniform phantom, comparing to previous phantoms. to construct the t 1 map of inclusion phantom, (9) was used in the similar way that was used for both uniform phantoms a and b. the b-mode image of the inclusion phantom and the strain profile of several specific points in this figure are illustrated in fig. 14 . fig. 15 shows the strain, t 1 map, and the fitting error map for the inclusion phantom. the maps in this figure show the outcome of the creep response after removing the initial 2 s of data."
"in the first uniform phantom study, the data were recorded for 85 s. thus, it was possible to use a double exponential to calculate the t 2 value, which is usually more than 30 s for that phantom. in medical applications, and especially for the breast (which is the future goal of this study), however, the recording time is often limited to the time the patient can hold her breath, which is usually less than 20 s. keeping this in mind, we limited the recording time to 20 s in the latter part of this paper. in these experiments, it was not necessary to use a double exponential due to the shorter recording time, thus a single exponential was applied. in both cases, the error fit is less than 5%."
"the timed finite state machine (tfsm) is a model based on well-known finite state machine (fsm), which allows explicit description of a time aspects of system behaviour. for example, reaction of a system can be different depending on the time moment an input action is applied to it. in the last few years the interest to the various problems of tfsm has increased. the main lines of researches covered by the post papers are the analysis problems: relations between tfsms [cit] and test generation methods against those relations [cit] ."
"in order to validate the compression device for performing material creep test, we used a standard mechanical testing instrument (bose electroforce, eden prairie, mn, usa) [cit] to apply force and record the resulting displacement and repeated the same procedure with automated compression device, then compared the results. we used a forcing function in the form of ramp and hold with 3-n final force and 18-n/s ramp rate (ramp duration of 1/6 s). we experimentally determined that 18-n/s ramp is the maximum ramp speed that we can use without any ringing effects for the gelatin phantoms used in our experiments. fig. 3 shows the simplified schematic of the bose mechanical testing instrument."
"it should be noted that phantom dimensions are smaller than the pressure plate of the compression device and also smaller than the plate of the bose instrument; therefore, the compression is applied similarly on the entire top surface of the phantom in both cases. in either system, the bottom surface of the phantom rests on a platform, where free slip boundary condition is assumed. this allows for minimal boundary condition effects, and, thus, the results would be comparable for both devices. fig. 4 illustrates the load (force sensor output) from our compression device and the bose instrument at force level of 3 n with 18-n/s ramp speed. fig. 4(a) shows the entire force signal. fig. 4(b) displays only the initial 10 s for better visualization. both devices reach their final force values in a fraction of second."
"in practice, it is not possible to apply a step force and study its response. it is, however, possible to approximate the step force with a ramp-and-hold force, where the speed of the ramp is adjusted to suit the response of the material under test. the ramp speed should be chosen fast enough such that the initial elastic response of the material can be easily separated from the slow creep response [cit] ."
"to test the performance of the device for different media with different stiffness and sizes, we used three different gelatin phantoms. the error fit for all the cases was less than 5%. the relative error related to t 1 values for the part made with same material but in different shapes was small, i.e., less than 5%."
"furthermore, the states (q, h) and (q, g1) are (f-)equivalent likewise the states (s, g) and (s, h) . that is why we keep only two states in tfsm, shown in figure 9 . iv. conclusion and future work the propositions 1 and 2 with corollaries give an approach for solving different problems of tfsms: first, the corresponding fsms should be constructed, then appropriate methods of fsm theory can be applied to solve the problem of interest and, finally, the result should be converted back to a tfsm. in this paper we used this approach to define, but more importantly, to construct parallel composition of given tfsms. however, there is a weak point in the presented work. we have not given a proof of the fact, that such a way to construct parallel composition gives a tfsm which describes a system, combined from two tfsms, operating in the slow environment setting, as it is done for fsm parallel composition [cit] . but propositions 1 and 2 give confidence, that such a proof can be obtained."
"m echanical properties of a medium can be assessed by different mechanical testing methods. these methods cover a large range of conventional mechanical testing techniques like indentation [cit], shear rheology [cit], compression testing [cit], uniaxial tensile testing [cit], magnetic force methods [cit], and bulge tests [cit] . elastography is a modality for evaluating the stiffness in tissue. this method can provide an insight about the local mechanical properties of the medium when it is combined with other imaging modalities like ultrasound [cit] or magnetic resonance imaging (mri) [cit] ."
"here, we apply the same process on the strain profile of each point within the phantom (see fig. 8 ) to calculate the retardation times t 1 and t 2 maps. fig. 9 shows the resulting t 1 and t 2 maps. the average values for these two maps in this figure are 3.4 and 33.6 s for t 1 and t 2, respectively. the related strain for t 1 part 1 is around 2% and for t 2 part 2 is around 3% . it is also noted that compared to the strain map in fig. 8(a), the retardation time maps in fig. 9 show a relatively uniform distribution with only few scattered variations. fig. 10 shows the fit error estimation map of the second-order kelvin-voigt model (5). this map was made using (7) . as it can be seen, the error is mostly less than 2%, which confirms the suitability of the second-order kelvin-voigt model as well as the overall strain tracking performance."
"to test measurement reproducibility, we repeated these experiments five times on the same phantom using the same force 3 n and ramp speed 18 n/s. table i shows the resulting t 1 and t 2 values for these measurements."
"in almost all commercial mechanical testing machines, the theoretical step stress required for the creep test is replaced by a highly controlled ramp function. however, the speed of such ramp stress should be high enough to be able to capture the shortest retardation times of the material under the test. the aim of this study is to design a compression device that is able to create such ramp forces with the additional feature of housing an ultrasound probe for continuous strain imaging during the test."
"the goal of this study is to design and validate a fully automatic device with the ability to apply an approximate step force to excite the viscoelastic creep response in tissue while allowing in vivo ultrasound image acquisition during the transient response of the tissue. to validate the device, we will compare its performance with a standard mechanical testing device and measure the related error. the long-term goal of this study is to employ this device to conduct creep test on a group of breast patients. previously, creep test studies of human breast have been conducted using manual compression [cit] ."
"the back plate is custom fit to the probe using a liquid castable plastic material (smoothcast 300, smooth-on, inc., easton, pa, usa). the four small load sensors are sandwiched between the back plate and the pressure plate. fig. 2 shows the position of the load sensors at the four corners of the back plate. thus, as the motor is applying the compression on an object, the resulting resisting force from the object is transferred to the load sensors by the pressure plate."
"where d (t n ) is the measured strain and d f (t n ) represents the fitted viscoelastic compliance curve based on the selected model. e(t n ) represents the residual error a normalized error can then be introduced as expressed in (7), where q quantifies the goodness of the fit in terms of the power of residual error e(t n ) relative to the power of the total measured strain"
appropriateness of a second-order kelvin-voigt model was tested. fig. 10 showed that the fitting error was less than 2% in most of the region across the phantom. repeatability tests also showed that the retardation times could be measured with small variations in measurement results.
"since the ultimate application of this compression device is in breast patients, the optimal range of the applied force would be less than 20 s. the device is attached to a platform, which is held by an articulated arm. this way there is no need manual handling of the compression device, thus avoiding possible hand motions."
"validation. figure 4 shows the distribution of valid suggestions over the 10 generated suggestions per failure as histograms. our validation against the delta grammar reported that on average 71% of the generated suggestions are valid per failure for cant.resolve. for cant.apply.symbol, the percentage of valid suggestions is higher at 98% since the code changes are syntactically simpler in nature (e.g., method argument changes). as it can be seen, the majority of the generated sequences are valid. we discuss the main reasons for invalid suggestions in section 7."
"for every build resolution session computed, our approach rst extracts the snapshot ids in the rst and the last builds. the rst id corresponds with the code snapshot that caused the diagnostic. the second id points to the code snapshot in which the diagnostic was resolved."
"based on the estimated emission factors of pollutants in table 3, the resulting frequencies using operational strategies and travel distance, the emissions of both scenarios in table 4 are calculated using eq (14) . the results of operational strategies-based model are compared with the fro strategy, without other strategies added, to illustrate reductions in emissions of approximately 13% for each of five pollutants: co 2, hc, co, no x, and pm 2.5 . the promising results indicate that the use of operational strategies can significantly reduce bus emissions and improve environment."
"lstms perform well for short to medium input sequences but fail on large sequences. attention mechanisms [cit] solve this limitation to a large extent by extending the attention span of the network. since our resolution-change feature sequences could potentially be long, in our network, we adopt the normed bahdanau attention [cit] ."
we generate features from each resolution change: a build diagnostic and the edits that x that diagnostic. we use the generated features as input into a neural network to learn patterns of transformations between build diagnostics and resolution changes.
"to capture the resolution-change features, we dene a domainspecic language (dsl) called delta. delta's grammar is formally specied in the extended backus-naur form (ebnf) for antlr [cit] and shown in listing 1."
"the proposed methodology is performed on a real life bus line (line 23) in dalian, china. line 23 runs from dalian university of technology (stop 1) to dalian university of foreign language (stop 20), visiting 20 stops with a length of 13.8 km, as shown in fig 2. it presents high demand due to its route along the main road connecting shopping centers (heishijiao, heping square, zhongshan square), a tourist attraction (xinhai square), and the city center (qinniwaqiao)."
"our results show that dd works well for two distinct error types with dierent x patterns. we expect to perform as well on other build-error types and plan to extend support to other types. we are not aware of any tools that repair java compilation errors to compare directly against in our evaluation. the most related work we found [cit] repairs syntax errors (a single error type only) in c, with a far lower success rate."
"data was collected in the morning peak hour 7:00-8:00 [cit] . specifically, the collected demand data consists of alighting and boarding passengers that were manually counted at each stop for each bus trip. average passenger boarding and alighting times were set at 2s and 1s per passenger, respectively. an average time is 0.3 min, consisting of bus deceleration/acceleration as well as door opening and closing times at each stop. the observed running times between two successive stops are shown in fig 2. the layover time for each strategy is 2 min. the running time without serving any stop from stop 7 to stop 19 is 10 minutes for the short turn strategy. based on the collected demand data, average load profiles are constructed, as shown in fig 3. as is illustrated by fig 3 there is a high passenger demand between stop 1 and stop 19 for line 23 in direction 1, and between stop 1 and stop 7 in direction 2. additionally, max-load is observed on route segments 2-3 in direction 1 and 4-3 in direction 2; it is because stop 3, a transfer stop of line 23, is located in a business office zone (software park service center). based on the characteristics shown in fig 3 it is possible to establish the sets of start and end stops of feasible operational strategies as is shown in table 1 ."
"future research could extend the optimization model with user's attributes consideration, such as values of travel time, ages, purposes of trip, and reliability measures required. the data of these attributes can be collected by smartphones using a big-data platform. the use of operational strategies to reduce cost and emission operational strategies can be also applied for improving electric bus operation's efficiency. nowadays the use of electric buses is growing rapidly to help the environment. however, there are issues around an efficient use of the limited battery capacity. generally speaking, electric https://doi.org/10.1371/journal.pone.0201138.g007"
"the conventional method for detecting source code changes is the unix line di [cit], which computes changes at the textual granularity of only line-level add and delete actions. this leads to a di which is largely dependent on how the source code is formatted. while line di is a popular method for human consumption, e.g., during code review, automatically inferring syntactic changes to the code from textual line dis is dicult."
"a previous large-scale study reported that professional developers build their code 7-10 times per day on average [cit] . the study found that build-time compilation errors are prevalent and cost developers substantial time and eort to resolve. build-time compilation errors 1 emerge when a developer compiles her code through a build management system such as bazel, gradle, or maven."
"for our running example, the source feature would include: for cant.apply.symbol, we augment the source feature with three types of data, namely, expected, found, and reason. expected pertains to the expected types inferred by the compiler, found shows the types found in the code, and reason represents a textual description of why the compiler cannot apply the symbol. target features. the target feature contains information about the resolution changes, i.e., ast changes made to the failing snapshot to resolve the build diagnostic."
"we focused on two error types (cant.resolve and cant.apply.symbol) in this work for two reasons: first, these cover the majority of java compilation errors developers make at google: 59% of all instances, 58% by cost ( table-ii ). our data show that, whether or not they are easy to x (r1), in practice developers spend a huge amount of time manually xing them. automating these repairs would free developers to focus on other issues. second, adding a new error type requires collecting relevant developer data and re-training the model, which is time-consuming."
"in the real-life transit system, multiple fare payment modes are used. table 6 shows the average estimated boarding and alighting times associated with the four different fare payment modes, i.e. on-board cash payment, contactless card, inserting coins, and off-board payment, which impacts bus dwell times and further influences bus operating cost as well as emissions. nowadays, the contactless card has become more and more common. but it is not realistic to assume that every passenger has such a card. above all, the tourists in a city usually do not have this type of cards for the local public transit services. therefore, it is more often that we see a mixed system used in the real-life public transit system. for example, on a bus you may use either cash on board or a contactless card, which depends on your convenience. besides, in some cities no changes will get back to you on the bus. in this case, the fare collection facility of this type of bus services is normally a toll collection machine and you shall put your money in it in the very front of the bus. this type of on-board cash payment (inserting coins) may incur much less time than the on-board cash mode with changes shown in table 6 but shall be slightly higher than one corresponding to contactless card because passengers often do not get their money out of the pocket until they get on board. this paper does not consider the mixed system but focuses on the four payment modes listed in table 6 . fig 7 represents the fleet sizes using the operational strategies-based bus system under four payment modes. certainly the number of vehicles required for using the off-board payment mode is the minimum across all modes because of saving bus dwell and running times. similar arguments hold for the other payment modes. fig 8 shows the emissions of the bus system using different paying modes. it is observed that a cash payment mode is significantly higher, with emissions, than those using inserting coins, contactless card, or off-board payment mode, especially in pollutants no x, pm 2.5, and co. the emissions for the transit system using the off-board payment mode are relatively close to that of contactless card mode. table 7 contains the differences of emissions between using cash payment (base case) and other modes; for instance, the reduction percentage of operational strategies to reduce cost and emission emissions using inserting coins is more than 12%. a higher percentage reduction is observed for contactless card and off-board with more than 20%. that is, the use of contactless card payment will considerably reduce emissions. the results of fig 8 and table 7 are of the case study in dalian. it is to note that the number of passengers using contactless cards is not high because of only a few places to buy and recharge them; also, it offers only a discount of 5% where all bus routes have a flat fare of 1 or 2 rmb yuan. thus, to save operating cost and improve the environment, fare makers should offer various fare-discount opportunities to encourage passengers to use contactless cards. finally a point worth mentioning is that nowadays smartphones have become an essential part of our life including their payment technology. it is likely, therefore, that most of the passengers, including tourists, will use this mode of payment if it will be encouraged to use. this will not only be convenient, but will also improve the environment. operational strategies to reduce cost and emission"
"our goal is to help developers to repair build errors automatically. previous research in the area of automated program repair has focused on nding patches when a test failure occurs through xed templates and search-based techniques [cit] . in this paper, we propose a novel approach, called dd, for automated repair of build-time compilation errors. our insight is that there exist common patterns in the way developers change their code in response to compiler errors. such patterns can be learned automatically by extracting abstract syntax tree (ast) changes between the failed and resolved snapshots of the code and feeding these as abstracted features to a deep neural network. this paper makes the following contributions:"
"recent studies [cit] suggest that models originally developed for analyzing natural language, such as n-gram models, are also eective for reasoning about source code. this has come to be known as the software naturalness hypothesis [cit], which states that large code corpora are statistically similar to natural-language text, since coding is also an act of human communication. following this naturalness hypothesis, we believe probabilistic machine learning models that target natural language can be further exploited for helping software developers. more specically, the idea we propose here is to formulate the task of suggesting build repairs as a neural machine translation (nmt) problem. instead of translating one natural language into another, in our case, a given source build-diagnostic feature is \"translated\" to a target resolution change feature that resolves the diagnostic."
"due to the fact that a common full route operation (fro) strategy is used for meeting imbalanced travel demand in most cities throughout the world, full advantage is not always taken of public transit resources. this inefficient operation situation will create the undesirable in-vehicle crowding in some segments with high travel demand, excessive empty seats in other segments with low travel demand, and increases of operating cost as well as round trip time. this may not only frustrate passengers, but may also result in a loss of resources and environmental deterioration."
"the mismatch between supply and demand becomes more and more apparent in the bus route that partially overlaps with a subway line in urban areas. because of having the subway with shorter in-vehicle times and higher reliability, passengers tend to use it if their origins, destinations, or both are serviced by the subway. as a result, on bus-route segments not overlapping the subway, passengers' overcrowding is often observed, whereas on overlapped segments low passenger loads with empty-seats are often observed. in this case, some approaches should be explored to achieve the better match between supply and demand on the bus route. one efficient method in previous studies is to use multiple operational strategies to suit the observed travel demand in the best possible manner, such as limited stop, short turn, deadheading, and mixed strategy. furth and day [cit] showed the use of operational strategies was able to increase bus productivity significantly with even loads on vehicles that would lead to substantial operating cost reductions."
the rst step required for learning repair patterns is obtaining developer data on compilation errors. we collect this data as part of the regular development cycle at google. we also use this dataset to characterize the prevalence and cost of dierent classes of compilation errors in practice.
our insight is that there are recurrent patterns in the way developers resolve build errors in practice. such patterns can be automatically inferred from resolution changes and leveraged to assist developers.
"bus round trip time. for a single bus line, the round trip time, ct h l, that it takes for a bus using strategy l to complete a full cycle of its route at operation period h, consists of running time, dwell time, and layover time:"
"at google, changes made to the source code in developers' ide clients are automatically saved in the cloud as temporal snapshots. this means a complete history of all saved changes made to the code is preserved with retrievable snapshot identiers. this feature allows us to go backward in time and retrieve a snapshot of the code in the state it was in at the time of a particular build."
"the ast dierencing algorithm rst tries to map each vertex on the source ast ast s to a vertex on the target ast ast t, by comparing the vertex labels. if any unmatched vertices are detected, it computes a short sequence of edit actions capable of transforming ast s into ast t . finding the shortest edit action is np-hard; therefore, heuristics are used to compute a short transformation from ast s to ast t deterministically [cit] . the nal output of the tree dierencing step is composed of a set of change actions that indicate moved, updated, deleted, or inserted vertices on the source and target asts:"
"studies on environment impacts, related to using public transport, focus mainly on the effects of alternative technology and of operation measures. alternative technology includes bus fuels e.g., diesel, gasoline, biodiesel, compressed natural gas, hybrid [cit], and electric, bus performance [cit], etc. compared with a diesel bus, an electric bus can reduce 19-35% in co 2 emissions through a life-cycle assessment [cit] . electric buses will further benefit the environment if a cleaner power grid and a higher system charging efficiency will be provided in the future. the operation-based improvements consist of transit signal priority, bus stop location, queue jumper lane, types of buses selection, and driver braking [cit] . during traffic congestion the use of transit signal priority can reduce greenhouse gas emissions by 14%. when considering the combined use of both alternative technology and operational improvements, emission reductions can been further improved to a level of 23% in greenhouse gas emissions [cit] ."
"the dataset we use for training and evaluation is described in section 2.4, which is composed of developer build data collected over a two months period at google."
"the performance of the case study is evaluated by the use of vehicles required for covering all passenger demand. the operational strategies mathematical problem is formulated as a mixed integer non-linear programming (minlp) problem. we used the outer approximation with both equality relaxation and augmented penalty (oa/er/ap) algorithm coded in the dicopt solver of gams to solve them [cit] . results of the system performance for both scenarios are shown in table 2 and fig 4. the optimal result of operational strategies-based bus system is to operate a combination of three strategies: fro, short turn, and limited stop, with frequencies of 18.11, 33.20, and 4.20 buses/h, respectively. fig 4 displays the service topological structures of these resulting operational strategies. as table 2 results, the use of operational strategies improves significantly the bus system performance, and cuts the number of vehicles required by 12.5%."
"this means within two months, developers spent approximately 21 months xing build errors. from this total, cant.resolve is again the most costly diagnostic kind by far, with 47% (⇡ 10 months) of the total active resolution cost. cant.apply.symbol acounts for 11% (⇡ 2 months) of the total active resolution cost. these toptwo error classes alone account for 58% of the total cost, which is approximately a year of developer cost in our dataset."
source features. the source feature pertains to the build diagnostic kind and its textual description. these can be included in the source features for any diagnostic kind without any diagnosticspecic knowledge.
"obtained by converting the vector potential a back to the time domain. convergence will be discussed in view of the works [cit] and the fractional step method, [cit] ."
"once build diagnostics are collected and resolution sessions are constructed, we pass the resolution sessions to the resolution change detection step of our pipeline. the goal in this step is to systematically examine how developers change their source code to resolve build errors."
"patch search techniques have a number of shortcomings in practice. first, they often require pre-dened templates of bug patterns and cannot learn new patterns. second, the patch generation process needs to search the vast program space, which can be costly as thousands of patches need to be generated to nd one that resolves the failure. finally, they have been shown [cit] to produce many false positives, i.e., they often x the test failure, but not the actual fault."
"the reduction in the number of vehicles by 12.5% can be achieved when using operational strategies, in comparison with applying fro strategy exclusively, which suggests operating efficiency improvement. the pollutant emission of transit vehicles are estimated by the moves emission model. we show that by applying operational strategies an approximately 13% reduction in emissions per pollutant (co2, hc, co, nox, pm2.5) is attained, compared with the fro scenario."
"the value of parameter θ is the time associated with bus deceleration/acceleration and with doors opening and closing at a stop. strategies serving a few stops, i.e., ls and mls strategies, accommodate larger values of parameter θ because of poor bus performance, inappropriate driving behavior, or poor traffic conditions. the results of the case study show that the use of operational strategies reduces the emissions in an average sense by 12.37% across all scenarios with different θ values. it is apparent that for higher θ values this reduction increases. it implies that the use of strategies for poorly-performed buses can further improve the environment."
"apparently, it has been proven to be more beneficial for a transit operation system when an additional strategy is introduced, not only using fro strategy. but in practice, most routes present more mixed and complex load patterns. for such demand situations, the implementation of multiple operational strategies appears to a more promising alternative than introducing a single strategy [cit] ."
in eqs (4) and (5) the number of passengers using each strategy is proportional to its relative frequency. the product term between the binary variables in eqs (4) and (5) ensures that passenger demand between two stops can use a strategy only if this strategy is related to these two stops.
"the lorentz detuning of an accelerating cavity, which is the change of the resonant frequency due to the mechanical deformation of the cavity wall induced by the fig. 2 . one cavity cell with field lines and exaggerated deformation, [cit] electromagnetic pressure is a coupled electromagneticmechanical problem. in a first step, maxwell's eigenproblem is solved"
is derived from the initial domain ω (0) and the iteration can be restarted with the computation of an eigenvalue. in the full paper this scheme will be discussed in more details. the focus will be on the spatial discretization with isogeometric analyses using nonuniform rational b-spline (nurbs) and de-rhamconforming b-splines [cit] .
"using each snapshot id, we then query the snapshot cloud server to obtain the code exactly as it was at that particular point in time."
"in the full paper the convergence, [cit], of this iteration scheme and tailored time integration will be discussed. it will be shown that the optimal time integration order depends on the iteration counter n, [cit] ."
"parsable ast. for our ast-di-based approach to work, the broken code must be parsable into an ast. with the missing symbol errors we examined, the code is always parseable resulting in a unique ast, but this will not be true for other diagnostic kinds. parsers can be designed to recover from syntax errors instead of failing fast [cit] . we may need to switch to such a parser to handle incomplete asts."
"in this paper, we studied build errors and how developers change the source code to resolve them in practice. we showed that patterns exist in such code changes. we proposed a generic technique to learn patterns of code changes, extracted from ast dis between failure and resolution pairs. we formulated automated program repair as a machine translation problem. using a deep neural network, our technique, dd, is capable of suggesting ast changes when given a build diagnostic as input. our evaluation on a large corpus of real developer data at google shows that dd generates correct xes with 50% accuracy for two compiler diagnostic kinds, with the correct x in the top three 85%-87% of the time."
"the non-uniformity of public transport passenger demand warrants the use of operational strategies to attain operating efficiency. we proposed a methodology to analyze the benefits of using transit operational strategies to reduce operating cost and undesirable emissions. in this work, four alternative operational strategies are considered: full route operation (fro), short turn (st), limited stop (ls), and a mix of limited stop and short turn (mls). a real life case study in dalian of china is conducted to illustrate the application of this proposed methodology."
"bleu. the next metric we use to assess the generated output of the model is bleu [cit] . bleu is a well-known and popular metric for automatically evaluating the quality of machinetranslated sentences. it has been shown to correlate well with human judgments [cit] . bleu calculates how well a given sequence is matched with an expected sequence in terms of the actual tokens and their ordering using an n-gram model. the output of the bleu metric is a number between 1-100. for natural language translations, bleu scores of 25-40 are considered high scores [cit] ."
one of the benets of using a compiled programming language is that programming mistakes can emerge at compilation time rather than when the program is executed. a failed build will often prompt an edit-compile cycle in which a developer iterates between attempting to resolve diagnostic errors and rerunning the compiler.
"as a nal step we identied 110,219 resolution sessions which contained only a single diagnostic (singular) and which ended in a successful build (green). we use these singular, green resolution sessions as training data since we can be sure that the change made by the developer actually resolved the diagnostic in question. our dataset excludes automated batch builds since we are only interested in interactive activity by developers. table 2 presents the top-ten most frequent and costly build errors in our dataset. the table shows the diagnostic kind, active resolution cost (average, min and max), the number of subsequent builds to resolve the diagnostic (average, min and max), the number and percentage of instances of each diagnostic kind, the relative active resolution cost with respect to the total, and a textual description of the diagnostic kind."
"the changed vertices detected by the ast di are indicated in grey in figure 2 . the ast di for the java code in the running example detects that there is an import vertex inserted into the root vertex, compilation_unit. the fully-qualied package name of immutablelist is also inserted as a child vertex into the new import vertex. for the build le, the change action detected is an update in the dependencies (deps), namely, the common base package is updated to common collect."
"ranking of correct repairs. the ranking of the correct suggestion in the list of the suggestions is an indication of how well the model can generate the correct repair. the higher its ranking, the sooner the repair can be applied. for each failure that dd generates a correct suggestion, we note its position on the list of 10 suggestions. (b) cant.apply.symbol figure 5 : position of the correct suggestion. table 4 presents our results for the two diagnostic kinds we evaluated. for each diagnostic kind, the table shows the achieved perplexity and bleu scores, the number of compilation failures in the test dataset evaluated against the trained models, the number of suggestions generated (i.e., 10 suggestions per failure), the average percentage of valid suggestions per failure, the percentage of correct suggestions overall, and the percentage of correct repairs that are ranked in the top 3 suggestions."
"moreover, this study contains the differences of emissions between using cash payment (base case) and other payment modes; for instance, in the case study the reduction percentage of emissions using inserting coins is more than 12%. a higher percentage reduction is observed for contactless card and off-board with more than 20%. that is, the use of contactless card payment will considerably reduce emissions."
"the remainder of this study is organized as follows: the methods used for achieving optimal bus operational strategies and for estimating their associated emissions, are presented in section 2. then a real life case study from dalian in china is introduced in section 3. the results are illustrated in section 4. finally, the main findings are summarized, and extensions of this approach are proposed in section 5."
"in eq (2), δ l p is a binary decision variable; it equals 1 if strategy l serves stop p, and 0 otherwise. the dwell time of strategy l at stop p can be saved if strategy l does not serve the stop p; that is, savings of passenger boarding and alighting times and the time associated with bus deceleration/acceleration and doors opening and closing at stop p."
"we propose a methodology consisting of two stages shown in fig 1 with the purpose to investigate and analyze the benefits of using operational strategies in reducing both the operating cost and the undesirable environmental impacts. that is, through the minimization of the operational cost to measure the reduction attained of the emissions. the first stage is to construct an operational strategies-based bus operations system to obtain the optimal combination of multiple operational strategies. four feasible operational strategies are included: full route operation (fro), short turn (st), limited stop (ls), and a mix of limited stop and short turn (mls). the second stage uses an emission model to estimate vehicle emissions before and after the implementation of the optimal operational strategies attained."
"given the high prevalence and cost of cant.resolve and cant.apply.symbol in practice, we focus on these two categories of build errors to generate repair suggestions in this work. note, however, that our approach is generic enough to be applied to any of these diagnostic kinds with minor adaptations. we use the cant.resolve kind as a running example in our paper. an identier must be known to the compiler before it can be used. an inconsistency between the denition of an identier and its usage, including when the denition cannot be found, is the root cause of this build error. this occurs when there is, for instance, a missing dependency (e.g., on another library), a missing import, or a mistyped symbol name."
"in the previous sections we have discussed the mutual coupling of transient and frequency-domain to static problems. the third example revisits the well-known iterative coupling of frequency to time domain problems. again, the electromagnetic field is given by the curl-curl equation, however since we are in frequency domain we can regard displacement currents"
"the whole process of generating resolution sessions, resolution changes, and features, as well as training the model is pipelined using sequential dependencies, which makes our whole learning process automated and repeatable."
"ast dis between the failing and resolved snapshots were computed for all green, singular resolution sessions in our dataset. we constrain the number of ast changes to be larger than zero and fewer than six, as higher numbers of changes often include refactorings, and in previous studies xes have been shown to contain fewer than six modications [cit] . we computed 110,219 green, singular resolution sessions in total, over 37,867 distinct java and 7,011 distinct build les."
"at this point, we have at our disposal two snapshots of the code at the broken and xed states, for each resolution session. to understand how developers change their code to resolve build errors, we compute the dierences going from the broken state to the xed one."
"we use the test datasets, containing the source and target features, for our evaluation. these are datasets that our trained models have not seen before. we evaluate each diagnostic kind separately. for each item in the test dataset, we retrieve the source feature and send it to the inference server to obtain repair suggestions. the target feature of the item in the test dataset, which is the x the developer performed to repair the build error, is used as a baseline to assess the 10 suggestions generated by dd."
"the rapid growth of private vehicle ownership and usage has been made significant accessibility and convenience for human travel activities. however it also produces a considerable amount of emissions including carbon dioxide (co 2 ), nitrogen oxides (no x ), hydrocarbons (hc), carbon monoxide (co), particulate matter (pm), etc. the international energy agency (iea) estimated transport activity contributing approximately 23% of co 2 [cit], 11% of pm 2.5, a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 and 28% of no x . these emissions need to be drastically reduced because of adverse impacts in terms of human health and environment. currently, the emission-reduction challenge of the transport sector is in an irreversible shift to low-emission mobility. with this in mind, public transport is a sustainable alternative enabling to respond to the increasing mobility needs with lower emissions [cit] . nonetheless, public transport modes of travel have a great potential for further improvement in terms of their operational efficiency as well as environment impacts."
"the intention with a resolution session is to capture the period of time that the developer is actively working on resolving the diagnostic. therefore, we dene the time window t to be one hour. this window represents a \"task switch window, \" i.e., if a developer has not performed a build within t, it is likely they have switched to some other activity; e.g., they could have gone to a meeting, lunch, or left the oce."
"recall that a build failure can contain multiple compilation errors, i.e., diagnostics. these build failures contained a total of 3.3 million diagnostics from which we were able to nd 1.9 million resolution sessions. the remaining 1.5 million diagnostics for which we found no resolution session correspond to those changes abandoned by developers or with more than one hour between build attempts."
"in total, there were 1,853,417 compiler diagnostics that were later xed within a resolution session. as the table shows, 51% (949,325) of those diagnostics are related to the compiler not being able to resolve a particular symbol, i.e., the cant.resolve diagnostic kind with a \"cannot nd symbol\" message. [cit], which showed 43% of build errors are caused by issues related to cant.resolve [cit], it seems the issues developers have with missing symbols have only been exacerbated. the next diagnostic kind in the table in terms of instances is cant.apply.symbol with 8% of total diagnostics. cant.apply.symbol happens when the compiler cannot nd a method declaration with the given types."
"following the generation of bus emission factors by the revised moves model, the total emissions of each pollutant (co 2, hc, co, no x, pm 2.5 ) are calculated in eq (14) ."
"consider the most frequent resolution change patterns for cant.resolve in table 5 . for \"add missing import statement\" and \"remove import,\" we do not require any location information because imports must be in a specic location in a java source le. for \"change the method call\" and \"change the missing symbol,\" we know the location of the missing symbol from the diagnostic text, and we can modify the code at that location. for \"add missing build dependency\" and \"change build dependency,\" we know which build rule produced the error, and we can make the change to its dependency list."
"correctness. table 4 shows that 18,051 out of the 36,101 (50%) failures in our test dataset received a correct repair suggestion, i.e., one of the 10 suggestions for that failure was an exact match with the actual developer x, for cant.resolve. for cant.apply.symbol, dd achieves a similar rate of correct suggestions, namely, 1,263 out of 2,687 (47%)."
"perplexity and bleu scores. recall that low perplexity (i.e., single digits) and high bleu scores (i.e., [cit] are desired for the models. as table 4 shows, our models reached low perplexity values of 1.8 and 8.5 for cant.resolve and cant.apply.symbol, respectively. also, the bleu scores achieved were high, namely 42 for cant.resolve and 43 for cant.apply.symbol."
"correctness. dd is able to suggest an exact correct suggestion for around half of the build failures in our evaluation. note that a correct suggestion is not a simple binary output. it is composed of a complex sequence of ast changes to be applied to the failing code. compared to the state-of-the-art repair rate of 27% for syntax errors [cit], we believe our 50% correctness rate is substantial in practice. our relatively high accuracy rate can potentially be attributed to the following properties of our dataset and approach:"
"we thank petros maniatis for his invaluable feedback on an earlier draft of this paper and thang luong, rui zhao, and eugene brevdo from the google brain team for helping us with the gnmt framework."
"perplexity. perplexity [cit] measures how well a model predicts samples. low (e.g., single digit) perplexity values indicate the model is good at predicting a given sequence."
a failed build can contain numerous diagnostics; each of these diagnostics might be new or might be the same as one reported from a previous build. we therefore rst set out to convert sequences of builds containing a particular diagnostic into resolution sessions.
"to analyze code changes at the syntactic level, we take a tree dierencing approach [cit] . we parse the broken and xed snapshots of each resolution session to generate the corresponding abstract syntax trees (asts). we have created parsers for java and the bazel build language in this project, although support for other languages can easily be added. then the asts of the broken and xed snapshots are passed into a tree dierencing algorithm."
"ranking. for the failures with correct suggestions, we evaluate the position of the correct suggestion within the list of suggested resolutions. figure 5 depicts the distribution of the position of the correct xes in the 10 generated suggestions. our data shows that the majority of the correct suggestions are on the rst position. for cant.resolve, 85% of the correct xes are in the top three positions. similarly, for cant.apply.symbol, 87% of the correct xes are in the top three positions."
"extensive research has been conducted to investigate the use of operational strategies in improving bus operating efficiency and service levels, but with little attention so far to the emissions performance of these operational strategies, except that alam and hatzopoulou [cit] applied a linear regressions analysis method to capture the environmental impacts of using a limited stop strategy. there is a need, therefore, to estimate the impacts of using operational strategies on emissions."
"given two asts, source ast s and target ast t, an ast di is a set of vertex change actions that transforms ast s into ast t ."
"syntactic validation. for validating the suggestions for syntactical correctness, we generate a lexer and parser from our delta grammar through antlr4. we pass each inferred suggestion through the delta lexer/parser. this way, we assess whether the model generates suggestions that conform to the grammar of the expected resolution changes. the output is binary, i.e., either the suggestion is valid or invalid."
"we also calculated the relative cost of build errors by multiplying the number of build-diagnostic instances by the average active resolution cost needed to resolve the diagnostic, for each diagnostic kind. the total cost amounts to 57,215,441 seconds for two months of data."
"every build initiated by a developer is automatically logged at google. the log contains detailed information about each build, including any compiler diagnostics, i.e., detailed error messages, along with a snapshot of the code that was built. for this study, we collected textual build logs for a period of two months, from january 23, 2018, to march 23, 2018 . the collected logs were subsequently parsed and analyzed to understand which build errors happen most frequently in practice. although our builddiagnostics framework is language-agnostic, in this paper, we focus on build errors pertaining to java projects."
"in eq (14), the amount of pollutant m emissions for a bus operation system is estimated by summing pollutant m emissions, produced by all buses, using optimal operational strategies. the pollutant m emissions of buses using a strategy l are obtained by multiplying the emission factor of pollutant m (unit: g/km) ef l, by the average travel distance of a bus using strategy l, lh l, and by the frequency of strategy l, f l . the emission factor of pollutant m can be generated by the revised moves model as is described above in this subsection. lh l and f l are obtained by solving the proposed optimization model in subsection 2.2."
"in this model, the objective function expression (6) is the sum of round trip times of feasible strategies multiplied by their respective frequencies f l . in the round trip time term, the first element is running time of a strategy; the second element is bus layover time at terminals or turning points; the third element is the total dwell times of a strategy at intermediate stops. constraint (7) guarantees that frequencies of strategies are nonnegative. constraint (8) makes sure the number of onboard passengers is less than the bus capacity. this constraint refers to onboard passengers at stop p when the bus departs from that stop. the first term on the right hand side of this constraint represents passengers boarding at stop p, and the second term is in-vehicle passengers passing through stop p. constraint (9) maintains waiting time per passenger within an acceptable service level. constraint (10) shows whether or not strategies serve stops; equals 1 if strategies serve stops, and otherwise 0. constraint (11) ensures that all stops, including start and end stops of strategies and intermediate stops, can be visited by fro and st strategies. constraint (12) makes sure that all strategies must visit their start and end stops. constraint (13) guarantees that strategies involved with skipping stops do not skip more than three successive stops."
"today, due to increased accuracy of modeling and simulation, multiphysical problems become more and more important in many engineering applications. often a monolithic approach, i.e., the solution of all subproblems at once, is cumbersome or even impossible because incompatible algorithms or software packages are involved. thus simulation engineers need to couple subproblems in an efficient and stable way, where subdomains are solved separately. this introduces a splitting error, which is mitigated by an iterative procedure."
"as optimizer, we employ the stochastic gradient descent (sgd) algorithm [cit] with a learning rate of 1.0. to mitigate over-tting the model, we set a dropout value of 0.2. the idea is to randomly ignore units from the neural network during training, which prevents co-adaptations on the training data [cit] ."
"to provide more contextual information to the machine-learning algorithm, we can optionally add more diagnostic-specic information to the source feature. for instance, for cant.resolve, we parse the snapshot of the broken code into an ast. we locate the missing symbol on the ast using the information provided by the compiler, such as its label and location. once the symbol is located, we traverse the tree to extract its ast path. in addition to the ast path of the symbol, its tree vertex type and label, as well as its child vertices (e.g., type arguments for method calls) are added to the source feature."
"finally, the features dataset is randomly partitioned into three chunks of 70%, 15%, and 15% for training, online evaluation of the model during training, and oine evaluation of the model against unseen features (see section 6), respectively."
"the processes of boarding and alighting are considered simultaneously (different doors for boarding and alighting) and boarding and alighting flows are independent of each other. therefore, the average passenger boarding and alighting times of strategy l at stop p and period h, dw"
optimization problem formulation. the use of operational strategies is considered for the reduction of the operation cost of a public transport system. the objective is to minimize the number of vehicles required for covering passenger demand while ensuring all passengers within an acceptable service level. the complete formulation of the optimization problem is as follows:
"note some operational strategies such as the limited stop strategy in table 2 and fig 4 are not very practical, because they have low frequencies and present almost the same as fro strategy. for a practical application of these operational strategies, the planner should take a closer look at the results to fix these situations. in this case, the limited stop strategy can be replaced by using the fro strategy."
"when a bus skips a stop, it must result in a reduction of round trip time. this stems from the reduction in dwell time consisting of passenger boarding and alighting times as well as time denoted by parameter θ, generated by bus deceleration/acceleration as well as doors opening and closing at a stop. variation in the value of parameter θ depends on bus performance and driving behavior, as well as traffic conditions. it is a factor that affects the use of operational strategies in a transit system. in fig 5, if the value of parameter θ is less than or equal to 0.1, the same strategy set, consisting of fro and st strategies, is obtained. when this value increases more than 0.2 and less than 0.4, ls strategy is added in the strategy set. this is because an increase in the value of parameter θ results in increasing round trip time. using ls strategy can control the increase of round trip time by using its skipped stop option. in addition, the frequency of st strategy almost remains the same, while the frequency of ls strategy presents an opposite trend to that of fro strategy. as this value increases beyond 0.4, mls strategy is introduced in the strategy set to further increase saving time in round trip time. it implies that for those routes with great time consumed by bus deceleration/acceleration as well as doors opening and closing at a stop, which may root in poor bus performance, bad driving behavior, or poor traffic conditions, strategies with serving a few stops presents more benefits in saving the number of vehicles required. it rises with the increase θ from 0.05 to 0.4. this indicates that the reductions of emissions can be attained further by controlling bus performance related to deceleration/acceleration and doors opening and closing at stops; these are associated with a small value of θ. table 5 shows the percentage reduction for each pollutant using operational strategies in comparison with the fro strategy (base case). the use of operational strategies reduces the operational strategies to reduce cost and emission emissions in an average sense by 12.37% across all scenarios with different θ values. it is apparent that for higher θ values this reduction increases. it implies that the use of strategies for poorly-performed buses can further improve the environment."
"for each resolution change in the resolution sessions, we generate a source feature and a target feature, separately. a pair of source and target features capture information about the build failure and ast changes made to resolve the failure, respectively."
"turning delta programs back into code modications. delta programs are essentially a dierence between two versions of the same program. dd predicts this dierence given an input constructed from the diagnostic. given an inferred delta program, we must apply that dierence to an unseen program. in particular, we need to know where to make a suggested change."
"we generate features and train two models separately for cant.resolve and cant.apply.symbol to compare the applicability of dd on dierent diagnostic kinds. in addition to the deep neural network setup described in section 5.2, we congure the network as follows. the maximum sequence length for both source and target is set to 100. the batch size is 128, and the number of training steps is 100,000. we congure the inference to generate 10 suggestions (see section 5.3). to train a model, we feed the source and target features of the train dataset as well as the vocabulary lists to the network. the model starts by creating the source and target embeddings for all token representations. to that end, a vocabulary is provided for the source and target for tokens that are meant to be treated uniquely. we also feed the val dataset to the network for online evaluation during training. our models are trained on google's internal cloud nodes with gpus."
"our deep neural network is built on top of tensorflow [cit] . it is composed of deep long short-term memory (lstm) [cit] recurrent neural networks (rnns) of 1024 units with 4 encoder and 4 decoder layers. as encoder type we use the google neural machine translation (gnmt) encoder [cit], which is composed of 1 bi-directional layer and 3 uni-directional layers."
"in this contribution we like to advertise the increased accuracy and stability due to iterative procedures by discussing three examples: field-circuit coupling in section 1, a mechanical-electromagnetic problem in section 2 and finally a thermal-electromagnetic problem in section 3. in the full contribution also implementation issues and the practical relevance of those iteration schemes will be discussed."
"in general, we should usually be able to infer where to make the suggested change either by construction (i.e., there is only one syntactically valid place to make the modication) or by leveraging the location information in the diagnostic text. putting the program repair tool into production. we intend to integrate dd into the development environment of developers at our company. to that end, we will need to verify suggestions before applying them to user code. we intend to build a system to speculatively apply suggested xes to a code snapshot and attempt to build them. we can then discard suggested xes that do not compile, and only present the ones that do to the user. there is also interesting user interface work to do here to ensure that the tool is actually useful to developers."
"the main contributions of this work are two-fold: (i) develop an integrated methodology, including an optimization model, to accurately assess the impacts of the use of operational strategies on operating cost and emissions; and (ii) use of strategy-based activated intermediate stops as variables in the optimization model. this study, for the first time, to the best of our knowledge, provides an assessment procedure of emissions of the use of multiple bus-operational strategies. the results will help making appropriate transit polices to handle environmental issues."
"in addition, dt h l., the total dwell time of strategy l along the route in period h that depends on passenger boarding and alighting times at each stop and average time θ consumed by bus deceleration/acceleration as well as doors opening and closing at each stop, is expressed as:"
"a source build diagnostic is considered correctly repaired if at least one of the 10 suggested repairs is valid and exactly matches the x the developer performed, i.e., the target feature (baseline) in the test dataset. we use textual string equality for comparing each suggestion with the baseline."
"the running time of strategy l along the route at period h, rt h l, may vary between periods because of the dynamic, stochastic and uncertain nature of traffic. generally speaking, the running time during peak hours is longer than during the off peak period. in this work, we divide the operational time window into several periods, and consider each hour as a single period. the layover time of strategy l during period h, lt h l, is determined according to the requirement of bus operation agencies."
"the s-mart advertising mechanism has rewards for both content producers and content consumers. by means of this ad mechanism, content producers are able to advertise their content to other members. content consumers can consume the ads recommended based on their empirical user profiles whenever they are short of tokens to buy their desired content items. figure 2 illustrates the targeted ad action. examples of empirical human data for user profiling include users' past interests (e.g., content type and category), users' past bidding and buying patterns (e.g., the percentage of their net wealth used while buying a content item) and social graph data (e.g., social connections of users and what their friends are doing). thus, this monetary system for online content trading sharpens user profiling."
we analyze the effect of each one of these parameters on the final seller revenue by starting with the most interesting parameter: number of copies of a file to sell.
"content consumers can spend their tokens to buy various digital rights to content items at different prices via different transaction styles. s-mart differentiates among view-only, download and resell rights of content items. these rights can be traded by using either marked price transactions or multiple-winner s-mart auctions. if content consumers cannot find files that they are looking for, they can request them by using s-mart's content request mechanism. to request a file, a content consumer enters its brief description, file type (document, audio, video ant etc.) and the tokens offered to access its different digital rights. if a content producer responds to the content request, the requester is notified via e-mail, and she can access these responses whenever she logs into the system. the user interface for requesting files is shown in fig. 3 ."
"we consider an imaging system that collects streaming measurements of the video fðx; y; tþ according to the following model. we let t denote a measurement interval (in seconds), and we suppose that one linear measurement is collected from f d every t seconds. (as we will discuss below, typically t will be much smaller than the nyquist sampling interval suggested by the video's bandwidth.) letting yðmþ denote the measurement collected at time mt, we can write"
"given the bidding and selling strategies of other students. note that leftover tokens at the end of the competition are not counted towards p i in (1) to prevent students from having hard upper limits (i.e., 100 tokens) in their bidding strategies."
"fortunately, if we assume that the video fðx; y; tþ has limited temporal bandwidth, we can simplify this recovery process to some degree. let us assume that fðx; y; tþ has temporal bandwidth bounded by ω t rad∕s. we note that the temporal bandwidth of will also be bounded by ω t rad∕s."
"the proposed answer reranking component is embedded in the qa framework illustrated in figure 1 . this framework functions in two distinct scenarios, which use the same ar model, but differ in the way candidate answers are retrieved:"
"this work introduces an online content trading platform, which we call sharing mart (s-mart), developed recently at princeton university to perform controlled experiments in social file sharing systems constructed on top of technological networks as overlays. s-mart is a virtual money based file sharing system, in which different digital rights (e.g., view only, download and resell rights) of various file types (e.g., video, audio, graphics and documents) can be traded by means of different transaction styles (e.g., marked price transactions and s-mart auctions). s-mart is envisioned as being a content equivalent of stock exchange markets, ebay and amazon. further details of the proposed system are provided in section ii. the current system [cit] has been used by 250 students in the school of engineering and applied science at princeton university. we are also willing to open part of the developed operational system to researchers interested in experimenting in file sharing systems with human subjects, similarly to what has been done with planet lab for communication network researchers."
"our algorithm is intended for reconstructing a video from streaming compressive measurements; specifically, we assume that one compressive measurement is collected at each time instant. (the single-pixel camera is a prototypical example of an imaging device that produces such measurements.) we note that a preliminary version of our algorithm 9 was intended for a different measurement model-one in which multiple measurements are collected from each frame but at a much lower frame rate. for example, 1000 measurements might be collected from each of 30 frames per second. while acquiring such measurements may be possible using other compressive imaging architectures, 4,5 our particular interest in this paper is on how to correctly deal with streaming measurements."
"our analysis of the temporal complexity of videos reveals an interesting tradeoff between the spatial resolution of the camera, the speed of any moving objects, and the temporal bandwidth of the video. in sec. 4, we explain how this tradeoff can be leveraged to address challenge 2 above. we propose a novel, multiscale algorithm for reconstructing video signals from compressive measurements. our algorithm begins by reconstructing a coarse-scale approximation to the video, having low spatial and temporal resolution. from this, we obtain a crude estimate of the motion vectors in the video, and we then use these motion vectors to define a sparsifying transform that enables reconstruction of the next-finer scale approximation to the video. our representation framework is built around the limat 8 method for standard video compression, in which motion compensation is used to improve sparsity in the three-dimensional (3-d) wavelet domain. we solve the chicken-or-egg problem by alternating between motion estimation and video reconstruction, proceeding to higher and higher spatial and temporal resolutions. at each scale, we employ the anchor frames described above to manage the complexity of the reconstruction process."
"feature values: our reranking framework uses real-valued features. the values of the discourse features are the mean of the similarity scores (e.g., cosine similarity using tf.idf weighting) of the two marker arguments and the corresponding question. for example, the value of the qseg by qseg sr1 feature in figure 2 is the average of the cosine similarities of the question text with the answer texts before/after by out to a distance of one sentence before/after the marker."
"this was modeled to add the linemodel and transformermodel of iec61968 to accept the linesegment and transformer of the distribution facility. figure 5 shows the cim db structure converted into rdbms. figure 6 shows the structures of the analog and discrete tables in the pdb. one of the fields, with the field name \"ems\", is an identifier that identifies the measured values that are essentially required for the system analysis and control application programs. figure 7 shows the relationship between rtdb and acm in detail. as shown in the figure, fast data transfer can be executed by means of memory copy due to the relationship of mapping tables between rtdb and acm."
"as a sparsifying basis to be used for reconstructing x a, we employ the lifting-based invertible motion adaptive transform (limat). 8 further details regarding limat are given in sec. 4"
"for the ya cqa corpora, 50% of qa pairs were used for training, 25% for development, and 25% for test. because of the small size of the bio corpus, it was evaluated using 5-fold crossvalidation, with three folds for training, one for development, and one for test."
"if the anchor frames are defined at the video's temporal nyquist rate, and if there are no additional assumptions made about the video, then one should not expect any temporal correlations to remain among the anchor frames. in many real-world settings, however, there will be objects moving within the scene, and the smoothness of the object motion can lead to temporal correlations, e.g., that can be captured via motion-compensated transforms. thus, in order to impose the strongest possible model on the vector of anchor frames, it may be helpful to look for sparsity in a motioncompensated wavelet transform. (for promising experiments that involve a simplified version of our algorithm -one that uses anchor frames but not with a motion-compensated wavelet transform -we refer the reader to a companion technical report. 15 ) in sec. 4, we propose one method for doing this while confronting the chicken-or-egg problem."
"our analysis in the previous sections has (1) revealed that videos that are sampled by imaging devices (including cs imaging devices) may have limited temporal bandwidth, (2) characterized the tradeoffs between the spatial resolution of the camera, the speed of any moving objects, and the temporal bandwidth of the video, and 3. explained how a relatively low-rate stream of \"anchor frames\" can be used to reduce the complexity of the reconstruction problem. in this section, we build on these insights and propose a complete algorithm for reconstructing a video from streaming compressive measurements. in order to construct an effective sparsifying basis for the video, this algorithm involves a motion-compensated wavelet transform, and in order to confront the chicken-or-egg problem, this algorithm is multiscale, employing anchor frames at a sequence of progressively finer scales and alternating between reconstructing an approximation of the video and estimating the motion vectors."
"these answer candidates are then passed to the answer reranking component, the focus of this work. ar analyzes the candidates using more expensive techniques to extract discourse and ls features (detailed in §4), and these features are then used in concert with a learning framework to rerank the candidates and elevate correct answers to higher positions. for the learning framework, we used svm rank, a variant of support vector machines for structured output adapted to ranking problems. 7 in addition to these features, each reranker also includes a single feature containing the score of each candidate, as computed by the above candidate retrieval (cr) component. 8"
"from the formulation above, we see that it is possible to focus the reconstruction process on recovery of a relatively low-rate stream of anchor frames x a, rather than the high-rate stream of frames x d measured by the imaging system. of course, in simplifying the cs problem, we have changed the very unknowns that must be solved for. in many cases, we believe that it will suffice merely to reconstruct and display the anchor frames themselves; however, we note that the raw video frames x d can be estimated from the reconstructed anchor frames by using the interpolation eq. (10) ."
"our reconstruction begins at the coarsest scale with an attempt to estimate x a;1 . motivated by our temporal bandwidth analysis, we begin reconstruction at a low spatial resolution for the following reasons:"
"the discourse marker model (dmm) extracts cross-sentence discourse structures centered around a discourse marker. this extraction process is illustrated in the top part of figure 2 . these structures are represented using three components: (1) a discourse marker from daniel marcu's list [cit] ), that serves as a divisive boundary between sentences. examples of these markers include and, in, that, for, if, as, not, by, and but; (2) two marker arguments, i.e., text segments before and after the marker, labeled to indicate if they are related to the question text or not; and (3) a sentence range around the marker, which defines the length of these segments (e.g., ±2 sentences). for example, a marker feature may take the form of: qseg by other sr2, which means that the the marker by has been detected in an answer candidate. further, the text preceeding by matches text from the question (and is therefore labeled qseg), while the text after by differs considerably from the question text, and is labeled other. in this particular example, the scope of this similarity matching occurs over a span of ±2 sentences around the marker."
"13 [cit] t21 all significance tests were implemented using one-tailed nonparametric bootstrap resampling using 10,000 iterations. svm rank 's regularization parameter c. for all experiments, the sentence range parameter (srx) for dmm ranged from 0 (within sentence) to ±3 sentences. 14"
"selling a file is different from selling a tangible good via an auction mechanism because multiple copies of a file can be sold in a single auction. therefore, we implemented the following uniform price, unit demand and multiple-winner file auction in s-mart. the seller enters the minimum price, auction start/end date and time, and digital rights and the number of copies of the file, m, to be sold. then, the file is sold to the highest m bidders at the price of the (m + 1) st highest bid. if the number of unique bidders is smaller than m, the file sale price becomes equal to the minimum price, and all bidders pay this minimum price. we first prove an important property of this auction."
"according to the following procedure, the cim class was converted to a relational database management system (rdbms), which is a type of off-line db management. the conversion procedure is as follows:"
"in our proposed algorithm, we use block matching (bm) 20 to estimate motion between a pair of frames. the bm algorithm divides the reference frame into nonoverlapping blocks. for each block in the reference frame, the most similar block of equal size in the destination frame is found and the relative location is stored as a motion vector. there are several possible similarity measures; we use the l 1 norm."
"traditional qa: in this scenario answers are dynamically constructed from larger documents [cit] . we use this setup to answer questions from a biology textbook, where each section is indexed as a standalone document, and each paragraph in a given document is considered as a candidate answer. we implemented the document indexing and retrieval stage using lucene 5 . the candidate answers are scored using a linear interpolation of two cosine similarity scores: one between the entire parent document and question (to model global context), and a second between the answer candidate and question (for local context). 6 because the number of answer candidates is typically large (e.g., equal to the number of paragraphs in the textbook), we return the n top candidates with the highest scores."
"lexical semantics: we trained two different rnnlms for this work. first, for the ya experiments we trained an open-domain rnnlm using the entire gigaword corpus of approximately 4g words. 13 for the bio experiments, we trained a domain specific rnnlm over a concatenation of the textbook and a subset of wikipedia specific to biology. the latter was created by extracting: (a) pages matching a word/phrase in a glossary of biology (derived from the textbook); plus (b) pages hyperlinked from (a) that are also tagged as being in a small set of (hand-selected) biology-related categories. the combined dataset contains 7.7m words. for all rnnlms we used 200-dimensional vectors."
"we then explain in sec. 3 how this limited temporal complexity can be exploited in addressing challenge 1 above. following standard arguments in sampling theory, we note that under various interpolation kernels, a stream of high-rate video frames (such as those measured by a single-pixel camera) can be represented as a linear combination of a low-rate (e.g., nyquist-rate) \"anchor\" set of sampled video frames. we then explain how the cs video problem can be reformulated by setting up a system of linear equations that relate the compressive measurements to the underlying degrees of freedom of the video (specifically, the anchor frames). this significantly reduces the number of unknowns that must be solved for. as we demonstrate, our use of interpolation kernels for reducing the burden of processing streaming measurements can also be much more effective than the traditional technique of partitioning the measurements into short groups and assuming that all measurements within a group come from the same frame. such raw aggregation of measurements-which actually corresponds to using a rectangular interpolation kernel in our formulation-can introduce significant interpolation error and degrade the reconstruction quality."
"furthermore, the design of the acm db proposed in this paper focused on the development of a common model that can be used by all application programs, as well as a data reduction method that can increase the execution speed and the convergence of the application programs. the common model was designed by taking into consideration the topological structure and facility models of the distribution system and by considering that it consisted of hierarchical and non-hierarchical structures. through the case study, using a large distribution real system, the proposed database model was validated as having no difficulty with application to a real environment, and the database size reduction and rapid execution of the application programs was proven through the reduction method. first, the analysis and control via the real-time and periodical application program execution could be achieved and compared to the existing distribution system operation system, which was run by existing line monitoring and fault recovery functions through the application of the database model proposed in this paper. second, a distribution system that interconnected distributed power systems could be designed by accepting models such as distributed power and new control equipment. thirdly, the memory burden of accepting a large database was relieved and the convergence and execution speed of the application programs were increased through system reduction."
"to recover α a and subsequently x a from the compressive measurements. 18 in order to get around this chicken-oregg problem, we propose a multiscale reconstruction approach."
"in the long literature of standard video compression 6 (not video cs), a variety of methods have been proposed to exploit spatial and temporal redundancies. one common approach combines motion compensation and estimation algorithms 7 with image compression techniques; for example, given a set of vectors describing the motion in the video, the limat framework 8 yields a motion-compensated wavelet transform (across the temporal and spatial dimensions) intended to provide a sparse representation of the video. while some of these central ideas can be absorbed into the cs framework, there is an important challenge that we must address. unlike the standard video compression problem where the frames of the video are explicitly available to perform motion estimation, in cs only random measurements of the underlying video are available. we are faced with a chickenor-egg problem: 9 given the video frames, we could estimate the motion, but given the motion we could better estimate the frames themselves."
"our formal analysis and the experiments described in sec. 2.3.1 pertain specifically to translational videos in which gðxþ is bandlimited, hðtþ has bounded velocity and bandwidth, and the entire contents of the frame translate en masse. however, real-world videos may contain objects whose appearance (neglecting translation) changes over time, objects that move in front of a stationary background, multiple moving objects, and so on. we suspect that as a general rule of thumb, the temporal bandwidth of real-world videos will be dictated by the same tradeoffs of spatial resolution and object motion that our theory suggests. in particular, the prediction of 2ω x γ þ 2ω h given by our theory may be approximately correct, if we let ω x be the essential spatial bandwidth of the imaging γ system, be the maximum speed of any object moving in the video, and ω h be the essential bandwidth of any object motion. this last parameter is perhaps the most difficult to predict for a given video, but we suspect that in many cases its value will be small and thus its role minor in determining the overall temporal bandwidth."
"to tease apart the relative contribution of discourse features that occur only within a single sentence versus features that span multiple sentences, we examined the performance of the full model when using only intra-sentence features, i.e., sr0 features for dmm, and features based on discourse relations where both edus appear in the same sentence for dpm, versus the full intersentence models. the results are shown in table 3 . for the bio corpus where answer candidates consist of entire paragraphs of a biology text, overall performance is dominated by inter-sentence discourse features. conversely, for ya, a large proportion of performance comes from features that span only a single sentence. this is caused by the fact that ya answers are far shorter and of variable grammatical quality, with 39% of answer candidates consisting of only a single sentence, and 57% containing two or fewer sentences. all in all, this experiment emphasizes that modeling both intra-and inter-sentence discourse (where available) is beneficial for non-factoid qa."
"the initial ramp up of user activity in s-mart was achieved by providing each new s-mart member with initial start-up funding (in terms of tokens) with which to begin file trading. therefore, the total amount of virtual money in the system increases with user involvement, and more content trading activities are expected to appear with an increasing number of tokens in the system. however, some users' token balances may decrease over time if they cannot attract enough interest in their content items from others. we support such users by means of s-mart's advertising mechanism, whose details are explained in the next section. furthermore, such users can also buy resell rights of popular content items in s-mart, and start contributing to s-mart's ecosystem by sharing these files with others. this creates a liquid secondary market for content similar to stock exchange markets, with increased availability of the same content from different users. in this ecosystem, some content brokers are expected to appear that buy resell rights to content when it is unpopular, and sell them for higher prices when it becomes popular. we also reward users with tokens for their positive contributions to s-mart such as rating and reviewing files that they have bought."
"1. we demonstrate that modeling discourse is greatly beneficial for nf ar for two types of nf questions, manner (\"how\") and reason (\"why\"), across two large datasets from different genres and domains -one from the community question-answering (cqa) site of yahoo! answers 3, and one from a biology textbook. our results show statistically significant improvements of up to 24% on top of state-of-the-art ls models [cit] ."
"when the master server experiences a problem, or when the db is being updated, running operations that fail are sent to a backup server and the rtdb is recreated using the operation information stored in the hard disk via middleware, as shown in the figure. therefore, the operation server should be re-booted to this end. in the db structure of the current das, the configuration definition for rtdb and its creation is coded within the middleware, which is characterized by a dependent structure where the db and the middleware are not separated. a code revision of the middleware is thus required; for example, a new rtu model is added."
"this paper serves two purposes. the first one is an introduction to a fully operational monetary system, the sharing mart system, for online content trading. it has been used by 250 students in the school of engineering and applied science at princeton university. in sharing mart, different digital rights (e.g., view only, download and resell rights) of various file types (e.g., video, audio, graphics and documents) can be traded by means of different transaction styles (e.g., marked price transactions and s-mart auctions). the second purpose is to present opening discussions regarding the capabilities of this platform as an experimental testbed in file sharing systems. to this end, a toy auction experiment and some initial empirical findings backed-up with game theory and auction theory have been presented. selected parts of the developed operational system can be opened to researchers interested in experimenting in file sharing systems with human subjects."
"the appearance of the hðtþ term within an exponent in eq. (1) can complicate the task of characterizing the bandwidth of fðx; tþ. however, by imposing certain assumptions on hðtþ, this analysis can become tractable."
"in summary, this analysis suggests that, for the majority of errors, the qa system selects an answer that is both topical and adjacent to a gold answer selected by the domain expert. this suggests that most errors are minor and are driven by current limitations of our answer boundary selection mechanism, rather than the inherent limitations of the discourse model."
there are two main types of users in s-mart: (a) content consumers and (b) content producers. content producers are the catalytic users with creative minds who are able to create interesting content items that attract attention from other users. content producers can also be content consumers when they want to buy others' files. these users usually do not have any difficulty in buying others' files since they typically have enough tokens in their bank accounts.
"another related algorithm 23 has been proposed in the field of dynamic medical resonance imaging (mri). this algorithm relies on the assumption that the locations of the significant coefficients of each video frame can be estimated via a motion-compensated frame. similar to the above, a motioncompensated frame, x mc, is obtained from motion vectors computed between a pair of estimates of the frames x 1 and x 2 . the initial estimates of the video frames are computed in a frame-by-frame fashion from their respective random measurements. when motion compensation is accurate, the indices of large coefficients of x mc will provide an accurate prediction of the locations of the significant coefficients of x 2, and this knowledge can be used to reconstruct an accurate estimate of x 2 . once a new estimate of x 2 is formed, this procedure is repeated for the next frame x 3, and so on."
"in this section, we conclude by describing the differences between our work and several other important ones in the literature. we also present simulations that demonstrate the performance of our algorithm."
"which ensures that the reconstructed video agrees with the motion vectors estimated from the preview video. while this method can be relatively successful in recovering videos from small numbers of measurements, according to the authors, the algorithm can have difficulty in reconstructing high spatial frequency components."
"the classical method used to discover the price of an item when a seller is uncertain about it is to let others submit their bids for this item (see [cit] . with this motivation, we have built an auction platform to enable content producers to solicit bids from content consumers to discover fair market prices for their content items. in this part of the paper, we first describe the s-mart auction platform and the multiplewinner s-mart auction. then, we report our initial results of a toy auction experiment with human subjects for online content trading."
"the control plane serves four main purposes: (a) as a central bank (i.e., it adjusts the total token amount in the system), (b) as a commercial bank (i.e., it updates user accounts after each file transaction and generates monthly trading reports), (c) as a police station (i.e., it preserves different digital rights via watermarking) and (d) as an investment bank (i.e., it enables a secondary market)."
"when averaged over the best three seller strategies, we found that the average number of copies maximizing the seller revenue is 5.33, which is close to the 6 copies result suggested by theorem 2. this further motivates us to ask the following question: what happens if all students set the number of copies to be sold to 6 by keeping other parameters (e.g., auction duration and minimum price) constant? in this case, the file sale price stays the same at the minimum price set in the beginning of an auction, and the total revenue equals the number of unique bidders times the minimum price. our findings are summarized in fig. 5 . except for one student, revenue of a seller summed over three auctions strictly increased. the average revenue increase is 20%. the student whose revenue stays the same already set the number of copies per auction to 6. these findings are another justification for employing the nash equilibrium seller strategy to maximize seller revenue."
"in our simulations, we construct synthetic cs measurements from the original high-rate video sequences. in order to reasonably apply cs to either of these video sequences, one would need more than 1000 measurements per second, and thus with the available videos (acquired at 1000 and 250 frames per second) we would not have a sufficient number of measurements if we synthesized only one measurement from each frame. thus, for the purposes of testing our algorithm, we collect more than one random measurement from each frame of the test video. (still, the number of measurements we collect from each frame is moderate-120 per frame for the first video and 800 per frame for the second video.) our setting remains very different from related ones, [cit] however, because we do not intend to reconstruct all of the high-rate frames directly (indeed, this would be difficult to do with so few measurements per frame); rather we will reconstruct a reduced set of anchor frames."
"this fact, which is illustrated in fig. 1(b), exemplifies a central theme of our work: filtering a video in the spatial direction can cause it to be bandlimited both in space and in time. once again, we note that similar conclusions were reached in the analysis of plenoptic 11 and plenacoustic 12,13 functions."
"the model chose a paragraph that had a similar topic to the question, but doesn't answer the question. these are challenging errors, often associated with short questions (e.g. \"how does hiv work?\") that provide few keywords. in these cases, discourse features tend to dominate, and shift the focus towards answers that have many discourse structures deemed relevant. for example, for the above question, the model chose a paragraph containing many discourse structures positively correlated with highquality answers, but which describes the origins of hiv instead of how the virus enters a cell. similar words, different topic (8%): the model chose a paragraph that had many of the same words as the question, but is on a different topic. for example, for the question \"how are fossil fuels formed, and why do they contain so much energy?\", the model selected an answer that mentions fossil fuels in a larger discussion of human ecological footprints. here, the matching of both keywords and discourse structures shifted the answer towards a different, incorrect topic."
"biology textbook corpus (bio): this corpus focuses on the domain of cellular biology, and consists of 185 how and 193 why questions handcrafted by a domain expert. each question has one or more gold answers identified in campbell's biology [cit], a popular undergraduate text. the entire biology text (at paragraph granularity) serves as the possible set of answers. note that while our system retrieves answers at paragraph granularity, the expert was not constrained in any way during the annotation process, so gold answers might be smaller than a paragraph or span multiple paragraphs. this complicates evaluation metrics on this dataset (see §5.3)."
"we can further analyze the average revenue per seller as a function of the number of copies by using the statistics in table ii regarding the relationship between the number of copies, the final sale price and the percentage of copies sold. as shown in fig. 6, the seller revenue peaks when the number of copies is around 5 to 6, which is also in accordance with the nash equilibrium seller strategy. we expect that student seller strategies would approach the nash equilibrium seller strategy more in terms of setting both the minimum price and the number of copies per auction with one piece of extra information: tell them before the experiment that empirical data shows that setting the number copies per auction around 6 maximizes seller revenue. this point is subject to further study but clearly shows the power of information and learning in game theory and auction theory to design systems at equilibrium."
"most dmss require the mounting of application programs for real-time system analysis and control in order to perform intelligent operation of responses and outage processes that are interconnected with renewable energy systems that have characteristics of high variance. however, in korea, a single dms is responsible for a number of systems that range from approximately 12 to 150 distribution lines (d/l), which is significantly large [cit] . thus, it is essential to design and implement a database for operational systems with specialized structures on which operations can be conducted and that can input/output necessary data within a short period of time in order to run real-time application programs in such large distribution systems. most studies on database design and implementation for the dms have focused on structure design for running application programs [cit], standardized database design using a common information model (cim) extensive mark-up language (xml) [cit], and a common model of an energy management system (ems) and dms [cit] . however, the related application programs, such as topology process and cim standard-based off-line db, were simple. the common model of ems and dms is not suitable for running real-time application programs since a number of nations (including korea) use a separate operation for ems and dms and the db size becomes excessively large. thus, a new standardized db structure is necessary that can run application programs quickly for real-time system analysis and control."
"lexical semantic features increase performance for all settings, but demonstrate far more utility to the open-domain ya corpus. this disparity is likely due to the difficulty in assembling ls training data at an appropriate level for the biology corpus, contrasted with the relative abundance of large scale open-domain lexical semantic resources. for the ya corpus, where lexical semantics showed the most benefit, simply adding 15 the performance of all models can ultimately be increased by using more sophisticated learning frameworks, and considering more answer candidates in cr (for bio). for example, svms with polynomial kernels of degree two showed approximately half a percent (absolute) performance gain over the linear kernel. however, this came at the expense of an experiment runtime about an order of magnitude larger. experiments with more answer candidates in bio showed similar trends to the results reported. ls features to the cr baseline increases baseline p@1 performance from 19.57 to 26.57, a +36% relative improvement. most importantly, comparing lines 5 and 9 with their respective baselines (lines 2 and 6, respectively) indicates that ls is largely orthogonal to discourse. line 5, the topperforming model with discourse but without ls outperforms the cr baseline by +5.24 absolute p@1 improvement. similarly, line 9, the topperforming model that combines discourse with ls has a +5.69 absolute p@1 improvement over the cr + ls baseline. that this absolute performance increase is nearly identical indicates that ls features are complementary to and additive with the full discourse model. indeed, an analysis of the questions improved by discourse vs. ls (line 5 vs. 6) showed that the intersection of the two sets is low (approximately a third of each set). finally, while the discourse models perform well for how or manner questions, performance on bio why corpus suggests that reason questions are particularly amenable to discourse analysis. relative improvements on why questions reach +38% (without ls) and +24% (with ls), with absolute performance on these non-factoid questions jumping from 28% to nearly 40% p@1. table 2 shows one example where discourse helps boost the correct answer to the top position. in this example, the correct answer contains multiple elaboration relations that are both cross sentence (e.g., between the first two sentences) and intra-sentence (e.g., between the first part of the second sentence and the phrase \"with myelination\"). model features associated with elaboration relations are ranked highly by the learned model. in contrast, the answer preferred by the baseline contains mostly joint relations, which \"represent the lack of a rhetorical relation between the two nuclei\" [cit] and have very small weights in the model."
"from a hardware perspective, it is not difficult to envision extending standard compressive imaging architectures to acquire compressive measurements of a video. for example, the single-pixel camera takes random measurements serially in time. each measurement corresponds to a random linear function of the image on the focal plane at the instant that measurement is collected. when the single-pixel camera is photographing a fixed scene, each measurement corresponds to the same image. when the single-pixel camera is photographing a scene containing motion, each measurement will correspond to a different \"frame\" of the video. the single-pixel camera is capable of taking many thousands of measurements per second. while we will continue to use the single-pixel camera as a specific example in this paper, other compressive imaging architectures (such as a cmos-based transform imager 4 and a coded-aperture imager compressive samples. the sheer volume of data in a raw video signal makes cs reconstruction a formidable task. as we explain in sec. 3, this problem is only exacerbated in compressive video systems that collect streaming measurements, where the number of measured frames can be in the thousands per second. naively reconstructing all of the pixels in all of these frames could literally involve solving for billions of unknowns every second. challenge 2: reconstructing a signal from compressive measurements requires an efficient sparsifying transform and a corresponding algorithm that can promote this sparsity in the reconstructed signal."
"this paper summarized the development and testing of the database in the ksdms, which is a next-generation distribution operational system. the db model proposed in this paper was explained by categorizing the various steps involved: cim-based off-line db, pdb for the db development of the operation server, acm db for the application program execution, and rtdb for real-time server operation and data interconnection with terminals. the problem of the dependence of db on the middleware, as experienced by the existing das, was solved by configuring rtdb and acm and separating a db server while running stable operations due to the completely redundant design."
"the parameters for revenue optimization as a seller in smart are the number of copies of the file, the minimum sale price, and the auction duration. students employed different seller strategies by varying each one of these parameters. a brief summary of student strategies is as follows."
"in the s-mart ecosystem, there are two kinds of content items: (a) positive-valued content items and (b) negativevalued content items. positive-valued content items are regular content items for sale whose values are paid to content producers by content consumers. negative-valued content items are advertisements whose values are paid to content consumers by content producers. an s-mart user can upload both types of content items to s-mart. to upload an ad, a brief description of the ad, an ad budget, number of tokens to be paid per ad view, and the ad type need to be entered. ads can also be targeted to a specific group of users by using s-mart's group formation mechanism. the user interface for uploading ads is shown in fig. 1 ."
"to process the distributed power operated as included in the distribution line, generatingunit model of iec61970 was modified and applied, and facilities that have connection relationships between distributed power and power generation to be connected were modeled to be placed in the lower hierarchy."
"in addition to our preliminary algorithm, 9 several others have also been proposed in the cs video literature that incorporate motion estimation and compensation. these are discussed below. we note that none of these algorithms were explicitly designed to handle streaming measurements. however, any of these algorithms could be modified to (approximately) operate using streaming measurements by partitioning the measurements into short groups and assuming that all measurements within a group come from the same frame. such raw aggregation of measurements actually corresponds to using a rectangular interpolation kernel in our eq. (11) . as one would expect, and as we will demonstrate in sec. 5.2, one can achieve significantly better performance by employing smoother interpolation kernels. to the best of our knowledge we are the first to propose and justify the use of other interpolation kernels for aggregating measurements. we do note that one algorithm from ref. 21 does present a careful method for grouping the measurements in order to minimize the interpolation error when using a rectangular kernel."
"(1) it is a db structure for measurement value acquisition and remote control of the switch for monitoring the distribution lines. thus, the overall db structure is configured on the basis of the switch unit (sw_frtu). (2) the database of the existing das does not provide a table structure for the operation of application programs for real-time analysis and control. input/output data tables for the operation of the application programs, and tables for data sharing between the application programs do not exist. if application program b wants to refer to the result of an operation in application program a, application program b needs to have the duplicate code of application program a. furthermore, the system model data (equivalent power at the transmission side, distributed power, transformer tab model, svr and shunt cap, and svc) for system analysis and control application programs are not defined. figure 3 shows the db characteristic of such an id structure. as indicated in figure 3, the db of the id structure used in the current distribution operation system should check the id of the substation field in the table generator; the db thus performs an id comparison with respect to \"substation 1\" six times and conducts searches with respect to \"substation 2\" six times in total in order to obtain the sum of the generation amount (kw) of the distributed generators that are affiliated with the same substation. the operational unit in the dms in korea is based on a branch office as the minimum unit. one branch office has as many as 10 substations and needs to monitor as many as 150 main d/l, thereby creating as many as 10,000 records per bus in a database. if the number of records as shown in the example of figure 3 increases to several thousand or hundreds of thousands, the time taken for searching records will also increase exponentially, and this is not suitable for the execution of application programs for large systems that require periodical system analysis and control."
"finally, although we have reported results above in terms of the reconstruction quality of the anchor frames themselves, it is worth reiterating that from the estimated anchor framesx a one can, if desired, also obtain an estimate of the frame-by-frame psnr values of high-rate pendulum þ cars video estimate. this video was obtained by interpolating the 32 anchor frames that were reconstructed with our algorithm. four of the reconstructed anchor frames are illustrated in fig. 4(e) . the blue line shows the psnr values of each frame, and the red squares mark the psnr values of each anchor frame. the psnr of the full reconstructed video sequence is 31.31 db, which is only slightly lower than the psnr of the reconstructed anchor frames."
"the body of work on factoid qa is too broad to be discussed here (see, e.g., the trec workshops for an overview). [cit] recently addressed the problem of answer sentence selection and demonstrated that ls models, including recurrent neural network language models (rnnlm), have a higher contribution to overall performance than exploiting syntactic analysis. we extend this work by showing that discourse models coupled with ls achieve the best performance for nf ar. the related work on nf qa is considerably more scarce, but several trends are clear. first, most nf qa approaches tend to use multiple similarity models (information retrieval or alignment) as features in discriminative rerankers [cit] . second, and more relevant to this work, all these approaches focus either on bag-of-word representations or linguistic structures that are restricted to single sentences (e.g., syntactic dependencies, semantic roles, or standalone discourse cue phrases)."
"s-mart's monetary incentives together with user ratings eliminate the free rider problem in file sharing systems. in s-mart, end-users determine prices of their content items in terms of a virtual currency called tokens based on the time and effort needed to produce them. therefore, initial file prices in s-mart reflect subjective evaluations of their producers."
s-mart consists of two main layers: (a) a data plane and (b) a control plane. the data plane can be implemented by using either peer-to-peer (p2p) technology or a classical server-client technology. the control plane is implemented by means of a central secure server. the data plane is responsible for data transfer and storage. the control plane is responsible for lightweight system control related functions. the current implementation uses client-server technology at the data plane. such a two-layered system design is more scalable in terms of potential future innovations.
"some methods for cs video reconstruction have been proposed that do employ a 1-d temporal sparsifying transform along with a 2-d spatial sparsifying transform. in essence, each of these algorithms corresponds to applying a different 3-d sparsifying transform ψ. one algorithm uses the 3d-dwt for ψ and reconstructs the entire video all at once. 1 another approach, termed \" c n \" and proposed for compressive coded aperture video reconstruction, 25 relies on small inter-frame differences together with a spatial 2d-dwt to produce a sparse representation of the underlying video. unfortunately, neither of these algorithms involves any motion compensation, and so these 3-d transforms will not be successful in sparsifying videos containing any significant levels of motion."
"discourse markers: a set of 75 high-frequency 12 [cit] list of cue phrases, and used for feature generation in dmm. these discourse markers are extremely common in the answer corpora -for example, the ya corpus contains an average of 7 markers per answer."
"(1) in the existing das, the memory database was subordinate to the middleware, and the off-line database was located in the operating server; this posed problems of maintenance/repair and server restart for database updating. also, for the addition of a new equipment model, the middleware code should be corrected and recompiled. in response to these problems, two changes were made in ksdms. first, a memory database (rtdb) for measurement information and the alarm processing of field devices, and a memory database (acm) for the implementation of application programs were given independent structures in the middleware."
"we have arrived at a cs problem in which the total number of unknowns has been reduced from mn to mn∕ v þ oð1þ . indeed, the largest dimension of any matrix or vector in this formulation is now limited to ðmnþ∕vþ oð1þ instead of mn. moreover, due to decay in γ, the matrix φγ will be banded, with zeros in all positions sufficiently far from the diagonal. this facilitates storage of the matrix and reconstruction of very long video sequences by breaking the recovery problem into blocks. (however, we consider only reconstructing a single sequence of the video in this paper.)"
"finally, one other related algorithm involves a dual-scale reconstruction of a video. 21 first, a sufficient number of low-resolution measurements are collected to permit a low-resolution preview of the video to be obtained using simple least-squares. motion information is then extracted from this preview video, and this motion information is used to help reconstruct the high-resolution video sequence. an optimization problem minimizes the sum of the l 1 -norm of the expansion coefficients of each individual frame in an appropriate sparsifying transform subject to a data fidelity constraint. additionally, the minimization problem is subject to a constraint such as"
"the reconstruction time of our algorithm depends mainly on the size of the unknown signal. thus, the reconstruction will be faster at coarser scales in the algorithm and slower at finer scales. the reconstruction complexity also depends on the motion vectors, since the time to compute a matrix vector product increases with the number of nonzero motion vectors used in limat. thus, the reconstruction time when using limat with zero-motion vectors will be comparable to the time required for, e.g., the 3d-dwt (which is based on orthonormal linear filters) but shorter than the time required for limat with nonzero motion vectors. using an off-theshelf windows-based pc with quad-core cpu, the total times for our algorithm to reconstruct 16 and 32 anchor frames of the candle video were roughly 30 and 230 min, respectively. to be more specific, for the reconstruction involving 32 anchor frames, the algorithm required 9 s at scale 1552 s at scale 2, and 13,679 s at scale 3. for the sake of comparison, the 3d-dwt, c n, and limat with zeromotion vector algorithms took approximately 2204, 16,106, and 2796 s, respectively. reducing the computational complexity of our algorithm is an important problem for future research. we do note, though, that while our algorithm may not currently be feasible for real-time reconstruction purposes, as with cs-muvi 21 it would be possible to view a low-resolution preview of the video in almost real-time."
"(1) many simple diverged branches occur in the distribution system, and for most of the system analysis, no significant effect is observed, despite the removal of the simple diverged branches. (2) many successive branches are not connected to units (switch units, transformer, etc.) in the distribution system. from the viewpoint of system analysis, the integration of these successive branches is possible, and the line impedance and section load need to be recalculated. (3) in order to analyze the voltage drop and protective coordination settings, terminals (or vulnerable branches) should be excluded from the aforementioned removal and integration."
"from a classical (not compressive) sampling perspective, if we expect that both ω x and ω y will be on the order of π rad∕pix, and if we assume that the 2ω x γ x þ 2ω y γ y term will typically dominate the 2ω h term, then in typical scenarios, to avoid temporal aliasing we should not allow a moving object to traverse more than ≈ 1 2 pix in any direction between adjacent sampling times. aside from exceptionally nonsmooth motions hðtþ, we do strongly suspect that the influence of the temporal bandwidth ω h will be minor in comparison to the influence of the 2ω x γ x þ 2ω y γ y term, and so in general a temporal nyquist sampling rate of 2ðγ x þ γ y þ samples∕s will likely serve as a reasonable rule of thumb. again, this rule of thumb illustrates the direct relationship between the speed of object motion in the video and the video's overall temporal bandwidth. as we will see in sec. 3, this bandwidth will impact the spacing of the anchor frames that we use to reduce the complexity of the cs reconstruction problem."
"even though the optimization problem faced by each student is complex, the experiment was set-up such that it has a simple, symmetric and socially optimal nash equilibrium."
"for a 2-d or 3-d video with limited temporal bandwidth, the separability of the fourier transform implies that the interpolation formulas presented above should hold for each spatial location (i.e., for each pixel). using the videos discussed in sec. 2.3, we have confirmed this in experiments evaluating the quality of interpolation as a function of the video properties, interpolation kernel, etc. again, these experiments are available in a companion technical report. 15"
this was modeled to set up a connection relationship and management structure that can measure corresponding information through frtu/fied while dcp was added to the model.
definition 1: an auction mechanism is said to be incentive compatible if it induces each bidder to submit a bid that sincerely reflects her true value for the item.
"we derive two ls measures from these vectors, which are then are included as features in the reranker. the first is a measure of the overall ls similarity of the question and answer can-didate, which is computed as the cosine similarity between the two composite vectors of the question and the answer candidate. these composite vectors are assembled by summing the vectors for individual question (or answer candidate) words, and re-normalizing this composite vector to unit length. both this overall similarity score, as well as the average pairwise cosine similarity between each word in the question and answer candidate, serve as features."
"finally, we comment on the allocation strategy of the multiscale measurements across the m frames, i.e., which of the m frames we associate with which scale. one straightforward allocation strategy would be a random one, e.g., randomly choose m 1 out of the m original frames and associate these with scale 1, randomly choose m 2 out of the m − m 1 remaining frames and associate these with scale 2, etc. however, we have found that in doing so, it is usually helpful to ensure that all anchor frames at a given scale s are automatically associated with scale s (or a coarser scale). the remaining (nonanchor) frames are associated randomly."
"we would like to point out, however, that limat is just one of many possible 3-d sparsifying bases and that our algorithm can be modified to use other choices of basis as well. we also note that the frame-by-frame reconstruction methods [cit] have the advantage of being more computationally efficient as they involve reconstruction problems of a smaller size. this may be an important factor in applications where computational resources are scarce and full temporal decompositions are not practical. we also reiterate that to some degree, the multiscale algorithm we present in this paper is a proof-of-concept inspired by our temporal bandwidth analysis. we see our work as an addition to-not a replacement for-the nascent cs video literature, and we believe that the ideas we expound (such as using anchor frames to reduce the complexity of reconstruction) could be combined with some of the other existing ideas mentioned above."
"finally, to place our work in the proper context, we reiterate the differences between a standard (\"noncompressive\") video capture system and the cs-based (\"compressive\") video acquisition strategy discussed in this paper. as we discussed in sec. 1, the primary advantages of a compressive video system are twofold: first, it reduces the physical burden of measuring the incoming video, as it does not require a complete set of samples to be obtained for every frame, and second, it reduces the computational complexity of the encoding process, as it does not require any spatiotemporal transform to be implemented at the encoder. these advantages do not make a compressive video system appropriate for all situations, however. for example, it is well known in cs that for a given sparse signal, the requisite number of measurements is slightly higher (by a small multiplicative factor) than one would require if the relevant sparse components could be directly extracted from the signal. standard video capture systems have the advantage of getting to \"look at\" the fully sampled video before identifying the critical features to be encoded. one would naturally expect, then, that for a given quality level, a traditional video encoder would require fewer bits than a compressive video encoder (one in which the cs measurements were quantized and converted to bits). standard video capture systems also have a second main advantage: the decoding algorithm can be much simpler than in a compressive video system. our comments on these points are not unique to our particular cs framework; indeed they are fully consistent with observations made in another recent paper dealing with cs for video. 26 (the encoder proposed in ref. 26 also has some philosophical similarities and differences with our work and others we have described. like our work, the system in ref. 26 can operate with frame-by-frame cs measurements and is intended to exploit spatial and temporal correlations in the video. unlike our work, however, the system in ref. 26 utilizes a special post-measurement encoding of the cs measurements to exploit temporal correlations. it also relies on raw aggregation (rectangular interpolation) of the measurements.) for our algorithm, however, a detailed comparison against a traditional encoder in terms of bit rate, power consumption, memory requirements, resiliency to errors, etc., is beyond the scope of this paper."
"the difficulty of long-term power supply planning as evidenced by the recent rotating power outages in korea as well as the regulations on co 2 emission around the world have encouraged the introduction of power systems using alternative energy, including renewable energy. moreover, smart grids, which have been important in the field of power systems in recent years, have motivated the gathering of new information for the operation of power systems. therefore, we believe that a distribution system with which small and medium-sized renewable energy systems can be interconnected should provide a stable and reliable power supply as it would alleviate the irregular output variance of renewable energy. the introduction of such a distribution system for renewable energy increases the instability of the system operation, and a number of application programs are therefore required for the accurate understanding and control of the system status."
"while an inherent function of the middleware (data interface between a server and nodes) was intact, the function of db creation was carried out using the application service interface (asi) and the configuration file. accordingly, an off-line db was separated and all information required for memory db creation was extracted and applied as a form of pdb; a configuration file was updated and stored in a disk array. through this arrangement, db update in the operation server can be carried out without interruption, while new rtu models can be added automatically through a configuration update (db update). second, the offline database was moved to a separate database server to enable an uninterrupted operation, irrespective of the operation server problems or database updating. (2) acm db was developed to run the application programs. in this way, db, rtdb, and acm can be divided into run real-time system analysis and control application programs, while placing little burden on the field data measurement and transfer, in order to perform the scada function of the existing das. (3) the off-line db was constructed on the basis of an international standard, iec61970 (cim), to prepare for future compatibility. a tool called k-builder was developed for engineering the off-line db, of which the cim db and system line drawing could be edited. table 1 summarizes the data model of the ksdms db. as shown in table 1, the cim db structure was configured following the code of the cim standard. as can be seen from the top of table 1, the starting point of each d/l has a generator that is equivalent to the transmission system followed by the main transformer and the lead point switch. the acm model represents the relationship between the cim terminal and the connectivity node according to the relationship between the facility and nodes. furthermore, a switch for the measurement of equivalent power (p and q voltage introduced to the transmission end) is added. the modeling of facilities used by the application program in the acm db, which is indicated at the bottom of table 1, has the following characteristics:"
"as mentioned previously, the operational unit in the ksdms is based on a branch office as the minimum unit. one branch office has as many as 10 substations, and as many as 150 main distribution lines can be included. in the distribution system, because many simple diverged branches or connected lines are found, the database can become excessively large and the processing can be considerably time-consuming for the application programs if the lines are processed using a topology model. furthermore, the singularity of an impedance characteristic matrix (y matrix, gain matrix) can become large, thereby creating a problem, such as during operations of an inverse matrix. therefore, in this paper, we proposed a system reduction model, the concept of which is as follows:"
"s-mart's incentive mechanism consists of monetary incentives and user ratings. monetary incentives eliminate freeriders, and the currency-based system provides anyoneanytime file trading capabilities for users. user ratings deter users from uploading corrupted or junk files."
"(1) all measurements in the distribution system can be measured that can only be performed by switching to the attached rtu that can transmit the measurement remotely (generally called automated switch). (2) the voltage in a node is either a measured value in a node, where an automated switch is attached, or the estimated value of the load flow and status estimation. step tap : tap step (p.u.)."
"the world has witnessed tremendous growth in both communication and social networks over the last decade. nowadays many people around the globe, from teenagers to elderly people and terrorists, are ubiquitously connected both physically and virtually. the viral growth of social networks is particularly impressive: facebook currently has more than 150 million active users and has an average of 250, 000 new registrants per day, with an average of 3% [cit] . this puts the networks, either social in virtual online space or technology networks among communication devices, at the center of our society. therefore, it is of particular importance to understand and reveal under-explored functional and topological interactions between social and technological networks."
"s-mart is more than simply a new approach to file sharing. it also serves as a powerful testbed to perform controlled experiments with human subjects to better understand human 978-1-4244-6826-3/10/$26.00 ©2010 ieee behavior in exchanging content under the guidance of pricing signals, and other sociologically complex phenomena such as time evolution of the popularity of a content item or a user. to illustrate its usage as a testbed, we designed a toy auction based competition among seven graduate students in the electrical engineering department at princeton university. the purpose of this auction experiment is to understand strategic behavior (or the lack thereof) of users in online auctions. we solicited students to submit buyer and seller strategies to trade files through s-mart. then, the submitted strategies competed against each other to garner the maximum number of points, which points were a combination of total seller revenue and the number of auctions won. we observed that the revenue earned by selling content and the probability of winning an auction increase as seller and buyer strategies approach the socially optimal nash equilibrium of the designed experiment. further details of the experiment and initial results are provided in section iii."
"we next quantify the effects of the minimum price on the final seller revenue. we first observe that the average minimum price over all 21 auctions was 96.67 tokens, and the median minimum price over the three best seller strategies was 95 tokens, which are close to the numbers suggested by the nash equilibrium in theorem 2. secondly, we observe that a high minimum price decreases the percentage of copies sold (i.e., all copies are sold in all auctions if the minimum price is less than 80 tokens but none of the auctions have all copies sold if the minimum price is greater than 100 tokens); a higher minimum price increases the final price in all auctions, and also increases the total seller revenue in 85.71% of all auctions. empirical data also shows that a higher auction duration increases the final file sale price. the average increase in the final price is 20.85% of the minimum price if the auction duration is between 64 − 96 hours. this decreases to 1.58% and 1.39% for auctions whose durations are between 32 − 64 hours and 0 − 32 hours, respectively."
the following hyper parameters were tuned using grid search to maximize p@1 on each development partition: (a) the segment matching thresholds that determine the minimum cosine similarity between an answer segment and a question for the segment to be labeled qseg; and (b) 12 we selected all cue phrases with more than 100 occurrences in the brown corpus.
"in order to eliminate random factors (e.g., forgetting to bid for an auction) in bidding, we developed a java library to enable automated bidding for auctions. in addition, it is hard for a student to remember all parameters of 18 ongoing auctions and to bid on time manually. java classes developed and their functions are shown in table i . students use these classes to program their automated bidding agents. these bidding agents run on students' computers, connect to the smart server, retrieve auction information to decide on a bid amount, and then submit the bids as shown in fig. 4 ."
"the discourse parser model (dpm) is based on the rst discourse framework [cit] . in rst, the text is segmented into a sequence of non-overlapping fragments called elementary discourse units (edus), and binary discourse relations recursively connect neighboring units. most relations are hypotactic, where one of the units in the relation (the nucleus) is considered more important than the other (the satellite). a few relations are paratactic, where both participants have equal importance. in the bottom part of figure 2, we show hypotactic relations as directed arrows, from the nucleus to the satellite. [cit] ."
"this was modeled to enable the use of different categories of measurement with the point configuration of the currently available distribution frtu, and the points are divided as follows:"
"after we have reconstructed an estimate of x a;1, it is then possible to compute a preliminary estimate of the motion in the video. although the resulting motion vectors will have limited spatial and temporal accuracy (since x a;1 does), they can be used to perform a motion-compensated reconstruction of x a;2 . (typically we choose s small enough so that the frames in x a;1 contain enough pixels to obtain a reasonable estimate of the motion vectors.) from this point, we iterate (as detailed below), alternating between video reconstruction and motion estimation, and proceeding to finer and finer scales. between each pair of adjacent scales, we double the spatial resolution and (as suggested by our analysis) double the temporal resolution as well."
"same subsection (50%): in these cases, the model selected an on-topic answer paragraph in the same subsection of the textbook as a gold answer. often times this paragraph directly preceded or followed the gold answer."
"theorem 2: the above experiment has a symmetric and socially optimal nash equilibrium at which m proof: given the seller strategies in the theorem, file prices stay constant at 100 tokens since there are 6 students competing for each auction. therefore, a student can win all auctions by bidding 100 tokens for each of them, which is the best s/he can do as a buyer. similarly, given the bidding strategies of the others, a student can sell 18 copies of three different files at three different auctions by setting the number of copies per auction to 6 and the minimum sale price to 100, which is the best a student can do as a seller."
"discourse trees: [cit] . for ya, we parsed entire answers. for bio, we parsed individual paragraphs. note that, because these domains are considerably different from the rst treebank, the parser fails to produce a tree on a large number of answer candidates: 6.2% for ya, and 41.1% for bio. in these situations, we constructed artificial discourse trees using a rightattachment heuristic and a single relation label x."
"we see that the two classes of methods described above have two distinct strengths. algorithms in the first class employ motion information in the reconstruction to better reconstruct the video sequence, while algorithms in the second class employ a temporal transformation to remove the temporal redundancies over a longer temporal support. in this paper (and in ref. 9), we use limat, which essentially combines these two strengths: limat performs a full motioncompensated temporal decomposition that, when seeded with accurate motion vectors, can effectively remove temporal redundancies even in the presence of complex motion."
"content consumers are the users who cannot contribute interesting popular content to the system. we support such users by means of start-up funds, the ad mechanism, the token rewarding mechanism and the secondary content market mechanism, as mentioned above."
"we briefly mention our empirical findings regarding buyer strategies. as already observed in the literature before (see [cit] ), many experienced bidders in ebay use the sniping strategy (i.e., bid at the very last minutes). the three students with the highest success rates snipe within the last 60 seconds of auctions. their average success rate is 96.87%. that is, they win 96.87% of all auctions in which they participated. furthermore, average number of bids per auction from the three most successful bidder strategies is 1.316, which is in compliance with the incentive compatibility property in theorem 1: just bid once with your true value. other students employ continues bidding strategy, and withdraw when the current price reaches to their true value of the file. their success rate reduces to 61.87%, and they bid 8.625 times per auction."
"we would like to point out that all of the above methods require, for each frame, a number of measurements proportional to the sparsity level of that frame (after appropriate motion compensation and a spatial sparsifying transform such as a 2-d wavelet transform). this is true simply because three of the above methods [cit] involve reconstructing the video one or two frames at a time. the fourth of the above methods 21 does involve jointly reconstructing the ensemble of video frames. however, this algorithm still requires a total number of measurements proportional to the sum of the sparsity levels of the individual frames because that quantity is what is minimized in the l 1 optimization procedure."
"it is important to note that these discourse features are more expressive than features based on discourse markers alone [cit] . first, figure 2 : top: example feature generation for the discourse marker model, for one question (q) and one answer candidate (ac). answer candidates are searched for discourse markers (italic) and question word matches (bold), which are used to generate features both within-sentence (sr0), and ±1 sentence (sr1). the actual dmm exhaustively generates features for all markers and all sentence ranges. here we show just a few for brevity. bottom: example feature generation for the discourse parser model using the output of an actual discourse parser. the dpm creates one feature for each individual discourse relation."
"we conclude this examination with our own series of experiments on real-world videos. these videos (courtesy of merl) were collected in a laboratory setting using a highspeed video camera, but the scenes being imaged contained natural (not particularly high-speed) motions. for each video, we select a 2-d \"slice\" of the 3-d video cube, extracting one spatial dimension and one temporal dimension."
"note that our marker arguments are akin to edus in rst, but, in this shallow representation, they are simply constructed around discourse markers and bound by an arbitrary sentence range."
"another related algorithm 24 involves dividing a video sequence into several groups of pictures (gops), each of which is made up of a key frame followed by several nonkey frames. the authors propose to reconstruct each key frame in a frame-by-frame fashion. given the estimates of the key frames, estimates of the nonkey frames are computed via a technique called motion-compensated interpolation. refined estimates of the nonkey frames are obtained in a frame-by-frame fashion, using the initial motion-compensated frame as the starting point of a gradient projection for sparse reconstruction (gpsr) solver. the authors propose a novel stopping criterion for the gpsr solver that helps find a solution that is not too different from the initial estimate. this procedure is then carried out for the next nonkey frame, and so on."
"an incentive compatible auction is also an efficient (i.e., items are allocated to those who most value it) and standard (i.e., items are allocated to the highest bidders) auction. ) . an alternative for s-mart auctions is discriminatory multiunit auctions (i.e., the highest m bidders get the file by paying their bids) however, the revenue equivalence principle (see [cit] ) states that the seller revenue tends to stay the same for different auctions when some mild conditions are met. therefore, we have decided to implement s-mart auctions in the current operational version of the s-mart system. content producers selling their files via s-mart auctions now face a complex stochastic revenue maximization problem over the optimization variables such as the number of copies of a file to be sold, minimum asking price and the auction duration. this optimization problem will be subject to our auction experiment in the next section."
"3. we show that discourse-based qa models using inter-sentence features considerably out-perform single-sentence models when answers span multiple sentences. 4. we demonstrate good domain transfer performance between these corpora, suggesting that answer discourse structures are largely independent of domain, and thus broadly applicable to nf qa."
"we argue that a temporal sparsifying transformation can help to decrease the sparsity level of a video signal and thus reduce the number of measurements that must be collected of that video. in particular, for videos with slowly moving objects or videos with stationary backgrounds, temporal redundancies may persist for longer than the duration of one or two frames. the above methods, however, are essentially employing a temporal transform with a temporal support of one or two frames. if one can successfully remove temporal redundancies over a longer temporal support, the overall sparsity level of the video will decrease, and this in turn will reduce the number of measurements that must be collected from each frame."
"the results of the transferred models that include ls features are slightly lower, but still approach statistical significance for p@1 and are significant for mrr. we hypothesize that the limited transfer observed for models with ls compared to their counterparts without ls is due to the disparity in the size and utility of the biology ls training data compared to the open-domain ls resources. the open-domain ya model learns to place more weight on ls features, which are unable to provide the same utility in the biology domain."
"therefore, we see that filtering such a video in the spatial directions can cause it to be essentially bandlimited both in space and in time."
"in this paper, we offer suggestions for confronting both of these challenges. we begin in sec. 2 with a short discussion concerning the temporal bandwidth of video signals. we argue analytically that, at the imaging sensor, many videos should have limited temporal bandwidth due to the spatial lowpass filtering that is inherent in typical imaging systems. under modest assumptions about the motion of objects in the scene, this spatial filtering prevents the temporal complexity of the video from being arbitrarily high."
(1) cope with the uncertainty of distributed power and automation of fault recovery through mounting of real-time system analysis and control application programs. (2) support for new operators using the result of the application programs. (3) increase in the compatibility of db structure through the use of standardized system structures.
"the step-by-step procedure for the system reduction method is explained below using the example shown in figure 8 . the black points in figure 8 indicate nodes, whereas the white points indicate a terminal branch for the protective coordination setting. figure 5c, and impedance is recalculated. however, terminal branches are excluded from the removal. (step 4) step 3 is iterated until all simple successive branches are integrated."
"to support these conjectures, we have identified the characteristic \"butterfly shape\" in experiments where gðxþ is not bandlimited (e.g., a gaussian bump convolved with the unitstep function), where hðtþ is not bandlimited (e.g., a triangle wave), where there are multiple moving edges, and where there are occluding objects. again, these experiments are available in a companion technical report. 15 in general, the estimated spectra continue to follow the predicted butterfly shape. however, it remains an open problem to support this with theoretical analysis. we do note that we believe our results are largely consistent with the classical study by dong and atick concerning the statistics of real-world videos. 16 although the videos in that study had relatively low temporal resolution, dong and atick did note a certain radial symmetry to the spectrum (with one term depending on ω t ∕ω x ) and observe that at low spatial frequencies the power spectrum will have a strong decay as a function of the temporal frequency."
"for ya, we used the standard implementations for p@1 and mean reciprocal rank (mrr) [cit] . in the bio corpus, because answer candidates are not guaranteed to match gold annotations exactly, these metrics do not immediately apply. we adapted them to this dataset by weighing each answer by its overlap with gold answers, where overlap is measured as the highest f1 score between the candidate and a gold answer. thus, p@1 reduces to this f1 score for the top answer. for mrr, we used the rank of the candidate with the highest overlap score, weighed by the inverse of the rank. for example, if the best answer for a question appears at rank 2 with an f1 score of 0.3, the corresponding mrr score is 0.3/2. more specifically, dmm and dpm show similar performance benefits when used individually, but their combination generally outperforms the individual models, illustrating the fact that the two models capture related but different discourse information. this is a motivating result for discourse analysis, especially considering that the discourse parser was trained on a domain different from the corpora used here."
"s-mart allows content producers to design their content stores, and trade different digital rights at different prices by means of different transaction styles. we will mention the details of s-mart's auction mechanism in section iii. content producers can also earn tokens by responding to content requests."
"relying on a proper discourse framework facilitates the modeling of the numerous implicit relations that are not driven by discourse markers (see ch. 21 [cit] ). however, this also introduces noise because discourse analysis is a complex task and discourse parsers are not perfect. to mitigate this, we used a simple feature generation strategy, which creates one feature for each individual discourse relation by concatenating the relation type with the labels of the discourse units participating in it. to this end, for every relation, we extract the entire text dominated by each of its arguments, and we generate labels for the two participants in the relation using the same strategy as the dmm (based on the similarity with the question content). similar to the dmm, these features take real values obtained by averaging the cosine similarity of the arguments with the question content. 9 fig. 2 shows several such features, created around two rst elaboration relations, indicating that the latter sentences expand on the information at the beginning of the answer. other common relations include attribution, contrast, background, and evaluation."
"in cases where the ksdms using a real-time database as proposed in this paper is applied to real-system operations, various issues can arise, such as the increase of the establishment cost for the distribution control center, the operator's training to teach how to operate the new system, the development of the related hmis, and the agenda for the maintenance/repair procedure of the new system. however, the introduction of the proposed system can result in the following operational advantages. first, real-time analyses and control operations are possible to enable adaptation to new equipment, such as integrating renewable energy resources, electric vehicles, and advanced metering infrastructure (ami), which should be introduced into the electric power distribution system in the near future. second, various application programs can be loaded into the system to support the operational tasks and decision-making of new operators who lack experience, and to enable the system operation using these support programs. third, the interoperability of the system can be ensured, which substantially facilitates the application of new field devices. a recent trend in the field of power distribution networks leads us to believe that the additional burden will be easily outweighed by the abovementioned advantages that have been made available by the introduction of the new operating system; the establishment of a real-time database as proposed in the present paper is considered as an essential technique of this process."
"in particular, we suggest that fðx; tþ could have limited temporal bandwidth in plausible scenarios where the prototype frame gðxþ and translation signal hðtþ have limited complexity. for example, in a physical imaging system, one may envision fðx; tþ as the video that exists at the focal plane prior to being sampled by the imaging sensor. it is reasonable to expect that, due to optical blurring and due to the implicit filtering that occurs from the spatial extent of each light integrator, the prototype frame gðxþ will have limited spatial bandwidth. similarly, if the camera motion is constrained or due to the physics governing the movement of objects in the scene, one might expect that the translation signal hðtþ will have limited slope and/or limited temporal bandwidth. in the sections that follow, we explain how such scenarios can allow us to bound the approximate temporal bandwidth of fðx; tþ."
"driven by several international evaluations and workshops such as the text retrieval conference (trec) 1 and the cross language evaluation forum (clef), 2 the task of question answering (qa) has received considerable attention. however, most of this effort has focused on factoid questions rather than more complex non-factoid (nf) questions, such as manner, reason, or causation questions. moreover, the vast majority of qa models explore only local linguistic structures, such as syntactic dependencies or semantic role frames, which are generally restricted to individual sentences. this is problematic for nf qa, where questions are answered not by atomic facts, but by larger cross-sentence conceptual structures that convey the desired answers. thus, to answer nf questions, one needs a model of what these answer structures look like."
"driven by this observation, our main hypothesis is that the discourse structure of nf answers provides complementary information to state-ofthe-art qa models that measure the similarity (either lexical and/or semantic) between question and answer. we propose a novel answer reranking (ar) model that combines lexical semantics (ls) with discourse information, driven by two representations of discourse: a shallow representation centered around discourse markers and surface text information, and a deep one based on the rhetorical structure theory (rst) discourse framework [cit] . to the best of our knowledge, this work is the first to systematically explore within-and cross-sentence structured discourse features for nf ar. the contributions of this work are:"
"in this section, we explain the details of the selected properties of the proposed virtual money based file sharing system. we start with a description of the ramping up of the initial user activity in s-mart."
"the emerging theory of compressive sensing (cs) has inspired a number of efficient new designs for signal acquisition in general and imaging in particular. architectures such as the \"single-pixel camera\" 1,2 provide a promising proof-of-concept that still images can be acquired using small numbers of randomized measurements. despite the apparently incomplete data collected by such devices, reconstruction of the signal can be accomplished by employing a sparse model specifying, for example, that a high-dimensional image may have only a small number of significant coefficients when expanded in the two-dimensional (2-d) wavelet domain. 3 there are numerous applications where it could be helpful to extend the cs imaging framework beyond still images to incorporate video. standard video capture systems require a complete set of samples to be obtained for each frame, at which point a compression algorithm may be applied to exploit spatial and temporal redundancy. in some applications, such as imaging at nonvisible (e.g., infrared) wavelengths, it may be difficult or expensive to obtain these raw samples. in other applications, it could be computationally challenging to implement a state-of-the-art video compression algorithm at the sensor. we argue that these burdens may be reduced by using compressive imaging hardware where random measurements are collected independently from each frame in the video and no addition al compression protocol is needed."
"our experiment was a 96-hour agent-based competition among seven graduate students at princeton as a part of a homework problem set. our subjects were given a lecture on game theory and auction theory before the experiment. therefore, they were familiar with strategic thinking in a game setting."
"some other functions of s-mart are group formation, internal messaging, blogging and detailed reports about user statistics. further details about them can be found in the smart user manual [cit] ."
"here, 94 of the 378 bio how and why questions have improved answer scores, while 36 have reduced performance relative to the cr baseline. of these 36 questions where answer scores decreased, nearly two thirds were directly related to the paragraph granularity of the candidate answer retrieval (see §5.1):"
"from carson's bandwidth rule for frequency modulation, 14 we have that for fixed ω x, at least 98% of the total power of e −jω x hðtþ must be concentrated in the frequency range"
in this section we present simulation results for our proposed algorithm. we compare our algorithm (using a linear interpolation kernel) to the 3d-dwt 1 and c n 25 temporal sparsifying transforms using a rectangular interpolation kernel. we also compare to a modified version of our algorithm that uses limat but with zero-motion vectors. the results demonstrate the benefits of combining motion compensation with a temporal sparsifying transform and the benefits of using a nonrectangular interpolation kernel.
"cqa: in this scenario, the task is defined as reranking all the user-posted answers for a particular question to boost the community-selected best answer to the top position. this is a commonly used setup in the cqa community [cit] . 4 thus, for a given question, all its answers are fetched from the answer collection, and an initial ranking is constructed based on the cosine similarity between theirs and the question's lemma vector representations, with lemmas weighted using tf.idf (ch. 6, [cit] )."
"we performed an error analysis of the full qa model (cr + ls + dmm + dpm) across the entire bio corpus (lines 17 and 25 from table 1 ). we chose the bio setup for this analysis because it is more complex than the cqa one: here gold answers may have a granularity completely different from what the system choses as best answers (in our particular case, the qa system is currently limited to answers consisting of single paragraphs, whereas gold answers may be of any size)."
"the rtdb simply consists of the address information of data points and their values as well as the mapping information of the off-line database. the detailed structure of the off-line database is shown in figure 2 . figure 2 shows the most important parts of the conventional das db. p and f inside the circles in figure 2 refer to the primary key and foreign key, respectively. as shown in figure 2, the database is a relational database structure consisting of the hierarchical structure of a substation (sub), main transformer (mtr_bank), distribution line (dl), and switch unit (sw_frtu). in the center of the overall structure, a switch unit is found and the distribution line (dl) has the identity (id) of a starting point switch unit (d/l lead wire circuit breaker (cb)). the switch unit is associated with different measurement tables (sw_mea_ty1, ..., sw_mea_tyn) depending on their types."
"answer window size (14%): here, both the cr and full model chose a paragraph containing a different gold answer. however, as discussed, gold answers may unevenly straddle paragraph boundaries, and the paragraph chosen by the model happened to have a somewhat lower overlap with its gold answer than the one chosen by the baseline."
"complexity the insight we have developed in sec. 2 suggests that many videos of interest may indeed be exactly or approximately bandlimited in the temporal direction. for problems involving cs of such videos, this implies that there may be a limit to the \"complexity\" of the information collected by streaming compressive measurement devices. one way of exploiting this limited complexity draws from classical interpolation identities for bandlimited signals. we briefly review these identities in sec. 3.1 before exploring their applications for cs reconstruction in sec. 3.2."
"(1) increase in uncertainty due to the introduced system's increase in distributed power, including renewable energy. such problems are no exception in korea; [cit] to develop korea's next-generation distribution operation system [cit] . during the ksdms development, we differentiated our system by defining it as a distribution management system that can be helpful for operators' decision making by mounting various application programs from the existing das; we call this system a monitoring system for distribution (scada for distribution). the aim of the system developed for this purpose was to achieve the following objectives:"
"to test the generality of these features, we performed a transfer study where the full model was trained and tuned on the open-domain ya corpus, then evaluated as is on bio how. this is 16 the results are summarized in table 5 . the transferred models always outperform the baselines, but only the ensemble model's improvement is statistically significant. this confirms existing evidence that ensemble models perform better cross-domain because they overfit less [cit] . the ensemble model without ls (third line) has a nearly identical p@1 score as the equivalent in-domain model (line 13 in table 1 ), while slightly surpassing indomain mrr performance. to the best of our knowledge, this is one of the most striking demonstrations of domain transfer in answer ranking for non-factoid qa, and highlights the generality of these discourse features in identifying answer structures across domains and genres."
"to test this, we use the ya corpus, which has the best-performing ls model. because we are adding two new discourse models, we now tune four segment matching thresholds, one for each of the dmm, dpm, dmm ls, and dpm ls models. 17 the results are shown in table 6 . these results demonstrate that incorporating ls in the discourse models further increases performance for all configurations, nearly doubling the relative performance benefits over models that do not integrate ls and discourse (compare with lines 6-9 of table 1 ). for example, the last model in the table, which combines four discourse representations, improves p@1 by 24%, whereas the equivalent model without this integration (line 9 in table 1) outperforms the baseline by only 15%."
"again, to be clear, the paragraphs above concern the implications of our analysis in classical (not compressive) sampling of a video. as we discuss in sec. 3, the streaming compressive measurements produced by a single-pixel camera may actually need to be acquired much faster than the video's temporal nyquist rate (because only one measurement is collected from each frame). the specific reason that the temporal bandwidth of the video will be relevant is because it will impact the spacing of the \"anchor frames\" that we use to reduce the complexity of the reconstruction problem."
"because these discourse models appear to capture high-level information about answer structures, we hypothesize that the models should make use of many of the same discourse features, even when training on data from different domains. table 4 shows that of the highest-weighted svm features learned when training models for how questions on ya and bio, many are shared (e.g., 56.5% of the features in the top half of both dpms are shared), suggesting that a core set of discourse features may be of utility across domains."
"yahoo! answers corpus (ya): yahoo! answers 10 is an open domain community-generated qa site, with questions and answers that span formal and precise to informal and ambiguous language. due to the speed limitations of the discourse parser, we randomly drew 10,000 [cit] using their filtering criteria, with the additional criterion that answers had to contain at least four community-generated answers, one of which was voted as the top answer. the number of answers to each question ranged from 4 to over 50, with the average 9. 11"
"the off-line db in the ksdms was developed in compliance with the iec61970 standards. since the cim standard did not include all information required for the operation of the distribution system, the iec61970 package was modified to suit the required information, whereas the line and transformer model given in the wiresext package of iec61968 was also adopted to take into consideration the distribution line and mtr as well as svr. the modified cim model is summarized in table 2 ."
"this work focuses on two important aspects of answer reranking for non-factoid qa: similarity between question and answer content, and answer structure. while the former has been addressed with a variety of lexical-semantic models, the latter has received little attention. here we show how to model answer structures using discourse and how to integrate the two aspects into a holistic framework. empirically we show that modeling answer discourse structures is complementary to modeling lexical semantic similarity and that the best performance is obtained when they are tightly integrated. we evaluate the proposed approach on multiple genres and question types and obtain benefits of up to 24% relative improvement over a strong baseline that combines information retrieval and lexical semantics. we further demonstrate that answer discourse structures are largely independent of domain and transfer well, even between radically different datasets."
"in this section of the paper, we report empirical findings about the auction experiment. we start our analysis with seller strategies, and observe that seller revenues of students are maximized as their seller strategies approach the nash equilibrium seller strategy in theorem 2."
"thus, loads were modeled using the total load between automated switches, while it was assumed that the application program estimated the value using the measured value obtained in a switch."
"where hðtþ is some function that controls how much (in pix) the prototype frame is shifted in the focal plane at each time step. because we have an interest in video imaging systems with high temporal sampling rates, our purpose is to characterize the temporal bandwidth of the video fðx; tþ."
"so far, we have treated ls and discourse as distinct features in the reranking model, however, given that ls features greatly improve the cr baseline, we hypothesize that a natural extension to the discourse models would be to make use of ls similarity (in addition to the traditional information retrieval similarity) to label discourse segments. for example, for the question \"how do cells replicate?\", answer discourse segments containing ls associates of cell and replicate, e.g., nucleus, membrane, genetic, and duplicate, should be considered as related to the question (i.e., be labeled qseg). we implemented two such models, denoted dmm ls and dpm ls, by replacing the component that assigns argument labels with one that relies on ls. specifically, as in §4.3, we compute the cosine similarity between the composite ls vectors of the question text and each marker argument (in dmm) or edu (in dpm), and label the corresponding answer segment qseg if this score is higher than a threshold, or other otherwise. this way, the dmm and dpm features jointly capture discourse structures and semantic similarity between answer segments and question."
"in this paper, we summarize the development and demonstration of a database for the korean smart dms (ksdms) that was recently developed in korea. the developed database structure consists of a cim-based off-line db, a physical db (pdb), and a real-time db (rtdb) for a real-time server operation and remote terminal unit data interconnection, and an application common model (acm) db for running application programs. the proposed database model was implemented and tested at the gochaing and jeju offices using a real system. through data measurement on remote terminal units and the operation and control of the application programs using the measurement, the performance speed and integrity of the proposed database model were validated, thereby demonstrating that this model can be applied to real systems. figure 1 shows the interrelationship between the databases in the existing das in korea [cit] . as shown in figure 1, real-time information (mainly switching information) acquired from the remote terminal unit (rtu) is stored in the rtdb of the middleware and is then stored in the off-line database. the rtdb is included in the middleware. this off-line database is managed by ms-sql."
"argument labels: we label marker arguments based on their similarity to question content. if text before or after a marker out to a given sentence range matches the entire text of the question (with a cosine similarity score larger than a threshold), that argument takes on the label qseg, or other otherwise. in this way the features are only partially lexicalized with the discourse markers. argument labels indicate only if lemmas from the question were found in a discourse structure present in an answer candidate, and do not speak to the specific lemmas that were found. we show in §5 that these lightly lexicalized features perform well in domain and transfer between domains. we explore other argument labeling strategies in §5.7."
"the argument sequences used here capture crosssentence discourse structures. second, these features model the intensity of the match between the text surrounding the discourse structure and the question text using both the assigned argument labels and the feature values."
"rather than reconstructing the full set of high-resolution anchor frames x a directly, our algorithm aims to reconstruct a sequence of approximations to the anchor frames. these approximations begin at a low temporal and spatial resolution and progress to successively finer resolutions."
all equipment is inherited from identifiedobject and modeled as it is divided by electrical connection relationships. iec61970 bay was modified into distributionline to place distribution facilities at distributionline in compliance with the current distribution management system.
"(6) if the database hierarchy needs to be separated, the facilities should be separated over both ends in a specific hierarchy. in the proposed db structure, the switches over both ends were modeled as a tie switch and assigned to the highest branch office. (7) the structure was modeled with an assumption that a generator (distributed generator) has an automated switch that can always measure an electricity generation amount. it was assumed that a very small distributed generator may not have an automated switch. (8) a parallel device (shunt capacitor/reactor) was modeled under the condition of switch input open. (9) svc, the same as that in the generator, was modeled assuming the presence of automated switches that can measure a reactive power amount. (10) in the case of the load, it was assumed that individual load measurement cannot be performed."
"produce an invertible transform between the original video and the lowpass fl k g and highpass fh k g coefficients. for maximum compression, the lifting steps can be iterated on pairs of the lowpass frames until there remains only one. ideally, with perfect motion compensation, the n − 1 highpass frames will consist only of zeros, leaving only one frame of nonzero lowpass coefficients, and making the sequence significantly more compressible. as a final step, it is customary to apply the 2-d discrete wavelet transform (dwt) to each lowpass and highpass frame to exploit any remaining spatial correlations."
"next, we consider the pendulum þ cars video, featuring two translating cars and an occlusion as one car passes in front of the other; the video was acquired at a rate of 250 frames per second. we select 4096 consecutive time samples from the video, and we extract 640 pixels from a certain row in each frame. figure 2 (e) shows the video, and fig. 2(f) shows the estimated spectrum. once again, we recognize the approximate butterfly shape and an approximate limitation to the video bandwidth, both spatially and temporally, and once again, we see a collection of lines with various slopes, but with a bit of \"thickness\" near the origin due to a possible ω h term. the slopes of these lines match what might be expected based on an empirical examination of the video itself. for example, the maximum slope appears to be on the order of 140 pix∕s, while the maximum translational speed of the cars appears to be on the order of 70 pix∕s. the overall empirical temporal bandwidth is less than 35 rad∕s, and consequently, this video's temporal sampling rate (250 frames∕s) may also have been higher than necessary."
the relationship structure of the database used in the ksdms is shown in figure 4 . the following paragraphs explain the difference in structure between the ksdms and the existing das.
"with a virtual rp, the number of the value 0 in 0-1 sketch or the mean value in gaussian sketch is almost equal to the number of aps heard in the real scenes. since, normally, the distance between its two neighbor rps is much smaller than ap's communication distance, and all three rps are in the similar scenario at the same time. hence, all complete sketches are trimmed with the uniform dropout rate."
"firstly, the fingerprint database, which is consisted of rsss and their corresponding information, such as preset rps' coordinates, sensor devices, los/nlos, collecting time, etc., is established in order sequence vectors from different aps in a specific indoor environment, and its corresponding variables are defined in table 1 ."
"moreover, considering that known lncrna-disease associations are very sparse, which may cause that there exist some lncrnas with no associations with any diseases, or some diseases with no associations with any lncrnas. hence, some potential associations between predicted lncrnas and diseases will be invalid. therefore, in this paper, we will rebuild the weight matrix w ld to solve this kind of problem as follows:"
"the functional similarity between lncrnas measures how similar their functions will be. in this section, based on the method proposed by the state-of-the-art literature [cit], for any given lncrnas l i and l j, supposing that l i and l j have known associations with a group of diseases"
"increasing research has shown that lncrnas play a crucial role in the occurrence, formation, diagnosis, treatment, and prognosis of diseases. the discovery of complex disease-associated lncrnas as biomarkers based on existing biological experiments is not only costly but also requires a large amount of clinical data. therefore, it is a future trend to integrate potential biological data resources and use developed computers to develop efficient and accurate computational models to predict potential new disease-related lncrnas. in this paper, we proposed a novel computational model called pmfilda to predict potential disease-associated lncrnas. in this model, we first integrated known lncrna-mirna associations, mirna-disease associations, and a small number of known lncrna-disease associations into a new weighted lncrna-disease association network. then, based on the newly constructed association network, through adopting the semantic similarity of the disease, the functional similarity of lncrna and the knn algorithm to update the weight network, an lncrna-disease association matrix w ld can be obtained. hence, through adopting the probability matrix decomposition scheme to decompose the matrix w ld into the feature matrix u of lncrna and the feature matrix v of the disease, we can finally construct our model pmfilda based on the two feature matrices to predict the potential associations between lncrnas and diseases. compared to existing state-of-the-art models, simulation results have demonstrated that our model pmfilda has better prediction performance. moreover, case studies of breast cancer, lung cancer and colorectal cancer also indicated that pmfilda can be used as a superior computational model to predict potential lncrna-disease associations. however, it is obvious that there are still some biases in our model. when we only use lncrna-disease associations and regardless of any mirnas, the performance of pmfilda may be reduced. to illustrate this situation, we did the following experiment. after processing the data, we obtained 246 pairs of lncrna-disease associations, including 44 lncrnas, 68 diseases. then we performed 100 loocvs on the pmfilda method, and the average auc value was 0.8111, and the standard deviation was 0.0073. when we used mirna, the average auc value was 0.8794 and the standard deviation was 0.0011. the reason for this difference is that when we don, t consider mirnas, the information we use for lncrna-disease may be incomplete. there may be some important associations that do not exist in the lncrna-disease data set. when the mirna node is added, these important relationships can be re-established. therefore, in our model, we need to consider not only the lncrna-disease relationship, but also the nodes that can improve the lncrna-disease relationship."
"where t d 1 denotes all the ancestor nodes of \"d 1 \" and itself, and e d 1 represents the set of edges in dag d 1 . moreover, for any disease d in dag d 1, its semantic contribution to d 1 can be calculated according to the following formula (2):"
"in terms of packet format, what packets sent from virtual machine contain are very similar to traditional internet packets. but entity-identity substitutes for ip address in the packet header. when the packet is intercepted by the hypervisor of the physical server, locators of both source and destination are inserted into the packet as an additional header. the packet is then sent out through physical links and forwarded hop by hop until it reaches the destination physical server. the destination physical server intercepts the packet and removes the outer header which includes locators, and delivers it to the destination virtual machine."
"in the training phase, the remainder is to design a training mode with a certain network and its inputs, objective function, conditions, etc. in this section, two algorithms with different training modes are proposed and defined as algorithm 1 and algorithm 2 separately. phenomenally, algorithm 1 is trained with the real data only once, however, the other is trained iteratively with both the real and the fake data. their structures are shown in fig. 1, and their steps and corresponding illustrations are defined."
"a wireless communication network sometimes covers a very large area, and its topology is very complicated, while a mobility-oriented cloud data center network is often restricted within a limited area (e.g. in a server room) and its topology is very explicit and uniform [cit] ."
"solving in l3 is generally to utilize ip tunnels and dynamic domain naming system (dns), allowing the data packages to be transferred to the current address. however, this kind of schemes is not high-efficiency since the tunnel will bring extra costs as well as triangle routing problem."
"as discussed above, entity-identity/location-identifier separation is adopted in mobility-oriented cloud data center network, implying that virtual machine is only aware of entity-identity. physical server should be responsible for encapsulating the packets sent from virtual machines with corresponding locators and sending them out through physical links of the cloud data center network. the packet format along the communication path is shown as figure 3 . obviously the communication between two virtual machines in a same subnet does not need the encapsulation."
"in mobility-oriented cloud data center network the physical server is connected to the tor switch through a physical interface, which is configured with an ip address, i.e. location-identifier. when a virtual machine starts running, the hypervisor should firstly get an entity-identity from the l1 location manager for it and register the entity-identityto-location-identifier mapping back into the l1 location manager, with the locator being the ip address of the physical interface of the physical server. it means a virtual machine uses the ip address of the physical machine to transmit data. if a virtual machine moves from one physical server to another, its locator is changed, so the hypervisor should update the corresponding entries in the l1 location manager with the new location-identifier. then the l1 location manager notifies this change to all the l2 location managers to mark the corresponding mapping entries deprecated or remove them directly."
"where ∆ will be set to 0.5 based on the suggestion proposed by the state-of-the-art literature [cit] . moreover, in the same way, it is easy to see that the dag of d 2 can be denoted as"
"the scheme we propose in this paper is different from what has been proposed thus far. by deploying the proposed scheme, the cloud data center can support mobility in virtual machine migration without much cost (e.g. tunneling) and drawbacks like triangle routing. moreover, compared to the schemes designed for the future network architecture, our scheme only needs to modify a little part of the existing protocol."
"in this section, we implemented case studies based on the optimal settings of above parameters to further verify the prediction performance of pmfilda. during simulation, for each given disease, its potentially relevant lncrnas predicted by pmfilda will be sorted according to their predicted scores in descending order. in addition, as a result, the top 20 predicted lncrnas related to the disease potentially will be recorded in the supplementary table s5, and then, two public databases such as mndr v2.0 and lncrna-disease database will be used to confirm these potential associations between the given disease and each of these 20 predicted lncrnas. in this section, we selected three kinds of common diseases such as breast cancer, lung cancer, and colorectal cancer as the targets of our case studies."
"in mobility-oriented cloud data center network, an entityidentity header is introduced into the packet so the packet header overhead will be enlarged. the packet formats in legacy cloud data center and that in the proposed mobilityoriented cloud data center network are shown as figure 5 (a) and figure 5 (b) respectively. for mobility-oriented cloud data center network, the total encapsulation overhead is given by equation (3):"
"the physical infrastructure of mobility-oriented cloud data center network is similar to legacy cloud data center network in topology, routing, forwarding and so on. therefore, the proposed mobility-oriented cloud data center network can be incrementally deployed upon a traditional cloud data center network with some updates on the edge physical servers. cloud computing allows business customers to scale up and down their resource usage based on needs [cit] . figure 1 gives a topology of two-layered cloud data center network architecture [cit] . edge servers are directly connected to the top of rack (tor) switches, and tor switches are connected to core switched in a fat-tree way for load balance and fault tolerance purposes. the essence of mobility in wireless network is that the location changes while the identity remains unchanged after the movement of mobile terminals. and this essence also works for the virtual machine mobility in cloud data center network. thus the mobility management techniques can be introduced into cloud data center to support the virtual machine migration across ip subnets (virtual machine migration inside of l2 network is naturally supported)."
the figure 6 (a) illustrates that a smaller mtu causes a bigger encapsulation overhead. the packet encapsulation overhead decreases as the mtu value increases. if the mtu value is great enough the packet encapsulation overheads of legacy scheme and entity-identity/location-identifier separation scheme are approximately equal. so the mtu value should be set as large as possible (but allowed by the network itself) to reduce packet encapsulation overhead.
"just like handover management for wireless communication network, mobility-oriented cloud data center network also needs migration management to trigger and handle vm migration. migration management of mobility-oriented cloud data center network consists of two functions, i.e. migration decision and migration execution. the former is to decide when to start a virtual machine migration, and the latter is to execute the actual migration behavior. the latter one is often incorporated in the hypervisor, which it is not the focus of this paper."
"suppose vm1 has created a communication session (i.e. upper layer service) with vm2 on server3. then vm1 is triggered to migrate from server1 to server2 by a scheduler (which is not included in the figure) according to some previously defined policies. the migration behavior is performed by the hypervisors on server1 and server2 automatically when the migration instruction is triggered on server1. the hypervisor on server1 collects the running mirror of vm1 and encapsulates it into packets to send to server2. the hypervisor on server2 receives these packets and restores the running mirror of vm1 on server2. the cached mapping entries on server1 should also be sent to server2, so that the location-identifier queries can be saved for the migrated virtual machine. before the restoration on server2 is finished, vm1 on server1 is still responsible for running the upper layer service. when the restoration is finished, vm1 on server2 takes over the job of running the upper layer service, and notifies all its communicating peers (i.e. hypervisor on volume 4, 2016 server3) of the new location-identifier based on the cached entries copied from server1. then server3 can use the new location-identifier to encapsulate packets from vm2 and send them to server2 correctly. moreover, the hypervisor on server2 should send an update message to refresh the corresponding entries in l1 location manager. on receiving the message, l1 location manager flushes this change to all l2 location managers in the cloud data center. here, the communication is between server2 and server3, and the migration completes."
"considering that known lncrna-disease associations are very sparse, there may exist some lncrnas with no associations with any diseases, or some diseases with no associations with any lncrnas. hence, some potential associations between predicted lncrnas and diseases will be invalid. therefore, in this paper, we rebuilt the weight matrix w ld based on knn algorithm to solve this kind of problem."
"solving in l2 is to utilize network virtualization technologies (e.g. virtual private lan service) to shield ip address changes after a migration. this kind of scheme requires the support of switch equipment. moreover, in essence, these schemes are to build a huge l2 network including all the nodes. to some extent, they will conflict with safety performance and management requirement."
"virtual machine mobility in cloud data center is very critical to both the user and the operator of the cloud data center. virtualization is a key technology that has enabled such agility within a data center [cit] . firstly, from the perspective of the user, virtual machine mobility (migration) can be used to move a service from a physical server to another for the sake of getting closer to its customers or getting more resources to better serve customers. this can improve user experience of the service, and potentially benefits the service providers by attracting more customers. secondly, from the perspective of the operator, virtual machine mobility can be used to reschedule the computing resource in the cloud data center for the purpose of power saving or meeting user requirements. this can lower the cost of the operator and enhance the profit, and also satisfy the users of the cloud data center. a highavailability system should be active most of the time, and this is the reason why its design should consider almost zero downtime and a minimal human intervention if a recovery process is demanded."
"under the condition of virtual rps without the real or fake data, the value s k,j c i is set to 0 directly when a virtual rp's two neighbor rps are both heard from an ap, and set to 1 otherwise. after generating, it can be set to 0,1, or a mean value estimated by algorithm em with the fake data."
"it describes the fake data quality in a low resolution. in the training phase, a fake rss should be smaller than the constant c when its corresponding real data heard from the kth ap is valid in the same rp."
"considering that the similarity between disease pairs can calculated by their directed acyclic graphs (dags) [cit], while estimating the semantic similarity of diseases, for any given disease, we will firstly express it as its directed acyclic graph (dag), and as illustrated in the following figure 2, in its corresponding dag, all annotated terms associated with this disease will be contained. for instance, in figure 2 the dags of two different diseases such as breast neoplasms (d 1 ) and liver neoplasms (d 2 ) are shown, and it is obvious that the dag of d 1 can be denoted as"
"in this part, the key point is to analyze influences of sketches with different dropout rates on algorithm 1. we train gan, cgan and ggan with real data collected in all 120 rps."
"since entity-identity/location-identifier separation is adopted in mobility-oriented cloud data center network, location management system is needed for dynamically tracking virtual machines which potentially migrate to another place. this is very similar to location management of mobile node in wireless communication network but still has some differences. determining when is best to reallocate vms from an overloaded host is an aspect of dynamic vm consolidation. and this directly influences the resource utilization and quality of service (qos) delivered by the system [cit] . compared to wireless communication network, mobilityoriented cloud data center network has some obvious characteristics as shown in table 2 . today's data center networks have been designed and operated with little considerations of energy efficiency. they are typically provisioned with redundant links and excessive bandwidth for accommodating peak traffic loads and potential link failures, and run well below capacity most of the time [cit] ."
"in a wireless communication network, the moving speed of mn varies in a very large range, and the faster mn moves, the more frequent handoff should be executed. but in a mobilityoriented cloud data center network, the handoff is not decided by the moving speed but decided by the user requirements and executed by a central scheduler."
"generally, in order to solve the mobility problem in virtual machine migration, there are two kinds of schemes: solving in l2 and solving in l3."
", and then, for any given diseases d 1 and d 2, the semantic similarity between them can be measured according to the following formula (3) obviously: figure 2 . the dags of the disease breast neoplasms and liver neoplasms. in addition, the disease term and its identification numbers are included in corresponding node. the common terms of the two diseases are illustrated by green nodes."
"3.3.3. weight redistribution in g ldw n based on the knn algorithm based on the above descriptions, it is easy to see that we can represent the network g ldw n with its weight matrix w ld, where"
"the fingerprinting localization usually consists of two phases: the offline phase and the online phase [cit] . in the offline phase, a fingerprint database is constructed with rss heard from aps in an indoor area, while signal receivers are at the centers of the predefined grids (each grid center is defined as a reference point (rp)). in the online phase, the observed rss determines the specific localization of a receiver according by a radio fingerprint mapping function. obviously, the fingerprinting localization accuracy is mainly influenced by two aspects, including the establishment/maintenance of rps' fingerprint database and the fingerprint matching method."
"where the row vectors u i and is v j represent the ith lncrna-specific and jth disease-specific latent feature vectors respectively. in addition, obviously, the above formulas (8) and (9) form a convex optimization problem, which can be solved by some existing optimization algorithms such as the iterative update algorithm [cit] easily."
"a mobile host in a wireless communication network sometimes moves very fast, so handoff is performed very frequently, e.g. a cell phone in a moving train, while in cloud data center a virtual machine in mobility-oriented cloud data center network executes the migration at a very low frequency because the migration consumes a lot bandwidth and possibly leads to packet loss, which implies it cannot be executed too frequently. so during a long period of time a mobile host in a wireless communication network may perform many handoffs while a virtual machine in a mobility-oriented cloud data center network may execute handoff only once or twice."
"a tenant of cloud data center may require robustness for its services strictly, e.g. it requires the service servers should not be located on a same physical machine. however, several virtual machines of this tenant may be previously allocated on a same physical machine because there was not enough resource on other physical machines. then if some resources on other physical machines is released, the virtual machines on the same physical machine should be distributed to different physical machines by live migration."
"similar with wireless communication network, the function of location management for mobility-oriented cloud data center network can also be also divided into two parts, i.e. location query and location update, and it is responsible for keeping the current location of a virtual machine and updating it whenever it moves. in this paper entity-identity/ volume 4, 2016 location-identifier separation is introduced into the communication between virtual machines, which means the identity of a virtual machine is indicated by entity-identity while its location is represented by location-identifier. by this separation, we signify each virtual machine with an entity-identity, and the location management system keeps a mapping from entity-identity to its locator (possibly more than one). as the number of virtual machine and the scale of cloud data center network is limited, and the handoff frequency is not as high as in wireless communication network, the load of the location management system is not as high as that in wireless communication network, so it can adopt a centralized or two-layer distributed architecture. figure 1 shows a twolayered distributed location management system for a cloud data center adopting a fat-tree like network topology. l2 location manager is located in a rack and connected to the tor switches, and layer 1 (l1) location manager is directly connected to the core switches. l2 location manager is responsible for caching the most recently and frequently used entity-identity-to-locationidentifier mapping entries in the rack, and l1 location manager stores the entity-identity-to-location-identifier mapping entries for all the virtual machines in the cloud data center. the mapping entries in l1 and l2 location manager are in the following formats, shown as table 3 and table 4 respectively."
"many fingerprint matching algorithms have been proposed to improve localization accuracies in recent literatures [cit], such as: probabilistic methods [cit], k-nearest neighbors (knn) [cit], support vector machine (svm) [cit], artificial neural networks (ann) [cit], and so on."
"moreover, in recent years, lung cancer is a leading cause of cancer-related deaths worldwide, regardless of gender. according to the disease patterns and treatment strategies, it can be roughly divided into non-small cell lung cancer (nsclc) and small cell lung cancer [cit] . to diagnose and treat lung cancers more effectively, researchers have paid lots of attention to the deregulation of protein-coding genes in the past few decades to identify oncogenes and tumor suppressors [cit] . however, recent studies have shown that lncrnas play a significant role in the development and progression of lung cancers [cit] . hence, in this section, we will implement pmfilda to infer the potential lung cancer-related lncrnas. in addition, as illustrated in the following table 3, it is easy to see that 14 of the top 20 potential lung cancer-related lncrnas predicted by our model pmfilda have been confirmed by authoritative biological experiments. for instance, malat1, hottip and meg3 ranked the 3rd, 4th and 5th in the list of our predicted results respectively, and among them, it is identified that malat1 is highly correlated with lung cancer metastasis [cit], will promote lung cancer cell movement by regulating motor-related gene expression [cit], and can be an important biomarker for the development of lung cancer metastasis [cit] . additionally, it is demonstrated that through knocking out hoxa13 by rna interference (sihoxa13), hottip can promote lung cell proliferation, migration, and inhibition of apoptosis, which could serve as a new biomarker and a therapeutic target for nsclc intervention [cit] . moreover, as for meg3, it is proved that the down-regulation of meg3 will enhance cisplatin resistance of lung cancer cells through activation of the wnt/β-catenin signaling pathway [cit] . additionally, the colorectal cancer (crc) has a high incidence in western countries in recent years [cit], and more and more research indicates that lncrnas play a significant role in the formation of crc [cit] . hence, in this section, we will implement pmfilda to predict the potential crc-related lncrnas. in addition, as shown in the following table 2, it is easy to see that eight of the top 20 crc-related lncrnas predicted by our model pmfilda have been confirmed by authoritative biological experiments. for instance, malat1, neat1 and tug1 ranked the 2nd, 8th and 10th in the list of our predicted results respectively, and among them, it is identified that malat1 may be a potential predictor of tumor metastasis and prognosis, and that the interaction between malat1 with sfpq may be a new therapeutic target for crc [cit] . in addition, it is proved that neat1 can be used as an indicator of tumor recurrence and colorectal cancer prognosis [cit], and the expression of neat1 in crc may play a carcinogenic role in the differentiation, invasion, and metastasis of crc, hence, the whole blood neat1 expression can be used as a new diagnostic and prognostic biomarker for overall survival in crc [cit] . moreover, it is demonstrated that the tumor expression of tug1 plays an important role in colorectal cancer metastasis, and tug1 can be used as a biomarker or therapeutic target for potential crc [cit] ."
"as for breast cancer, according to the reports of relevant literatures, it is very common in the group of women [cit] and may be caused by a variety of molecular alterations. for example, studies have shown that the formation of breast tumors is closely related to lncrna [cit] . hence, predicting breast cancer-associated lncrna and identifying lncrna markers are important for the diagnosis and treatment of breast cancer [cit] . in this section, we will implement pmfilda to discover the potential breast cancer-associated lncrnas. in addition, as shown in the following table 3, it is easy to see that 12 of the top 20 breast cancer-related lncrnas predicted by our model pmfilda have been confirmed in authoritative databases. for example, malat1, hotair and h19 ranked the 1st, 2nd and 3rd in the list of our predicted results respectively, and among them, it is proved that malat1 has functional and prognostic significance as a metastasis driver in er negative and lymph node negative breast cancer [cit], hotair will be overexpressed in approximately one quarter of human breast cancers and increased in expression in primary breast tumors and metastases [cit], and the down-regulation of h19 will significantly reduce colony formation and anchorage-independent growth of breast and lung cancer cells [cit] ."
"with real rps, sketches are the same as the fingerprint database but without real rss values replaced by s k,j c i, whose value is set to 0, 1, or a mean value. the value 0 means that the real rss is valid and 1 is on the contrary. straightforwardly, the mean value can be set equally to the average value of the real data heard from an ap in a rp. if so, the sketch is so perfect as to overfit, and the network loses to learn data fluctuation feature and others. alternatively, the mean is set to the mean value of the gaussian model on the assumption that rsss heard from an ap in a rp obey gaussian distribution. in this case, rsss heard from the same ap in different rps obey gaussian mixture models (gmms). with unknown distances between ap and rps, each gaussian model's mean can be calculated by the expectation maximization (em) algorithm [cit] shown as algorithm em."
"the reference [cit] surveys one internet mobility approach which uses identifiers instead of traditional ip addresses to name mobile hosts, contents, or other entities. this approach can be divided into three categories: (1) mobile ip (mip) and its derivatives, which uses a special ip address called the home address (hoa) to identify a mobile node. however, its major drawback is triangle routing problem. (2) identity/locator split designs, which points out that an ip address has both host identifier and locator semantics embedded, and a split of the two is necessary. the mobile node (mn) keeps its identifier unchanged and obtains a new ip address as its locator. and mdn falls into this category. (3) future internet architectures. however, the mobility mechanisms rely on the specific network architecture, thus it is hard to summarize."
"here, we do not analyze the influences on algorithm 2, because: a) the real data is enough to train networks, b) it's better to illustrate influences of varying parameters on generators directly and clearly with only real data, c) algorithm 2 has a variable data volume due to the fake, so it's hard to illuminate results, and d) we analyzed the localization accuracies after proper parameters were selected in the next part, and they can illustrate the two algorithms' performances directly."
"the tenants of the cloud data center may dynamically require more computing resource according to their service expanding. this will cause new allocation of resources and the virtual machine distribution of the tenant in the cloud data center should be optimized to meet specific requirements, e.g. virtual machines of a tenant should be located as close as possible to get high throughput. then some virtual machines need to be moved to proper places through live migration."
"step 1: obtaining the set of different lncrnas shared in both ld and lm. in addition, for convenience, we denoted the set of these shared lncrnas as con_l."
"firstly, we downloaded the datasets of experimentally validated known mirna-disease associations and lncrna-mirna associations from the two authoritative databases such as hmdd [cit] and starbase [cit] separately. then, after having further unified the names of mirnas in these two datasets, we could obtain 246 common mirnas from both of these two datasets. for convenience, we denoted the set of these shared mirnas as con_m. thereafter, based on these 246 shared mirnas in con_m, we finally downloaded 4704 different mirna-disease associations and 9086 different lncrna-mirnas associations from above two authoritative databases. in addition, for convenience, we denoted the set of these 4704 different mirna-disease associations as md and the set of these 9086 different lncrna-mirnas associations as lm separately. moreover, through statistics, there are 373 different diseases in md and 1089 different lncrnas in lm respectively (see supplementary materials tables s1 and s2)."
"all abovementioned sketches are defined as the complete sketches since their structures are the same as the fingerprint databases. hence, the sketch values not 1 are randomly dropped with a special percent (dropout rate) and set to 1 to prevent overfitting further. if all sketch values are dropped, the sketch is defined as non-sketch, and it's the same as no outline condition. simply, the sketch with value 0 or 1 is defined as 0-1 sketch, and the other with the mean value or 1 is defined as gaussian sketch."
"in our method, we constructed a weighted lncrna-disease association networks based on the known lncrna-disease, microrna-disease and lncrna-microna association networks. it may be useful to discuss the contribution of lncrna-disease associations separately here. hence, without considering the relationship between lncrna and disease, we constructed a weighted network of lncrna-disease through using known lncrna-microrna associations and microrna-disease associations only. based on the steps in section 3.2, we finally obtained 304 lncrna-disease associations including 60 lncrnas and 73 diseases. thereafter, we further obtained the corresponding weight matrix w ld, and then performed loocv 100 times on the pmfilda. the results were shown in the following figures 7 and 8, obviously, the auc value achieved by pmfilda based on three association networks can be increased by 0.0763 than the auc value achieved by pmfilda based on two association networks."
"step 2: obtaining the set of different diseases existed in md and ld. in addition, for convenience, we denoted the set of these shared diseases as con_d."
"since the probability matrix decomposition is based on the decomposition of the standard matrix, supposing that w ld is a positive distribution with gaussian noise, then we can define the conditional distribution over the w l d as:"
"the figure 6 (b) gives a similar result as figure 6 (a) but in terms of the data transmission efficiency. smaller mtu implies lower data transmission efficiency, while big mtu brings higher data transmission efficiency. the data transmission efficiency increases as the mtu increases. so a big mtu should be adopted to promote the data transmission efficiency in both legacy and entity-identity/location-identifier separation based communication scheme."
"the state of the data center network may change during runtime due to link failures or other reasons, and this may result in a situation that some virtual machines cannot get enough bandwidth they have required. in this situation, these virtual machines need to be moved to proper positions which can meet their network resource requirements."
"where t l2 is the length of l2 header, t location−identifier is the length of location-identifier header, t entity−identity is the length of entity-identity header, t tu is the length of tcp or udp header in mobility-oriented cloud data center."
"network traffic is decided by the traffic from virtual machines, which maybe further decided by the terminal users. if the traffic between some virtual machines change, e.g. grows greatly, then the network may be not able to allocate more bandwidth to these virtual machines from current links. to address this problem, virtual machine migration should be executed to move these virtual machines to proper places with enough bandwidth."
"mobility is nature in the world and has evolved into an inherent feature and a key driving force of the future network [cit] . mobile internet services are typically provided by servers in cloud data center, usually with virtualizationbased cloud computing technologies."
"the rest of this paper is organized as follows. section ii illustrates the motivation of the work and some related works, and section iii presents a mobility-oriented cloud data center network. in section iv we will introduce mobility management for the proposed mobility-oriented cloud data center network. section v gives a case study and the performance evaluation and section vi concludes this paper."
"we changed the dropout rate of sketches from ten percent to 1. it means that a certain percent of aps aren't heard in sketches but heard in reality. meanwhile, considering the room size, receiver can hear from almost all aps anywhere (either the real or the virtual rp). so, the dropout rate of sketches is set uniformly in the real and fake cases."
"to illustrate directly, we selected the reserving rate (100dropout rate) in fig. 3 . we provided the source code and common dataset on the website https://github.com/shutaozh/ data_augmenter_with_cgan. results are shown in fig. 3 ."
"the generating progress is also divided into two phases: the training phase of gan and the generating data phase. in the training phase, random noise is the generator's input, rss fingerprint data is the real sample and as the discriminator's input, and two sketches with different priori knowledge are the conditions of the generator and the discriminator simultaneously. one manner trained with real rps only once is defined as algorithm 1, and the other which is cyclically trained with real and virtual rps is algorithm 2. in the generating phase, the trained generators with sketches are used to augment data."
"the rest of this paper is organized as follows. section ii exhibits the data augmenter, including architecture, objective function, sketches and training methods. some quantitative evaluation metrics of the data augmenter are proposed in section iii. experiments are given in section iv to illustrate performances of augmenters. finally, some conclusions and future works are given in section v."
"to train gan rapidly and stably, two sketches described the rough outline of the fingerprint database are proposed as the network's conditions. although the simplest sketch is the real data, it would lead to overfitting and make the network instable."
"long non-coding rnas (lncrnas) are a class of important heterologous ncrnas that differ in length from mirnas by more than 200 nucleotides [cit] . for a long time, lncrnas have been considered to be transcriptional noise, and only recently have these views been changed by increasing evidence [cit] . related studies have shown that lncrna plays an indispensable role in many biological processes, such as chromatin remodeling, gene transcription, protein transport and trafficking, and epigenetic regulation [cit] . in addition, the dysregulation of lncrna in coronary artery disease, autoimmune disease, neurological disorder, and various cancers suggests that lncrna plays an important role in many complex diseases [cit] . recently, lncrnas are increasingly attracting the attention of researchers in the field of bioinformatics [cit] ."
"under the conditions of the same fingerprinting matching method and the given accuracy metric (i.e. 1 meter), it needs n rps without a data augmenter and n rps in this paper. hence, sp is defined as 1 − n/n . by fixed conditions, the larger sp is, the better. it indicates data augmenters' effects directly and validly."
"the training pc is equipped with intel(r) xeon(r) cpu e5-1620 v4 cpu, 32gb memory, a nvidia titan xp display card, 500gb ssd and 2tb disk."
"here, we also investigated the influence of knn algorithm on our method from two aspects. one is that we do not use knn algorithm to deal with the weighted network g ldmn, the other is to update the weighted network g ldmn with other algorithms, such as k-means algorithm. when pmfilda is directly executed without using knn algorithm to process the weighted network g ldmn, the result is shown in figure 9 . it is easy to see that pmfilda could achieve an average auc of 0.8794 while the weight of g ldmn was reallocated; however, while the weight of g ldmn was not reallocated, pmfilda can achieve an average auc of 0.8042 only, which demonstrated that it can improve the performance of our model through adopting the knn algorithm to re-allocate the weight of g ldmn . and in addition, to estimate the impacts of other algorithms, we selected the k-means algorithm for further testing. after performing loocv 100 times, we presented the simulation results in the following table 2, and from observing the results in table 2, it is easy to see that the performance of knn is better than k-means."
"in the future, a new localization mapping approach should be studied to improve accuracy with the fake data. meanwhile, combining our algorithm with some other fitting or interpolation methods to construct fingerprint database with extremely little rps."
"as usually, very small rss is invalid and it can be considered as the constant c. this index represents the case that the real is very small (e.g. -90db) but the fake is constant c, and the case that the real is constant c but the fake is very small. it indicates the network's generalization ability."
"for analysis we use the two-layered cloud data center network architecture in figure 1 as the example, and suppose each rack consists of a l2 network, and core switches are l3 switches which connect l2 networks. when a virtual machine moves, two possible results may occur, i.e. moving inside of the l2 network and moving outside of the l2 network. if it moves inside of the same l2 network, then the location managers do not need to update, and this movement is transparent to its communicating peers. but if it moves outside of the l2 network, then the location managers need to update, and all its communicating peers should be notified of this movement."
"entity-identity/location-identifier separation is introduced into mobility-oriented cloud data center network to potentially support virtual machine mobility flexibly. a location management system is adopted to map the entity-identity to location-identifier and support dynam-volume 4, 2016 ical location-identifier update whenever virtual machine migrates."
"based on the above assumption, the average signaling cost can be consequently expressed as equation (6) . we can see that the signaling cost is decided by the probability of staying inside of the same l2 network or moving out of the l2 network. we introduce c notify ij, c l1update i, c l2update i to represent the signaling cost of notifying a communicating peer j, updating l1 location manager, updating l2 location manager when a vm migrates to another l2 network j. for the sake of simplicity, we assume that the number of communicating peers m obeys normal distribution whose mean is 100 and variance is 15. besides, we use the parameter values as shown in table6. figure 7 illustrates the cumulative distributed function (cdf) of signaling cost. as we can see from figure 7, the scheme with higher p inside has lower signaling cost. generally, the size of l2 network where the virtual machine is currently located determines p inside . however, although the signaling cost can be reduced by increasing the scale of l2 network, it cannot be reduced unlimitedly, because a large l2 network brings complex network management task and many other challenges. a compromised scale of l2 network should be adopted in real cloud data center, to maximize the value for the user and the provider."
"where r min is the minimal value in the fingerprint database, and r max is the maximum value on the contrary. the smaller the normalized value is, the stronger the signal strength."
"3) in the comprehensive point of view, algorithm 1 with the 0-1 sketch is more effective and stable than others; 4) we also found that all of adversarial networks without gradient penalty were hardly convergent in a limited epoch. the quantitative evaluation metrics are significant to improve the network quality and the data."
"the localization awareness is a trend for the hyper-connected society to grow rapidly. tough outdoor localization-based service (lbs) benefits from the global positioning system (gps) [cit], robust lbs for indoor applications is still an open problem [cit] ."
"as is known that in wireless communication network, handover is often decided by the signal strength and policies of load balance between adjacent cells and so forth. however, factors are different in mobility-oriented cloud data center network, and the following factors should be considered for deciding a migration schedule."
"based on the set of lm and md, we can construct the lncrna-mirna association network and mirna-disease association network according to the following steps respectively:"
"conclusions are shown as follows. 1) localization accuracies of any data augmenter except gan are better than no augmenter when the percent is larger than 20 and smaller than 80. reasons are: a) all networks are good as long as the training data is enough, b) the fake data is valid except gan, and c) accuracies are depended on knn after rps are enough. 2) algorithm 1 (cgan or ggan) has poorer performance than algorithm 2 (re-cgan or re-ggan) as the percent is smaller than 70. the reasons are that the real and the fake data enrich the fingerprint database for cyclically training in algorithm 2, and the networks of algorithm 2 have strong antijamming capability to the fake. 3) algorithm 1 is better than algorithm 2 when the percent is larger than 60. the reasons are: a) the data is enough to train all networks, and b) it has a little influence of the inaccurate fake data on generator's parameters in algorithm 2. 4) algorithm 1 is robust and can improve the localization accuracy after the percent is larger than 80. however, the improvement is limited due to knn. 5) fixed localization accuracy index, sp can be inferred by table 4, and sp of algorithm 1 is larger than sp of algorithm 2 when the accuracy index is high and vice versa. during the experiments, algorithm 2 is much slower than algorithm 1 because of training generative adversarial networks repetitively. typically, augmenter based on re-ggan is slowest than others due to the gaussian sketch. though re-gan was defined, the results are not shown here due to the poor performance of the fake data by gan."
"live virtual machine migration is to ensure continuous service provisioning to the hosted applications during the vm memory transfer process. it can be divided into two phases: vm image migration and reconnection. vm image migration is to transfer the file system, the state in memory and cpu of the vm. generally, we can use pre-copy method or post-copy method. a pre-copy vm migration transfers entire memory image before resuming vm at the target server. on the contrary, post-copy vm migration approach captures and transfers the vm's minimum system state to the target server before the vm resume phase [cit] . totally, the halt time is close to the transfer time of dirty pages. since the scale of dirty pages is far less than the image, live virtual machine migration can save a multitude of halt time. reconnection is to rebuild the connection between the new vm and its communicating peers. when vm migration is generally only inside of layer 2 (l2) network, the characteristic is that ip address does not need to change. according to the updating request, the network only needs switch to update its address resolution protocol (arp) mapping table. however, if a virtual machine migrates across ip subnets, because the access point address has been changed, it brings two problems. firstly, the data packages sent to the original address cannot be routed to the current address. secondly, the tcp associated with the original ip address no longer works."
"consequently, when there is vm migration across ip subnets, the mobility problems will arise in the reconnection phase. most of the state-of-the-art researches on virtual machine migration focus on live virtual machine migration optimization on bandwidth, storage, power or other performance metrics [cit], or the strategy of virtual machine placement [cit] . however, legacy cloud data center network cannot support service mobility (i.e. virtual machine mobility) in an easy way. although live virtual machine migration allows resources from one physical server to be moved to another transparently with little or no interruption [cit], it will face the mobility problem (i.e. the address for communication will change) when a virtual machine migrates across ip subnets. this challenge hinders the development of many new services and also restricts the flexibility of computing resource configuration in the cloud data center. there is still a lack of research on the virtual machine migration across layer 3 (l3) networks."
"in two-layered cloud data center network architecture shown in figure 1, suppose a vm in rock1 (here we call it vm-1) wants to talk to anther vm in rock2 (here we call it vm-2). rock1 and rock2 are in different ip subnets. vm-1 encapsulates the packet with the entity-identity of vm-1 and vm-2 and sends it out from its internal virtual interface. this packet is intercepted by server-1 and parsed to get the entity-identity of the destination, i.e. the entityidentity of vm-2. server-1 buffers this packet and queries the location-identifier of vm-2 from the virtual machine location management system (which will be discussed in following sections). upon receiving the query, the virtual machine location management system looks up its database to find the mapping entries for vm-2, and sends the result back to the server-1. the server-1 then caches the entityidentity -to-location-identifier mapping entry of vm-2 for future use especially after migration and the use of this cache can greatly improve the performance for locator query. the format of cache entries is as shown in table 1 . a column called requester is added in each entry to indicate who is communicating with this vm recently."
"3) five quantitative evaluation metrics of gan are proposed in wifi-based localization area firstly, and some of them are also used as the gradient penalties for generators."
"if the number of running virtual machines on a physical machine is very small, then these virtual machines can be moved to other physical machines through live migration, and this physical machine can be turned off to save energy. this can greatly reduce the operating expense (opex) of cloud data center."
"in this study, our major contributions are as follows: firstly, we constructed a novel weighted lncrna-disease association network through integrating the known lncrna-mirna association network, the known mirna-disease association network and the known lncrna-disease association network. secondly, based on the semantic similarity of disease and the similarity of lncrna function, we adopted the knn algorithm to update the newly constructed weighted lncrna-disease association network. thirdly, based on the probability matrix decomposition model, we proposed a novel computational model called pmfilda to predict potential lncrna-disease associations, which cannot only predict the potential associations between lncrnas and disease contained in the experimentally validated lncrna-disease associations, but also predict the potential associations of its elements in unknown datasets. to improve the efficiency of our model, in the future, we plan to integrate more intermediate nodes such as genes to update the weighted lncrna-disease association network. in addition, we also believe that the results [25, [cit] of the mirna-disease association prediction field will promote the development of lncrna-disease correlation prediction. moreover, while studying the association prediction of lncrna-disease, focusing on the research results in other fields will also broaden our horizons."
"since known lncrna-disease associations were considered in our prediction model pmfidla, in this section, we download three kinds of gold standard datasets consisting of known lncrna-mirna associations, mirna-disease associations, and lncrna-disease associations from relevant authoritative databases, respectively."
"in this section, we evaluate performances of the two algorithms with different sketches in two aspects: quantitative evaluation measures and localization accuracy. meanwhile, we analyze the influences of sketches' dropout rates on the algorithms. the results are valuable and help to select an algorithm with a sketch to generate more valid rss data in a specific application scene."
"2) two sketches (0-1 sketch and gaussian sketch) with different priori knowledge are designed as the conditions of gan. sketches make adversarial networks learn data features more rapidly and stably. meanwhile, different sketch elements can be constructed by artificial experiences to adapt varying scenes, such as: los/not los, heterogenous network and so on."
"indoor experiments were conducted in our working room with a size of approximately 21 m by 13 m, where wifi signal is available through installed aps (total 10 in test). there are many obstacles (e.g. desks, chairs, persons, cabinets, etc.) and walls, forming the complex radio propagation environment, as shown in fig. 2 . 120 rps were collected and the distances between two adjacent rp fluctuated from 0.8 m-3.2 m due to the specific distribution of obstacles. three testers equipped with different android phones, including a samsung galaxy s9, a realme x and a huawei mate20, participated in the data collection simultaneously. in the offline phase, the dataset was collected over two days, while each tester stayed at one rp in five minutes. in the online phase, each tester walked at one meter per second repeatedly to collect test data."
"1) the quality of data generated by either cgan or ggan is better than gan due to the conditions from artificial experiences. 2) all three networks' diversities are poor, due to the generator's penalty is inclined to the data quality. 3) hp, rfp and ris are good once the dropout rate is smaller than 0.6. when the dropout rate is one or close to 1, sketches are hard to describe the outline of the fingerprint database. in this case, either cgan or ggan is nearly equal to gan. 4) cgan is nearly equal to ggan since features of the real data are learned by the adversarial networks automatically, and element values of sketches have almost no effects on qualities of the generating data."
"in wireless communication network we refer mn to mobile host while in mobility-oriented cloud data center network we refer it to virtual machine instead. the number of mns in a wireless communication network is often much larger than that in a mobility-oriented cloud data center network. a huge wireless communication network may contain billions of mns, e.g. the cellular network of china mobile, but a huge mobility-oriented cloud data center network may contain tens to hundreds of thousand physical servers which can be virtualized to millions of virtual machines."
"to achieve a specific localization accuracy with few or limited rps by the same mapping function (e.g. knn), many fake data of virtual rps are generated by a trained generator g of a conditional adversarial network in this paper, and these data can mix the spurious with the genuine."
"the impacts of mtu on the packet encapsulation overhead and the packet transmission efficiency are shown as figure 6 (a) and figure 6 (b) respectively, and other parameter values used in the simulation are shown as table 5 ."
"the probability of virtual machine migration is associated with specific virtual machine migration scheme. thus it is hard to determine an exact migration probability. we assume that every l2 network in the cloud data center except the current one has equal probability to be the destination of a moving virtual machine. then with p inside, the probability of still staying inside current l2 network, the probability of moving outside of the l2 network to a specific l2 subnet can be expressed as equation (5):"
"in this paper we concentrate on mobility issues in the cloud data center. to meet various requirements of the internet services and provide better user experience, virtual machines should be able to migrate from one location to another. this paper focuses on resolving the challenge of ip mobility problem after a virtual machine migration across ip subnets. we propose a mobility-oriented cloud data center network architecture based on design principles of the mdn. by entity-identity/location-identifier decoupling, we introduce a location manager to map the entityidentity to the current location of target vm. the mapping table helps to relocate the vm so that cloud data center can provide seamless service. this architecture can address mobility issues when a virtual machine migration among ip subnets."
"in the past decades, various indoor localization techniques have been proposed, such as infrared equipment localization [cit], rfid localization [cit], sound localization [cit], vision localization [cit], bluetooth localization [cit], ultrasonic localization and so on, and wifi-based localization attracts great attention [cit] since its most applications are realized with the received signal strength (rss) measured by usual commercial wireless devices without any additional hardware. among those wifi-based methods, fingerprinting approaches are remarkable over other methods due to the no need of access points' (aps) localizations, no line of sight (los) requirements, and superior localization accuracies [cit] ."
"to evaluate the robustness and prediction performance of pmfilda, in this section, the leave one out cross-validation (loocv) was implemented based on the experimentally verified lncrna-disease associations. in loocv, each pair of known lncrna-disease associations is used as a validation set, while other known lncrna-disease associations are used as training sets. moreover, all the lncrna-disease pairs without experimentally verify are used as candidate samples. the ranking of the test sample relative to the candidate sample needs to be evaluated after the implementation of pmfilda. when a threshold is given, if the test sample ranks above the given threshold, then we will regard that a correctly positive sample has been predicted by pmfilda, otherwise we will regard that a correctly negative sample has been predicted by pmfilda. moreover, while different thresholds are set, a series of true positive rate (tpr) and false positive rate (fpr) can also be obtained according to the following formulas:"
"secondly, we downloaded the dataset of experimentally validated known lncrna-disease associations from the mndr v2.0 database [cit], and for convenience, we denoted the dataset of these downloaded known lncrna-disease associations as ld. furthermore, to adapt the downloaded data to our prediction model, we would further process the original data as follows:"
"we evaluated the performance of the proposed scheme, such as packet encapsulation overhead, packet transmission efficiency and signaling cost. the results show that the proposed scheme could effectively solve the mobility issue of vm migration in cloud data center network. furthermore, we take a case study to demonstrate the proposed scheme, and which shows our solution is easy to deploy and has good compatibility with current ip networks."
"the data from the user service are packed into the app data field of the packet. from equations (1), (2), (3), (4) we can see that the packet encapsulation overhead and the packet transmission efficiency are both decided by the mtu parameter. as is known that mtu is a variable parameter, and different kinds of networks provide different mtu values, e.g. ethernet provides 1500 bytes as its mtu."
"1) a novel type of data augmenters based on conditional generative adversarial networks (gans) is proposed to generate data from low-level to high-level resolution. meanwhile, augmenters with different cyclical strategies and conditional sketches are analyzed to offer choices."
"with the rapid development of high-throughput sequencing technology, thousands of lncrnas have been discovered in mammalian transcriptions. numerous studies have also revealed the important role of lncrna in biological processes and the significant effects in complex human diseases [cit] . there is no doubt that lncrnas are closely related to complex human diseases, and more importantly, some lncrna-disease associations have been experimentally confirmed. for example, the expression of xist is up-regulated in glioma tissues and gscs. functionally, xist knockdown exerts tumor suppressor function by reducing cell proliferation, migration and invasion, and inducing apoptosis [cit] . lncrna hotair is highly expressed in prostate cancer and is associated with the growth and aggressiveness of prostate cancer cells [cit] . hence, it is meaningful to identify as many potential lncrna-disease associations as possible. however, up to now, due to the high costs of traditional biological experiments, the lncrna-disease associations supported by biological experiments are still very limited. therefore, it is highly desirable to develop effective computational models to predict potential lncrna-disease associations. in recent years, some computational models have been developed already, and all these models can be approximately divided into three different categories such as the machine learning-based models, biological network-based models and the models without relying on known lncrna-disease associations [cit] ."
"upon receiving the location-identifier reply from the virtual machine location management system, the server-1 encapsulates the previously buffered packet with locationidentifiers of vm-1 and vm-2. then the packet can be delivered to server-2 through the data center network hop by hop in legacy forwarding way. upon receiving the packet, server-2 parses the packet header to withdraw the entity-identity-tolocation-identifier mapping of vm-1 and caches it, and then it removes the location-identifier header from the packet, and finally passes the packet to vm-2. the packet sent back to vm-1 from vm-2 goes in the same way but without a locator query because the location-identifier of vm-1 has been cached before."
"in addition, supposing that the matrices u and v satisfy the gaussian prior to a mean of 0, then the priors of u and v can be denoted as follows:"
"the penalty is to narrow the gap between the real and the fake, e.g., the real is very large but the fake is very small, or the real is negative but the fake is positive. typically, the penalty of the second case is larger than the first."
"therefore, it is obvious that maximizing the log-posterior on u and v with hyper-parameters being kept fixed in formula (13) will be equivalent to minimizing the following objective function:"
"the cloud data center should provide seamless and ubiquitous connectivity for humans, machines, content, and services whenever and wherever. in the mdn [cit], one of the main principles is decoupling of entity-identity and location-identifier. although this idea comes from wireless mobile communication system, it can serve as a great inspiration to solve the mobility issues in cloud data center networks."
"meanwhile, the convergence rate of gan is slower than the others, and ggan is trained fastest. and, gan diverged many times in a limited epoch."
"if fully qualified domain name (fqdn) is used in the communication, then when a virtual machine moves, the corresponding entries in dns does not even need to be updated, because it only keeps the mapping of fqdn-toentity-identity for the virtual machine, and the entity-identity of the virtual machine never changes."
"to better serve mobility management for virtual machines in cloud data center network, decoupling of entity-identity and location-identifier is adopted to split the identity and the location of the virtual machine by defining separated name spaces. the two name spaces are fully overlapped, and both from the planned ip address pool of the cloud data center. it means that the entity-identity of a virtual machine and the location-identifier of another virtual machine could be a same value."
"performance analysis should consider a total signaling cost introduced by a mobility management scheme. in mobilityoriented cloud data center network, the virtual machine migration behavior is usually executed by the hypervisor kernel, but the migration behavior itself has nothing to do with the mobility management, so the signaling exchange between the source hypervisor and the destination hypervisor during the migration procedure is outside the scope of this paper. this paper is focused on the signaling cost of the virtual machine location management."
"a surprising phenomenon in table 3 is that the number 10.0.0.1 appears at both the entity-identity and locationidentifier columns. this is because entity-identity and location-identifier are completely separated namespaces and encapsulated in separated segments of the packet header. suppose a mobility-oriented cloud data center network contains one million virtual machines, then the total memory needed to store the whole mapping entries on l1 location manager is about 8m bytes which is not a huge consumption. a ttl column is added to the entries in l2 location manager to indicate the duration of the entry. an entry whose ttl is 0xffffh means this entry is a static entry and will always exist on l2 location manager until manually removed."
rss heard from an ap in a fixed rp obeys the gaussian distribution. this index indicates that the fake should obey the same gaussian distribution as the real in the same scene. it is also used in g's loss function as a penalty to generate valid values.
"for deployment issue, this scheme can be incrementally deployable for a specific tenant of the cloud data center, while keeping other tenants working in a traditional way. this can be realized in cooperation with the virtual machine management platform, like openstack, cloudstack and so on. so it provides a very flexible choice of deployment for future cloud data center."
"to establish the wifi-based fingerprint database under the condition of sparse rps, a novel data augmenter based on the conditional adversarial network is proposed. to illustrate its effects, we proposed two algorithms with different sketches, and defined five quantitative evaluation metrics based on the artificial experiences and the data features. experiments demonstrate:"
"where l l2 is the length of l2 header, l ip is the length of ip header, l tu is the length of tcp or udp header in legacy packet. and mtu l2 means max transmission unit (mtu) for l2 network. and the efficiency of data transmission for legacy packet can be expressed as equation (2):"
"as we have discussed above, in this context, taking advantage of advanced design philosophy and methods about mobility management, we proposed a mobility-oriented scheme for virtual machine migration in cloud data center network to solve the mobility issues when a virtual machine migrates across l3 subnets."
"with the increasing popularity of mobile devices and unprecedented advancement of cloud computing technology, cloud data center has become one of the most effective ways to provide resources for computing and storage. as a new computing model, cloud computing enables flexible management on the resources of computing, storage and networking. it also requires more advanced networking technologies like flexible control over the network traffic, network virtualization, dynamical network resource management, network programmability and so forth. inefficient resource management policies poorly exploit system resources within cloud data centers [cit] . to better serve the mobile terminals with mobile computing services, an efficient way is to improve the capability of service provider through cloud computing technology like computing virtualization, optimal resource scheduling, service migration and so forth. as a method of reconfiguring the computing resource in cloud data center, virtual machine (vm) migration plays a very important role in cloud computing system in terms of optimal resource scheduling and service migration. the service servers running on virtual machines are potentially triggered to move to a proper position, even if across geographical distances according to the service requirements, the defined policies of the cloud data center or green communication purpose and so on without interruption [cit] ."
"we need to find the values a + and b + for which this quantity attains its largest possible value. let us first, for each a +, find the value b + for which the expression (3) attains its maximum possible value. in the formula (3), the first of the two expressions, namely, the expression a + ·b +, is increasing from 0 to a + as b + goes from 0 to 1. the second expression (1 − a + ) · (1 − b + ) decreases from 1 − a + to 0 as b + goes from 0 to 1. thus:"
"at first, quantum effects were mainly viewed as a nuisance. for example, one of the features of quantum world is that its results are usually probabilistic. so, if we simply decrease the cell size but use the same computer engineering techniques, then, instead of getting the desired results all the time, we will start getting other results with some probability -and this probability of undesired results increases as we decrease the size of the computing cells."
"in these terms, can see that the wh transformation from (c 0, c 1 ) and (c 0, c 1 ) is a rotation by 45 degrees followed by a reflection with respect to the x-axis:"
"however, researchers found out that by appropriately modifying the corresponding algorithms, we can often not only avoid the probability-related problem but, even better, make computations faster. the resulting algorithms are known as algorithms of quantum computing; see, e.g., [cit] ."
"of data -i.e., billions of cells -within a small area, we need to attain a very small cell size. at present, a typical cell consists of several dozen molecules. as we decrease the size further, we get to a few-molecule size, at which stage we need to take into account the fact that for molecules and atoms, physics is different: quantum effects become dominant; see, e.g., [cit] ."
"the only case when we can detect the eavesdropping is when alice and bob have the same orientation, but eve has a different one. there are two such cases:"
"what will happen if, in addition to alice and bob, an eavesdropper -who, in cryptography, is usually called eve -also gains access to the same communication channel? in non-quantum eavesdropping, if eve has access to the corresponding communication channel, she can measure each bit that alice sends and thus, get the whole message. in non-quantum physics, measurement does not change the signal; thus, bob gets the same signal that alice has sent -and so, neither alice not bob will know that somebody eavesdropped on their communication."
"remaining problems and what we do in this paper. in addition to the current cryptographic scheme, one can propose its modifications which also serve the same purpose. this possibility raises a natural question: which of these scheme is the best?"
"shor's algorithm allows quantum computers to effectively find p i based on n and thus, to read practically all the secret messages that have been sent so far. this algorithm is one of the main reasons why governments throughout the world are investing in the design of quantum computers."
"preparing to send a message. now, for each of the remaining (n − k) bits, alice and bob openly exchange orientations r i and s i . for half of these bits, these orientations must coincide."
"quantum computing will enable us to decode all traditionally encoded messages. one of the spectacular algorithms of quantum computing is shor's algorithm for fast factorization of large integers; see, e.g., [cit] ."
"let us analyze the resulting optimization problem. one can easily see that, once the values a + and b + are fixed, the expression (1) that eve wants to minimize is a linear function of e + : namely, it can be described as"
"to be able to do it, we need computations to be as fast as possible. while computations are already fast, there are many important problems for which we still cannot get the results on time. for example, it has been shown that, in principle, we can predict with a reasonable accuracy where the tornado will go in the next 15 minutes, but at present, the corresponding computations take days on the fastest existing high performance computer."
"sender and receiver must use the same orientation. let us show that if for some bit, alice and bob use the same orientation, then bob will get the exact same signal that alice has sent."
"one of the main limitations on the speed of modern computers is the fact that, according to modern physics, the speed of all the processes is limited by the speed of light c ≈ 3 · 10 5 km/sec; see, e.g., [cit] . as a result, for example, for a typical laptop of size ≈ 30 cm, the fastest we can send a signal across the laptop is 30 cm 3 · 10 5 km/sec ≈ 10 −9 sec -during this time, a usual few-gigaflop laptop performs quite a few operations."
"if there is an eavesdropper, then, with certain probability, the signal received by bob will be different from what alice sent. thus, by comparing what alice sent with what bob received, we can see that something was interfering -and thus, we will be able to detect the presence of the eavesdropper."
"as we have seen from the description of the current algorithms, alice and bob can only use bits for which their selected orientations coincide, because in this case, the message bit remains unchanged. if in this case, it so happens that eve selects the same orientation, then her observation will also not change this bit, and thus, we will not be able to detect the eavesdropping."
"this probability depends on eve's selection e + . as typical in game-theoretic situations, we would like to maximize the probability of detection in the worst case for us, when eve uses her best strategy. eve's strategy is to minimize the detection probability (1) . so, we want to find the values a + and b + for which the minimum of the expression (1) over all possible values e + is the largest possible. in other words, we want to maximize the following expression:"
"extracting numerical information from a physical object is nothing else but measurement. thus, to extract the information from alice's signal, bob needs to perform some measurement on this signal."
"quantum cryptography: an unbreakable alternative to the current cryptographic schemes. the fact that rsa-based cryptographic schemes can be broken by quantum computing does not mean that there will be no secrets: researchers have invented a quantum-based encryption scheme that cannot be thus broken. this scheme, by the way, is already used for secret communications."
"if we represent each of the two pairs (c 0, c 1 ) and (c 0, c 1 ) as a point in the 2-d plane (x, y), then the above transformation resembles the formulas for a clockwise rotation by an angle θ:"
