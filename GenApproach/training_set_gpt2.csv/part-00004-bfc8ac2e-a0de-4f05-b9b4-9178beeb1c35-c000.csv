text
"if we make abstraction of the actual java statements and the way they would be interpreted by a java compiler (i.e., sequential propagation of synchronous and blocking operation calls), the classifier behavior of each active object c1 and c2 is theoretically started asynchronously and performed on its own thread. what is important to notice is that active objects are simply started by the execution model, and finish their execution once their associated classifier behavior terminates. there is neither a well identified entity in the execution model describing scheduling rules, nor synchronization primitives that could be used by the scheduler to synchronize running active objects (e.g., operations or signal receptions that could be associated with class object of the execution model depicted in figure 1 )."
"each visitor class of the execution model basically provides an interpretation for the associated fuml class, and therefore explicitly captures the corresponding execution semantics. globally, the execution model can be considered as the model of an interpreter for uml models specified with the fuml subset. figure 1 illustrates a part of this global architecture. it represents the relationship between syntactic elements of the fuml subset (left-hand side of figure 1 ) and corresponding visitors of the execution model (right-hand side part of figure 1 ). for example, the execution semantics associated with the concept of class (which is part of the fuml subset) is defined by the class object from the execution model. it is important to notice that the execution model exploits the mechanisms provided by fuml for concurrency and asynchronous communications. for example, classes objectactivation (which encapsulates the execution of an event dispatch loop, enabling a given active object to react to event occurrences) and classifierbehaviorexecution (which encapsulates the concurrent execution of the classifier behavior associated with the type of an object) are active classes, i.e., classes whose instances have their own thread of control. in principle, the execution model thus explicitly captures the concurrent aspects of fuml execution semantics. in practice, however, the management of concurrency is buried inside the architecture of the fuml execution model. regarding our preliminary objective, this is an important limitation of the fuml execution model: the place where concurrency is handled in the execution model must be accessible and explicit, so that it can be conveniently tailored to the needs of particular application domains. in the two following sections, we first discuss this limitation and its relationship with the usage of java as a concrete notation for the description of behavioral aspects of the fuml execution model (i.e., mainly, behaviors associated with operations of classes from the execution model). then, we more generally discuss the absence, in the architecture of the execution model, of explicit mechanisms for scheduling and synchronizing instances of concurrent entities (i.e., active objects)."
"the example illustrated in figure 2 describes a simple application model that we want to simulate using the fuml execution model. it contains two active classes (c1 and c2) whose instances will communicate via signal exchanges (s1 and s2). the classifier behaviors of c1 and c2 are respectively described by activities c1behavior and c2behavior. c1 asynchronously sends a signal s1 to c2, and then waits for a reception of a signal s2 from c2. on the other side, c2 waits to receive a signal s1 from c1. after the reception, it asynchronously sends a signal s2 to c1."
"profiles are the default uml extension mechanism for tailoring uml2 to specific application domains, from both syntactic and semantic terms. extending uml2 syntax is well achieved, with explicit stereotype definitions capturing the syntactic extensions. unfortunately, the semantic extensions (potentially implied by a profile)"
"more generally, in the field of model-based simulation of concurrent systems, generic approaches such as ptolemy [cit] and modhel'x [cit] should also be considered. ptolemy focuses on modeling, simulation, and design of concurrent, realtime, embedded systems. this approach is based on the notion of actors which communicate through an interface which hides their internal behaviour and is composed of ports. models are built from actors and relations between their ports, with a director in charge of interpreting the relations between ports and the values available on the ports. the director of a model gives the execution semantics of the model as the rules used to combine the behaviors of its component actors. in fact, a director may represent a family of execution semantics and may have parameters such as a scheduling policy. ptolemy comes with a number of directors ranging from synchronous data flow for discrete time signal processing to continuous time for modeling physical processes. it supports a discrete event model of computation which is similar to the execution model of systemc, as well as a process network model of computation in which asynchronous processes are synchronized on the availability of their inputs (contrary to csp, producing data is never blocking, only getting data may block a process if the data is not yet available). in ptolemy, actors are autonomous entities with a behavior which may be executed in its own flow of control. however, in many models of computation, actors are activated in sequence according to a static or dynamic schedule. what is important to notice here is that the director / actor architecture of ptolemy is flexible enough to support multiple models of computation, that is to say multiple execution semantics. regarding the fuml execution model, a similar architecture could be adopted: active objects and/or action executions could be considered as actors, and the explicit entity responsible for scheduling their execution could be a kind of ptolemy director. defining a specialization of the execution model for a given application domain (i.e., explicitly capturing the execution semantics implied by a profile) would therefore basically come to extending corresponding classes in the execution model, and overloading or implementing some of their operations."
"our long term objective is to reflect timed and concurrent execution semantics as introduced in hlam by extending the general-purpose execution model of fuml. ideally, this extension would first rely on fuml mechanisms for concurrency and asynchronous communications, and then add support for time. this extended execution model would typically provide support for model-based simulation, a design technique that has proven useful for rapid prototyping of real-time and embedded systems [cit] ."
"as explained in the introduction to this article, fuml [cit] formalizes the execution semantics of a subset of the uml2 metamodel. particularly, this subset contains mechanisms for the description of concurrent systems (i.e., classes can be active. see [cit], section 13.3.8 for more details). it also includes support for the specification of asynchronous communications (i.e., signal, sendsignalaction, signalevent, see [cit], section 13.12.24, 11.3.45 and 13.3.25) . the semantic formalization, called execution model, takes the form of a uml model specified with the fuml subset itself, simply by considering the fact that the fuml execution engine is a particular executable fuml model. it defines the operational procedure for the dynamic changes required during the execution of a fuml model. in the following section, we start by providing an overview of the execution model. then, we discuss limitations of the execution model regarding the management of concurrent executions."
"to be clear, the fact that the resulting java implementation is mono-threaded and purely sequential is not a fundamental issue per se. indeed, as we will see in the related works section, most state-of-the-art simulation tools are also sequential and mono-threaded. however, these tools include explicit mechanisms for simulating concurrency, usually with a well indentified entity which is responsible for triggering the execution of the various behaviors, according to a given scheduling policy. the real issue with the current architecture of the execution model is that there are no equivalent mechanisms, and that executions obtained via the execution model are purely sequential. let us illustrate this issue with a simple example."
"following this pattern, supporting different scheduling policies amounts to refine the class selecnextactionstrategy (see figure 5 ) for each new policy and to overload the selectnextaction() operation to capture the underlying behavior. in our case, we introduce the class selecnextactionstrategy, whose operation selectnextaction() is overloaded in order to encapsulate the behavior of one particular scheduling policy. for example, fifoselectnextactionstrategy is a concrete class that implements a simple fifo strategy (i.e., by \"fifo\", we simply mean that actions are executed respecting their order of appearance in a list of action activations such as shedulinglist). in order to plug the scheduler onto the fuml execution model, we also modify the behavior of activitynodeactivation in order to let the scheduler determine the next action to be executed after a given activitynodeactivation finishes the execution of its visited action. figure 6 shows a sequence diagram of an interaction trace between the scheduler and an action. the scheduler executes the operation selectnextaction ( ) that chooses one action from its scheduling list according to a certain policy. its implementation actually consists in delegating the choice to a selectnextactionstrategy class (in this case, the policy is the one of fifoselectnextactionstrategy. note that the loci class dynamically determines the various semantic strategy classes to be used, provided it has been correctly configured before launching the execution). then, the scheduler triggers the execution of the selected action. the behavior of the selected action is performed by the operation doaction( ). the operation sendoffer( ) then propagates tokens to the next actions that can logically be executed after it, but it does not trigger anymore the execution of these actions. the scheduler indeed calls updateschedulinglist() to add these potential successors into the scheduling list. the next action to be executed is selected by calling selectnextaction(). this behavior is repeated until the scheduling list becomes empty (i.e., the execution of the activity is finished)."
"have not yet reached a similar degree of formalization. they usually take the form of a natural language description, just like the semantic description of the uml2 metamodel. the informal nature of this description leaves the door open to several (potentially contradictory) interpretations of a given model and does not lend itself to unambiguous model-based exploitations. this is particularly critical when considering complex notions such as time and concurrency, which are central issues to the design of real-time and embedded software."
"the ongoing omg standard on the semantics of a foundational subset (fuml) for executable uml models defines a general-purpose execution model for a subset of uml. this subset includes non trivial mechanisms, carrying concurrent and asynchronous execution semantics (e.g., active objects, signals, etc). our objective was to evaluate how far the current definition of the fuml execution model can support formalization of concurrent and temporal semantic aspects required for real time embedded system design and analysis. as shown in the study, the current form of the fuml execution model is not suited to this objective, mainly due to the way concurrency and asynchronous communications are actually handled."
"things should however evolve with the ongoing omg standard on the semantics of a foundational subset for executable uml models [cit] . this standard indeed defines a formal operational semantics for a subset of uml2 called fuml (foundational uml). the operational semantics of fuml takes the form of an executable uml model called \"execution model\" (that is to say, a uml model defined with elements from the fuml subset 1 ), which is precise enough to be considered as an interpreter for fuml models. while foundational, this subset includes non-trivial mechanisms carrying concurrent and asynchronous execution semantics, such as active objects (i.e., objects with their own execution thread) and asynchronous communications via signal passing. these notions are essential when considering concurrent real-time systems, such as in the marte profile [cit] (modeling and analysis of real-time and embedded systems) and in particular in its hlam sub-profile (high level application modeling), which provides support for designing concurrent real-time systems with extensions inspired by the concept of real-time active object [cit] ."
"classifier behavior classifier behavior fig. 2 . fuml model of a simple asynchronous system figure 3 shows a sequence diagram of a sequential execution trace respecting the java statements of the operational fuml execution model. the hypothesis for this execution trace is that two active objects c1:c1 and c2:c2 have been created, and that c2 has been started before c1 2 . lifelines appearing in the sequence diagram of figure 3 represent instances of classes from the fuml execution model. the interactions between these lifelines show how the model specified in figure 2 is actually interpreted by the fuml execution model (in this case, all the execution is carried out in one thread)."
"in section 2, we start by highlighting fuml limitations. in section 3, we discuss works related to model-based simulation of concurrent systems. we show how principles underlying these approaches could be integrated in the standard execution model of uml. in section 4 we propose a modification of the execution model, which mainly consists in introducing an explicit scheduler. section 5 then concludes this article and sets guidelines for future research."
"to capture several scheduling policies that could correspond to different execution semantics, we rely on the strategy pattern proposed by the execution model, itself based on the class semanticstrategy (for more details about the strategy pattern, see [cit] ). in the fuml execution model, semanticstrategy is used to address semantic variation points of uml, with a refinement of this class for each semantic variation point of uml (e.g., there is a class called getnexteventstrategy, which is introduced to address the uml semantic variation point related to the selection of an event from an object's event pool). fixing a given semantic variation point then comes to refine the corresponding strategy class, by providing an implementation for the operation capturing the strategy."
"in section 2, we have shown that the executions performed by the fuml execution model are purely sequential. we have highlighted the absence of an explicit entity responsible for scheduling the execution of actions. we have identified in section 3 different approaches for modeling and simulation of concurrent systems. each approach contains an entity and primitives to control behavior executions. we propose in this section a lightweight modification of the execution model following this general idea. the goal is to break the sequential execution and provide the ability to control the start of each action execution, in a way that can be easily overloaded (so that it is possible to cope with multiple scheduling policies). we introduce for this purpose an explicit scheduler into the execution model, as illustrated in figure 5 ."
"the execution model has been defined following the visitor design pattern [cit], where almost each class of the execution model has a relationship with a class from the fuml syntax subset (except for a package called loci, where classes locus, executor and executionfactory are not visitors, and are just used for setting up the execution engine)."
"we have mainly shown that the current architecture of the fuml execution model suffers the lack of explicit mechanisms for manipulating and synchronizing concurrent entities. existing solutions for embedded system simulation indicate that it is possible to provide much more adapted and realistic solutions. we proposed some concrete modifications regarding the architecture of the fuml execution model, inspired by these solutions. we took care of minimizing changes in the architecture, so that we can leverage as much as possible on the existing execution model (and all the work that its definition implied). the proposed solution is mainly intended to show that a modification of the fuml execution model is technically feasible and reasonable in terms of efforts. however, further experiments are still required to validate the proposed modifications. additionally, this solution only reflects executions by a single unit of computation (i.e., mono-processor). the case of executions onto multiple processing units will be investigated in future works."
"this architecture is not well suited to our primary objective: specializing the execution model in order to reflect concerns of the real-time domain. for this purpose, we believe that introducing an explicit and well-identified entity responsible for scheduling active objects and/or action executions is mandatory, along with wellidentified primitives for synchronizing and scheduling concurrent entities. existing solutions (discussed in the next section) in model-based simulation of concurrent systems could inspire the modifications required by the execution model."
"the class scheduler manipulates a list of activitynodeactivation (i.e., this class represents the visitor class of uml::action) depicted by the property schedulinglist, which contains the list of all actions ready to execute (i.e., an action is ready to execute if all its control and data tokens are available). scheduler offers several operations that can be used to control executions of actions. these operations are called in the body of start ( ) which actually start the behavior of the scheduler. the operation selectnextaction( ) determines the next action to be executed, by extracting an element from schedulinglist, according to a given scheduling policy. the operation updateschedulinglist ( ) determines the potential successors for the last executed action (i.e., with respect to control and data dependencies within the executed activity) and adds them to the scheduling list."
"beyond these technical details, it is important to notice here that this sequential propagation of operation calls will finally result in a valid execution trace (i.e., an execution trace respecting control and data dependencies expressed between actions in the application model being simulated). basically, once an action execution terminates, it will simply trigger the execution of another action that can logically be executed after it. the problem here is that the mechanisms which determine the next action to be scheduled is buried inside the implementation of each actionexecution visitor class. if we want the execution model to be easily customizable for the realtime domain (which is our primary objective), we clearly need to extract this scheduling aspect from visitor classes, and add an explicit entity that would be responsible for scheduling the execution of actions. once the entity which is responsible for scheduling action executions is clearly identified, it can be easily specialized to capture various execution schemes, corresponding to various execution semantics (i.e., semantics implied by a profile definition). perceptive readers may wonder whether the need for an explicit scheduler is the consequence of the sequential java implementation."
"on the right-hand side of figure 3, the instance of classifierbehaviorexecution represents the execution of the classifier behavior of c2. once it is started, it performs the accepteventaction. from the execution model standpoint, it consists in registering an eventaccepter for s1 within a list of waiting event accepters (i.e., call to operation register()). it captures the fact that the execution of c2 is now waiting for an occurrence of s1. however, the execution of c2 does not actually wait for an occurrence of s1 (i.e., with the strict interpretation of the java statements, the classifierbehaviorexecution is not executed on its own thread). instead, it returns to the main activity, which continues the execution by starting the classifier behavior of c1. the execution flow of c2's classifierbehaviorexecution will be further continued, after an explicit notification. on the left-hand side of figure 3, when the classifier behavior of c1 starts (i.e., call to execute() emitted by the activityexecution), it executes the sendsignalaction. the semantics associated with the sendsignalaction is captured in the execution model by calling the operation send() of target object c2, which in turn calls the operation send() of objectactivation. it results in adding a signal instance s1 to the event pool associated with the object activation of c2. in order to notify the classifierbehaviorexecution of c2 that a signal is available for dispatch (and therefore that its execution flow can potentially be continued if there is a matching eventaccepter), a call to _send(new arrivalsignal()) is emitted, which in turn causes a call to dispatchnextevent(). this operation dispatches a signal from the event pool and matches it against the list of waiting event accepters. if an event accepter matches, a call to the accept operation of the accepteventaction is performed and the classifier behavior of c2 continues the execution by sending signal s2 to c1. the execution of this sendsignalaction results in a call to operation send() on target object c1, which in turn implies the sequencing of operations described above."
"coupling with existing and more static approaches such as timesquare [cit] could also be considered. timesquare provides an environment for modeling and analyzing timed systems. timesquare supports an implementation of the time model introduced in the uml marte profile and the ccsl language (clock constraint specification language). it displays possible time evolutions as waveforms generated in the standard vcd format. these evolutions constitute a scheduling trace. timesquare takes as input an uml model and a ccsl model applied to the uml model. the ccsl model is used to specify time constraints and apply a specific behavioral semantics on a model. the result produced by timesquare is a sequence of steps (scheduling map) that can be used by external tools for analysis/simulation purposes. concretely, coupling the fuml execution model would mean that a ccsl model must be generated for a given application model, and that the generated model reflects the time and concurrent semantics of the application domain for which a profile is defined. scheduling maps generated by timesquare could then be \"played\" by the execution model. again, modifications in the architecture of the execution model would be required, and would mainly consist in adding an explicit entity responsible for triggering executions of active objects and actions, with respect to the scheduling map generated by timesquare."
"like ptolemy, modhel'x defines a unique generic simulation engine to support all mocs. consequently, modhel'x is well adapted for heterogeneous systems modeling. it adopts a model-based approach where the whole behavior is represented by a set of blocks, ports and unidirectional lines. a snapshot-based execution engine is proposed for interpreting this structure (see figure 4) . figure 4, a model execution is a sequence of snapshots. to compute each snapshot, the algorithm provides the model with inputs from the environment and builds an observation of the outputs, according to its current state. this process has a generic structure: first, choose a component to observe, then observe its behavior in response to its inputs, and propagate this observation according to the relations within the model structure. this generic algorithm for executing models relies on such primitive operations which can be refined for each model of computation. the semantics of these operations define the semantics of the model of computation. indeed, modhel'x has a more generic execution engine and provides a finer grain description of models of computation than ptolemy. concretely, we could also get inspiration of this architecture to modify the fuml execution model. a class encapsulating the snapshot-based execution engine could be integrated in the execution model, and specializing the execution model for a given application domain would basically come to provide particular implementations for the operations described above."
"to evaluate the sum-rate performance of the presented algorithms, we plot the sum-rate verses different transmit power values for fixed number of sub-channels and users in fig. 4 . this figure shows that our presented isa-sopara and sba-sopara algorithms result in considerably higher sum-rate than the epes algorithm for the entire range of transmit powers values. as for as, the performance comparison of our two presented algorithms is concerned, the sba-sopara algorithm shows better performance than the isa-sopara algorithm and it is close to the optimal algorithm in terms of achieved sum-rate. this figure illustrates that initially when the transmit power is 0.2 watt, the sum-rate of the optimal algorithm, sba-sopara algorithm, isa-sopara algorithm and epes algorithm are 31 mb/sec, 26 mb/sec and 17 mb/sec, respectively. the sum-rate gradually increases with increase in transmit power and reach up to 40 mb/sec, 32 mb/sec, 28 mb/sec and 25.2 mb/sec, respectively, for all four algorithms in the above mentioned order. this is because the link's rate is in direct relation to sinr, and sinr is, in turn, in direct relation to transmit power. to illustrate the impact of number of associated users on sum-rate performance of the presented algorithms, we plot sum-rate against different number of users in fig. 5 while keeping n and p max u fixed. it is clear from the figure that our presented algorithms (isa-sopara and sba-sopara) shows better performance than the epes algorithm while they are closed to the optimal algorithm in term of sum-rate maximization. among our presented two algorithms, sba-sopara algorithm performs better than the isa-sopara algorithm."
"step 2: assign a subchannel n to a user k which achieves highest data-rate on it. this gives a user subchannel pair (k *, n * ). 4: step 3: update n m k * and n m,f k * . 5: step 4: satisfying the subchannel exclusivity and adjacency constraints, repeat step 2 and 3, respectively, until n m goes empty. 6: end"
"where p k,n is the transmit-power of kth user on nth subchannel and p max k represents the maximum per user transmit power. the sinr of kth user on nth subchannel is given in the following equation. where"
"based on the assumptions in system model, the joint rrh-association, sa and the pa problem is formulated in this section. the objective of this joint optimization problem is sum-rate maximization subject to transmit-power budget, subchannel exclusivity and adjacency constraints."
"in isa algorithm, the achievable rate of every user on each subchannel is calculated iteratively and then the subchannel is assigned to the user having highest achieved rate on that subchannel in each iteration. after assigning a subchannel to a user in the above mentioned way, the achievable rate on the immediate next subchannel is calculated for every user. if the user having highest achieved rate on this subchannel is the same to which the immediate previous subchannel has been assigned, then this subchannel is also assigned to the same user. otherwise, the previously assigned user will not be considered for the remaining subchannels due to the adjacency constraint of sc-fdma. this way, all the subchannels are assigned to the users. the step wise operation of the isa algorithm is given in algorithm 1."
"the solution is carried out using integer relaxation method. for each user k, the proposed algorithm associate the user with rrh which is most beneficial for this user. we represent this solution by α i+1 and then next iteration begins. the flow chart of the presented two-step algorithm is given in fig. 2 . the proposed techniques will converge after a few iterations. in our numerical simulation, both the techniques take only 3 iterations to converge. volume 6, 2018"
"in this paper, we considered a sc-fdma based multitier c-ran and presented an iterative algorithm performing the joint optimization of rrh-association, sa and pa for sum-rate maximization in ul. the presented iterative algorithm solves our joint optimization problem in two steps. sa and pa are performed in the first step while the second step of the presented algorithm is concerned with rrh-association. we compared the performance of our presented isa-sopara and sba-sopara algorithms with optimal and epes algorithms and the simulation results show that our presented algorithms provide better performance than epes algorithm and are close in performance to optimal algorithm. while among the presented isa-sopara and sba-sopara algorithms, the later performed better."
"the second step of the proposed algorithm is concerned with rrh-association optimization. in this step, we find the optimal rrh-association (α) for problem 6-10 with p i+1 . after obtaining sub-channel allocation n m k and power allocation p i+1 in the first step, the rrh-association problem can be rewritten as"
"finally, both the proposed isa-sopara and sba-sopara algorithms are compared in terms of computational complexity in fig. 8 . to this end, the algorithms are run for various number of iterations for which the computational time has been found. the figure reveals that the sba-sopara algorithm is 34% more efficient than the isa-sopara algorithm in terms of computational complexity."
"in fig. 6, we plot the sum-rate verses different number of subchannels for fixed k and p max u . the fig. 6 clearly shows that our presented isa-sopara algorithm and volume 6, 2018 sba-sopara algorithm result in higher sum-rate the epes algorithm while among the two presented algorithms, sba-sopara algorithm outperforms isa-sopara algorithm."
"the fig. 7 shows the effect of cell size on network sum-rate where, the behavior of network sum-rate against 500 m, 1000 m, 1500 [cit] m cell radii is investigated for k, n and p max u . the network sum-rate performance of the all four algorithms, (i.e., optimal algorithm, sba-sopara algorithm, isa-sopara algorithm and epes algorithm) is in the same order as it is in figure 3, 4 and 5. the figure shows that for 500 m cell radius, optimal algorithm, sba-sopara algorithm, isa-sopara algorithm and epes algorithm result in 28 mb/sec, 38 mb/sec, 47 mb/sec and 62 mb/sec, respectively, which gradually reduce to 31 mb/sec, 26 mb/sec, 20 mb/sec and 11 mb/ [cit] m cell radius. this is because, the farther a user is from its rrh, the more the signal-strength drops and the lower the data rates that it can reliably achieve."
"the remaining of this paper is ordered as follows. in section ii, the system model and problem formulation are presented. in section iii, a two-step algorithm is proposed to solve the problem formulated in section ii while section iv contains the simulation results and discussions. the paper is concluded in section v."
represents the objective function which is sum-rate maximization of the system. equation 7 represents the maximum per user transmit power constraint while equation 8 represents the rrh-association constraint which tells that a user can be served by a single rrh. the constraint in equation 9 reflects the fact that each subchannel can be assigned to one user at a time. the constraint in equation 10 ensures that all the subchannels in set n m k are contiguous.
"where m is the set of rrhs, n m is the subchannel set available at rrh m and k is the set of total users. furthermore, n m k and n m,f k shows the sub-channels set currently assigned to user k connected to rrh m and the feasible set of subchannels for user k connected to rrh m, respectively."
"we compare the performance of our presented isa based sub-optimal power & rrh association (isa-sopara) and sba based sub-optimal power & rrh association (sba-sopara) algorithms with optimal algorithm and equal power and equal subchannel allocation (epes) algorithm. for optimality, we use ipa and integer relaxation method. integer relaxation method is use for optimal rrh-association while ipa is used for optimal pa. in the epes algorithm, each user is assigned an equal set of contiguous sub channels while the maximum transmit-power of each user is equally divided among the contiguous subchannels assigned to it."
"the joint rrh-association, sa and pa problem in equations 6-10 is minlp. even for a given rrh-association and power allocation, the optimal subchannels assigning among the users alone is prohibitively difficult. this difficulty mainly arises due to the subchannels exclusivity and adjacency constraints given in equations (9) and (10) which results in extremely large search space. consequently, the exhaustive search is not practical and the solution of the problem in its current form is prohibitively complex. therefore, to solve the joint optimization problem in equations 5-9, a two steps iterative algorithm is proposed which jointly optimizes subchannel assignment, power allocation and rrh-association. the proposed two steps algorithm is discussed in the following subsections."
"in sba algorithm, the available subchannels are divided into small groups containing equal number of contiguous subchannels in such a way that the number of groups is equal to the number of associated users. unlike the isa algorithm, in sba algorithm, the achievable rate of every user is calculated on each group of subchannels and then the group of subchannels is assigned to the user with highest achieved sum-rate on this group. the user which has been assigned the subchannel group, will not be considered for assigning the remaining subchannels groups. the remaining groups are assigned to the remaining users in the same way. the step wise operation of the sba algorithm is given in algorithm 2."
"to address the above challenges regarding multi-tier networks, a new mechanism known as cloud-radio access network (c-ran) has been proposed by the researchers which consists of two parts; a group of remote-radioheads (rrhs) part and a centralized-baseband-unit (bbu)-pool part [cit] . the rrhs and bbu-pool are interconnected through the fronthaul links while macrocell bs (mbs) is connected with core networks through backhaul links. the bbu pool executes upper layer functions and baseband signal-processing, whereas, the rrhs normally perform as radio-frequency (rf) transceivers and only perform basic rf functions [cit] . c-ran has the ability to increase networkcapacity and energy-efficiency, reduce the network capital and operating expenditures and manage the inter-tier and the inter-cell interference [cit] . these benefits of 5g multi-tier networks can be achieved if small cells are deployed densely in a c-ran architecture."
"we consider the ul of a sc-fdma based multi-tier 5g c-ran which contains total of 3 communication tiers as illustrated in fig. 1 . each tier is served by a particular rrh: for example, tier 1 is served by macrocell-rrh, tier 2 is served by picocell-rrhs and tier 3 is served by femtocellrrhs. furthermore, there are total of m rrhs (macrocell-rrh + picocell-rrhs + femtocell-rrhs) where (m − 1)/2 femtocell-rrhs and the same number of picocell-rrhs are underlayed in the macrocell. our presented multi-tier c-ran consists of a bbu-pool connected to an mbs-rrh through backhaul link and small cell rrhs (i.e., femtocells and picocells) connected to the bbu through fronthaul links. in the presented architecture, transmitting control signals to c-ran and data transmission are the duties of mbs-rrh and small cell rrhs, respectively. moreover, sc-fdma is considered as ul multiple access technique in our presented multi-tier c-ran."
"furthermore, it is supposed that there are total k randomly deployed users. the available transmission bandwidth is subdivided into n subchannels. all the n subchannels are reused at each rrh due to which the users of each rrh causes interference to the users of all the other rrhs in the system. on the other hand, the users associated to each individual rrh will share the n subchannels orthogonally. that is, at each rrh, a subchannel can be assigned to one of its associated users at a time thereby fulfilling the subchannel exclusivity constraint of sc-fdma [cit] . the subchannel adjacency constraint of sc-fdma [cit] should also be fulfilled, whereby only adjacent multiple subchannels can be assigned to a user. in addition, it is supposed that each user is served by a single rrh."
"accuracy in age prediction for validation and test sets of five similar, evaluated network architectures compared to baam are detailed in the supplemental data (appendix a)."
"our results show that complex processes, such as bone tissue maturation and adaptation, can be reconstructed by in vivo imaging-based deep learning and quantitative analysis of learned features. even subtle changes, as occurring during one remodeling cycle of mice bones [cit], could be identified, as our model predicts four time points between 26 and 28 week old mice based on in vivo µct images with an accuracy of more than 95%. this classification is presumably linked to bone mass, shape, micro-architecture and material properties alteration through different length scales, as all these processes change in a sitespecific manner during growth, maturation and aging. these changes take place by modeling, remodeling, mineralization and demineralization processes [7, 25, [cit] and are directly or indirectly encoded in the image created by interaction of photons with matter, as the resulting attenuation is, besides bone mass in the photon beam, dependent on the local atomic number, and therefore calcium content."
"mask r-cnn utilizes a region proposal network (rpn) to generate image regions that possibly contain an object. each region is ranked based on its \"objectness score\" (i.e. the probability of an object being present in a specified area) and then the top n most probable object regions are maintained."
the analysis of overall achievements of presented networks as examples of variance in characteristics of dnns lead to designing the baam network . it over-performs the sample 27
"in the classification section, the application will read all the images in the herlev dataset based on the cancer class. the images used at the classification stage are only the cervical cell regions. by involving the image's mask, before the image of the cervical cell is copied and grouped according to the cancer class, the binary mask image will be applied to the original image so that a new image consists of only two parts, namely the cell part of the cervix and the background (colored black). this new image is then resized into 200 pixels for its width and is proportional in length. the image that has been resized is then copied into a specific folder according to the classification case that we want to train, namely two folders for binary classification cases and seven folders for 7 class classification cases. the dataset is then ready to be trained by the vgg-like network."
"as shown in table 3, our proposed segmentation using mask r-cnn produces high average performance, i.e. 92% precision, 91% recall and 91% zsi for all cell types with low standard deviation. only the normal columnar type produces a performance result below 90%."
"after 1000 training iterations, the accuracy of age prediction (training and validation sets) reached 92% and 93% with a loss of 0.21 and 0.20, respectively. after 7000 iterations, the accuracy and loss values for training and validation sets were 100%, 99%, 0.02 and 0.03, respectively (fig. 4a-b) . therefore, the network was considered as trained after 7000 iterations."
"and distal (t3,f3), cf., fig. 2 . the summation of intensity values of the saliency map in each region normalized to the summation of intensity values of the saliency map in the bone region (tibia and fibula) are defined as a measure to indicate the importance of each region for the age estimation (attention, [0 − 1]). extracted labels for tibia (purple) and fibula (green) and the 6 regions with equal proximal-distal height on each label. t1-t3 and f1-f3 regions belong to tibia and fibula, accordingly."
"a segmentation method is applied to separate the cell nuclei from its cytoplasm and then classifies them using the k-nearest neighbor (knn), which resulted in an 84.3% classification accuracy with no validation and 82.9% classification accuracy with 5-fold cross validation [cit] . a knn method is also used to classify normal and cancerous cells on microscopic biopsy images after the segmentation process using k-means [cit] ."
"unlike the original vggnet which uses 3 fully connected layers before the softmax layer, we only use 1 fully connected layer. in each convolution layer that uses the max pooling layer, a dropout layer is added with a ratio of 0.25 while the dropout ratio of the only fully connected layer (beside the softmax layer) is 0.5. in addition, there is a batch normalization at each network layer except in the input and softmax layers. the proposed vgg-like net architecture is shown in table 2 ."
"research on the automated screening of pap smears has moved from cytology to histology over recent years. the combination of information from a multitude of computerized histology and cytology documents was used on the brazilian cervical cancer information system (siscolo) for sensitivities above 90% [cit] . however, cytology testing continues to be used in most countries because of its affordability and efficiency in identifying cervical cancer in routine testing."
"a major challenge in disease diagnosis is interpreting information-rich (imaging) data. this challenge is at the same time a great opportunity, as there exits nowadays artificial intelligence-based methods that have the capabilities and power to analyze relationships within rich datasets, e.g., relationships of particular dynamic biological phenomena. artificial intelligence, for example, has been used to diagnose alzheimer disease based on magnetic resonance imaging (mri) [cit] or to analyze skin lesion for diagnosing malignancy [cit] . similar to the previous two examples, one can also use artificial intelligence to analyze (re)modeling of bone using x-ray images (eg. µct, hr-pqct). as scatter and attenuation information of µct images contain information about material composition, distribution and amount, they potentially contain all structural information that is needed to asses bone maturation [cit] . despite the fact that recent studies can extract from 2d and 3d x-ray image data more features describing bone quality through assessment of vbmd and microstructure [cit], information on bone (re)modelling rates are only obtained through invasive histomophometry analysis of iliac crest bone biopsies. fortunately, recent advances in artificial intelligence towards deep learning now enable further data analysis by utilizing high-throughput image data. compared to traditional machine learning methods [cit], deep learning methods do not only exhibit an improved prediction accuracy, but also provide the ability to visualize learned features, to link discovered features with clinical relevance. the first applications for bone age assessment in pediatrics using deep neural networks (dnn) showed already some success in classifying/predicting bone age from 2d x-ray images [cit] . further, they provide confidence that dnn-based methods can also provide insights into the underlying processes of skeletal maturation and bone (re)modeling."
"the mask r-cnn approach builds on the faster r-cnn and makes two significant contributions: (1) it replaces the roi pooling module with a more accurate roi align module; and (2) it adds a branch for each roi, as shown in figure 3 . this additional branch is responsible for predicting the actual mask of an object/class. the masking branch splits off from the roi align module prior to our fc layers and then consists of two conv layers responsible for creating the mask predictions themselves. the mask r-cnn output has three kinds of prediction, i.e. class/label prediction, bounding box prediction and mask prediction. mask r-cnn can leverage different architectures such as resnet, vgg, squeezenet, and mobilenet as their backend/backbone, making it possible to decrease the size of the model produced by the segmentation training stage, making it feasible for deployment on a mobile device and potentially increase frame per second (fps) throughput as well. in our study, the mask-rcnn backbone is applied as a resnet-based feature pyramid network (fpn) with the refined extraction layers of features and the reduced subsequent extraction layers of features according to all the cervical cell images."
"the results of the segmentation are then applied on the original image dataset before being handed over to the classification training algorithm, as illustrated in figure 1 (b). the input image (image source) for classification is the cervical cell (colored black), as shown in figure 2 . in the classification stage, we employ the vgg-like network, which is a more compact version of the vgg network for faster training. figure 1 (c) illustrates the testing process during classification. the cervical cell images are segmented using mask r-cnn to isolate the cervical cells and then they are processed in the trained vgg-like network. based on the final score for each class (the 2 or 7 classification problem), the system determines in which class the cervical image belongs."
"our study has also limitations. first, the chosen samples may not represent a larger population. however, we validated our method in a three-fold manner and received accuracy above 95%. additionally, structural parameters derived from µct and histomorphometric indices of bone formation, which we have measured previously in these mice, are similar to those reported in similarly aged female c57bl/6 mice [cit] . adhering to the principles of the three rs, specifically reduction of animal number, motivated us to re-examine these datasets, rather than perform new studies on additional mice. second, skeletal aging in mice differs to that in humans [cit] and thus, translation of these results to human bone behavior requires further investigation. since age-related bone loss in humans resembles to a certain extend findings in mice [cit], it can be speculated that a future application to human bone might resolve similar patterns."
"this section provides an in-depth discussion of the segmentation and classification results applied on the herlev dataset obtained by the proposed method. previous approaches applied to the problem of cervical cell detection in the herlev dataset are compared to the results obtained from our research. we employed mask-rcnn which builds on the faster r-cnn as a promising approach that uses pixel-level prior information to acquire better semantic features so that it can efficiently detect and localize the whole cervical boundary regions with high accuracy while simultaneously generating a high-quality segmentation mask for each instance. it overcomes the difficulties that are widespread in whole cervical cell images. more importantly, the mask r-cnn approach is conceptually simple to implement because it does not need complex pre-processing steps since feature selection is conducted by the mask r-cnn algorithm. mask r-cnn has a small computational overhead that enables a rapid system for training."
"in this study, we present a deep learning approach applied to 2d projection x-ray images of bones as an end-to-end tool for site-specific, spatio-temporal assessment of bone tissue maturation and intervention effects. by simultaneous evaluating several relevant hierarchies, our method allows us to reconstruct continuous biological processes such as aging or adaptation of bone. we developed and evaluated our method on pre-clinical µct data of mouse bones and investigated bone adaptation in response to in vivo tibial compressive loading. to do so, we first evaluate, if our method allows identifying short-term, skeletal maturation-related changes in the proximal tibiae and fibulae based on µct images. therefore, we analyze short-term (15 days) dynamic skeletal maturation processes in adult female mice bones. second, we evaluate, if our model can be used to identify treatment results and relate these to load-induced surface (re)modeling (\"rejuvenation\")-effects. furthermore, by analyzing the learning process of our dnn through saliency maps, we quantify the spatial localization of the network attention, permitting determination of where in the bone the \"skeletal age\" information is manifested."
"we trained the model with the adam optimization algorithm [cit] . the network is initialized with a truncated normal distribution function (standard deviation: 0.1). training is carried out for 7000 iterations with a batch size of 100 images. at every 500 th step the model is applied on a batch of 200 images of the validation set. the initial learning rate is set to 0.1, then an exponential decay at every 25 th step with a 0.96 rate is performed. implementation, training, validation and testing of the network was performed using google tensorflow [cit] on a computer with a single nvidia geforce gtx 1070 gpu."
a clustering technique using fuzzy c-means (fcm) was used to segment pap smear images [cit] . one of the drawbacks of fcm clustering is that it fails to detect all the valid clusters in a colour image segmentation. [cit] presented a trainable weka segmentation classifier for cell segmentation and an enhanced fuzzy c-means algorithm to classify cervical cells.
"several research studies on segmenting and classifying the nucleus have been overviewed in this section. however, it might not be possible to classify cervical cells with only nucleus data. the segmentation of the whole cell is therefore more suitable [cit] . each cell is then classified using specific classifiers after the segmentation step. [cit] created a two-level cascade classifier to automatically detect cervical cancer cells from thin liquid-based cytology slides. the neural mlp feedforward network of levenberg -marquardt was used to classify the cervical images of 100 patients [cit] . classification of cervical cell images is done with deep learning [cit] . the performance of this type of classification, however, is not very high [cit] ."
"in almost all imaging system analysis, image segmentation is an important and demanding task. it is difficult for individuals to precisely analyze the segmentation of all parts of cervical cells (nuclei and cytoplasm) in pap smears. poor cell segmentation can lead to poor analysis results. accurate and automatic computer-assisted segmentation on the whole cervical cell is necessary for cervical cancer screening and diagnosis. a set of 50 images was screened for the segmentation of cervical cells using mean-shift and median filtering, and for the further processing of the segmentation result using morphological operators [cit] ."
"given the proven performance on reconstructing bone tissue maturation and adaptation processes, we expect our study to pave the way for future studies to investigate a wide variety of biological processes involving continuous morphological changes. this may include developmental and aging stages, the progression of diseases [cit] or the response to treatments, and dynamic processes that have often been reduced to binary classification problems. automated computational analysis, as shown here, could reveal morphological changes at much earlier stages than recognized previously. therefore, the effects of loading duration on overall bone adaptation might be another future application, as recently shorter loading durations have been suggested [cit] . furthermore, as features can be used to classify biological structures based on morphology [cit], a combination of the proposed deep learning and saliency maps approach can lead to more detailed insights into the contribution of individual localized biological processes on the overall adaptation \"state\" of the bone."
"it is known that bone fracture resistance increases with skeletal maturation and aging, as well as in response to certain therapy. whole bone fracture resistance is determined by bone quantity, which encompasses geometric, microarchitectural, and material properties (i.e., trabecular architecture, mineralization, crosslinking, microcracks). little is known about the interplay between all of these properties/factors contributing to compromised or recovered bone quality. most previous studies have focused on individual features, while lacking global optimization, as individual contributions of static (e.g. trabecular bone volume or bone mineral density distribution) or dynamic (e.g. bone formation rate) features. here, we propose a deep learning approach to tackle this challenge by creating a model that utilizes the complete content of x-ray attenuation images. we developed a µct image-based method enabling an end-to-end age prediction with high accuracy, that allows identifying bone subregions relevant for this classification. we utilized the developed method to study skeletal tissue maturation and the localized rejuvenation effects of in vivo dynamic controlled loading on mouse bone tissue age."
"the objective of cervical segmentation is to divide a cell into two areas, i.e., the whole cell which consists of the cytoplasm and nucleus, and the background. sample images from the herlev data set at every phase in the mask-rcnn segmentation are shown in figure 6 . the original images were masked with a white color for cytoplasm and nuclei, and black for the background."
"this work proposes a method of cervical cell segmentation and classification. the herlev pap smear dataset was used for testing. first, we employed the mask r-cnn segmentation algorithm to partition the cell regions. second, by classifying the segments detected from the first phase with a smaller visual geometry group network, we identified the whole cell areas. to fully utilize the spatial information and prior knowledge in mask r-cnn, we use restnet10 as the network backbone. in this paper, we use two types of performance measures, i.e., segmentation and classification performance. we summarize the performance of our segmentation using precision, recall, zsi and specificity, whereas the classification performance is evaluated using f1 score, accuracy, sensitivity, specificity and h-mean."
"instance segmentation algorithms attempt to partition the image into meaningful parts and associate every pixel in an input image with a class label (e.g., person, road, car, bus) [cit] . while object detection produces a bounding box, instance segmentation produces a pixel-wise mask for each individual object. however, instance segmentation does not require every pixel in an image to be associated with a label. instance segmentation can be solved using two steps, i.e., performing object detection to draw bounding boxes around each instance of a class and then performing semantic segmentation on each of the bounding boxes [cit] ."
"in total, 79 µct image sets of both proximal tibiae and fibulae were collected during two weeks of tissue maturation (right limb) and adaptation (left limb). both limbs, including the tibia and fibula, were scanned and combined within one imaging procedure (cf. fig. 1b ). in vivo µct was performed at an isotropic voxel size of 10.5 µm (vivact40, scanco medical, switzerland; 55 kvp, 145 ma, 600 ms integration time, no frame averaging). the mice were scanned starting from the growth plate and was extended for 432 slices (4536 µm) in the distal direction. to prevent motion artifacts and obtain reproducible scan regions and bone orientations, mice were anesthetized and kept during the scans in a fixed position using a custom-made mouse bed. in time intervals of 5 days between imaging sessions, 4 sets of images were acquired. two weeks of aging in mice are approximately equal to one year of aging in humans [cit] . datasets were previously morphometrically analyzed using formation and resorption dynamics analysis software [cit] . furthermore, previous analysis showed that repeated radiation (4 scans) did not effect bone microstructure [cit] . the scanner was calibrated weekly against a hydroxyapative (ha) mineral phantom; and monthly for determining in-plane spatial resolution. animal experiments were carried out according to the policies and procedures approved by the local legal representative (lageso berlin, g0333/09)."
the advantage of our method is that we do not need complex pre-processing steps since feature selection is conducted by the mask r-cnn algorithm. the limitation of our work is the need for higher processing power compared to the other methods. future study should focus on the use of a deeper network to improve the performance results.
"for 100 randomly selected images of the validation set (21, 29, 26 and 24 images of days 0, 5, 10 and 15, respectively), the network correctly predicted all classes except for day 10, where 96% were assigned correctly to day 10 and 4% were assigned to day 15. the resulting confusion matrix for comparison of predicted and true age of the images is almost completely diagonal (fig. 4c ). in the test set, 7 out of 8 images were correctly predicted (fig. 4d) . only one image of day 15 was wrongly classified as day 0. in the group of images of the left tibia acquired at day 0, 19 out of 20 images were predicted correctly, only one image was wrongly classified as day 5, resulting in 95% accuracy (fig. 4e ). with these accuracy values above 95% in all evaluations, we consider our network performing sufficient for age-prediction."
"in this study, there are two kinds of performance measurements, i.e. segmentation and classification performance. we summarize the performance of our segmentation using precision, recall, a zijdenbos similarity index (zsi) and specificity, whereas the performance of the classification is evaluated using f1 score, accuracy, sensitivity, specificity, and h-mean. the prediction results obtained from the confusion matrix include:"
"a set of 104 3d images were gaussian filtered and binarized using a global threshold of 273/1000 (456 mg ha/cc), which was determined based on the grey value histogram of the whole roi. fibula and tibia were manually separated, automated segmentation was performed to separate trabecular and cortical bone regions of tibiae, as described earlier [cit] . total tibia bone volume (tibia bv, µm 3 ), tibia trabecular bone volume (tibia tb.bv, µm 3 ), tibia cortical bone volume (tibia ct.bv, µm"
"the problem with the original r-cnn approach is that it is still incredibly slow. furthermore, we are not actually learning to localize via deep neural network, instead, we are leaving the localization to the selective search algorithm. r-cnn only classifies the roi once it has been determined as ''interesting'' and ''worth examining'' by the region proposal algorithm, which is selective search."
"the training phase in segmentation and classification has a different preprocessing scheme. in the segmentation stage, preprocessing begins by separating the image data of the cervical cell from its mask. in the case of the herlev dataset that we use, the original image and mask data are still mixed in one folder which corresponds to the cancer class name. this collection of images is read based on the file name pattern and is then separated into only two types of images, namely the original image of the cervical cell and the mask. when the preprocessing application finds that what is being read is a mask image, the image will be converted into a binary image, that is, white for pixels which are a part of the cervical cells (a combination of cell nuclei and cytoplasm) and black for the other pixels. the original image of the cervical cell and its binary mask image is then resized into 200 pixels with a length that is proportionally adjusted based on the new width. the two groups of images are ready for further processing, namely network training using mask r-cnn."
"the network of the fast r-cnn comprises the following phases: (1) use an image and its bounding box as the inputs; (2) extract the feature map; (3) obtain the roi feature vector by applying roi pooling; (4) for each region proposal, calculate the bounding box location and the class label prediction using two fully connected layers."
"datasets were separated into three groups: 1) a training and validation set, 2) a test set to further eliminate the possibility of overfitting of the trained model, and 3) an application set. the training and validation sets consists of 71 images from 18 mice at all time points of the right control tibia. the test set contains 8 images of two randomly selected mice separated from the training and validation set (2 image per time point). the application set contains 78 images of the left loaded tibia and fibula."
"we combined an experimental study with longitudinal imaging and a deep learning framework. this allowed for a bone (tissue) age prediction and an identification of bone sites that primarily contribute to age classification. thus for the first time, we could directly track short-time aging in bone in a temporal manner. by quantitatively analyzing saliency maps of learned features, we could show that the metaphyseal parts closest and most distant to the growth plate are highly contributing to the temporal age information encoded in bone images during tissue maturation. we could further show that loading triggers dynamic processes leading to a younger appearance of the bone. more specifically we could temporally quantify these rejuvenating effects, as bone receiving 15 days of loading treatment was classified 5 days younger than the contra-lateral internal control. we demonstrated that our loading regime induces structural correspondence between younger and older tibiae, while in fibulae, despite causing (re)modeling, no rejuvenation effect could be detected. one possible biological interpretation of these findings is that loading recovers the age-related bone loss in the tibia -therefore it rejuvenates, whereas the fibula, which is at the age investigated in the present study does not incur bone loss, and therefore is adapting to the loading in a non-physiological manner (in terms of rejuvenation-related strengthening through bone (re)modeling). these findings and the introduced method provide an ideal framework to further improve our understanding of skeletal aging in mice as well as in humans. it further demonstrates that machine-learning based characterization can help to better monitor and understand dynamic changes in bones due to aging and disease and may help to optimize treatments for bone disease such as age-related bone loss and osteoporosis. of 0.27 (fig. a.8a-b gray line) . while, baam, network 1, network 2 and network 3 with depth of 7, 7, 7 and 10 respectively have similarly the best performances ( accuracy: fig. a.8a-b black, green, red and yellow lines respectively) . therefore, after a certain depth, no extra performance is gained. the reached accuracy and loss values for all the networks after 7000 training iterations is shown in table a next, comparing the prediction results of different networks for validation set based on their confusion matrices further demonstrates that networks 1-4 (fig. a.9a-d) have similar good performance and network 5 (fig. a.9e ) fails to predict day 5 images correctly. it is further seen that baam (fig. a.9f ) provides the most diagonal confusion matrix hence is the best performing network."
"bones receiving additional loading were estimated to be younger. already after day 5 of loading, predicted age starts to diverge from actual age, which might suggest a restructuring in response due to the new local loading conditions. from day 10 onward, the bones appear to be significantly younger (4 and 5 days younger after 10 and 15 days of loading, respectively). this might reflect an increase in bone strength after restructuring. however, this needs to be investigated in detail by analyzing orientation of individual trabeculae or in-silico modeling of bone strength. previous studies showed in adult c57bl/6 mice adaptive adjustments in the shape of formation and resorption sites at trabecular [cit] and cortical sites [cit], mineralization dynamics [cit] as well as material properties on a macro-and micro-scale [cit] in response to loading. these adaptations have been shown to differ between different bone sites, such as endocortical and periosteal surface [cit] or at metaphyseal versus diaphyseal sites [cit], leading to bone shape adaptation, as e.g. second moment of inertia at proximal metaphysis changes with loading in 22 week old mice [cit] . all this information is potentially analyzed in a combined manner by the deep neural network. here, we established a link to bone volume changes and found a significant correlation between predicted age and cortical as well as trabecular bone volume. future studies will investigate further correlations, e.g., by quantifying structural changes in sites identified as age-determining."
"we implemented two classification scenarios i.e. 2-class and 7-class classification problems. figure 6 illustrates the training process of our vgg-like network during its 250 epochs for the binary classification problem in one of its folds. the training accuracy is quite stable while the validation accuracy sometimes drops in the middle of a full epoch. therefore, we train the network in hundreds of epochs without an early stopping mechanism to reduce overfitting. table 4 shows that our proposed method for the binary classification problem (normal and abnormal) achieves high performance results with low standard deviation in all metrics for 250 epochs, i.e. 96.5% f1 score, 98.1% accuracy, 96.7% sensitivity, 98.6% specificity, and 97.7% h-mean. the confusion matrices on the testing dataset and on all the datasets confirm this claim, as shown in table 5 and table 6, respectively. from all the datasets, as shown in table 6, only one instance of abnormal cells was misclassified as normal and three instances of normal cells were misclassified as abnormal. table 7 details the confusion matrix of the testing dataset (20% of all datasets) in 7-class classification. the classification report as shown in table 8 achieves 94% f1 score of the micro average, 94% f1 score of the macro average, and 95% weighted average. table 9 details the confusion matrix of all the datasets (917 data) in 7-class classification. the classification report, as shown in table 10, achieves 99% f1 score of the micro average, 99% f1 score of the macro average, and 99% weighted average. figure 7 shows that the 7-class problem network training suffers the same issue as that of the binary classification problem, namely the validation accuracy sometimes drops significantly in the middle of its full epoch training. table 11 shows that the same proposed vgg-like network can also address the 7-classification problem without suffering much loss to the binary classification. the average results of classification performance yield a high accuracy of 95.9%, high sensitivity of 96.2%, high specificity of 99.3%, and high h-mean of 97.7%. the results detailed in table 9 show the confusion matrix of all images in the herlev dataset. the most misclassified cell type is moderate dysplastic with 3 instances (wrongly) predicted as mild/light dysplastic which is still in the same category as abnormal cells."
"accuracy refers to a classifier being correctly categorized in a two-class issue, i.e., normal or abnormal, whereas in the seven-class problem, accuracy refers to a classifier being correctly classified as carcinoma insitu, mild dysplasia, moderate dysplasia, columnar, intermediate squamous, superficial squamous, or severe dysplasia. sensitivity denotes that a classifier correctly classifies abnormal data as abnormal (true positive). specificity denotes that a classifier correctly classifies normal data as normal (true negative)."
"in the segmentation training stage as shown in figure 1 (a), transfer learning is applied on mask r-cnn weights trained using the coco dataset. the coco dataset has 2,500,000 labeled instances in 328,000 images and contains 91 common object categories with 82 of these having more than 5,000 labeled instances [cit] . in this proposed method, the purpose of segmentation is to isolate the cervical cell area from its surroundings. the segmented area of the cervical cell covers both nuclei and cytoplasm. the cytoplasm can influence how a cervical cell is classified."
"the rest of this paper is structured as follows. section 2 overviews the related works on cervical cell segmentation and classification; section 3 discusses the materials and describes the methods used to segment and classify cervical cells. the experiment analysis and evaluation of segmentation and classification is given in section 4 and a discussion is presented in section 5. finally, this study concludes in section 6."
"additionally, we show a link between the age prediction dynamics to age-related trabecular bone loss occurring between the age of 26 and 28 weeks [cit], which is in accordance with other studies reporting a loss in trabecular bone volume in c57bl/6 mice from 26 weeks onward [cit] . therefore, these mice are expected to be in a phase of starting trabecular bone loss. however, in the proximal fibula, we observed an increase of cortical bone volume in the same mice between 19 to 22 weeks [cit] . unfortunately, this and other studies investigated the fibula bone volume changes only in young mice during skeletal maturation [cit] . in adult humans, higher deterioration of the tibia than the fibula with aging has been reported [cit] . here we detected no significant changes in the fibula bone volume, therefore also no correlations between age prediction and fibula bone volume. however, it has to be taken into consideration, that our volume of interest was centered on the proximal tibia."
"manual cell screening often results in large variations in the quality of specimens, such as the uneven distribution of the cellular material that can lead to dense clumps which light cannot penetrate whereas other parts of the specimen may have many overlapping cells which hinders an accurate interpretation. moreover, a manual visual examination is time consuming and the analysis and classification of hundreds or thousands of cells can be inaccurate due to human error. when cell examination for abnormality is carried out by a computer, the cell must be scanned at high resolution to reliably extract the features. due to size and shape variations of normal and abnormal cells, accurate cell segmentation and classification is crucial to differentiate between normal and abnormal cells."
"we implemented the algorithm in python and performed all of the experiments using nvidia k80s 12gb, linux operating system, 4 virtual cpus and 61 gb memory. we trained the mask-rcnn using resnet101 as a backbone architecture for 40 epochs using a learning momentum of 0.9, a learning rate of 0.001, and weights decayed by 0.0001."
"the objective of this work is to develop a method to segment whole cervical cells, both single and overlapping, from conventional pap smear images, and then classify them to identify normal and abnormal cells. the proposed method comprises two steps. the first stage partitions the cell regions using mask r-cnn segmentation. the second stage defines the whole cell area (nucleus and cytoplasm) by classifying the segments from the initial stage. the classification in the second phase includes a training and testing phase as shown in figure 1 . we employ mask r-cnn in the proposed segmentation process and use resnet10 to fully utilize the spatial information and prior knowledge as the backbone of the mask r-cnn. the primary concept of mask r-cnn is to segment and build pixel masks for each image item automatically. we employ a smaller vgg-like net to classify the segmentation results, which is inspired by the family of vgg networks."
"feature-maps are down-sampled after the 2 nd and 4 th convolutional layer with a window size and stride equal to 2. the kernels in all convolutional layers are 3 * 3 with a stride of 1. in the consecutive convolutional layers, 4, 8, 16 and 32 feature maps are computed, respectively. we flatten the activation of the last convolutional layer into a vector and pass it to the ending fully connected layer with 32 and 4 features from which the last one 6 represents the 4 age classes. at last, a softmax function is applied on the flattened layer to calculate the probability distribution for each age class."
"three svm-based approaches (standard svm, svm combined with rfe algorithm, and svm combined with the pca algorithm) are used to classify the cervical cancer dataset from the repository of university of california at irvine [cit] . nucleus and cytoplasm segmentation and classification using multi-class svm classifiers such as polynomial svms, quadratic svms, gaussian rbf svms, and linear svms resulted in 95% accuracy [cit] . svms were also used to separate the nucleus from the cervical smear model with 95.134% precision for adaptive segmentation based on the gvf snake model [cit] . in order to improve the classification performance, the artifacts were removed from the cytology images in the bethesda system dataset using an svm, resulting in a true classification of normal and abnormal cells of 85.19% and 88.1% respectively [cit] . using ultra-large cervical histological digital images, a combination of svms and the block-based segmentation technique utilizes robust texture feature vectors to enhance classification efficiency for cervical intraepithelial neoplasia (cin) diagnosis [cit] ."
"by passing an image through the convolutional layers, feature-maps of the image are produced at which the position of the encrypted patterns in kernels are accentuated. this leads to extraction of hidden features of the structure. the deeper the image goes through the network, the more complicated patterns are recognized by the network to perform a classification. at each convolutional layer, the feature map extraction is performed by activating the neurons with the rectified linear function and adding a set of bias terms. these weights and bias values are optimized at each training iteration to provide a higher accuracy."
"in this study, the whole pap smear cell is segmented and classified using deep learning. the evaluation was carried out on the herlev pap smear dataset [cit] . mask r-cnn was used in the segmentation process. a cell image is segmented into cell (a combination of nucleus and cytoplasm) and background. mask r-cnn, an extension of faster r-cnn, is a well-known method for tackling the issue of instance segmentation by predicting a segmentation mask pixel-to-pixel for each region of interest (roi). mask r-cnn implementation is simple and requires only a small computational overhead, therefore quick experimentation is possible. the segmented whole cell (nucleus and cytoplasm) regions from the segmentation step are classified into a 2-class problem (normal and abnormal) and a 7-class problem (superficial squamous, intermediate squamous, columnar, mild dysplasia, moderate squamous, severe dysplasia, carcinoma in situ) using a smaller visual geometry group-like network (vgg-like net)."
"preclinical studies using micro-computed tomography (µct) aiming to assess bone maturation and (re)modeling have focused on selectively extracting mechanical or morphological features such as mineralization [cit] or bone volume [cit] and their alterations [cit] . although these approaches decode certain aspects of structural changes in bone, they neglect the underlying interplay and concurrency of (re)modeling and (de)mineralization. the measures extracted from these properties are selective and therefore not sufficient to predict fracture in diseases such as osteoporosis. to provide more precise descriptions of the disease phenotype, the diverse manifestations must be captured allowing one to distinguish healthy bones from diseased ones and young bones from aged ones to define disease onset and progression into sub-classes. this would permit a much more precise understanding of bone quality, as well as a better prediction of fracture risk and treatment outcome."
"compared to dnn developed for skeletal bone age assessment in pediatrics [cit], we therefore went further towards a prognostic tool. saliency maps were previously suggested to qualitatively visualize the importance of different bones in maturation of pediatric hands [cit] . here, we show, for the first time, a quantitative analysis of the distribution of learned features by employing saliency maps [cit] . this allows to identify the localization of agerelevant information in bone. future investigations may analyze the dynamics of aging and treatment-induced regeneration in a site-specific manner."
"(1) as far as we know, this work is the first to implement mask r-cnn and the transfer learning technique to segment the whole cervical cell."
"there are three primary goals of object detection [cit] i.e., given an input image to obtain 1) a list of bounding boxes for each object in the image, 2) a class label associated with each bounding box and 3) the confidence score associated with each bounding box and class label. instance segmentation takes object detection a step further. instead of predicting a bounding box for each object in an image, we now want to predict a mask for each object, giving us a pixel-wise segmentation of the object rather than a coarse, perhaps even unreliable bounding box."
"to study potential rejuvenation effects of mechanical loading on bones, 78 images of bones subjected to in vivo loading are analyzed (application set). at day 0, 95% of bones were classified with their actual age. for the loaded bones at day 5, 10, and 15, predicted age differed noticeably from actual age (fig. 6a) . after 5 days of loading, 47% of the images were classified as being 5 days older, 26% as being 5 days younger, and only 16% were identified with their actual age. after 10 days of loading, 55% of the bones were classified as younger than their actual age (30% as day 0 and 25% as day 5). a total of 35% were identified with their actual age and 10% were classified as to be older. after 15 days of loading, 74% of the bones were classified as younger than their actual age. for 5%, bamm predicted that the images belong to mice that were 15 days younger, 11% appeared to be 10 days younger and 58% were classified as 5 days younger than their actual age. only 26% of the bones were classified by its actual age, i.e., day 15."
"the herlev pap smear dataset collected by herlev university hospital (denmark) and the technical university of denmark [cit] was used to evaluate the proposed framework. the dataset consists of 917 images, categorized manually by qualified cytotechnicians and physicians into 7 classes as outlined in table 1 ."
"for our proposed method, we use a more compact version of vggnet. we design a network architecture with a total of 7 layers and with a convolution filter channel value that is smaller (a maximum of 128 channels) than the original version of vggnet. we do this to save computing costs and speed up the calculation process."
"a customized dnn consisting of 7 layers with four convolutional, two pooling and one fully connected layer (fig. 1f ) was designed. the output layer consists of four classes: day 0, day 5, day 10, day 15. its performance compared to other possible architectures was evaluated (supplemental data appendix a)."
"an input image is presented to the network and its features are extracted via the pre-trained cnn (i.e., the base network). these features, in parallel, are sent to two different components of the faster r-cnn architecture. the first component, the rpn, is used to determine where in an image a potential object could be. at this point, we do not know what the object is, just that there is potentially an object at a certain location in the image. the proposed bounding box rois are based on the roi pooling module of the network along with the features extracted in the previous step. roi pooling is used to extract fixed-size windows of features which are then passed into two fully connected layers (one for the class labels and one for the bounding box coordinates) to obtain our final localizations. in essence, we now place anchors spaced uniformly across the whole image at varying scales and aspect ratios. the rpn will then examine these anchors and output a set of proposals as to where it is possible an object exists. in this faster r-cnn, the complete object detection pipeline which takes place inside the network is: (1) region proposal; (2) feature extraction; (3) computing the bounding box coordinates of the objects; and (4) providing class labels for each bounding box."
"here, we described a method for estimating bone age as a surrogate for bone quality in mice imaged with µct. our method can be rapidly translated to clinical applications by examining clinical µct (eg. hr-pqct at a voxel size of 61 microns), which is increasingly used in clinical trials to investigate vbmd and microstructural changes in response to pharmacological treatments [cit] ."
"the aim of applying data augmentation is to increase the generalizability of the model which can increase the dataset size and classification accuracy while preventing overfitting [cit] . in this study, data augmentation is used both in the segmentation training phase and the classification training phase. we used several geometric transformation methods on the herlev dataset for data augmentation, i.e. top-down translation, left-right translation, horizontal reflection, vertical reflection and rotation. for each training data image, the application will select randomly what kind of geometric transformations will be applied to the image. figure 2 shows the augmented data results for classification using 30-degree rotations and 5 pixels of translation applied on the herlev dataset."
"once we have our proposed locations, we crop each of them individually from the input image and apply transfer learning via feature extraction. r-cnn utilizes feature extraction to enable a downstream classifier to learn more discriminating patterns from these cnn features. the fourth and final step is to train a series of svms on top of these extracted features for each class."
"bone is a hierarchical, dynamic, living tissue whose primary function is mechanical integrity, providing protection to internal organs and enabling mobility. internal and external stimuli cause continuous (re)modeling making bone a highly dynamic structure. both, bone stiffness and strength depend on properties such as mass and shape, the distribution of mass (geometry and architecture), micro-architecture and microscopic material property distribution [cit] . human and animal studies show that skeletal maturation and aging affect both, bone micro-architecture [cit] and tissue material properties [cit] . formation and resorption dynamics in trabecular [cit] and cortical bone [cit] are altered with aging in a site-specific manner [cit] . as a result, with increasing age a net bone loss occurs [cit], often resulting in osteoporosis and a subsequent increase in fragility fracture risk [cit] . the rules governing age-related alterations in bone composition, organization, and elasticity across structural hierarchies are, however, to date not completely understood."
"the trained baam network (after last training and validation step) applied to the only physiological loaded right tibia (each time point) is further evaluated to identify key regions and features describing the age of the bones. to determine the regions in the images that the network is focusing on for age prediction, saliency maps are calculated. respectively, a backpropagation is calculated for computing the vanilla gradients [cit] . the loss gradient is additionally backpropagated to the input data layer. by taking the l1-norm of the loss gradient of the input layer, the resulting heat map intuitively represents the importance of each pixel for age prediction. these maps convey the locations in the image at which the network focuses to predict the age of the bone. at last, saliency maps are normalized to a [0 − 1] range to enable comparability between different images."
"as seen in figure 5, the image source column is the cervical cell images taken from the herlev dataset as is, without any image processing. the original mask is converted from the ground truth mask (from color to a binary image) provided in the original herlev dataset. this converted mask will be used to train the mask r-cnn and to measure the quality of our network. the predicted mask is the binary mask generated by the trained mask-rcnn while the overlaid image shows the area from the image source which is predicted as the cell area and will be fed to the vgg-like network."
"to conclude, loading seems to change the appearance of bones towards a younger age. in our study, this effect is greatest in tibiae than in fibulae and mainly manifested in the trabecular region, which is the compartment most affected by bone loss in early osteoporosis [cit] . since fragility fractures in relatively young individuals are mainly vertebral compression fractures, therefore \"trabecular fractures\" [cit], an adaptation towards a younger trabecular bone might be in combination with an macroscopic adapted cortical shell contributing to reducing fracture risk in early osteoporosis."
"to determine the spatial localization of attention of the network in the process of age assessment, six subregions were defined within the proximal tibia and fibula. therefore, first the tibia and fibula were manually segmented. next, these two labels were further divided into 3 regions with the same heights (0.56 mm), o.e., proximal (t1, f1), middle (t2, f2), 7"
"cancer is a life-threatening disease and has become a major burden worldwide. global cancer data reveals that cervical cancer is the fourth most prevalent disease among females, with an approximately 90% fatality rate in underdeveloped and developing nations due to the absence of public knowledge of its causes and impacts [cit] . fortunately, this lethal disease can be detected by the regular pap smear testing of the cervical cells. the cell samples which are collected at the outer opening of the cervix are placed on a glass tube and stained by a pathologist for examination under a microscope to determine if there are any defects/abnormalities that indicate a pre-cancerous phase [cit] ."
"similar to the original r-cnn, the fast r-cnn algorithm [cit] still utilizes selective search to obtain region proposals, but a novel contribution, region of interest (roi) pooling, is made. in this new approach, fast r-cnn applies the cnn to all the input images and extracts a feature map from it. roi pooling works by extracting a fixed-size window from the feature map and then passing it into a set of fullyconnected layers to obtain the output label for the roi."
"at last, the age prediction performance of the networks on the test set demonstrates the superiority of baam to other designed networks. while, networks 1-5 achieve 5, 5, 6, 5 and 5 correct predictions out of 8 images of test set respectively (fig. a.10a-e), baam reaches the highest number of correct age prediction with 7 out 8 (fig. a.10f )."
"to investigate, if the observed restructuring leads to a younger appearance of the bones, we compared the distribution of attention on loaded bones after 15 days, classified younger, to the control bones of the classified ages. the similarities of distribution of attention, together with the comparability of correlations in 1) predicted age and trabecular bone volume in loaded bones and 2) real age and trabecular bone volume in control limbs, lets us speculate, that loading induces tibial restructuring towards a younger appearing bone. this rejuvenation is strongly manifested in the dynamic trabecular structure. this is also the only bone compartment, where we detected an age-related short-term loss of bone volume in the control limb and a constant bone volume with time in the loaded limb. this finding is further supported by previous studies investigating trabecular bone gain in response to loading [cit] by comparing loaded proximal tibia of adult mice with internal nonloaded control limbs."
"raw data were reconstructed using standard filtered backprojection implemented in the software of the scanner. resulting images were cropped to contain the tibia and fibula in independent image sets (fig. 1c) . these images varied considerably in size, location and orientation of the bone. therefore, a preprocessing pipeline that standardizes the images is essential for training a deep learning model. the first step of this pipeline normalizes the sizes of the input images, in which, the algorithm determines the maximum extension in lateral-medial and anterior-posterior direction as well as the most distal bone part inside the image. second, preserving their aspect ratios, a padding of the images in lateral-medial and anterior-posterior direction is performed (fig. 1d) . therefore, a stripe of additional voxels with the same gray values is placed at the border of the image on the padded plane perpendicular to the padding direction. next, images are cropped to the minimum z-stack number (of all the images) from distal direction. the 3d images are projected in mediallateral direction onto the anterior-posterior / distal-proximal plane ensuing a 2d image, which, due to the medial-lateral symmetry, nullifies the symmetry-related skeletal difference between the left and right tibia and fibula. after pre-processing, all 2d images are 733 (y) by 161 (z) pixels in size (fig. 1e) ."
"our proposed segmentation using mask r-cnn produces the best average performance, i.e. 0.92±0.06 precision, 0.91±0.05 recall and 0.91±0.04 zsi and 0.83±0.10 specificity for all cell types with a low standard deviation. only the normal columnar type produces a lower performance result of below 0.90. we implemented two classification scenarios i.e. 2-class and 7-class classification problems. our proposed method for the binary classification problem (normal and abnormal) yields high performance results with a low standard deviation for all metrics for 250 epochs, i.e. 96.5% f1 score, 98.1% accuracy, 96.7% sensitivity, 98.6% specificity, and 97.7% hmean, whereas the classification performance for the 7-class problem yields a high accuracy of 95.9%, high sensitivity of 96.2%, high specificity of 99.3%, and high h-mean of 97.7%."
"current methods assessing bone maturation and aging mainly focus on specific dynamic features [cit] . recently developed machine learning methods, however, consume the complete image content while performing a classification task. recently, first applications to preclinical and clinical image data of bone showed, that a machine learning classification into healthy and disease state is achievable [cit] . going from there the next step, we developed a framework allowing for a tracking of dynamic bone changes in physiological aging/maturation conditions. additionally, by training with physiologically loaded bones and applying the network on in vivo loaded bones, we demonstrate loading causes bone (re)modeling and we further analyzed the dynamics of this bone (re)modeling, without the need for transfer learning [cit] ."
"the bone age prediction model is further applied to the application set (fig. 3) . the predicted age of each bone at each time point is compared to the actual bone age to investigate rejuvenating effects of load-induced bone (re)modeling on skeletal age. rejuvenation is defined as the delta age predicted at day 0 and day x divided by the delta time between day 0 and day x. key regions and features describing the estimated rejuvenated age of the bones are determined (saliency maps). figure 3 : first, the network is trained on physiologically loaded bones to predict age (top). second, the trained network is applied on images of bones treated with additional loading to investigate the rejuvenation effects of treatment (bottom)."
"given the fact that osteoporosis causes worldwide more than 8.9 million fractures per year [cit], it is essential to develop a precise and comprehensive analysis of phenotypic changes and abnormalities at all relevant length scales. assessing the onset of osteoporosis and disease progression is therefore challenging. within clinical practice, dual energy x-ray absorptiometry (dxa) and biochemical markers remain the standard methods of monitoring osteoporotic patients receiving pharmacological treatments. the t-score is derived from measurements of the areal bone mineral density (abmd), which is obtained by dxa [cit] . dxa is a useful clinical tool, but has several limitations including restriction to a two-dimensional image, lack of distinction between trabecular and cortical bone, lack of information on bone microarchitecture, difficulties in edge detection and projection artefacts. additionally, the predictive ability of this method is low [cit] with less than half of all nonvertebral fractures occurring in postmenopausal women having an osteoporotic t-score [cit] . biochemical markers are indirect indicators of the rates of formation and resorption of bone and give no insight into its quality or mechanical properties. furthermore, like all biochemical markers they are subject to pre-analytical, analytical and post-analytical sources of variability and the results may be affected by a range of non-skeletal conditions. high-resolution peripheral quantitative computed tomography (hr-pqct) is emerging as a powerful non-invasive bone imaging modality capable of assessing volumetric bmd, microarchitecture and strength, and distinguishing cancellous and cortical bone. additionally, micro-finite element and homogenized finite element models based on hr-pqct imaging are increasingly used to predict bone stiffness and strength [cit] . the bone microarchitecture international consortium (bomic), which combined individual-level prospective data from eight cohorts (7254 individuals, mean age: 69 ± 9 years), recently reported that hr-pqct parameters improved fracture prediction beyond femoral neck abmd or fracture risk assessment tool (frax) scores alone [cit] ."
"whole cell segmentation is a more difficult problem than nucleus segmentation. accurate whole cell segmentation is paramount to achieving high accuracy in classification performance. the advantages of applying mask r-cnn as our segmentation method compared to the other aforementioned methods are: (1) it is simple, flexible, and fast to train and does not need complex algorithms or parameter tuning; (2) it selects the features automatically; (3) it is conceptually simple and does not need complex pre-processing steps; and (4) it is flexible and can leverage different architectures such as resnet, vgg, squeezenet, and mobilenet as its backbone. the advantages of applying vgg-like net are: (1) our network is deep enough to obtain high accuracy; (2) it is faster for training; (3) it is possible to decrease the size of the model produced by the segmentation training stage; and (4) it is feasible for deployment on a mobile device and potentially increase frame per second (fps) throughput as well. in general, the results show that our work using mask r-cnn and a deep cnn classifier with a smaller vggnet is effective."
"3) interval observers for sensor faults: similarly, in (3) letting f ia be the identity matrix, the models of the plant under sensor faults can be obtained. the interval observer corresponding to the j s -th system sensor mode can be expressed aŝ"
"steps 12 and 13, exemplify a websocket client query requesting for available virtual objects the dedicated actor response with appropriate data. the process of the actor querying the database is not shown in figure 7 . then, in step 14, the websocket client sends a message indicating what virtual objects to watch or subscribe to. once the dedicated actor receives this information, it subscribes to the corresponding virtual object topics to receive updates of them in step 15. variables in square brackets \"[]\" indicate a list of those variables."
"2) interval observers for actuator faults: in (3), letting g is be the identity matrix, one can obtain the models of the plant under actuator faults. the interval observer corresponding to the model of the j a -th system actuator mode is designed aŝ"
"on the other hand, we were interested in finding a programming model that facilitated the creation of distributed, flexible, and self-healing systems, mandatory features for the woe. for this reason, the actor model [cit] seemed to be the ideal model. this model or paradigm of programming facilitates the creation of systems with those requirements and, in addition, thanks to the fact that it allows concurrency, it is capable of taking advantage of the different device computing cores [cit] ."
"the websocket client sends a message to the websocket actorsystem, concretely to an actor named websocket master, to initiate a registration process that starts at step 8 and ends at step 10. if successful, the websocket master deploys a new actor which will be responsible for handling the messages sent over the previously established websocket connection (wsconn). the subscription process ends at step 11 when the dedicated actor sends and ack (\"ok\" message) meaning that the registration process was successful. as with the mqtt registration process, the associated information is saved into the database."
"the contribution of this paper is twofold. first, it extends interval observer-based approaches to the case of fi, which implies that interval observers can independently implement fdi without the help of other fi techniques such as the fault signature matrices. second, it establishes a set of fdi conditions to guarantee interval observer-based fdi."
"the dataport-pecanstreet project [cit] provides a huge dataset of smart meter data. [cit] . the dataset contains data from 633 households and records of energy consumption and production at 15 min interval. each record contains information about a maximum of 66 smart appliances per household but each household has no more than 15 smart appliances generating smart meter readings. a total of 124,331,328 individual smart meter records are available and 28,257,120 contain energy usage readings. for this reason, the payload size has been fixed to 4 bytes to fit a smart meter reading in each message and load tests are performed until total transmitted messages per burst resemble a scenario in which each household owns 15 smart appliances. as the experiments will be performed in an emulation setting and the goal is to perform stress tests, the interval of 15 min per smart meter reading has not been taken into account. we also perform 3 bursts where each device sends a message. the interval between bursts is of 1 s. bursts are performed in order to analyze how actor mailboxes behave. table 3 summarizes the description of the experiment. figure 8 shows the results. as shown in figure 8, the mailboxes of the actors tend to saturate at each sample, but the timedelta of the first message is less than 500 ms. a high percentage of messages are affected by the queue size, as the mean shape is closer to the maximum. increasing the burst interval to 5 s, smooths the timedelta mean as shown in figure 9 . this means that a higher percentage of timedeltas compared with figure 8 is closer to the minimum. it also shows that the maximum is 1.3 s approximately, compared to 5 s in figure 8 . the results obtained conform to time needs in non-critical smart grid applications [cit] . increasing the burst interval to 5 s, smooths the timedelta mean as shown in figure 9 . this means that a higher percentage of timedeltas compared with figure 8 is closer to the minimum. it also shows that the maximum is 1.3 s approximately, compared to 5 s in figure 8 . the results obtained conform to time needs in non-critical smart grid applications [cit] ."
"for example, both coap and mqtt are currently two of the most promising standard iot protocols. coap, an open standard, was designed specifically for the iot and to be directly compatible with http [cit] . it is a protocol based on request/response through udp (user datagram protocol) packets and follows the same schemes as http, allowing the creation of rest resources. the mqtt protocol, designed by ibm, is now also an open standard, but follows a publisher/subscriber paradigm over tcp, so it becomes more complicated to establish a translation bridge between http-rest and mqtt [cit] . table 1 shows the differences and similarities between coap, http, mqtt and websocket protocols. as mentioned, the mapping between coap and http is direct, since the topology of both is request/response (req/resp). on the other hand, the topology between http and mqtt is different, so the translation between these two protocols is more difficult to achieve. another example of (almost) direct mapping is between websocket and mqtt, since the bidirectional communication of websocket can be considered a particularity of the publisher/subscriber protocol (pub/sub) of mqtt. thus, http and websocket are the two web technologies with different topologies that allow translation between iot protocols with request/response and bidirectional topologies respectively. table 1 also shows other protocols that can be part of the iot, which have more similarities with http or websocket according to their topology in the same way that occurs with coap and mqtt. in addition to the two protocols already presented in table 1 and many other proprietary and open-source protocols for iot, protocols that are more adapted to the specific needs of electric utilities are beginning to emerge, particularly in sectors where the infrastructures are becoming obsolete. however, those protocols become more difficult to adapt to the woe because: (1) its specificity reduces the interest of third parties of contributing to the exposure of the devices (or sets of them) to the wot and, therefore, the companies should invest more capital in the mapping and (2) in case it is of interest for the company to expose some of their devices, the wot adds a stack of new technologies, opening new security holes in their systems. these challenges are significant in the woe, because the systems involved cover a large number of devices that use very specific protocols adapted to their needs."
"the difference of the mean, minimum, and maximum parameters between samples at 1 s interval and 5 s interval are not abrupt, which means that reducing the interval between bursts from 5 s to 1 s does not have a great impact on performance, even with messages that have a payload equal to 1224 bytes. we conclude that our system is capable of delivering 1000 messages aggregating nine messages in their payload within a 1.42 s at an interval of 1 s between 1000 messages, which is equal to 9000 aggregated messages delivered in less than 1.5 s and at a mean rate of 600 ms. the experiments start when 2 messages are aggregated, since the aggregation of 1 message is nonsense."
"hybrid cloud data management: this module provides a data storage and processing system that intrinsically adapts to the smart grid's topology in a scalable and flexible way. also, it implements an algorithm (also referred to as orchestrator) to decide whether collected data should be stored at the private cloud or could be placed at the public cloud. such decisions are taken considering the smart functions' requirements (e.g., reliability, delay, and cybersecurity) associated with the collected data [cit] ."
"the architecture must provide abstractions so that developers can interact independently with the devices of the communication protocol, either towards the heterogeneity of the iot protocols or towards the homogeneity of the wot protocols."
"in this paper, the term web of energy is used to refer to the critical features that an architecture of the web of things must fulfill to be applied to the domain of smart grids. we highlight the requirements related to the implementation of smart functions, such as the distribution, resilience and self-healing of the system as well as the difficulties of renewing the traditional energy generation and management systems, both in terms of device and the software, which are generally interrelated. given those challenges, the use of the actor model is proposed to overcome them. this paradigm is designed specifically to perform the modeling of concurrent and distributed systems, thus directly improving the inherent characteristics of distributed systems. in this sense, a proposal of middleware architecture using this paradigm for the creation of a ubiquitous sensor network has been presented in this paper."
"although the actor model can be used to create agents and, therefore, multi-agent systems (mass), this article focuses on the exposure and use of the model per se in iot or wot. as can be observed, when an actor receives a message, it is stored in a mailbox that is specific to each actor and inaccessible to the rest of the actors. this mechanism maintains a private status and isolates an actor from the rest."
"each virtual object will be accessible from an http rest interface and, in case the features of the virtual object allow it, from a websocket link."
"the accessibility layer acts as an interface between the iot and web technologies. therefore, it is the closest layer to the heterogeneity of iot protocols, such as mqtt (message queuing telemetry transport) [cit] or coap (constrained application protocol) [cit], among others. in this layer, solutions are found in the form of a proxy or bridge between the iot and http protocols."
"additionally, since the principle of the proposed technique for actuator and sensor fdi is similar, in this paper only fdi of actuator faults is discussed in detail. however, the extension of the method to the case of sensor faults is summarized afterwards."
"to conclude the experiments, we directed our efforts to developing a web portal to visualize the data available through the http and the websocket protocol, which prototypes the woe. 1000 devices have been simulated, emitting synthetic smart meter data every 5 s. these sensors are distributed throughout the city of barcelona. figure 12 shows one of the functionalities of the system, a heat map updated every 5 s in near real-time. however, figure 12 only shows a few of those sensors because the web browser performance is rapidly decreased as more and more objects need to be painted. a possible solution could be to aggregate smart meter readings and show the aggregation value in the heat map. figure 13 shows the result of consulting the value of one of these sensors by clicking in the \"more…\" link shown in figure 12 . the chart label corresponds to the universal unique identifier (uuid) of the sensor. the values in the vertical axis correspond to the measurements obtained by the sensor. this simulated sensor reads voltage, so a \"v\" inside the parentheses is displayed. the values in the horizontal axis show the time at which the voltage reading is obtained, every 5 s. the data shown updates in near real-time. the application is designed with the ability to interact with each of these sensors, although in this initial version only the query of values is allowed. all queries are executed in near real-time and without delays, despite the large volume of data. this confirms that the design of the prototype is consistent and allows the extension of the rest of the woe."
"while the overestimations on the initial prediction of the volume of internet connections were acknowledged and duly rectified, the fact that our society is becoming increasingly connected to the internet is undeniable. this acceleration has been promoted thanks to the advances of both silicon, which makes it possible to embed increasingly smaller computing units in everyday devices, and advances in low-power wireless protocols. at the same time, internet and web applications targeted at controlling and managing internet-connected devices are being continually improved. we are being encouraged to integrate internet-connected devices into our lives, where social network services serve as the easy-to-use interface between people and devices."
"k ) denotes the residual zonotope predicted by the healthy interval observer at time instant k and 0 represents the zero vector. if (10) does not hold, it is assumed that a fault has occurred at k."
"when the system is under the i a -th actuator mode, the residual zonotopes predicted by the j a -th interval observer is defined as"
"where j denotes the j-th element in the set sequence and n represents the set of natural numbers, is a (rpi) approximation of the mrpi set of (1). moreover, as j tends to infinity, the set sequence converges to the mrpi set."
"in order to guarantee the correct and timely fi, a waiting time t is necessary after a fault is detected. this waiting time is used to delay fi process such that the incorrect fi possibilities are completely avoided. the procedure of this proposed fdi method is presented in algorithm 1."
"those new agents and components of the smart grid could become the most enriched elements due to their added information and technology features. however, the upgraded features of the end user side come at the cost of requiring more technology in order to make them usable. due to the complexity (i.e., stringent levels of service reliability and availability), magnitude (i.e., large-scale areas), and new agent profiles inherent to smart grids, practitioners have recently addressed the digital transformation of power electric networks by proposing flexible and future internet based architectures [cit] ."
"recent advances in smart grids have explored the feasibility of considering the electrical power distribution networks as a particular case of the iot. certainly, this specific domain poses appealing challenges in terms of integration, since several distinct smart devices from different vendors (wired or wireless sensors, smart meters, distributed generators, and so on), often using proprietary protocols and running at different layers, must interact to effectively deliver energy and provide a set of enhanced services and features [cit] . although the latest developments of the iot field have definitely contributed to the physical connection of such an overwhelming amount of smart devices, several issues have arisen when attempting to provide a common management and monitoring interface for the smart grid [cit] ."
"the experiment description setting 2 is similar to the experiment description setting 1 but instead of increasing the number of simulated devices, it emulates the aggregation of messages in the payload field. the experiment shows 1000 devices sending messages with payloads that aggregate messages, emulating that each device is a smart gateway that aggregates messages. table 4 summarizes the experiment description."
"moreover, new agents and components have been raised inside this new paradigm. for example, the prosumer (load producer and load consumer) as the last link in the electricity value chain is easily the most important creator of value within the smart grid. prosumers will be given a more active role in new business model generation. the most important connections for prosumers in the electricity value chain are the distributed system operator or the aggregator/retailer in addition to the energy"
"the accessibility layer acts as an interface between the iot and web technologies. therefore, it is the closest layer to the heterogeneity of iot protocols, such as mqtt (message queuing telemetry transport) [cit] or coap (constrained application protocol) [cit], among others. in this layer, solutions are found in the form of a proxy or bridge between the iot and http protocols."
"the minimum message size includes the message id, the virtual object id, the payload, the unit of measure and the time the value was sensed. with payload or value equal to 0 bytes, the minimum message size is 132 bytes. the message id and virtual object id are raw uuids (universally unique identifier) (36 bytes per uuid with hyphens) but an extra compression could be applied by encoding them to base64 (22 bytes) or base85 (20 bytes), although the latter encoding is not url-safe. the timestamp is represented as a 13-byte string. finally, the unit of measure is represented as 1 byte and the remaining bytes account for json-encoding (javascript object notation) characters. extra compression can be achieved using binary formats [cit] but it was not within the scope of the project."
"on the other hand, characteristics of scalability, discoverability, and interoperability are only partially tackled by the wot as it serves as the abstraction layer that comprises technologies that enable homogeneous communication among devices by means of http and semantic web ontologies. however, a global iot or wot, the latter being an iot empowered by web technologies, still has scalability and discoverability issues. as the number of internet-connected devices grows, the lack of global scalability and discoverability functionalities will become more evident. in this regard, a novel iot paradigm, the social internet of things (siot) is gaining momentum."
"contrary to the rapid evolution experienced in the last decade of information and communication technologies (icts), electric power distribution systems have remained exceptionally steady for a long time. therefore, the smart grid concept, the addition of a telecommunication infrastructure to the electrical domain, has enabled a plethora of new services and opportunities (e.g., accurate grid monitoring, real-time energy consumption, and customer-side generation, etc.), which are currently driving a major revolution in the energy sector [cit] . in fact, smart grids are conceived to improve traditional electric power networks in several dimensions [cit] such as information, business models and scalability, in order to meet the highest standards of power quality and thus ensuring that the electric power grid is cost effective and sustainable [cit] ."
"the satisfaction of (18) implies that the limit setȓ iaja ∞ does not contain 0. thus, one only focuses on the proof of the second part as follows. since residual zonotopes and their bounding zonotopes are determined by (13) and (14), without loss of effectiveness, the main elements used next will be these set-based dynamics."
"in this novel paradigm, communication between social virtual objects becomes mandatory and the amount of messages exchanged between devices is expected to increase. several works aim to provide example scenarios where siot can be leveraged [cit], to name a few. in this sense, we highlight [cit], where the authors use thing-to-thing social relationships to encourage communication to optimize energy usage by the heat, ventilation, air conditioning (hvac) system in their laboratory. results show that the comfort-aware hvac system ensures users' thermal comfort, while at the same time reducing energy costs with respect to static and traditional methods. an ongoing challenge for us is the study of the implications of reactive, asynchronous communication together with computation distribution properties of the actor model in a siot system, in terms of performance and how these abstractions facilitate the development of this kind of applications. further work on this subject encompasses the creation of proper simulation tools or modules to facilitate the simulation of siot enabled nodes and networks at multiple network layers (e.g., applications installed on top of constrained devices and fog or cloud applications)."
"two basic wot-enabling methodologies are considered. embedding web servers to devices, or the creation of gateways [cit] that serve as aggregators or proxies between iot protocols and web protocols. cloud solutions are also found in this layer, generally transverse to more layers of the wot, such as thingworx [cit], watson iot platform from ibm [cit], octoblu [cit] or evrythng [cit], among others. these cloud solutions offer iot-wot translation gateways."
"composition: it enables the integration between the different representations of the devices that, ultimately, allows the integration between the different functionalities of the physical devices."
"however, many devices do not have the necessary features to directly access the web through http or websocket. to be able to connect to the web, they need to use gateways that are responsible for translating the heterogeneity of the iot protocols to the homogeneity of the wot. to do this, it is necessary to establish translation bridges or mappings from iot to wot protocols and vice versa."
"since each interval observer matches one certain system mode, it means that each interval observer has different dynamical behaviors under different modes. since a fault occurrence always induces the corresponding uncertainties on dynamical behaviors of interval observers during the transition, there exist possibilities that at some time instants several residual zonotopes predicted by several different interval observers simultaneously contain 0 during the transition."
"to solve the integration of heterogeneous devices, the use of the concept of the wot [cit] has been proposed to access multiple devices using the same interface provided by web technologies, implying uniformity in communication protocols (http and websockets) and uniformity in the model of data representation and device discovery (web thing model [cit] and semantic web [cit] ). this concept is, so far, difficult to apply in real scenarios involving energy management due to the risk of creating new security vulnerabilities, the lack of devices that implement standards of the wot and the opposition of the industry of energy to include external modules or devices in their proprietary systems."
"a device which communicates via mqtt initiates a registration process with the mqtt actorsystem via the mqtt broker. as shown in steps 2 to 7, the device subscribes to a topic dedicated to configuration purposes, which includes the registration process, and sends a registration request to the mqtt master. if the registration process is successful, which is the flow shown, the mqtt master deploys a dedicated actor responsible for handling the messages sent over the configuration topic and the data topic (datatopicout). the actor also subscribes to the proxy layer to receive messages directed to the device (step 6). finally, the actor responds with an ack (\"ok\" message) to the device communicating that the registration process was successful. the virtual object that represents the device and associated information is registered in the system's database (action not shown in figure 7 ). note that the actions prepended with the prefix \"vo\" (virtual object), (e.g., vosubscribe) correspond to actions performed by a dedicated actor, thus the vosubscribe action is performed by the actor deployed by mqtt master in step 4. also, the topic or message used in a \"vo\" action contains the id of the virtual object and therefore messages and topics do not collide between virtual objects. 3."
the results encourage us to keep investigating in this direction. we acknowledge that our system and experiments do not cover all of the scalability and flexibility requirements in smart grid ict systems but we have presented the benefits of the actor model for such systems and developed an initial prototype for further research and development.
"this layer enables the composition of the different functionalities exposed by the device's representations. taking into account that each representation is accessible through the http protocol, it is easy to program a script so that the different devices act in a coordinated way. the elaboration of this idea consists of providing the web user (human) with a visual interface to compose different device relationships. solutions such as thingworx composer [cit], node-red [cit] from ibm or octoblu [cit] or ifttt [cit] exemplify the composition of physical devices (physical mashups)."
"to conclude the experiments, we directed our efforts to developing a web portal to visualize the data available through the http and the websocket protocol, which prototypes the woe. 1000 devices have been simulated, emitting synthetic smart meter data every 5 s. these sensors are distributed throughout the city of barcelona. figure 12 shows one of the functionalities of the system, a heat map updated every 5 s in near real-time. however, figure 12 only shows a few of those sensors because the web browser performance is rapidly decreased as more and more objects need to be painted. a possible solution could be to aggregate smart meter readings and show the aggregation value in the heat map."
"the paper is organized as follows: section 2 describes the related work regarding smart grids, web of things, and web of energy, detailing their value proposals and associated challenges. section 3 explains the problem of the direct integration of smart grids, web of things, and their communications protocols in order to compose the web of energy. section 4 illustrates the hybrid architecture that is proposed that covers the needs of a smart grid infrastructure and provides the advantages of integrating the smart grids' ict with the web of things, that is, the web of energy. in section 5, we conduct experiments to evaluate the performance of the prototype system and discuss the results. section 6 concludes the article. finally, section 7 discusses further work."
virtual objects space and proxy layer: this section is made up of those modules responsible for representing each physical device through a virtual object and the publisher/subscriber protocol.
"a woe prototype based on the actor model has been implemented. our goals were: (i) to address some of the key issues related to icts in smart grid systems while demonstrating the the application is designed with the ability to interact with each of these sensors, although in this initial version only the query of values is allowed. all queries are executed in near real-time and without delays, despite the large volume of data. this confirms that the design of the prototype is consistent and allows the extension of the rest of the woe."
"this layer groups all the methods that allow authentication and authorization of different actors to perform an action on the virtual representation of the device and, ultimately, on the physical device. it groups simple authentication and authorization methods from user authentication and password to the use of more advanced protocols such as api or oauth keys."
virtual objects space and proxy layer: this section is made up of those modules responsible for representing each physical device through a virtual object and the publisher/subscriber protocol.
"better performance can be achieved by aggregating messages (experiment 1). the slope of the mean function in experiment 1 is approximately 0 and the slope of the mean function in experiment 2 is positive. no messages were lost in either experiment 1 nor 2. while results in experiment 1 conform to time needs of non-critical smart grid applications, results in experiment 2 conform to some of time needs in critical smart grid applications [cit] . we also conclude that message aggregation is preferred when possible, as it reduces the overheads of the entire system."
"actorsystem via the mqtt broker. as shown in steps 2 to 7, the device subscribes to a topic dedicated to configuration purposes, which includes the registration process, and sends a registration request to the mqtt master. if the registration process is successful, which is the flow shown, the mqtt master deploys a dedicated actor responsible for handling the messages entities perform actions by sending asynchronous messages. actions between entities are represented in figure 7 by directed arrows and the associated label gives information about the action performed and the involved and exchanged information in those actions. the information shown in parentheses can be seen as variables whose name gives semantic hints about their content. the schematic is explained as follows:"
"in this section, we present an analysis of the successfully implemented prototypes that are governed by the techniques analyzed above. our goal is to analyze the real-time performance of our system and to study how it behaves under different types of load. concretely, the experiments measure the time interval from the moment the sensor data is sent by the device or smart gateway until it arrives at the websocket client."
"in order to emulate the maximum number of network hops and distributed resources to approximate a real scenario, we have used the maximum number of lab resources that are at our disposal. we have used a macbook pro to simulate a large number of devices and two virtual machines (vm). each virtual machine is allocated in a different physical computer. table 2 shows the specifications for each lab resource and the entities/services (figure 7 ) allocated in them."
"prototypes of some of the layers described above have been implemented for the proof of concept. specifically, the virtual objects space and the protocol abstraction layer have been implemented. also, a publisher/subscriber protocol for the proxy layer has been considered to communicate two remote servers. below both are detailed:"
"thus, whenever a fault occurs, after a waiting time assessed by the eigenvalues of the interval observer, (15) enters into steady state. then, the set value ofx iaja k can be sufficiently 3 close to that ofx iaja ∞, which means that x iaja ∞ can approximately describe the dynamical behaviors of the system after the waiting time."
"the remainder of this paper is organized as follows. section ii introduces the notions of zonotopes and invariant sets. section iii introduces the plant and interval observers. the fdi algorithm is presented in section iv. in section v, guaranteed fdi conditions are established. an extension of the approach for sensor faults is briefly discussed in section vi. in section vii, the examples are used to show the effectiveness of the proposed approach. in section viii, general conclusions are drawn."
"in this way, the woe could represent an opportunity for the electric companies to have virtual representations of their more flexible devices (based on software, updatable, configurable, and with the possibility of deploying new applications on top of them), enabling a low-cost distribution and management of the electric network [cit] . the woe facilitates the sharing of data from different devices (charging points for electric vehicles, smart metering, or monitoring of substations) with third parties. in addition, the use of web technologies allows the creation of multiplatform visualization tools without installation cost, being able to offer simple and usable graphic interfaces for a greater adoption for the distribution systems operator (dso) or any user interested in the consumption or production of energy (prosumer). thus, we can identify different fields related to energy management in which the woe would be very useful:"
"although [cit] were quickly lowered to 26 [cit] and 20.8 billion in the same year [cit], it is evident that the number of devices connected to the internet is increasing every day. how to access all these sensors and actuators through a uniform interface is undoubtedly one of the biggest challenges of the iot. currently, the iot is divided into self-contained areas, that is, there are proprietary solutions that help the integration of a specific set of devices, but this structure is far from the overall integration planned for the iot. for this reason, the use of existing web technologies for the global integration of devices is proposed. the basic prerequisites for enabling the iot devices on the web are two: (1) minimum capacity for data processing and (2) connectivity to the network (it is not necessary to connect directly to the internet)."
"since the prediction of interval observers and the computation of interval vectors is based on zonotopes, the discussions in the remaining of the paper are mainly based on zonotopes."
"the architectural model is depicted in detail in figure 6 . in order to transmit data to end nodes of this architecture (device/smartgateway and websocket client in figure 6 ), we have developed a communication protocol at the application layer that exploits the real-time capabilities of the proposed architecture. the protocol is depicted in figure 7 and explains the actions conducted by each entity involved in the communication process. involved in order to transmit data to end nodes of this architecture (device/smartgateway and websocket client in figure 6 ), we have developed a communication protocol at the application layer that exploits the real-time capabilities of the proposed architecture. the protocol is depicted in figure 7 and explains the actions conducted by each entity involved in the communication process. involved entities are: 1. in step 1, an actor named mqtt master that has been initially allocated in the mqtt actorsystem subscribes to a wildcard topic such as +/config/out to receive incoming information about new devices. the plus sign in the wildcard topic means that the subscription is to all topics that match a string prepended by /config/out."
"the energy service companies (escos) will play an important role in the future electricity market as energy-oriented commercial businesses. escos can be described as specialists in providing a broad range of comprehensive energy solutions, including the design and implementation of energy saving projects, energy conservation, energy infrastructure outsourcing, power generation and energy supply and risk management."
"when the plant (3) is under a sensor fault, similarly, residual zonotopes predicted by the j s -th interval observers under the i s -th system mode can be derived as"
"virtual objects: when physical devices must perform actions on other physical devices they will send the instruction against a virtual representation (virtual object or thing). the objective of this virtual representation is to increase the resources and functionalities of physical devices. we cannot provide a detailed list of added features, as they are specific to each solution. even so, our goal is to provide a method (code injection) so that these functionalities can be added dynamically. some common functionalities are related to reasoning (artificial intelligence) or caching. 5."
"although the actor model can be used to create agents and, therefore, multi-agent systems (mass), this article focuses on the exposure and use of the model per se in iot or wot."
"a woe prototype based on the actor model has been implemented. our goals were: (i) to address some of the key issues related to icts in smart grid systems while demonstrating the benefits of the actor model in such ict-smart grid use cases and (ii) to implement a prototype for further research. the smart grid can be seen as a specific internet of things application, with its own constraints and requirements. specifically, we have addressed the heterogeneity of protocols by means of the wot and some of the near real-time requirements of smart grid ict systems. we have created virtual objects, a common abstraction in iot scenarios, by means of the actor model. each virtual object is represented by at least one actor, a lightweight reactive agent. heterogeneity is addressed in the prototype system by the capacity of each implemented actor to translate heterogeneous protocols (mqtt, for example) to web protocols (http or websocket). performance tests show that the initial model meets some of the time needs of time-critical smart grid ict applications when virtual objects aggregate several smart meters (messages that aggregate messages). in such cases, the timedelta mean slope remains neutral even at bursts of 9000 aggregated messages at a 1-s interval between bursts. tests involving more devices have to be carried out in order to explore the scalability possibilities of the system. note that system is flexible in that the actors are allocated/deployed when needed. experiments also show that the system is suitable for non-time-critical smart grid traffic when messages can't be aggregated before reaching the prototype system. finally, a simple web-based interface to visualize sensor location and sensor readings in near real-time has been implemented as the wot interface to the smart grid, the web of energy."
"for each considered system mode, an interval observer is designed to match the corresponding mode. according to (11) and (12), the residual zonotope at time instant k is rewritten as"
sharing: it is in charge of preserving the privacy between the device representations. it also manages the authentication and authorization to access the representations by actors who do not own the device.
"in step 1, an actor named mqtt master that has been initially allocated in the mqtt actorsystem subscribes to a wildcard topic such as +/config/out to receive incoming information about new devices. the plus sign in the wildcard topic means that the subscription is to all topics that match a string prepended by /config/out."
"findability: it provides the discovery of the representations of the different devices, uniformly modeling the access method (through the http protocol) and establishing relations between them at the moment of their representation."
"in the woe architecture, the devices have to become virtual objects [cit] or things, although virtual objects created from aggregations of other virtual objects can also be created."
"in figure 11 a comparison between experiments 1 and 2 with the description setting where the time between bursts is 5 s is shown. the mean timedelta obtained in both experiments shows that a better performance can be achieved by aggregating messages (experiment 1). the slope of the mean function in experiment 1 is approximately 0 and the slope of the mean function in experiment 2 is positive. no messages were lost in either experiment 1 nor 2. while results in experiment 1 conform to time needs of non-critical smart grid applications, results in experiment 2 conform to some of time needs in critical smart grid applications [cit] . we also conclude that message aggregation is preferred when possible, as it reduces the overheads of the entire system."
"it is obvious that the web of things is continually extending to different application domains. nevertheless, in order to continue this trend, it is essential that the software applications involved are built following guidelines that guarantee their performance, accessibility, and high availability. in this sense, an overall development model agreed upon and approved by different authors is becoming increasingly specific."
"2) fi using interval observers: the proposed fi technique is based on a bank of interval observers and each observer is designed to match a given system mode. at each time instant, a set of residual zonotopes predicted by the bank of interval observers can be obtained. after the transition from one operating mode to another, the residual zonotope matching the current mode should include 0 and simultaneously all the other residual zonotopes not matching the current mode should always exclude 0."
"wherex 0 k andŷ 0 k are respectively state and output zonotopes predicted by the interval observer at time instant k. eventually, it is guaranteed that the predicted zonotopes forŷ k (or"
"in order to establish a set of guaranteed fdi conditions, this paper is interested inx iaja ∞ at infinity. in fact, it is not possible to accurately computex iaja ∞ . then, one has to compute an approximation forx iaja ∞ and as long as the precision of the approximation is satisfactory, it can be used to replace the use ofx iaja ∞ . by following theorem 2.1 and proposition 2.1, assigning an arbitrarily initial zonotope 2 for (15) and iterating (15), a satisfactory approximation ofx iaja ∞ denoted as s iaja with the center o ia ja can be obtained. 2 note that according to theorem 2.1 a rpi set of (15) can be obtained. thus, if the initial zonotope is rpi, it is guaranteed that s iaja is a rpi approximation ofx iaja ∞ . if the initial is not rpi, a non-rpi approximation forx iaja ∞ can be obtained. however, as long as the iterative time is sufficient, the non-rpi approximation can also be satisfactory."
"this paper proposes an interval observer-based guaranteed fdi approach by using a bank of interval observers. for guaranteed fdi, a set of fdi conditions are established by analyzing the limit sets connected with invariant set notions. the advantage of the approach is that it can precheck whether the faults are detectable and isolable without the need of guaranteeing that residual zonotopes predicted by all the interval observers are separatable from each other. the following research is to explore ways of further reducing the conservativeness of fdi conditions for sensor faults."
"thanks to the basic principles of an actor and the properties that derive from them, we also propose that each actor (or group of them) represents the virtualization of a physical device, increasing the resources and functionalities of such a device and isolating the execution failures of the other actors, that is, of the system. actually, this proposal is not novel because there are different references [cit] that propose this same use for the iot. our intention is to provide a working framework and an implementation for the woe that meets its expectations. it is also our objective to promote the use of the actor model because it provides important abstractions to develop distributed systems, flexible, self-healing, and designed to achieve maximum and optimal use of resources thanks to parallel computing."
"remark 2.2: according to theorem 2.1 and proposition 2.1, one can compute a (rpi) approximation for the dynamics (1) with an arbitrarily expected approximate precision towards the mrpi set. ♦"
"the experiment description setting 2 is similar to the experiment description setting 1 but instead of increasing the number of simulated devices, it emulates the aggregation of messages in the payload field. the experiment shows 1000 devices sending messages with payloads that aggregate messages, emulating that each device is a smart gateway that aggregates messages. table 4 summarizes the experiment description. the results of the experiment are shown in figure 10 . note that the x-axis shows aggregated messages instead of sent messages. for example, the message size of a message that aggregates messages is calculated as:"
"the objective of the work presented in this paper is to create an architecture based on the iot paradigm to manage the storage and communication needs of the smart grids and at the same time link the smart grids with the end user through the methodologies of the wot. for this, a bidirectional human-to-machine interface is established, inspired by the wot that allows the ubiquitous control of the energy systems, the woe [cit] ."
"the equation (15) shows that the time-variant term is (a− l ja c)x iaja k, which means that the difference of values of x iaja k at different time instants is determined by the shape ofx iaja 0, while the contractive factor is determined by the placement of the eigenvalues of a − l ja c that corresponds to the j a -th interval observer."
"there are very few libraries focused on functionalities that involve mathematical calculations intrinsic to machine learning or distributed computing models. this is due, once again, to the idiosyncrasies of this type of languages, which are mainly used to serve dynamic web pages."
"by substituting u, w and v to replace u k, ω k and η k in (13a), respectively, one can compute a bounding zonotope denoted asx iaja k+1 to boundx iaja k+1 at time instant k + 1, and the center and segment matrix ofx iaja k+1 are derived as"
"the main element of the wot proposal is the use of web technologies for the transmission of information. as far as web protocols are concerned, there is the http(s) protocol and the websocket (ws) or websocket secure (wss) protocol. http is a request/response protocol while websocket allows a bidirectional connection between the client and the server."
"rest to vo/query: the purpose of this layer is to translate the different uris generated from the information of physical devices to actions on virtual objects or things. in this way, we do not act directly on physical devices but through virtual objects. this layer also includes an interface for more complex queries about a thing or the relationships of different things."
"the future work to be done is focused on two different aspects. on the one hand, we want to expand the woe application, providing it with more functionalities and allowing us to interact with the devices. we also want to integrate different types of sensors, with unequal capacities and performances."
"the main goal of smart grids is to provide better services and features (also known as smart functions), for both consumers and for producers and prosumers. in addition, the increased use of distributed and renewable energy generation requires changes in the electricity management system. it is necessary to improve automation systems, distributed intelligence, real-time data mining and management to improve network control functions, simplify configuration and also reduce system recovery and self-healing times."
"two basic wot-enabling methodologies are considered. embedding web servers to devices, or the creation of gateways [cit] that serve as aggregators or proxies between iot protocols and web protocols. cloud solutions are also found in this layer, generally transverse to more layers of the wot, such as thingworx [cit], watson iot platform from ibm [cit], octoblu [cit] or evrythng [cit], among others. these cloud solutions offer iot-wot translation gateways."
"comparing (13) with (14), it is seen that as long as the dynamics ofx thus, one can obtain the set-based dynamics of (14), which is derived as"
"the model proposed by the wot [cit] aims to solve the challenge of the heterogeneity of the iot devices. this is principally achieved by generating translators or mappings between the language spoken by each device (communication protocol and data format) and the language that we can consider \"universal\" due to its widespread use: web technologies. specifically, we have the http protocol and the restful apis [cit] . these translations enable the devices to speak the same language, which makes them accessible in a homogeneous way to either human or computerized actors, like other devices. the actions to be taken on these devices can be both to act and to sense."
"if we focus on the data flow, it is composed of two states that can be understood as a cycle: from heterogeneity to homogeneity and from homogeneity to heterogeneity."
"over the last decade, smart grids have led the revolution of the electrical grid, transforming it into a set of automated and efficiently controlled processes by the incorporation of icts. smart grids promote electrical energy management in a distributed and flexible manner. however, the current management systems are (i) centralized regarding their management; (ii) located in independent locations between them; and (iii) managed by fragmented applications, without integration between them and only intercommunicated thanks to specific communication channels, which are generally proprietary."
"increasing the burst interval to 5 s, smooths the timedelta mean as shown in figure 9 . this means that a higher percentage of timedeltas compared with figure 8 is closer to the minimum. it also shows that the maximum is 1.3 s approximately, compared to 5 s in figure 8 . the results obtained conform to time needs in non-critical smart grid applications [cit] ."
"proxy layer: according to the needs of this proof of concept, the proxy layer has been implemented through a publisher/subscriber protocol. in this way, the virtual objects that represent the devices can publish the captured data and can subscribe to messages also collected by other devices or to action messages sent by other devices."
"note that the process explained can be reversed. once the devices (device and websocket client) connect to the architecture, both can perform the same actions. in fact, at the software level, the functionalities of both virtual objects are encoded equally (classes in object oriented programming)."
"it is defined as, at least, the maximum of all the settling time of all the interval observers such that residual zonotopes predicted by interval observes not matching the current system mode do exclude 0 by waiting t after the detection of a fault. ♦"
"since the coefficient calculation is made offline, there is no restriction on the duration of the processing. we can thus achieve the needed interpolation factor with a mathematical software package. then, from h i (n), we obtain the samples corresponding to the output sampling instants."
"these experimental results may then be compared to the theoretical predictions. before doing this, however, in order to assure ourselves that the results of fig. 7 (1) . table 5 summarizes the tests carried out."
"similar tests as those performed in section 4.1 on a downsampler were performed on an up-sampler. the results, which for space reasons are not included here, demonstrate that the proposed system is applicable for up-sampling as well."
"the down-sampler system is summarized in fig. 5 . the coefficient values are stored in (2a +1) read only memories, roms, each of which has n memory locations. we recall from fig. 1 that each μ k is associated with one output sample; we thus know precisely where output samples are located with regard to the input samples during one period of μ k . thus, each rom stage contains a coefficient value that weights an input sample at a given output time instant within this period, in order to calculate an output sample. stated another way, at the i th instant in a period, the i th location of roms 1 to (a +1) weights the (a +1) input samples to the left of the output sample being calculated, and the i th stage of roms (a +2) to (2a +1) weights the a input samples to the right. the weighted input samples are then summed with a parallel adder to give the output sample. we note that the output sample calculation is entirely synchronized to f in . the \"dual clock\" fifo makes the output samples formerly calculated at f in pass to the f out clock domain. we call d ep the depth of this fifo. in order to tackle the arbitrary ratio problem, the \"master control\" block, which is the core of the structure, precisely controls the incrementation of the rom indices when a new output sample is to be calculated using the input samples that arrive in the registers."
"the paper describes an fpga-based, arbitrary ratio resampler for sdr applications that allows the designer to control sfdr while providing a simple solution requiring no additional clock, thus making it appropriate for fpga clock-limited designs. the proposed sfdr-based approach provides estimates of resource utilization that reliably match actual resource utilization figures obtained after the fpga circuit has been implemented, thus making it a valuable tool for design prototyping."
"the second series of tests were also performed for different filter types. in fig. 8, the results of the two series of tests in the case of the parks-mcclellan filter are summarized, where each set of curves presents is compared to the corresponding theoretical result that we saw in fig. 3 . as it is explained at the end of section 3.1, this theoretical result is upper bounded by an sfdr value imposed by the noise floor, which is approximately 116 db for 19-bit. for each tested filter, the errors between the sfdrs results and the theoretical sfdrs were calculated too. fig. 9 shows these errors for the parks-mcclellan filter. for the other filter types, as the case of fig. 9, the calculated errors reveal that the results given by the down-sampling system are globally positive as these errors rarely go down below -10 db. also, it justifies the choice of a 10 dbmargin, as mentioned in section 3.1. the hardware experimental results are thus in reasonable accord with theory."
"bruce denby holds a bs from caltech, ms from rutgers university, and phd from the university of california at santa barbara, all in physics. [cit] he has been full professor of electrical engineering in france, currently at the pierre and marie curie university in paris. his research interests are in signal processing and wireless communications."
"let h(n) be the discrete impulse response of the digital reconstruction filter to be used in our re-sampling process. there is an equivalent continuous impulse response h c (t) such that if we sample h c (t), we obtain h(n). indeed, from the value of the ratio f in /f out, it is possible to precisely calculate the time position of each output sample from the input samples time positions. in other words, it is possible to calculate the output time instants from the fractional interval μ k, the fraction of one input sampling period between an output sample k and the input sample m k that immediately precedes it (fig. 1) . using this time instant, along with h c (t), we can determine the impulse response coefficients which must weight the input samples ( fig. 1) . in this way, the filtering process can generate an output sample at any desired output instant."
"we have presented a practical fpga re-sampling architecture suitable for sdr implementations, which takes as its starting point the barker structure [cit] . compared to other src systems suitable for fpga designs, ours does not require a third clock at a multiple of f in or f out, as is the case with the majority of the methods cited in section 2, and is thus suitable even for very high sampling frequencies. the sfdr requirement of the targeted application is incorporated directly into the design process via a simple calculational procedure. it is furthermore shown in table 4 that the expected resource utilization estimated in the design phase closely matches actual resource utilization after hardware implementation. although not intended for applications in which input and output clocks are completely decoupled (see for example [cit] ), the proposed structure provides a simple, practical alternative to other techniques of re-sampling, for the use cases treated."
"unfortunately, none of these methods proposes a clear way of avoiding the use of a third clock with a frequency that is usually a multiple of the input or the output clock frequency, as it is often proposed to manage the interface between the two clocks domains. this creates a problem in the context of vhf applications; indeed, the frequency of such a clock could quickly reach the limits of what the fpga can support, or what a standard quartz oscillator can generate. by using a resampling structure similar to that of barker [cit], however, while at the same time properly interfacing the two clock domains, the classical fir filter technique should in fact be able to provide practical re-sampler designs that do not include a third clock."
"in this paper, we present the detailed implementation of such a structure, a re-sampler that is similar to the barker structure, but which is based on a classical rather than polyphased fir implementation, and which is distinct from the other re-sampling structures studied in the fact that it conveniently interfaces the two clock domains without the use of a third clock."
"the design is implemented on an altera stratix ii ep2s180f 1020c5 fpga chip using a stratix ii dsp development board. complete hardware design is done in altera quartus ii v9.1, with synthesis and implementation. table 3 summarizes the hardware resources used."
"in what follows, let b be the one-sided bandwidth of the input signal and β be the ratio 2.b/f in . two questions now remain, namely:"
"the structure of the up-sampler is similar to that of the downsampler (fig. 6) . the fifo retains its role of interface between the two clock domains. master control manages the reading of the fifo in order to avoid underflow. indeed, using an \"upsampling map\", master control ensures that the rom indices are incremented and the fifo read at the correct instants. the binary map for up-sampling is of course not identical to the one for down-sampling. whereas for down-sampling the map is synchronized on the input times, for up-sampling it is synchronized on the output times. when a map element is set to 1, for up-sampling we must increment rom indices. master control also enables registers \"reg e\" which are registers with an \"enable\" input. when \"enable\" is set low, the \"reg e\" output keeps its previous value. otherwise, it works like a simple d flip-flop register; this is useful when we calculate two or more output samples from the same input samples."
let q and n be respectively the numerator and denominator of the simplified version of the fraction f in /f out . let a be an integer so that (2a +1) is the reconstruction filter order.
"open access this article is distributed under the terms of the creative commons attribution license which permits any use, distribution, and reproduction in any medium, provided the original author(s) and the source are credited."
"olivier romain holds an engineering degree from ens cachan, ms from louis pasteur university, and phd from pierre and marie curie university, paris, all in electronics. [cit], he is head of the architecture department at etis laboratory. he is full professor of electrical engineering at cergy-pontoise university. his research interests are in system on chip for broadcast and biomedical applications."
"1. feature extraction: the 3-d structural texture features of retinal layers can be used to identify the fluid-filled regions [cit] . a set of 47 features are extracted from each voxel, which are summarized in table 1 . the first 15 features (features 1-15) describe the local image structure, the next 30 features (features 16-45) describe the image texture, and the last two features (features 46 and 47) are the distances to the ilm and rpe floor, respectively."
"when the algorithm executes the last iteration, it stops and finishes with a vertex v f (stop condition satisfied). this means that there is no point"
topological approach: algorithms using this approach concentrate on information given by the connectivity graph and try to infer boundaries from its topological structure. they often require nodes to gather information about a large neighborhood or entail sophisticated algorithms with high computational cost.
"in this section, we describe a new method called reset and restart which will be combined with the centralized algorithm lpcn in order to find the boundary vertices of a connected euclidean graph by starting from any vertex. as a byproduct, this method also allows to find the vertex with the minimum xcoordinate in the graph. however, if the considered system is distributed then the reset and restart concept must be combined with the d-lpcn algorithm. in this paper we assume that the nodes/vertices of the graph are not mobile. however, the mobility is considered as future work."
"in this step, we extract the structural texture and intensity score from the denoised sd-oct volume by random forest classification. the thickness score is computed from the thickness map generated by the layer segmentation. then, three types of feature information are linearly combined to construct the probability map, which is viewed as a high-level representation of the sd-oct volume."
"the remainder of this paper is organized as follows. section 2 presents related work from the fields of polygon hull determination and boundary node detection in wireless sensor networks. a mathematical formulation of the problem of polygon hull finding is proposed in section 3. in section 4, the lpcn algorithm and the reset and restart method are presented. moreover, the integration of reset and restart into lpcn is described. in section 4.3 the distributed version of rrlpcn is presented. the convergence of our method is demonstrated in section 5 in which simulation results and an implementation into real sensor nodes are shown. section 6 briefly describes the use of our approach for two other applications, and section 7 concludes the paper."
"rrlpcn is developed and validated theoretically using the cupcarbon and tossim simulators. the distributed version of our proposed algorithm (d-rrlpcn) is applied to the boundary node detection problem in a wireless sensor network. the simulation results show that d-rrlpcn consumes less energy compared to some existing algorithms. after validation by simulation, a real implementation is conducted using two types of wireless sensor nodes, telosb and arduino xbee."
"in this article, we have proposed a new technique called reset and restart, which in combination with lpcn and d-lpcn allowed us to introduce two new algorithms: rrlpcn and d-rrlpcn. both algorithms now find the polygon hull of a euclidean graph without any assumption on the starting node where the second is the distributed version of the first and particularly suited for wireless sensor networks. we have also validated the proposed method theoretically and computationally."
"tossim, the tinyos simulator, uses its hierarchical model by replacing lower level hardware components with software-emulated ones. this approach reduces the hole between the simulator and the real environment. by substituting low-level components, a high fidelity between reality and the simulation environment is achieved. one of the advantages of the simulation with tossim is the reuse of the code since the code used for the simulation is the same that will be exported to the real sensor nodes."
"after the layer segmentation, we calculate an interpolated normal rpe [cit] using a curve fitting procedure to tackle the deformation of the retinal structure. first, a second-order polynomial fitting is applied for coarse interpolation using the leftmost and rightmost 100 pixels from the elevated rpe, after which the point with a distance larger than 10 pixels to the interpolated curve are removed. this is assuming that there are no dramatic retinal structure changes caused by ped at that region. then, we apply the second-order polynomial fitting again using those refined points."
"serous retinal detachment, such as pigment epithelial detachment (ped) and neurosensory retinal detachment (nrd) as shown in fig. 1, is a prominent characteristic of csc. mechanical stress resulting from increased intra-choroidal pressure reduces rpe adhesion and alters hydro-ionic rpe regulation, which in turn causes ped. nrd is often associated with a mechanical abrasion resulting from an active flow through a break in the rpe [cit] . therefore, the segmentation of serous retinal detachment-associated fluid is important for evaluating the severity and progression of chorioretinal impairment."
"comparing with other state-of-the-art approaches, our method achieves desirable performance: (1) our method obtains tpvf, fpvf and dsc of 92.1%, 0.53% and 93.3% for nrd segmentation, and 92.5%, 0.14% and 84.6% for ped segmentation; (2) [cit] achieved tpvp and fpvf of 86.5%, 1.7% for sead segmentation in amd subjects; (3) [cit] reported dsc of 60% for fluid segmentation in a public data set; (4) [cit] achieved dsc of 77% in fluid segmentation by using deep neural network; (5) [cit] obtained tpr, fpr and dice of 93%, 12% and 89% for fluid segmentation in csc patients. note that the comparison is an indirect one using different data set with different types of fluids."
"in some situations, the convex hull does not sufficiently reflect the geometric properties of a data set. the \"concave hull\", called here \"polygon hull\", is more accurate for geometric analysis. this polygon hull approach is a more advanced method designed to capture the precise shape of the data set's surface. however, the construction of such a hull is not straightforward. most algorithms require the knowledge of a starting point, supposed to belong to the border."
"in summary, our 3-d automatic segmentation method demonstrates good agreement with the manual segmentation method both qualitatively and quantitatively. our method can be used as an automatic tool to evaluate serous retinal detachment, including nrd and ped, in cases of csc, and has the potential to improve the clinical treatment of csc."
"for a quantitative evaluation, the performance of the segmentation results is measured by the true positive volume fraction (tpvf), false positive volume fraction (fpvf), positive predicative value (ppv) [cit], and dice similarity coefficient (dsc) as follows:"
"in this step, we linearly combine the various feature information to construct the probability map. the intensity and thickness scores are obtained based on two assumptions: (1) the fluid always shows up as a low reflection region and (2) the fluid region is associated with a larger thickness than the normal part of the retina. the thickness map is calculated between the ilm and rpe for nrd detection, and between the rpe and interpolated rpe for ped detection (fig. 3(b), 3(c) ). then, we apply fuzzy c-mean clustering to the thickness map and obtain three categories [cit] . the distance between ilm and rpe floor are plotted with a red-green-blue (from thick to thin) color scale in fig. 3(b), 3(c). the thickness map reveals a few distinct clusters that match the general shape of our object. the category with the highest thickness is considered to be the object, and the category with the lowest thickness is considered the background. the category with a moderate thickness is combined either with the object or the background. in this study, we are mainly interested in obtaining the likelihood of a pixel belonging to the fluid in the thickness map and the degree of the output membership belonging to the class with highest thickness is assigned to the corresponding pixels on the map (i.e. columns in b-scans), which is denoted as the thickness score m s . the lighter blue columns in fig. 3 (f) and 3(i) indicate the restricted regions used for detecting candidate fluids."
"there are several limitations which need to be improved upon in future studies. some very small fluid regions are treated as artifacts in our method, and thus they can hardly be detected (shown in fig. 7(a) ). however, this defect does not significantly influence the quantitative evaluation, as we correctly segment large structures well. figure 7 (b) shows a false positive in the 3-d segmentation results, which indicates a high probability of the false positive region belonging to the fluid. therefore, we need to extract more robust and discriminative features to eliminate those false positives in the future studies. in addition, we believe the manual threshold of the label function * ( ) u x from eq. (11) can be adjusted to alter the true positive and false positive detection rates as needed in clinical practice. our results come from 37 cases, and thus our findings need to be validated using a larger cohort in a future study. although our framework is currently applied to detect serous retinal detachment in cases of csc, we believe it can be applied to segmentation of other abnormalities, such as symptomatic exudate-associated derangement (sead), with only minor modification."
"segmenting retinal lesions without retinal layers might lead to ignoring the potentially valuable clinical information regarding the retina. exploring their complex interaction can yield improved segmentation results on severely diseased cases. [cit] extended the loosely coupled level set model to jointly segment retinal layers and lesions with topology-disrupting retinal diseases, where lesions were modelled as an additional spacevariant layer denoted by auxiliary interfaces. [cit] applied 3-d graph search on the trained probability map and utilized auto-context methodology to refine the results exploiting the complex interaction between different retinal structures. the complementary information between retinal layers and associated lesions are useful and meaningful for fluid detection and segmentation, which we intend to explore in future work."
"a wsn consists of autonomous sensor nodes distributed in space to cooperatively monitor physical or environmental conditions such as temperature, sound, vibration, pressure, motion, etc. typical wsn applications may require the random deployment of sensor nodes over a large target area. besides, military surveillance systems may also require the detection of activities around the boundaries of the area under surveillance. thus, the system should be able to detect and identify any object entering or leaving the monitored area. therefore, developing mechanisms to identify network boundary nodes is a significant and challenging problem."
"according to previous reports [cit], eq. (4) is equivalent to the continuous max flow problem as follows: fig. 4(c) . in this study, we perform the segmentation on the probability map rather than on the original sd-oct volume (fig. 4) . therefore, we modify the cost functions of the continuous max flow model as:"
there are several other problems and domains where we can apply the algorithms proposed in this paper (rrlpcn and d-rrlpcn). we can cite two examples:
"sd-oct images are subject to speckle noise caused by the low temporal coherence and high spatial coherence of the optical beams [cit], which significantly influences the performance of the image processing algorithms. [cit] applied the median filter to the normalized sd-oct volume and learned the size of needed filters from the convolution neural network structure. the heavy speckle noise might negatively influence the segmentation of retinal layers and fluid contours. in our protocol, we apply the widely used bilateral filtering for speckle noise reduction and edge preservation, which facilitates the layer segmentation and feature extraction procedures [cit] . we used standard deviation values of 3.0 for space and 0.5 for range parameters in our study. layer segmentation is another pre-processing step. here, we adopt a 3-d graph search algorithm [cit] to delineate the ilm and rpe floor (fig. 1.) using the well-established iowa reference algorithm [cit] . the 3-d graph search algorithm converts the multiple retinal surface segmentation problem to that of finding a minimum closed set in a node-weighted digraph with appropriate feasibility constraints, and it can effectively solve this optimization problem in low-order polynomial time. we used octexplore r3.6 for layer segmentation based on optimized 3-d graph search [cit] . in our oct data, the software worked well for ilm and rpe layer segmentation even there might be other abnormalities. furthermore, our approach allow incorrect layer segmentation because we only need the candidate regions for fluid segmentation."
"despite recent advancements, automatic serous retinal detachment segmentation remains a challenging task because of several critical problems. first, the accuracy of layer detection might be influenced by the presence of abnormal retinal structures. second, the fluid-filled regions sometimes appear as regions with weak contours and an ill-defined shape. furthermore, both nrd-and ped-associated fluids have similar intensities and shape profiles, which makes automatic segmentation even more difficult."
"dinh [cit], and chu and ssu [cit] have introduced algorithms that exploit the same basic information. each node constructs a graph induced by its neighbors at the exact 2-hop distance. it is checked whether these graphs form closed circles. this is done by verifying the connectivity of sub-graphs in dinh's case, by tree construction and analysis in the approach of chu and ssu, who further provide a correctness proof for their method."
"the most challenging cases are the patients with both nrd and ped. figure 7 compares the results of the reference standard, the initial classification, and our method in such a case. although a small nrd-associated fluid region is present, our method achieves good agreement with the manual segmentation method, and shows fewer false positives than the random forest classification results. for a quantitative evaluation, tables 2 and 3 summarize the tpvg, fpvf, ppv, and dsc of the various approaches using the reference standards for nrd and ped, respectively. our method achieves a tpvg that is close to the initial classification, but has better fpvf, ppv and dsc compared to the initial classification. therefore, the proposed method can be viewed as a refinement procedure after the initial random forest classification, which eliminates most of false positive regions. the proposed method is also superior to other two algorithms in overall performance. these results indicate that the machine learning-based segmentation performs better than the pure image processing-based approaches, as machine learning methods can utilize the high-level representation from the learning samples. for statistical analysis, we apply a linear correlation analysis and bland-altman reproducibility approach to compare the proposed method with the reference standard of the specialist. the linear correlation results and bland-altman plots for the analysis of nrd and ped segmentation are shown in fig. 8 and fig. 9, respectively. figure 10 and fig segmentation. the bland-altman results confirm stable agreement between the automated method and the reference standard. simultaneous segmentation of nrd and ped is challenging, and our results demonstrate a similar performance whether we are segmenting one type of abnormality or both types of abnormality."
"statistical approach: this type of algorithm tries to exploit statistical properties such as node degrees to detect boundary nodes. as long as nodes are evenly distributed, this approach works quite well since boundary nodes usually have fewer neighbors than interior nodes. however, as soon as node degrees fluctuate noticeably, most statistical approaches produce misclassifications. besides, these algorithms often require unrealistically high average node degrees."
"energy efficiency is one of the most critical factors for wsn applications. the lower the energy consumed by each node, the longer the network can perform its mission. for this reason, our simulations are based on this measure of performance. we calculate the average of energy consumed by each node in milijule (mj) according to simulation parameters. figure 8 represents the average consumption of nodes in mj according to their position (boundary nodes, neighbors of boundary nodes and inner nodes). the purpose of this histogram is to compare the consumption of the nodes in the network according to their positions. as can be noted, the boundary nodes consume more energy, because they are the ones that participate most in the algorithm, followed by the neighbors of boundary nodes and finally the inner nodes. as we have seen in figure 8, the boundary nodes consume more energy in comparison to other nodes. in figure 9, the consumption of these nodes according to their number of neighbors is examined. there is a correlation between the energy consumption of these nodes and their number of neighbors. the more the number of neighbors increases the more they consume energy. one of the solutions to reduce this consumption is the reduction of the network density."
"the algorithm starts from the red vertex (see part (a)), which is set as a starting vertex and with an x-coordinate equal to x min . then it computes the next vertex among its neighbors, which is the neighbor forming the minimum polar angle between a fictitious vertex, itself (the red vertex) and its neighbors. in our example, the next vertex is the green vertex (see part (b)). since the xcoordinate of the green vertex is less than the x-coordinate of the red vertex, the red vertex is reset, the new starting vertex is the green vertex whose x-coordinate is set to the new x min (see part (c)). in the same way, the new starting vertex computes the next vertex (see part (d)). since the x-coordinate of the next vertex is less than x min, the next vertex is the new starting vertex, its x-coordinate is set as x min and the previously selected vertex is reset (part (f)). using the same principle, the new starting vertex restart the process and computes the next vertex, since this time the x-coordinate of the next vertex is greater than x min . this process continues without restart, until reaching the most left green vertex (part (g)). since the x-coordinate of this vertex is less than x min, all previously selected vertices (part (h)) are reset, and the process is restart from this vertex. the algorithm selects all boundary vertices, until arriving at the starting vertex, which computes the next vertex as the same vertex selected in the previous round, and the stop condition is satisfied, i.e., all the boundary vertices are selected (part (i))."
"in this paper, we propose a new method called reset and restart. it is an extension of the lpcn algorithm giving a new algorithm called rrlpcn, which allows to find the polygon hull of a euclidean graph without any condition on the starting vertex. we also present a distributed version of this algorithm, called d-rrlpcn, together with an application in the field of wsns."
"for wireless communication, we have used the xbee module from digi. xbee is basically used for this purpose. it offers the ieee 802.15.4 connectivity in the 2.4ghz ism band. there is a \"pro\" version which provides large communication range, denoted as xbp24, and a \"normal\" version, denoted as xb24. the term \"series 1\" is employed for the 802.15.4 version and \"series 2\" for the zigbee version. the xbee module uses a uart (serial interface) to communicate with the main board. the advantage is simplicity and the possibility to re-use many serial tools. for our implementation, we have used the xbee series 1. the indoor communication range of this module is 30 m whereas the outdoor range is nearly 100 m. with low power consumption and data rates of up to 250 kbps, xbee devices are particularly suitable for fast prototyping of wireless sensor network applications. it is possible to build a simple star-structured network or a complex mesh network using these devices. figure 13 shows a wireless sensor network with 8 arduino/xbee sensor nodes deployed randomly. the sensor nodes which have a green led turned on are the boundary nodes detected by the d-rrlpcn algorithm. fig. 13 : execution of d-rrlpcn using real arduino/xbee sensor nodes."
"we also test our method in csc cases with only ped. figure 6 shows that highly reflective holes inside the fluid are misclassified in the initial classification, while our approach can tackle this problem by adding probability information based on the thickness score and minimizing the fluid surface using the continuous max flow optimization. we compare our method with a layer segmentation-based method [cit], which detects the ped footprint between the segmented elevated rpe and estimated normal rpe floor. the large smoothness constraint might lead to under segmentation of the fluid, which is shown by the yellow arrow in fig. 6(a) ."
"boundary node detection is an essential problem in wireless sensor network applications. it is necessary for keeping track of the coverage range and events entering or leaving the region and for tracking communication with the external environment. it can be used for extracting further information about the structure and robustness of the network which is employed for routing, controlling and management purposes. suppose we have a wireless sensor network with all nodes deployed randomly. there can be two kinds of nodes: those which are on the boundary and the interior ones. we assume that each node can get its localization within the deployment zone. many existing methods can be used for this information [cit] ."
"in this research, we have studied sensor nodes using arduino and digi xbee modules. arduino is an open-source computer hardware and software company, project and user community that designs and produces devices for building digital applications and interactive objects that can sense and control the physical world. these devices are based on a family of microcontroller board designs produced primarily by smart projects in italy, and also by numerous other vendors, using different 8-bit atmel avr microcontrollers or 32-bit atmel arm processors. in this work, we have used arduino uno r3 based on the atmel atmega328 microcontroller."
"in order to validate the proposed algorithm regarding optimality and convergence, we will present, in the following, some mathematical concepts that will allow us to define the final version of the proposed algorithm."
the work introduced by shirsat and bhargava [cit] only requires a node to know an anti-clockwise order of its neighbors. each node checks for empty cones and cordless paths in the connectivity graph of its 2-hop neighborhood.
"the d-rrlpcn algorithm has been developed and simulated using the two simulation environments cupcarbon and tossim. the results show that it consumes less energy than d-lpcn and hop-based, dbd, and lvp. the algorithm has been implemented in two real sensor platforms, telosb and arduino xbee. as future work, we suggest an extension of the rrlpcn algorithm to 3d space and a proposition of a distributed version for boundary node detection in 3d wireless sensor networks."
"the proposed method significantly improves the initial classification results by using the probability map. the probability map flexibly represents the voxel score for the segmentation task by combining various feature information depending on the target abnormality. for example, we force a higher weight for retinal thickness for ped segmentation compared to that used for nrd segmentation, because the thickness between the elevated rpe floor and interpolated rpe is an important factor for judging ped. furthermore, the continuous max flow optimization aims to force the smoothness of the segmented region and minimize its surface simultaneously. thus, the problem of highly reflective artifacts inside the fluid can be tackled, as shown in fig. 6 ."
"in figure 12, a comparison with existing methods is given. we have compared the energy consumption of three algorithms, namely hop-based [cit], distributed boundary detection (dbd) [cit], and located voronoï polygons (lvp) [cit] with that of d-rrlpcn. the energy consumption of boundary nodes is calculated according to their number of neighbors."
"2. we utilize a probability map representation combining various features, which improves the segmentation results for the continuous max flow optimization 3. the proposed approach can sequentially segment ped-and nrd-associated fluid by using different restricted regions. figure 2 illustrates the flowchart of the proposed method. first, structural texture and intensity scores are calculated by a random forest classification, and a thickness score is constructed from the clustering on the thickness map obtained after layer segmentation. these scores indicate the degree of confidence of the corresponding voxels belonging to the retinal detachment-associated fluid, according to the respective features. then, those three types of feature information are linearly combined to build the probability map, which denotes the degree of membership of each voxel belonging to the fluid. finally, a continuous max flow approach is applied to obtain the refined segmentation on the probability map."
the methods proposed by ghrist and muhammad [cit] and de silva and ghrist [cit] detect holes by utilizing algebraic homology theory. they are centralized and rely on restrictive assumptions on the communication model.
"we segment ped and nrd sequentially by using different restricted regions. in the training phase, we only consider voxels between the ilm and rpe floor for nrd detection to discard voxels related to irrelevant tissue types. for ped detection, we allow an additional ± 20 voxels in the z-direction (axis shown in fig. 3(a) ) between the rpe floor and the interpolated rpe. then, the detection of nrd and ped can be treated as two separate binary classification problems, which might allow ped and nrd to overlap due to the specification of the search regions."
"low reflection regions, such as artifacts and blood vessels, might cause misclassification. we eliminate those potential false positives based on the size and position of the detected regions. regions smaller than 100 voxels in each b-scan and regions placed beyond the candidate fluid region determined from the thickness map are removed. we believe these regions are more likely to be the false positives. we intend to keep more true positives rather than removing the false positives. the size threshold depends largely on the prior knowledge of the experimental data set. in practice, we extend the scope of the candidate fluid region by an additional ± 117µm ( ± 10 pixels) and ± 236µm ( ± 5 pixels) in x-axis and y-axis (axes shown in fig. 3 ) on the thickness map, respectively. the x-axis and y-axis information are the same on both the thickness map and original sd-oct volume."
"the structural texture score, intensity score, and probability map for ped detection, respectively, in the current oct b-scan. the yellow dash line indicates the restricted region in b-scan."
"martincic and schwiebert [cit] have presented an algorithm that requires each node to know the positions and communication links of its 2-hop neighborhood. using this information, the node decides whether it is enveloped by a circle of other nodes. if this occurs, the node is located in the interior of the network."
"the algorithm combined with an algorithm to characterize the selected zone can be used to draw object contours in images. the user has to click on a pixel of the zone to study, and starting from this point, rrlpcn can detect the contour of the selected zone (object). this technique can be applied to identify the shape of a tumor, for example. rrlpcn can be very useful in the medical field. indeed, detecting the boundary of a tumor with high precision is a very crucial task for the medical practitioner. with rrlpcn, this task can be done by selecting any pixel of the radio representing this tumor. defining the boundary of a two-dimensional multi-robot system: a multi-robot system can be described as a set of robots operating in the same environment. in these systems, we can use a boundary as a formal practical definition of what is inside and outside the network. knowing the boundary would allow us to estimate the perimeter of the configuration. for a surveillance application [cit], robots on the boundary can specialize in target monitoring, and notify the network when a target has entered or left the tracking area. at each moment one of the robots can launch the d-rrlpcn algorithm, and the boundary robots will be detected."
"to extract the structural texture score from the sd-oct volume, we apply a random forest classification based on the 3-d structural and textural features. the output probability is considered to be a similarity in texture of the given voxel with respect to the fluid."
"if v is a boundary vertex of g, then there is no subset of v forming a polygon and containing v in its interior. we can illustrate definition 1 in figure 1 (a) which shows a graph with two kinds of vertices. the boundary vertices are those that belong to the orange edges, and the other ones are non-boundary vertices. the orange area of figure 1(b) shows the geometrical area formed by the graph. figure 1(c) shows a case where one boundary vertex is removed. as we can see, the new shape of the graph is different from the original one. this means that the deleted vertex is a boundary vertex. however, if we remove a non-boundary vertex, as shown by figure 1(d), the new geometrical area formed by the vertices of the graph is the same as the original one, which means that the deleted vertex is a non-boundary one."
"we also define a message alphabet, described in table 1, and the functions used in the algorithm in table 2 . algorithm 3 exhibits the main steps performed by each sensor node for the identification of the boundary nodes of a wireless sensor network. these steps can be explained by the flowchart of figure 7 . in this case, the nodes are completely independent and the x-coordinate of the starting node is sent to the next boundary node in each iteration. if a node receives an x-coordinate which is smaller then it own x-coordinate, this node will send to the previous found boundary nodes a message to reset them as non-boundary nodes. then, it will restart the process of finding the boundary nodes."
"in this article, we propose an algorithm to find the polygon hull in a connected euclidean graph based on lpcn (least polar-angle connected node), which allows to start from any point of the graph. a distributed version of the algorithm is proposed in view of its application to boundary node detection in wireless sensor networks."
"it is possible to not activate all network nodes at the same time, which will increase the lifetime of the network and reduce the consumption of the boundary nodes. the d-lpcn algorithm consists of two phases. the first phase allows to find the starting node which is the node with minimum x-coordinate (in this paper we call this part \"min finding\"). the second phase allows to start from the starting sensor node found in the first phase to obtain a boundary of the network. figure 11 shows the energy consumption of the boundary nodes depending on their number of neighbors, for d-rrlpcn, the min finding, d-lpcn without the first part (min finding) and the last graph representing the sum of the two phases of d-lpcn. considering the two phases of d-lpcn, d-rrlpcn is more efficient regardless of the number of neighbors."
"compared with the pure image processing-based segmentation methods, our method achieves a higher overall performance. the learning procedure can effectively extract the high-level representation from the raw intensity values for each voxel. therefore, our method is robust and can avoid misclassifying other fluid-like regions, such as low reflection artifacts and blood vessels. moreover, although nrd and ped share similar intensities and shape profiles, we can discriminate between these two types of abnormalities using a region restriction learning strategy. the post-processing steps based on the candidate region's size and position restricted by the thickness map are utilized to eliminate any potential false positives."
"central serous chorioretinopathy (csc) is an idiopathic disease of the posterior pole of the retina, which often appears as serous retinal detachment accompanied by leakage of altered retinal pigment epithelium (rpe) [cit] . spectral domain optical coherence tomography (sd-oct), which capitalizes on recent advances in optical imaging, is the primary imaging modality for the diagnosis of csc. state-of-the-art sd-oct devices can non-invasively capture a high-definition, cross-sectional profile of retinal layers and pathological changes in the macular area, and thus allow radiologists to make detailed anatomical assessments for proper csc treatment [cit] ."
"1. in the restart step, the starting vertex will select that vertex, which forms the minimum polar angle between the starting vertex, a fictitious vertex, which is an imaginary vertex situated on its left and its neighbors, as shown by figure 3 . in this figure, the edge between the starting vertex and the selected one is colored green. 2. after the selection of the next vertex, we have two possible cases: either the x-coordinate of this vertex is greater or equal to x min or it is less than x min . (a) in the first case, we continue the execution of lpcn and select a subsequent vertex. (b) in the second case, we run reset and set x min equal to the current vertex x-coordinate and we restart the algorithm from this vertex. figure 4 shows an example where all the x-coordinates of the selected vertices are greater than x min ."
"same edge for the second time in the same direction. in addition to finding the boundary vertices, rrlpcn also finds the vertex with minimum x-coordinate which is the last starting point of our algorithm. the distributed version of the proposed algorithm, called d-rrlpcn, is then applied to boundary node detection in wireless sensor networks. it has been implemented using real sensor nodes (arduino/xbee and telosb). the simulation results have shown our algorithm to be very performant in comparison to other algorithms 1 ."
"adriano and santos [cit] have suggested an algorithm based on nearest neighbors. it assumes that the currently selected point is related to its k nearest neighbors. then it selects the point forming the least polar angle with the current point, just as in jarvis' algorithm [cit] . a polygon hull is not obtained for any set of points, and the value of k must be adapted to each case."
"building a wireless sensor network system needs development and integration of many hardware and software components. figure 13 shows the overall architecture of the wireless sensor network system that we have developed. the system includes a number of distributed wireless sensor nodes, where each sensor node is a combination of a microcontroller, an xbee radio transceiver, and a battery."
"in this section, we present a state of the art on algorithms to find convex, concave or polygon hulls of a set of points in the plane and of euclidean graphs as well as on algorithms to detect boundary nodes of wireless sensor networks. in this paper, we do not deal with security and mobility problems. for this, it is suggested to refer to existing methods [cit] ."
"2.3.1 continuous max flow model applied to the generated probability map this segmentation problem can be formulated by a spatially continuous min-cut problem [cit], which aims to force the smoothness of the segmented region and minimize its surface simultaneously:"
"we can conclude that finding the boundary vertices matches with finding a vertex with minimum x-coordinate. thus, algorithm 2 shows the pseudo-code of rrlpcn to find the boundary vertices by starting from any vertex of a network. additionally, a flowchart is presented in figure 5, which resumes the main steps of the introduced method. p c ← an arbitrary vertex chosen from v . 3:"
"assessment of serous retinal detachment plays a vital role in csc diagnosis and treatment. automatic segmentation of serous retinal detachment is a challenging task, especially when nrd and ped occur simultaneously. the abnormal retinal structure, weak contours, and illdefined fluid-filled regions make the segmentation task difficult. furthermore, both nrdand ped-associated fluids have similar intensities and shape profiles, which makes the segmentation task even more difficult. in this paper, we propose a 3-d automatic segmentation method for serous retinal detachment in cases of csc. first, a probability map is constructed from learning samples using random forest classification. the probability map is a linear combination of structural texture, intensity, and layer thickness information. then, we apply a continuous max flow optimization algorithm on this probability map to segment the fluid-associated region with serous retinal detachment. our method is a full 3-d method which can handle both nrd and ped segmentation simultaneously in sd-oct images from cases of csc well, while most of the previously published studies only focus on one type of abnormality."
"to begin with, virtual epidemics allow for valuable scientific inquiry by observing the behaviors of oneself and other whyvillians during the epidemic outbreak. we can provide tools for visualizing various aspects of this outbreak in the larger community. assist students who often have issues in interpreting complex visualizations. other forms of assistance could be provided in sidebars or short tutorials or in offline classroom discussions."
"in conjunction with the aforementioned focus on central europe, the study has been carried out in an area enclosed by a rectangle of 38 zonal grid-points (28.125 w to 43.125 e) and 36 meridional grid-points (21.45 n to 88.57 n). some cyclone centers in subsequent figures might not have been labelled, since they are outside this area."
"a straightforward and pragmatic system was developed which derives the positions of the minima in the gridded 6-hourly geopotential fields (cf. sect. 3.1). using a set of displacement criteria those minima are stringed together to form tracks. this produces a catalogue of storm tracks as well as a wealth of information concerning size, intensity and displacement of the moving lows."
"moreover, virtual epidemics as proposed offer an integrated approach because historical, social and ethical considerations are all part of scientific inquiry. by providing experience at all three levels of the scientific process of investigative epidemiological research, journal of virtual worlds research -virtual epidemics as learning laboratories 13"
"it is only recently that the learning potential of virtual worlds has moved into the focus of public attention [cit] . while virtual worlds have been in use for many years [cit] the unplanned virus outbreak in the world of warcraft™ became a story in the mainstream news media [cit] and then a case study for epidemiologists [cit] . lofgren and fefferman argued that virtual epidemics were more than just a game: they could help us learn about human behavior in real epidemics -research difficult to replicate in real life due to ethical considerations. virtual viruses can simulate many aspects of real life outbreaks, in some instances even seemingly 'deadly' consequences when a player avatar loses its accumulated powers and resources and has to start all over again. while death in cyberspace does not equal death in real life, to players the consequences were real in terms of actual time, social capital, and dollars spent on having their avatars reach certain levels."
"in addition to the position and the geopotential height of the center, the \"number of pixels\" a measure of the area covered by the cyclone is registered -each pixel being a grid point. it is the number of grid points in the 38"
the flow regime of the transition seasons and the winter is characterized by strong gradients which simplifies the centers' identification and the tracking in comparison to summer. [cit] may help illustrating this fact.
"in the case of \"martin\" the algorithm which identifies storm centers was able to find closed isobars and generate a storm track. this is due to the changed synoptic situation, because the strong artic sea low which was dominant during the track of \"lothar\" had weakened after 27 decemberat a great distance near the hemispheric cold center, located over the canadian arctic at that time, another very strong low was forming. thus a comparably weak gradient over the north sea and central europe caused \"martin\" to stand out much more clearly and a successful track identification was possible."
these learning laboratories are a first proposal for dedicated spaces in virtual worlds that provide a more in-depth investigation and understanding of lived experiences in virtual epidemics. one popular form of assessment in massive communities is crowd-sourcing in which virtual world participants rate contributions and hence provide feedback about their value to other players. other forms of feedback and assessment are summaries of activity logs that inform teachers whether students have participated in essential activities.
"the center recognition performance depends on the existence of closed isolines, i.e., a topology in which a gridpoint is surrounded by neighbours which all exhibit higher geopotential. for cyclones that are lined up to form storm tracks these requirements are well met. a few exceptions are the relatively shallow fields that may occur in the summer months and a configuration in which a small scale storm is located at the perimeter of a deep and very large low (a kind of \"dent in a crater wall\")."
"the virus spread fairly quickly and within three days of its launch, incidence of the disease had peaked and infected more than 4000 community members. we know from our tracking data that visits to the whyville cdc before the outbreak of whypox are close to non-existent with the exception of the occasional curious peek or accidental visit. this all changed once whypox arrived: the number of visits jumped to 5,386 in a two-week window."
"our proposal of virtual epidemics offers a new model for learning in virtual worlds because it leverages particular design features of virtual worlds: real time, personal representation, and massive numbers of players. the idea of learning laboratories moves beyond the recognized value of virtual laboratories and thus could be more easily adopted by commercial virtual worlds. participation in and study of virtual epidemics offer an authentic context within which to become familiar with the need for, and practice of, these aspects of empirical experimentation, one that is not only familiar but also of personal relevance as virtual worlds have become the new meeting place for youth. by bringing the mindset and tools of epidemiologists to virtual epidemics, we offer a viable approach for youth to become engaged in science and provide them with new lenses to understand the virtual worlds they inhabit."
"850 hpa geopotential height data from ncep reanalyses [cit] were used in the study to develop the identification methods for cyclone centers and tracks. [cit] until now. the \"alternative product\", which has been generated by ecmwf (era40, [cit] ) [cit] and cannot be used to study the immediate past. there are other products, such as era interim, which extend into the immediate past and have a higher resolution. yet, they do not extend far enough back to establish a useful climatology -for that, at least 30 years would be required. it is noteworthy that due to a higher model internal resolution (era40 (t106) to ncep (t62)) era40 is able to produce a greater number of cyclones [cit] . this may be one reason why not all observed cyclones can be found by the used method."
"a center is defined as an individual, contained minimum which is surrounded by larger values in all directions. small, shallow lows which are a mere dent in a much larger cyclone can only be detected if there are appropriate neighbouring grid points within the spatial resolution of the grid point data set. finding the deepest cyclone in a geopotential field is a trivial exercise -it is just necessary to determine the field's minimum."
-global climate projections [cit] : the poleward displacement of the tracks can be attributed in part to the expansion of the subtropical hadley circulation -future changes in precipitation would thus occur even if the cyclone intensity remains unchanged.
"whyvillians will gain a direct understanding of the processes of infectious disease in populations, but importantly, they will also see how applied scientific research can provide recommendations for action to protect the public."
the authors have developed their own approach to produce a tracking algorithm that is as straightforward and simple as possible -similar to what imilast is proposing. only the geopotential in a coarse grid of about 2
"in this article we want to take the idea of virtual epidemics a step further by promoting them as learning laboratories for students to develop a better understanding of infectious disease, scientific inquiry, and social impact of infection-related behavior. our goal is to illustrate how interventions such as virtual epidemics that impact both the community and the individual player can create authentic learning opportunities or laboratories. we characterize these learning laboratories as 'authentic' because professional epidemiologists now use virtual epidemics as a way to identify pertinent factors for modeling real-life behavior (see exhibit 1). as a case in point, we use the outbreak of a virtual epidemic called whypox in whyville.net, a tween virtual world. [cit]"
"common to most methods is a two step procedure: first a cyclone detection scheme is devised, followed by a cyclone tracking procedure. the methodologies employed by different working groups cover a broad range. in exemplary form, three strategies will be briefly described. [cit] identifies minima of the 1000 hpa geopotential field (z1000) within a neighbourhood of 8 grid points of a reference point. in order to be able to include very weakly developed centers, a minimum gradient (20 gpm per 1000 km) in a given area (radius 1000 km) is required. the centers which were so identified are lined up to form cyclone tracks by a next-neighbour search. [cit] . they are also identifying the cyclones as local minima in the z1000 field. a local minimum is required to be surrounded by a closed contour which is 15 gpm higher than this local minimum. the cyclone tracks are sought by calculating a potential position for the next time step as a first guess. then the candidate in greatest proximity of that first guess position within a 1000 km perimeter is chosen. [cit] has focused particulary on the cyclone identification aspect. [cit] . according to synoptic experience, it is rather difficult to identify atmospheric minima in the southern hemisphere; there, cyclones are identified as local maxima of the cyclonic gradient wind vorticity which is required to surpass a threshold. then, the first guess strategy is applied when constructing the tracks: a predicted position is compared with the cyclone position as it occurs in the subsequent time step."
"the whyville cdc features an archive with information about previous infections and a bulletin board where players posted hypotheses about the causes of whypox and its etiological characteristics. within these postings, most whyvillians agreed that people got better within two weeks, but there was greater variety in their speculation about how whypox was spread. the majority of suggested mechanisms of infection mirrored those types of interactions commonly assumed by the public in real-world epidemics, even in the absence of scientific evidence. others thought you became infected from the sun or not wearing warm clothes when it was cold outside. the process of postulating these causes and effects, and comparing them to observed or reported case histories is the same exploratory process employed by both members of the worried public and professional epidemiologists during the early stages of any outbreak."
"whypox outbreak in which we gathered information about participants' online interactions and personal experiences, and finally discuss a series of learning laboratories that can serve journal of virtual worlds research -virtual epidemics as learning laboratories 6 as models for other educational interventions in informal spaces with possible bridges into science classroom activities."
"are outside the range of the map. moreover, it is possible that centers have no label; in such a case the cyclone would be outside the investigation window mentioned in sect. 2."
"f the track of storm \"anatol\" (the center is indicated by the letter a) in the sea level pressure field on 3 and (white isolines). the 12-hour displacement of the center is marked by a red line in subfigure (b)-(d). the ound on 2 december 06z until 5 december 06z are given in tab. 2."
"we also found preliminary evidence that whyvillians use experimental simulations to test hypotheses about rates of infection and epidemiology. within the whyville cdc, simulators let participants set and test different parameters for the rate and duration of infection. we observed that the frequency of the epidemic simulator use peaked during the whypox outbreaks: over 1,400 simulations were performed by 171 of the 438 online players who participated in a study. we saw that 116 of them engaged in some form of more systematic investigation by running the simulations three or more times and half of them demonstrated significant improvements in the accuracy of their predictions [cit] ."
"an aspect of item 1 above should be pointed out: the grid used is not equidistant and there is a potential for bias when assessing cyclone displacement in a kilometric sense to the effect that some tracks may not be captured if cyclones move (i) extremely fast, (ii) on a zonally oriented track and (iii) in very high latitudes. in an example shown later in this paper (cf. table 2 ) a displacement of up to ≈ 400 km occurs for a very fast moving cyclone in 60"
"using the a virtual epidemic like whypox as a basis for such experimental designs will still involve an initial period of validation and testing of a model developed for real-world epidemiological prediction. for instance, existing models need to be tailored to represent virtual world interactions, durations of contact, in-world transportation and movement, and consistency of friendship groups as well as the etiology of the whypox itself. it also would need to be tested whether or not the existing epidemiological models do achieve accurate predictions of within-whyville disease dynamics. this will be of great importance when determining the scope of interpretation of resulting infection outcomes to broaden a general epidemiological understanding. it is likely that altering the whypox itself to more closely mimic specific aspects of real-world disease etiologies will allow more controlled experiments, and more accurate model outcomes and predictions, and therefore more ease in accurate interpretation for broader use."
"-1 jan 12z: only two centers were identified, since the trough to the north-east of iceland was not sufficiently large to fulfil the criteria -this is again the case on 2 jan 00z (fig. 2c), while on 2 jan 12z ( fig. 2d ) a size is re-gained that enables identification."
the aim of the study was the development of an algorithm that can identify storm tracks in an objective manner. an important application would be the analysis of gcm data in order to determine future developments in climate dynamics as they would emerge from the changing placement of storm tracks and intensity of storms.
"it is also necessary to manipulate further the existing whypox to permit concept validation within the virtual world. a greater diversity of relevant etiological and epidemiological parameters, journal of virtual worlds research -virtual epidemics as learning laboratories 7 such as infectivity, severity of symptoms visible to others, and mechanism of transmission of infection, will allow epidemiologists to design a full series of controlled experiments exploring the reactions of individuals to diseases that differ in precisely controlled and defined ways. since diseases in the real world frequently differ in more than one aspect from each other, it is difficult to know which specific characteristics are inciting which reactions in the population. by controlling the strains of whypox in this way and observing the differences in responses, researchers can begin to better understand human reactions to disease. further, they can explore the roles played by education, rumors, media reporting, and public health initiatives at shaping or mitigating those responses."
"1 http://www.proclim.ch/imilast/index.html this makes the method applicable to long-term climatologies such as ncep reanalyses, as well as to gcm scenarios of the ipcc-ar4 generation. the term robust is to be interpreted in the context of the method's ability to identify the cyclone centers and produce cyclone tracks from coarse input."
"the 500 hpa level has been chosen just for illustration purposes, since centers are particularly clear in this level of low divergence -the identification itself uses the 850 hpa geopo- tential level. surface pressure data would be an option but they pose several problems: (i) it would be problematic to track cyclones over elevated terrain 3 (ii) in comparison with upper air data, the structures which are to be identified exhibit a higher degree of detail which reduces the efficiency of the center-finding algorithm, and (iii) particularly in winter, the surface level is frequently decoupled from cyclone activity by inversions which also reduces the efficiency of the algorithm. the identification of centers is carried out in steps of 6 h -the chart archive used for illustration, on the other hand, contains just 00z and 12z information. a few remarks concerning the charts, figs. 2 and 3: -1 jan 00z: there were three cyclones identified, which are numbered according to their depth. the trough near the northern ural was not identified as an individual cyclone because no closed isoline could be constructed within the grid resolution (cf. sect. 2). however, low #2 fulfilled this criterium. the strong cyclone over the hudson bay is outside the identification window used for this study."
"finally, epidemiologists can observe relative transmission rates and numbers of whyville inhabitants ultimately infected by the different strains of whypox. since they will have complete knowledge of the purposefully designed epidemiological parameters, they will be able to tease out the direct impact of the observed behaviors on the outbreak dynamics and determine which social, environmental and disease-related characteristics were most influential at altering individual and group behaviors in response to disease threats. these types of questions are of critical importance when making public health decisions, and can be uniquely addressed in virtual worlds because of the complete availability of use and interactivity data."
"researchers are just beginning to develop assessment tools for virtual worlds, in particular those that can leverage the analyses of player behaviors and interactions in an efficient manner [cit] ."
a thought experiment may be carried out to illustrate the next steps of the approach: imagine the reservoir of a dam which has flooded a settlement. if we assume the water level
"already in the first half of the 20th century synoptic studies of the connection between the strength of the atmospheric flow and the displacement of cyclones/anticyclones were being carried out (e.g., [cit] ) . [cit] s [cit] s the discussion of existence, frequency and future modification of storm tracks was reviewed (e.g., [cit] ) . the progress in the development of atmosphere models led to particular involvement of moving intense cyclones as identifiers of the modelled climate's dynamics, as e.g., [cit] summarized. moreover, cyclones of that kind have a major impact, e.g. on the extremes of wind and precipitation. [cit] . the authors note a puzzling contradiction: a cyclone center moves as a consequence of the atmosphere system's tendency to achieve an energy balance. the baroclinic instability in a region should be reduced by a moving system. why, the authors are asking, are there frequent similar tracks, often used in rapid succession? there ought to be a mechanism which sustains baroclinic instability despite this flagging process. their conclusion, based on comparably coarse assumptions of the climate system's components and low resolution climate models was that the heating by ocean currents, such as the gulf stream triggers the formation of a corridor in which cyclones propagate."
"as stated above, the identification of cyclone tracks is a research field that dates back at least to the late 19th century. [cit] . in that paper it is also pointed out that cyclone frequency and track climatologies exhibit major differences depending on the methodologies employed. this sparked the establishment of the imi-last (intercomparison of mid-latitude storm diagnostics) [cit] 1 [cit] . the imilast tasks range from a survey of methods (more than 20 methods are documented!) to assess their individual uncertainties which may lead to a set of recommendations concerning purpose-specific tracking strategies. on the background web page of imilast it is emphasized that the use of as simple as possible metrics should be strived for. techniques that aim for simplicity and easy reproducibility are the focus of several studies, e.g., [cit] ."
"as an important result the analysis of summer data yielded that the center identification algorithm frequently found 6 to 10 centers in a chart -in winter conditions this number is are clearly smaller. yet the temporal sequence of positions only leads to a few tracks. figures 7-9 show the genesis and propagation of a cyclone which caused large amounts of rain in central europe. as an extreme example, between 12 [cit] 07z and 13 [cit] the rainfall amount at the saxon station zinnwald-georgenfeld was 312 mm [cit] . it caused the elbe flooding, inflicting major damage to infrastructure across the area, including the historic center of dresden."
there is a predominant probability that the wind will decrease in the mediterranean area and increase over northern europe. the modifications of the cyclone tracks have a clear influence on winter precipitation.
"based on these developments and our initial observations, we contend that a virtual epidemic like whypox can offer promising opportunities for participants to design and implement empirical experiments that investigate human response to social, infectious risks."
-observations [cit] : there is a poleward displacement of the storm tracks; the frequency of cyclones is reduced but their intensity increases. climate variability with respect to extratropical lows is large and the problem of track identification becomes larger because of constant changes in the observation system.
"the actual assembly of the tracks from individual centers takes into account that the motion along the track should stick to several criteria: for one, a track is accepted if it lasts for at least 48 h (eight consecutive time steps); furthermore there should be no displacement larger than 10 grid points (of the ≈ 2"
"a method has been presented that is capable of determining storm tracks in gridded geopotential/surface pressure data sets of rather coarse spatial and temporal resolution, as they are provided by reanalyses and gcms. it combines a geometric identification scheme for local minima with a set of rules to determine highly probable tracks from the arrays of identified centers in subsequent fields."
"f the track of storm \"lothar\" (the center is marked by the letter l) in the sea level pressure fields on 25 and (white isolines. the track of storm \"martin\" which followed right after \"lothar\" has been marked with an and (d). east of the british isles. an intermediate stadium is included in the 06z row of that day in table 1 . it indicates a westward motion. the center can thus be found at 12z over northern ireland (fig. 2a) . between 18z, 1 january and 12z, 2 january only very little motion is visible. then the center begins its eastbound track along the 51st parallel. this motion receives an additional northward component (cf. center \"4\" in fig. 2f ) between 3 january 12z and 4 january 00z. simultaneously, a bubble-like motion towards the mediterranean sea occurs. it develops into a cutting-off low which, on 4 january 00z (fig. 3a) appears so markedly that an individual center is recognized. the new mediterranean cyclone is moving only very little, nevertheless it can be identified until 6 january (not fully covered in the displayed sequence of charts). the previous track of the center that remained in central europe can be traced across the baltic sea into northern scandinavia. the cyclone arrives at the kola peninsula on 5 january 12z (cf. fig. 3d ). table 2 [cit] and received the name \"anatol\" by the meteorologists from the free university of berlin 5 . it developed from a trough in the north atlantic between greenland and iceland and moved only little until 3 january 00z. after that it deepened quickly and moved swiftly across the northern central europa towards the baltic states. www.adv-sci-res.net the track of storm \"martin\" (the center is denoted \"m\") in the sea level pressure field between 27 and 29 te lines). the episode which is relevant for central europe has been displayed in four surface pressure charts (fig. 4) . as shown in table 2 the low was the second largest in the map area up to 3 december 12z -the strongest could be found near the hemispheric cold center over the arctic sea near spitsbergen. thereafter and until 5 december, \"anatol\" had the lowest core pressure in the entire map sector."
"similar to the classification of circulation patterns into socalled großwetterlagen, first attempts towards a classification system for storm tracks in the european sector were carried out at the end of the 19th century. the meteorologist wilhelm jacob van bebber published his system -which correspondence to: f. kreienkamp (frank.kreienkamp@cec-potsdam.de) has been in use ever since -in 1891 in meteorologische zeitschrift (van bebber, 1891) . the formative influence to his approach was maritime meteorology, in particular the identification of \"heavy weather\". his requirement was that there are preferred pathways for the movement of storms into central and eastern europe. the van bebber classification, as shown in fig. 1, however, was based on observations from only a few years and station network of rather low density."
"as illustrated in the previous section each chart contains \"candidates\" for moving cyclones but in nature there are also formation and dissolution processes as well as phases in which the identification algorithm is unable to find a closed center. for each date, i.e., every six hours, all cyclones are entered into the calculation in ascending order of their core geopotential. the track-forming algorithm is considering all cyclones that are entirely within the 38"
"our proposal of developing learning laboratories builds on previous research of computer-based tools and virtual worlds for learning about various aspects of infectious disease. while wearable or handheld computer applications have been designed for students to experience short-term simulations of epidemic outbreaks in classroom settings [cit], virtual worlds have extended these investigations in time length, complexity, and number of participants by creating large-scale, graphic, and real-time simulations. this emerging field is led by developments of quest atlantis [cit], rivercity [cit], and whyville [cit] . most pertinent is research on the virtual world river city where students learn about the process of scientific inquiry by examining the sewer system and housing conditions in the 19th century, simulating some of the first functional epidemic control models in public health (snow, 1855) . students use their avatars to interview the city's virtual inhabitants and collect data about sanitary conditions within the city but the avatars did not experience the diseases themselves. studies have shown that previously low-performing students are more motivated to learn about the scientific process [cit] . these initial findings prompted us to investigate online activities and experiences in a virtual epidemic -described in the following section in more detail."
"at the same time that whypox was peaking, articles begin to appear in the whyville times. in the february 6, 2005 and february 13, 2005 issues, when whypox was the most prevalent, 3 of 20 and 5 of 21 articles, respectively, appeared in the weekly issues. in these articles, whyvillian authors discussed when and where they discovered whypox, theories for how it was transmitted, and even a scam where some whyvillians \"offered\" to heal those infected if only they would be given passwords to accounts so that they could use their \"computer genius\" to cure people. interestingly, many articles reported discovering whypox journal of virtual worlds research -virtual epidemics as learning laboratories 10 in one place and thought it might be a joke, but then realized something different was happening when they found the effects to be more widespread. as one author described it: \"other times before this morning, people would go around faking the whypox and saying achoo. i played along this morning, fake sneezing like everyone else. but little did i know, they were sneezing beyond their control.\" many whyvillians felt that whypox was like a real infectious disease, citing its contagious nature as the most realistic feature. these very aspects of whypox made it an integral part of whyville community life; they also provide the foundation for considering virtual epidemics as a context for learning laboratories."
"whyville can provide virtual laboratories in which players identify the strain of whypox virus with which they are infected and then design a vaccine that might protect them against future outbreaks. this approach would also emulate current flu vaccine design where researchers try to identify and predict strains that will be prevalent in the coming season and in particular geographic regions. while such approaches are not feasible in current school laboratories due to the lack of equipment, these online versions can engage students in equivalent processes of detecting patterns in strains and designing appropriate vaccines."
"with whypox, we also have the unprecedented opportunity to add a historical context to the study of virtual epidemics. early whypox outbreaks documenting players' experiences and theories about the causes and spread in discussion boards, community newspaper articles, and graphs. students can use these materials to study patterns and experiences of past outbreaks, engage in comparative analyses, and make predictions about current outcomes. while many young players do not have extensive experience in learning with primary sources, previous research suggests that even elementary students can understand and use sourcing strategies [cit] public panels in the ethics lab finally, we know from the reactions of whyville players that fear of infection impacted player behavior during the outbreak ranging from ostracism to players expressing interest in getting infected. the social ramifications of prevention, protection, and interaction with others provide an authentic context for players to discuss ethical issues associated with vaccination, quarantine measures, and public health. for instance, appropriate measures can be taken within the community to control the spread of disease in ways that blanket, nonspecific measures may not be able to achieve. participation in un-monitored online discussion boards can at times be problematic because players might veer off topic but seeding discussions with prompts or questions can help focus postings."
"-understanding and attributing climate change [cit] : the poleward displacement of the main tracks, as observed in the last 50 years will continue into the future with high probability."
"by observing and participating in whypox, simulating the infection rates and other parameters, investigating the history of virtual epidemics, and discussing ethical and social concerns, we can engage youth in understanding key aspects of infectious diseases and epidemics that are a central part of living systems in the grade 5-8 science curriculum."
"f 500 hpa charts from 1 to 3 [cit] (source: www.wetterzentrale.de). the centers found by the ithm are plotted in red; they are sorted by the intensity of the cyclone. www.adv-sci-res.net to drop, individual structures and building re-appear 2 . perhaps the bell tower of a church (center of the strongest cyclone). then the top of a smaller belfry (center of a secondary cyclone with the second-lowest core pressure). then maybe the top of a high building (center of a tertiary cyclone). with further retreat of the water, another roof may appear, and so on. the search algorithm has to be capable of finding connected \"buildings\" as well as \"building parts\". this is achieved by stepwise lowering of the water level (raising of the reference level) until the first structure shows. a new structure is recognized as soon as subsequent water level lowering (reference pressure rise) yields only points which have lower pressure than the (successively raised) reference pressure. figures 2 and 3 show examples for identified centers."
"virtual epidemics like whypox can provide some insights into behaviors previously unconsidered in epidemiological modeling, even without the benefit of designed experimentation. for example, players who undertook personal risk in order to satisfy curiosity were observed during the world of warcraft outbreak -these behaviors had not before been explored epidemiologically, but do have the potential to drastically affect the overall risks of disease spread."
"whyville.net is a massive virtual world with currently over 4. [cit] study [cit] ) in which we recruited 438 online whyville players between ages 10 and 16 years and surveyed them about their science and technology interests, whyville activities, and understanding of whypox. we also tracked their online movements and chat interactions journal of virtual worlds research -virtual epidemics as learning laboratories 9 before, during, and after the whypox outbreak. in addition, we conducted a content analysis of writings about whypox published in the weekly online newspaper the whyville times."
this paper focuses on the description of the tracking algorithm. in a subsequent paper the classification of storm tracks will be addressed. comparisons with the historic classification of van bebber will need to be made.
"as can be seen in fig. 5, a cyclone with a closed isobar around the center is located east of newfoundland on 25 december 00z. its position, west of 30 w, is outside the map area of this study. subsequently, \"lothar\" is a mere marginal dent in the outer reaches of an extended arctic sea cyclone with a strong pressure gradient."
"given the millions of players in virtual worlds, the success of our design could make virtual epidemics accessible to thousands of learners in classrooms, afterschool clubs and homes. most importantly, virtual epidemics provide a test bed for youth to experience and understand issues about communicable diseases and vaccination that are at the forefront of today's public debate. public health prevention and protection measures are connected to behavioral changes and conceptual understanding of infectious processes and interactions. it is here where a better understanding of epidemic dynamics, real or virtual, can have a broader impact on personal and societal health."
"participation in whypox will offer players a first hand experience with an epidemic outbreak because it provides an authentic context for studying communicable diseases that cannot be replicated in life for ethical reasons. we envision the following observatories, simulators, archives, and public panels as examples of such learning laboratories."
"virtual epidemics also allow for valuable scientific experimentation by providing simulators that allow players to test different scenarios. here players could test different parameters of an epidemic by running controlled scientific experiments to answer narrowly defined, specific questions about the influence of individual factors within a multifactor, complex biological system. such simulators allow for formulating, testing and subsequently rejecting or accepting a hypothesis -key features of the scientific inquiry process. in addition, such online simulations allow for embedded assessments as students' growing accuracy in predictions could be used as a measure for their better understanding of salient features. the combination of observed and hypothetical behaviors offers intriguing opportunities to help players understand the idea of using inferred relationships from observed data to make predictions about future, unknown outcomes for use in decision making for public policy. while students' issues with hypothesis formulation and testing have been documented in the research literature, offline support in form of teacher questions and feedback and online support in form of discussion forums could provide much needed assistance and help integrate these activities into the science classroom curriculum [cit] ."
"in order to select features that are reproducible between observer delineations, we performed a 2-way random intraclass correlation coefficient (icc) (absolute agreement, average type) on pair combinations of readings (1/2, 1/3, 2/3) and a lin's concordance correlation coefficient (ccc) on the intra-observer pair. a feature was considered as highly reproducible and therefore selected if all three icc values were above 0.8 and the ccc value was above 0.9. all other features were eliminated."
"as detailed here, many aspects of the pheprob approach were designed to improve genetic association test power: leveraging the diagnosis code counts as a quantitative variable instead of dichotomizing it; integrating healthcare utilization in the structure of the mixture model; and using the continuous model-predicted probability of being a disease case as an outcome instead of thresholding it to identify case-control status. unfortunately, it is difficult to disentangle the impacts on power of each of these aspects. for example, a patient's healthcare utilization c enters the method as a parameter in the binomial mixture model; avoiding reliance on c would necessitate using a different parametric mixture model, but performance differences due to the change in model and the reliance on c cannot be rigorously disentangled. similarly, we propose using the modelpredicted probability rather than a dichotomous outcome in the genetic association test; this is because we feel that the probability better carries forward phenotyping uncertainty into the test, but it is also not obvious how best to dichotomize that predicted probability into a case-control outcome in the absence of any gold-standard outcome data, and whether patients with mid-level predicted probability should be excluded in such a dichotomization."
"optimization algorithms proposed by researchers can be categorized into two groups: deterministic algorithms and stochastic algorithms. deterministic algorithms usually need the information of gradient. they are effective for problems with one global optimum, while they might be invalid for problems with several local optima or problems with gradient information unavailable. compared with deterministic algorithms, stochastic algorithms require only the information of the objective function [cit] . they have been successfully used in many optimization problems characterized as nondifferentiable and nonconvex."
"we studied two independent mri datasets of 74 lacrymal gland tumors and 30 breast lesions from two different centers. two pairs of readers performed three two-dimensional delineations for each dataset. texture features were extracted using two radiomics softwares (pyradiomics and an in-house software). reproducible features were selected using a combination of intra-class correlation coefficient (icc) and concordance and coherence coefficient (ccc) with 0.8 and 0.9 as thresholds, respectively. we tested six absolute and eight relative gray-level discretization methods and analyzed the distribution and highest number of reproducible features obtained for each discretization. we also analyzed the number of reproducible features extracted from computer simulated delineations representative of inter-observer variability."
"where is a solution randomly chosen from the population and 1 and 2 are learning factors ranging from 0 to 1. init is the initial value of 1 . iter max is the maximal number of iterations. iter is the number of iterations, and is a nonlinear index."
"institutions have different ehr systems and different approaches to using those systems. as a result, phenotyping methods typically have varying performance across institutions. this variability is especially notable when phenotyping is based only on thresholded diagnosis codes. for example, using at least 1 code for ra to identify cases was shown to have a ppv of 22%, 26%, and 49% across 3 institutions, while using at least 3 codes was shown to have a ppv of 55%, 42%, and 73% across the same 3 institutions. 21 this highlights another difficulty of existing phewas methods based on thresholding-even for a single disease, the best threshold for defining a case may vary across institutions. a method like pheprob that has the potential to adapt to the underlying distribution of diagnosis codes at an institution may be effective in the face of this heterogeneity, and further evaluating pheprob's performance and robustness across healthcare systems is a direction of future research."
"küçüksille [cit] pointed that the update equations of velocity in basic ba provide local search only with the guidance of the best-so-far solution, which makes the ba easily entrapped 4 scientific programming into a local minimum. in order to improve the search ability, they proposed a modified equation:"
"with no additional covariates (such as age and gender), our genotype-phenotype test is essentially model-free-it is fundamentally testing whether the genotype and phenotype are statistically independent. thus, it is a valid test across a wide range of true disease-genotype models. with additional covariates, our model is still valid across a wide range of true disease-genotype models so long as the genotype is independent of those covariates. relaxing this assumption to produce an even more robust test is a direction of future research."
"step 2, we test whether the snp is associated with the probability of having the phenotype, and calculate a p-value quantifying the strength of that association. essentially, we calculate the model's predicted probability,p yi, for each individual as defined above, and we usep yi in place of a typical 0/1 outcome in a logistic regression model with the genetic marker g and any clinical variables such as age and gender included as covariates in the model. we fit the model using logistic regression estimating equations and calculate a robust variance estimate to use for testing whether there is any association between the disease and the genetic marker, controlling for clinical variables."
where old represents a solution chosen in current population by some mechanism and is a random vector drawn from a uniform distribution. is the average loudness of all bats at the time step .
"once the new solutions are generated, the local search is invoked by bats' random walk. if the pulse emission rate of the th bat is smaller than a random number, select old from the population and generate a new position new as follows:"
"obl can be used not only to initialize the population initialization, but also to improve the population diversity during the evolutionary process. once the opposite population generated, it is combined with the original population for selection. individuals with fitness ranking in the top half of all candidate solutions will be selected as the current population. the pseudocode of obl is given in algorithm 3."
"the second term in the modified search equation is a factor affecting the velocity of the solution with guidance of the best solution *, which emphasizes exploitation. the third term is another factor that affects the velocity with help of randomly selected solution which emphasizes exploration. effects of the best solution and the kth solution are adjusted by changing the value of 1 . experiments results indicate that the modified algorithm can outperform ba in most test functions."
"with the development of natural science and technology, a large number of complicated optimization problems arise in a variety of fields, such as engineering, manufacturing, information technology, and economic management. these real world problems usually have an objective function with lots of decision variables and constrains, appearing with discontinuous, nonconvex features [cit] . optimization methods are always needed to obtain the optimal solution of these problems."
"at the beginning, bats do not know the location of food sources. they generate a randomly distributed population of solutions, where denotes the number of bats in the population. each solution can be produced within the search space as follows:"
"bat algorithm, a metaheuristic algorithm proposed recently, has been successfully used to solve different types of optimization problems. however, it also has some inefficiency on exploration and exploitation capabilities."
"in this study, a new variant of bat algorithm called obmlba is presented. the exploration and exploitation abilities of ba are balanced and enhanced in the search process by combining the information of the best-so-far solution and the neighborhood solutions into the search equations. at the same time, the proposed equation uses sinusoidal function to define the bat pulse frequency, allowing flexibility in changing the direction of . furthermore, lévy flight random walk is taken into consideration to escape from local optima. the concept of opposition based learning is introduced to the algorithm to improve the diversity and convergence capability. the proposed approach has been successfully used on 16 benchmark functions. results and comparisons with the basic ba, dlba, and other state-ofthe-art algorithms show that obmlba has outperformed considered approaches in most of the functions. however, these experiments are just small scale optimization problems. the performance of the proposed algorithm on large scale optimization problems and real world optimization problems should be deeply investigated. the future study of this method may focus on investigating the mathematical foundations and applications of the proposed algorithm."
"this study has some limitations. we did not normalize signal intensity on the mr images before extracting texture features. the ibsi recommends to use the normalization on raw mr data because it is based on arbitrary units. the normalization may impact texture feature values, especially when dealing with multiple mr machines and when comparing feature values among patients [cit] . however, we focused on the reproducibility of texture features between observers and not between patients. therefore, for a given patient, the signal intensity values were constant in the image and the range in the roi was impacted only by the difference in delineations. when performed after intensity normalization, the experiment 1 yielded comparable results, as detailed in s4 table. many other sources of variability may impact the results of the radiomics pipeline and should be investigated, including acquisition parameters, denoising, spatial interpolation methods applied on images or regions of interest, texture features calculation methods. for instance, we observed significant differences in terms of number of reproducible features between radiomics softwares. as delineations and pre-processing steps were the same, two differences may explain this. first, we extracted 57 texture features per delineation with our in-house software versus 69 with pyradiomics. second, the calculation method of co-occurrence matrices and other derived matrices is different between the two softwares, as explained in s5 table, both described in the ibsi guidelines. there are currently no recommendation to favor one method over another. it may be noted that the delineation of dataset 2 was performed only by junior radiologists with 6 months of experience in breast mri, which may impact the clinical significance of each specific roi. however, the purpose of the study was not to have clinically exact rois but rather to study the impact of differences in delineations among observations. finally, the method presented here to select inter and intraobserver reproducible features does not take into account whether the selected features were informative (i.e. predictive of an outcome). it is only one step meant to exclude non-reproducible features and keep the maximum number of robust potential candidates for new imaging biomarkers. however, the final purpose of the radiomics process is to identify potential imaging biomarkers based on their capacity to predict an outcome. we believe it is desirable to have the largest number of reproducible features so that potentially predictive features were not erroneously excluded for technical reasons."
"manual segmentation dataset 1. two readers reviewed the whole imaging dataset 1, a senior neuro-radiologist specialized in orbital imaging with 8 years of experience ( � blinded � ) and a junior radiologist with 6 months of experience ( � blinded � ), blinded to patient id, medical history, lab results and pathological results. each reader independently performed a 2d manual delineation of the two lacrimal glands of each patient on each mr sequence. the slice of delineation was independently chosen by each reader, considered as the slice where the lesion had the largest diameter. the delineation included both the tumor and the normal glands. a total of 444 rois were drawn by each reader (2 lacrimal glands per patient on 6 mr sequences for 37 patients). one reader ( � blinded � ) performed a second segmentation session to assess the intraobserver reproducibility. the segmentations were performed using an in-house software developed on matlab [cit] b, the mathworks, natick, ma, usa) and dedicated to the radiomics analysis [cit] ."
"whatever the discretization method, the dataset and the mr sequence, the three most reproducible features were: glrlm gray-level non-uniformity, gldm gray-level nonuniformity, gldm dependence non-uniformity. the three least reproducible features were: gldm large dependence low gray-level emphasis, glcm cluster shade and glrlm long run low gray-level emphasis."
"we assessed the influence of gray-level discretization methods on radiomics texture feature inter-and intra-observer reproducibility in two independent clinical mr datasets. texture feature reproducibility was shown to substantially depend on the gray-level discretization method. the absolute discretization (fbs) method was found to provide a higher number of reproducible features than the relative discretization (fbn) method. the relative discretization gray-level discretization impacts reproducible mri radiomics texture features was also found to give more different results when varying the bin numbers than the absolute discretization when varying the bin sizes. although the gray-level discretization has been assessed in pet and ct studies [cit], few studies have focused on mr images. molina & al. [cit] assessed the variability of texture feature values on post-contrast t1-wi in three patients with glioblastomas and one patient with a non-small cell lung cancer brain metastasis, using a fbn method with 8, 16, 32 and 64 gray levels. they found that modifying the discretization parameters led to significantly different results. more recently, [cit] investigated the impact of the absolute and relative discretization methods on patient ranking compared to the visual assessment of the heterogeneity of diffuse intrinsic pontine gliomas on 30 patients and 4 mr sequences (t1-wi, postcontrast t1-wi, t2-wi and fluid attenuation inversion recovery (flair)). they concluded that using an absolute (fbs) discretization method provided more consistent results compared to the visual assessment of the heterogeneity. the image biomarker standardization initiative (ibsi) [cit] has recently proposed guidelines to help select the best gray-level discretization for each purpose. the recommendations were to use relative discretization when dealing with mr raw data, though no reference is given to support this recommendation. to the best of our knowledge, no study has investigated the influence of absolute and relative discretization on multiple mr sequences, multiple datasets and multiple softwares."
"the pheprob method is designed to increase power for studies which rely on (or are limited to using) icd codes for phenotyping, with direct applications to phewas. it was designed for studies screening across hundreds to thousands of phenotypes, where creating individual highly accurate algorithms for each phenotype is not feasible. while it may serve as a starting point for investigators interested in detailed studies on a specific phenotype, it was not designed for this purpose."
"exploration and exploitation are two critical characteristics in the updating process of an algorithm. exploration is the ability of the algorithm to find the global optimum solutions in different areas of the search space, while exploitation refers to the capability of the algorithm to find better solutions with previous experience. researches in the literature show that, for a swarm intelligence algorithm, the exploration capability should be employed first so as to search in the whole space, while the exploitation capability should be considered later by improving the quantity of the solution in the local search process [cit] ."
"experiment 4-reproducible features using pyradiomics and computer simulated delineations on dataset 1 according to the gray-level discretization. texture features were extracted from each dice coefficient associated roi as described in the feature extraction paragraph, using the pyradiomics software."
"experiment 3-reproducible features using pyradiomics on dataset 2 according to the gray-level discretization. for each discretization method, we repeated the same analysis using the pyradiomics software on dataset 2."
"in table 6, obmlba is compared with some state-ofthe-art algorithms, such as abc, pso, ocabc, clpso, and olpso-g. results of these algorithms are all derived directly from their corresponding literatures [cit] . results show that obmlba has outperformed other methods on 6 of 7 functions. obmlba performs worse than abc and ocabc only on rosenbrock function."
"among the 1462 patients in the lipid study, the mean age was 63.6 years and the proportion of female subjects was 80%. also, 64.7% of this group had at least 1 billing code starting with 272 (disorders of lipoid metabolism); and among those with at least 1 billing code, the median number of billing codes was 8 and the maximum was 171. the results of the genotype-phenotype association tests are presented in table 1 . using the thresholding method, the p-values for association between the ldl grs and hyperlipidemia were 0.126, 0.123, and 0.142 for defining cases as having at least 1, 2, or 3 billing codes. using the pheprob approach we observed the expected significant association between the ldl grs and hyperlipidemia with a p-value of .001."
"the paper is organized as follows. after the introduction, the basic concept of ba is introduced in section 2. the proposed obmlba is presented in section 3. section 4 evaluates the performance of the proposed algorithm by comparing results with other algorithms. finally, conclusions and future work are discussed in section 5."
"this approach allows the model connecting diagnosis codes and probabilities to differ disease-to-disease based on the features of the disease distributions. a more detailed description of the simulation settings and the statistical models and methods is available in the supplementary appendix. implementation is available in the pheprob r package, available from the authors on request."
"this study was performed on two datasets: the dataset 1 was prospectively acquired in a tertiary referral center specializing in ophthalmic diseases with the approval of the institutional research ethics board, of the ile-de-france ethics committee (france) and adhered to the tenants of the declaration of helsinki [cit] -a00364-45, nct02401906); signed informed consent was obtained from all subjects. the dataset 2 was retrospectively built in a tertiary referral center specializing in breast lesions with the approval of the local ethics committee of tenon hospital (paris, france) and the institutional review board (irb \"blinded\"), which waived the necessity for informed consent. information was given to all patients."
"in order to evaluate the performance of the proposed algorithm, we select 16 standard benchmark functions to investigate the efficiency of the proposed approach. these functions are presented in table 1 . the function set comprises unimodal functions and multimodal functions, which can be used to test the convergence speed and local premature convergence problem, respectively. 1 ∼ 5 are continuous unimodal functions. 7 is a discontinuous step function."
"according to the analyses above, it can be clearly observed that obmlba provides outstanding performance with fast convergence speed and high convergence accuracy on most of the test functions."
"where i(x) is the intensity of voxel x, bn the bin number and i bn (x) the discretized gray-level of the voxel x. we tested eight different fixed bin numbers: 8, 16, 32, 64, 128, 256, 512 and 1024."
"experiment 1-reproducible features using pyradiomics on dataset 1 according to the gray-level discretization. for each discretization method and each mr sequence of dataset 1, we analyzed the highest number of reproducible texture features obtained from the pyradiomics software with fbs versus fbn discretization methods, using an exact fisher test. we also assessed if there was a difference in reproducible feature distributions among chosen bin sizes for fbs or among chosen bin numbers for fbn, using chi-square tests."
"lévy flight, which is based on the flight behavior of animals and insects, has been variously studied in literatures [cit] . it is defined as a kind of non-gauss stochastic process with a random walk whose step sizes are based on lévy stable distribution."
"to overcome the potential bias of the small number of readers that may not represent the true inter-and intra-observer segmentation variability, we developed a computer simulation of a high number of delineations (5000) based on morphological binary openings, closings and elastic transforms applied to a manual reference delineation for each roi of dataset 1. the dice coefficient, which is a measurement of similarity between two segmentations, was calculated between each simulated roi and the human reference one. then, one simulated segmentation per dice coefficient interval of 0.05 from 1.0 (no transformation) to 0.65 (maximum tested transformation) was randomly selected. this interval from 1.0 to 0.65 was chosen to encompass the whole variability different manual segmentations may provide. this led to 7 randomly selected simulated rois for each associated human reference roi with dice coefficients ranging from 0.65 to 0.95. the segmentations simulations were visually checked to make sure it was consistent with a potential manual delineation variability. an example of computer simulations is given in fig 1 ."
"all feature extractions and discretization methods were implemented using python language for pyradiomics software and matlab language for our in-house feature extraction software. all statistical analyses of extracted data were performed using r-3.3.3 (r foundation, vienna, austria) [cit] . p -value � 0.05 was considered statistically significant."
"bats modify the solution based on the information of the current location and the best source food location. bats navigate by adjusting their flying directions using their own and other swarm members' best experience to find the optimum of the problem. at this stage, for each food source position, a new position was formulated as follows:"
"medical imaging is progressively shifting from conventional visual image analysis to quantitative personalized medicine thanks to the recent development of data-driven analysis methods like radiomics [cit] . radiomics is a high-throughput mining of quantitative features from medical imaging that enables data to be extracted and applied within clinical-decision support systems to improve diagnostic, prognostic, and predictive accuracy [cit] . the potential of radiomics-based phenotyping in precision medicine is encouraging [cit] but the diversity of the implementation methods of the radiomics pipeline and the absence of widespread standards result in a high variability of the possible approaches that may lead to non-replicable results. more specifically, one of the steps in the radiomics process is feature reduction, to decrease data dimensionality and allow correlation of imaging features to predict outcome. feature reduction can be performed in a number of ways, but a frequently-used method is based on the selection of the most reproducible features, based on the hypothesis that reproducibility is a mandatory quality for an imaging biomarker derived from the radiomics process. this feature-reduction step will therefore impact the results of a radiomics study, and may lead to potentially discard a highly informative feature because it is not reproducible enough to be used in clinical routine. however, there are no current recommendations or standardization of this step. radiomics texture features are calculated from gray-level co-occurrence matrices and from other derived matrices that are computed on images after a gray-level discretization. the graylevel discretization consists in clustering pixels according to intensity values to facilitate the calculation of texture features [cit] . two approaches to discretization are commonly used. the first, called relative discretization, involves clustering of the pixels in the image to a fixed number of bins (fbn method); the other, called absolute discretization, uses a fixed bin size (fbs method). the gray-level discretization has been shown to substantially impact feature values extracted from pet [cit] and ct [cit] images. however, few studies have addressed the effect of gray-level discretization of clinical mr images [cit] . particularly, its effect on texture feature reproducibility is unknown."
"electronic health records (ehrs) contain a wealth of comprehensive patient information. when linked with genomic data, the combined information provides a powerful platform to study associations between genetic variants and a variety of diseases, disorders, and other conditions. ideally, disease cases and controls in large, diverse populations would be identified automatically using data in the ehr, and linked to genetic markers assessed on collected blood samples. 1 however, there is currently a mismatch wherein the ability to extract accurate information about patient phenotypes from ehrs lags behind genotyping. 2 for example, although ehrs often contain diagnosis codes for specific diseases, the presence or absence of these codes is not perfectly correlated with the presence or the absence of the disease. previous studies have successfully replicated established genetic associations by building phenotyping algorithms using both structured data such as diagnosis codes and unstructured data such as physicians' notes accessed by natural language processing. [cit] a major limitation of these approaches is the requirement for labor intensive chart review to establish gold standard labels on a subset of cases. such approaches are difficult to scale when multiple phenotypes are of interest."
there are lots of versions of lévy distribution. mantegna algorithm [cit] has been tested to be the most efficient method to generate lévy distribution values. it has been adopted in many evolution algorithms in order to escape from local minimum.
"in this context, the purpose of this work was to investigate how pre-processing of images, and specifically gray-level discretization methods, could impact the number and type of reproducible texture features extracted from clinical mr images."
"experiment 2-reproducible features using an in-house matlab-based software on dataset 1 according to the gray-level discretization. for each discretization method and each mr sequence, we repeated the same analysis on the data extracted from the dataset 1 using our in-house feature extraction software."
"to achieve a better balance between exploration and exploitation behavior of ba, it is highly required to modify the global search approach to explore the search region and develop a local search method to exploit in the nonvisited regions. accordingly, modifications of the search equation and local search strategies for ba are studied in the research. a modified algorithm called obmlba is proposed in this study. the main difference between obmlba and other bas is the interaction behavior between bats. in the proposed approach, the bat explores the search space with the help of the best-so-far solution and the neighborhood solutions by using echolocation, which can well balance the exploration and exploitation. the new population is generated by using a new modified position update equation with frequency defined by sinusoidal function. lévy flight random walk is used to exploit the local search space whereas opposition based learning is used to introduce opposition based solutions in population op. individuals of and op are merged together, and the best individuals are used as a new population for the next generation. the proposed method has been tested on 16 benchmark functions. comparison with variants of ba and other state-of-the-art algorithms demonstrates the effectiveness and superiority of obmlba in solving optimization problems."
"our results may be explained by the calculation process used to perform relative and absolute discretization methods. when using relative discretization (fbn), the intensity range of the segmented image will impact the bin size, which in turn determines the intensity histogram, the co-occurrence and other derived matrices used to compute texture features. most of the inter and intra-observer segmentation variability is related to slightly different contours providing intensity outliers, thereby changing the intensity range of the region of interest. conversely, the absolute discretization (fbs) method is independent from the intensity range of the segmented image and so may be less sensitive to inter-and intra-observer delineation variability."
"on the whole, algorithms proposed in this study such as mlba and obmlba work better than other bas considered. and obmlba is more effective than mlba."
"the gray-level discretization method had a direct impact on texture feature reproducibility, independent of observers, software or method of delineation (simulated vs. human). the absolute discretization consistently provided statistically significantly more reproducible features than the relative discretization. varying the bin number of relative discretization led to statistically significantly more variable results than varying the bin size of absolute discretization."
"from the convergence characteristic graphs, it can be obviously observed that obmlba is the fastest algorithm in all the considered bas. mlba is slower than obmlba but faster than dlba, lba, and basic ba. obmlba exhibits the best accuracy and achieves the fastest convergence speed. the results indeed indicate the advantage of the modified position update equation and obl strategy."
"this study showed that the gray-level discretization of mr images influences the number and distribution of inter and intra-observer reproducible texture features. we showed that using a fixed bin size method (absolute discretization) allowed preserving a higher number of reproducible features which may be potential imaging biomarkers candidates. considering the impact of the discretization method on results, the precise method used should be clearly documented in each radiomics study to improve the replicability of the results, as recommended in the recent evaluation criteria and reporting guidelines published to improve reliability, comparability and generalizability of radiomics-based studies [cit] . the standardization of reporting radiomics studies is an essential step toward the credibility and subsequent adoption of the data-driven radiomics process."
"where i(x) is the intensity of voxel x, bw the bin size and i bs (x) the discretized gray-level of voxel x. the term [min(i(x)/bs) + 1] ensures that the bin count starts at 1. we tested six different bin sizes: 1, 5, 10, 20, 25 and 50."
"29 figure 1 . workflow of the pheprob method. true disease status of patients is unknown; instead, the number of billing codes for each disease is observed. the pheprob method clusters individuals based on billing codes, and tests for an association between a genetic marker such as a snp and the clustering-based probability of being a case."
"we used two independent and very different datasets for external validation, two pairs of readers independently delineating the datasets, computer simulations of manual delineation variability, two delineation softwares to get rid of the delineation software confounding factor, and two radiomics softwares for which the calculation method of gray-level co-occurrence matrices and other derived matrices was slightly different. the purpose was to determine whether our results applied to different data, quantification methods, or organs. the fixed bin size method of gray-level discretization consistently provided less variable results. there are currently no specific guidelines to address the choice of the best bin width or bin number. available studies use bin numbers varying from 8 to 1000, as suggested by the ibsi [cit], or bin widths from 1 to 75 [cit] . this allows for differing ranges of signal intensity in rois, while still keeping the texture features informative and comparable between lesions. the ideal choice probably depends on the target lesions and the wish to enhance coarse or fine textures. we tested a large panel of bin widths and bin numbers to consider the largest possible choices and not limit our conclusions to a selected range of bin widths and/or numbers."
"as in mutation operation of de, the frequency is an important parameter that affects the convergence performance of the algorithm. the value of changes according to the predefined formula. in basic ba and dlba, is defined in (2), (11), and (12), respectively. here we use a sinusoidal formulation [cit] permitting certain flexibility in changing the direction of . the frequencies 1 and 2 are defined as follows:"
"supporting information s1 table. table. experiment 1 results after intensity normalization. reproducible features using the pyradiomics software and manual delineations on dataset 1 according to the graylevel discretization. the intensity normalization was performed as follows: images were centered by their mean and scaled by their standard deviation; intensity values below or above three standard deviations from the mean were excluded; intensity values were shifted to get only positive values; intensities were then scaled to the initial common range (all images of each sequence had a similar range). the normalization was not applied to the parametric adc map. (docx) s5 table. pyradiomics versus in-house software texture features calculation methods. pyradiomics computes one matrix per distance and/or angle (according to the texture feature), normalizes the matrix, calculates one feature value per normalized matrix and averages the results to obtain the final texture value. our in-house software computes a weighted sum of all non-normalized matrices obtained with the different distances/angles (diagonals are weighted by p 2), then directly calculates the final feature value based on this unique matrix. these two methods are described in the ibsi guidelines. there is no current recommendation to favor one method over another."
we focused on the simulated rois associated with a dice coefficient of 0.85 because the mean dice coefficient of manual delineations of pooled sequences of dataset 1 (experiment 1) was 0.84. we performed feature reduction according to the delineation variability by selecting features for which the icc between the reference human roi (dice 1) and the simulated roi (dice 0.85) was above 0.8. we then performed the same analysis as described in previous sections.
"as technology and science develop dramatically fast, more and more complicated optimization problems need to be solved by advanced numerical methods. as an important heuristic optimization algorithm, researches on how to improve the convergence accuracy and efficiency of ba are of great significance to improve the heuristic optimization theory and solve the real world optimization problems."
"association between hla drb1 or ptpn22 and billing codes for ra among the 14 985 patients, 12% had at least 1 diagnosis code for ra. among those with at least 1 billing code, the median number of billing codes was 14 and the maximum was 838. the results of the genotype-phenotype association tests are presented in table 1 . the p-values for the association between ra and the hla drb1 snp using all three thresholding methods and using the pheprob approach were less than 0.0001; the p-value for the association between the ptpn22 and ra was less than .0001 using the thresholding methods and 0.0002 using the pheprob approach."
"we assessed the influence of gray-level discretization on the inter and intra-observer reproducibility of texture features because most of the radiomics studies currently integrate the inter and intra-observer reproducibility as a data reduction step to select robust features and exclude all others [7, [cit] . this step, and how it is performed, will therefore greatly impact the features that may emerge from the radiomics process as potential imaging biomarkers."
"example 2: partners biobank and rheumatoid arthritis we additionally studied a population where gold standard labels were available for the phenotype rheumatoid arthritis (ra). the partners healthcare biobank comprises 14 985 [cit] with both clinical ehr data and genetic data. 32 for a subset of 546 of these patients, chart review was performed to confirm the presence of the most common autoimmune inflammatory joint disease, rheumatoid arthritis (ra). we extracted data on 2 of the strongest genetic risk alleles for ra, rs9268839 in hla drb1 and rs2476601 in ptpn22. the phewas codes 714 and 714.1 correspond to ra and other inflammatory polyarthropathies. based on patients' counts of these codes, we tested whether hla drb1 and ptpn22 were associated with ra, adjusting for age, gender and race, using both the standard thresholding method and the pheprob approach. for the subset of 546 individuals with true ra status known (from manual chart review), we sought to better understand how well the standard thresholding methods and the pheprob clustering step were separating cases and controls. to do this we estimated the false positive rate (fpr), the recall, the precision/ppv, the negative predictive value (npv), and the f1 score for the threshold-defined case-control status, focusing on s 2, the most popular approach in the literature. 12, 16, 18 for pheprob, we estimated the same quantities, after thresholding the method's estimated probability at its mean to define a binary outcome."
"for ft-nrp, during initialization, steps 1-3 require a complexity of oðnþ. steps 4, 5, 6, and 8 cost oð1þ. if random selection is used, steps 7 and 9 require a total of oðð1 à !þðe maxþ þ e maxà þþ. thus, initialization requires oðnþ. if nearest-boundary selection is used, an additional sorting cost of oðn logðnþþ is required, and initialization costs oðn logðnþþ. for maintenance, notice that the complexity of fixerror is oð1þ (fig. 7) . thus, the maintenance phase requires only oð1þ for every update received by the server."
"third, the contributions of apparently internal variability modes (ipo and amo) to global monsoon variability and the role of air-sea interaction will be evaluated based on the hist-resipo and hist-resamo experiments of tier-2. combined with cmip6 deck and damip experiments, the roles of external forcing (ghg, aerosol, solar, etc.) and internal variability can be quantified. the impact of tropical volcanic eruptions on the global monsoons can be explored specifically by analyzing volmip. current state-of-the-art climate models still show bias in the simulation of monsoons [cit] . we acknowledge that attention should be paid to the model bias in the analysis of model outputs, although multi-model ensemble/intercomparison approach is a useful way to reduce the uncertainty related to model bias."
"our findings show that the output of the saa method remains unchanged even when casualty transportation capacity is reduced by 10%. however, by using our proposed algorithm a significant increase in the number of ccps is observed, which contributes to 47 locations for establishing ccps in the case of risk averse dm ( ) -refer to figure 6 (b). in other words, the results reveal that the more conservative a dm is, the more the number of ccps that will need to be operationalised. further, using our proposed algorithm, as the coefficients of increases, the number of existing building from figure 6 (a) and (b), it can be concluded that our proposed methodology, which is based on robust stochastic programming model, enables a dm to cope with the infeasibility issues due to the dispersion of scenarios and generates more efficient solutions which are feasible for any scenarios. furthermore, being more risk averse in an uncertain decision-making environment results in opening more ccps among the existing buildings and open-spaces and therefore being closer to the affected areas. this fact emphasizes the necessity of providing fast and efficient medical services to the casualties from the shortest possible distance. in the following section, the role of accessibility to the services in ccps and its impact on the number of lives lost is explored."
"however, subject programs that are considered have many of the same programming constructs as large programs. the proposed approach should therefore be able to handle real and more complex programs. the claim is, however, a matter of further investigation."
"we will examine whether decadal and multi-decadal variability of local monsoon systems and coherent changes of the global monsoon can be reproduced in the amip-hist experiment. first, the skill of reproducing interannual and interdecadal changes in the regional monsoons will be compared with long-term observed records in local monsoon regions, such as using the all-india rainfall index from 1870 [cit] from climatic research units (cru; [cit] . the simulated monsoon circulation can be compared with two twentieth-century reanalyses from the national oceanic and atmospheric administration (20cr) and european centre for medium-range weather forecasts (era20c), which are also derived from agcm simulations driven by observational sst, where surface pressure (marine wind additionally used in era20c) records are assimilated [cit] . second, the interannual variability of the monsoon systems has experienced dramatic interdecadal variations during past 60 years (e.g., [cit] s to present; [cit] ) . the amip-hist results will be used to ex-plore whether similar modulations occurred during the past 150 years, and what mechanisms are responsible for them."
"keeping the mean free path constant, the same set of parameters proved to be a fine real-life solution for several systems sizes of 6, 10, 20 and 30 agents. moreover, our parameter set keeps on working well in simulation with significantly increased (up to 12 m/s) speed. these two facts represent a very promising feedback on our approach for handling difficult traffic situations."
"as explained, fraction-based tolerance adopts the concept of false positives and negatives. this tolerance applies to all entity-based queries, i.e., both rank-and nonrank-based queries. an example of fraction-based tolerance for nonrank-based queries is the reporting of alert messages about network sources, which yield a traffic volume within an abnormal range. for security purposes, it may be acceptable that these messages are sent to the network administrator even if the message is a false alarm. the network source wrongly reported can then be regarded as a false positive. as for rank-based queries, consider a cam system [cit], where feedback information is returned from production lines to adjust the parameters. in these systems, sensors are installed to monitor the parts and discover the patterns of similar features. users issue the k-nn queries to mine multimedia data streams (e.g., images) for unknown patterns, where the features are multidimensional such as position, shape, size, surface characterization, material properties, etc. those features are difficult to specify based on a numerical tolerance (because the user may not have a sense of how much the error value should be, as discussed in section 1). a fraction-based tolerance, on the other hand, is more intuitive in quantifying the quality of results [cit] . based on the fraction-based tolerance, the system will trigger an alert if certain percentage of discrepancies in the result is reached."
rq2: how effective is the proposed hybrid (adaptive pso and de) algorithm for optimal test data generation with respect to the convergence speed (mean number of generations) at termination?
"let us examine fig. 6a again, where the original fixerror protocol (on the left) puts s y into the set a by giving it an ½l; u bound, while s i is removed from the answer, i.e., put in set c. as a result, the false positive filter associated with s y is \"lost.\" our new approach avoids this by treating s i as a false positive instead (shown in dotted lines on the right). this is possible because although s i is no longer a valid answer, we can put it into any of the sets b, c, and d. more importantly, by putting s i into set b and s y into set a, the \"loss\" of s i from the answer is immediately compensated by the presence of s y . thus, correctness is restored, and the number of false positive filters remain the same as before."
"guaranteeing correctness. to make sure that both false positives and false negatives are met, we combine (14) and (15) so that the following is achieved:"
"in this section, we discuss a validation analysis which is based on optimality gap estimation between the objective value at a solution found by the proposed algorithm and the optimal value of the true problem."
"in the proposed study, an adaptive pso algorithm is hybridized with the de algorithm incorporating local neighborhood search strategy. the synergy between pso and de algorithms has resulted in a more powerful global search algorithm. the local neighborhood search strategy helps in exploring more promising candidate solutions to overcome the problem of local optima."
"the first step of multi-drone coordination was to achieve synchronized flight of a flock of drones as a common task for all agents [cit] . as a second step, here we intent to organize the traffic flow of a flock of drones, where coordination is still needed but the flight mission of each agent is independent and thus there are a lot more conflicts in the air to be resolved on the spot. moreover, we investigate the most difficult traffic scenarios of densely packed aerial drones. we forge situations where the drones are compelled to use every bit of the available airspace if they want to reach their targets quickly. additionally, the traffic flow is demanded to be collision-and oscillation-free. all of these self-organized global features need to emerge from individual behaviour based on limited local information. our model runs in two dimensions to harden the task by limiting possible movement directions to the horizontal plane, but it can be used to handle full 3d traffic as well, if needed."
"we now study how to exploit fraction-based tolerance for a range query-a nonrank-based query. consider a protocol that uses no tolerance: each stream filter is assigned the constraint ½l; u at the beginning. any violation in a filter has to be reported to the server, and query answers are updated correspondingly. correctness is guaranteed, since essentially, each filter evaluates the range query on the stream it is responsible for. we call this algorithm zero-tolerance protocol for nonrank-based query (zt-nrp)."
"we also perform testing on synthetic data, where 5,000 stream sources are generated. the time between each data item generated follows an exponential distribution with a mean of 20 time units. the values of a data stream item is uniformly distributed in [0, 1,000]. when a new data value is generated, its difference from the previous value follows a normal distribution with a mean of zero and a variance of 60 units. for range queries, we assume that the default query range is [cit] . for rank-based queries, we simulate the k-nn queries with k equal to 60. these parameter values are tuned in such a way that interesting trends can be observed and compared with the real data set. we also examine the effect of varying the value distribution of the synthetic data set."
"for the ft-nrp protocol, the default value of ! is zero, and immediate compensation is not used. these are the same parameter settings for the experiments used in our previous paper [cit] ."
"the meta-heuristic search techniques guided by a fitness function have been adopted to generate optimal test data mainly according to a structural test adequacy criterion. from the literature on structural test data generation, it can be inferred that branch coverage and path coverage are the most often used and wellunderstood measures [cit] . for branch coverage, fitness values are calculated by finding approximation level and branch distance for a target branch from control flow graph [cit] . data-flow coverage criterion has not been used much [cit] due to difficulty in writing test cases that satisfy data-flow dependencies of a program. [cit] defined different types of fitness functions for structural testing; data-flow test criteria being classified as node-node-oriented methods. recently only there has been more work on search based test data generation for data-flow coverage using ga as the algorithm of choice [cit] . now, other highly adaptive searchbased techniques such as pso [cit] and aco [cit] are also being applied to generate test data for data-flow coverage due to simplicity and faster convergence. aco [cit] and harmony search [cit] has also been applied to generate structural test data for branch coverage."
"the performance of the proposed hybrid (adaptive pso and de) algorithm has been experimentally evaluated and compared with that of de, pso, ga and random search for data-flow coverage. it is shown that the proposed hybrid (adaptive pso and de) algorithm outperformed de, pso, ga and random search with respect to the measure 'mean number of generations' for all the population sizes that are considered. for the measure 'mean percentage coverage', performance of the proposed hybrid (adaptive pso and de) algorithm is comparable to that of de, pso and ga for smaller population sizes (10 and 15); however, only the proposed hybrid algorithm achieved full data-flow coverage as the population size is increased to 20 and 25 for complex subject programs. performance of random search is worst. here, we have explored a promising hybrid optimization algorithm for test data generation. in future, we intend to fine tune the algorithmic parameters and work upon more complex subject programs."
"the ft-nrp protocol ensures that at any time during query execution, no more than a fraction þ of query answers are false positives, and no more than a fraction à of results are false negatives. as shown in fig. 5, ft-nrp consists of two phases: initialization and maintenance of filter constraints."
"to investigate how the four new features of agile selfdrive, anisotropic repulsion, selective friction and radial table i . arrows show the direction of motion and the time needed for the agent to reach its target. the right-hand rule bias is not included in the ruleset, we chose similar situations for better comparability."
"moreover, since the number of stream sources could be large, a stream server could be crippled by the large volume of data. this slows its response to standing queries that require real-time processing [cit] . it is thus important to lower the message volume so that transmission cost as well as server's load can be reduced."
"step 1(iii)(a) calculates the quota of ½à1; 1 filters that can be allocated to stream sources with ½l; u filters. if this value (equal to e maxþ à n þ ) is larger than zero, we assign ½à1; 1 to s i . we choose s i with the assumption that if s i crosses the ½l; u bound now, it is likely to cross the same bound again later. thus, any future updates generated from s i can be filtered."
in the next section we tune the free parameters with realistic simulations to obtain an optimal working model to be used on real drones as well.
"18 problem, we define the scenario-based decision variables accordingly. let ( ) indicate the number of casualties registered with injury severity level at ccp in period under scenario . constraint (28) guarantees that this latter does not exceed the inflows of casualties from the affected areas to a ccp."
"a k-nn query can be viewed as a range query: if we know the bound r that encloses the kth nearest neighbor of the query point q, then any objects with values located within r will be an answer to the k-nn query. we can use this idea to design a filter scheme for k-nn query (with zero tolerance). we call this protocol zt-rp. during initialization, it computes r, and then, distributes r to all the stream filters. if no responses are received from the streams, the server is assured that all k objects are within r, and they are still the k nearest neighbors of q. since no error is allowed, if any object enters or leaves r, we have to recompute r so that r still encloses the k nearest objects. in addition, the new r has to be announced to every stream."
"to perform optimal drone traffic, we build upon our previously proposed force-based simulation model of uav traffic [cit] but will highly exceed it in the basic concepts about the role of the interactions and the generality of the solutions even in situations where our previous model failed (e.g., avoiding traffic jams around common targets). furthermore we will also demonstrate the functionality of our model with actual field experiments using 30 fully autonomous drones."
"notice that fixerror involves the sending of three messages in the worst case. it also reduces the number of false positive/negative filters. by preserving some of these filters in the initialization phase and deploying them carefully (i.e., using incremental deployment), the penalty due to the execution of fixerror can be reduced, as shown by our experiments."
the california national guard announced that establishment of a ccp capable of providing an intermediate-level medical care requires a minimum of 48 hours to set up.
"as a result of proposition 2, the new pooling of design decision variables gives rise to the evolution of the control variables accordingly. we introduce the evolving control variables by ( ) as a vector of non-negative variables and subsequently the evolving absolute deviation function and infeasibility variable, ( ) and ( ), respectively. according to the proposition 2, we then reconstruct the robust saa programming, as represented in (44) - (48)."
"although zt-nrp can reduce communication costs, it may generate unnecessary updates. consider fig. 1 again, where the maximum allowed fraction of false positives is 0.1. suppose the black-colored object, previously inside the range r, has just moved outside. this triggers an update by zt-nrp. this is not needed, since the black object is the only false positive out of the 10 answer objects. it can thus be included in the answer."
"we now examine the performance of our protocols. sections 6.1, 6.2, and 6.3 analyze the computation, communication, and power consumption of zt-nrp and ft-nrp, respectively. section 6.4 presents the experimental setup, and section 6.5 discusses the results."
"highresmip, volmip, dcpp, and cordex, will help answer some fundamental scientific questions about the global monsoon and will help provide guidance about the future of monsoons as the planet's climate changes. it is also hoped that the gmmip will provide a good platform for the international climate modeling community in the collaboration of monsoon studies."
"the model output from the gmmip simulations described in this paper will be distributed through the earth system grid federation (esgf, http://esgf.llnl.gov) with digital object identifiers (dois) assigned. as in cmip5, the model output will be freely accessible through data portals after registration. in order to document cmip6's scientific impact and enable ongoing support of cmip, users are obligated to acknowledge cmip6, the participating modeling groups, and the esgf centers (see details on the cmip panel website at http://www.wcrp-climate.org/index.php/wgcm-cmip/ about-cmip). further information about the infrastructure supporting cmip6, the metadata describing the model output, and the terms governing its use are provided by the wgcm infrastructure panel (wip) in their invited contribution to this special issue. along with the data itself, the provenance of the data will be recorded, and doi's will be assigned to collections of output so that they can be appropriately cited. this information will be made readily available so that published research results can be verified and credit can be given to the modeling groups providing the data. the wip is coordinating and encouraging the development of the infrastructure needed to archive and deliver this information. in order to run the experiments, data sets for natural and anthropogenic forcings are required. these forcing data sets are described in separate invited contributions to this special issue. the forcing data sets will be made available through the esgf with version control and dois assigned."
"our single-query protocol can be extended to handle situations where more than one continuous query are executed at the same time. first, we apply the initialization step of ft-nrp to each query involved. the filter bounds computed for each query are sent to the stream sources. if there are m concurrent queries, each stream source will be associated with m filter bounds. second, during maintenance, the value of a stream is checked against the m filter bounds. the new value will only be sent to the server if it violates any of these bounds. since the update is only sent once even if the value crosses the boundaries of one or more filter bounds, some communication overhead can be saved."
"each optimization algorithm has its own advantages and disadvantages. also, one optimization algorithm will not work well for all the optimization problems. de, a meta-heuristic search-based algorithm, has been applied to several optimization problems [cit] to demonstrate its potential. [cit] has explored hybridization of pso with de applied to the design of digital filters. however, de has not been applied for test data generation and optimization problem [cit] ."
"def-use associations can be represented as node-node fitness functions [cit] . def-use associations specify the node of definition and the node of use for the program variables in the cfg without specifying a concrete path between the nodes. this implies that the first objective to reach is the definition node and then the use node, without however, specifying a path through the cfg. the distance to a node is represented by the standard minimizing metric given below:"
"there is already vast literature on optimizing ground traffic, which is restricted in most cases to quasi-onedimensional roads. this boundary condition together with human imperfection in concentration, reaction time and limited sight cause jams, slow traffic and also many accidents [cit] . there is an ongoing revolution to wipe out these inevitable human faults by autonomous cars [cit] ."
"uncertain parameters in one unique problem in order to provide effective design solutions; further, to formulate such a problem in the form of a two-stage robust stochastic optimization model."
"streams whose values fall within ½l; u are returned to the user. a range query is nonrank-based since the decision of whether a stream contributes an answer is independent of others. here, we use q to denote an entity-based standing query and aðtþ to denote the answer set returned at time t. we use jaðtþj to denote the cardinality of aðtþ."
"where ( ) denotes the number of ready-to-evacuate casualties with injury severity level at ccp in period under scenario . constraint (31) certifies that the number of casualties transported from a ccp to the allocated hospitals cannot exceed the number of ready-to-evacuates. note that only casualties with injury levels of intermediate and immediate have to be evacuated to hospitals, since they require further medical treatments."
"the same principle can be applied to the scenario in fig. 6b, where s y is not a query answer. instead of putting s i into b and s y into c (as in the original fixerror), we can do the following (shown in dotted lines):"
"casualty management and other operational decisions. in particular, optimizing the location and allocation decisions at the strategic level with the hierarchical integration of the periodical policy decisions lead us to a two-stage stochastic optimization model. with this motivation, we reflect on a robust stochastic optimization approach, which simultaneously optimizes the number of ccps, location, allocation, and capacity decisions at the strategic level and scenario-based casualty triage, casualty registration, casualty holding, and casualty transportation decisions in a multi-period planning setting, while satisfying the system constraints enhancing the problem objective function."
we compare ccp location decisions found by saa method and the proposed robust stochastic optimization with feasibility restoration variables with respect to coefficients of * +.
"a robust model is regarded as a model that returns solutions which are feasible for any given scenario realizations. due to the variability of the uncertain parameters, a stochastic programming model might be infeasible for some scenario realizations. one of the most probable reasons for infeasibility in a stochastic programming model is the variability of scenario realizations, which corresponds to the inflows of casualties [cit] . this issue, which is coupled with the limited available physical capacity of each potential node for establishing a ccp, corresponds to constraint (23)."
constraints (32) verifies the equilibrium casualty state transition in the consecutive periods in which the number of hospitalized casualties from the previous period plus the number of registered casualties of the current period is equal to the number of ready-to-evacuate casualties and the hospitalized casualties of the current period.
"ccps) and to the hospitals. in our model there are two uncertain parameters, namely, the number of casualties and the transportation capacity. the motivation for using these variables is based on the hazard profile that was associated with the bhopal disaster. the direction of the wind determined the number of people that inhaled the toxic gas. if the wind movement was in the direction of build-up population centres (called as wards) then this would affect more people. furthermore, the demographics associated with a ward could have a bearing on the severity associated with inhaling the gas. for example, inhalation of the gas had different sensitivities associated with children and elderly people compared to the rest of the population [cit] . our model, therefore, considered this uncertainty in the number and severity of casualties. the motivation for the second uncertain parameter (transport capacity) is based on the generally accepted fact that developing countries often have inadequate transportation and which is likely to affect emergency evacuation [cit] . [cit] recommend that the people living in the vicinity of hazardous plants should be made aware of the sources of transportation/ambulances for emergency evacuation. however, in a disaster of such magnitude, it is important to consider not only public transport but also private vehicles for the transportation of casualties (as happened in bhopal). ownership of private vehicles will usually depend on the socio-economic status of the people living in different wards. further, public transport capacity will also be dictated by transport infrastructure available in different population centres. in order to account for these variations, our model includes transport capacity as an uncertain parameter."
"2. climate extremes: extreme events such as megadroughts and flooding are frequent occurrences in monsoon domains. gmmip will allow the impact of changing lower boundary forcing on the statistics of extreme events to be examined in a consistent manner. 3. clouds, circulation and climate sensitivity: a reasonable simulation of monsoon circulation is a prerequisite for a successful simulation of monsoon precipitation (e.g., [cit] ) . at the same time, tropical precipitation is strongly dependent on convection, with monsoon precipitation biases very sensitive to convective parameterizations and therefore clouds. these parameterizations also lead to large uncertainties in climate sensitivity (e.g., [cit] ) . by comparing the performance of climate models with relatively high and low resolutions, and model simulations with and without air-sea interaction processes, gmmip will attempt to link monsoon precipitation simulation with the fidelity of the large-scale circulation and the latest remote sensing estimates of clouds."
"the robust stochastic optimization modelling approach described in section 3 is implemented through a computational study using data scenarios modelled on the bhopal gas tragedy that occurred in india over three decades ago. more specifically, we consider a hypothetical case of a gas leak in bhopal in today's date and which follows the hazard propagation profile (e.g., wind direction, affected wards) [cit] . the underlying data for the study, which includes the population of specific wards (population areas/catchments), available transportation in the city, existing infrastructure (including schools and hospitals), open spaces, and other model-specific parameters, was obtained through census data and from local municipal reports. we conducted one field trip to get access to some of this information. the data thus obtained was used to estimate the required parameters, which were then used to model the scenarios for the computational study. in this section, we also discuss the efficiency of our proposed modelling approach and present the solution sensitivity analysis to provide further insights to humanitarian logistics planners and practitioners."
"the joint usage of these interactions disrupts this simple hierarchical picture. it turns out in real flights that agentagent interactions can not be driven by repulsion, as it ends up in oscillations if the alignment is small enough to let anything happen in the sky. therefore, we need an agile selfdriving term to keep the agents away from collision-course first. such a well designed conflict-prevention term liberates the repulsion term from its last resort role, so it may become anisotropic to make the agents slip through frequented areas quicker. the smooth flow provided by these terms allow the agents to reduce the number of neighbours to align their velocity to, with behaviour-driven selective friction. as the agents handle well their neighbours moving in any direction, an additional queueing term can be introduced to wait for each other patiently at a common target, minimizing the blocking effect of other queuers."
"load shedding. in this method, the database server drops data tuples that arrived at the server site, in order to lower the resource demand. query processing also has to be accomplished under some quality-of-service (qos) requirements [cit] . although filtering techniques also attempt to reduce resource utilization by dropping tuples, they are generally different from load shedding. for filtering, the dropping of tuples occurs in stream sources rather than in the server. hence, filtering can save more communication costs."
"to conclude, the new fixerror can retain the false positive/negative filters in cases figs. 6a and 6b, but not in fig. 6c . thus, in most situations, the performance of the protocol will not deteriorate due to error fixing. the extra cost of immediate compensation compared to the old fixerror is that a message is sent to s i, for converting its filter from ½l; u to ½à1; 1. our experiments show that this is worthwhile. fig. 6 . cases handled by fixerror. fig. 7 . the fixerror routine (at the server side)."
"this set of constraints refers to two capacitated resources in the model, i.e. physical capacity limitations for casualty treatment at both ccps and hospitals, and the available transportation capacity to move casualties in the network."
"in order to measure the efficiency of the proposed logistic network design and related operations, we applied important metrics related to disaster management. we present the results in the following sections."
"based on the priority level of proposed scientific questions, the main experiments of gmmip, which are summarized in table 2, are divided into tier-1, tier-2, and tier-3 of decreasing priority (fig. 3) . in order to diagnose internal variability, at least three members integrated from different initial conditions are required for tier-1 and tier-2 experiments. pending the availability of computer resources at gmmip-committed table 1 . description of models participating gmmip. climate-modeling centers, realizations with more than three members are encouraged."
"in this section, we first present a generic robust stochastic optimization modelling approach and then apply this approach in the proposed ccp logistic network design problem where the uncertain number of casualties with different levels of injuries, and uncertain casualty transportation capacity, are described by a set of realizations or scenarios for their values."
"climate models are useful tools in climate variability and climate change studies. however, the performance of current state-of-the-art climate models is very poor and needs to be greatly improved over the monsoon domains [cit], b) . as one of the endorsed model inter-comparison projects (mips) in the sixth phase of the coupled model inter-comparison project (cmip6) [cit], the global monsoons model inter-comparison project (hereafter gmmip) aims to improve our understanding of physical processes in global monsoon systems by performing multi-model intercomparisons, ultimately to work towards better simulations of the mean state, interannual variability and long-term changes of the global monsoons. the contributions of internal variability (ipo and amo) and external anthropogenic forcing to the historical evolution of global monsoons in the 20th and 21st century will also be addressed."
"where, for each, (19) - (21) guarantee that the flow of casualties in the network is considered where the allocation in the network is certified."
"this section briefly describes the bhopal tragedy in india, often known as the worst industrial accident in the world and provides a computational investigation into the humanitarian logistics network design for establishing ccps in the affected areas. on december 3, 1984, a highly toxic cloud of methyl isocyanate (mic) leaked from a pesticide plant in bhopal, the capital city of the state of madhya pradesh, the second largest state in india. the leak was the consequence of a large volume of water entering one of the methyl isocyanate storage tanks around 9:30 pm the day before. this triggered off a a c c e p t e d m a n u s c r i p t 27 chemical reaction resulting in a tremendous increase of temperature and pressure in the tank and consequently led to an explosion. more than thirty years have passed since the gas explosion, but the bhopal saga is far from over. during our trips to the plant site and conversations with the volunteers at the ngo clinics as well as the local slum dwellers, we were told that of the 800,000 people living in bhopal at that time, no one knows exactly how many people were affected that night."
"the main drawback of this simple protocol is that it is sensitive to an object's value crossing r. when this happens, r has to be recomputed and announced to every stream! let us investigate how this problem can be alleviated."
"the shape of the h ↑↑ (.) and h ↑↓ (.) functions is determined by the anisotropy parameter a. if anisotropy is 0, we get the isotropic repulsion with u"
"in the first scenario drones are placed in a 'crosswalk' situation: half of agents start from one side of the arena, while the rest from the opposite side. every time an agent gets to its target it takes a new target randomly selected on the opposite edge of the arena. initially this results with two fronts of agents colliding into each other (same as with people on green light on a crosswalk [cit] ) and later several agents meeting near the center with a potential headon collision. these path conflicts form a testbed for the anisotropic repulsion and the self-driving term, as the agents have to evade each other effectively, and form lanes to open up space for the rest. increasing the number of agents, while keeping the mean free path constant makes the targets more dense on the one-dimensional arena-edge, so queueing also plays a more and more important role."
"from the experimental results as shown in tables 7-10, it can be seen that the mean number of generations is least with the proposed hybrid algorithm for all the subject programs and for all population sizes that are considered. there is a substantial reduction in mean number of generations with the proposed hybrid algorithm for benchmark programs such as 'triangle classifier', 'quadratic equation', and 'previous date' that have multiple and nested conditions along with equality conditions. this is also true for other programs taken from the repository [cit] such as 'sort', 'stack', 'vector', and 'linked list'. as expected, the mean number of generations decreases as the population size increases due to a wider search space. the performance of random search is worst with respect to the mean number of generations to achieve same data-flow coverage for smaller population sizes and for programs with multiple and nested conditions. random search did not achieve full data-flow coverage for any of the subject program. this has resulted in higher values for the measure 'mean number of generations' at termination. it can be inferred that the proposed hybrid algorithm with adaptive inertia weight and neighbourhood search strategy is the best performing approach for all the subject programs and for all population sizes that are considered with respect to the measures collected. the proposed hybrid algorithm and the other meta-heuristic search techniques (de, pso and ga) are all guided by the same novel fitness function; the better performance of the proposed hybrid algorithm can be attributed to the inclusion of adaptive inertia weight and neighbourhood search strategy."
"population size 20 (except for program# 2 and 6) and for population size 25 (except for program# 6). for population size 25, de achieved 100% data-flow coverage only for program# 1 and 7; pso achieved 100% data-flow coverage only for program# 1, 2, and 7; ga achieved 100% data-flow coverage only for program# 1 and 2. therefore, the convergence speed (mean number of generations) information for population size 25 (best performance for all the approaches) is used for statistical difference test. in the first step, friedman aligned 1xn test, a nonparametric multiple comparison statistical test [cit], is applied to check for significant differences between the performance of the proposed hybrid algorithm and the other algorithms. average rankings of all the algorithms are obtained that provide a fair comparison of the algorithms; a low value indicates higher rank. the unadjusted p-value is also computed through normal approximations; the smaller the p-value, the stronger the evidence against the null hypothesis. the value of α (level of confidence) is set to 0.05. in the second step, if the null hypothesis of equivalence of rankings is rejected, a post hoc test (holm's procedure) is applied to report adjusted p-values by adjusting the value of α in a stepdown manner to compensate for multiple comparisons. here, the proposed hybrid algorithm acts as the control algorithm and its performance is compared with the rest of the algorithms used for comparison."
"every meta-heuristic search algorithm suffers with the problem of local optima. another issue related to metaheuristic search algorithms is boundary constraints. there are no set mechanisms to deal with such problems. hence, in this study, an effort is also made to handle the problems of local optima and boundary constraints and to improve the exploitation ability of the algorithm. a neighbourhood search strategy ( figure 4 ) is introduced to sample more promising candidate solutions to overcome these problems. it is summarized as follows:"
"compared to this, our proposed methodology suggests opening up to 43, on average, locations for establishing ccps and is fairly relative to the risk aversion attitude of a dm. ccp location decisions are more of the essence when the available casualty transportation capacity decreases to 80%, on average."
"branch distance provides a measure of how close the program execution was to traverse the alternate edge of the critical branch. branch distance is normalized in the range [cit] using a normalization function v, such that the approach level always dominates the branch distance."
"in this section, the effect of varying population size on the performance of the proposed hybrid algorithm with adaptive inertia weight and neighbourhood search strategy is analyzed. the performance is also compared with other meta-heuristic techniques and random search."
the analysis plan will focus on the scientific objectives of gmmip. we list the key scientific questions that we hope that the community will be able to answer following the implementation of gmmip below.
"for the other meta-heuristic search techniques (de, pso and ga), all guided by the same fitness function, mean percentage coverage is between 94%-99% for all the subject programs and for all population sizes that are considered. de achieved 100% data-flow coverage only for program# 1 and program# 7 for population size 25. pso achieved 100% data-flow coverage only for program# 1, program# 2, and program# 7 for population size 25. ga achieved 100% data-flow coverage only for program# 1 and program# 2 for population size 25. however, the proposed hybrid algorithm outperformed de, pso and ga with respect to the convergence speed in all the cases. performance of random search is worst; mean percentage coverage achieved is minimum for all the subject programs for all population sizes that are considered. this provides an explanation for high mean number of generations when percentage coverage is less than 100% as then the algorithm terminates only after 10 3 generations."
"it can therefore be concluded that the proposed hybrid (adaptive pso and de) algorithm is the best performing algorithm and is significantly different from the other algorithms (de, pso, ga and random search) being compared. the proposed hybrid (adaptive pso and de) algorithm has stronger ability to generate test data with higher data-flow coverage as well as convergence speed as compared to de, pso, ga and random search techniques."
"first, in the next section we will give a detailed introduction to our agent-based algorithm that is executed on all drones in a distributed way."
"where θ is the heaviside step function, and d is a smooth and optimal braking curve to decay velocity as a function of distance, with explicitly taking into account the motion constraint of finite acceleration capabilities:"
". define fraction-based tolerance for rank-based and nonrank-based queries, . present protocols that exploit fraction-based tolerances, . derive the computation, communication, and energy costs of the protocols, and . perform extensive experiments on the protocols, using both real and synthetic data. the rest of this paper is organized as follows: we discuss related work in section 2. section 3 defines the semantics of fraction-based tolerance constraints. section 4 presents protocols for maintaining filter constraints for fraction-based tolerance of nonrank-based queries. then, section 5 explains how the nonrank-based query protocol can be extended to support rank-based query. in section 6, we analyze the resource consumption for our protocols and present our experimental results. we conclude the paper in section 7."
"the medical services are immediately provided to the registered casualties diagnosed with injury severity level . depending on the severity of injuries, the length of the hospitalization period, during which the casualties have to be kept and treated at ccps, is denoted by . after completing the hospitalization period, these casualties become ready-to-evacuate to the corresponding hospitals. constraint (30) reflects on the evacuation operations."
"differential evolution (de) algorithm was given by storn and price [cit] . it is a stochastic population-based global optimization algorithm that uses an evolutionary differential operator to create new offspring from parent chromosomes. unlike ga, de works upon real-valued chromosomes. the differential operator of de replaces the classical crossover and mutation operators of ga."
"automated test data generation is still an open problem in spite of decades of research. in the field of sbst, ga has been the algorithm of choice for control-flow coverage criteria. very recently only, other highly adaptive search-based techniques such as pso have been employed for structural test data generation. de is another simple to implement and highly adaptive searchbased technique that has been not yet applied for automated test data generation. among the structural test adequacy criteria, data-flow coverage test adequacy criterion has received relatively little attention. this paper presents a hybrid (adaptive pso and de) algorithm with neighbourhood search strategy for optimal test data generation in accordance to the all-uses data-flow coverage test adequacy criterion."
"our review of the literature on stochastic programming approaches specific to casualty management problems has shown that no existing model has taken into consideration the three features presented above. the main purpose of this study is to provide a specific representation of an integrated casualty management structure in an uncertain environment while the system constraints are met. to the best of our knowledge, the modelling of ccp logistic network design problem with the characteristics mentioned above has not been studied in the literature so far."
"to illustrate the above concepts, let us consider a continuous range query, a service commonly found in location-based and sensor applications. in this query, a user is interested in stream values that fall within a region called query range. this query can be used in an intelligent transportation system, which monitors the movement of gps-enabled vehicles inside a geographical region for an extensive amount of time. another example is wildlife tracking, which allows real-time analysis of the movement of animals that gather around a landscape structure, e.g., a water hole [cit] . the animals, attached with location sensors, can be monitored for a month's time by a continuous range query with the surrounding area of the water hole as a query range. fig. 1 illustrates a continuous range query over locations of moving objects. a rectangle r (in solid lines) is specified by the user. the query returns the identities of objects whose locations are inside r (these objects are colored in gray)."
"severe weather events and natural disasters have displaced approximately 32 [cit] and numbers are projected to continue rising [cit], over the past ten years, natural disasters affected almost 1.7 billion people, including 0.7 million killed, and resulted in 1.4 trillion dollars in damages worldwide."
"to choose which stream sources are assigned the false positive/negative filters in steps 7(i) and 9(i). we propose two heuristics: 1) random-stream sources are randomly selected and 2) boundary-nearest-only stream sources with values closest to the user-defined query range ½l; u are considered. while random requires only oð1þ time, boundary-nearest is more expensive-it requires sorting the distance of the stream value from the query range in oðn logðnþþ times. for boundary-nearest, however, objects closer to the query boundary have a higher chance for being assigned the false positive/negative filter constraints. these objects are also the ones that are likely to cross the boundaries and trigger updates. the assignment of filters to these objects can thus increase the chance that updates are dropped by the filters, resulting in better performance."
"step 1(iii)(b) then increments the number of false positive filters allocated. case 2. v v i 6 2 ½l; u. stream s i satisfied q immediately after ½l; u was installed to its filter, but s i is no longer the answer to q at time t u ."
"to achieve solution robustness, (mulvey & ruszczyński, 1995) measured the dispersion of the objective values by minimizing the average of standard deviation (or absolute deviation) of the objective values over all scenarios. this metric guarantees that the second-stage solutions are close to any scenario realizations applied in the problem [cit] . in order to avoid nonlinearity resulting from the standard deviation formulation, we instead utilize the absolute deviation one, denoted by ( ) in (8) for each scenario ."
"this interaction forces the agents not to approach their targets, but to wait patiently in a queue instead. the queue is virtual, the agents do not form lines in real space. queueing can be treated as a very useful collective behavioural state of the agents (as a form of instantaneous, self-organized order hierarchy) to avoid unnecessary self-excitation in the repulsive range in certain situations. this term helps to prevent giant over-packed jams around a frequented target, where even the agent who has reached its goal and wants to move to another one can not move due to the dense packing. however, the model also needs to take care of situations, when despite their ability to move freely towards their target, multiple agents freeze because a few agents closer to the target are stuck in a jam. the following method rules out these situations: 1. check if the agent who is closer to the target of q-agent than the q-agent itself and is the closest neighbour of the q-agent is going to the same target as qagent. 2. if it is going elsewhere, and it is closer to q-agent than r 0, then we do not queue up behind q-agent, since he is stuck in a jam independent of our queueing."
"changes in the precipitation and atmospheric circulation of the regional monsoons are of great scientific and societal importance owing to their impacts on more than two-thirds of the world's population. prediction of changes to monsoon rainfall in the coming decades is of great societal concern and vital for infrastructure planning, water resource management, and sustainable agricultural and economic development, often in less developed regions."
"the tier-2 experiments are initialized from the \"historical\" run year 1870 [cit] with historical forcings. additionally, the variation in the tropical pacific and north atlantic sst are restored to the observation in the \"hist-resipo\" and \"hist-resamo\" runs, respectively. the tier-2 \"hist-resipo\" (historical anthropogenic forcing plus restoring ipo sst) run is a pacemaker-coupled historical climate simulation that includes all forcings as in the cmip6 historical experiment, but with sst restored to the model climatology plus observed historical anomaly in the tropical lobe of the ipo [cit] domain ( (fig. 4a ). there are several restoring methods to realize such \"pacemaker\" simulations (see the appendix a). to ensure stability during integration, we recommend nudging to the specified sst described above with a 10-day timescale (see the appendix a for technical details). similarly, the tier-2 \"hist-resamo\" (historical anthropogenic forcing plus restoring amo sst) run is a pacemaker-coupled historical climate simulation that includes all forcings but with the sst restored to the model climatology plus observational historical anomaly in the amo [cit] domain (0- (fig. 4b) ."
". in open space this vector has a magnitude of v spp, a predefined travelling speed. however, if there is any neighbour in the fig. 2 . selectivity of alignment. neighbors of the blue agent show three cases when alignment can be switched off as each of them holds only one danger or efficiency criteria and thus motion remains safe and becomes more efficient without alignment. these three conditions are: d1) coming towards the agent; d2) being in front of the agent; e3) moving towards the target of the agent. for sake of simplicity the blue agent moves exactly towards its target. auxiliary lines represent angular thresholds of the one and only held criterion for each neighbor."
"another important factor that is used in robust optimization approaches, also known as model robustness, is to generate solutions values which satisfy all system constraints for any given scenarios. due to the uncertainty inherent in mass casualty flow management, it is very likely to observe the infeasible solutions. we also evaluate the infeasibility produced in the model for the 144 instances when using the stochastic modelling approach and compare with the corresponding values when applying our proposed solution algorithm by the usable transportation capacities. it has been observed that the stochastic"
"in pso algorithm, a large value of inertia weight facilitates exploration (global search) of the input search space and a small value of inertia weight facilitates exploitation (local search) of the input search space for the optimal solution. various inertia weighting strategies used in the literature have been categorized into constant, random, time varying and adaptive inertia weight strategies [cit] . in constant and random inertia weight strategies, value of inertia weight is either constant or is chosen randomly during the search. in time varying inertia weight strategies, inertia weight is defined as a function of time or iteration number. here, value of inertia weight is independent of the state of the particles in the search space. in adaptive inertia weight strategies, state of the particles in the search space (feedback mechanism) is used to adjust the value of the inertia weight."
"isotropic repulsion is generally used in flocking algorithms to avoid agents getting too close to each other. if they move into the same direction during flocking, repulsion sets the average inter-agent distance. but if they move in the opposite direction during a traffic situation, repulsion pushes agents away from their desired direction, creating oscillations."
"in this study, fitness value of the particles is used to adjust the inertia weight. ratio α of the particle's fitness to the average fitness of the swarm is calculated as shown in equation 3 below:"
"3ðp t þ p r þ. in addition to communication, a sensor needs to collect information from the external environment. we use sensing power (p s ) to denote the amount of energy required for a sensor to acquire a data value (e.g., temperature, location). we assume that unless a sensor is installed with a false positive/negative filter, it collects data at a fixed periodicity (or duty cycle). at any instant of time during maintenance, for zt-nrp, there are n stream sources installed with ½l; u filters. thus, the total amount of sensing power consumed in the maintenance phase is np s . on the other hand, ft-nrp only has ðjaðtþj à n þ þ þ ðjs à aðtþj à n à þ stream sources installed with ½l; u filters, and so, the sensing power required is ððjaðtþj à n þ þ þ ðjs à aðtþj à n à þþ â p s . the higher the number of false positive filters (n þ ) or false negative filters (n à ), the more sensing energy is saved. in the rest of this section, we present the experimental results for the network bandwidth and power consumption performance of our protocols."
"note that the takeoff position of the drones had some effect on initial transient performance. while the algorithm is prepared to prevent risky situations, neither of anisotropic repulsion, selective friction, agile self-drive and radial queueing is purely optimized or even dedicated to handle situations starting from risk already. consequently, when the drones were not spread out initially as in the simulation but took off very close to each other (e.g., with 5-7 m spacing), the initial movement of the drones was somewhat erratic as they started from within the range of anisotropic repulsion, which pushed them away from each other at high speed. it is not an actual fault, because this is a situation what the algorithm would never allow to happen. nonetheless this is a field, where there is space for improvement, e.g., by introducing a new behavioural phase to handle such \"panic\" situations effectively. anyhow, after the initial transient, the fleet always found a formation where every drone had its own safe space, and from that point on, they reached their targets fast and smoothly while using the available space smartly."
"in general, if a killing node is traversed, a fitness value of 0 is assigned to the test case ti and it is discarded; otherwise equation 11 or equation 12 is used to compute the fitness value. test case ti is said to be optimal if its fitness value is 1 i.e. the target is covered."
"the main goal of this paper is to study how filter bounds can be deployed in data stream sources in order to exploit fraction-based tolerance for different continuous query types. let us use the range query example in fig. 1 again to illustrate our solutions. based on the maximum fraction of false positives allowed, we first compute the number m of stream sources that can be false positives. then, m stream sources that are currently satisfying the query are requested to stop sending their data to the system. for the remaining streams that satisfy the query, they are assigned with the filter bound with the range r. notice that regardless of whether the values of the m \"shutdown\" stream sources satisfy the range query, the query answer is still acceptable within the tolerance. hence, as long as no updates are received by the system, the query answer remains correct with respect to the tolerance. similarly, a maximum number of false negative stream sources are stopped from reporting updates. since only the stream sources with the filter bound r send their updates when their values cross the bound, our protocol saves network and energy costs of data transmission."
"in this section we introduced a traffic model that generates a desired velocity as a sum of three velocity terms provided by the specific interactions, as it can be seen in eq. (1). this velocity depends on the world agents observe around themselves (set of r j s and v j s) and the actual values of the free model parameters, which are summarized in table i ."
"statistical analysis is performed to establish conclusion validity i.e. to validate the effectiveness and efficiency of the proposed hybrid (adaptive pso and de) algorithm over other techniques that have been considered for comparison. it is shown that the proposed hybrid (adaptive pso and de) algorithm is significantly different to de, pso, ga and random search that are considered for comparison; all except random search have been guided by the same fitness function. adaptive inertia weight and neighbourhood search strategy have improved the performance of the proposed hybrid (adaptive pso and de) algorithm with respect to the measures collected. threats to conclusion validity may arise from the fact that the infeasible uses / infeasible data-flow paths are identified and eliminated by manual analysis. also, results for the proposed hybrid algorithm and other techniques have been compiled with respect to the experimental setup used for the present study."
"the fixerror routine restores query correctness by replacing the false positive/negative filters with ½l; u filters. recall from step 2 in fig. 5 that the violation of correctness requirements is due to the removal of an answer (in step 2(i)). let us investigate how fixerror tackles this problem. for the purpose of explanation, we classify the streams into four disjoint sets, based on the status of filters, and whether they belong to the query answer:"
"the uncertain parameter that is used most often in network design is demand value, which corresponds to the flow of casualties in the humanitarian context. in addition to this, we also incorporated uncertain transportation capacity into the model to achieve more realistic and more reliable results. the set of all possible flow of casualty scenarios and available transportation capacity scenarios are denoted by and, respectively. at the second stage, while realizing the possible scenarios and, the response decisions, including (i) triage, (ii) registration, (iii) treatment, and (iv) evacuation, are adopted over the planning period. let ( ) be the solution of planning and operating decisions at the second-stage depending on the scenario ( ), where represents a set of all combinations of scenarios and . let assume ( ) is the probability of each scenario occurrence, where"
threats to construct validity may arise from the fact that the performance of the proposed hybrid (adaptive pso and de) algorithm is evaluated with respect to the measures 'mean number of generations' and 'mean percentage coverage' for a particular subject program. other measures such as total number of fitness evaluations and average search time may have also been used for evaluation.
"b. prescribing the sst directly in the first layer of ocean component: in the restoring regions, the sst is equal to the model climatology plus the observational anomaly using formula (a2)."
"where, * + defines a set of uncertain parameters, subjected to noisy input data, associated with the second-stage decision-making problem. objective function (3) optimizes the control variables in the second-stage decision-making problem subject to noisy parameters. equation (4) . therefore, the expected value function is represented as follows:"
", what is the distance where two frontally moving agent can stop with late reaction and limited deceleration. moreover, we forge two target scenarios demonstrating the two most challenging aspects of traffic: i) multiple path conflicts; ii) flow bottlenecks."
"third, we proposed a robust stochastic optimization solution approach to cope with the infeasibility issues which may occur in the stochastic optimization problems. our proposed approach returns robust solutions which are close to any given scenarios with minimum dispersion from the optimal values. this has been validated by the optimality gap analysis."
"if fixerror is not invoked, we check whether there are still some false negative filters not assigned by incremental deployment yet (step 2(iv)). if so, we assign the ½1; 1 filter to stream source s i . this assignment does not violate the false negative requirements, since there are false negative filters not allocated. we assume that s i is likely to have values crossing the ½l; u bound in the near future, so that the ½1; 1 filter can suppress this update."
"to reduce undesired alignment, every agent may select the neighbours it aligns to. to decide whether alignment should be used for a neighbour (i.e., it is part of the set a i ), the agent first checks three conditions. the first two represent possible danger: d1) the neighbour comes towards the agent (with fixed angular threshold ± π 4 ); d2) the neighbour is between the agent and its target (with fixed angular threshold ± 2π 3 relative to the agent-target direction). the third condition is related to efficiency: e3) the velocity of the neighbour points towards the target of the agent (with fixed angular threshold ± π 2 ), i.e., they both head to similar directions in a flock. if at least two of these three conditions are valid, we use alignment. otherwise, we switch it off as the situation is safe enough and would not be efficient enough with alignment. fig (2) visualizes all conditions when alignment can be neglected."
"we just discussed how to view a k-nn query as a range query for the purpose of constraint deployment. recall that the definition of fraction-based tolerance is the same for k-nn query and range query. to develop a fraction-based tolerance protocol for a k-nn query, one may consider transforming a k-nn query to a range query, and then, directly apply ft-nrp. unfortunately, this is incorrect. as we show shortly, the þ and à parameters of a k-nn have to be first converted to two other values."
"for zt-nrp, the initialization process needs a cost of c b for broadcasting the ½l; u bound to all stream sources. during maintenance, a cost of c u is needed for a stream source to report its value. as for ft-nrp, during initialization, all stream sources send their initial values in response to the broadcast request from the server (step 1). therefore, a cost of c b þ n c u is needed. then, the server broadcasts a message which consists of the ½l; u bound to every stream source, with a cost of c b . after the initial set of false positives and negatives is determined, the ft-nrp needs a cost of ð1 à !þðe maxþ þ e maxà þc d to deploy the false positive and false negative filters to stream sources (steps 6-9). thus, the communication cost during initialization is"
"statistical analysis is performed to validate the effectiveness and efficiency of the proposed hybrid (adaptive pso and de) algorithm with adaptive inertia weight and neighbourhood search strategy over other meta-heuristic search techniques (de, pso and ga) and random search applied for test data generation in accordance to data-flow coverage criterion. the experiment on each subject program was repeated 100 times. from the experimental results as presented in section 7.4, it can be seen that the proposed hybrid algorithm as well as the other meta-heuristic search techniques (de, pso and ga), all guided by the same fitness function, have comparable results with respect to the measure 'mean percentage coverage' for population size 10 and 15. the proposed hybrid algorithm achieved 100% data-flow coverage for all the subject programs for"
"in eq. (6) the a · π jump at π 2 magnifies the lane-forming phenomenon, what is already present in similar systems at equilibrium [cit], but we want to accelerate its construction to increase the flow. similarly, the deviation from the identity function in h ↑↓ (.) results in the acceleration of evading provided also by the self-driving term, as can be seen later."
"the parameters þ and à are user-specified. the system has to guarantee that the fraction-based tolerances are met. we assume that þ and à are both smaller than 0.5, because in most scenarios, users are not interested in results with more wrong answers than correct ones. this assumption is also required for guaranteeing the correctness of our protocols."
"definition 2 (fraction-based tolerance). given query q, answer set aðtþ, maximum false positive tolerance þ, and maximum false negative tolerance à, the answer set aðtþ is correct w.r.t."
in our momentary force-based model the ith agent (with position r i and velocity v i ) calculates its momentary desired velocity according to the interaction terms described above:
"to minimize the uncertainty in the parameter choice we also swept around all the final parameters and based on 5 simulations at each parameter value we chose the most effective version for the real flight system size. switching between the human pattern-recognition and the data-driven computer analysis is called the centaur method, and has been successfully applied in medical challenges [cit] . the concluded parameter sets were so close for the two target scenarios that we kept the set of the more difficult crosswalk scenario. the final parameters can be seen in table i ."
the following research questions are formulated to evaluate the performance of the proposed hybrid algorithm: calculate the difference between the jth components of the position vectors of particle k and particle l 32.
"while numerical tolerance is useful, choosing an appropriate value of it may not be straightforward. in particular, specifying a numerical tolerance requires some knowledge about the relative distances or spread of the objects. for instance, should be 1 meter or 100 meters? in a sensor network, various kinds of data such as humidity, temperature, and uv-index are collected [cit] . if only a numerical tolerance is allowed, the user may need to know a reasonable range of error for each data type. also, if a data stream contains multidimensional data (e.g., location, speed) or multimedia data (e.g., images), a numerical, or value-based error, could be difficult to specify. a bad choice of the numerical tolerance may significantly weaken the value of a query. from fig. 1, we can see that a large number of objects whose locations are outside r (i.e., those colored in black and white) are also included in the answer. to solve this problem, a user has to be careful not to set too large. however, if is too small, it may not be very useful for improving the system performance. thus, finding a reasonable value of can be difficult."
"selection heuristics. we explore how ft-nrp is affected by the assignment of false positive/negative filters during initialization. specifically, we compare the performance of two heuristics: random and boundarynearest. fig. 9f shows that boundary-nearest is only slightly better than random. the difference is small because the assumption that values closer to the query boundary are more likely for its future values to cross the boundary again may not hold. for example, an object can be moving away from the boundary, and so, even if it is close to the boundary during initialization, it is not necessarily worth to assign a false positive/negative filter to its corresponding stream source. as we will illustrate, the use of incremental deployment-assign a fraction of 1 à ! filters to the stream sources, and only allocate a filter to the stream source when it generates an update-often results in a better performance."
"the solution approach proposed in this section is partly inspired from the sample average approximation (saa) technique [cit], which is based on an approximation of the stochastic model by an equivalent deterministic mixed-integer programming (mip) model. the methodology incorporates the saa method, the robust counterpart problem and the feasibility restoration technique to solve the stochastic ccp network design problem with uncertain parameters."
"considering both solution robustness and model robustness represented in (8) and (9), respectively, the robust counterpart of the second-stage stochastic programming is given as follows."
"the motivation for the four major interactions in our model can be viewed as steps from rudimentary to complex behaviour. repulsion and alignment are fundamental terms of coordination used in the majority of flocking models [cit] . repulsion is in charge of avoiding collisions, while alignment aims to synchronize motion and thus in general to diminish oscillations. additionally, we use selfdriving to make the agents move circumspectly around each others and prevent collision conflicts, and self-organized queueing behaviour, which is already halfway between local and global problem-solving, as agents share information about the global target of their local neighbours in the queue."
we use csim 19 [cit] to simulate the environment illustrated in fig. 3 . we test the performance based on both real and synthetic data.
"real experiments were performed with our tailor-made quadcopter fleet, designed for swarming missions. the drones are based on a pixhawk low-level autopilot [cit], running a custom modified version of the arducopter code [cit] . positioning is based on gnss, which required an open air outdoor space. the low level autopilot receives desired velocity commands at 20hz rate from an odroid minicomputer. this high-level autopilot executes our traffic algorithm, using information received from other fleet members through an ad-hoc wireless network in the form of udp packages. communication between drones and computation of the actual control signals remained local and distributed at all times."
"let t be the current time instant with t ! t u . there are two different cases of updates to consider. case 1. v i 2 ½l; u. this means that s i, previously not in the result, is now an answer. we handle this by inserting s i into aðt u þ (step 1(i) ). the number of false positives, e þ ðtþ, is unchanged. as jaðtþj becomes jaðt u þj þ 1; f þ ðtþ cannot be more than e maxþ ðtuþ jaðtuþjþ1 (3), and is less than þ (11)."
"this section presents the possible validity threats [cit] for the proposed study. threats to internal validity are considered in the context of sbst. the choice of algorithmic parameters such as population size, inertia weight, acceleration constants, maximum velocity, mutation scaling factor, crossover constant affects the performance of the meta-heuristic search algorithms. preliminary experiments were carried out to determine the appropriate values for the various algorithmic parameters for the proposed hybrid (adaptive pso and de) algorithm."
"several damip experiments are useful to gmmip. the histall (enlarged ensemble size of historical all-forcing runs in deck), histnat (historical natural forcings-only run), histghg (historical well-mixed ghg-only run), and histaer (historical anthropogenic-aerosols-only run) experiments of damip will be used in the analysis of changes in global monsoons dating back to 1870. analyzing combinations of the histall, histnat and hist-ghg ensembles will allow us to understand the observed evolution of global monsoon precipitation and circulation www.geosci-model-dev.net/9/3589/2016/ geosci. model dev., 9, 3589-3604, 2016 changes since 1870 in the context of contributions from ghg, the other anthropogenic factors and natural forcings. the contributions of these external forcings to global monsoon changes will be compared to those from modes of internal variability such as the ipo and amo."
"one of the key recommendations of this report is the importance of designing an efficient logistical network in cases of disasters with mass causality. with this motivation, a robust two-stage stochastic programming model is formulated to develop a logistics network design problem for the casualty collection points in the event of a disaster."
"1. put s i into b by associating s i with the ½à1; 1 filter. 2. put s y into d by associating s y with the ½1; 1 filter. from fig. 6b, s y is replaced by s i . also, the displacement of s z from d is compensated by the arrival of s y . thus, the sizes of all the four sets remain the same, and correctness is fixed. also, no false positive or false negative filters are compromised after the fixing procedure."
"if the goal of the fleet is not to flock together coherently, alignment may seem superfluous. indeed, it turns out to be useful in any stochastic collective flight to reduce oscillations and to prevent dangerous situations by eliminating too high velocity differences between close-by neighbors. in other words, while repulsion is a form of excitation, alignment acts as a useful damping term. our alignment term is"
"how would a user express a tolerance for this query? one possibility is to let the user choose a numerical tolerance, say, and the system guarantees that the id of any object returned to the user must be located inside the dashed-line rectangle (but not necessarily inside r). this kind of tolerance, expressed as a numerical value, is often assumed by filter-bound-based approximation techniques. in fig. 1, the filter bound is exactly the dashed-line rectangle and installed in each moving object. this means that even if an object is outside r, it needs not report its location to the system as long as it does not cross the filter bound."
"let us first consider the computation cost of the processing server in zt-nrp. in the beginning, zt-nrp just needs to send the ½l; u bound to the stream sources. any update received from the stream sources will be used to refresh the query answer. thus, both initialization and maintenance need oð1þ time."
"in the proposed hybrid (adaptive pso and de) algorithm, a differential velocity term inspired by the de mutation scheme is computed by taking the difference of the position vectors of any two distinct particles randomly chosen from the swarm. a random number r is generated between 0 and 1. if r is less than de crossover probability, equation 7 (given below) is used to update the velocity of a particle. in equation 7, the cognitive term (second term) in equation 1 is replaced by the differential term scaled by de mutation scaling factor."
"in the second 'star' scenario all of the agents have one common destination: the center of the square-formed, lsized arena. having reached this destination they head to a randomly selected point on the edge of the arena. this bottleneck situation is a perfect testbed for the radial queueing behaviour (and could be of interest e.g. for drone package delivery applications where a central post office has to be reached by all agents)."
"initialization. to ensure that no more than a fraction þ of the answer set (i.e., aðtþ) can be wrong at any time t, the server first captures the states of the streams at time t 0 (step 1). then, aðt 0 þ and y ðt 0 þ; the set of objects which do not belong to aðt 0 þ, are evaluated (steps 2 and 3). next, the algorithm broadcasts a filter bound ½l; u to all the stream sources involved (step 4). a subroutine called calerror is invoked in step 5, which computes the maximum number of false positives (e maxþ ) and false negatives (e maxà ) allowed, without violating query correctness (refer to the bottom of fig. 5 ). according to (3),"
"besides range query, we also study filter bound protocols for a rank-based query, another important query type in streaming applications. contrast to a range query, a rankedbased query returns ids of objects based on their relative rankings. for example, in a transportation system, a longstanding k-nearest-neighbor (k-nn) query can be issued, which continuously returns the ids of k vehicles closest to a given query point [cit] for a long period of time. in a habitat monitoring system, scientists may be interested in tracking the k areas that yield the highest temperature, in which case a top-k query can be used [cit] . as another example, in network traffic analysis, it is important to identify heavy hitters [cit] . a heavy hitter is an ip source that delivers a large number of packets to the monitored network, which is also likely involved in dos attacks. if the user is interested in monitoring the k sources that yield the largest traffic volume, then a continuous top-k query can be used. notice that both the range query (a nonrank-based query) and the k-nn/top-k query (a rank-based query) return sets of object ids, rather than numerical values as answers. these entitybased queries are good candidates for using fraction-based tolerance, which does not use numerical values."
"robust counterpart problem accounts for the second-stage decision-making problem with uncertain parameter realization, without loss of generality, we mainly focus on the formulations given in (6) -(7)."
"the organization of the remainder of this paper is as follows. in section 2, we provide a literature review and highlight the main contributions of this paper. section 3 represents a generic robust optimization modelling approach and a two-stage formulation for the problem context presented in the paper. section 4 contains a robust stochastic optimization procedure as well as the validation procedure. in section 5, we study the application of the model to the case study; we provide experimental results for extensive realistic problem instances; we discuss these results and performance of the solution methodology."
"in our anisotropic repulsion term we distinguish agents based on the difference between their direction of motion. if the jth agent is close to the ith agent, but their direction of velocity is the same (with an angular threshold of ± π 3 ), then it is not the same type of threat as if they go into the opposite direction. in the latter case, it is better to evade each other than to oscillate. in fact, in jam situations it also seems favourable to form some kind of chain of the agents going in the same direction."
"as a result of numerous simulation, r qr defines the queueing behaviour more intensively than r q, so the latter will be ruled out from the free parameters, with the following estimation. the n agents closest to the common target all take each other into account if the furthest is r qr /2 away from the target. this implies that the nth agent is nr q away. all the n agents need free space with radius r s around themselves, so (nr q ) 2 nr 2 s . from all this we get"
"one direct way of lowering transmission cost is to drop some of the data items generated from the stream sources. the drawback is that the server may have to process queries based on inaccurate data. however, if we can carefully select the items to be dropped, the accuracy of a query answer may only be affected slightly. consider a transportation monitoring system, where vehicles within a geographical region (e.g., the city center) can be tracked [cit] . for vehicles that are stationary, or are located far away from the region, it may not be necessary for these vehicles to always report their positions to the system (in order to have high query accuracy). more generally, for many standing queries, a user may accept an answer with the maximum error allowed (or tolerance) in exchange for lower resource consumption and better timeliness in query processing. other examples for which controlled query errors are acceptable include wide-area resource accounting and load balancing in replicated servers. intelligent protocols have been proposed [cit] to wisely control when stream sources should report updates. the goal of the protocols is to reduce communication overhead, while at the same, time userspecified tolerances are met. these protocols make use of filter bounds-a system-specified range of values. a stream source only reports an update if its value crosses the bound."
"the two metrics used to measure the protocol performance are: 1) the total number of messages generated and 2) the amount of energy consumed, based on the cost model described in sections 6.2 and 6.3. for the parameters of power consumption, we adopt the specifications used by an mica2 mote [cit] : the operation voltage of each sensor is 3.6 v, the transmission power p t is 77.4 mj (at a distance of þ10 dbm), the reception power is 25.2 mj, and the sensing power p s is 2.52 mj. the sensing frequency is once every 5,000 time units."
"in such applications, stream sources are installed to collect and report the states of various entities. a large number of sources (e.g., gps-enabled devices, internet hosts, and wireless sensors) report their updated values (e.g., locations, tcp packets, and temperature values) continuously to the processing server. these systems often have limited bandwidth or energy resources. for example, in sensor networks, sensing devices usually have scarce battery power and communicate in a low-bandwidth environment."
"therefore, an approximate ( ) confidence interval for the expected true objective value is represented in the form of ( ), using equations (51) and (53). a statistically valid interval on the true objective value (with confidence at least ), denoted by ̂, and the statistical optimality gap percentage, denoted by ̂, are given in equations (54) and (55), respectively, as follows:"
"in step 6, ! is a system parameter that we call the deployment fraction. intuitively, ! controls the number of false positive filters assigned to stream sources in the initialization phase. why don't we assign these filters to all the e maxþ ðt 0 þ and e maxà ðt 0 þ stream sources in step 7(i)? the main reason is to provide more flexibility in choosing the appropriate number of stream sources to shut down. in particular, we may not know in advance which subset of the streams contributing to the answer aðt 0 þ would be better associated with the ½à1; 1 filters during the initialization phase. if a stream source is wrongly assigned a ½à1; 1 filter (e.g., the values generated from that stream are constant), this filter is \"wasted\" since it does not save any potential update that crosses the ½l; u bounds. on the other hand, for a stream whose value changes around the ½l; u bounds, it is better to associate this stream source with a false positive/negative filter. the maintenance phase, explained next, allocates unused filters generated in the initialization phase to stream sources only when they signal to the system that their values have crossed the filter bounds. we term this method, which gradually allocates false positive/negative filters to the stream sources, as incremental deployment. we will also explain later how incremental deployment can be beneficial to ft-nrp. we will also illustrate in the experimental results that it is often better to have a nonzero value of !, rather than deploying the filters all at once. false negative tolerance can be exploited in a similar way. let n à ðtþ be the number of stream sources allocated the false negative filter constraint (denoted by"
"when à is less than 0.5. hence, a k-nn query answer has a size between k 2 and 2k. this property will be used in the design of our protocols."
"in this section, research questions, algorithmic parameters settings, details of the subject programs, and experimental results are provided. de, pso, ga and random search techniques are also implemented for comparison with the proposed hybrid (adaptive pso and de) algorithm."
"in such uncertain environment, decision makers are to act without exact or complete information about number of casualties from the affected areas and the transportation capacity for moving casualties to ccps and hospitals. these factors cannot be confidentially estimated due to the unpredictability of time, place and severeness of a disaster as well as the changing roadway infrastructure as a result of disaster impacts . in the context considered here, the number of casualties with different levels of injuries coming from the affected areas over the planning horizon and the transportation capacity for moving casualties are uncertain parameters. the uncertainty about future realizations of these parameters are considered in the form of random sample of scenarios incorporated in the problem formulation."
"our future research will investigate a hybrid simulation-optimization approach for casualty evacuation based on ccp network structures that has been identified in this work. the inclusion of the medical supply flow from the multiple available hospitals to the established ccps for the purpose of casualty treatment can be another direction to develop this problem towards a more realistic context [cit] . in this regard, simulation approaches like discrete-event simulation (des) could be used for modelling of healthcare supply chains [cit] ). yet area of interest is the use of qualitative system dynamic at the tactical level as an alternative to the scenario generation in the optimization model to overcome the complexity of the problem [cit] ). an extension to the robust minmax regret stochastic programming model can be another interesting research topic to consider in the humanitarian logistics network problem [cit] . as the casualty accessibility to ccps plays an important role in humanitarian logistics, a maximal accessibility network design can be further extended [cit] ."
"in this section, the validation procedure, represented in figure 4, is used to examine the accuracy of the solutions found by the proposed robust optimization solution method. all instances are tested and their associated statistical optimality gap values are computed according to the validation procedure. a lower bound solution with 95% confidence level is computed using the averaging procedure with replication size and scenario size . then, using the best solution found from the average, the sampling procedure is applied with sample evaluation scenario size to generate an upper bound with 95% confidence level. we then calculate the statistical optimality gap percentage for each instance. the results are reported in tables b1-b8, in appendix b. to provide a clear view of the optimality gap percentage over the instances and its relationship with dm risk aversion attitude, we represent the average of optimality gap percentage over the instances for each corresponding value of * + in figure 8 . as can be observed, the optimality gap has a decreasing trend as the weight corresponding to dm risk aversion attitude increases. it is due to the fact that instances with higher weight of dm risk aversion attitude have the objective function with low variability and therefore with less optimality gap. it can be concluded that the more conservative the dm is, the less the optimality gap that exists. we finally compare the convergence rate of the proposed solution methodology to that of saa method by reporting the dual objective value and the best integer bound found by the solver in each iteration corresponding to an instance in figure 9 . results represent that the proposed algorithm converges to optimal solutions after about 100,000 iterations while the corresponding number related to the saa method is over 200,000, which shows the fast convergence rate of the proposed algorithm. this is due to the fact that the feasibility restoration technique is able to facilitate the proposed stochastic robust optimization approach to perform more efficiently and rapidly."
"the proximity metric has been quantified as the average path length to a ccp in the network structure for all instances. it has been observed that the network configuration by our methodology enables a dm to improve the proximity metric in a ccp logistical network design. we notify that a small improvement in the proximity metric can result in a significant increase in the number of lives saved. results also show that reduction in transportation capacity in stochastic programming can lead to increasing the dispersion of the solutions, however, our stochastic robust optimization approach is able to achieve solution and model robustness approaching the optimal solutions. we realize that the optimality gap in the stochastic programming can be improved by taking risk aversion attitude which results in less variability of the objective values."
generate sample scenario for scenario calculate the objective value ̂ ( ̅ ) with the given solution ̅ next calculate the objective value ̂( ̅ ) with the given solution ̅ and all scenarios
"this comparison is illustrated in figures 6(a) and 6(b) showing two levels of available transportation capacity after a disaster strikes. in these figures, the coefficients of can be considered as the risk aversion attitude of a decision maker (dm), where 0.1 attributes to a risk incentive dm and 100 relates to a risk aversive dm and is represented on the x-axis. the average number of opened ccps over the number of involved instances is represented on the y-axis. results illustrated in figure 6 (a) show that, when on average 90% of casualty transportation capacity is available, the saa method opens 39 locations for establishing ccps, on average, and is not sensitive to the risk aversion attitude of a dm."
"the tier-3 experiments is generally the same as the \"amip\" run in the cmip6 [cit] except that some key topographies or the air-land sensible heat flux are modified. the aim of the \"orographic perturbation\" is to geosci. model dev., 9, 3589-3604, 2016"
"casualty management operations emphasize the necessary functions including (i) registration, (ii) temporary hospitalization, and (iii) evacuation to hospitals or safer places, in the humanitarian logistics [cit] . considering this sequence of operations explained in the context of the"
"in order to measure the quality of a complex emergency network design, we introduce the proximity metric which is defined as the total distance travelled in the network to the number of links associated with all pair nodes, i.e. from the affected areas to the established ccps, also known as the average path length. proximity is an important metric in humanitarian logistics and has been extensively used in this context [cit] . let us indicate the solution value of allocation decision variables by ̂ . the proximity metric is then formulated as ∑ ∑ ̂ ∑ ∑ ̂ which represents the average path length to reach a ccp. results illustrated in figure 7 reveal that our proposed robust optimization method designs a network in which the average path length (shown as a dash-line in figure 7 ) has improved in comparison to the saa method. this can be confirmed by the results of increasing in the number of ccps that are opened, as illustrated in figure 6 . we then investigate the number of lives loss, also known as mortality in this work, to see whether it is influenced by the average path length improvement. as shown in figure 7, on average, the mortality rate experienced a significant reduction from 438 individuals to 294 individuals due to the decrease in the average path length. in general, figure 7 suggests that a small improvement in proximity to ccps can result in a significant decrease in the number of lives saved. this analysis also addresses the equity, also known as fairness, which tackles the discoordination of operational decisions for providing appropriate emergency services to casualties. when it comes to relief contexts, this metric measures the unsatisfied demand associated with each operational decision over the planning period [cit], which refers to mortality in our case. moreover, the average path length which denotes the rapidity is also widely considered as the equity metric [cit] . it can be interpreted from figure 7 that overall, the equity metric has been improved by the modelling approach we proposed in this work."
"where a is the acceleration limit, p is a linear gain in the v-r plane and d is a distance offset for d(.) in the v-r plane. the square root part corresponds to a constant deceleration. the reason one needs to cut it -continuously to first order -with a linear part is that a little noise in distance can cause enormous difference in output velocity near the point of infinite derivative."
"s.t. constraints sets (13) - (17), and constraints sets (19) - (35). where, the first three terms in (36) denote the first-stage objective function and the last term denotes the expected objective function of the second-stage problem."
"despite the contribution of the abovementioned efforts on the interdependency of casualty transportation and shelter (ccp) location decisions in humanitarian logistics network design, the main shortcoming is the neglect of temporal hierarchy relationship between the strategic and planning decisions and the dynamicity of casualty arrivals as illustrated in figure 1 . strategic decisions are adopted at the beginning of response phase in an uncertain environment where exact or complete information about the number of casualties are not available. then, scenario-based multi-period decisions are made during the response phase in which we assume the horizon is composed of a set of discrete operational cycles. note that a user makes periodical decisions on a timely basis (e.g., hourly, 8-hourly, 12-hourly and daily). in fact, the consideration of casualty state transition from one operational cycle to the next and of hierarchical setting between decisions results to a multi-period two-stage stochastic program model for the humanitarian logistics network design problem."
"rank-based query. given a number k 2 @ (k is called the rank requirement), a rank-based query returns ids of objects that rank kth or above. here, we use k-nn queries to illustrate filter protocols, since such queries are common in systems like computer-aided manufacturing (cam) in a productline monitoring system, mobile environments, and network traffic monitoring [cit] . a cam, for example, uses k-nn queries to discover similar patterns over multidimensional data obtained from sensors installed in production lines. a k-nn query can also answer k-min and k-max queries. notice that a k-min (k-max) query is just a k-nn query by setting the query point q to à1 (respectively, þ1). 2. a range query is specified by an interval ½l; u."
let e maxþ ðtþ be the maximum number of answers that can be incorrect in aðtþ and e maxà ðtþ be the maximum number of stream sources that satisfy the query but are excluded from aðtþ. from (1) and (2)
"we also note that the effect of immediate compensation is more profound at smaller values of !. this is because at a large !, more false positive/negative filters are preserved for future use, and it is less likely for fixerror to be invoked (fixerror is only executed if all the false positive/negative filters have been assigned). in fact, although not shown in the graph, when ! is larger than 0.5, immediate compensation almost yields no benefit. thus, immediate compensation is more useful for small ! values."
"in this paper, we developed protocols to improve the performance of data stream management systems. the main idea of these protocols is to translate fraction-based query tolerance to filter bounds, and transmit these bounds to the stream sources. the stream sources can then conditionally drop their updates without affecting query correctness. we designed algorithms that initiate and maintain filter bounds for nonrank-and rank-based queries. we further presented two variations of these protocols, namely, incremental deployment and immediate compensation. through detailed testing on real and synthetic data, we verified the effectiveness of our protocols in reducing communication and energy costs."
"the main external threat to validity is the choice of subject programs that may limit the generalization of results of the proposed study to real and more complex programs. also, a different population size apart from those considered may produce different coverage results."
"while aerosol-cloud interaction (aci) effects are partially incorporated in gcms with various levels of complexity, the aerosol-radiation interaction (ari) effect, which is believed to have more explicit impact on land-sea thermal contrast by reducing the surface solar insolation, is fully incorporated in most of the cmip6 models. to investigate the aerosol impacts on monsoon climate including both local forcing and remote forcing effects, we will examine the responses of climate models to natural (solar variability and volcanic aerosols) and anthropogenic (ghgs and aerosols) forcings based on deck and damip experiments. in particular, we will quantify and compare the separate climatic response of natural vs. anthropogenic forcing, as well as aerosol vs. ghg forcing, over the global monsoon area (e.g., [cit] ). we will analyze how different forcings influence the general circulation and precipitation characteristics, such as extreme events, shift of precipitation spectrum, and diurnal cycle."
"c. prescribing the surface net heat flux to restore the sst indirectly. [cit] for hist-resipo like experiment. in the restoring regions, the heat flux is restored using formula (a3). here α has the same meaning as that described in eq. (a2)."
"where * + is a set of vectors of fixed coefficients of the first-stage decision-making problem and of free of noise in input data. objective function (1) represents objective function of the first-stage decision-making problem and the expected optimal value of the second-stage decision-making problem, defined by ( ), and equation (2) denotes the structural constraints with fixed parameters."
"in relation to robust optimization, the overall performance and reliability of solutions are measured by calculating the coefficient of variation, i.e. standard deviation-to-mean ratio, through all the scenarios [cit] . we then calculate the coefficient of variation corresponding to decision variables used in the model over 144 instances and represent the minimum, mean, and maximum value of the coefficient of variation of each variable over all instances (see table 1 ). results show that the coefficient of variation of all operational decisions are considerably low, such that, for the majority of them it is less than 1%. however, the coefficient of variation value corresponding to the strategic decision of capacity allocation is 2.15% on average and which is not too large. in general, it shows that the solution values have low variability and are quite reliable."
to assess the star scenario we calculated the time between two agents reaching the central target in the queue. the statistical average of 20 simulations for each of the 48 mean free path values between 22 and 365 m is mostly independent of density (viz. reaching the central point is a bottleneck) with an overall value within 12±1 s. the number of collisions was zero in every simulation. visualizations of both scenarios can be seen in the supplementary video.
"a few technical challenges need to be addressed for filter bound protocols that exploit fraction-based tolerance. first, as we will show, when value updates are received from stream sources, the tolerance requirement can be violated (e.g., the fraction of false positives is larger than the allowed threshold). we tackle this problem by carefully adjusting the filter bounds of the stream sources so that query correctness can be restored. second, this correctness maintenance process, which involves several message exchanges between the server and the stream sources, can be expensive. moreover, some stream sources that are considered as false positives/negatives and are not involved in data transmission may have to be \"waken up\" to send data in order to maintain query correctness. we propose two new techniques, namely, incremental deployment and immediate compensation, in order to address these problems. the incremental deployment technique reduces the chance of expensive error fixing by only allowing a fraction of false positive/negative stream sources to be \"shut down.\" the immediate compensation method is designed to force an active data stream source to stop transmitting data, in order to compensate the \"waking-up\" of another stream source source."
"the range of α is [cit] . for lower values of α, increasing inertia weight can strengthen the particle's search capability. for values of α that are closer to 1, smaller inertia weight should be used. the inertia weight wi for the i th particle is therefore defined as a linear function of α and is calculated as follows:"
"essentially, given the tolerance þ and à, the values of þ and à must be configured to satisfy (16) . to maximize the benefit, þ and à should set as"
"software testing aims at assessing the quality and reliability of software product by detecting as many defects as possible. the cost of software testing increases exponentially with the size of input search space, thereby making manual testing a difficult and tedious task. there are software testing tools available with capture and playback features to automate the execution of test scripts. however, the test cases are manually selected by the human tester and may not be optimal. it is therefore desirable to generate optimal test data that reveals as many errors as possible according to a test adequacy criterion [cit] . structural (white-box) testing tests software for its structure and has the inherent capability to expose faults. the structural test adequacy criteria can be statement coverage, branch coverage, or path coverage that aim at executing every statement, branch or path respectively at least once. data-flow coverage, an effective and robust test adequacy criterion, focuses on the definition and usage of variables in a program. dataflow testing, therefore, could lead to more efficient and targeted test suites."
"step 6 computes the value of n þ ðtþ, which is a fraction ð1 à !þ of the maximum false positives allowed (with ! 2 ½0; 1). out of the jaðt 0 þj answers that satisfy the range query, we assign the ½à1; 1 filter constraints to n þ ðt 0 þ of them (step 7). since n þ ðtþ does not exceed the maximum false positives allowed, if no ½à1; 1 stream sources reply, the false positive requirement is met, i.e., f þ ðtþ þ . moreover, as n þ ðtþ stream sources are \"shut down\" from emitting updates, the amount of communication is reduced. as illustrated in our experimental results, this approach can also save battery power in a sensor network, since the sensors can be \"shut down\" and consume less energy than active sensors."
"the influence of the large-scale orography on the asian summer monsoon includes both mechanical and thermal forcing. various mechanisms have been suggested concerning the topographic effects; however, an overarching paradigm delineating the dominant factors determining these effects and the strength of impacts needs further study. we will analyze the tier-3 experiments to provide a benchmark of current model behavior in simulating the impact on the monsoon of the tip (as well as surrounding regions of significant orography; see table 2 for detailed descriptions) so as to stimulate further research on the thermodynamic and dynamic influence of the tip on the monsoon. in particular the relative contributions of thermal and orographic mechanical forcing by the tip on the asian monsoon will be addressed. we will extend the studies from the tip to other highlands including highlands in africa, north america and south america."
"the instances described in section 4.1 are solved after scenario generations on a 64-bit operating system server with a 2.7 gigahertz cpu on intel(r) processor and 72 gigabytes of ram. the proposed robust stochastic optimization approach, shown in figure 3, is performed using the optimization solver gams tables b1-b8 of appendix b."
"if the set of constraints (7) has no feasible solution, the second-stage decision making problem is infeasible. under this condition, there exists at least one scenario realization, for which ( ) ( ) and so ( ( )) . on the other hand, this problem could be unbounded depending on the first-stage variables and scenario realizations and hence ( ( )) ."
"while several humanitarian logistical problems studied the response network design for providing medical supply from prepositioned warehouses or staging areas to the affected people through the points of distribution (pod), this paper focuses on a network design with casualty response planning from the"
"as casualty flow in humanitarian logistics is unpredictable, in case of failure in the robust saa model, the feasibility restoration technique proposed in this paper allows us to reconsider the ccp logistic network structure and adopt appropriate design decisions accordingly. it is clear that the operational decisions at the second stage will improve as a result of improvement in the design decisions."
"to the best of our knowledge, a two-stage stochastic modelling for the ccp location problem with uncertain number of casualties with different levels of injuries under a multi-period settings and uncertain transportation capacity is lacking in the literature. given that this problem has the same nphardness property as a basic facility location problem, we have developed a heuristic robust method for solving the problem. this paper extends the literature related to the humanitarian logistic network design in the following three ways."
the optimality gap estimation is a way to evaluate the quality of stochastic solutions in two-stage programming where the true objective value is finite and the second-stage solution is feasible for almost every realization of the random data.
"a standing query q is associated with a tolerance constraint. we focus on the fraction-based tolerance, a kind of nonvalue-based tolerance. the remaining of this section presents and explains the definition of this tolerance."
the outputs of dcpp near-term climate prediction experiments will be used to assess the skill of global monsoons in initialized decadal climate prediction. the c-component of dcpp is similar to the tier-2 [cit] . the outputs will be used to add to the ensemble size of pacemaker experiments from gmmip tier-2 [cit] period.
"strategic decisions on the number, location and allocation of ccps are made through anticipating the plausible scenarios for the operational decisions in the second stage. although several studies in the location-evacuation literature investigated the humanitarian logistic network design, they almost considered deterministic or mean-value information. in this paper, we develop a two-stage stochastic programming modelling approach to cope adequately with the uncertainty inherent in disaster contexts, where the value of stochastic information is high. it has been shown that the inclusion of uncertainty at the strategic level improves the quality of the ccp design decisions [cit] ."
"in logistic network design, there exists temporal hierarchical structure between initial design considerations and the subsequent planning and operational decisions; this implies that these decisions are made under uncertainty [cit] . [cit] emphasized that individual optimization of the logistical decisions may not guarantee an optimal solution for the whole operation. [cit] showed that the integration of the design and planning decisions could improve the quality of solutions in network design when demand is uncertain. due to unpredictability concerning the magnitude of a disaster, number and location of"
"where represents the provisional cost of travelling per distance unit. the objective function of the first stage model, presented in (12), contains the expected cost of the second-stage problem after uncertainty realization and the cost related to the travelled distance. constraints (13) ensure that each affected area can be allocated to each established ccp. constraints (14) show that only the established ccps are allowed to be allocated to the existing hospitals. constraints (15) represent that each affected area must be allocated to either established ccps or existing hospitals. constraints (16) guarantee that each operating ccp must be allocated to at least one of the existing hospitals. the binary decision variables are given in (17)."
"the classical stochastic programming is likely to be infeasible especially when the distribution of uncertain parameter is unknown, or the uncertain parameter realizations do not follow a specific distribution [cit] . due to the lack of information or imperfect data in disaster management, such as, location and time of disaster, it's severity in terms of number of casualties, and available transportation capacity subsequent to the disaster, the parameters are almost unpredictable or are forecasted with a wide range of variability. this, coupled with the need to execute a large number of scenarios, will most likely produce infeasible solutions to the stochastic programming [cit] . to tackle possible infeasibility due to the presence of uncertain parameters and the risk attributed to the decision-maker, robust counterparts problem is proposed. its purpose is to find an optimal solution that satisfies all constraints for any uncertainty realization while reducing the risk of dispersion of the objective function value. in the robust optimization literature, two performance metrics that have been widely applied are the concept of solution robustness and model robustness [cit] . our robust counterpart problem studies both solution robustness and the model robustness concepts simultaneously. since the"
"the saa method is performed when a feasible solution exists and the problem has a finite objective value [cit] . however, the uncertain parameters in humanitarian logistics may not have an identical distribution or a known distribution parameter. in such a situation, the saa method is prone to return infeasible solutions by violating some of the constraints in at least one scenario. to tackle this challenge, we provide a robust counterpart problem for the represented saa method, involving robust solution and robust model, proposed by, in the following subsection."
"s.t. constraints (13) - (17), constraints (19) - (22), constraints (24) - (35), and constraints (38) - (42) that is to say, the model does not guarantee that obtaining solutions satisfy the system constraints for all scenario realizations and do not converge to the optimal solution. this failure can be partly due to the inappropriate set of location and allocation decisions or inadequate capacity acquisitions in the network structure in the first-stage decision-making problem. in other words, not all choices of design decisions give rise to feasible solutions. to achieve feasible solutions when the infeasibility variables return nonzero values, we apply a feasibility restoration technique on the non-algebraic constraints, i.e."
"uavs in common airspace are already challenging current centralized traffic-organizing schemes while their number is expected to explode in this decade. to assist or even replace human airspace control efforts, hybrid and fully decentralized aerial traffic solutions will become a key in the near future. to facilitate this trend, we show a self-organizing solution to control large numbers of autonomous drones with individual flight tasks. we use an agent-based algorithm that has been inspired by recent, custom-tailored variants of the original vicsek-model [cit], but also contains optimized traffic-specific interaction terms. the local nature of this approach promises the scalability of the solution to even global scale."
"a problem of fixerror is that the false positive/negative filters deployed to the stream sources will be replaced by the ½l; u filters. this causes more communication, since an ½l; u filter allows updates to be generated. to alleviate this, we introduce the immediate compensation, which attempts to maintain the number of false positive/negative filters."
"so far 21 international modeling groups have committed to contributing to gmmip (as shown in table 1 ). the diversity of the groups from different countries and regions demonstrates that the global monsoons topic appeals to a wide range of modeling and research communities. the models with various structures, physical parameterizations, resolutions, etc., will provide a large sample size to help reveal the causes of monsoon variability on interannual and longer timescales in the climate system. based on the experimental protocol (see sect. 3), both atmosphere-only and fully coupled oceanatmosphere versions of these models will be used."
"a robust solution is characterized by its proximity to the optimal solution of a stochastic programming model. we incorporate solution robustness by the inclusion of the mean absolute deviation of the secondstage solutions, indicated by ( ), over the number of scenarios in the saa model, as follows:"
"while most previous works in filter bound algorithms assume value-based queries (e.g., sum and avg), we study the entity-based queries. we investigate the computation and communication costs of our protocols. we also perform simulations to examine the effectiveness of our protocols. although we assume one-dimensional data here, our techniques can be generalized to higher dimension cases. to summarize, our contributions are:"
"this section presents the experimental results for various subject programs. for each subject program and each testing approach, 100 experiments were carried out. the measures collected are as follows:"
"the complexity of two-or three-dimensional aerial traffic with ever-growing number of agents can go beyond that of ground traffic [cit], even though the only infrastructure needed, air, is present everywhere and in general there is a lot more space in three dimensions than on one dimensional roads. at the same time, aerial traffic might also be constrained by virtual roads and restricted air spaces, close-to-ground air traffic has to handle obstacles in 3d, while motion and communication in the air is always much more challenging than on the ground [cit] . centralized path-planning -e.g. game theory based [cit] or bioinspired [cit] -calculates close to optimal routes for a couple of agents, but the scalability of such approaches is questionable due to central communication and computational complexity. self-organization is an excellent direction to address all these difficulties [cit] . this approach is used by biological systems [cit], and technological innovations [cit], too."
"if this neighbor exists, this will be the one in front of the agent in the queue, called the q-agent. in this case, the agent stops at a queueing distance r q further from the target than the q-agent. if the q-agent does not exist, the agent wants to slow down to zero exactly at the target."
"step 1: for each particle, euclidean distance is calculated from the other particles in the input search space using the position of particles. accordingly, other particles within a threshold euclidean distance (determined by preliminary study to fine-tune the algorithmic parameters) form the neighbourhood. euclidean distance between two particles xi and xj in the n-dimensional search space is given by the following equation:"
how effective is the proposed hybrid (adaptive pso and de) algorithm for optimal test data generation with respect to the convergence speed (mean number of generations) at termination?
"as discussed earlier, robust optimization approach enables dms to generate solutions while reducing the risk of dispersion and ensuring the solution concentration in an uncertain environment. in this work, we measure the dispersion of the objective function values over all given scenarios, found by the proposed model, as a metric to evaluate the solution robustness. this metric gives rise to the standard deviation of the objective values which represents the closeness between them. results illustrate that the dispersion of objective values in the uncertain environment increases, as the transportation capacity contributes to 10% reduction, on average. results also suggest that the standard deviation of our proposed optimization approach is slightly larger than the stochastic programming method; this can be due to network expansion and the resultant distribution of entities throughout the optimized network. overall, the dispersion of the objective value in both cases, i.e. saa method and the proposed stochastic robust optimization method, are negligible (less than 10 -8 )."
"in this paper we presented an agent-based, decentralized and scalable solution for difficult 2d air traffic situations. we proposed four new interaction terms: repulsion became anisotropic and alignment became selective to adapt more to traffic situations, while queueing and self-drive got introduced to resolve oscillations and jams. eliminating these problems enabled our uav fleet of 30 drones to complete random missions of coordinated flight with conflicting tasks in dense environments."
"where, c1 and c2 are positive learning constants called cognitive and social scaling parameters chosen in such a way that their sum never exceeds 4, and r1 and r2 are two random numbers in the range [cit] . the inertia weight w controls the impact of the previous history on the new velocity of the i th particle. a particle's velocity in each dimension is clamped to a maximum magnitude vmax. the position and velocity of each particle in the swarm are continuously updated until an optimal solution is achieved."
"since two sets of uncertain parameters are concerned in this paper, i.e. the number of casualties and available transportation capacity, two sets of scenario generation should be realized in this model. by generating independent number of casualty scenarios given as * +, and"
new position xi j of particle i in dimension j is the position of the best particle in the neighbourhood 39. } 40. } 41.
"the tier-1 experiments are extended atmospheric model intercomparison project (amip) runs from 1870 [cit] . this is the entry card for gmmip. all external forcings (solar, aerosol, ghgs, etc.) should be derived from those used in the historical simulation of the cmip6 fully coupled model. this will allow for a direct comparison of the historical simulation"
"the experimental results reveal some practical and managerial insights confirming the importance of ccp logistical network design and operational response decisions in an uncertain environment. the findings show that the network configuration obtained by our proposed methodology has a significant difference with the saa method. more specifically, the proposed approach opens more ccps and is more sensitive to the transportation capacity; this can be contrasted with the saa method where no significant sensitivity has been observed. we notice that a conservative decision maker (dm), with risk aversion attitude, tends to open more ccps in an uncertain decision-making environment."
"thus r q can be defined by free parameters r qr and r s . there is one last possible situation when the agent has to stop before its target: when a neighbour is behind the target, but its security zone of radius r s intersects with the line segment between the agent and the target. this shall be resolved by stopping at this intersection."
the final step in de algorithm is the fitness-based selection of either target vector or trial vector in the next generation. f and cr are the control parameters of de. the performance of de depends on the manipulation of target vector and difference vector in order to obtain a trial vector.
"in the beginning of this section the origin and nature of the terms was introduced. after having rigorously explained the exact functioning of the terms, fig. 4 gives a general overview of the new features through two-agent interactions. as one can see on the first subfigure, isotropic repulsion with a constant target velocity leads to oscillating and ineffective motion and unselective alignment holds off the agents from leaving the impact zone even after passing by each other. anisotropy and selectivity offers a quick resolution of the two-agent conflict (second subfigure), but resulting trajectories are still not smooth enough. changing the constant target velocity to the one given by the self-driving term (third subfigure) smoothens out the trajectory a bit, but with the compromise of much slower motion. making the self-drive agile (fourth subfigure) further enhances the smoothness of the curves, improves the performance while maintains mutual avoidance. in the final solution distance between agents is increased, which increases safety but creates longer paths. however, speed is maximal most of the time which compensates for the loss of efficiency."
"meeting false negative requirement. again, there are two types of false negatives for a k-nn query. as shown in fig. 8, the first type of false negatives is caused by streams like s 3, whose last reported value v 0 3 is not within r, and is assigned with false negative filters. later, its new value v 3 is within r and its rank is raised to k or higher. the server does not know this, and so, s 3 is a false negative. the number of false negatives is at most k à, the maximum number of false negative filters. the second type is caused by stream sources with false positive filters like s 1 . again, s 1 was among the top-k objects since its last reported value v 0 1 is within r. however, its new value v 1 is less than v 2, so s 2 ranks k or higher (without notifying the server). the maximum number of this kind of false negatives is thus jaðtþj þ, the maximum number of false positive filters. since the maximum number of false negatives for k-nn query is given by k à, the sum of the two kinds of false negatives k à and jaðtþj þ must be less than k à . equation (7) simplifies this to"
"once the values of þ and à are correctly set, we can extend ft-nrp to exploit the fraction-based tolerance of k-nn queries. the corresponding protocol, called ft-rp, differs from ft-nrp in two aspects:"
"in the core framework of cordex phase 2 (cordex2 hereafter), a core set of regional climate models (rcms) downscales a core set of global climate models (gcms) over all or most cordex domains at 10-20 km resolutions [cit] . the comparisons of cordex2 historical climate downscaling with the driving gcms historical simulations, will give insight into the importance of model resolution and the added value of rcms in the simulation of climatology and variability of global monsoon, especially the global land monsoon. a comparison of cordex2 evaluation framework experiments forced with daily mean sst to highresmip tier-1 runs over global monsoon domains will provide information on the similarities and differences of the added values derived respectively from high-resolution global models and regional climate models."
"which means that at any time t, the number of false negatives (e à ðtþ) cannot exceed k. moreover, the number of correct objects in the answer returned to the user, i.e., jaðtþj à e þ ðtþ,"
"here, termination criteria is either 100% data-flow coverage or 10 3 generations, whichever occurs first. maximum number of generations is set to 10 3 . for more complex programs, the maximum number of generations may be increased. mean number of generations, however, is not indicative of full dataflow coverage."
"the tier-1 [cit] pinatubo eruption, using the same volcanic forcing recommended for the cmip6 historical simulation. it will be used in comparison with observations to understand the global monsoon response to injection of stratospheric aerosols over the tropics and to study impact mechanisms on global monsoon precipitation and circulation changes. via its ensemble design, volmip can address the substantial uncertainty associated with the effects of volcanism during the historical period."
"to increase the population diversity, a 'crossover scheme' is applied. the difference vector exchanges its components with the target vector xi to obtain the offspring/trial vector ui. the most common crossover in de is 'uniform crossover' as given below:"
"from the experimental results as shown in tables 7-10, it can be seen that the proposed hybrid algorithm with adaptive inertia weight and neighbourhood search strategy, achieved highest mean percentage coverage for all the subject programs and for all population sizes that are considered. only the proposed hybrid algorithm achieved 100% data-flow coverage for all the subject programs for population size 20 (except for program# 2 and program# 6) and for population size 25 (except for program# 6). for population size 10 and 15 also, the mean percentage coverage is 97%-99% with the proposed hybrid algorithm. for each program, infeasible uses, if any, were not considered while measuring dataflow coverage. infeasible uses, if any, are determined by careful manual analysis as it is not possible to write an algorithm for analyzing a given program to determine if a given element in the coverage domain is feasible or not [cit] . this, in addition to the novel fitness function, adaptive inertia weight and neighbourhood search strategy has resulted in full data-flow coverage as the population size is increased from 10 to 25."
"for immediate compensation, an extra message for updating the filter of s i may be needed, and so in the worst case, its cost is"
"consider the def-use path# 5 (1, 7, 8) for coverage from table 2 . this is a dpu-path that tests for 'equilateral triangle' condition. node 1 (source) and the p-use edge (7, 8) (target) form the two objectives -their dominance paths to be covered by an input test case. there are three cases -if the dominance paths of both the nodes are covered, fitness value of the input test case is 1 and it is optimal. however, if a partial aim is covered (one of the two nodes) or none of the nodes is covered, fitness value of the input test case is computed using equations 3.2 and 3.4."
"gmmip aims to answer four primary scientific questions: by focusing on addressing these four questions, we expect to deepen our understanding of the capability of models to reproduce the monsoon mean state and its natural variability as well as the forced response to natural and anthropogenic forcing, which ultimately will help to reduce model uncertainty and improve the credibility of models in projecting future changes in the monsoon. the coordinated experiments will also help advance our physical understanding and prediction of monsoon changes."
"the physical capacity for casualty treatment at established ccp is limited to, where represents the area of that ccp, in square-meter unit, and indicates the required surface to provide medical services to an individual, on average. however, the required capacity division for each injury severity level should be determined at each established ccp. the capacity division, shown by ( ), represents the part of capacity of ccp dedicated to injury severity level under scenario . this decision variable is adaptive to the uncertainty inherent in the model. constraint (22) guarantees that the capacity division is implemented at the established ccp and constraint (23) assures that the total capacity divisions do not exceed the total physical area of a ccp. constraint (24) verifies that the inflows of casualties from the affected areas to an established ccp do not violate its dedicated capacity for each injury severity level."
constraint (27) takes into account the current uncertain flow of casualties under scenario with injury severity level transporting from the affected areas to the established ccps and the hospitals.
"the tier-1 experiments of highresmip, which consist of amip runs with a minimum horizontal resolution of 25-50 km, will be used to compare with standard resolution control configurations and examine the added benefit, if any, of high-resolution models in reproducing both the mean state and year-to-year variability of global monsoons. it should be noted that the boundary conditions (both of sst and sea ice) used to build the amip experiments of highresmip is a new data set with daily time frequency [cit], which may make a differences when compared with standard amip forced by monthly data sets. the tier-2 [cit] s forcing including anthropogenic ghg concentrations and aerosol forcing, will be used in the analysis of climatology and variability of global monsoons, which aims to understand the role of air-sea interaction in modulating the simulation skill of the monsoon mean state and year-to-year variability. the anthropogenic aerosols are required to be prescribed in highresmip experiments following a standard method in cmip6 deck [cit], rather than interactive aerosol processes embedded in atmosphere general circulation models (agcms). different ways to deal with aerosols could lead to different aerosol distributions as well as aerosol forcings, which should be taken in consideration when comparing with gmmip experiments."
"the vast majority of studies in disaster and emergency management have focussed on the distribution of relief in the aftermath of disasters [cit] . in this context, stock location, resource allocation, and commodity flow from predefined warehouse locations to affected areas have been the most impactful variables to optimize for the construction of relief distribution networks. casualty management problems, such as the one presented in this paper, can similarly be construed in terms of ccp location, casualty medical treatments, and casualty flow from the affected areas to safer places and hospitals. in spite of the importance of casualty management in humanitarian logistics, relatively little attention has been paid to this subject [cit] . our work is, therefore, a contribution to this literature; specifically, we are concerned with the casualty management functions of disaster management that are caused by human error, such as a c c e p t e d m a n u s c r i p t 5 industrial accidents, and which are implemented after a disaster strikes (response phase of disaster planning)."
". a: stream sources in aðtþ with ½l; u filter, . b: stream sources in aðtþ with ½à1; 1 filter, . c: stream sources not in answer set (s à aðtþ) but with ½l; u filter, where s is the set of all stream sources, and . d: stream sources in s à aðtþ with ½1; 1 filter. notice that b is the set of stream sources installed with false positive filters, and d contains all stream sources with false negative filters. the legend on the top of fig. 6 illustrates these four sets of stream sources. the shaded boxes represent sets of stream sources with the ½l; u filters, while the white ones depict those with ½à1; 1 or ½1; 1 filters."
"ðjaðt c þjà2þàe maxþ ðt c þ, which is smaller than à because à 0:5. by assigning the constraint ½l; u to the filter of s z, correctness 2 is also met. this case is shown in fig. 6c ."
"the pre-industrial control simulations from each modeling group's deck experiments will be used to study the relation between global monsoon and ipo/amo at decadal timescale. comparing the control simulation (constant forcing) with the gmmip tier-2 decadal mode relaxation experiments in which all historical forcings are added will then allow us to find which parts of apparent decadal variations in the monsoons are caused by underlying sst, and which are more forced by externally driven sources, such as volcanic emissions. the cmip6 historical simulations will also be used to examine the response of the global monsoon to external forcings such as anthropogenic ghg and aerosol emissions. the results of cmip6 historical simulation will be compared with those of hist-resipo and hist-resamo in tier-2 to identify the relative contributions of external forcing and apparently internal modes of variability (ipo/amo)."
"since fixerror can be expensive, its execution should be avoided if possible. recall that our protocol uses the incremental deployment technique-that is, only some false positive/negative filters are deployed initially. intuitively, the correctness is \"stronger\" than required by the tolerance, and thus, reduces the chance for fixerror to be called."
"using the average and standard deviation estimators for m replications of samples of size n, an approximate ( ) confidence lower bound of the true objective value, denoted by, is given as follows:"
"a disaster may result in numerous lives being lost. however, the severity of potential threats in the aftermath of a disaster can be mitigated by providing fast and essential aids through intermediary sites."
"the main difference between the evolutionary optimum and the final parameter setup was in the role of the repulsion. evolution utilized repulsion for collision avoidance but in the real system we wanted to avoid repulsion-driven oscillations completely so we decreased the range of repulsion and the anisotropy and increased the safety distance of selfdrive instead as a preventive, conservative action, with the compromise of some fitness loss."
"a tool is developed for instrumenting programs and to generate def-use paths. dominator tree is generated manually. infeasible paths, if any, are determined by careful analysis of the program."
"similarly, man-made disasters have human, environmental and economic consequences. examples of such disasters include stampede, nuclear or chemical plant explosion, emergencies resulting from incorrect handling/transportation of hazardous materials, water contamination and oil spill. man-made disasters happen mainly due to accidents, negligence or incompetence. with the global increase in the number and severity of the disasters, researchers from different disciplines are increasingly paying attention to disaster management problems."
"since e à ðtþ is also unchanged, (12) also holds. thus, correctness 2 is upheld. the rest of step 1 uses the false positive filters not deployed in initialization. specifically, step 1(ii) invokes calerror to update e maxþ ."
"the range of the inertia weight is [0.5, 1]. pso is computationally inexpensive. the ability of pso to balance between local exploitation and global exploration of the search space enhances searching ability and avoids premature convergence towards the optimal solution."
"our simulation framework imitates realistic conditions by providing the agent imperfect information about its neighbours and its sensors due to limited communication range, non-zero communication time delay t delay and noise. the simulation also limits the velocity of agents to reach the desired value defined in eq. (1) with finite acceleration a max . all details about the used realistic conditions can be found in our previous work [cit] ."
we are interested in entity-based queries-those that return identifiers of objects as answers [cit] . we classify entity-based queries into rank-based queries and range queries.
"in a program, the definition and use occurrences of each variable are identified. a variable is said to be defined in a program statement (def-node) if a value is associated with the variable. a variable is said to be used in a program statement if its value is referenced for computational use (c-use node) or a predicate use (p-use node). data-flow testing should cause the traversal of def-clear sub-paths from the variable definition to either some or all of the p-uses, c-uses, or their combination. empirically, the all-uses criterion has been shown to be most effective compared to the other data-flow criteria [cit] . table 2 provides the list of all-def-use paths and node no. dominance path"
"let us recall that ( ) is the second-stage decision-making problem. as discussed earlier, expression (37) has to be minimized to achieve solution robustness. therefore, it is included in the objective function of the saa model. as it contains the absolute function which makes the saa model nonlinear, we apply a linearization approach to guarantee the convexity of the solution space. (37) is included in the minimization objective function, we can substitute it by the following expressions:"
"maintenance. we now discuss how updates generated from stream sources with ½l; u filters should be managed. assume that the server receives an updated value v i from s i at time t u . immediately, prior to receiving v i, according to correctness 1, the following must hold (using (3) and (4)):"
"the first-stage objective function follows the recommendations of the report [cit], where the ccp locations should be close enough to both affected areas and hospitals so as to facilitate the efficient movement of casualties (here, the casualties are people who have been affected by the disaster). this objective function considers a fixed cost that can be assigned to each potential location for establishing a ccp and relative cost associated with distance to travel from the affected areas to the"
we build up our control algorithm from four unique interaction terms. first we will give a descriptive explanation of each term which will be followed by exact mathematical equations.
"the steps of the proposed hybrid (adaptive pso and de) algorithm are given in figure 5 . the flowchart is given in figure 6 . inputs to the algorithm are an instrumented program, dominator tree of the program, list of def-use paths to be traversed and the killing nodes if any, number of input variables, domain range of each input variable, and the algorithmic parameters: population size, pso acceleration parameters, pso maximum velocity, de mutation scaling factor and de crossover probability. adaptive inertia weight is used as given by equations 3 and 4. for data-flow coverage criterion, the design of fitness function is explained in section 6.2 below. initial value of pbest and gbest is 0. the algorithm is run once for each uncovered def-use path. if the selected path is not covered by any member of the current population, fitness value is computed for each member. accordingly, for each particle, the personal best position pbest and the global best position gbest can be updated. during the evolution process, particle's position and velocity is adjusted according to equations 2 and 7 respectively. if the updated position of the particle is out of input domain range, a local neighbourhood strategy is applied. then, the greedy selection scheme of de is used to generate the new population. the evolution process continues until the termination criteria is met. the other uncovered paths are also checked for coverage. the output is an optimal test suite and a list of def-use paths marked as covered or uncovered, if any."
"step 2(i) removes this \"bad answer\" from aðt u þ so that the size of aðt u þ is reduced by one. step 2(ii) then updates e maxþ and e maxà . we check in step 2(iii) whether one of the constraints in (1) and (2)"
"in our previous study [cit], a novel maximizing fitness function is proposed for data-flow coverage adequacy criterion based on the standard metric (equation 9) and dominator tree. dominance relations between the nodes of the cfg are used to obtain pathcover for the nodes of the selected def-use path. the fitness function considers each def-use path as two objectives. for a dcu-path, the first objective is to cover the dominance path of the definition node and then to cover the dominance path of the use node. for a dpupath, the first objective is to cover the dominance path of the definition node and then to cover the dominance paths of the nodes of the p-use edge (u1, u2). a dpu-path is formed for both the branches (t/f) of the predicate node. a test case is evaluated with respect to the selected def-use path by executing the program under test with it as an input and recording the nodes that are covered. if a killing node is traversed between the source node and the use node, a fitness value of 0 is assigned to the test case and it is discarded. the fitness value is 1 if all the nodes of the dominance paths of both the objectives are covered; otherwise closeness of the test case to the missed objective (branch distance) is computed."
"here, xj and xk denote the position of particles j and k respectively (i≠j≠k) that are randomly chosen from the swarm. a survival of the fittest mechanism is also followed by incorporating the greedy selection scheme of de as given by equation 6. therefore, the particle either moves to a better location or remains at its previous position in the input search space. the current position of a particle will always be its best position."
"the tier-1 experiment of gmmip, i.e., the extended amip, uses the same resolution as in the deck [cit] . the amip-hist specifies external forcings that are consistent with those from the same model's cmip6 historical simulation over the 1870 [cit] period. to comprehensively investigate the proposed gmmip scientific questions, such as the impact of high resolution and roles of different forcing agents, the output from other related mips will be used in the diagnostic analysis of gmmip as described below."
"monsoon rainbands such as the mei-yu-baiu-changma front usually have a maximum width of about 200 km [cit] b) . climate models with low or moderate resolution are generally unable to realistically reproduce mesoscale cloud clusters embedded in the rainbands, thus partly leading to biases in the mean state, variability of monsoon precipitation and the northward propagation of these rainbands. we will examine the performance of high-resolution models in reproducing both the mean state and year-to-year variability of global monsoons. high-resolution rain-gauge observations and satellite precipitation products will be used to evaluate model performance."
"for the medical treatment, the available capacity dedicated to each injury severity level of a ccp should be taken into account. this matter is represented in constraint (29). let us recall that ( ) represents the number of casualties for temporary hospitalization. it states that the number of casualties receiving temporary hospitalization services cannot be more than the dedicated capacity divisions at a ccp. note that ( ) refers to the cumulative hospitalized individuals that corresponds to constraint (32)."
"19 lives lost due to facility capacity limitation occurs. similarly, constraint (34) states that when the number of ready-to-evacuate exceeds the casualty outflows, lives lost due to the limitation in transportation capacity occurs."
"where the double indices refer to the interaction between the ith and jth agents, from the viewpoint of agent i. the magnitude of repulsion v rep ij is calculated by the function"
"the agent reduces its target velocity iteratively. it chooses its closest neighbour which is inside the rectangle with length of the distance of the previously chosen neighbour (or the target at the start of the iteration) and half-width of a safety distance r s, pointing in the direction of v target . the agent separates the component of v target pointing towards the chosen neighbour and reduces it according to d(r ij, r s, p target, a target ). this reduced component is rotated with an angle of t · sin"
step 1. averaging procedure ( ) for replication generate sample scenario solve the proposed algorithm outlined in figure 3 and save ( ) compute the approximate ( ) confidence lower bound using (51) next
"ccps. an overall view of ccp establishment and operations is presented in figure 1 . in existing literature, several terms have been interchangeably used to denote these facilities, such as field treatment site [cit] or alternative care facilities [cit] . however, for consistency, we have used ccp for casualty collection point or facilities that are functionally similar to ccps. ccp locations are identified before the disaster occurrence, i.e. during the preparedness phase, but selected after the disaster has occurred, i.e. in the response phase (see figure 1) . after choosing the right location and establishing 1 the ccps, the following operational and tactical decisions are to be made in the response phase at ccps: (i) triage, (ii) casualty registration, (iii) casualty medical treatment, (iv) casualty evacuation, and finally (v) shutting down the site(s) [cit] . uncertainty affects strategic ccp location decisions, and which have a bearing on tactical and operational decisions. the network design decisions are strategic decisions that are made when forecasting uncertain parameters. planning and operational decisions, on the other hand, are usually made when parameters are more obvious (e.g., the parts of the city that may be affected due to an unfolding weather-related event). it is arguable that including strategic decisions would improve the quality of"
"to solve the above problem, we propose a fractiontolerance protocol for nonrank-based query (or ft-nrp in short). section 4.1 describes the framework of ft-nrp. section 4.2 investigates how tolerance can be restored if it is violated (by the arrival of data updates). we propose an enhanced protocol in section 4.3. section 4.4 extends ft-nrp to handle multiple queries."
"the experimental results have been presented above in tables 7-10 and figures 7-16 . in context of the research questions formulated for this study, the experimental results are analysed and discussed in this section. rq1: how effective is the proposed hybrid (adaptive pso and de) algorithm for optimal test data generation to achieve 100% data-flow coverage of a program?"
"in many data stream applications, battery power is a precious resource. for example, in wireless sensor networks (wsns), sensors are only equipped with a limited power source (e.g., 0.5 ah, 1.2 v) [cit] . therefore, it is important to preserve energy resources for the sensors in these systems."
"we describe the simulation results for range queries, a nonrank-based query, in section 6.5.1. section 6.5.2 presents the results for k-nn query, a rank-based query."
"for this study, various benchmark programs have been selected from other researchers' work [cit] in the area of sbst. experiments are also performed on programs taken from the sir repository [cit] . source code of the academic programs is taken from standard reference books [cit] . the programs, as given in table 6 below, have diverse structural elements such as loops, equality conditions, logically connected and nested predicates. a tool has also been developed for the instrumentation of programs and for listing of def-use paths."
"the scenario-based two-stage stochastic programming model represented above is a complex large-scale optimization problem, as a large number of scenarios is involved for uncertain parameters realization. to solve the two-stage stochastic ccp network design problem represented above, we are inspired by the saa technique [cit], which is based on an approximation of the stochastic model by an equivalent deterministic mixed-integer programming (mip) model. the saa model incorporates the equivalent deterministic mixed-integer program of the second-stage decision-making problem into the first-stage decision-making problem. the saa method has mainly been used to find near-optimal solutions for two-stage stochastic problems (schütz, [cit] ."
"in fact, this constraint verifies the additional ccp nodes are required for accommodating the inflows, as the existing areas of potential ccp nodes are not sufficient. to overcome this issue, we apply a model robustness approach, in which an infeasibility variable ( ) is taken into account in the system constraints, as represented in (9). the infeasibility variable ( ) shows the amount of infeasibility of each scenario in the model. it is clear that ( ) if the model is feasible. otherwise, it returns a positive value. however, a huge penalty number is assigned to the infeasibility variable ( ) in the objective function of the model to avoid being infeasible for all scenarios. we then modify the constraint (23), which refers to the -th ccp capacity limitation, by adding the infeasibility variable ( ), as follows:"
"correctness requirement 1. at every point in time, if no resolution is required, then the results of all running continuous queries remain valid within their tolerance constraints. correctness requirement 2. immediately after filter resolution is completed, the tolerance of a query is satisfied assuming that stream values do not change during resolution."
"the rest of the paper is organized as follows: section 2 provides a brief description of automated software test data generation process and related work. section 3 provides an overview of data-flow analysis. sections 4 and 5 provide a brief description of pso and de algorithms. section 6 describes the proposed hybrid algorithm. section 7 gives the experimental results. section 8 provides the discussion and the detailed statistical analysis of the experimental results. section 9 deals with threats to validity and limitations of the proposed hybrid algorithm. finally, section 10 gives the conclusion."
"the model robustness, which focuses on infeasibility issue as a result of a violation of data-driven parameters, takes into account infeasibility penalty in the objective function of the second-stage decision-making problem. in this modelling framework, the constraint violation is measured by an infeasibility variable vector ( ), where a positive value of ( ) show the amount of infeasibility of the corresponding scenario in the model. it is clear that ( ) if the model is feasible. the mean value of probable infeasibilities is then penalized in the objective function."
"i.e., a complexity of oðnþ. during maintenance, each update from a stream source will incur a cost of c u . if resolution takes place, it requires a cost of c d þ c u for requesting a value from a false positive stream source (s y ). if the protocol further requires a false negative stream source s z be interrogated, a cost of ðc d þ c u þ is needed. therefore, the cost of maintenance is at most"
"1. water availability: the water resources in global monsoon domains are greatly affected by the anomalous activities of monsoons. the summer monsoons produce more than 80 % of the annual rainfall in some areas, e.g., in india, africa and australia, and the percentage is more than 60 % averaged across all global monsoon regions (fig. 2) . understanding the mechanisms of monsoon variability on interannual and longer timescales as posed by gmmip will lead to improvement of monsoon prediction and projection and provide useful information for policymakers in water availability-related decision-making."
"s.t. constraints (8) - (9), where the expected cost function is presented in the first term of (10) and solution robustness and model robustness are given in the second and third terms of (10), respectively. since the terms stated in (10) need to be unified, we use coefficients and to provide a compromised objective function, as denoted in (11)."
"in the existing literature, only a few authors have addressed the stochasticity in an integrated ccp network design problem with multi-period planning settings. [cit] developed a scenario-based bi-level programming model for the shelter location model with the evacuation consideration for a realistic case study of north carolina and highlighted the impact of transportation when selecting the location decisions. proposed a scenariobased two-stage stochastic shelter location model considering casualties (evacuees) allocation to the nearest facility to minimize the expected total evacuation time. [cit] provided the exact solution based on benders-decomposition algorithm to the model formulated by . they showed the importance of the inclusion of uncertainty in planning for evacuations."
"calculate optimality gap calculate the statistical optimality gap percentage given in (55). if the gap is acceptable, stop; otherwise increase n and/or m and return to step 1 output statistically valid bounds on the true objective value (with confidence at least )."
"due to the uncertainties in physical parameterizations in current models, particularly in convection schemes [cit], the best way to address the above questions is through a multi-model framework in order capture the range of possible responses to forcing. the multi-model database to be produced for cmip6 [cit], in conjunction with the gmmip experiments will provide an opportunity for advancement of monsoon modeling and understanding. gmmip will also contribute to the grand challenges of the world climate research programme (wcrp) and address them in the following way:"
"q and answer set aðtþ returned to the user, let e þ ðtþ denote the number of streams in aðtþ that fail q, and e à ðtþ be the number of streams that satisfy q but are not in aðtþ. the fraction of false positives and the fraction of false negatives of q at time t, denoted by f þ ðtþ and f à ðtþ, respectively, are:"
"second, the main aim of the problem considered here is to optimize ccp design decisions in view of the existence of the temporal hierarchy structure between the strategic and operational decisions over the planning period. the time setting between these decisions as well as the distinct time-horizon granularity are incorporated in the proposed model to capture the dynamic nature of lifesaving operations in the response phase. in this research, we deal with an integrated humanitarian logistic network problem in which strategic decisions are made in the first-stage model and operational decisions with anticipation of uncertain factors are made/revised during the multi-period planning horizon. [cit] . it is important to note that in this modelling approach the objective is to use the anticipated decisions optimized for each operational cycle under all scenarios, so that more efficient and robust ccp design solutions are generated at the strategic level. from the practical point of view, strategic decisions include the number and location of ccps to be opened, ccps capacity allocation, allocation of affected areas to established ccps, hospitals allocation to established ccps and alternative ccp locations. these decisions, also known as design decisions, are made immediately following a disaster. planning decisions such as casualty triage, casualty registration, casualty medical treatment and casualty transporting, then need to be made over the whole of the planning period. the number of casualties (with several levels of injuries) and the available transportation capacity are uncertain throughout the proposed network. due to the hierarchical structure of strategic and planning decisions, finding an optimal solution for one activity is not usually sufficient for the whole of the response phase. therefore, the focus of this paper is to present a model that reflects the hierarchical structure of the strategic and planning decisions in the presence of"
"for future work, we will enhance our algorithm to adapt to the addition of stream sources during query execution. we also want to examine how the value distribution across different data streams can be used to design better filter assignment policies. we will address how other data stream optimization algorithms (e.g., synopses construction and load shedding) can employ the notion of fraction-based tolerance. we will also study how this new notion can be used to design filters for other query types (e.g., joins). another interesting study is to design a new correctness notion that combines the advantages of value-based and nonvalue-based tolerances, and develop filtering protocols accordingly. such a tolerance could be useful when a user is not only concerned about the number of false positives, but also that the actual values of the false answers are not too far away from the query range. another interesting question is how our protocols can be applied to sensor-networked environments, where each sensor can be viewed as a stream source. the challenge is to exploit the unique characteristics of sensor networks (e.g., sensors are arranged in a hierarchical manner) in order to further optimize the use of energy and bandwidth resources."
"the attempts to reduce the cost of software testing by automating the process of software test data generation have been constrained by the ever increasing size and complexity of software. in the early period of automated test data generation, gradient descent and meta-heuristic search (mhs) algorithms such as tabu search, hill climbing and simulated annealing [cit] . in the past two decades, evolutionary search-based algorithms such as genetic algorithm (ga) have been widely employed for test data generation as an effective alternative [cit] . a search-based approach captures the test adequacy criteria as a fitness function that is used to guide the search. due to an extensive application of search-based algorithms to test data generation problem, the approach has come to be known as search based software testing (sbst, coined by harman and jones). recently, the focus is on the use of other highly adaptive search-based techniques such as particle swarm optimization (pso), ant colony optimization (aco) and differential evolution (de). it has been observed that ga and aco have slow convergence towards the optimal solution. pso and de are conceptually very simple and the knowledge of previous good solutions is retained by all the members of the current population by means of constructive cooperation among them. pso and de have been found to be robust in solving optimization problems; however, the performance depends on control parameters. pso has been shown to be well suited for test data generation with better performance than ga [cit] . hybridization of search-based algorithms for test data generation has also been reported in literature. ga with a local search algorithm [cit] and more recently, ga with pso has been applied for test data generation in some studies [cit] ."
"in this section, overall performance of the proposed hybrid (adaptive pso and de) algorithm is compared with de, pso, ga and random search with respect to the measures collected. tables 7 -10, as given below, summarize the results of applying the various testing approaches to the set of chosen subject programs for (10, 15, 20, 25) . range of the input integer variables is taken to be 0-100; range is different for variables of program# 3, 4, and 7 as per the requirement of each program. the results are further discussed in the next section."
"analysing the data, we observed that the coefficient of variation of generated number of casualties of 7 wards out of 33 was about 80%, while it was above 120% for 26 wards out of 33, which represents a considerable uncertainty inherent in the generated number of casualties."
"in our matheuristic approach, while alns explores the neighborhoods to find promising routes, after every ω iterations we further enhance the current best solution by optimizing the charging decisions along the tour of an ev by fixing the sequence of customers visited. [cit] . they name this problem as fixed-route vehiclecharging problem (frvcp) after the fixed-route vehicle-refueling problem (frvcp) [cit] solve the evrp by using a sequence-first splitsecond approach where they first construct a tsp tour and then split it to extract vehicle routes by ignoring the ev range limit. if any of the resulting routes is energy-infeasible, then they try to repair it by solving frcvp. this approach cannot be implemented for solving evrptw because building vehicle routes without considering the recharge needs/durations of evs and then trying to 16 insert stations may cause many time-window violations. so, at each iteration of alns our repair procedure yields a feasible solution which is further improved by means of mathematical programming using cplex."
"for most of the alns destroy and repair mechanisms we resort to the neighborhoods utilized in keskin and çatay (2016) . in addition, we propose new removal and insertion methods specific to the fast charging nature of the problem."
"1 translation x/y center object in target area 2 translation x/y/z center, fill object in target area 3 rotation z rotate object to face screen 4 rotation trackball rotate object to face screen 5 rotation x rotate object to face screen 6 rotation y rotate object to face screen 7 zoom zoom object to fill target area 8 rotate-scale-translate (rst) center, fill object in target area for the frame interaction was described in section 3 ( fig. 7(a) ). we removed the interaction frame for the mouse condition (fig. 7(b) ). in contrast to typical 3d scientific visualization applications, we decided not to use menus or buttons to switch between translation, rotation, or zoom modes when using the mouse to avoid measuring additional travel distance and time necessary to reach these buttons. instead we chose mouse+keyboard combinations as shown in table 1 . the system ran on windows 7, and the mouse pointer speed was set to the average speed (half-way between slow and fast). in both conditions participants stood at approximately arm-length away from the display. the 800 dpi optical mouse was placed on a side-table of 1.03 m height."
"finally, for the 15-customer instances, three problems were solved optimally with model 1 and none with model 2. furthermore, model 2 could not yield a feasible solution in one problem."
"we optimize the recharging decisions of the best solution of that round every ω iterations, which we refer to as cplex call frequency. on the one hand, choosing this number too small may increase the run time. on the other hand, choosing it too large may deteriorate the solution quality."
"given a homogeneous fleet of evs, evrptw-fc aims to determine a set of routes involving customers with known demands, delivery time windows, service durations, and recharging stations with different types of chargers. the charging levels can be classified into three categories: level 1 (1.4 kw to 1.9 kw), level 2 (4 kw to 19.2 kw), and level 3 (50 kw to 100 kw) [cit] . in line with the current technology, we assume that every station is equipped with three types of chargers, which may be referred to as normal, fast, and super-fast charger, respectively. while recharging takes less time in fast and super-fast charging options, the unit cost of energy is higher since the installation of the chargers requires substantial electrical infrastructure and the equipment is more expensive. the charge durations are linear with respect to time at the first phase of charging which corresponds to almost full battery while the second phase is nonlinear and can take hours to obtain a fully charged battery [cit] . on the other hand, it is a common industrial practice to operate within the first phase because recharging the battery up to full capacity can adversely affect its lifespan [cit] . so, without loss of generality we assume linear recharging times in this study. in addition, we allow only one recharge between two consecutive customers which is the realistic situation within the context of urban logistics. our objective function is hierarchical where minimizing the number of vehicles is the primary objective while minimizing the total cost of energy consumed is the secondary. to describe the general setting and highlight the advantage of using fast chargers we employ the instance c104c10-s3 [cit] which involves ten customers and three stations."
"our aim is to shorten the charge durations which may allow serving more customers along the tour and thus reduce the number of vehicles. eventually, the charge-related decisions will later be optimized in the second phase using a solver as described in section 4.4."
"our work touches on existing approaches in several different domains. we first briefly talk about interactive visualization in general, including some techniques that employ direct touch. then, we specifically discuss approaches for direct-touch interaction in 3d environments."
"however, the solution times were significantly smaller with model 1. for the 10-customer instances, cplex failed to prove the optimality of five problems within the time limit using model 2 while all problems were solved to optimality with model 1, again in substantially less time."
"the results reveal that our matheuristic provides significantly better solutions than fort using both configurations (a) and (b). reducing the number of iterations in configuration (b) deteriorates the solution quality by 2% on the average while the average run time is approximately ⅛ of that of configuration (a). [cit] reported an average run time * the cost figures are kindly provided by gregorio tirado. [cit] . of 647 seconds whereas our matheuristic (a) and (b) spent on the average 1352 and 186 seconds, respectively. even though the problem addressed is slightly different and the two methods were executed on different processors, we believe that these results show the superiority of our matheuristic and confirm its effectiveness."
"our study also showed areas for improvement of our techniques. first, we plan to experiment with heuristics to detect erroneous input. we also found ways to improve our chosen heuristic for determining the depth of the rotation axis for x-/y-rotation which also serves as the depth of the 'sticky' plane for x-/y-translation. it became apparent that this is an issue, in particular, in the free exploration task when people tried to rotate around the most prominent cluster of the particle cloud which was not in the center of the dataset. this means thateven if this cluster of the dataset was in the middle of the widget-a rotation is almost always performed around an axis behind or in front of the cluster, resulting in the cluster moving away from the middle of the widget. this issue can be solved for particle-based datasets by determining the rendering depth of all of the particles within the view volume before a rotation or translation interaction is initiated. then, the average depth of these, potentially weighted by their 2d screen position, is used to determine the rotation and translation depth. for particle datasets with dense clusters this results in a stable rotation behavior according to what is shown on the screen. we implemented this technique and in our experiments it solves the mentioned issue."
"as interactive displays become part of our everyday work environments, they provide ubiquitous new data analysis platforms that can encourage alternative forms of scientific data exploration and pro- mote the use of scientific visualization techniques even by non-experts. touch interaction has been the focus of previous research projects in the visualization context [cit], but much remains to be learned about the effects of providing touch input in scientific visualization. we need to learn more about how to re-design desktop and mouse-based systems for direct touch, for which scientific data analysis scenarios direct touch and traditional interfaces may be most suited as interaction alternatives, and, on a higher-level, how direct touch changes the ability of viewers to understand data and draw insights."
"since the problem has two types of vertices, namely customers and recharging stations, their removal will have different impact on the solution. so, we employ separate customer removal (cr) and recharging station removal (sr) operators for destroying the solution."
"we use three initialization approaches for comparison. the first uses the best-known solutions reported in keskin and çatay (2016) . basically, these solutions were obtained by only allowing normal recharge at the stations and can be considered as an upper bound for the fast recharge case."
"when we utilize ia 3 for initialization, the solution is constructed very fast by the greedy algorithm whereas the matheuristic is performed for 25,000 iterations. in other words, we allow a more intensive search during the initial solution generation in the former case whereas in the latter case matheuristic is the only actor and it benefits from the mathematical programming more rigorously."
"\"avgtime\" is the average of the run times in seconds for each subset and \"#better\" in the last column reports the number of instances in which model 1 gives better solutions than model 2."
"in sift [cit], identical key points are extracted from images after filtering them with 2-d difference of gaussian filters. on the other hand in 1-d sift algorithm, key points are extracted using color histograms. similar to the sift algorithm, in 1-d sift approach, difference of gaussian (dog) filters are used. instead of using the image itself, color histogram of the image is filtered with 1-d dog filters. after constructing the octaves both local minima and extrema points are determined in each level. if it is possible to backtrack an extrema or a minima location from coarsest level to the highest level, that location is taken as a key point. later on the gradient values of the main color histogram is extracted. with these gradient values a feature vector is created. thus we will be representing the image with many feature vectors where their number is equal to the number of key points extracted. in figure 2, the feature vector extraction process for the 1-d sift algorithm is graphically explained. key point locations are shown with a red dots on the 32-binned rgb histogram. for the keypoint at index 120 the gradient values are paired together and according to their signs and their magnitudes placed into feature vector. the negative values are summed and inserted into the first element where positive ones are also summed and placed into the second element of the feature vector. thus, a feature vector with four pairs is constructed. in addition to the this feature extraction process, a new feature extraction approach is also followed. as we mentioned before the lbp histogram contains important information about edges, spots etc. thus instead of taking the gradient magnitudes we simply took the histogram magnitudes itself in feature vector construction. this mod update for the 1-d sift algorithm is called magnitude 1-d sift (m-1-d sift) algorithm. in this work, 1-d sift algorithm is applied to the lbp histograms. our aim here is to classify the h&e stained cancer images using these two algorithms. different lbp histogram extraction processes are followed and the related feature vectors are extracted. later these feature vectors used in classification process."
"in the design of our technique we were guided by several complementary goals. we designed our technique to: g1: encapsulate all seven common degrees of freedom for 3d interaction in one joint interaction paradigm, g2: support the manipulation of the space itself rather than specific objects within it, g3: not require intermediate in-space interaction proxies, g4: require only spring-loaded modes, g5: be generic to be applicable to many different 3d scientific data representations, g6: allow for both large-scale and precise interactions, g7: in its base form require just one touch-point, g8: be easily extensible, g9: be intuitive and require little learning time, and g10: be 'competitive' with current techniques for 3d scientific data exploration."
"remove customer with succeeding station operators introduced by keskin and çatay (2016) where customers are removed along with the station visited immediately before or after serving that customer. at each iteration, one of these cr operators is selected randomly to remove customers from the solution and put them in a removal list. the value of depends on the total number of customers and is determined randomly between and ̅ using a uniform distribution."
"in this paper, we tackled the electric vehicle routing problem with time windows and fast chargers (evrptw-fc). in evrptw-fc, the stations are equipped with multiple chargers which vary in power supply, power voltage, and maximum current options. we considered three charger types, namely normal, fast, and super-fast. we formulated two different mathematical models of this problem and compared them in terms of solution quality and computational time. since the medium and large size problems are intractable, we developed a matheuristic approach to solve the problem efficiently. our approach combines alns with an exact method. in alns, while we employed destruction and repair algorithms from the literature, we also introduced new mechanisms specific to the nature of the problem. in the exact method, we fixed the sequence of the customers visited by each vehicle in the solution provided by alns and utilized cplex solver to optimize the charging related decisions. we also developed an efficient mathematical formulation for this fixed-route single-vehicle problem to be able to find the optimal solution in reasonable run time."
"evrp is an extension to the capacitated vehicle routing problem (vrp) where a fleet of evs is used instead of icevs. the energy stored in the battery is consumed along the journey proportional to the distance travelled and the ev may need recharging to complete its tour. recharging may be performed at any battery state of charge (soc). the stations are scarce and recharging may require a significant amount of time, compared to short refueling times at petrol stations. in this paper, we address the evrp with time windows (evrptw) [cit] . evrptw assumes that recharging time is a linear function of the energy transferred and the battery is fully charged. [cit] relaxed the full charge restriction and allowed partial recharging with any quantity up to the battery capacity, which is the current practice in real-world applications."
"interactions that allow people to place and manipulate cutting planes have been employed for the exploration of scientific datasets for a long time (e. g., [cit] ). this interaction is also important to understand the extracted dti fiber tracts in relation to the brain's structure in our examples. therefore, we explored how to allow users to manipulate axis-aligned cutting planes using direct-touch interaction on a 2d surface. this means we need to provide a means of creating cutting planes on all six sides of the dataset and to drag them along the three axes."
"as a lethal disease, cancer effects many people around the globe. [cit], approximately 14 million people experience this disease every year and 8 million patients have died because of it. namely, \"cancer\" is a general term for malignant tumors. these rapid growing abnormal tumors invades different tissues and organs in time. this process is called metastatic invasion. since this invasion is the one of the major reasons of the deaths, observing its level is important."
"we adapt the greedy and best station insertions introduced in keskin and çatay (2016) using the cost criterion as follows: when a recharging station is inserted in a route, first, we try the normal charger since it is the cheapest option. if the normal charge is infeasible due to its longer duration, we try fast and super-fast chargers consecutively. this procedure is repeated for all feasible stations and candidate stations are determined along with the charger type. then, the insertion is performed according to the criteria used in the corresponding si operator."
"the average results are presented in table 8 and detailed results are given in appendix c. in this table, \"n\" and \"s\" refer to the number of customers and number of stations in the data, respectively. \"avg tc\" and \"avg time\" report the average total cost and the average computation time (in seconds) of the corresponding problem set. \"% imp\" shows the percentage improvement achieved by our matheuristic for each configuration and calculated as (fort − matheuristic)/fort."
"in table 1 the resulting classification accuracies are given. as it is shown in table 1 that, m-1-d sift approach gives better results than non-modified version. since there are some color differences between the normal and cancerous images, adding color histograms to lbp histograms greatly increased the success of the experiments."
"constraints (5) and (6) keep track of departures from and arrivals at the depots. constraints (7) guarantee that all evs departed from the depot arrive at the depot at the end of their tour. service times are controlled by constraints (8)- (10). constraints (11) and (12) observe the load on the vehicle and make sure that total load does not exceed the cargo capacity. constraints (13) and (14) keep track of battery soc when departing from customers and stations, respectively. constraints (15) define the bounds for variables and while constraints (16) ensure that evs depart from the depot with full battery. constraints (17) determine the amount of energy transferred while constraints (18)- (20) control which charger type is utilized for recharging. note that and cannot be 1 simultaneously because of nonnegativity of 3 variables. finally, constraints (21) and (22) define the binary decision variables."
"we first investigate how different initialization approaches and optimization frequencies affect the solution quality in order to determine the best configuration. next, we examine the benefits of utilizing fast chargers in terms of fleet size and energy costs."
"overall, participants were able to explore the space effectively and could name several interesting aspects of the data based on overviews as well as detailed views. we observed some people trying unsupported gestures such as twisting their fingers to make small rotations. our technique could be extended to include new types of input gestures on multi-touch screens but further work needs to determine the most useful multi-finger gestures for these types of space transformation."
"while much of the previous work on direct-touch data exploration has considered work with specific objects within a 3d space, our focus is on manipulating the space as a unit which is important for many scientific datasets, such as those found in particle simulations in astronomy. fi3d does not require separate menus or dedicated interaction widgets inside the space itself. our goal is to ensure that the space itself is used solely for representing the data visualization itself and that the technique can be generically applied to different types of 3d scientific data. fi3d makes use of the visualization space's borders and can be used on hardware that supports dual-or even just singletouch input. by focusing on a single-and dual-touch technique, we can take advantage of all modern types of touch surfaces and design our interactions to be fundamentally simple but easily extensible. our interface allows for full 7 dof manipulation using only single-touch. we support translation in x-, y-, and z-direction, orientation with respect to the 3d coordinate system, and uniform zoom. furthermore, we present how an additional touch can be used to constrain certain interactions to allow for precise or integrated exploration of scientific data. we applied the interaction technique to two case studies and evaluated it in comparison to traditional mouse interaction. we show the utility of our technique for the exploration of particle simulation data from astronomy and for the investigation of fiber tract data in human brains. for the latter we also describe how to use an extension of the technique for the manipulation of cutting planes."
"in an effort to explore this space we designed and studied fi3d (frame interaction with 3d spaces), a novel direct-touch technique that allows users to explore three-dimensional data representations and visualization spaces. this ability is essential to many tasks in scientific visualization, in particular for data such as medical volume scans, volumetric physical simulations, or models in astronomy and cosmology. touch interaction for 3d data exploration is a challenging endeavor [cit] because we have to deal with an under-constrained problem: mapping 2d input parameters to 3d transformations in space."
"translation in z and rotation around z: the transformation along or around the z-axis can be controlled individually (i. e., by involving a mode switch) by mapping one dimension of the 2 dof of touch input to the specific transformation: for example, the 2d y-motion of touch can be mapped to z-translation, while the angle of a circular motion around the screen center can be mapped to a rotation around the z-axis. the latter can be mapped directly, while the former needs to employ a certain control/display ratio."
"in this paper we presented fi3d, a design study for enabling direct-touch interaction with three-dimensional scientific visualization spaces. fi3d is based on using the frame around a centered visualization and spring-loaded modes to enable users to navigate the 3d space in 7 dof and requires in its basic form only single-touch interaction (goals g1, g4, g7). it differs from previous techniques in that it allows interaction with the space itself and does not require large objects to be present to constrain the mapping (goals g2 and g3). if more simultaneous touches are available, however, we demonstrated that the technique can also support constrained interactions (g6). we discussed the application of the technique to two different scientific visualization domains to demonstrate its generality and extensibility (g5 and g8). in addition, we reported on a user study comparing fi3d to common mouse-and-keyboard interaction. this study showed that fi3d was competitive for translation and integrated rst interaction (g10) while being slower for rotation and zoom. the latter effect, however, we attribute largely to technical issues with the specific touch sensing hardware we used. moreover, our study also showed a clear preference of participants to use touch interaction for the exploration task, e. g., because of the immersion and control it provides and that the technique was easy to learn and use (g9)."
"transportation systems have a major impact on global energy consumption and co2 emissions with a share of around 20-25%. in the us, 26% of the total greenhouse gas (ghg) [cit] was generated by transportation systems that utilize fossil fuels (www.epa.gov). furthermore, 74% [cit] was moved by trucks and the freight volume is expected to grow by 39% [cit] . similarly, the eu reported that transportation was a main contributor with 23.2% [cit] and freight transport activity is predicted to grow by around 80% [cit] (ec.europa.eu)."
"moreover, we also see that the superiority of ia 2 in type-1 problems is usually in terms of the number of vehicles while in type-2 problems ia 1 performs slightly better than ia 2 in total cost. the former is an expected outcome as the utilization of super-fast chargers may significantly cut down the recharge time at stations and allow the ev serve more customers along its route which will translate into a reduction in fleet size. however, the latter can be considered as a surprising result and we will further elaborate on this issue in the next section."
"the authors developed two heuristics and designed 52 instances with varying sizes from 20 up to 500 customers to test their performances. [cit] introduced evrp with time windows (evrptw) as an extension to gvrp. in contrast to short refueling times in gvrp, the recharging operation may take a significant amount of time in evrptw. recharging starts at any soc, continues until the battery is full, and its duration is proportional to the energy transferred. the objective is to minimize the total distance travelled by using minimum number of vehicles. the authors proposed a hybrid 6 variable neighborhood search (vns) and tabu search (ts) method and tested its performance on the instances for gvrp and multi-depot vrp with inter-depot routes. [cit] data and reported their results. [cit] tackled the same problem by considering different charging strategies and developed branch-price-and-cut algorithms to solve them optimally. [cit] extended evrptw to include a mixed fleet of evs and icevs. the objective is to minimize the total cost defined as a function of speed, gradient and cargo load. [cit] addressed the fleet size and mix vehicle routing problem with time windows where the fleet consists of evs only. they minimize the total cost of vehicle acquisition and distance travelled. both studies used alns as the solution methodology. [cit] and keskin and çatay (2016) relaxed the full recharge restriction and allowed batteries to be recharged up to any level. the former minimized the number of vehicles, travel time, waiting time, and recharging time, developed a variable neighborhood search branching method, and used it to solve small size instances. [cit] to formulate evrptw with partial recharges (evrptw-pr) and proposed an alns approach that improved some of the best-known results in the literature. [cit] formulated a more effective mathematical model for gvrp by reducing the number of variables and eliminating dominated stations for each pair of customers. [cit] addressed evrp by allowing partial recharges using multiple technologies."
"in this paper we are particularly interested in the challenges of providing data exploration capabilities for 3d visualization spaces. in many scientific desktop-based systems widgets are used for navigating in and manipulating a 3d space. typical techniques include the arcball [cit] or similar virtual trackballs [cit], coupled with techniques for movements along all three spatial axes. considerably more research has been conducted on 3d interaction in non-desktop based systems using dedicated input and output hardware [cit] such as virtual reality environments like the responsive workbench [cit] or the cave [cit] . one important advantage of these virtual environments is that they afford direct manipulation with the 3d worlds [cit] because both stereoscopic projection of the virtual world and its manipulation (through tracking or by using haptic devices [cit] ) happen in-place."
"because precise control is of high importance in scientific visualization, we explored the possibilities of frame interaction for constraining selected transformations further. while so far it is possible to single out rotation around the z-axis, rotation around the x-/y-axes has been integrated: the motion during rotation determines an axis parallel to the x-/y-plane around which the rotation occurs. to instead permit constrained rotation around either x-or y-axis we propose to employ dual-touch and the frame sides. here we make use of the fact that the four frame sides are perfectly vertical or horizontal. we let users specify an axis around which one aims to rotate by touching one side of the frame with their non-dominant hand while the dominant hand then can be used to perform the rotation (fig. 6) . for example, for rotating only around the y-axis one would place one finger on either vertical frame side and then use the other finger (inside the frame or when also starting from one of the vertical frames) to rotate a constrained trackball (i. e., a virtual track-cylinder). similarly, when one has already started trackball rotation through perpendicular motion originating in any frame side, one can constrain this interaction at any time by placing another finger in one of the four frame sides, the horizontal ones for rotation around the x-axis or the vertical ones for y-rotation."
"our preliminary experiments revealed that calling cplex after 200 and 500 iterations shows a good compromise. so, we decided to consider these values for further investigation. our stopping criterion is a limit on the number of iterations. for different initialization algorithms, we set different limits. we perform 25,000 iterations of alns when we utilize ia 1 and ia 2 to generate the initial solution. then, we apply the matheuristic for 10,000 iterations."
"we use the greedy, regret-2, time-based, and zone insertions as proposed in keskin and çatay (2016) . in addition, we employ these mechanisms only with the fastest recharging option when a station insertion is needed to feasibly add a removed customer into a tour. we refer to these new operators as fast recharge (fr) greedy, fr regret-2, fr time-based, and fr zone insertions."
"rst interaction: the two-touch pinching gesture [cit] has been popularized in the media and is perceived by the general public to be an integral part of multi-touch interaction. therefore, we also make use of this 4 dof technique for the interaction with 3d spaces, comparable to the 2d rst interaction [cit] . we realize this rst mapping by combining the individual mappings for x-/y-translation, rotation around the z-axis, and zoom the same way as the 2d mapping, simply taking the 3d control/display ratio considerations into account. the center of transformations is always the middle between the two touching points, and the translation is 'sticky' for the plane located at half the space interval that is taken up by the visible part of the dataset."
"rotation. the analysis of task completion time showed a significant effect for all four rotation tasks with the mouse condition being significantly faster than touch in all cases. table 3 gives an overview of the significance scores for the rotation tasks. hence, for rotation we did not achieve our goals of providing a competitive alternative to (good) mouse-based interaction in terms of speed. we discuss our hypotheses of these results in the discussion section. the post-study questionnaire asked participants to rank the two techniques on a 7-point likert scale according to whether rotation was easy to perform. both techniques scored a median if 6 (agree) on this question."
"the problem is solved using cplex and the optimal solution is illustrated in figure 1 in figure 1 .a, we see the optimal solution of the problem when only normal chargers are available at the stations. two vehicles travel a total distance of 273.93 units at a total energy cost of 273.93."
"the results are presented in table 7 . the computation times reported in columns \"time\" are in seconds. the values given following \"c\" and \"s\" in the instance names represent the number of customers and stations, respectively. note that cplex results reported with a run time of 7200 seconds show the best upper bounds found within the given time limit and are not necessarily the optimal solutions. cplex solves all 5-and 10-customer instances to optimality."
"in this paper, we combined both lbp and 1-d sift algorithms together. our aim here is to classify the normal and cancerous h&e stained liver tissue images. additionally, a new feature extraction approach for 1-d sift algorithm is implemented and used for same purpose. the outline of this paper is as follows. in section 2, a brief information about both lbp and 1-d sift algorithm is given with the explanation of the new feature extraction process. section 3 presents the conducted experiments and resulting classification accuracies."
"for each configuration, we performed 30 runs for each instance and reported the best results in table 4 . \"#veh\" and \"tc\" represent the number of vehicles needed and total cost of energy, respectively. the best solutions among six different configurations are indicated in bold. the row \"#best\" shows the total number of instances for which the corresponding configuration yielded the best solution."
"when we analyze the results for type-2 instances we observe that fast charging is able to reduce the fleet size in only one instance (c204) out of 27 and by only one vehicle. since the time windows can be easily satisfied in these problems the vehicles can serve more customers on their routes and the number of evs is already few (between 2 and 4). hence, a reduction in the fleet size is usually impossible. we also observe that fast charging does not help cutting down the costs either: total energy cost is reduced in 12 instances and the average improvement is only 0.17%. the main contributor to this average value is problem r211 where a 2.54% reduction in energy cost is achieved. [cit] who highlighted the minor influence of wide time-window constraints on recharging decisions."
"wayfinding task. in the wayfinding task we let participants freely explore the dataset with an interaction method of their choice. participants had the freedom to use either touch or mouse or a combination of both. 75% of participants chose to only use touch, while 25% of them chose to use a combination of both. participants typically began exploring the dataset by zooming in and looking at the center in more detail. to explore the general shape of the datasets they then used trackball rotation and constrained rotation to get a more precise understanding. to perform bigger space transformations, participants tended to use the two-point rst interaction, moving interesting regions of a cluster to the center and simultaneously re-orienting the 3d space. for smaller changes, in contrast, participants used translation and constrained rotation around one of the axes separately. the two- fig. 9 . translation for mouse and touch for a single participant (left two images, resp.) and for all participants overlaid (right two images, resp.)."
"another extension we added post-study allows users to specify a center of rotation: one static touch determines the center, the other touch starts on the frame to initiate trackball or z-rotation. this technique integrates nicely with rst and frame-initiated rotations and also uses the aforementioned heuristic to determine the rotation's z-depth."
"in all the sr operators, recharging stations are removed from the solution after every iterations. is determined in a similar way as based on the number of stations visited in the current solution."
"scaling and zooming: in perspective projection, z-translation results in objects increasing or decreasing in their visible size on the screen. other possibilities to achieve a similar effect but without moving the camera are to enlarge the dataset itself (scaling) or to change the field of view angle of the virtual camera (dolly-zoom). in the context of 3d exploration, both have advantages and disadvantages. scaling, for instance, requires a center of transformation that is located exactly at the point of interest; otherwise focus objects may move further away or closer to the viewer. the dolly-zoom, in contrast, has upper limits (an angle of 180°) but is independent of the depth of the focus objects. this last consideration lead us to using dolly-zoom in our interaction in addition to z-translation."
the objective function (23) represents the same energy cost as (1) but with different terms. all constraints in model 1 remain in model 2 except constraints (17)- (21) which are associated with charger types.
"these goals set our technique apart from previous approaches. we specifically do not require dedicated objects to be present inside the visualized data space (g3). this is a crucial design decision since many data sources in scientific visualization inherently do not have (large) dedicated objects (e. g., particle clouds, 3d flow fields, or voxel data) which could be used as interaction proxies. therefore, we enable interaction with and exploration of the space itself (g2), as opposed to objects within the space as done previously. this goal implies that, for example, we cannot rely on a surface being present to extract the constraints for the 3d interactions (e. g., [cit] ). we also strive for our 3d interactions to be intuitive extensions (g9) of 2d rst direct touch manipulations [cit] . both constraints result in that we have to find meaningful heuristics, for instance, to determine the axes for rotations or virtual planes where touch interactions are 'sticky' [cit] . at the same time, we do not want to rely on a large number of gestures (g1, g9) that users have to learn and remember or system-controlled modes (g4) that have negative usability implications. another design criterion is that we want to be able to perform both broad interactions to explore large-scale structures and precise and constrained interaction (g6) to examine fine details. the separation of spatial manipulations required for this goal is often challenging in multi-touch environments [cit] . finally, we want to enable users to control all degrees of freedom on single-touch surfaces (g7) so that our technique can take advantage of all types of currently available touch-surfaces. at the same time, we designed the technique to be easily extensible (g8) so that it can take advantage of multi-touch interaction where available."
"we found that participants's acquaintance and years of practice with the mouse influenced their performance. participants preferred the mouse for time-pressured tasks due to its familiarity but preferred touch for open-ended exploration. in terms of speed, the touch technique did not show improvement over the mouse but generally also did not incur large time penalties for the travel tasks. yet, we found that both touch and mouse could be considered practically equivalent in terms of speed only for the 2d translation task. the interaction logs revealed that participants made more precise and straight movements in the touch condition (fig. 9) while mouse movement was considerably more noisy. the proprioception afforded by each technique likely played a role here. it would be interesting to further test the value of touch interaction for tasks in which precise movements along specific paths are required. a further investigation in this direction could also shed more light on why some participants preferred touch due to an increased sense of \"having things under control.\""
"this technique allows users to interact with three axis-aligned cutting planes, one for each of the coordinate axes. however, we also want to be able to distinguish between cutting away from the front and cutting away from the back of a dataset. to enable users to make this distinction we allow them to start cutting away from both sides and to move the cutting planes through the entire dataset. the side from which a cutting plane is started is the side that is initially cut away, but this interaction can be started from both sides. also, if the cutting plane leaves the dataset on one side, while a user continues the interaction, a new plane is started at the opposite side."
"for solving evrptw-fc, we propose a two-phase matheuristic approach where in the first phase, we attempt to find good heuristic solutions using alns and then improve them using an exact method in the second phase. for the exact method, we resort to cplex solver but any open-source or commercial solver can be utilized instead. matheuristics use mathematical models in a heuristic framework and they have been applied to various routing problems. [cit] for the details of the approach and an overview of implementations."
"in this section, we compare the best results obtained by the proposed matheuristic with the best results that alns of keskin and çatay (2016) yields for the single charge case where the stations are equipped with only level 1 (normal) chargers. the results are presented in table 6 . in this table, \"q\" stands for the battery capacity and \"tc/td\" refers to the total cost which is equivalent to total distance travelled in the normal charge case. columns \"#l1\", \"#l2\", and \"#l3\" report the total number of recharges performed by using level 1, level 2, and level 3 chargers, respectively, and \"qty\" shows the corresponding quantity of energy transferred. the improvements over the single charger results are highlighted in bold."
"transportation will remain a major and growing source of ghgs in the future. hence, governments initiated new environmental measures and targets for reducing emissions and cutting the dependency on fossil fuels. for instance, us government targets reducing ghg emissions 20% since transportation plays a major part in ghg emissions and road transport contributes with a 75% share, the new regulations bring limitations to the use of internal combustion engine vehicles (icevs). in the eu, the use of icevs will be reduced by 50% [cit] . city logistics in major european urban centers will be co2 [cit] ."
"our experiment aimed to evaluate how our touch interaction compared to a traditional mouse interface in terms of both quantitative and qualitative measures. the touch technique was rated highly for its overall ease of use and for each individual travel task. participants found that it is easy to remember how to perform interactions, that they were able to interact precisely, and efficiently. touch seemed to invoke a sense of more direct connection to the data and increased immersion with the displayed information. touch interaction also provided additional integrated interaction functionality compared to the mouse: the rst technique was frequently used and highly valued by participants."
"further investigation on these results reveals that #l2 and #l3 values are 0 in most of the instances. so, these slight improvements were achieved by an extended search of the solution space of single charger case rather than by using fast or super-fast chargers. noting again that ia 1 constructs the initial solution by using only normal chargers, these results now explain why the matheuristic using ia 1 showed a better performance in type-2 problems in section 5.1.1."
"the two values coincide because the problem involves only normal chargers and one unit of energy is consumed to travel one unit of distance. on the other hand, figure 1 .b shows that all customers can be served by only one vehicle in a single tour when fast and super-fast chargers are available."
the objective function (1) minimizes the total energy cost which consists of three terms. the first term corresponds to the total cost of energy recharged along the route. the second is the total cost of initial charging at the depot. all vehicles are recharged fully using the cheapest (slowest) charger type at the depot overnight. the third is associated with the battery soc at the end of the trip and deducts the value of the remaining energy from the total cost since that amount of energy has not been consumed en-route. constraints (2)- (4) are the connectivity constraints which ensure that each customer is visited exactly once and each recharging station may be visited at most once.
"after the task completion times collected during the study were logtransformed to comply with the normality assumption of the data analysis, time was analyzed using a repeated-measures anova (frame, mouse). the results are broken into the four main types of travel tasks."
"in this study, we extend the evrptw-pr by introducing fast charging option and refer to this problem as evrptw and fast charging (evrptw-fc). basically, we assume that the stations are equipped with multiple charger types. they vary in power supply, power voltage, and maximum current options, which affect the recharge duration. we formulate this problem as a 0-1 mixed integer linear program and propose a matheuristic approach to solve it efficiently. our approach combines the adaptive large neighborhood search (alns) with an exact method. at each iteration of the alns, the feasible solution is destroyed by removing certain customers and stations from their routes and then repaired by inserting the removed customers back to the solution along with stations when recharging is necessary. when a station is inserted, the charger type and recharge quantity are also determined. the solution found by alns is then improved periodically by solving a mixed linear integer program which optimizes the decisions associated with recharge stations, charger types, and recharge quantities given the sequence of the customers visited."
"˗ the proposed alns involves new destroy and repair mechanisms specific to the nature of the problem. ˗ for a given sequence of customers, we propose a novel formulation of the charging subproblem that can be solved to optimality fast."
"for this purpose we employ a technique similar to the previously used constrained rotation interaction. touching the frame with one finger allows users to lock the dataset itself in place while the second finger is placed in the center of the widget to initiate the cutting plane interaction. as soon as this finger touches the center part, three colored crossing lines are shown (fig. 11), each one in the direction of one coordinate axis, projected to screen-space. the next motion of the touching finger selects the coordinate axis along which the cutting plane will be moved depending on which of the colored lines is closest to the finger's 2d motion. this is similar to methods employed by 3d modeling tools with 2d input such as blender to select translation directions. afterward, motions of the second finger on the 2d surface can easily be mapped to translations of the cutting plane in 3d space."
"section 3 describes the problem and presents the mathematical models. the proposed solution approach is described in section 4. section 5 presents the computational study and provides the numerical results. finally, the paper closes with concluding remarks in section 6."
"to be able to effectively explore fiber tract data in the context of the brain's anatomy it is often important to identify the subsection of all fiber tracts that connect two different regions of the brain. traditionally, this is done by placing regions of interest by means of mouse and keyboard [cit], but mouse gestures have also been explored [cit] . we support this interaction by making use of two simultaneous touches inside the exploration widget (which we previously had mapped to rst interaction). these touch locations are used to define two independent locations in 3d space through picking. because this approach would normally only allow us to specify locations on the surface of the brain, we combine it with the previously described cutting plane interaction. this means that we determine the intersection of each picking ray with the surface of the visible brain section (fig. 12), either a cutting plane intersection or the outer surface of the brain. these 3d locations are used to query the list of fiber tracts and only the ones that pass through the neighborhood of both locations are shown. when the fingers are lifted again, the set of fiber tracts that was selected last continues to be shown, so that further interaction with the cutting planes can be used to reveal further spatial relationships of fibers to brain anatomy."
"the mouse was significantly faster than touch for the rotation tasks which we explain largely with hardware issues. the dvit technology we used to capture inputs [cit] relies on triangulating shadows of touching fingers in front of a strip of ir-leds. this results in less responsiveness in certain touch configurations which was apparent, in particular, in the rotation tasks in the study where participants tended to touch the screen often inadvertently with parts of the whole hand instead of one or two fingers only. during these rotation tasks we thus noticed interference between the two simultaneous touches, with one input having an occasional response delay of up to 0.5 s. this delay prohibited participants from finishing the tasks quickly. we hypothesize that with more reliable sensing the timing results will differ. for the zooming task we did not expect touch to outperform the mouse as turning a scroll wheel is faster than a touch+drag action. yet, our zoom technique provided additional functionality. two participants commented that they preferred touch due to the continuous nature of the zoom which the mouse interaction was not able to provide. while participants were ca. 3 s faster in the rst task using touch, we did not find a significant difference. we have no direct explanation but suspect that fatigue played a role. forcing participants to take longer breaks between tasks may have helped to get a clearer answer for this task."
"as an alternative to the two-touch fiber selection we also experimented with a dedicated interaction with the regions that select the fibers. here we use relatively small axis-aligned boxes to select fibers of interest. to re-position these boxes users touch one of the sides of the box facing the viewer, specifying one of the dataset's main axes. the selected side is highlighted and 2d motions of the touching finger can now be mapped to translation of the box along the specified axis. this technique has the advantage that it is highly accurate and each box is independent. this means that one can specify more than one region of interest to show fibers that are passing through them. nevertheless, it takes careful positioning to place a box in 3d space as intended."
"evs can be classified as battery electric vehicles (bevs), hybrid electric vehicles (hevs), and fuel-cell electric vehicles (fcevs) such as cars, vans, trucks, electric trains, airplanes, boats, and two-wheelers. in this paper, we refer to ev as a commercial road bev. a fleet of evs can be used in a variety of transport needs such as public transportation, home deliveries from grocery stores, postal deliveries and courier services, distribution operations in different sectors. the main advantages of evs are zero tailpipe emission, high efficiency, and low operating noise [cit] . the number of moving parts in evs are much less than that of icevs and evs do not require regular oil changes [cit] . in addition, due to the regenerative breaking, break wear is used less which reduces the maintenance costs [cit] . on the other hand, operating an ev fleet has several drawbacks such as low achievable driving range, limited number of recharging stations, and long battery recharging times [cit] . these limitations and additional complexities make the route planning of an ev fleet a 4 challenging combinatorial optimization problem. as a result, electric vehicle routing problem (evrp) and its variants have attracted considerable attention in the recent literature."
"we compare the efficiency of models 1 and 2 [cit] . the data set includes three subsets of 12 problems, each involving 5, 10, and 15 customers. we used cplex 12.6.2 solver running on a single thread and the time limit is set to 7200 seconds. the experiments were carried on a workstation with intel xeon e5 3.30 ghz processor and 64 gb ram. the results are summarized in table 3 . the column \"#cust\" gives the number of customers in the problem set. \"#opt\" refers to the number of optimal solutions found within the time limit whereas \"#nfs\" indicates the number of instances for which no feasible solution could be found."
"in the second approach, we implement the alns of keskin and çatay (2016) by allowing the super-fast recharge only and feed its solution to initiate the matheuristic. the last approach randomly puts all customers into the removal list and applies the fr greedy ci heuristic."
"the combination of these frame interactions with single-touch x-/ytranslation started from the center of the widget allows us to provide control for all main 7 dof necessary for the exploration of 3d visualization spaces (g2) in one integrated paradigm (g1), without requiring dedicated objects to be present (g3). compared to the integrated 6 dof single-/dual-touch technique discussed in section 3.1, however, the frame-based interactions allow us to separate out the interactions and, consequently, permit users to control the exploration more precisely (g6). for instance, users can affect the rotation around z independent from x-/y-translation and both without affecting the zooming, and vice versa. furthermore, we are able to control all 7 dof with only a single touch (g7) and, therefore, our technique can be used on touch-displays that only provide a single simultaneous touch location. nevertheless, many people nowadays expect two-point rst interaction in touch-enabled interfaces. our technique was designed to be easily extensible (g8) and we provide rst interaction (fig. 5 ) when people start their interaction with two fingers inside the frame and if the used touch technology has this capability. this way we give the user the choice of either fast and integrated interaction with two fingers or precise and separate interaction with the frame."
"rotation around x-and y-axes: in traditional mouse-based interfaces, this type of rotation is often achieved with a trackball/arcball metaphor [cit] . in a touch-interface, this type of rotation can be easily achieved by treating a touch input like a mouse input. similar to the translation case, it is necessary to pre-select a specific location for the center of rotation. if dedicated objects exist in space, typically their center of mass or a specified pivot point is used. in our case-without such dedicated objects-we have to rely on a heuristic. we use half of the depth range covered by the visible part of our dataset. this means that those parts of the dataset which are in front of the near clipping plane are not considered in this case. we also use the same heuristic for determining the 'sticky' plane for x-/y-translation."
"we conducted an informal evaluation to understand the usefulness of touch and frame interaction in this second context, in particular also compared to software packages that are normally used for brain and fiber exploration (e. g., trackvis [cit] ). for this purpose we invited a neuroscientist to our lab who has experience working with tools like trackvis. we started by explaining the frame interaction technique and then asked him to try it himself. while he was working with the tool, we asked for comments and observed how fast he was able to learn to use the interaction techniques."
"our matheuristic also solves them optimally; however, it requires more computational time in most of the instances. on the other hand, the matheuristic outperforms cplex in 15-customer instances both in terms of solution quality and run time. the improved results are highlighted in bold in the table. we see that our matheuristic achieved better cost figures in two instances (c103c15-s5 and rc204c15-s7) and provided a solution with one less vehicle in another instance (rc202c15-s5). the latter case corresponds to a major improvement as the fleet size is reduced to a single vehicle from two. we believe that these results show the effectiveness of the proposed matheuristic approach."
"we observe that the initialization approaches have a significant effect on the performance of the matheuristic. while ia 2 yields better solutions in type-1 instances where customers have narrow time windows, ia 1 performs better in type-2 instances which involve customers with wide time windows. in other words, determining the initial solution through alns by considering only normal chargers works better in type-2 problems whereas using the same initialization approach with super-fast chargers has a better performance in type-1 problems."
the fuel is consumed proportional to the distance travelled. the problem does not consider any vehicle capacity or customer time windows. the objective is to minimize the total travel distance.
"independent of the value of ω, the results obtained by using ia 3 are inferior than those given by the other two initialization approaches. in other words, searching for a good initial solution pays back the effort spent."
"when we compare the 3-charger results with 1-charger results we see that fast charging is more beneficial when the customers have narrow time windows (i.e. type-1 problems). this is expected because the time spent at stations for recharging the ev can be reduced significantly with fast chargers and the vehicle may be able to serve additional customers along its route including customers that cannot be served otherwise due to strict time-window restrictions. if the ev can serve more customers along its route then more efficient solutions may be constructed which require less number of vehicles and/or consume less energy due to shortened travel distance. the cost of total energy may also go down depending on the charger types utilized and the quantity of energy transferred. in type-1 data set, out of 29 instances we have achieved better solutions in 28 whereas the solution for one instance (c101) has not changed."
"vrps with afvs context have been studied by several researchers in recent years. [cit] present the recharging vrp where the evs are recharged at selected customer locations during the service. the primary objective is to minimize the number of vehicles while the secondary objective minimizes total costs associated with travel distance, service time, and recharging. the latter is a penalty cost incurred at each recharge. the charging time is assumed constant and the battery soc when departing from a customer can be full (fully charged) or 80% of the capacity (partially charged). the authors used an iterative route construction and improvement procedure to solve the problem. erdoğan [cit] considered green vrp (gvrp) in which afvs are refueled at stations en-route. refueling times are assumed constant and after refueling the tank becomes full."
"ease of use. one of our goals was to design an intuitive, easy to use and learn touch-interaction technique for 3d data. on a 7-point likert scale participants agreed (median 6) that touch was easy to use, and disagreed (median 2) that it was difficult to remember how to interact with the frame and to use it in general. given these positive responses we feel that our g9 was achieved."
expensive station removal: our aim in this removal heuristic is to save from energy cost by eliminating unnecessary recharges using more expensive charging options. the operator lists the stations (chargers) in the non-increasing order of the cost they incur and removes a pre-determined number of stations from the top of the list.
"overall, model 1 provided the optimal solutions faster and in many instances, it provided better upper bounds when the time limit is reached. so, we decided to use the results obtained with model 1 to benchmark our solution methodology that we will describe in the next section."
"specifically, we were able to obtain route plans requiring less evs or reducing energy cost or both in all instances where the time windows are narrow. on the other hand, the influence of the availability of fast chargers was minor when the time windows are wide."
"in alns, we used the same parameter values as reported in keskin and çatay (2016) (see appendix b). [cit] . in the optimization phase, we employed cplex 12.6.2 with its default setting using single thread. the matheuristic was implemented in java programming language and the experiments are conducted on the same workstation described in section 3.2.3."
"the wayfinding task required participants to explore a 3d astronomical dataset consisting of a particle simulation representing different masses (fig. 8) . we asked them to examine the data for five minutes, exploring different parts and different scales, and to report any interesting or strange aspects they noticed. in particular, we asked them to explore and describe the 3d shape of the clusters in the core region."
our aim in performing test (b) [cit] in terms of run time. note that they coded their algorithm in fortran 95 and executed on an intel
"the level of metastasis can be graded by a pathologist under a microscope with the help of certain tissue stains. this staining process helps to reveal the related parts and makes the cancerous cells distinguishable under the microscope. hematoxylin and eosin (h&e) staining is a commonly used procedure to this end. it is possible to observe cancer cells in a tissue stained with h&e staining. however, investigating tissues under a microscope is a time consuming process. in order to aid the pathologist while working with h&e stained tissues, computer based algorithms and tools are developed [cit] . also computer programs such as imagej [cit] and fiji [cit] are also being used by the pathologist due to their successful built in machine learning tools."
"in this work, we expand our previously implemented 1-d sift algorithm with the new m-1-d sift feature extraction approach. it is shown that the both methods are compatible with different lbp histograms. it is experimentally proven that with the use of presented approaches, it is possible to classify the h&e stained liver tissue images with 88% accuracy. as a future work we are planning to use different types of lbp histograms and conduct additional experiments and further extend the m-1-d sift feature extraction process."
"least used station removal: the motivation behind this heuristic is to reduce the cost of visiting recharging stations often. so, we attempt to eliminate unnecessary recharges and satisfy the energy needs of evs by visiting less number of stations. this can be achieved by utilizing the visited stations to recharge the battery as much as possible instead of recharging small quantities with frequent visits. the operator lists the stations (chargers) in the non-decreasing order of the quantity of energy they charge and removes a pre-determined number of stations from the top of the list."
"in addition, the number of vehicles is decreased by two in 8 instances and by one in 20. the fleet size is reduced in all r-and rc-type problems. furthermore, both the fleet size and energy cost are improved in 13 type-1 instances. we can conclude that the improvements in type-1 problems were accomplished by utilizing fast and super-fast chargers effectively based on the number of recharges and the energy quantities given in the last four columns."
"in figure 3, h&e stained liver tissue images are shown. as it can be seen from the figure 3 that there is an obvious pattern difference in between normal and cancerous images but some similarities are also exists. thus, we conduct different experiments to reveal these differences:"
"and, +1 are used to determine the charger type if the ev is recharged at a station between customers and + 1. note that these variables are defined for only consecutive customers and the number of, +1 and, +1 variables is the same as the number of arcs on the route. the mathematical model is formulated as follows:"
"a relatively small area of previous work deals with touch-based interaction with scientific visualizations in 3d spaces. direct-touch spatial interaction with 2d elements on a 2d surface are more common as the interaction is fairly straight-forward: x-/y-motions of a single finger or pen can be directly mapped to matching translations of virtual 2d elements. adding mode-less capabilities for object orientation to 2d spatial movement, however, then requires the mapping of 2 dof input to 3 dof of output (location in x and y and orientation) [cit] . the rotation-and-translation (rnt) technique [cit] solves this problem for touch-interaction by determining the orientation of an object based on the spatial movement of a touch-point on the object over time. tnt [cit], a related technique, uses a pen's spatial orientation to set a virtual object's orientation. with the advance of multi-touch surfaces it also became possible to use more than one input point to control virtual objects; e. g., the popular two-touch interaction technique [cit] (also named rotate-scale-translate, rst [cit] ) uses two fingers (4 dof) to control location, 2d-orientation, and size of objects."
"design. we used a repeated-measures design with the withinsubject independent variable input device (frame, mouse). each participant performed 4 runs of 4 trials for each input device and task. for each run we chose 4 unique starting positions of the space per task and varied their order between runs using a latin square. for rotation tasks we additionally chose two rotation directions per run which was also varied using a second latin square. tasks were always performed in the same order (table 2 ) and the order of presentation of the two input devices was counterbalanced among participants. the first two runs were discarded as practice runs for the final analysis of the data."
"the targets set by governments and the new regulations encourage the usage of alternative fuel vehicles (afvs) such as solar, electric, biodiesel, lng, cng vehicles. many municipalities, government agencies, non-profit organizations, and private companies are converting their fleets to include afvs, either to reduce their environmental impact voluntarily or to meet new environmental regulations (erdoğan [cit] ) . consequently, the advancements in the electric vehicle (ev) technology have gained momentum in parallel with the growing environmental concerns in societies."
"to improve the solution quality, we employ a post-optimization procedure systematically throughout the alns process. this procedure uses a commercial solver to optimize the chargerelated decisions along each ev route by fixing the sequence of the customers. these decisions include the locations of the stations, selection of the charger type, and the amount of energy"
"to be able to accommodate controls for the remaining two parameters-zoom and z-translation-we add additional regions to the frame. for zooming we use the four corners of the regular frame (fig. 4(a) ). downward motions started from the top corners zoom in, subsequent upward motions zoom out. the bottom corners have the opposite behavior, initial upward motions zoom in and subsequent downward motions zoom out. translation along the z-axis is made possible using two extra frame elements at the top and the bottom of the widget (fig. 4(b) ). here, the perspective shape of the extra frame elements suggests the direction of movement. starting motions from the top downward moves the camera away from the dataset, while motions from the bottom initially upward move the camera closer to dataset. all these specific regions act as spring-loaded modes for adding these additional degrees of freedom (g4). to further support usability, we always display icons associated to the chosen actions (as shown in fig. 3 and 4) to make it easier for users to remember the mappings."
"overall, the role of touch for scientific data exploration will have to be explored further. the tradeoffs of speed over precision, ease of use, and perceived immersion require further attention for scientific data exploration. participants named several preferences for touch which warrant further analysis, such as: embodiment or a sense of feeling connected to the data, walk-up and use scenarios for different types of user groups as touch was rated \"intuitive\" and \"easy to use,\" long-term vs. short term usage scenarios in terms of \"fatigue,\" and new audiences for scientific data exploration as some participants described touch to be more \"fun,\" \"refreshing,\" and \"innovative.\""
"tasks. we tested eight travel and one longer wayfinding tasks. travel is characterized by low-level actions that control position and orientation of the 3d space, while wayfinding requires a cognitive component and may include planning, higher-level thinking, or decisionmaking [cit] . wayfinding is important to scientific analysis but we also wanted to test travel tasks as a fundamental component of wayfinding to see more specifically how our technique supported these individual lower-level aspects. table 2 summarizes the eight travel tasks. we tested the four main interaction techniques translation, rotation, zoom, and the integrated rst technique. as constrained rotation requires participants to perform a bi-manual task by touching the frame on different sides of the 3d display, we tested all three individually to understand if difficulties would arise due to the spatiality of this interaction."
"technique integration: the 4 dof dual-touch technique for x-/ytranslation, rotation around the z-axis, and zooming can easily be combined with the 2 dof single-touch control of trackball x-/y-rotation. this results in a combined single-and dual-touch interaction for controlling a single large 3d space. this 6 dof technique, however, has a number of problems in our context of scientific visualization. while the single-touch rotation can control the orientation without affecting the zoom parameter, this is not easily possible with the two-touch technique. as both fingers are controlled by the user to some degree independently, it is difficult to achieve translation or rotation while leaving the scale of the space unaffected. similarly, translation in x and y cannot be performed independently from rotation around z and zooming. finally, we are only able to control 6 dof and it is not possible to manipulate the space's location along the z-axis. therefore, we need to involve other elements to integrate this last dof (g1) while also allowing single-touch-only interaction (g7). this is described next."
"as in removal heuristics, different insertion mechanisms are designed for customers and recharging stations. a customer insertion (ci) mechanism is used after every cr operation whereas the station insertion (si) follows only an sr operation."
"in the field of information visualization, the challenges of interacting with data have been analyzed in several survey articles [cit] . these surveys focus on specific data interaction techniques such as selection, exploration, change of representation, or filtering. [cit] further distinguish between higher-level interactions (explore, analyze, etc.) and lower-level interactions (filter, sort, cluster, etc.), correlating those with representation intents (depict, identify, compare, etc.) and interaction techniques (dynamic query, zoom, brushing, etc.). while these overviews have targeted information visualization, interaction is no less important for scientific visualization [cit] . we share the same fundamental notion that useful insight often only emerges out of the interactive data manipulation experience. yet, in scientific visualization we focus on visualization spaces in which most of the data has preconceived spatial meaning and, hence, many interactions for data exploration have to support explorations that match a user's existing (likely physically-based) mental model of a dataset [cit] ."
"next, we discuss how we realized these goals in two stages: (1) transitioning from 2d direct-touch interaction to 3d space manipulation and (2) mapping specific direct-touch interactions to specific 3d manipulations using frame interaction with fi3d."
"in this study, we assumed that all stations were already located and equipped with all types of chargers. however, this may not be the case considering the high installation costs and lack of infrastructure. so, the problem can be extended to a location routing problem where the recharging stations are sited, their charger equipment and capacities are determined, and the evs are routed simultaneously. further research on this topic may also address the heterogeneous fleet case where the vehicles vary by their cargo capacities, battery condition and age which affect their cruising range and discharge/recharge durations. furthermore, we assumed that recharging stations and chargers were always available. in real life, there may be queues in the stations and the evs may need to wait for service. alternatively, it may drive to another station. so, variability in recharging times can be investigated within the stochastic context. the authors are currently working on this extension."
"interactive 3d scientific visualizations have made a significant impact in many different disciplines. yet, these systems are not typically regarded as being easy to learn or use [cit] . touch-based interfaces can potentially improve this situation as users of touch-based systems commonly associate them with being 'intuitive' and 'natural.' part of the recent popularity of touch-based interaction is certainly due to the dedicated ui design and the novelty of touch as an interaction paradigm, but research has also shown that it can indeed be more effective than indirect forms of interaction. for example, touch interaction has been shown to outperform mouse input for the selection of targets on the screen [cit], to facilitate awareness in collaborative settings [cit], and to provide somesthetic information and feedback that is beneficial for effective interaction both in real and virtual environments [cit] ."
"this is due to the reduced recharging times at stations which enable the ev to catch the time windows of all customers. in this case, the total distance travelled is 239.13 and total cost of energy is 267.60. although fast and super-fast recharges are more expensive, the total cost of energy decreases because the ev makes a shorter trip consuming less energy."
"the first thing the participant remarked was that he was missing two-point rst interaction, pointing to the screen with his two arms and suggesting a rotation and zooming motion. this is interesting and only reinforces our earlier point of including rst for general frame interaction. despite the expected rst interaction not being available in this tool, however, the neuroscientist was able to interact with the fiber tract visualization immediately and did not need to be reminded of the mappings. he commented that he liked the idea of using the frame and that it was very easy to use. the neuroscientist particularly liked the way of manipulating the cutting planes and compared this technique to trackvis, saying that trackvis has three small viewports with extra sliders to control the slices and that he appreciated that our technique does not require such extra windows. he also enjoyed the possibility to explore fiber tracts that connect two regions by selecting them with two simultaneous touches. however, he disliked our second technique of moving the selection boxes by touching theirs sides and 'pulling' or 'pushing' them. he compared this technique to the software packages he is used to which often have spheres that can be dragged parallel to the view plane. he said that this view plane motion is easier and more intuitive to use than dragging the boxes one axis at a time."
"at the start of each travel task, the data space was repositioned and a target image was shown (see fig. 7 (a) and 7(b)). participants were asked to reposition the space as quickly and accurately as possible to visually match the target image. as participants would always reposition the whole space itself to achieve a matching we did not have to separately measure target selection and travel times for these tasks. to aid the matching process, a transparent red target area was displayed that served as a reference frame into which the target had to be moved. when participants let go of the display or mouse button, we calculated whether the target image had been matched and in this case stopped the trial automatically. if participants felt that they could not match the target image they could abort the trial, but this never occurred."
"in this section we present a second case study to demonstrate that the concept of frame interaction with 3d spaces can also be employed in visualization domains other than particle simulation. specifically, we show how we applied the concept to an illustrative medical visualization tool [cit] and discuss some adaptations specific to this domain."
"to inform the design of fi3d, we first looked at related traditional mouse-based interfaces such as 3d viewers. here, different interaction modalities-often combined with key combinations-are used to map the 2 dof mouse input to 7 dof navigation in 3d: motion along the x-, y-, and z-axes, rotation around the x-, y-, and z-axes, and (uniform) scale or zoom. we thus chose to incorporate these techniques in our direct-touch technique. before describing the final interaction design, however, we examine the different necessary interactions individually:"
"we tested the performance of our algorithm on both small and large benchmark instances from the literature. our numerical results in small-size instances showed that our matheuristic outperformed cplex both in solution quality and run time. in large-size instances, the results revealed the advantage of using fast charging in terms of fleet size and energy consumption."
in addition to these basic interactions we also need to provide mappings for two more exploration tasks: the manipulation of cutting planes and the selection of a sub-section of fiber tracts.
"to understand how people would perform with and rate our frame technique, in particular related to our goals g9 and g10, we conducted a comparative study. since the mouse is currently the standard interface for 3d interactive desktop applications in scientific visualization it was chosen as the baseline and compared to the frame technique based on speed, accuracy, and qualitative feedback for eight travel tasks and one wayfinding task. since these two technique are considerably different in interaction style, a comparison can show where tradeoffs exist and how our technique can be further improved. based on previous work on the comparison of touch and mouse interaction [cit], we hypothesized that touch interaction would not outperform the mouse for tasks that required 3d interaction according to one interaction type (translation, zoom, rotation only) but that the touch technique would outperform the mouse for tasks in which multiple integrated types of interactions were required. we further hypothesized that the touchtechnique would score higher on questions related to how immersed the participants felt in the data and would be generally preferred."
"in summary, he said that the interaction techniques are intuitive to use and that he appreciates the ability to work with data on a single large viewport without much clutter while all important interactions are possible. also, he suggested to investigate collaborative scenarios since scientists usually work together when analyzing fiber tract data so that touch interaction on a wall display as we use it would be beneficial. while the results from one participant certainly cannot be generalized, it still gives some evidence for the applicability of frame-based direct-touch interaction with visualizations of medical 3d datasets."
"since the primary objective is to minimize the number of vehicles, we devote some iterations throughout the alns to this purpose. after every iterations we dedicate consecutive iterations to remove customers using the random route removal or greedy route removal operators and insert them using fr ci heuristics described in section 4.2.1. using only super-fast chargers when recharging is needed decreases the duration of the recharge and allow serving more customers along the route which is not possible otherwise due to time-windows restrictions. this yields longer routes with more frequent visits to customers and thus offers an opportunity to reduce the total number of vehicles in the fleet."
"the performance of the system highly depends on: first, the precision of the channel quality estimator and second, the number of packets transmitted in a specific scenario. in fact, errors in channel quality estimation can lead to erroneous schedules, and few transmitted packets lead to an insufficient number of samples to observe the average reliability computed by the scheduler. the investigation of the impact of different link quality estimation techniques on the proposed scheduler is left as future work."
"furthermore, we introduce an aging process for all the unscheduled frequencies in a specific time slot. it consists of artificially improving the pdr of unscheduled frequencies such that they can be explored in the future and not blacklisted by the scheduler. the aging process is implemented using the ewma filter. for every transmission opportunity, the pdrs of all the rbs are artificially \"improved\" by"
"3) implementation of the model: in this section, we describe in details how the interface of the wdetserv model is implemented. this is illustrated in fig. 3 . a) topology: the topology provided to the routing procedure of the industrial qos framework is the frame size link topology described in section ii-c2. b) getdelay: for each edge of the frame size link topology, the getdelay function returns the worst-case delay of a packet transmission using the subframe size corresponding to the given edge. the transmission of a packet lasts up to one subframe duration s i t s, where t s is the duration of a slot. however, considering that a burst of b packets could arrive just after the beginning of a new frame, each packet would have to wait for b additional frames to be fully transmitted. as such, the worst-case delay of a packet is"
"we consider a setup in which wsn motes are connected to the wired infrastructure via wgs in a star topology fashion. this topology is particularly common in a dense industrial scenario where gateways are extensively installed across the factory (wisa [cit], wirelesshart [cit] ) and it is suitable for low-latency applications as control messages can be delivered faster. in fact, multiple hops negatively affect latency and are more difficult to organize."
"in order for the routing procedure to operate on a single topology, the topologies provided by both the wired and wireless models have to be merged. this operation is illustrated in fig. 2 . indeed, as each wg appears once in both detserv [cit] and wdetserv topologies, the final topology is simply obtained by merging the individual topologies at the different wgs."
"research in industrial communication networks has always pursued the integration of wired and wireless networks in order to provide end-to-end industrial qos. for this reason, a considerable number of hybrid solutions merging wireless and wired in industrial networks has been developed."
"as there are no schedulers addressing the reliability in our scenario, we compare our approach with the state-of-the-art scheduling routines. we have selected a classic approach known from the lte resource scheduling: maximum throughput (mt). mt scheduler is a greedy approach that allocates the resources to the applications that can make the best use of them. in the case of lte, this implies users with higher channel quality. in our case, however, the throughput (per rb) is equivalent to the pdr γ m t,f . hence, we allocate a rb to the application with the highest success probability. the basic pseudocode for the scheduler is presented in algorithm 1. the original mt scheduler is classified as best effort. our adapted version has a certain degree of qos-awareness. if a mote has already achieved its reliability requirement, it is not allocated any more rbs (algorithm 1, line 18 )."
"since the release of the tsch medium access and the 6tisch architecture [cit], considerable amount of work was performed concerning the problem of scheduling in iwsn. different distributed schedulers were proposed in tsch [cit] . however, most of the decentralized solutions provide dynamic trafficbased scheduling in a multihop network [cit] . here, reliability is investigated only as a result of internal interference caused by frequency reuse. furthermore, several centralized traffic-based schedulers have been studied for tsch [cit] . also in this case, the scheduling algorithms allocate resources based on the amount of traffic, and aim at minimizing the latency and the buffer occupation. as centralized schedulers build conflict-free schedules, there is no packet loss due to internal interference, however the effect of external interference is not taken into account."
"in order to handle subframes, the scheduler still operates identically and on the same scheduling graph. however, when running the larac algorithm, the scheduling graph is split according to the considered subframe size. in the example of fig. 4, for a subframe size of two slots, the scheduling graph would be split at node n 2 . for a subframe size of one slot, the scheduling graph would be split at nodes n 1, n 2, and n 3 . the scheduler then runs the larac algorithm on the different subgraphs and returns a schedule only if a path has been found on all the subgraphs. this ensures that the target reliability level is guaranteed for all the iterations of the considered subframe within the superframe."
"industrial networks carry critical messages, e.g., measurement and actuation signals for large automated production facilities. most of these critical messages must be delivered within a given time; otherwise, actuators can misbehave, thereby potentially leading to important material or physical damage [cit] . as such, we define the industrial qos requirements of an application by means of two parameters: its delay bound and its target reliability. the target reliability defines the percentage of sent packets that have to reach the destination within the delay bound."
"the details of the channel quality estimation procedure are described in section iv-a, while sections iv-b and iv-c present the different types of schedulers implemented for the evaluation."
"as such, [cit] identify the requirements of a network model. first, the network model must provide the routing procedure a topology on which to perform path finding. second, the network model must provide the following interface to the routing procedure (see fig. 1 ). 1) getdelay: it provides, for each edge in the topology, the delay of this edge. it allows the routing procedure to ensure that it provides a solution respecting the delay bound of the application. 2) hasaccess: it checks whether enough resources are available at a given edge. it is used to ensure that the new application will not violate the delay bound of previously accepted applications in the network. 3) registerpath: after the routing procedure is completed, it updates the network model to reflect the addition of the new application. 4) deregisterpath: it updates the network model to reflect the removal of an application. additionally, a getcost function can be optionally defined for each edge in order to provide to the routing procedure a way of knowing which solution to prefer if several are available. the getcost function should be defined so as to maximize the amount of applications that can be accepted. the routing procedure then tries to find the path with minimal cost that satisfies the delay constraint. the resulting routing problem corresponds to a constraint shortest path (csp) problem [cit] ."
"first, in section v-b1, the correctness of the reliability-based scheduler of section iii is evaluated with simulations under the assumption of perfect channel quality information. second, in section v-b2, the impact of channel quality estimation is evaluated in a static interference scenario. in section v-b3, dynamic interference scenarios are shown and the suitability of our dynamic scheduling algorithm is demonstrated. finally, in section v-b4, we present the performance of the mt scheduler, and compare it to the reliability-based scheduler."
"as earlier mentioned, we repeat each test 100 times to get statistically significant results. we do not observe a significant difference among runs, and the standard deviation of the fault rate and locations among these runs is negligible. more details about these 100 runs are summarized in table. ii."
"in this article, we solve the issue of seamless end-to-end industrial qos provisioning in hybrid wired/wireless networks. we achieve this by means of two contributions."
"the state-of-the-art solution to external interference in order to guarantee reliability is blacklisting, i.e., excluding interfered channels. however, [cit] showed that [cit] so that industrial qos in hybrid wired/wireless environments can be provided. reliability can be improved by using all channels independently of the interference scenario."
"uniqueness measures inter-chip variation by evaluating how well a particular puf circuit design can be differentiated between k different devices. ideally, a puf circuit is expected to produce an average interchip hamming distance (hd) of 50% by comparing the response from two devices supplied with the same challenge. the uniqueness, representing the average inter-chip hd, is defined as:"
"b) reroutes upon channel quality updates: as the quality of the wireless channel changes, it potentially leads to the violation of previously provided qos guarantees. in such a case, a global routing decision allows to reroute an application in the backbone (e.g., taking a faster path) such that the higher delay of the wireless link is still sufficient to fulfill the requirements of the application (trigger reroute arrow in fig. 3) . c) multiple wgs: a mote could have the opportunity of choosing between different wgs in its vicinity. global routing allows to favor the usage of one wg over another in order to be able to fulfill the requirements of the application."
"in this section, we describe the integration of the proposed reliability-aware scheduler with the wdetserv model presented in section ii-c and its interaction with the interface."
"however, in iwsn, guaranteeing reliability is a difficult task. to tackle this challenge, we propose the first reliability-based dynamic scheduler for tsch that is able to guarantee a target reliability in presence of dynamic interference, and is required by the wdetserv model to enable seamless qos provisioning in hybrid networks. the scheduler provides reliability by combining retransmissions over interfered channels, thus efficiently using the available radio resources. the proposed scheduler was evaluated by means of simulations in a proof-of-concept iwsn in presence of dynamic interference. the results prove the suitability and flexibility of the proposed scheduler and motivate further work. in particular, the interplay between the scheduler and the link quality estimation should be investigated, and a new probing mechanism should be designed to retrieve link quality information in a timely manner and ensure the safe operation of the network."
"at the end of every frame, the wg sends the up-to-date link qualities to the scheduler, which replies with new schedules fulfilling the target reliability according to the new link qualities. in this way, at the beginning of the next frame, the wg can distribute the new schedules by including them in the enhanced beacon (eb), a broadcast control message that provides necessary information for the operation of the tsch network, such as synchronization and radio resource management. it can happen that this message is not correctly received by all the devices. in this case, the motes can temporarily silence their radios (avoiding potential interference in the network) and wait for the next eb. as control messages can be transmitted using robust radio resources (e.g., non-interfered frequencies), in our evaluation, for simplicity, we assume that no eb is lost and we leave the evaluation of the network under this condition as future work."
"frequencies 9 and 11 with corresponding pdrs equal to 0.68 and 0.9, respectively. the allocation provides an application reliability of 0.987, which fulfills the requirement of 0.9."
"finally, we analyze the dynamics of the scheduler with increasing interference conditions. this leads to critical situations where some devices cannot be served by the network. this is done to evaluate the performance of the scheduler, however, in a real implementation, the devices should be served by backup links, and the effect of interference should be carefully taken into account such that the qos can be guaranteed even in worstcase conditions. also in this case, a probing procedure is needed to acquire accurate channel quality information."
"its expected value γ m t,f is the pdr of the frequency f at time t of mote m. for every rb in the schedule s m of mote m, the wg updates the corresponding pdr with the outcome of the transmission by feeding an exponentially weighted moving average (ewma) filter"
"in this section, the reliability-based wireless scheduler presented in section iii is evaluated by means of network-wide simulations of an iwsn operating in an interfering environment. for this, a custom python discrete event simulator is used."
"furthermore, unlike wired links that are assumed to be 100% reliable, wireless links are naturally subject to packet losses. to overcome this issue in tdma wireless networks, dynamic scheduling of communication resources, which reacts to changes in the channel quality, is needed. this is particularly challenging for iwsn due to the low-power nature of the communication and the harsh industrial wireless environment. thanks to the ieee standard 802. 15 .4e (tsch), radio resources can be abstracted in a time-frequency resource grid, and resource blocks (rbs) can be allocated to industrial applications by selecting specific time and frequency values. different rbs have different time-varying packet delivery ratios (pdrs), which might not meet the reliability requirements of the application. the state-of-the-art solution to this problem is blacklisting [cit], i.e., excluding interfered channels from the rbs, thus reducing the capacity of the system. to overcome the capacity limits, a centralized scheduler that is able to compute a reliable schedule, which combines rbs and does not exclude them, is needed. although several scheduling techniques for tsch have been proposed, there is no scheduler that supports reliability guarantees efficiently."
"we perform an experiment to study the effect of the environmental temperature on the behavior of faults when v ccbram is lowered below v min . toward this goal, the hardware board is placed inside a chamber that its temperature can be regulated by a heater. we monitor the on-board temperature using pmbus commands. through experiments, brams fault rates are extracted and shown in fig. 4 under various on-board temperatures, 50"
"in this article, we solve the issue of qos provisioning in hybrid wired/wireless networks for industrial applications. we achieve this by providing an interface that enables seamless industrial qos provisioning across wired and wireless domains. in order to achieve this, the delay bound and target reliability of an application must be fulfilled in both networks."
"in this paper, we propose a new mpuf design with resistance to machine learning attack by using a weak puf to obfuscate the challenge of a strong puf design. a case study is presented by constructing a mpuf using the picopuf [cit] and a conventional apuf design. two widely employed machine learning based attack techniques, lr and cma-es, are utilised to analyse the resistance of the proposed mpuf design to such attacks. the proposed mpuf design achieves a 50% prediction rate using the lr attack compared with 100% for the conventional arbiter puf design. using cma-es, although the proposed design can be successfully predicted for a small challenge size, it is still significantly more resistant to such attacks than the apuf. however, when the challenge size is increased to just 32 bits the cma-es attack can only achieve a prediction rate of approximately 80% for the mpuf design, even with a large sample set of 10,000 crps, whereas it can still achieve a 100% prediction rate for the conventional arbiter puf. two mpuf designs are also implemented on each of 11 xilinx artix-7 fpgas. the uniqueness and uniformity metrics for the proposed mpuf design exhibit good results of 40.60 % and 37.03 % respectively. this significantly improves upon previous work into multi-puf and illustrates the design's feasibility for implementation on fpga."
"first, in section ii-c1, we describe the context in which the proposed network model can be used. second, in section ii-c2, we describe the topology provided by the model to the routing procedure of the qos framework. third, in section ii-c3, we describe the implementation of the interface shown in fig. 1 ."
"in this section, the details of the implementation of the dynamic scheduling procedure in a tsch industrial wsn are presented. dynamic scheduling is a periodic operation that consists in: first, sending up-to-date channel quality information to the scheduler, and second, retrieving up-to-date schedules from the scheduler, in a timely manner."
"where, p n, q n, r n and s n are the notations of the delay segments in each stage of the conventional arbiter puf. the parity check vector"
"the remainder of the article is structured as follows. in section ii, the end-to-end qos routing framework is presented together with the first contribution, wdetserv. in section iii, the reliability-based wireless dynamic scheduler is introduced. in section iv, the details of the implementation of the reliabilitybased wireless scheduler are discussed, and, in section v, evaluated by means of simulations in an interfered iwsn. finally, in section vi, the conclusions are drawn and future work is discussed."
"the simulations have proven that the proposed reliabilitybased dynamic scheduler is able to provide the required industrial qos in presence of dynamic interference. in this work, proof-of-concept results demonstrate the suitability of the reliability-based scheduler for iwsn, and leave space for further improvements which are discussed in this section."
"in this section, we present our second contribution, the reliability-based dynamic scheduler. in fact, as discussed in section ii-c, a wireless scheduler that provides reliability over lossy wireless links, and enables seamless qos provisioning in hybrid networks, is needed."
"pufs are a promising lightweight security primitive which use manufacturing process variations to generate a unique digital fingerprint for an electronic device, e.g. application-specific integrated circuits (asics) or fpgas. since even the manufacturer cannot control these variations, pufs are inherently difficult to clone, providing additional tamperevident properties. since no two pufs are identical, the same n−bit input challenge generates a different n−bit response on different devices. such a security primitive provides a number of advantages over current state-of-the-art alternatives and allows for higher security protocols and applications, e.g. key storage and device authentication."
the rest of this paper is organised as follows. we present the new lightweight mpuf design in section 2. in section 3 we present a mathematical model of the proposed mpuf design and compare it to a conventional arbiter puf design. the lr and cma-es modelling attacks of the proposed mpuf design are discussed in section 4 and a performance evaluation is presented in section 5. we conclude with a summary and discussion of our results and future work in section 6.
"a typical next-generation industrial automation system consists of different levels [cit] : the field level, where sensors and actuators are directly involved in the automation process, the automation level, where process control decisions are taken by industrial controllers, such as programmable logic controllers, and the management level, where best-effort ip traffic is exchanged. automation and management levels are typically connected to wired networks, while devices in the field level are wirelessly interconnected by an industrial wireless sensor network (iwsn)."
"the proposed mpuf design is composed of the n− picopuf designs [cit] and an apuf design. using both picopuf and apuf models, the proposed mpuf design model can be described as"
"besides and complementary to the redundancy techniques, qos can be provisioned via appropriate management of wireless resources through scheduling. in general, scheduling of wireless resources is an np-hard combinatorial problem [cit] of mapping the demands to the time-frequency resources. in contrast to iwsn, it is a mature research topic in cellular networks, such as 3gpp long term evolution (lte). the scheduling problem in lte is typically considered as a tradeoff between resource utilization and fairness. schedulers in lte can be classified into two types: best effort and qos aware. maximum throughput (mt) and proportional fair (pf) are classic examples of the first type. mt allocates the resources to the users with the best channel quality, hence, maximizing the total resource utilization under the demand saturation assumption. on the other hand, pf weights the current channel quality and the transmission history of a user, achieving a balance between fairness and utilization. qos-aware schedulers are typically based on the prioritization of packets with strict delay or data rate requirements [cit] . however, in lte, reliability is outside of the scope of the scheduling problem, as it is achieved by other techniques, such as hybrid automatic repeat request and adaptive modulation and coding."
"the proposed scheduler operates by finding a path in the scheduling graph such that the reliability of the found path, i.e., the product of the unavailabilities (1 − γ t,f ) of the chosen edges, satisfies the target reliability requirement of the application r 0, i.e.,"
"the proposed mpuf design comprises an array of n elementary 1-bit cells to generate an n−bit response. the design of a 1-bit response generation cell is shown in figure 1, and is composed of a group of weak pufs and a strong puf. the outputs (responses) of the weak pufs are xored with an n−bit challenge and the result then forms the challenge for the strong puf."
"the value of β must be carefully chosen as it introduces a potential penalty in reliability when exploring channels with low pdr, and should be, as discussed in section v-c, tailored to the interference behavior."
"comparing the mathematical models of both the conventional apuf [cit] by itself and the proposed mpuf designs, we can see that the proposed mpuf design demonstrates higher complexity than the conventional apuf since the actual input challenge to the apuf is obfuscated and masked."
"uniformity represents the proportion of zeros and ones in a puf response. ideally it should be 50%, i.e. half ones and half zeros in a response, making it difficult for an attacker to guess the response of a particular device. for device φ i and an n -bit response the hamming weight (hw) percentage is defined in equation 6 ."
"in this section, we present the proposed wdetserv model, a network model for tdma-based wireless communications that can be used as part of the framework described in section ii-b. in the wdetserv model, the maximum packet size, the minimum packet size, the maximum burst and the sustainable rate of the application have to be known. unlike wired links, which are assumed to be 100% reliable, wireless links are naturally subject to packet losses that cannot be neglected. for this reason the model relies on a reliability-based scheduler that will be described in section iii."
"2) routing topology. frame size link topology: every tdma wireless link is modeled as a set of directed edges defining the different frame sizes that can be used. in order to avoid ambiguity, we refer to these frames as subframes, in contrast to the main tdma superframe. the size of every subframe constraints the maximum delay and reliability that can be achieved using this subframe."
"the industrial qos framework is independent of the network model (see fig. 1 ). as such, in order to provide a hybrid wired/wireless industrial qos framework, it is sufficient to provide a network model for the wireless domain that satisfies the requirements described in section ii-b."
"in our implementation, channel quality estimation is passively performed by the wg upon reception of data packets. for every mote, the wg estimates the pdr of every frequency used for communication over time. the results of the estimation are sent to the scheduler at the end of every superframe, which keeps the network radio resources up-to-date."
"(5) where, r i and r j represent n -bit responses from two puf circuits φ i and φ j supplied with the same challenge c. figure 8 shows a histogram of the uniqueness result for the proposed mpuf design, which has an empirical mean of 40.6 % and a standard deviation (std) of 8 %. this is equivalent to the uniqueness value of the flip flop based arbiter puf (ff-apuf) design [cit] . moreover, compared to the uniqueness results from 5.44 % to 10.82 %) achieved by previous work on multi-puf [cit], the proposed mpuf design demonstrates a significantly higher capability to differentiate between different devices."
"extending γ t,f introduced in section iii, we define the pdr of a rb corresponding to time-frequency pair (t, f ) for a specific mote m as rb m t,f . the pdr exhibits a stochastic behavior that can be modeled by a bernoulli random variable [cit]"
"first, section v-a summarizes the details of our simulations together with the wireless propagation environment and the model of the external interference. then, in section v-b, the simulation results of our reliability-based scheduler are shown and discussed in different interfering scenarios. finally, in section v-c, we discuss the observations deduced from the evaluation."
"every simulation performs the following steps. initially, in the absence of channel quality information, an arbitrary default pdr is assumed for every frequency. afterwards, motes join the network until the scheduler is not able to find a schedule that fulfills the target reliability. finally, actual packet transmission starts, and correct channel quality information is provided to the scheduler, which adjusts the number of devices in the network according to the available capacity, eventually deallocating devices if the target reliability cannot be reached."
"where b is the maximum burstiness of the application. as resources are assumed to be correctly allocated by the scheduler, this bound will always be satisfied for each application."
"in fig. 9, we plot the evaluation results of the mt scheduler implementing the algorithm from section iv-c in the dynamic interference scenario, with the same simulation parameters of the proposed reliabilitybased scheduler in fig. 8 . we observe that the mt scheduler achieves the target reliability for all motes, with an average of 0.917. the delay ecdf of the mt scheduler is comparable with the reliability-based scheduler. as mt is greedy with respect to the frequency allocations, it consumes more resources, which has resulted in slightly higher average reliability at the expense of supporting only 9 motes, compared with the reliability-based scheduler that supports 22."
"the picopuf puf design, [cit], demonstrates both high reliability and uniqueness, two important metrics in puf designs. the design of a 1-bit response generation cell is designed to compactly fit into one fpga slice, as schematically shown in cross-couple nand gates are used as an arbiter to decide which signal arrived first, and outputs the response as a binary value 0 or 1. the apuf is one of the best studied strong puf designs, and an example is shown in figure 3 . to form a 1-bit apuf, two parallel cascaded n−stage multiplexer (mux) chains and one flip flop are used. two muxs are constructed as either a cross-or straight-through connection based on the input challenge bit. after propagation through the final n−th stage, the two signals arrive at the cross-coupled nand gates which determines which signal arrived first, and outputs a 1-bit response, either 0 or 1, accordingly."
"1) perfect channel quality information: simulations under the assumption of perfect channel quality information are performed to verify the correctness and test the performance of the reliability-based scheduling algorithm independently from the channel quality estimation. figs. 5 and 6 show the empirical cumulative distribution function (ecdf) of the packet delay and reliability for all the motes supported by the scheduler during the entire simulation. the results show that the proposed scheduler is able to meet the target reliabilities of 0.9 and 0.99, and fulfills the latency bound of 160 ms by achieving a maximum delay of 80 ms when the channel quality information is known."
"to support the communication requisites between controller and field devices, strict industrial quality of service (qos) requirements must be satisfied. we define industrial qos as the delay bound and target reliability of an application. these requirements must be satisfied in both wired and wireless network domains."
"finally, we are working on effective mitigation techniques. toward this goal, in addition to evaluating the adaptation of existing techniques, e.g., ecc [cit], htm [cit], we are working on customized methods, as well, accounting the observed behavior of faults discussed in this paper."
"as a result, a wdetserv model operates independently for each wg in the network. this is illustrated in fig. 2 . while the wired part of the network, including the wgs, is modeled by a unique wired detserv model, the different wireless links associated to each wg are modeled by independent wdetserv models. hence, in the following, we discuss the operation of a single wdetserv model."
"where s is the set of edges constituting the found path, i.e., the schedule. the unavailability metric is multiplicative, i.e., the metric values of the individual edges of a path have to be multiplied in order to obtain the end-to-end metric value of the path. however, most state-of-the-art routing algorithms can only deal with additive metrics [cit], i.e., with metrics whose end-to-end value corresponds to the sum of their values on the individual edges. as a result, transforming the unavailability metric into an additive metric allows to use state-of-the-art algorithms. to do so, we apply the logarithm operator to (2). this successively yields"
"we model the wireless communication resources by means of a time-frequency grid, assuming a tdma and fdma combined medium access. the combination of a time slot and a frequency is called resource block (rb) and it is identified, here, by the time-frequency pair (t, f ). by means of a schedule, it is possible to allocate different rbs from the time-frequency grid to an application. the pdr of a rb corresponds to the probability of a successful transmission in this rb and is defined as γ t,f ."
"first, we describe how we model the time-frequency grid as a graph (section iii-a). second, we explain how the scheduling problem can be mapped into a constrained shortest path (csp) problem (section iii-b). third, we present an algorithm able to solve the reliability-based scheduling problem (section iii-c). finally, in section iii-d, we show how the proposed reliabilitybased scheduler can be integrated in the wdetserv model presented in section ii-c."
"in our implementation, data communication between the motes and the wg (introduced in section ii-c1) is always uplink. for every rb allocated by the schedule, a given mote transmits either a data packet, if available, or a substitute probing packet. the wg receives the whole traffic of the network and it is assumed to be capable of selectively receiving or transmitting over the total 16 ieee 802.15.4 frequencies of the 2.4 ghz ism unlicensed band. normal motes are limited to a single frequency for transmission or reception in every time slot."
"additionally, two effects introduce a penalty on the system performance and should be improved in further work: first, the channel quality estimation is currently performed using data packets; second, the aging process adopted to explore new links introduces capacity in the system that is explored using precious data packets. instead, specific probing packets should be sent to estimate the quality of the communication channel. this prevents packet loss when the channel quality worsens, therefore improving the reliability of the application. thus, in order to improve the system performance, a probing mechanism should be designed and introduced."
"as the proposed mpuf design exhibits good resistance to lr machine learning attacks, it is also expected to achieve good puf design metrics. in this paper, the key metrics of uniqueness and uniformity are evaluated. to evaluate these metrics, the proposed mpuf design is implemented on digilent nexys 4 boards that comprise xilinx artix-7 fpgas. two mpuf designs have been implemented on each of 11 fpgas producing a total of 22 individual implementations for testing."
"to predict the proposed mpuf design using a lr algorithm, a group of tests on different numbers of training samples is carried out. figure 5 shows the prediction rates for both the conventional apuf and proposed mpuf designs with training sample sets of size 3000, 5000, 10000, [cit] 0, with the separate test sample data set the same size as the training one. it can be seen that the conventional apuf can be successfully predicted with high reliability and the proposed mpuf demonstrates good resistance to the lr machine learning attacks."
"each mote communicates solely with the wg to which it is connected and we assume that motes connected to different wgs do not interfere with each other. this can for instance be achieved during the network planning phase by appropriately placing the wgs, or with coordinated scheduling."
where p is the set of all the paths from the first node to the last node of the scheduling graph. the problem described by (5) and (6) corresponds to a csp problem.
"a scheduler is responsible for providing reliability guarantees through appropriate scheduling. upon receiving a registerpath (resp. deregisterpath) call, the scheduler schedules (resp. unschedules) the given application according to its reliability level to the subframe corresponding to the given edge [schedule (resp. unschedule) arrow in fig. 3] . the result is then forwarded to the wg in order to notify the involved motes (update schedule arrow in fig. 3) . in order to evaluate the reliability of a given schedule, channel statistics must be collected. this information is obtained by the wg and forwarded to the scheduler (channel stats arrows in fig. 3 ). upon any channel statistics change, applications reliability requirements can get violated. in this case, the scheduler tries to reschedule the applications such that their respective target reliability values are met again (reschedule arrow in fig. 3) . if the rescheduling succeeds, the wg receives and forwards the new schedules to the devices. if the rescheduling fails, end-to-end rerouting is necessary (trigger reroute arrow in fig. 3 ). indeed, changing the paths taken in the wired infrastructure will potentially allow to use other subframe sizes on the wireless link, thereby potentially making the application schedulable again. the concept of end-to-end routing through the wireless and wired links will be further discussed in section ii-d2. the operation of the scheduler corresponds to the second contribution of this article and is detailed in section iii. d) hasaccess: it verifies that communication resources are available for a given application checking that:"
"to check if an application request can be accommodated with a given target reliability level (hasaccess arrow in fig. 3), the scheduler runs the larac algorithm on the scheduling graph. if the algorithm returns a path and the data rate and packet size constraints are satisfied, there is a feasible schedule and access is granted. otherwise, the request is denied. once an application has to be scheduled (resp. unscheduled) [schedule (resp. unschedule) arrow in fig. 3 ], the scheduler removes (resp. adds back) the edges constituting the found (resp. previously allocated) path from the scheduling graph, thereby ensuring that other applications cannot (resp. can) use the reserved (resp. freed) rbs. once an application has to be rescheduled (reschedule arrow in fig. 3), the scheduler simply unschedules it and then schedules it again. in case no schedule is found, the scheduler notifies the upper layers that the rescheduling failed. in this way, for instance, the application can be served by other network interfaces. if applications have different priorities, the highest priority applications are rescheduled first, such that the applications that have to be evicted from the network are the least priority ones. upon reception of updated channel statistics (channel stats arrow in fig. 3), the scheduler updates the γ t,f values associated to the different edges of its scheduling graph."
"where ܲ are bus active power injections, ߠ are bus angles and the coefficient matrix ‫ܪ‬ is real symmetric and nonsingular. solving for ߠ provides the bus angles. for a rigorous mathematical presentation of the above see [cit] ."
the weighted least squares formulation is the weighted alternative to lsq. a diagonal weight matrix ܹ is used to assign different importance to equations. then the system is solve so as to minimize the weighted residual
"there are two key outcomes of this work. first, it transforms the pf problem into two linear systems. this comes in substitution of the usual iterative methods. second, it drastically reduces computational requirements with a small expense in precision. these two merits can combine in novel scenario-based contingency screening for the smart grid. this could even be implemented in hard real time, using effective computing hardware or software."
"one the other hand, it is well demonstrated that a linear system version of the algorithm can have significant computational benefits compared to a full nr solution; approximately only 1/5 of the time required for large cases."
"for systems with high ‫ݎ/ݔ‬ ratios such as typical transmission level systems, ‫ܣ‬ is expected to have a higher condition number than ‫ܣ‬ ொ . this can be explained by the fact (6)"
this solution ensures that the voltage profile satisfies the preset modified reactive power injection to pq buses. the latter is though (5) strongly linked to the reactive power injection to the buses.
"the two suggested versions of the algorithm, li and qq, have been time profiled. for this, an average over multiple runs of the time break-up of the algorithm was done, for a system of 3120 buses. results of this relative intra-profiling are shown in tables iv and v, for li and qq respectively. all computations were done on a typical modern desktop pc (pentium core i7 4x2.80 ghz, 8 gb ram)."
"the power flow problem is one of most fundamental problems in power system engineering. it deals with the determination of the voltage profile of a power system, given some generation constraints. formally the problem can be stated as a system of non-linear equations."
"in the qq version, it is clearly shown that the pf problem is reduced to the solution of two linear systems, one for the angles and one for the magnitudes. the two, require approximately equal time. all other linear-system-based versions have analogous breakup."
"the above solution ensures, within the limits of the approximations, that the voltage profile begets the required modified active power injection to pq buses. the latter is through (5) strongly linked to the active power injection to the buses."
"equations (6) and (7) describe complex linear systems, which can be handled to retrieve the voltage magnitude estimates. known and unknown variables are summarized in table i. since the unknown ߜܸ's appear only in pq bus equations, only the latter are retained."
"the li version is dominated by the solution of the lsq problem. the solution of the linear system of the voltage angles, as per standard dcpf, is included in row \"other\". all other lsq-based versions exhibit similar timing characteristics. as it can be seen, the li version is not really practically applicable, as the solution of the underlying lsq problem scales badly with system size. actually, it is orders of magnitude slower than a typical nr method, that can be used instead to solve the system \"exactly\"."
"numerous well-established solutions exist for the above based on iterative procedures. the newton-raphson [cit] and the fast-decoupled power flow methods [cit] are perhaps the most well-known and used methods. the dc power flow (dc-pf) is a non-iterative alternative to the above problem [cit] . although the dc power flow provides only an approximate solution to the problem, it still remains very popular, especially in the frame of location marginal pricing (lmp)-base market applications [cit] . in its classical formulation, after a sequence of approximations it results in a linear system linking active power injections to bus voltage angles."
"a problem with the dc-pf is that it does not give any information on bus voltage magnitudes. attempts have been made to tackle the problem before [cit] . this work deals with the same problem. it provides a rigorous reference on the problem and proposes a set of different solutions. in brief, an estimate of the bus angles is obtained using a classical dc-pf formulation, and then an approximation of the voltage magnitude profile is made, using power balance constraints. this paper is organized as follows. section ii introduces the reader to the classical dc-pf formulation. section iii extends the latter with bus voltage magnitude estimation. in section iv, results are presented concerning the accuracy and the computational requirements of the proposed method. finally, conclusions are drawn in section v, and future potential research opportunities are highlighted."
"to sum up, qq is the version of the algorithm that is qualified from this work. it will be this version that will be more carefully examined in the rest of this paper. for comparison purposes, results of the li version will also be quoted. fig. 1 and 2 present the average and the maximum bus voltage estimate error respectively. results are given against system size for the minimum achieved error over all versions, for the suggested lsq based version -li, and for the suggested linear system based version -qq. as it can be seen from the figures the quality of the estimate of the algorithms remains quite unaffected by system size."
"system (3) is nonlinear. the nonlinearity can be weighed in two ways. they are termed quad-and inv-approximation for the rest of this paper, and they are defined hereunder. 1) quad-approximation: expanding (2)"
all versions of table ii are applied on sample power systems contained in the matpower package [cit] . a precise newton-raphson (nr) power flow is first run in order to obtain a trusted solution of the steady state.
"if only the modified active power set of equations of (9) is retained, then the resulting linear system is determined, i.e. it has equal number of equations and variables. subsequently, a unique solution can be attained."
"given the higher degree of coupling between v and q in power systems, the q-formulation is expected to provide results that are somewhat more precise, compared to the pformulation. this has been also experimentally observed."
"തതതതത . x least-squares versions (lx & wx) greatly perform better than linear system versions (px & qx) in either average or inf-norm metrics. this can be explained by the fact that they have more information at their disposal that the latter. however, this comes at a great computational cost, as it will be shown later in timing results. x computationally demanding be it, the li version generally performs best. x for linear system versions: qq performs better for smaller scale systems, while qi performs better for larger scale systems. aside from precision considerations, linear-system based versions are found to have significant computational benefits."
"the algorithm presented in the previous section has been implemented in matlab, for all approximations and formulations. in this section, all versions will be compared for precision on systems of different size. additionally, a time profiling will be performed to assess their speed."
"in this paper an extension to the dc power flow method was presented. in addition to the bus voltage angles of the standard dcpf version, bus voltage magnitudes estimates are provided. for this, the mathematical formulation of the problem was stated, and 8 different ways to tackle it have been proposed. precision and timing characterization of the methods in question completed the study. the qq version of the algorithm was qualified as \"best\", by an empirical study on a plethora of results. a very promising future challenge is to come up with an automated way of selecting the best version of the algorithm for the power system in question."
"for the reference of the reader, the formulas for the real and the imaginary part of the modified admittance and the modified apparent power are as follows"
"different formulations of table ii will be referred to using 2-letter subscripts, one for the formulation (p, q, l for lsq, and w for wlsq) and a second for the approximation (q for quad, i for inv); the character x is used as a wildcard."
"drawing on ollerenshaw and creswell's descriptive restorying methods [cit], a narrative describing each participant's activities was composed [cit] . first-and second-generation activity theories were used to help focus restorying (e.g., recording what tools nursing students used, who they communicated with, verbalized goals). additionally, except for ahs, uhms, and pauses, each video was transcribed verbatim, including the social utterances of all participants (e.g., student participants, standardized patients, and other simulated participants). lastly, each narrative and transcription was re-reviewed for accuracy. this first stage served to disaggregate the rich and dense activities of the scenario. fig. 1 activity system diagram. the figure illustrates the mediated relationship between subjects and tools and the interrelationships among role assignment and rules and conventions of participation [cit] following restorying, open coding procedures were used to categorize student participants' activities. this entailed reading and coding each restoryed narrative and transcript to identify the tools, roles, and goals of each participant [cit] . the instructional design documents for each scenario were also read to search for additional tools, roles, and scenario goals."
"coupled with the registration [cit] ) and template building ) already included in the ants, atropos is a versatile and powerful software tool which touches multiple aspects 1 atropos is one of the three fates from greek mythology characterized by her dreaded shears used to decide the destiny of each mortal. also, consistent with the entomological motif of our ants, acherontia atropos is a species of large moth known for the skull-like pattern visible on its thorax. of our brain processing pipeline. we use atropos to address brain extraction, grey matter/white matter/cerebrospinal fluid segmentation, label fusion/propagation and cortical parcellation. we also allow atropos to interact with the recently developed n4 bias correction software [cit] ) in an adaptive manner. [cit] no stand-alone em methods are currently listed. we also evaluate atropos performance on two brain mri segmentation objectives. first, threetissue classification. second, we test our ability to parcellate the brain into 69 neuroanatomical regions to illustrate the practical value of the low-memory implementation within this paper. although atropos may be applied to multivariate data from arbitrary modalities, we limit our evaluation to tissue classification in t1 neuroimaging in part due to the abundance of \"goldstandard\" data for this modality. consistent with our advocacy of open science (not to mention the facilitation of analysis due to accessibility) we also only use publicly available data sets. for this reason, all results in this paper are reproducible with the caveat that users may require some guidance from the authors or other users in the community."
"role assignment influenced student participants' clinically relevant activities (i.e., use of physical clinical tools and artifacts, social interactions, structured interventions); student participants sequenced and integrated clinically relevant activities, namely the use of physical clinical tools and artifacts, social interactions, and structured interventions, to make sense of the clinical presentation student participants coordinated and distributed activity among peers assigned other roles."
"the complete script for the single-subject brainweb study is based on generalizing atropos bwebrf40figureexample.sh, which is available in the ants toolkit (svn release 711 or greater) and which reproduces fig. 2 results. the templatebased normalization procedure for the brainweb 20 and the hammers evaluation data is based on freely available scripts included with ants, ants_ multitemplate_labeling.sh and buildtemplateparallel.sh. a release version of antswith a final version of atropos-will be prepared with the final version of this manuscript."
"our goal is to find the layout with the lowest energy, so in the simulations we also keep the lowest energy e min and the corresponding layout x min each time we find a new lower-energy layout."
"among all four scenarios, regardless of scenario type, all three clinically relevant activities (i.e., physical clinical diagnostic questioning interactions in which the student participant sought specific information from the patient, and/or their support person(s) to formulate a diagnosis, or assess the impact of a therapy \"how is your pain right now?\"; \"when was the last time you went to the bathroom?\""
"7 segmentation is then repeated, with the same parameters, but with the n4-corrected image as input. the resulting algorithm is similar to those that fix segmentation parameters while estimating bias and fix bias while estimating segmentation parameters. thus, with this simple evaluation, we are able to compare the impact of bias on the combination of n4 and atropos and also the validity of our prior label image initialization. results of these evaluation scenarios, in terms of dice overlap, are shown in fig. 3 . because overlap ratios with n4 bias correc-tion approximate those of the zero bias data, we may conclude that simple n4 pre-processing is adequate to correct even the 40% rf bias level. an example of this procedure, using brainweb data with 40% rf bias, is in fig. 4 . we supply the information necessary to repeat the results in this figure in the script entitled 'atroposbwebrf40figureexample.sh' which is available in the ants atropos documentation folder as of svn commit 711. the script may be easily modified to run the whole evaluation. figure 4 shows the results of simultaneously using proton density and t1-weighted brainweb data to perform the segmentation. this multivariate input data outperforms the univariate t1-weighted data alone."
"situational management interactions including statements where student participants sought to manage or direct patient care actions, such as seeking help or giving direction to peers assigned other roles. these interactions were directed towards peers or other healthcare professional roles."
"if the gaussian likelihood model is chosen, the list sample of intensity values and corresponding weights comprised of the posterior probabilities are used to estimate the gaussian model parameters, i.e. the mean and variance. for the non-parametric model, the list sample and posteriors are used in a parzen windowing scheme on a weighted histogram to estimate the observational model [cit] )."
the algorithm terminates if it reaches the maximum number of iterations or produces a change less than the minimum threshold change in the posterior. e.g.
"initialization in all cases, the user defines the number of classes to segment. the simplest initialization is by the classic k-means or otsu thresholding algorithms with only the number of classes specified by the user. otherwise, the user must provide prior information for each class in the form of either a single n-ary prior label image or a series of prior probability images, one for each class. the initialization also provides starter parameters."
"following each stage of analysis, the findings were presented to an interpretive community for feedback and guidance. the interpretive community was comprised of two individuals with expertise in sbl in nursing education and two educational psychologists with expertise in studying complex learning environments. the community provided guidance related to categorizing participant activity and made recommendations about data analysis efforts."
"by undertaking an in-depth analysis of participant activity, the findings also highlight the complex and emergent properties of these scbs activity systems. this was exemplified in the activity analysis in which the components (e.g., subject, tools, objects) of the scbs activity system were not isolated from each other but were dynamic and continuously interacting with each other (see tables 6 and 7, fig. 2 for example) [cit] ."
"following discovery, all videos and accompanying instructional design documents were reviewed to assess quality [cit] . of 34 videos, 19 were eliminated because the videos were incomplete, had poor sound quality, or could not be transformed for video analysis. the 15 remaining videos were categorized by scenario name, scenario emphasis, nursing student participant roles, patient and clinical roles, and length. following this step, four high-quality videos were selected for analysis, making sure to include videos capturing common motivations for using scbss, namely a non-emergent patient assessment, an urgent patient presentation, a team-based scenario, and communication skills. table 1 summarizes the videos selected for in-depth analysis."
"(2) judge whether this point falls into a certain object through computing the distance between this point and the centroid of the object. if this point does not fall into any object, save it as a vacant point and go to (3); otherwise, go to (1)."
"and where x is any particular labeling configuration on x (in other words, any labeling permutation on x is a priori possible). the mrf locality condition is then,"
"the t ik is the prior probability value at site i which was mapped, typically by image registration, to the local image from a template data set. the user may also choose mixing proportions equal to"
"these included activities that were governed by a set of predetermined rules or methods. for example, medication administration is guided by a specific series of rules that student participants follow to ensure that they are giving the correct drug, the correct dose, via the correct route, to the correct patient. structured interventions also included activities such as palpation, auscultation, or visualization, during which the student participants used their hands, ears, and eyes in coordination with physical tools and props. for example, the auscultation of lung sounds required that student participants to use their stethoscopes and follow a pre-designated approach to obtain lung sounds. the complexity of structured interventions ranged from less complex to complex, including less complex interventions such as using an automated blood pressure device to assess blood pressure, to more complex, such as conducting a complete postpartum assessment."
"given two circles cir i (p i, r i ) and cir j (p j, r j ), d ij is their overlapping depth (fig. 6) . the detection formula is given by 2 2 ( ) ( ) 0."
"several initialization strategies have been proposed to overcome the characteristic susceptibility of em algorithms to local optima. common low-level initialization steps include uniform probability assignment [cit], otsu thresholding [cit], and k-means clustering [cit] ). [cit] in which a dense spatial distribution of gaussians is used to capture the complex neuroanatomical layout with subsequent processing used to conjoin subsets of such gaussians belonging to the same tissue classes. recently, reseachers have begun to rely on spatial prior probability maps of anatomical structures of interest to encode domain knowledge [cit] b; [cit] . these spatial prior probability maps may also provide an initial segmentation. related technological developments model partial volume effects for increased accuracy in brain segmentation [cit] ."
"following institutional review board (irb) approval, 34 previously video-recorded scenarios were identified. because the identified video recordings were captured during the national council of state boards of nursing (ncsbn) national simulation study, approval from the ncsbn was also sought. the identified videos had been recorded during the third semester of an accelerated second-degree baccalaureate program during a nursing the childbearing family course. all student participants portrayed in the videos had participated in scbs activities at least every other week, so they had the requisite experience that was sought."
"atropos encodes a family of bayesian segmentation techniques that may be configured in an applicationspecific manner. the theory underlying atropos dates back 20+ years and is representative of some of the most innovative work in the field. although we summarize some of the theoretical work in this section, we recommend that the interested reader consult the deep literature in this field for additional perspective and proofs behind the major concepts."
abbreviations at: activity theory; beme: best evidence medical evaluation; dp: deliberate practice; irb: institutional review board; lpp: legitimate peripheral participation; ncsbn: national council of state boards of nursing; sbl: simulation-based learning; sbme: simulation-based medical education; scbs: scenario-based simulation
"brain segmentation methods have relied on user interaction for many years [cit] . atropos is capable of benefitting from user knowledge via an initialization and optimization that depends upon a spatially varying prior label image passed as input. rapid, sparse labeling-with visualization provided by itk-snap (www.itksnap.org)-enables an interaction and execution processing loop that can be critical to solving segmentation problems with challenging clinical data in which automated approaches fail. [cit] and which has spawned many follow-up applications. the atropos prior label image prespecifies the segmentation results at a subset of the spatial domain by fixing the priors and likelihood (and, thus, the posterior) at a subset of i to be 1 for the known label and 0 for each other label at the same site. the user input therefore not only initializes the optimization, but also gives boundary conditions that influence the em solution outside of the known sites. while the graphbased min-cut max-flow solution is globally optimal for two labels, only locally optimal optimizers are available for 3 or more classes. thus, in most practical applications, em is a reasonable and efficient alternative to boykov's solution. furthermore, one may automate the initialization process. we provide this capability to allow the user to implement an interactive editing and segmentation loop. the user may run atropos with sparse manual label guidance, evaluate the results, update the manual labels and repeat until achieving the desired outcome. this processing loop may be performed easily with, e.g., itk-snap."
"where the normalization term, 1/y, is a constant that does not affect the optimization [cit] . given choices for likelihood models and prior probabilities, the bayesian segmentation solution is the labelingx which maximizes the posterior probability, i.e."
"the dominant frameworks for describing how simulations support learning emphasize increasing access to structured practice (i.e., repeated or deliberate practice) and the provision of feedback [cit] . however, our understanding of how simulations support learning (e.g., structured practice, feedback) may be influenced by learning strategies employed in skills-based simulations which prioritize focused practice of a singular skill, while scenario-based simulations present student participants with a holistic problem or situation to analyze. for example, in issenberg and colleagues' best evidence medical education (beme) review, of the 109 included studies, 90 (83%) examined skills-based contexts [cit] . additionally, mcgaghie and colleagues' meta-analysis compared simulation-based medical education (sbme) using deliberate practice (dp) with learning in the clinical setting; of 14 included studies, 12 (86%) addressed learning in skills-based simulation contexts versus scenarios [cit] ."
"in either parametric or non-parametric terms. given its simplicity and good performance, in the parametric case, p k is typically defined as a normal distribution, i.e."
"assumptions about bias correction may be thought of as another prior model. as such, the typical segmentation processing pipeline begins with an intensity normalization/bias correction step using a method such as the recently developed n4 algorithm [cit] . n4 extends the popular nonparametric nonuniform intensity normalization (n3) algorithm [cit] in two principal ways:"
"this category included physical items that formed the simulated system that student participants were expected to interact with or use to achieve their goals. examples of physical clinical tools included a blood pressure cuff, stethoscope, thermometer, or personal protective equipment (e.g., alcohol gel, gloves). clinical artifacts included diagnostic findings, such as vital sign data, physical assessment findings (e.g., fundal height, fetal waveforms, urine output), and lab results. clinical artifacts were either provided to the participant as a component of the patient's medical record or were derived when the participant engaged in an activity to determine the result. for example, the primary nurse in the uncomplicated postpartum assessment scenario was required to perform the assessment and determine fundal height."
"the activity analysis also highlights the nested activities within the scbs activity system [cit] . barab and colleagues define nested activities as those activities or actions that could be conceived of as separate activity systems. for example, although the faculty-selected goals of these scenarios were explicit, the students often voiced their own goals during the scenario (see, fig. 2, for example). this nestedness was especially highlighted in the team-based scenario in which each participant's activities differed, such as one participant's practice of external uterine massage while another participant prepared medication, yet they all worked towards the common goal of resolving the postpartum hemorrhage. these complex, emergent, and nested properties could have significant implications for formative and summative assessment of student participants."
"temporarily place the centroid of the chosen object at every vacant point. here, if the chosen object is a rectangle, it is placed in two ways: one is with its long edge parallel with the x axis, and the other is with its long edge perpendicular to the x axis. then, compute the extrusive elastic potential energy e j that associates to every vacant point and placing pattern. finally, formally place the centroid of the chosen object a j at the vacant point, where the extrusive elastic potential energy of a j is the lowest. with the positions of other objects unchanged, we gain a new layout x′."
"subsequent development included the use of markov random field (mrf) modeling [cit] to regularize the classification results [cit] ) with later work adding heuristics concerning neuroanatomy to prevent over-regularization and the resulting loss of fine structural details [cit], b) . [cit], commonly referred to as fast (fmrib's automated segmentation tool), which is in widespread use given its public availability and good performance. more recently, a uniform distribution of local mrfs within the brain volume and their subsequent integration into a global solution has been proposed obviating the need for an explicit bias correction solution [cit] )."
"where j x′y′, j x′z′, and j y′z′ are the products of inertia of the whole system with respect to coordinate system o′x′y′z′. their calculations are as follows:"
"in addition, learning associated with scenario-based simulations often prioritizes reflection and debriefing to support learning [cit] . for example, according to rudolph and colleagues, reflection helps individuals make sense of their experiences and scrutinize their assumptions and beliefs [cit] . palaganas and associates suggest that student participants can also engage in sensemaking efforts through discussion with faculty and peers when they pause to reflect during a scenario [cit] . furthermore, according to fanning and gaba, participation in a simulated encounter is often considered a normalizing event in which student participants engage in a shared experience (e.g., code team response) that, in turn, enables reflection processes [cit] . although reflection is critical to learning, these perspectives primarily emphasize the role that verbal discourse can play in learning processes, yet studies examining how student participants may learn through participation in a scbs are limited. furthermore, framing learning as an activity that occurs after a simulation reflects traditional cognitive perspectives of learning."
"\"i'm going to take some vitals and check things out. ok?\"; \"i'm feeling that your uterus is hard and its shrunk down under your belly button which is great, that's what we're looking for right now.\"; \"you'll probably get a little more cramping when you're nursing.\""
"a purposive sample of four video-recorded scenario-based simulations representing commonly cited motivations for employing them (e.g., team training, communication skills) was sought [cit] . additional sampling criteria included (a) scbs that were designed to represent the complexity and the social practices of caring for patients and (b) inclusion of student participants who regularly engaged in scbss, which was reasoned would afford analysis of student participants' typical activity, rather than the activity of student participants becoming familiar with engaging in a scbs."
"our prior work, [cit] . overall, the atropos em extension improved these results further. however, in a few regions of the mid-brain, the atropos em segmentation performed significantly worse. this is not surprising, in that atropos em assumes that signal from the likelihood and mrf term is valuable in improving the segmentation. this assumption held for amygdala and lateral ventricles among other areas. however, in pallidum and corpus callosum (the most significant areas with loss of performance), this is not true. we believe the explanation is that the intensity varies within these structures and that a more complex intensity model (or finer parcellation) would be needed here. an alternative solution would be to use boundary conditions for these structures, as in the priorlabelimage atropos initialization option."
"role assignment influenced student participants' combinations of activities. for example, across all four scenarios, student participants assigned the role of primary nurse engaged in more social interactions than student participants assigned as support nurse. this included social interactions with the patient, their support persons, and other healthcare professionals. additionally, in all scenarios where multiple student participants engaged, primary nurses partook in more structured interventions than did student participants assigned as the support nurse. for example, in the uncomplicated postpartum assessment scenario where the goal was for student participants to complete an uncomplicated postpartum assessment, the primary nurse was responsible for conducting the complete assessment, whereas the support nurse's activities were more focused, such as assessing vital signs or evaluating urine output (see table 5 ). furthermore, student participants assigned as the support nurse engaged in more uses of physical tools and artifacts while they completed their structured interventions. additionally, student participants assigned to support nurse roles partook in fewer social interactions with the standardized patient, support persons, or other healthcare professionals. the analysis suggests this difference in activity may have occurred because student participants assigned the role of primary nurse were expected to complete more complex goals, such as conducting a complete uncomplicated postpartum assessment or recognizing that the laboring patient in the post-epidural hypotension scenario needed escalation of care. this added complexity is also reflected in the frequency counts, where in all four scenarios, primary nurses engaged in more activity overall (see table 4 ). additionally, in both the post-epidural hypotension and postpartum hemorrhage scenario, support nurses arrived later, and when they arrived, they were often assigned specific structured interventions, such as administering fluid or fundal massage, by the primary nurse. table 5 provides a summary of the types of clinically relevant activities student participants engaged in in all four scenarios based upon their designated clinical role. italics indicate most frequent activity performed by participant"
"bayes' theorem provides a powerful mechanism for making inductive inferences assuming the availability of quantities defining the relevant conditional proba-bilities, specifically the likelihood and prior probability terms. bayesian paradigms for brain image segmentation employ a user selected observation model defining the likelihood term and one or more prior probability terms. the product of likelihood(s) and prior(s) is proportional to the posterior probability. the likelihood term has been previously defined both parametrically (e.g. a gaussian model) and non-parametrically (e.g. parzen windowing of the sample histogram). the prior term, as given in the literature, has often been formed either as mrf-based and/or template-based. an image segmentation solution in this context is an assignment of one label to each voxel 2 such that the posterior probability is maximized. the next sections introduce notation and provide a formal description of three essential components in bayesian segmentation, viz."
"the findings also raise new questions about how simulations support learning and simulation-based instructional design practices; these findings warrant further investigation. questions may include the following: (a) are these activity codes consistent among diverse health professions disciplines (e.g., physicians, nursing, allied health professionals), areas of specialization (e.g., emergency medicine, anesthesia), and a continuum of participants (e.g., students, practitioners, interprofessional teams); (b) how might activities in skills-based or procedurally based simulations be similar or different; (c) how do student participants' activities change or differ as they develop or gain greater understanding over time; (d) how do standardized and simulated participants' activities contribute to the learning process in a scbs; and (e) how might embodied communication (e.g., gesture, visual gaze) contribute to sense-making, coordination and distribution of activity in a scbs?"
"data the brainweb data is freely available. we used single-subject brainweb data as is but added a metaformat data header to the raw binary files. an example copy of this header is contained in atropos bwebrf40figureexample.sh. the 20-subject data, however, required excluding non-cerebrum tissue classes. the hammers data was also used as is (http://www.brain-development.org/)."
"social and emotional support interactions including statements intended to give reassurance, empathy, or encouragement. these interactions were directed to the patient or their support person(s)."
"in this example, the participant portraying the primary nurse struggled to provide a cohesive accounting of the patient's situation to the support nurse. this interaction, in turn, led the support nurse to quickly assess the situation herself by conversing with the patient and evaluating the patient's vital signs (clinical artifact). although this exchange highlights how a student participant can the student participant portraying the primary nurse appears to thoughtfully sequence her structured interventions and social interactions (including gathering information from the patient) to construct an understanding of the patient's condition and uterine height. she makes her understanding of the situation explicit when we she educates and counsels the patient on her findings table 7 struggling to make sense and coordinate the two student participants portrayed in this scenario struggle to coordinate their care. although the primary nurse appears to have some understanding of the clinical presentation, she struggles to describe the situation to the support nurse when she arrives. faced with this, the support nurse makes a quick assessment of the situation by examining the fetal heart rate tracing and the maternal vital signs. she makes her understanding of the situation explicit when she describes the situation to the patient and then again when she recommends a plan to the primary nurse to roll the patient on her side struggle to engage in ideal sequencing of activities, the student participants in this vignette still made the effort to thoughtfully sequence their care."
"given a circle cir i (p i, r i ), if cir i embeds into the standing column cir, which is considered as a fixed circle, the calculation of the overlapping depth between cir i and cir is the same as in section 4.3."
"given a rectangle rect j (p j, α j, a j, b j ), if rect j embeds into the standing column cir, which is considered as a fixed circle, the calculation of the overlapping depth between rect j and cir is the same as in section 4.1."
"by combining the wl sampling method with heuristic layout update strategies and the ls procedure, we propose a hybrid wl-ls algorithm for the layout optimization of the satellite module. the calculation procedure of the wl-ls algorithm is outlined as follows:"
"interactions in which the student participants tried to, a) prepare the patient for a future action, b) provide the patient or support person(s) with assessment findings, or c) instances where students explicitly provided patients with information about self-care."
"\"hi, can i have nursery come in here?\"; \"i need some help in here.\"; \"i need someone to give ah…do massage and someone to get a straight cath.\" tools and artifacts, social interactions, and structured interventions) were present. additionally, all student participants, regardless of role assignment or scenario goals, engaged in a combination of these activities. table 4 provides a summary of the frequency of these activities among all student participants across all four scenarios."
"researchers in aging often focus on accurately segmenting the t1 mri of elderly controls and subjects suffering from neurodegeneration, for instance, via siena [cit] . a recent evaluation study compared knn segmentation, spm unified segmentation and siena and found different performance characteristics under different evaluation criteria [cit] had similar findings when comparing spm5, fsl and freesurfer. these studies suggest that no single method performs best under every measurement and, along with the no free lunch theorem [cit], highlight the need for segmentation tools that are tunable for different problems and research goals."
"an exploratory descriptive analysis was selected using multiple qualitative methods, namely, narrative, video, and activity system analysis, to support the goal of rich description of the types of activities student participants engaged in that may support learning. the strategy of using multiple qualitative methods was selected to help better account for the complex and dynamic activity usually present in a scbs."
parameter update (m-step) note that the posteriors used in the previous iteration are used to estimate the parameters at the current iteration. we use a common and elementary estimate of the mixing parameters:
the following three features were taken into account when considering how scbs may support learning: the dominant frameworks for describing how scbs support learning emphasize increasing access to structured practice and the provision of feedback; studies examining student participants' experiences during scbs participation suggest learning may occur through participation; and that the direct examination of participant activity is limited. the purpose of this study was to examine the types of activities student participants engaged in and then analyze their patterns of activity to consider how scbs participation may support learning. the aims of this study were (1) to make explicit the types of activities student participants engage in and (2) to examine how student participants' activities during participation in scenario-based simulations may contribute to their learning. the research questions were:
"this study makes explicit the types of activities in which student participants across diverse types of scbss engage, which offers important insight into what student participants practice during such an activity. the findings also suggest that learning within scenarios may take place while student participants work to sequence their activities and make sense of the clinical presentation; in this way, the findings add an alternative perspective to how scbss may support learning. importantly, these findings add new insights into the nuances and complexity associated with scenario participation and offer added detail about how scenarios can be designed and implemented to scaffold student participants' learning."
"this type of update is known as soft em. hard em, in contrast, only uses sites containing label l k to update the parameters for the kth model. a similar pattern is used in non-parametric cases. em will iterate toward a local maximum. we track convergence by summing up the maximum posterior to the right of the icm code image, we focus on a single neighborhood with a center voxel associated with the icm code label of '10'. each center voxel in a specified neighborhood exhibits a unique icm code label which does not appear elsewhere in its neighborhood. when performing the segmentation labeling update for icm, we iterate through the set of icm code labels and, for each code label, we iterate through the image and update only those voxels associated with the current code label probability at each site over the segmentation domain. the e-step, above, depends upon the selected coding strategy [cit] ). atropos may use either a classical, sequential checkerboard update or a synchronous update of the labels, the latter of which is commonly used in practice. synchronous update does not guarantee convergence but we employ it by default due to its intrinsic parallelism and speed. the user may alternatively select checkerboard update if he or she desires theoretical convergence guarantees. however, we have not identified performance differences, relative to ground truth, that convince us of the absolute superiority of one approach over the other."
"1. what types of activities do student participants engage in during participation in scenario-based simulations? a. what is the frequency and regularity of these activities across different scenario types? 2. how do student participants engage in scenario-based simulations (e.g., how do they organize their activities, interact with each other)?"
"during the second stage of analysis, which supported the goal of analyzing how student participants engaged, a second cycle of coding was conducted [cit] . this involved drawing on second-and third-generation at using the categories of activities generated in phase one to create subject-tool-object activity system diagrams (see figs. 1 and 2) for each student participant. this step served to make explicit what tools and artifacts mediated student participants' goals to help describe how they engaged during the scenario. the use of third-generation at supported analysis of scenarios in which multiple student participants were engaged (e.g., uncomplicated postpartum assessment) to support analysis of their interactions and multiple perspectives."
"to date, empirical studies in health professions simulation have frequently focused on determining if sbl supports improvements in participant satisfaction, diverse learning outcomes, and in some cases, outcomes in the clinical setting. for example, several meta-analyses suggest that sbl supports improvements in medical knowledge, psychomotor and communication skills, time to complete a skill, and self-efficacy [cit] ."
"would permit casting the other prior terms inside the definition of u(x) in the form of the external field v i (x i ) but, for clarity purposes, we consider them separately."
"the atropos algorithm is cross-platform and compiles on, at minimum, modern osx, windows and linuxbased operating systems. the user interface may be reached through the operating system's user terminal. because of its portability and low-level efficiency, atropos can easily be called from within other packages, such as matlab or slicer, or, alternatively, integrated at compile time as a library. a typical call to the algorithm, illustrated here with ants example data, is:"
"initial codes were then compared across the four videos using video analysis software (studiocode) to determine their regularity and frequency. accounting for regularity and frequency helped verify initial codes and identify new codes that were not revealed during earlier coding efforts. following this step, initial codes were consolidated into three major categories by grouping similar initial codes together."
"as mentioned previously in the introduction, different groups have opted for different likelihood models which have included either parametric (gaussian) or non-parametric variations. however, these approaches are similar in that they require a list sample of intensity data from the input image(s) and a list of weighting values for each observation of the list sample from which the model is constructed. in general, one may query model probabilities by passing a given pixel's single intensity (for univariate segmentation) or multiple intensities (for multivariate segmentation) to the modeling function, regardless of whether the function is parametric or non-parametric. these similarities permitted the creation of a generic plug-in architecture where classes describing both parametric and nonparametric observational models are all derived from an abstract list sample function class. three likelihood classes have been developed, one parametric and two non-parametric, and are available for usage although one of the non-parametric classes is still in experimental development. the plug-in architecture even permits mixing likelihood models with different classes during the same run for a hybrid parametric/non-parametric model although this possibility has yet to be fully explored."
"gm is also known as the steepest descent method. the search direction is the negative gradient direction. we adopt an adaptive step size in this study. if the energy increases after one iteration, which indicates that the step size for this step is too large, we decrease h to 0.8h. if the energy decreases, we keep h. the procedures of the gm with an adaptive step size are described as follows:"
"yamagata-lynch defines activities as \"mediational processes in which individuals and groups of individuals participate, driven by their goals and motives, which may lead them to use new artifacts or cultural tools\" (p. 17) [cit] . participants can include an individual or a group of people engaged in an activity [cit] . their goals or objectives are the physical or mental product(s) they seek during participation (e.g., postpartum assessment) [cit] . tools include culturally specific artifacts that participants use to achieve their goals [cit] . for example, when a participant assesses a patient's vital signs, they may use a manual or automated blood pressure device to achieve this goal. rules are the guidelines, conventions, or protocols that govern participants' activity in the system, either explicitly or implicitly, such as patient care guidelines or commonly accepted assessment processes [cit] . roles are the division of labor among actors in the system [cit] . figure 1 provides a generic at diagram which is often used to heuristically represent at concepts."
"is the extrusive elastic potential energy of the jth object exerted by other objects, and s j is the area of the jth object. strategy 2 randomly generate 100 vacant points in each surface, where vacant points are the points that are inside this surface but not inside any object. the procedures are as follows:"
"an efficient layout update strategy is also impactful in the wl sampling simulations. according to the characteristics of the layout optimization of the satellite module, we propose the following heuristic layout update strategies: strategy 1 in each surface of current layout x, we pick out an object a j that has the largest relative extrusive elastic potential energy e j /s j to relocate, where"
"to provide a more intuitive interface without the overhead costs of a graphical user interface, a set of unique command line parsing classes were developed which can also provide insight to the functionality of atropos. the short version of the command line help menu is given in listing 1 which is invoked by typing 'atropos -h' at the command prompt. both short and long option flags are available and each option has its own set of possible values and parameters intro- duced in a more formal way in both the previous discussion and related papers cited in the introduction. here we describe these options from the unique perspective of implementation."
"higher dimensions than 4 are possible although we have not yet encountered such an application-specific need. multiple images (assumed to be of the same dimension, size, origin, etc.), will automatically enable multivariate likelihoods. in that case, the first image specified on the command line is used to initialize the random, otsu, or k-means labeling with the latter initialization refined by incorporating the additional intensity images, i.e. an initial univariate k-means clustering is determined from the first intensity image which,"
"furthermore, the findings are also consistent with prior guidance associated with selecting scenarios to give student participants opportunities to practice diverse communication skills [cit], breaking bad news, [cit] or supporting clinical situations related to death and dying [cit] . importantly, the activity systems analysis highlights how scbs affords student participants access to selecting and sequencing social interactions with clinically relevant activities (see, for example, tables 6 and 7 ). these findings may provide insight into how partaking in scbs might support learning clinical reasoning or engage in diagnostic decision-making."
"these components are common across most em segmentation algorithms. we use x to denote a specific set of labels in i and a valid, though not necessarily optimal, solution to the segmentation problem."
"similar to its predecessors, atropos employs the em framework [cit] ) to find maximum likelihood solutions to this problem. the following sections detail the atropos em along with choices for the likelihood and prior terms."
"to support this study's goal of in-depth analysis of participant activity, this study was informed by engeström's first-, second-, and third-generation perspectives on activity theory (at). at is often used by researchers as a descriptive tool to map the interactions between individuals and their environment [cit] . systematically mapping activity can help researchers examine meaningmaking processes as embedded in dynamic emergent systems [cit] . according to engeström, first-generation at emphasizes vygotsky's mediated action triangle that highlights how individuals use artifacts or tools to achieve their goals [cit] . engeström's second-generation at, often referred to as activity systems analysis, advances first-generation perspectives because it helps elucidate the collective nature of human activity by accounting for the roles individuals play and the rules that may guide them [cit] . this added emphasis on roles and rules is intended to highlight the complex interactions within an activity system [cit] . third-generation at supports analysis of multiple points of view to highlight interactions between activity systems [cit] . the use of third-generation at requires the analysis of at least two activity systems, though multiple activity systems may be available for examination [cit] ."
"label update (e-step) given the initialization and fixed model parameters, atropos is capable of updating the current label estimates using either a synchronous or asynchronous scheme. the former is characterized by iterating through the image and determining which label maximizes the posterior probability without updating any labels until all voxels in the mask have been visited at which point all the voxel labels are updated simultaneously (hence the descriptor \"synchronous\"). this option is specified with --icm [cit] . however, unlike asynchronous schemes characteristic of icm, synchronous updates lack convergence guarantees. to determine the labeling which maximizes the posterior probability for the asynchronous option, an \"icm code\" image is created once for all iterations by iterating through the image and assigning an icm code label to each voxel in the mask such that each 4 due to the lack of parameters in the non-parametric approach, it is not technically an em algorithm [cit] ). however, the same iterative maximization is applicable and is quite robust in practice as evidenced by the number of researchers employing non-parametric models (see the introduction). 5 consider n sites each with a possible k labels for a total of n k possible labeling configurations. for large k 3, exact optimization is even more intractable than for the traditional 3-tissue scenario."
"organization of this work is as follows: we first describe the theory behind the various components of atropos while acknowledging that more theoretical discussion is available elsewhere. this is followed by a thorough discussion of implementation which, though often overlooked, is of immense practical utility. we then report results on the brainweb and hammers dataset. finally, we provide a discussion of our results and our open source contribution in the context of the remainder of this paper and of previous and future work."
"consistent with our previous discussion, we offer both an mrf-based prior probability for modeling spatial coherence and the possibility of specifying a set of prior probability maps or a prior label map with the latter extendable to creating a dense labeling. to invoke the mrf '-m/-mrf' option, one specifies the smoothing factor (or the granularity parameter, β, given in eq. 9, and the radius (in voxels) of the neighborhood system using the vector notation '1x1x1' for a neighborhood radius of 1 in all 3 dimensions. this radius is defined such that voxels including but not limited to those that are face-connected will influence the mrf."
"considering this scaffolding approach is consistent with lave and wenger's theories regarding legitimate peripheral participation (lpp). lpp indicates that newcomers, such as novice student participants in a scenario, can gain greater experience in a community of practice when they have access to opportunities to engage in simple or lower risk tasks that are nonetheless important to the community's goals [cit] . furthermore, per lave and wenger, participants benefit from both direct participation in a meaningful activity, while also benefiting from modeling provided by their more capable peers [cit] . additionally, scaffolding using such an approach is consistent with recent best practice guidance issued by the ncsbn, which indicate that simulation objectives should be aligned with student participants' developmental level [cit] ."
"the frequency and regularity analyses may offer new insights into ways to consider scaffolding student participants' experiences by thoughtfully considering the complexity of each role portrayed in the scenario. for example, the analysis suggests that student participants assigned to roles with greater levels of responsibility (i.e., primary nurse) conducted more complex care, relied on more subjective artifacts (e.g., palpation of the fundus), and were required to determine when and what support persons were assigned to do. thus, simulation stakeholders could consider assigning student participants with greater amounts of training or ability to more complex roles. conversely, less experienced student participants could be considered for support roles that may entail less complexity. this may enable learners of diverse abilities to simultaneously partake in a scbs."
"atropos uses expectation maximization to find a locally optimal solution for the user selected version of the bayesian segmentation problem (cf. eq. 1). after initial estimation of the likelihood model parameters, em iterates between calculation of the missing optimal labelŝ x and subsequent re-estimation of the model parameters by maximizing the expectation of the complete data log-likelihood (cf. eq. 5). [cit] which yields the optimal mean and variance (or covariance), but sets the mixing parameter γ k as a constant. the atropos implementation estimates γ [cit] . 4 when spatial coherence constraints are included as an mrf prior in atropos, the optimal segmentation solution becomes intractable."
"by contrast, some studies examining student participants' experiences in simulation suggest they may learn during scenario participation. for example, kneebone and colleagues assigned student participants to one of two procedural scenarios: insertion of a urinary catheter or wound closure using a hybrid simulation strategy [cit] . student participants reported that in addition to learning the designated skill, they also became aware of the importance of maintaining patient privacy, learning where to find supplies and materials, and interacting with the standardized patient [cit] ."
"specifically, with respect to segmentation, there exists a third advantage with n4 over n3 in that the former permits the specification of a probabilistic mask as opposed to a binary mask. recent demonstrations suggest improved white matter segmentation produces better gain field estimates using n3 [cit] . thus, when performing 3-tissue segmentation, we may opt to use, for instance, the posterior probability map of white matter at the current iteration as a weighted mask for input to n4. this is done by setting the '--weight-image' option on the n4 command line call (see listing 2) to the posterior probability image corresponding to the white matter produced as output in the atropos call, i.e. 'atropos --output'. n4 was recently added to the insight toolkit repository 6 where it is built and tested on multiple platforms nightly. the evaluation section will illustrate inclusion of atropos, n4 and ants in a brain processing pipeline."
"student participants achieved their goals by making sense of the clinical presentations by coordinating their efforts with their peers assigned supportive clinical roles, the standardized patient and simulated participants portraying other roles (e.g., charge nurse). coordination of activity with others was present in all four scenarios and included efforts to perform structured interventions and meet the faculty-intended goals of the scenarios. for instance, the previous example in table 7 demonstrates how the support nurse's interactions buttressed and guided the primary nurse's activity. additionally, in the fetal demise scenario, although the participant engaged in the scenario without the co-presence of another nursing student or healthcare professional, she coordinated her activity with the patient and her support person. this highlights the critical role that all participants in a scbs (e.g., student participants, standardized patient(s), and simulated participants) can play. in addition to coordinating efforts, student participants also distributed the workload of care across multiple team members. for example, in the uncomplicated postpartum scenario, the primary nurse and secondary nurse coordinated their activities to complete the goal of an uncomplicated postpartum assessment (see table 5 ). together, they achieved the appropriate provision of care. figure 2 highlights how three student participants collaborated and distributed care to support a patient experiencing a postpartum hemorrhage."
"suppose that all n objects, the satellite module, and its fixed components are smooth elastic solids. since no object transfers between any two different surfaces are allowed, once one object is distributed onto a certain surface, it will never overlap another object on a different surface. so, we need to calculate simply the overlapping depth between each object and the fixed vessel, and the overlapping depth between any two different objects which are distributed onto the same surface. using the quasi-physical strategy, the extrusive elastic potential energy of the whole system is as follows:"
"various approaches have been proposed to compute the overlapping depth or the interference between objects, for example, the no-fit polygon [cit], the octree method [cit], and the projection-separation approach [cit] . in this study, we do not adhere to a single approach. instead, according to the graphical characteristics of the two objects involved, we adopt different approaches that are most efficient. for example, if the two objects are rectangles, we adopt the projection approach; if the two objects are circles, we compare the distance between their centroids with the sum of the two radii; if the two objects are a rectangle and a circle, we adopt an approach based on a no-fit polygon. in the related previous work, discussions of approaches are quite general. here, we give the specific calculation of overlapping depth which is suitable for orthogonally placed rectangles and circles."
"data analysis was conducted in multiple stages. stage one focused on identifying the types of activities in which student participants engaged in and verifying their frequency and regularity across all four videos. stage two focused on re-aggregating the activity system to support analysis of the interactions between student participants and the scbs context, which included the physical environment, standardized patients, and other simulated participants, for example [cit] ."
"efforts to sequence clinically relevant activity included instances in which student participants were successful (e.g., table 6 ) as well as instances in which they struggled. table 7 provides a selected sequence of activities conducted by the primary and support nurses during the post-epidural hypotension scenario. in this scenario, the primary and support nurses initially struggle to coordinate their efforts to provide care when the primary nurse relays limited information about the patient's situation. prior to the sequence in table 7, the primary nurse had started her intrapartum assessment, determined that the patient was experiencing hypotension, lowered the head of the bed, called for help, and partially rolled the patient on her side."
"when optimizing the layout design of a satellite module, it is easy for an algorithm to get trapped in local minima separated by high-energy barriers. to address this problem, we use a hybrid wl-ls method, which incorporates the ls procedure based on gradient descent and heuristic layout update strategies into the wang-landau sampling method. to improve the efficiency of wl-ls, we adopt an accurate and fast method for computing the overlapping depth between two objects (such as two rectangular objects, two circular objects, or a rectangular object and a circular object) embedding each other. numerical results show that wl-ls outperforms methods in the literature. there are also several problems that need to be solved in the future: (1) in this study, the rectangles are placed orthogonally, which restricts the flexibility of the layout design. we need to find the methods for computing the overlapping depth with respect to arbitrarily placed rectangles. (2) the distribution of objects onto different surfaces is not discussed in this paper, and we simply adopt the allocation scheme proposed in the literature. in the future, we will focus research on these aspects."
"although this study provides an in-depth examination of the types of activities student participants engaged in during high-quality scbs participation, the strategy of rich description required the use of a limited number of video-recorded scenarios. additionally, these videos depicted senior nursing student participants who could function independently with limited or no support from faculty, thus the analysis may only reveal the types of activities in which more experienced student participants engage. future research should include analysis of diverse levels of learners (e.g., novice, intermediate) and diverse types of learners (e.g., physicians, respiratory therapists) who partake in sbl activities. furthermore, the choice to use diverse types of scenarios did not allow for analysis of how consistent the patterns of activity were for a single scenario type (e.g., communication). future research should take consistency into consideration. lastly, although this analysis provides a framework that can be used to describe how observing student participants' activities may support learning, this analysis did not include participants' reflections on their activity. future efforts could include the use of stimulated video-recall, which could be used to triangulate student participants' intended goals."
"importantly, the findings suggest that these scenarios afforded student participants the opportunity to reflexively sequence clinically relevant activities, which includes their practice of aggregating patient data and diagnostic artifacts, to make sense of the patient's clinical presentation and make decisions related to treatment and care. students' activities also included their reflexive interactions with other student participants, standardized patients, and other simulated participants (e.g., charge nurse), further highlighting the complex activity systems created when working with scbss. moreover, when a scenario required more than a single student participant to provide care, they coordinated their activities with each other and distributed the workload to achieve their goals. thus, in addition to the individual performance of clinically relevant activities, engagement with standardized patients and other simulated participants and peers was an integral component of sense-making efforts during scenario participation."
"however, although both skills-and scenario-based simulations are commonly employed, scenario-based simulations (scbs) may have unique properties in design and implementation that set them apart which, in turn, may influence learning processes. for example, skills-based simulations primarily emphasize the focused teaching and practice of a specific procedural skill [cit] . by comparison, scbs are often employed when the desired learning outcomes include working in a team-based context, practicing communication skills, or responding to a crisis or critical patient event [cit] . additionally, whereas skills-based simulations often seek to minimize complexity, scbs are employed to incorporate the complexities associated with clinical practice, including engaging socially with the patient or support persons (e.g., simulator, standardized patient) and interacting with other healthcare professionals [cit] ."
"these findings are consistent with prior research emphasizing the role that simulations play in affording student participants access to practice diverse, clinically related skills [cit] . the findings are also consistent with prior guidance indicating that scenarios support learning to interact with conscious patients and other team members [cit] . importantly, by focusing on participant activity, these findings make explicit some of the major categories of activities student participants may gain access to during scbs participation, specifically the use of physical tools and artifacts, diverse social interactions, and structured interventions."
"we introduced atropos, the theory and implementation details and documented its performance in a variety of use cases. we also showed evidence that the openly available n4 bias correction can easily be used with atropos to improve segmentation. furthermore, we used multiple subject brainweb data to build dataset-specific priors that provided the most consistent segmentation performance across tissues. finally, we used majority voting to initialize an atropos em solution to a 69-class brain parcellation problem. significant improvements were gained in multiple brain regions, in particular in temporal lobe cortex, the hippocampi and amygdalae and the lateral ventricles. this work, in summary, proves the applicability of atropos in both basic and extended use cases."
"student participants' observed performance of clinically relevant activities did not appear to be random; instead, they sought to thoughtfully sequence their activities to make sense of the clinical situation the scenario presented. student participants sought to achieve the complex activity of sequencing by integrating clinically relevant activities (i.e., use of physical clinical tools and artifacts, social interactions, structured interventions). this included interactions with peers, the standardized patient, and simulated participants portraying the patient's support person or other health care professional roles. for example, table 6 presents a sequence of the activities and social interactions in which the participant portraying the role of primary nurse engaged while conducting a portion of the uncomplicated postpartum assessment. in this example, the primary nurse sequenced her activities by first educating the patient about her assessment plans, performing the planned structured intervention of auscultating the abdomen, followed by palpating for the height of the fundus. additionally, together with her physical exam findings, the primary nurse further made sense of the patient's clinical presentation by engaging in dialog with the patient by asking additional diagnostic questions. this example highlights how the use of physical tools and artifacts and social interactions are critical to the sensemaking process. additionally, she made her understanding of the findings explicit when she verbalized her documentation plans and when she educated and counseled the patient about her findings. student participants in all four scenarios demonstrated similar patterns of activity."
"atropos has a number of parameters defined within listing 2 and will function on 2, 3 or 4 dimensional data. however, the majority of the time, users will be concerned with a smaller set of input parameters. here, we list the recommended input and an example definition for each parameter:"
"since c is symmetrical about the x and y axes, the equations for the other parts of c can be deduced easily. to calculate d ij, the region enclosed by c is divided into nine sub-regions (fig. 4) . if p i falls into s 2, s 4, s 6, or s 8, d ij equals the distance from p i to tl, ll, rl, or bl, respectively. for example, in fig. 4 (p j, α j, a j, b j ), d ij denotes the overlapping depth between rect i and rect j . l ix and l iy are the lengths of the edges of rect i parallel with the x axis and the y axis, respectively, and l jx and l jy are the lengths of the edges of rect j parallel with the x axis and y axis, respectively (fig. 5) . to examine whether two rectangles overlap, a straightforward method is to project rectangles rect i and rect j onto the x and y axes, and then to check whether the projecting lines overlap. the detection formulae are shown as follows:"
"furthermore, lasater [cit] examined student participants' experiences of learning clinical judgment using scbss. the findings suggested that student participants learned during the scbs when the simulator physiologically responded to their actions, thus allowing them to understand the consequences of their actions (e.g., cardiopulmonary depressions following administration of a narcotic agent) [cit] ."
"as with other classes that comprise ants, atropos uses the insight toolkit as a developmental foundation. this allows us to take advantage of the mature portions of itk (e.g. image io) and ensures the integrity of the ancillary processes such as those facilitated by the underlying statistical framework. although atropos is publicly distributed with the rest of the ants package, we plan to contribute its core elements to the insight toolkit where it can be vetted and improved by other interested researchers."
"central to all three generations of at is the inseparability of learning and doing. according to at, learning is conceptualized as practice [cit] . this contrasts with traditional learning theories that position learning as occurring prior to or after an individual's performance in an activity. importantly, when discussing activity, activity theorists are not just concerned with what individuals do, but are also interested in doing as a transformative process [cit] . for example, nursing students participating in a scbs often must assess a patient's vital signs. in assessing vital signs, students may perform specific activities, such as palpating a pulse or using a blood pressure device. the practice of assessing vital signs is governed by specific rules, such as how and where to place the blood pressure cuff or where to locate the pulse. in performing these activities, nursing students gain new knowledge about the patient that they must then integrate into their care plans. they must consider the information gained, decide what it may mean (e.g., hemodynamically stable or unstable), and, in turn, determine what they should do next."
"n4biasfieldcorrection listing 2 n4 short command line menu which is invoked using the '-h' option. the expanded menu, which provides details regarding the possible parameters and usage options, is elicited using the '--help' option along with the other images, provides the starting multivariate cluster centers for a follow-up multivariate k-means labeling. more details on each of the key implementation options are given below."
"the frequency and regularity analysis shows that these activities were consistent among all participant roles and across all four scenarios. the activity and frequency analysis also suggests that role assignment can influence the complexity of student participants' experiences. for example, table 5 demonstrates how student participants assigned the role of primary nurse conducted more complex tasks or had greater levels of responsibility as compared to support nurses."
"a more general advantage which extends beyond the scope of the experimental evaluation section of this paper is the flexibility of atropos. this includes not only n-tissue segmentation and dense volumetric cortical parcellation, as reported in this work, but atropos is also used in conjunction with our ants registration tools for robust brain extraction which has reported good performance in comparison with other popular, publicly available brain extraction tools )."
"the layout design of a satellite module involves placing a certain number of objects, including various instruments and devices, in a particular satellite module, while satisfying various constraints with specific objectives. stability and service life are essential requirements of a successful layout design, and other performances of the whole satellite module system should also be considered."
"object with respect to coordinate system o″x″y″z″, and m i is the mass of the ith object. θ x′ (x), θ y′ (x), and θ z′ (x) are calculated as follows:"
"the atropos software is freely available to the public. we release this code not only to make it available to clinical researchers but with the hope that other researchers in segmentation will provide feedback about the implementation decisions that we made. em segmentation is non-trivial and there are numerous design alternatives available not only in the models selected but also in the icm coding, alternatives to icm and the method in which prior and likelihood are combined. due to the flexibility of atropos, we also hope that some of its capabilities, though not evaluated here, are explored by the segmentation or clinical community."
"three clinically relevant categories were identified, including (a) use of physical clinical tools and artifacts, (b) social interactions, and (c) performance of structured interventions. table 2 summarizes these three categories of activities, their operational definitions, and examples from the data."
"in this case, atropos will output the segmentation image, the per-class probability images and a listing of the parameters used to set up the algorithm. a useful feature is that one may re-initialize the atropos em via the -i priorprobabilityimages [...] option. this feature allows one to compute an initial segmentation via k-means, alter the output probabilities by externally computed functions (e.g. gaussian smoothing, image similarity or edge maps) and reestimate the segmentation with the modified priors. finally, the functionality that is available to parametric models is equally available to the non-parametric models enabled by atropos."
"this category included exchanges that student participants have with others in the simulated context, such as peers, standardized patients, and other simulated participants noelle embodied the patient while a standardized patient portrayed her voice to support verbal social interactions (e.g., patient, patient's support person, and anesthesiologist). social interactions are also considered tools because, according to at, the use of language, often called a sign, is also deemed a tool that individuals may use to transform their environment. social interactions were categorized into four sub-categories, including diagnostic questioning, education and counseling, social and emotional support, and situational management. table 3 summarizes the categories of social interactions identified in all four scbs, key characteristics, and examples from the scenarios."
"where v ij is typically defined in terms of the kronecker delta, δ ij, based on the classical ising potential (also known as a potts model) [cit] )"
"the findings derived from analyzing participant activity demonstrate the multiple ways in which scbss can support learning: engagement in clinically relevant activities (i.e., use of tools, social interactions, and structured interventions), sequencing clinically relevant activities and social interactions to make sense of the clinical presentation, and coordination and distribution of the workload."
to each of the k labels corresponds a single probabilistic model describing the variation of f over i. we denote this set of k likelihood models as
"to test the computational performance of the wl-ls algorithm, we applied it in two instances. both instances are based on the international commercial communication satellite module intelsat-iii with different technological parameters, one with 51 objects and the other with 53 objects. we implemented the wl-ls algorithm in the java programming language and ran it on a pc with 1.5 ghz cpu and 2.0 gb ram. for each instance, the wl-ls algorithm was run 50 times independently to optimize the layout of objects on the four bearing plate surfaces. according to these 50 results, we can gain a pareto optimal set for each instance. in this study, we specify the solution with the smallest enveloping radius as the optimal solution for preference."
"input images to be segmented if more than one input image is passed, then a multivariate model will be instantiated. e.g. -a image.nii.gz for one image and -a image1.nii.gz -a image2.nii.gz for multiple images."
"the activity diagrams in fig. 2 highlight how student participants distributed the workload across three nursing students to obtain the shared goal of stopping the postpartum hemorrhage in a hemodynamically unstable fig. 2 coordinating and distribution of activities of postpartum hemorrhage. this diagram demonstrates how three student participants distributed their activities and goals to achieve the larger goal of treatment of the patient's postpartum hemorrhage and hypotension. goals (objects) reflect participants' utterances during the scenario patient. although all student participants worked towards the same goal, each undertook a specific goal (e.g., catheterizing the patient, administering fluids)."
"with z a normalization factor known as the partition function and u(x) the energy function which can take several forms [cit] . in atropos, as is the case with many other segmentation algorithms of the same family, we choose u(x) such that it is only composed of a sum over pairwise interactions between neighboring sites across the image, 3 i.e."
"when a new layout x′ is obtained by heuristic layout update strategies, it may be very close to the global optimal layout, so any random layout update in the wl sampling algorithm may make search far from (and even farther and farther from) this global optimal layout. to avoid this, we adopt the local search (ls) method based on the gradient method (gm), which is a quasi-physical algorithm [cit], to search for an optimal layout near x′."
"scbs are also described as possessing characteristics associated with sociocultural and situated learning practices. for example, in a scbs, a narrative is employed to guide student participants' engagement in which learning activities are designed around a story or a problem that needs to be explored or resolved [cit] . furthermore, in a scbs, student participants are assigned to designated clinical roles, such as that of the nurse, physician, or other healthcare professionals, and are expected to conform to the behavior-and practice-oriented conventions of their assigned roles [cit] . however, despite these differences in use, design, and implementation, simulation-based learning (sbl) research rarely disaggregates differences between skills-based and scenario-based simulations."
"lastly, the findings potentially extend our understanding of how scenarios may support learning team-work behaviors when they afford student participants opportunities to coordinate care and distribute the workload with peers and other healthcare professionals, while simultaneously interacting with the material environment (e.g., stethoscope, thermometer, diagnostic findings). these characteristics are consistent with hutchins' [cit] concept of distributed cognition which suggests that interaction is \"deeply multimodal and composed of a complex network of relationships\" (p. 376) [cit] . multimodality refers to the different embodied mediums or tools (e.g., physical tools, social interactions) that individuals use to achieve their goals [cit] . this complexity and multimodality is reflected in tables 4, 5, 6, and 7 and in fig. 2, which highlight the frequency and diversity of activities in which student participants engaged. thus, scbs could alternatively be framed as simulated clinical systems in which the unit of analysis would include examination of how student participants coordinate while integrating the use of culturally relevant artifacts to achieve a goal. viewing scenarios as simulated clinical systems potentially provides simulation stakeholders with alternative ways to examine how student participants collectively achieve goals that go beyond solely relying on verbal reflection or outcomes assessment of performance [cit] ."
"software the ants software is available at http:// www.picsl.upenn.edu/ants with download and compilation instructions at http://www.picsl.upenn.edu/ ants/download.php. svn release 711 was used for the examples and evaluations performed in this paper. some components of ants depend on the insight toolkit. the most critical dependency, for atropos, is the itk statistics framework used to implement the univariate and multivariate parametric models. we linked to the git version of itk current as of dec. 1, 2010. see http:// www.itk.org/wiki/itk/git/download for instructions on git itk."
"5 although many optimization techniques exist [cit] for a concise summary of the myriad optimization possibilities)-each with their characteristic advantages and disadvantages in terms of computational complexity and accuracy-atropos uses the well-known iterated conditional modes (icm) [cit] ) which is greedy, computationally efficient and provides good performance. the em employed in atropos may therefore be written as a series of steps:"
"an overview of atropos components can be gleaned, in part, from the flowchart depicted in fig. 2 . given a set of input images and a mask images, each is preprocessed using n4 to correct for intensity inhomogeneity. for our brain processing pipeline, the mask is usually obtained from the standard skull-stripping preprocessing step which also uses atropos. initialization can be performed in various ways using standard clustering techniques, such as k-means, to prior-based images. this initialization is used to provide the initial estimate of the parameters of the likelihood model for each class. these likelihoods combine with the current labeling to generate the current estimate of the posterior probabilities at each voxel for each class. at each iteration, one can also integrate n4 by using the current posterior probability estimation of the white matter to update the estimate of bias field."
"to perform the job ads analysis, we followed a two-step approach. (1) we use the term frequency to extract the most commonly used terms in the job ads and (2) we extract the relationships between the words to create the data science taxonomy."
"word-sense disambiguation is the task of identifying which sense (meaning) of a term is used in a sentence or in a set of documents when the term has multiple meanings. 10, 11 in general, commonly used approaches for word-sense disambiguation requires two inputs, ie, a dictionary that contains the senses, which have been already disambiguated, and a corpus of terms to be disambiguated. dictionary-based methods exploit the hypothesis that words that appear ''near'' a term are related to each other and that this relation can be observed in the definitions of the terms. 12 therefore, finding all possible dictionary definitions may disambiguate terms and select the one which has the biggest word overlap between the definition and the related words within a given text."
"regardless of the particular ri technology, the systems provide a complete, metric and precise 3-d surface model of the patient. they also estimate the table transformation that brings a pre-defined region of the intra-fractional patient surface in congruence with a reference at a fine scale. a number of studies have shown that these techniques obtain a high degree of precision for patient setup for thoracic and abdominal tumor locations [cit] . in addition, most ri systems are able to capture dense 3-d surface data in real-time, allowing for continuous patient monitoring during the course of a treatment session. however, existing solutions are designed with a focus on setup verification and require an initial patient alignment with conventional techniques using lasers and skin markers [cit] . this manual initial coarse setup is both a time-consuming and tedious procedure."
"for quantitative evaluation of the proposed framework, we have benchmarked the performance of the descriptors on real data from a low-cost ri device (microsoft kinect). indeed, the method is generic in a sense that it can be applied with various ri technologies. first, in an experimental study on anthropomorphic phantoms, we investigate the method's potential for multi-modal ri/ct registration. thereby, we underline the benefits of the proposed method for partial matching. second, we study the performance of the algorithm on data from healthy volunteers (ri/ri registration), where deformations may occur due to variations in patient body distortion and respiratory motion."
"building the taxonomy e-co-2 service helps building taxonomies from a relevant corpus. to move from simple word count to more complex statistical measures, words need to be ''lemmatized,'' which is the process of grouping together different forms of a word so they can be analyzed as the same (eg, ''scientist'' and ''scientists'' should be considered as the same word). these processes allow building the term dictionary, which is a list of all unique words used in the corpus. in the next step, we use a set of hybrid term extraction methods to rank the relevant terms. during the relation discovery, we first build non-hierarchical relations, and with the use of hypernym-hypernym relations, we build hierarchical relations within each cluster."
"the proposed approach may be improved toward two directions, ie, increase the quality and accuracy of the results and speed up the performance of the overall system, to enable quasi-real-time execution."
"using the language-processing technique, we have managed to find similarities between job ads, cv, and course description using edison data science competencies. over time, these lists of competencies are likely to become out of date and will not reflect anymore the need of the job market. the e-co-2 analyzer can be used to align definitions and terminology used within edison with the current status of the job market."
"(1) to create a taxonomy of the terms used in data science job ads and (2) to discover the relationships between the terms to identify skills and competencies required by these job ads. the taxonomy helps to identify the relationships between the skills and competencies used in data science related documents, such as course descriptions, job ads, blogs, and papers. it will help to map data science generic terms used in job ads, cvs, and course descriptions into more concrete skills and competencies, which, in turn, can help to avoid a mismatch between job and cv profiles, courses, and trainings. one primary use of the taxonomy is to validate and update the edison data science framework over time,"
"the course developer provides a description of the course and e-co-2 service provide a measure about the distance of that course content from the job market requests; if more courses or trainings are available, then the course will be also compared to other similar courses. figure 10 shows how the profile of three courses compare to the job market profile derived from the data set containing 1000 job ads (blue)."
the inventory is publicly available on the edison website 2 and a detailed analysis is available through earlier publication. 3 another challenge facing the analysis of the data science landscape is the absence of commonly accepted definitions for this emerging field; the collected information is highly unstructured and noisy (a term can be used to describe different concepts while multiple terms are used to describe one single concept).
"e-co-2 analyzer uses as a reference for comparing the various profiles the data science competence groups defined in the edison data science framework (edsf). 28 the four competence groups described in the nist definition are further extended in the edsf and refined into subgroups of competencies. in total, 30 competencies have been identified and used as a reference for comparing data science programs, job ads, user's profiles, etc. the data science competencies are divided into five groups, ie, data science analytics (dsda), data science engineering (dseng), data science management (dsdm), data science research methods (dsrm), and data science domain knowledge (dsdk). 28 to compare data science documents using edsf, we have implemented the data processing pipeline described in section 3. the e-co-2 service is available as a docker image that can be downloaded from the e-co-2 github repository. 23 the github repository includes the data sets that have been used to generate the results shown in figures 9 and 10 . to construct each of the competence groups vectors (dsda01, dsrm06, etc), we used the edsf where it defines each compliance group. from each compliance group, we manually extracted keywords and representative terms. for example, for dsda01, we extracted terms such as unsupervised machine learning or data mining. next, for each of these keywords and terms, we found the relevant wikipedia page on which we perform a term extraction and measure the tf-idf, which allows us to construct a vector for each competence group."
"the database is used to efficiently store and query input documents, analysis results, a context corpus, and category vectors. queries can be used to perform analytics in relation with trends. for example, given that the database is populated with enough historical data, one can query for the average similarity of job ads in relation to data analytics. several parameters can influence the choice of database. since most data representations these days are in json format, a mongodb may be more appropriate. one of the benefits of using mongodb is that there is a wide range of tools that allow us to store and query data and map them naturally to object-oriented programming languages. moreover, the document-based architecture of mongodb allows our schema to evolve with the requirements."
"the problem becomes bigger when analyzing the data science job vacancies, ie, an initial term extraction process based on 1000 job ads using a simple term frequency count produced approximately 300 000 terms. one of the few accepted definitions of data science is proposed by nist. nist big data working group (nbd-wg) published their first release of big data interoperability framework (nbdif) [cit], 4 consisting of seven volumes. volume 1 provides a number of definitions, in particular, those of data science, data scientist and data life cycle, which have been used as a starting point for this analysis. nist defines data science as a set of multidisciplinary competencies and skills at a very high level of abstraction. however, the definition proposed by nist is not enough to analyze educational programs, in particular on how well they cover the necessary competencies. within the edison project, a more detailed study showed large discrepancies in the data science field, at least in terms of content and focus on required skills."
"clustering, 15 farthest first, 16 and em. 17 to achieve this goal, we calculate tf-idf for all words contained in the extracted definitions of each category. these tf-idf values are saved and used to measure the cosine similarity of the documents. figure 2 presents the pseudo-code of the training process to obtain the profile of a document for a related subject; we first need to obtain a vector that will numerically represent that document."
"the main ibmq function is written in c for computational efficiency, and wrapped in convenient r code. ibmq employs the openmp api to perform modelling operations in parallel, facilitating the analysis of large datasets. a sparse matrix representation enables efficient matrix calculations (see supplementary material). ibmq adopts object-oriented programming, making use of existing s4 classes (e.g. eset and snpset). the main functions are:"
"many programs appear to place an equal sign between data scientist and business analyst. while business analysis might be considered a special case of data science, the opposite is certainly not correct."
"the proposed framework is composed of three stages (see fig. 3 ). first, the local topology of the template (ri) and reference surface (ri/ct) is encoded by our modified descriptors that are specifically designed to handle surface data from distinct modalities. second, point correspon-dences are established by descriptor matching and pruned by incorporating a geometric consistency analysis. third and last, the optimal rigid-body transformation is estimated w.r.t. the coordinate system of the treatment table. the initial patient setup can then be performed by adjusting the table based on the estimated transformation."
"competencies and learning outcomes are seldom defined explicitly. the presented analysis should be seen as an approximation. simultaneously, considering a large number of programs that were analyzed and that a simple competence group model was used; we believe that the analysis is consequential so long as one is careful about the type of conclusions to draw from it."
"finally, in figure 6, we look at balance in programs depending on what type of source they are coming from. we clearly see that, for almost all cases, more than 50% are significantly imbalanced. the only exceptions are programs that come from cross-department collaboration, where more than 50% of programs are balanced. there are some minor differences between other sources, but they should not be over-interpreted in the early stages of data science curricula development."
the focused crawler is used to collect document related to the job market and data science education. the focused crawler systematically browses specific resources on the web to retrieve new documents. the behavior of the focused crawler is defined by the following policies.
"figure 3 e-co-2 service architecture. e-co-2 will act as a backend to the community portal 3 through rest api figure 3 shows an overview of the architecture of the e-co-2 service. at the top level, the e-co-2 api provides methods that allow an api consumer service to analyze documents and retrieve results. the local catalogue manager plugs-in into multiple data sources giving the opportunity to have a unified view of various data sources. connecting to a larger overlay network, it is possible to discover and request multiple datasets that can be used to perform more analytics. the focused crawler is used to collect documents from the web related to the job market and data science education. the database is used to efficiently store and query input documents, analysis results, the context corpus, and the category vectors (see section 3.3). the task scheduler queries the database at regular time intervals for new documents that need to be analyzed or updates in the context corpus and schedules tasks for the e-co-2 analyzer. the e-co-2 analyzer is the main analysis component responsible for (1) providing a similarity matrix of an input document against the competence groups derived from the edsf tables and (2) generating category vectors based on a context corpus."
"recently, eqtl mapping studies (where gene expression levels are viewed as quantitative traits) have provided insight into the biology of gene regulation. among eqtls it is customary to distinguish cis-eqtls from trans-eqtls. the former share the same locus as the expressed gene, while the latter are located on loci different from the expressed gene. many eqtls, particularly trans-eqtls, form trans-eqtl hotspots where one single nucleotide polymorphism (snp) is linked to the expression of several genes across the genome. despite this observation, most available eqtl analysis tools treat genes as independent, and as such these methods are * these authors contributed equally † to whom correspondence should be addressed underpowered to detect trans-eqtl hotspots [cit] . likewise, some available packages (such as ggtools [cit] ) allow for data analysis and visualization of results, but are currently limited to univariate analyses. we present an integrated hierarchical bayesian model that jointly models all genes and snps to detect eqtls. the ibmq r/bioconductor package incorporates genotypic and gene expression data into a single model while 1) coping with the high dimensionality of eqtl data (large number of genes), 2) borrowing strength from all gene expression data for the mapping procedures, and 3) controlling the number of false positives to a desirable level."
"that ct preprocessing can be performed offline prior to the first fraction. in order to improve the snr of the ri measurements, we combine temporal averaging (over 150 ms) with edge-preserving filtering, invalid range measurements are restored using normalized convolution [cit] . the patient surface can be segmented from the background by incorporating prior information about the treatment table plane. the preprocessed ri (ct) meshes consist of ∼15k (20k) vertices. note that ct data typically covers only a portion of the ri scene."
"the e-co-2 service is compliant with the restful approach as it is to provide a rest (representational state transfer) api, which allows other systems to access and manipulate its resources using a uniform set of stateless operations. rest is a stateless protocol with standard operations that help in performance and reliability. it is becoming the de facto standards are rest over http. rest may be less descriptive than soap (simple object addressing protocol) but is less strict, which allows for greater flexibility. rest is also lighter and less complex than soap. other transport protocols may be more suitable for delivering data to a given service. however, http is widely used as a transport protocol because http actions give a clear indication on the service design and functionality. moreover, the design of commonly used web portals follows the rest specifications. therefore, we opted to use rest over http for delivering the service api. the api to analyze documents and retrieve results provides the following methods."
"this disparity should rather be even lower, but we assumed that a stricter criterion would be misrepresentative at this early stage of data science curriculum development. between 20 pp and 30 pp, we classified programs as having a small imbalance. if the disparity exceeds 30 pp, it effectively means that one of the competence groups cannot be covered at all or only to a marginal extent, while one of the others exceeds 60%, which means it dominates the program. we classified such programs as having a significant imbalance."
"as described in previous sections, the e-co-2 analyzer compares the similarity between the context vector and the word vector. assuming that the word vector contains a set of terms extracted from a large and representative data set, the terms that appear in the word vector but not in the context vector should be used to better align the descriptions and terminology within edison. at the same time, the terms that appear in the context vector but not in the word vector are redundant and therefore need to be reconsidered."
"in this emerging and dynamically evolving field of science, a number of challenges are still facing the different stakeholders on both the demand and the supply sides, ie, students have to find the right educational program that will lead them quickly to find their first job, human resources departments have to select the right candidate for a given professional profile and describe the right competencies required for a give vacant position, and finally, trainers and educators have to design and adapt data science programs to develop the competencies and skills needed for the business sector as well as research careers. we analyzed 1000 job ads related to the data science with the following objectives:"
"in addition, a crawler can set a heavy load on servers especially if it is implemented with a high degree of parallelism, which may be regarded as an unwanted ddos attack."
the data science education analysis was performed manually and took several months while the job market analysis was automated and data was collected in a few days. it demonstrates a clear need for developing the e-co-2 as a service to help to accelerate the future gap analyses. 23 we describe two scenarios to show how e-co-2 service can be used to extract compare data science related text.
an approach based on the dual nature of lambda architecture may help in addressing both challenges. the successful implementation of this approach may lead to the creation of quasi-real-time observatory about competence and skills not only in data science but also in other fields.
"for the purpose of eliminating false correspondences, the set of correspondences is pruned by applying a geometric consistency check [cit] . based on an iterative scheme, we successively penalize and remove matches that exhibit inconsistent surface normals and locations. the set of re-"
"patient setup reproducibility is one of the major challenges in fractionated radiation therapy (rt) planning and treatment. precise alignment is a mandatory prerequisite for the success of rt, improving the balance between complications and treatment and providing the fundamental basis for high-dose and small-margin irradiation application. prior to each fraction, the patient must be accurately aligned w.r.t. the target isocenter that has been localized in planning ct data. for setup verification and correction, radiographic portal imaging, cone-beam ct and ct-on-rails may be applied. however, this involves additional radiation exposure to the patient. non-radiographic techniques that locate electromagnetic fiducials [cit] are an accurate alternative, but require the patient to be eligible for the invasive procedure of marker implantation."
"hybrid approaches use a proper combination of the previous two steps. 8 most of these approaches depend on statistics and use syntactic rules as a complementary method to filter the appropriate terms. therefore, in these hybrid approaches, a linguistic analysis is performed to exclude words like pronouns and verbs. this step may also be applied to identify patterns and sequences of part-of-speech and pass these sequences on to statistical measures to rank possible terms. other approaches include linguistic information in the ranking process. 9 the biggest challenge for any term extraction approach is its validation. judging the accuracy of any approach involves a human expert that needs to evaluate the results."
"the task scheduler queries the database at regular time intervals for new documents that need to be analyzed or updates in the context corpus and schedules tasks for the e-co-2 analyzer. this component makes sure that the correct arguments are set and instantiates the e-co-2 analyzer to execute a mapreduce job. this component is responsible for setting the priority of tasks. for example, the e-co-2 analyzer may be set to analyses a large set of documents, but at the same time, a user uploads a cv to the portal. in this case, the scheduler should set the user's request for higher priority. in the overall design, it is important to have isolation between components. the task scheduler gives a good separation between the actual analysis, the api communication, and the database implementation. there are several issues when considering task scheduler design; one of them is task cancellation. in many cases, tasks should be canceled either because the results of the task are not needed anymore or because that particular task is using too many resources preventing the completion of other tasks. as mentioned earlier, it is important for tasks to have priorities. however, care must be given to ensure that low priority tasks will not wait forever."
"such courses might be relevant to certain programs and business schools, but it seems they are used as a rushed solution, due to the limited relation of these courses to the rest of the program, to superficially cover missing elements in the program. it is important to note that we excluded from this argument specialized courses in economy, financial analysis, or similar."
"it resulted in the identification of 3 core competence groups, ie, (1) data science analytics (including statistical analysis, machine learning, data mining, business analytics, and others), (2) data science engineering (including software and applications engineering, data warehousing, big data infrastructure, and tools), and (3) domain knowledge and expertise (subject/scientific domain related). these core competence groups correspond to the skills groups identified in nist big data interoperability framework. 24 skills and competencies are equivalent terms; however, competencies are more often used in an education setting, whereas the term skill is more common in the professional training setting."
"the majority of data science-related programs are offered by computer science departments. these programs are the most generic compared to the programs offered by other departments, which are more tuned to a specific field (eg, genomics or bioinformatics) or disciplines (eg, statistics). the study showed the emergence of data science programs offered by multiple departments, which is not common in high education. there is quite a variety of contents that is now published under the umbrella of data science label. the educational programs covered by the study showed that most of these programs cover data analytics, as defined by nist, to a sufficient extent. however, (computer) engineering competencies are often missing in programs not originating from computer science or computer engineering departments."
"in addition, two meta competence groups we have identified by the project, ie, (1) data management and governance (including data stewardship, curation, and preservation) and (2) research methods for research related professions and business process management for business-related professions. however, we decided not to include these groups as separate in this analysis. due to the limited quality of data, as most programs do not define competencies well enough, including two additional groups that overlay the three core ones could lead to misleading results. the results were discussed with edison expert liaison group, whihch consisted of leading industry and academia representative, to ensure the quality of results."
"the crawler is composed of two components, ie, a scheduler and a database. the scheduler enforces the policies described above and a document fetcher responsible for obtaining the relevant document. since resources may have different formats for representing documents and might expose different apis, we will then have a different fetcher for each resource. these types of apis are mostly focused on interacting with that particular system. some web pages make changes to their structure and data while these changes in the data structure would be reflected in the api months later. moreover, unavailability and downtime at some api endpoints may go unnoticed for days. since we want to have up-to-date information from many resources, we incorporate the offered apis to the crawler but also use the publicly available contents while considering the bandwidth limitations and the crawling policies mentioned above."
"in this section, we present our analyses of both data science education and job market to stress the need for a synergy between the two sides."
"we present here a short summary of the analysis. 59% of european ( figure 5a ) and 50% of non-european ( figure 5b ) programs are significantly imbalanced. this means that one of the competence groups is not covered properly or not at all. additional 14% and 15% of programs, respectively, have smaller imbalances. only 27% and 35% of the programs, respectively, could be considered balanced, despite the fact that the threshold we in a large subset of programs, in which domain knowledge appears to be properly covered, a deeper inspection reveals that the offered courses over emphasize generic management and business skills. there is little conceptual connection between courses offered to cover domain knowledge and those covering other competence groups."
the students provide an up-to-date cv and list of jobs she/he is interested in and e-co-2 service will return a polar presentation showing which job fits better with the current cv; it will also show the competencies missing to match the other job ads; the students will have then the choice either to follow courses or trainings to develop the missing skills or apply for the job that fit his current cv. a similar scenario is when the hr department has multiple candidates and wants to shortlist the applicants to select only the two or three most relevant to a given job opening in the company. figure 9 shows a cv profile compared to three different job ads. it is clear from the candidate cv fits better the two job ads and is missing one specific skill for the third one.
"the collection of representative keywords is further enriched with terms extracted from the context corpus. this context corpus is a collection of documents that are closely related to the subject that encloses the categories to be used as a mapping target. these documents may contain definitions or simply revolve around a specific subject and are used as an input for a term extraction process, which will save the extracted terms in a context terms file. figure 1 presents the pseudo-code of the profiling process."
our datasets (template data t and reference data r) are represented as two sets of pairs of surface coordinates x and their associated feature descriptors f :
"a course or training developer wants to check whether her/his course or training is still relevant for students who want to develop competencies for a given data science professional profile. in addition, she/he wants to compare her/his courses. this is a simplified scenario of a program director that wants to select a set of courses and trainings to create a curriculum targeting given data science professional profile."
"matching local invariant feature descriptors is a key component of a variety of computer vision tasks in the 2-d and 3-d domain such as registration, object recognition, scene reconstruction or similarity search in databases. 3-d surface registration is more relevant to the topic of this paper. thus, we will focus our discussion on this subfield. in this context, a popular method for solving the alignment problem of two or more sets of points or surfaces is the iterative closest point (icp) algorithm [cit] . however, in the presence of gross misalignments, the algorithm depends on a proper initialization in order to prevent the iterative transformation estimation from being stalled by local minima [cit] . furthermore, icp is not designed to handle partial overlap in case of occlusions, clutter and viewpoint changes."
"each program in the edison inventory was analyzed in detail to determine to what extent courses in its curriculum cover the identified competence groups. some courses might naturally cover more than one group. in some cases, especially in the case of project courses (eg, master thesis), they might provide coverage of all areas simultaneously. such aspects were accounted for during our analysis."
"statistical approaches produce a ranked list of terms identifying the most important terms extracted from a text and usually start by identifying all the unique words that appear in a text. they also construct all possible n-gram that can be identified. to determine the ''term-hood'' of each term and rank it accordingly, statistical approaches use several metrics. term frequency (tf) is one of the most common and simple metrics used for statistical term extraction; it measures how frequently a term occurs in a document. the inverse document frequency (idf) measures how important a term is. combining the two provides the term frequency-inverse document frequency (tf-idf), which is a statistical measure, used to evaluate how important a term is to a document in a corpus. the tf-idf is the most established measure. however, other metrics that rank candidate terms such as t-score, c-value, dice coefficient, etc, may offer the e-co-2 analyzer more accuracy for the term extraction."
the hierarchical relation discovery used hypernym-hypernym relations included in online dictionaries. hypernym shows the relationship between a generic term and a specific instance. we followed a three-step approach.
"this work has shown that it is possible to develop semi-automatic service that analyzing the huge amount of available data (courses, website, books, open positions, job advertisement, etc) could establish a direct correlation between the skills and competencies the business sector demand and courses or training that education sector offer. this correlation may help both sides to be more (and more quickly) aligned. at the same time, the same conceptual design for the service could support the competency evaluation and gap identification for trainees and students, ie, each of them will be empowered with an instrument that let them taking full control of their educational and career development paths, adapting the course of studies on the quickly changing market landscape and the variability in their personal interests."
"data preprocessing. the patient surface is extracted from ct data using a thresholding based region growing segmentation and a marching cubes algorithm on the resulting binary segmentation mask followed by laplacian mesh smoothing. subsequently, the mesh was decimated in order to reduce the computational complexity. let us remark"
"over the past few years, due to advances in sensor technology, several devices for non-radiographic and non- figure 1 . schematic illustration of the proposed automatic initial patient setup: the patient's intra-fractional surface is acquired with an ri device and registered to planning ct data (depicted in gray). the estimated transformation (blue) that brings template and reference in congruence is then applied to the treatment table."
"the e-co-2 analyzer based on tf-idf metric has been implemented as a series of mapreduce jobs to perform two main functions, ie, training and classification. the choice to use mapreduce is motivated by the many published studies in the field of text processing, which show good performance achieved by various mapreduce implementations. [cit] training task ( figure 4a ) performs term extraction with the addition of the a priori algorithm and constructs a ''category vector'' for each of the categories or competences we wish to identify. for each of the categories, it is necessary to manually set a collection of keywords, definitions, and descriptions that are representative of each category. the quality of the classification depends on the accuracy of these keywords, definitions, and descriptions. therefore, the keywords, definitions, and descriptions have to be concrete, representative, and contain specific terms."
"in this paper, we propose an ri-based approach that enables a markerless and automatic initial coarse rt patient setup, superseding the need for lasers and skin markers. without spending extra time, the initial patient setup is performed in an unlabored manner. since this is only an initial alignment to be followed by position refinement [cit], the accuracy requirements are rather low (isocenter position within ± 50 mm) [cit] . the proposed method can be applied to reference surfaces either acquired by the ri device prior to the first fraction, or extracted from planning ct data. it estimates the optimal table transformation from point correspondences between local features. we have extended state-of-the-art descriptors to handle distinct mesh resolutions and topological variations that occur due to the low signal-to-noise ratio (snr) of ri sensors. by design, the approach can handle gross initial misalignments and cope with partial matching [cit], see fig. 1 . this is a fundamental prerequisite for both the mono-modal (ri/ri) and multi-modal (ri/ct) case, where the intra-fractionally covered template surface region may differ severely from the reference. typically, the field of view of the ri device is considerably larger than the ct scan volume."
"our surface registration framework relies on feature descriptors that encode the local geometric topology in a translation-and rotation-invariant manner. however, the descriptors are not invariant to scale on purpose, as we incorporate the metric scale of the anatomical surface topology as an important characteristic. placing great importance on robustness and repeatability, we have extended two alternative descriptors (meshhog, riff) for multi-modal application. both rely on histograms of oriented gradients (hog) [cit] which have shown to be leading edge in terms of classification performance for the 2-d case. as a baseline, we compare these hog-like descriptors to the well-established technique of spin images [cit] . below, we outline the descriptors' functional principles and inevitable adaptations for the problems at hand."
"the e-co-2 api follows standardized protocols and methods (rest api, json, and service-oriented architecture) to make the integration with the portal easier and also provide re-usability and flexibility. this is achieved by separating the business logic from the exposed functionality."
"in order to speed up the overall system execution and willing to pursue some historical data analysis about the variability of skills, new (big data) architecture needs to be considered. as such, an initial evaluation of the lambda architecture 41 has been done. taking into account the amount of data to be evaluated will increase rapidly over the years (both for the increase of produce new data and the need to store historical ones);"
"data science is an emerging field of science, which is rapidly gaining importance in both academia and business sectors. during the last years, we have registered a sudden increase in the number of universities and industry programs and courses labeled as data science (or big data) or claiming to offer data science related content. a survey has been performed in the context of the eu funded edison project 1 and aimed at identifying the skills and knowledge present in these offerings. the survey covered over 300 educational programs and over 100 academic and industry courses; it was primarily based on what was advertised and published on the courses' websites. the information ranged from detailed to limited, which increased the complexity of the analysis. nevertheless, to our knowledge, it is the most complete and detailed analysis up to date."
"to analyze the data science job market and identify its needs, we applied well-known text processing data processing pipeline described in section 2. the term extraction was performed on a dataset containing 1000 job ads for data scientist for several experience levels and functions, extracted from linkedin ® . the term extraction process showed that the majority of terms used data science job ads is related to computer science, math, and statistics, indicating that these skills are relevant in this field. moreover, specific programming languages and platforms seem to be included in many job ads and further investigation could reveal which programming languages and platforms are considered important compared with. 25, 26 for example, the degree of a hypernym node that connects more hypernym-extracted from a text should give an indication of the term's importance. this way, the graph can become more ''balanced'' by providing an intuitive level of abstraction in terms. figure 7a shows the required years of experience in the job ads: 82% of job ads targeting mid-career applicants, while entry-level opening represents only 10%. candidates suited for mid-career opening should have graduated at least five years before the emergence of data science as a professional profile, and thus, it is likely that current potential applicants might not have the multidisciplinary background required for the job opening. figure 7b shows the type of work the applicants will be doing in her/his job. in sections 4.1 and 4.2, we presented the analysis of both current data science education offerings and the needs of the job market, and immediate conclusion of two analysis is that job market is looking more for senior and mid-career staff (82%) rather than fresh graduates (11%). this suggests that education has to also offer short training education and not only long programs ba, mss, and phd. a second important outcome which came out of the analysis is the fact that the job offers are dominated by profiles in research and it departments (research 42%) and (it 26%); this also suggests that education has to carefully balance between teaching engineering skill and scientific research method."
"science. the output of this process is a hierarchical taxonomy of the skills required by data scientists 5 described by the terms used to refer to the tasks and output expected for any given skill. automating the extraction of skills from job ads and other documents is a key step to being able to address stakeholder needs; in particular, it may help in following trends and thus generating the knowledge needed to define the appropriate career paths or it may be useful to keep curricula up-to-date with job market demand over time. in this section, we describe the approach used to develop the tools to automate the analysis of a large corpus of documents."
"classification ( figure 4b ) compares an input text, which may be a job ad, a cv, or a curriculum description with the set of available category vectors that are created during the training phase. for each of the category vectors, the classification provides a similarity measure that indicates how close the input document is to the available category vectors each representing a category or a competence."
"the competencies classification (e-co-2) service is a distributed automated service designed to enable data science gap analysis. it can identify the similarity of a document against a set of predefined categories. it can, therefore, be used to perform a gap analysis following the edison classification to identify mismatches between education offering and business sectors demand. students, data analysts, educators, and other stakeholders can use this tool to identify the gaps in their skills and competencies and identify the most suitable educational path to fill these gaps. 18 moreover, by constantly collecting data from sources like job ads and postgraduate programs, we will be able to identify trends from both the job market and education."
"term or terminology extraction attempts to identify the body of terms used in a subject or content. 6 a term may be a single/multi-word expression that has a particular meaning within a specific domain. 6, 7 there are three main approaches to term extraction, ie, (1) statistical,"
"solutions proposed to address job ads to cv matching often require some extras to work properly like social media data or extra input from the cv owners. a bilateral recommendation system was developed to improve the match between people and jobs. 29 it is based on task-related and social aspects of human and social capital or person-environment fit. the approach considers two dimensions, ie, (1) matching individuals to task (fitness of individual to job or p-j) and (2) individual to another individual's (fitness of individual to the working environment group, vocational, and organization). two components are developed, ie, cv-recommender and job-recommender. both recommenders are based on a probabilistic hybrid recommendation engine based on a latent aspect model that tries to derive individual preferences as a combination of preference factors. for the approach to work, not only cvs and job description but also cv owners were asked to rank the job based on their preference. obviously, this approach does not scale to a large number of jobs and a large number of cvs. web finder is a web application with the aim to match cvs based on skills with respect to a given job. cvs are ranked by comparing the skills from the resume to the skills required in the job description. web finder is based on named entity recognition (ner) approach; it uses a statistical classifier to identify named entities; a classifier is trained annotated training set, which contains at least 15000 sentences to work properly. 30 [cit], where the information that is extracted from cvs and job description document is complemented by social media data. bollinger 31 demonstrated that the addition of social media and external data improves the classification accuracy dramatically in terms of identifying the most qualified candidates. schmitt and caillou 32 used a deep neural network to match the collaborative filtering representation properties. the aforementioned authors used information inferred from the interactions between job recruiter and job seeks differs from the information that could be extracted from cvs or job announcements. other similar approaches combine information collected from a linkedin account with information from applicant's blogs to match a person to derive the candidate's relevance score for the applied position. 33 e-gen 34 is a natural language processing and information retrieval system analyses the candidate's answers, which are composed of the cover letter and the cv and computes a relevant ranking of the candidate's application. comparing to all the cv-matching approaches, we propose a method that could be tuned to specific by selecting the reference against which can compare cvs and job ads. many commercial solutions like ''search and match,'' 35 daxtra search, 36 match, 37 and rchilli cv automation 38 based on proprietary solution such as aspire content processing platform 34 and aim at automated cv/resume matching where cvs and job ads are vectorized in a multidimensional space including job titles, skills, experience, qualification, location, salary range, industry sector, etc. unfortunately, there is not a lot of information about these tools available to allow a deeper analysis. however, from the dimensions considered for matching, it is clear that these tools are generic and do not consider the domain-specific needs. our approach focuses on data science jobs because it is still not well defined both recruiters and applicants are using different terminology to point to the same or similar skills. our approach offers a reference based on the edison data science framework to match skills but also help to identify and rank the skills based on the competence groups relevant to different data science job profile. beyond cv and job ads matching our approach can help both recruiters, job seekers, and trainers to improve the description and the content of cvs, job ads, and trainings to better achieve their respective goals."
"we propose extensions to state-of-the-art descriptors that enable its application for robust multi-modal surface registration. first, we have modified the meshhog descriptor to achieve invariance to mesh resolution and robustness to topological variations due to noise and quantization effects. second, we introduce a scheme that extends the 2-d riff descriptor to the domain of 3-d surfaces. our method is based on a correspondence search engine that enables partial matching, and that is resilient w.r.t. minor deformations that occur in practice due to body distortion and respiratory motion. to our knowledge, the automation of initial coarse setup in rt has not been addressed yet."
"based on the techniques and approaches described in the previous sections, a text profiler has been implemented and can be trained to provide the profile of a document based on a set of predetermined categories. the first step is to determine the categories to which the documents need to be mapped. for each of the categories, it is necessary to manually set a collection of keywords that are representative of the target category."
"one should expect, in principle, roughly uniform coverage of each competence group. balance in covering competence groups is a key to educating successful data scientists. however, small differences in coverage can of course occur. edison proposed that the disparity between the most and least covered competence group should not exceed 20 pp. (percent point), so that the program can still cover the whole spectrum of data science field. we deliberately avoid using exact points since european and american system operate differently."
"when the terms are obtained, a word-sense disambiguation on each term is performed and its definition extracted. the set of extracted definitions represent the collective meanings for each category and can be encoded as a vector that numerically represents each category."
"some of the extracted terms are not accurately disambiguated due to insufficient information provided by online dictionaries. combining more resources may help in a more accurate disambiguation and a more complete picture of the skills required. a potential solution to this problem could be the use of existing taxonomies (ie, acm taxonomy) to compare and validate the proposed results. taxonomy alignment is a challenging issue, which has to address heterogeneities between different taxonomies, ie, one aspect relates to the lexical heterogeneity, where classes of taxonomies may be semantically equivalent while the terms used for expressing them might differ. 39 another aspect is their structural heterogeneity, where relationships between concepts of taxonomy are different from those of another one. 40 using semantic similarity measures, which involve statistical and linguistic approaches, we may be able to identify similar concepts from existing taxonomies."
term extraction is one of the key aspects of the e-co-2 analyzer. term or terminology extraction attempts to identify the body of terms used in a subject or content of the targeted document. a term may be single or multi-word expressions that have a particular meaning within in specific domain. the e-co-2 analyzer uses a hybrid approach using pos taggers to filter out words and attempt to reduce the search space.
"analyzing millions or even thousands of documents manually to identify skills is labor-intensive, and therefore, we can employ natural language processing and text-mining methods to extract these skills through the analysis of terms used in a given set of documents related with data"
"linguistic or contextual approaches attempt to identify syntactical patterns in a text in order to extract terms. usually, terms tend to have characteristic syntactic structures. part-of-speech (pos) taggers are used to identify these structures. however, linguistic approaches are language-dependent and therefore are not flexible and adaptable to other languages."
"from (42), we know that ( ) is decreasing in . to prove the convergence of ecorl, we only need to show that ( ) becomes smaller than with the number of iterations. we now show that is nonincreasing in successive iterations of the algorithm. we have"
"(i) an ee optimization problem for corl system with imperfect sensing results is established, subject to the individual power budget, peak pu's interference constraints, and circuit power consumption in the total power expenditure, as well as considering subcarrier transmission mode. particularly, our model can be easily extended to many practical scenarios with necessary modifications."
"step 4. get the current optimal solution and update pheromone trail. if the current iteration is less than, turn to step 2; otherwise end the hybrid-rhm-svn algorithm."
"in this section, we provide a mathematical formulation for the reliable virtual network mapping problem. we map virtual networks with survivable strategy in the substrate network. the reliable virtual network mapping formulation is composed by the substrate network and virtual network requests modeling, virtual networks mapping problem modeling, and virtual network survivability analysis."
"the ant colony optimization algorithm is a random search optimization algorithm that simulates ants foraging by iterations [cit] . in the ant colony optimization algorithm, ants can release pheromone information when looking for food on the path. the pheromone has a certain scope to cause the attention of other ants. if the food is in a short distance, the ants in the path may be back and forth many times and leave more pheromone. the more pheromone is left, the higher probability of this path can be chosen. we use the ant colony optimization algorithm to solve the mapping problem, and the fitness function, transition probability, and pheromone trail update need to be redefined as follows."
"in this paper, we formulate the reliable heuristic mapping problem as an integer linear program in the following manner with the objective function of minimizing the cost of the substrate network."
"definition 1 (fitness function). the fitness function of the ant colony optimization algorithm fit is related to the objective function, which is the cost of mapping:"
our simulation experiments evaluate three algorithms listed in table 1 . svne-orp is based on the protection mechanism and svne-tap is based on the recovery mechanism. reliable mapping method based on optimization sharing redundancy mechanism svne-tap [cit] reliable mapping method based on topology-awareness and reoptimization mechanism
"it is noticeable that objective function equation (22) in op2 is not a concave function, and the solution for it will be of high complexity. in this section, we first use fraction programming [cit] to transform the problem into an equivalent convex optimization problem and then use the lagrangian technique to transform the equivalent problem into a corresponding dual problem. subsequently, an optimal iterative algorithm is proposed."
"(1) the objective function (1) is to minimize the cost of the link mapping. for a specific virtual network request, the resources allocated to the virtual nodes are identical. however, the resources assigned to the virtual links depend on the length of the substrate links. for this reason, we use the bandwidth usage of the substrate links to measure the cost of mapping."
"where 1, 2, 3, and 4 are the nonnegative lagrangian dual variables for constraints (26)- (30), respectively. and the dual problem of op3 is given by"
"step 2. update the transition probability according to the pheromone trail, maps virtual nodes and links in accordance with nodes and links allocation strategy in section 4.2, respectively."
"the rest of this paper is organized as follows. section 2 introduces the system model considering the imperfect sensing. the ee power allocation optimization problem is analyzed, and we outline the proposed algorithm for its solution in sections 3 and 4. finally, simulation results are presented in section 5, and conclusions are drawn in section 6."
definition 2 (transition probability). the transition probability is the probability that the virtual node can be mapped to the substrate node . the ant's transition probability at time is related to the pheromone trail:
"step 3. obtain the fitness value fit for each artificial ant. if the fitness value in the current iteration is lower, the new mapping solution is better than the original. use the new mapping solution instead of the original optimal solution best ."
"request. similar to the substrate network, a virtual network request is also modelled as an undirected graph by v, is the node set that the virtual node corresponding to the substrate node with failure has been removed and the primary-cut set pcs v, ⟨, v, \\ ⟩ of the virtual topology exists at least one link not routed over any of the links passing through the substrate node with failure."
"the recovery rate of virtual networks, which is the ratio of the number of virtual networks that has been successfully recovered to the number of virtual networks that is affected by the substrate failure, is an important evaluation of reliable virtual network mapping. the recovery rate of virtual networks suc can be formulated by"
where vnr acc is the virtual network requests that has been mapped successfully. vnr is the virtual network requests that arrived. 1 is a positive number close to zero infinitely.
"(iii) finally, extensive numerical simulation results corroborate our theoretical analysis and demonstrate the effectiveness of the proposed method. we found out that opportunistic relay protocol as compared to always relay-aided transmission protocol in cr networks is able to achieve higher performance in terms of ee metric. furthermore, comparisons with relevant works from the literature show that the ee is slightly deteriorated if the su does not account for spectrum sensing errors."
"cognitive radio (cr) networks have emerged as an efficient solution to the problem of spectrum scarcity and its underutilization. this is achieved by granting sus' opportunistic access to the white spaces within pus' spectrum while controlling the interference to pus. orthogonal frequencydivision multiplexing access (ofdma) is recognized as an attractive technique for cr due to its spectrum shaping flexibility, adaptively in allocating vacant radio resources, and capability of analyzing the spectral activities of pus [cit] . incorporating cooperation into cognitive radio networks results in substantial performance gains in terms of spectrum efficiency (se) for both pus and sus [cit] . besides the se, the ee becomes a key issue for future wireless networks since energy cost imposes both financial and ecological burden on its development. ee power allocation especially is of crucial importance for cognitive relaying network design [cit] . this is because that high ee is a basic premise for sus to achieve high utilization of the limited transmit power which is consumed to not only improve se but also implement some additionally important functionalities, for example, spectrum sensing."
"(ii) we probe into the optimal power allocation scheme with incorrect sensing in corl system. on the basis of corl model, we proposed a novel eeoriented optimal power allocation iterative algorithm by exploiting the fractional programming and bisection method to completely solve the optimization problem, which reduces computation complexity significantly and yields a good tradeoff between ee and computational complexity."
(3) constraints (4) and (5) enforcing each virtual node in a virtual network request can only be assigned to one substrate node. any two virtual nodes cannot be assigned to the same substrate node. virtual nodes from different virtual networks can be assigned to the same substrate node.
(2) constraint (2) means that the rest available cpu resources of substrate nodes should meet the demand constraints of virtual nodes. constraint (3) means the locations of the substrate node loc( ) and the virtual node loc( ) should be at a distance within the range of ( ). ‖ loc( ) − loc( )‖ denotes the distance between and .
"the reliable virtual network mapping is one of the major problems in network virtualization researches. an efficient reliable mapping algorithm plays an important role in accepting more virtual network requests and generating more revenues for infrastructure providers. different from the traditional protection mechanisms and recovery mechanisms, we proposed a hybrid reliable heuristic mapping method based on the survivable virtual network concepts inspired by survivable routing in wavelength division multiplexing wdm optical network. we formulate the reliable mapping problem as an integer linear program and propose a virtual networks survivable strategy. we propose a node allocation strategy, link allocation strategy, failed node, and link recovery strategy and use the ant colony optimization algorithm to achieve the approximate optimal mapping solution. the simulation results show that the survivable virtual network is a better idea of dealing with reliable mapping problem and our algorithm outperforms the previous approaches in the acceptance rate of virtual networks, the average revenue of mapping, and the recovery rate. in the future work, we will focus on how to optimize nodes and links allocation strategies to reduce the time usage of mapping."
"during the execution of mapping, the rest of available cpu resources and location of the substrate nodes should meet the demand constraints of the virtual nodes in (2)∼(5). the virtual links can be mapped to the substrate links needing to satisfy the condition that the rest available bandwidth resources of the substrate links should meet the demand constraints of virtual links in (6) and (7) . the details of the hybrid-rhm-svn algorithm are described as follows."
"step 1. if the virtual node is assigned to the substrate node, calculate the primary-cut set pcs v, ⟨, v, \\ ⟩ of the virtual network with removal of ."
"inspired by the survivable routing in wavelength division multiplexing wdm (wavelength division multiplexing) optical network in literature [cit], we propose the concept of survivable virtual network, which is different from traditional protection mechanisms and recovery mechanisms. in wdm optical networks, substrate resources are allocated to different logical networks. any failure in the substrate network will lead to service interruptions. survivable routing in wdm optical network means when the substrate network failure occurs, the affected optical network maintains the maximizing degree of connectivity, which can maintain a tolerable level of service and the ability for quick recovery [cit] . in this paper, the survivable virtual network needs to consider not only the links mapping, but also the nodes mapping, which is more complex than the survivable routing in wdm optical networks. to make the virtual networks with survivability, the mapping of virtual network needs a survivable strategy. when the substrate network is under a node failure, the rest of the mapped virtual networks needs to remain connected to ensure the continuity of virtual network services and finally uses the node migration and link remapping to repair the affected virtual networks."
"where is a positive parameter and can be interpreted as a pricing factor for sus' power consumption. hence, another problem is formulated as op3: max (26), (27), (28), (29) ."
"it also classifies cuts of the graph into primary-cuts and secondarycuts based on the connectedness of the partitions. a cut is called a primary-cut if and only if both of the induced subgraphs are connected components. for the primary-cut pcs v, ⟨, v, \\ ⟩, the subgraphs that formed by node sets and v, \\ are connected components. this conclusion provides inspiration for virtual network survivability in our reliable virtual network mapping method."
"in this paper, adaptive power allocation is investigated to maximize ee for corl with spectrum sensing errors considered in the system model. to achieve the optimal solution, the ee-maximization problem is simplified and transformed into an equivalent concave form, and then we use the lagrangian technique to transform the equivalent problem into a corresponding dual problem. finally, optimal allocation algorithm is proposed. to the best of our knowledge, adaptive ee power allocation is investigated to maximize ee for corl considering spectrum sensing errors which has not been discussed in the literature. our main contributions of this paper are summarized as follows:"
". by the definition of ee function, reducing transmission power of the larger rate hop will increase system ee under the condition that the system rate is a constant [cit] . obviously, the original hypothesis leads to contradiction. therefore, the original proposition is true."
"is the gaussian tail probability, denotes a common threshold used across all subcarriers, 2 denotes the noise power received at each su, and is the sensing time used by the su when sensing the primary behavior in each frame. is the sampling frequency during the sensing time and is the pu's average snr. via (4), can be rewritten as"
"where, ( ) indicates the interference introduced into pu on subcarrier when st or sr transmits on subcarrier with unit transmission power [cit], and it can be expressed as"
"from op2, we observe that constraints are either linear or convex, but objective function equation (22) is not a concave function. actually, the problem of op2 belongs to the quasi-concave programming, which has been proved in our previous work [cit] . in the next section, we will show that 6"
"in this paper, we have studied the resource allocation problem for ee power allocation in ofdm-based cognitive opportunistic relay links with spectrum sensing errors considered. to maximize the ee of the sus under joint individual transmit power and interference constraints, we proposed an optimal power allocation algorithm using equivalent conversion and transform the equivalent problem into a corresponding lagrangian dual problem. the simulation results show that when imperfect spectrum sensing is not taken into account, excessive interference will be introduced to pu; however, the ee is about 6.5% larger than that obtained by ecorl scheme. meanwhile, the proposed strategy can improve ee significantly compared to the always relayaided scheme in cr networks, and the proposed algorithm exhibits a good convergence performance both theoretically and in simulation analysis. in the future, the energyefficient resource allocation problems for more complicated green cognitive radio networks (e.g., multiuser scenario with imperfect channel state information) should be considered."
"in order to reduce the cost of the substrate network, a hybrid reliable heuristic mapping method based on survivable virtual networks is proposed. in hybrid-rhm-svn, we formulate the reliable mapping problem as an integer linear programming model. firstly, we calculate the primary-cut set of the virtual network subgraph where the failed node has been removed. then, we use ant colony optimization algorithm to achieve the approximate optimal mapping. the links in the primary-cut set should be selected from the substrate paths that do not pass through the substrate node corresponding to the virtual node that has been removed first."
"where ( v, ) is the revenue of mapping one virtual network successfully. and are weights of cpu and bandwidth capability. 2 is a positive number close to zero infinitely."
(4) constraint (6) indicates that the virtual link can be mapped to the substrate link which needs to satisfy the condition that the rest available bandwidth resource of the substrate link should meet the demand constraints of the virtual link.
"the acceptance ratio measures the percentage of virtual network requests accepted by the mapping algorithm over a period of time. similar to the previous works, the acceptance rate of virtual network requests acc can be formulated by"
the revenue gives us a clear insight into how much infrastructure providers will gain when accepting a single virtual network request. the average revenue of mapping virtual networks rev denotes how much infrastructure providers will gain in the long run:
"international journal of distributed sensor networks we can obtain optimal solution of ee-maximization problem by exploiting special structure of the objective function. to this end, the monotonically increasing and strictly concave characteristic of the numerator (p s ) in (22) is summarized in theorem 2."
"from the above observation, we have that ( ) is decreasing in and is nonincreasing in successive iterations of ecorl. therefore, ( ) is nonincreasing in successive iterations. furthermore, ( [ ] ) does become zero from lemma 3, which follows that fact that ( [ ] ) does become smaller than . therefore, the ecorl algorithm will always converge to optimum."
"this lemma indicates that, at the optimal parameter *, the optimal solution to op3 is also the optimal solution to op2. hence, solving op2 can be realized by finding the optimal power allocation of op3 for a given and then update until (31) is established. for a given, the optimal power allocation can be obtained using convex theory [cit] because of the convex characteristic of op3. hence, the existing water-filling power allocation approach gives the solution to it [cit] . however, besides adapting the power allocation on all subcarriers, we need to consider international journal of distributed sensor networks 7 subcarrier transmission mode. the lagrange function for op3 is constructed as"
"let be the posterior probability that the su detects subcarrier as being used by pu which is indeed occupied. using bayes' theorem and the law of total probability [cit], can be derived as international journal of distributed sensor networks where,1 and,0 represent the events that pu is active and idle on subcarrier and̂, 1,̂, 0 are the sensing results that subcarrier is occupied or unoccupied by pu, respectively. is the posterior probability that the evidence subcarrier is really unoccupied given that su senses it to be unoccupied, which can be expressed as"
"in this section, a memory architecture using bics for protection against seus is presented to illustrate the failure mechanisms and provide a foundation for the reliability analysis presented in the following sections."
", which may be stored in different places in the network. therefore, the interest will be divided into several sub-interests before arriving at the destinations. every sub-interest requests for a subset i u of u ."
"by this way described above, an interest  , un will be divided several times before arriving destinations, and the sub-interests will form a multicast tree. suppose the tree has form in figure 1, where wu , the leaf nodes of the tree do not need to send interests to other nodes. these coded blocks will be sent back from the leaf nodes to the branch node. in the branch node, the coded blocks of   obviously, since the same coded blocks cached in different nodes may be responded to interests multicasted by the same consumer, the received coded blocks can't be cached in the branch node for satisfying future interests. therefore, each node only caches or stores original blocks. the received coded blocks will be used only for responding the pending interests and be collected locally for gathering enough coded blocks to decode more original blocks."
interests for sending out of two (or more) different interfaces of the node toward two (or more) destinations. the dividing operation is defined as follows:
"previous studies have focused on reliability analysis for memories protected with single error correction (sec) codes [cit] . typically, a code is applied to every memory word. the codes allow correction of single errors. consequently at least two errors on the same word are needed to cause a failure. a commonly used complementary technique is scrubbing. in scrubbing, memory words are read periodically and any errors are corrected. in this way, the accumulation of errors over time is avoided, thus minimizing the probability of failure [cit] . more recent research focuses on the effects of multiple bit upsets (mbus) [cit] ] on memory reliability [cit] . mbus affect bits stored on physically adjacent memory cells. the most common approach to deal with mbus in memories is to interleave bits such that the bits from a logical word are physically separated [cit] . this ensures that only one bit per word is affected by a single mbu event."
"a more elaborated model can be used in situations where the low arrival rate assumption is not valid. in this case, memory behavior can be represented by the markov model shown in figure 3 . the states correspond to different numbers of errors in the memory: s 0 represents zero errors, s 1 represents one error, and so forth. a transition to a new state is caused by an error arrival or correction of an existing error. for an arrival, the transition is to a state that has one more error, if the new error occurs in a block that has no previous errors, or is to the initial state s 0 if the error falls in a block that has an existing error. this latter transition models a failure as a restart of the system. for a correction, the transition is always to the state that has one less error. it should be noted that the correction time is independent of the accumulated number of errors."
"in a final experiment, the power consumption of the bics approach was compared to that of the traditional scrubbing process. the number of read cycles was used as a figure of merit for this study. in order to make the results comparable, the conditions for the experiment were selected such that the reliability of both techniques was the same. the techniques have the same reliability when eqs. (5) and (6) are equal. this leads to the following relation between t s and t c ."
"the data editor module supports workflows for digitizing label information from a specimen image. the label image (or label part of the image) is made available to the editor where optical character recognition (ocr) and automatic parsing of this information into database fields (natural language processing, npl) may be applied. voice recognition is currently in the experimental stage and may become available in the future for entering data. if a duplicate record was previously entered by a different institution, the system has the ability to quickly retrieve and import this information into the data entry form, thereby avoiding the need of retyping the same data for each duplicate specimen. this is a powerful feature for saving time and reducing errors in new entries as well as the existing one as they are collaboratively edited."
"the administration control panel (fig. 2) allows collection leaders to manage their collection's profile, statistics, permissions, mass specimen imports or updates, other batch processing, a complete and configurable provenance log of all occurrence record edits, darwin core archive (dwca) publishing [cit], data cleaning, duplicate clustering (e.g. for multiple herbarium specimens derived from an individual plant), and data back-up functions."
"many symbiota portals are experiencing rapid growth in collection and occurrence record numbers, due in part to digitization projects supported by the nsf-adbc program. jointly these numbers and trends speak clearly of an increasing acceptance of symbiota's bottom-up approach and flexibility in creating regionally or taxonomically themed biodiversity data communities."
"the rest of the article is organized as follows: in section 2, the bics-based memory architecture is described, providing the basis for the reliability models presented in section 3. in section 4, the models are validated with an extensive set of simulations, illustrating their use in selecting the optimal memory configuration. finally, in section 5 the conclusions of the work are presented."
"to evaluate the accuracy of the proposed models, an extensive set of simulation experiments was conducted. in the simulations, errors were inserted following a poisson process while for the correction time a uniform distribution with mean t c was used. corrections were performed one at a time so that errors on different blocks can accumulate, as discussed before."
"(1) :, ovm is the cumulative request contained in one or more interests received from interface i, and the node has not responded it yet."
"below we offer an overview of symbiota features available to data contributors who have been granted data editing rights beyond those of general users. the platform supports wide-ranging interconnections among management tasks, resources, modules, and the resulting floras and faunas produced by symbiota portal contributors. additional information regarding symbiota's data management functions is available at http:// symbiota.org/docs/symbiota-data-management-tools/. each symbiota module provides consistent editing interfaces employing intuitive icons and tabs. screenshot of the homepage of the seinet-derived san pedro riparian national conservation area checklist, a 'member' checklist of the arizona flora biotic inventory 'family' [cit] (makings, 2014 . checklist administrator functions are available in the top right corner (\"a\" -administration, \"v\" -manage linked vouchers, \"spp\" -edit species list). the selected screenshot shows the entire list in alphabetical order, taxon authors, common names, and notes & vouchers. clicking on vouchers listed for each taxon will display specimen details including images, when available. a google map thumbnail can be clicked for a more expansive map view."
"dynamic flora and fauna checklists. symbiota is designed to replace traditional, static floras or faunas with a dynamic approach. modules are provided for each information concept found in such tomes -checklists, keys, taxonomic treatments and taxon descriptions with distribution maps and images. linkages between the information content of each module provides for dynamic and user-driven data retrieval. natural history specimen and (where appropriate) observation records are the core of any symbiota portal, stored centrally in a relational schema based on the darwin core standard (http:// www.tdwg.org/standards/450/). a standard specimen search engine with auto-completion and pick-list functionality facilitates taxonomic and geographic searches. returned is a list of specimen records that match the criteria (fig. 5) and which is instantly downloadable."
solving the markov model provides the probability of finding the memory in each state. these probabilities can be used to calculate the probability of failure on the arrival of a new event as
"the main feature of the interactive keys is the capability to dynamically assemble character and character state information for any given taxon group combination. the module can handle vastly different sets of characters for disjointed taxonomic groups within a single dataset. it furthermore optimizes displaying only those characters and states that differentiate taxa under consideration at any given step in the identification process. with every decision a user makes during the keying process, characters and states are added and/or removed, starting with more generally applicable characters and gradually moving towards specialized traits (fig. 6 ). screenshot of results displayed based on the search criteria 'grand canyon' which returns 378 records in 142 taxa. the specimens panel (center tab) shown on the left presents an abbreviated summary view for each record that is expandable (not shown here). a species list (left tab), shown on the right and maps view (right tab, unselected), shown on bottom, are also available. a data icon in the top right corner facilitates downloading of the entire search results in darwin core or symbiota csv text file format (no permissions required). high-density occurrences of records are 'integrated' at coarser geographic scales and become resolved into separate latitude/longitude points at finer levels."
"notice that since t c is a function of the block size b, t s depends on b 2 . the number of read cycles was the following."
"the models were validated using a wide set of simulation experiments that illustrate their applicability. the influence of the size of the memory blocks that share bics on memory reliability was studied. finally, a comparison of the number of read cycles required for a given mttf level using scrubbing and bics protection was conducted. the results show the potential savings of the bics-based approach."
"the search also returns a taxon list for the chosen parameters; for instance, \"flora of arizona\", \"bats of the phoenix metropolitan area\", or \"lichens of the grand canyon\". any such checklist may be used within symbiota to dynamically generate other components of a flora or fauna; i.e., an interactive key, taxon descriptions, distributions and images. the rapidly deployable on-line feature based on actual specimen information is largely unique to this platform and can support answering many research questions, as well as determining under-or un-collected regions."
"symbiota adheres to the open source paradigm (http://opensource.org/osd.html; [cit] ). all software programs required to set up and operate new symbiota portals are openly and freely available. portal installation and data population instructions are provided via the symbiota software project website (http://symbiota.org). the symbiota code base is regularly updated and available through a subversion repository (svn) hosted by source forge (http://sourceforge.net/projects/symbiota/). although sourceforge has been the repository of choice, as the user and contributor community grows the code may be moved to github (https://github.com) for easier access and communication among developers. the symbiota cms is written uniformly in the server-side php integrated with client-side javascript, with a mysql back-end database. any php compliant web server can be used, yet the software has been most thoroughly tested using apache http server."
"symbiota uses a centralized, server-based infrastructure solution for multiple parallel or inter-dependent information communities. the centralized model enhances performance and furthermore answers directly to the needs of many natural history collections; in particular medium-or small-sized collections with limited access to it personnel and infrastructure [cit] . the challenge of constant upkeep and integration of physical server infrastructure, software and databases, data formats, and content is restricted to a single portal managing entity in the network; e.g. a major research university, museum, or long-term funded biodiversity informatics resource such as idigbio [cit] ). the portal-managing node assumes the responsibility for technical support of the network. all other portal member collections and all contributing individuals are freed from these tasks."
"for cpa, ndn usually use a signature verification mechanism to resolve it [cit] . ndn is named for content. for named content, the name and content will be bound together for certification [cit] . the signature formula is homomorphic signature on the transferred content blocks and names. thirdly, using dynamic public key technique, our scheme authenticates each generation without updating the initial secret key. finally, using homomorphic hash can verify the signature receiving by intermediate nodes or the destination nodes. our scheme is suitable for ndn, which can not only against cpa for ndn, but also effectively against intra-gpas / inter-gpas. therefore, our scheme should resist the cpa. it needs to guarantee the three content attributes of data validity, relevance and provenance in low communication overhead and calculation cost."
"portals encourage participation through shared branding and diversified forms of engagement. symbiota can leverage and enrich biodiversity data through the creation of portals led by thematically coherent research communities. these communities tend to have shared taxonomic and/or geographic concentrations; and consequently shared interests to work collaboratively towards virtual floras and faunas that constitute authoritative, high quality vouchered treatments of a region's biota. in tailoring towards such communities, symbiota has become a biodiversity information platform which is configurable, customizable, and independently manageable by each research community, and thus analogous to the scratchpads approach [cit] )."
"taxon profile pages are the central vehicle in symbiota for conveying information on a given taxon (fig. 8) . the editing interfaces allow information managers to: (1) provide synonyms and vernacular names; (2) view all annotated images (collection vouchers, specimens photographed in situ); (3) specify the sequence in which images appear on species profile pages; (4) add new images (either stored natively or just linked to the portal); and (5) add a (typically brief) natural language description of the taxon's morphological characters, including notes on natural history and distribution. multiple feature-based descriptions can be added (original, revision, etc.), [cit] ."
"readers interested in establishing new portals or joining existing portals can find more detailed, step-by-step instructions on the symbiota software project website at http:// symbiota.org."
"portal content management. user permissions allow for task-specific access control. for each portal, highest-level access rights are typically confined to a small group of portallevel super administrators who can assign initial rights to collection and project leaders. from that highest level, additional rights assignments can cascade down to the individual collection and project domains. this promotes arrangements where no single person has overwhelming responsibilities to manage rights, and no single-point bottlenecks are created in developing a portal's contributor community. 'global' or 'local' portal leaders can manage rights of their collection and project members at the appropriate levels."
"in contrast to traditional print publications, these on-line checklists are fully rooted in voucher data records. a suite of management tools are available to build checklists, link vouchers, and coordinate species lists with herbarium level specimen annotations that result in the correction of misidentified vouchers. maintenance of a set of checklist is facilitated by the ability to organize lists in a hierarchical relationship where parent checklists automatically inherit taxa and vouchers from all children checklists. this is particularly useful for creating integrated state and county lists; e.g., as currently in use by park service employees in the southwestern united states. mangers can keep track of area-specific vouchers without the need of explicit collection level permissions from each herbarium holding the specimens. the hierarchical relationship of the checklists results in consolidated lists and reports available to the regional managers that are based on the independent efforts of a network of local data managers."
"in this paper, we consider this kind of initiative cpa. for example, an attacker targets a specific router of r . attacker send a polluted content to the router of r, and it will be the router of cache. therefore, r receive a polluted content. the router will send this polluted content, leading to polluting the entire network, if it receives a real interest packet."
the reliability of memories protected with bics and a per-word parity bit was analyzed in this article. two models were presented which enable quick evaluation of the mttf. these models allow designers to select the optimal configuration to meet a given reliability level.
"beyond sharing core biodiversity data formats and tables, symbiota portals are customizable in numerous ways that suit specific performance and engagement needs of the corresponding communities. the concept of modularity applies to both the process of developing the software and the actual application instances. this approach promotes screenshot of the occurrence data form corresponding to the seinet specimen asu0023423 -echinomastus johnsonii (parry ex. engelm.) e.m. baxter (common name: johnson's fishhook cactus -pertaining to the asu vascular plant herbarium collection. this is the primary symbiota contributor interface through which individual specimen records are entered and edited. at the highest level, contributors can switch from the occurrence data tab to the determination history, images (see also right half), genetic links (e.g. to genbank; [cit] ), and administrative tabs. entry personnel can zoom in on the label. many data fields have inherent autocompletion or uniqueness/compliance checking functions, or can be expanded via the +-pencil icon for more fine-scale data entry. the latest identification data field section is linked to the taxonomic thesaurus, thereby ensuring correct integration of the scientific name with the portal-level taxonomy. georeferencing support tools including google earth mapping and an embedded geolocate module. under curation, the processing status may be set to (e.g.) \"pending review\" to support filtering and data quality control practices among collection members. interoperability and extensibility. modularity is manifested at different levels, as follows: (1) application modularity -the modules for managing specimens, biotic inventories, identification keys, and species profile pages are designed to function independently of one another. additional modules such as those supporting label image transcription (e.g., crowd sourced transcription, optical character recognition, and natural language parsing) or remote specimen identifications are turned on or off according to a portal's needs. custom front-end frames, logos, texts, rotating images, and interactive identification games represent additional configuration options. (2) data modularity -a portal's member collections are represented as independent units, each with its own management regime (fig. 2) . at the same time they are fully integrated through the underlying symbiota database installation. (3) portal modularity -several portals such as seinet, the intermountain regional herbarium network, sernec, and the madrean archipelago biodiversity assessment are branded as separate portals by their geographically defined project themes, yet they all share a single data source supplied as a network of replicated databases."
"correction will fail if a second seu hits the same block before the correction process finishes. if the second seu occurs in the same word as the first, no error will be detected by the parity check. if the error hits a different bit position in a different word in the same block, correction will fail because the system will be unable to determine which column fault corresponds to which word parity error, as shown in figure 2 . finally, if the second error hits the same column as the first one, correction will stop as soon as the first error is found and the second error would not be corrected. in summary, correction fails if a second seu hits the same block before the first seu has been corrected."
"the main difference is the b factor (block size). this is related to the fact that in scrubbing the second error has to fall in the word in which the first error occurred in order to cause a failure (considering sec protection). in the case of bics protection, failure occurs if the second error falls on the same block of b words where the first error occurred."
"named data networking (ndn) is one of the future internet architecture, which name is used for routing. and router for ndn can cache content, so that data transfer is faster and the efficiency of content retrieval is improved."
"symbiota plays a pivotal role in north america in mobilizing small-to medium-sized natural history collections to enable voucher-based biodiversity research. beyond recent funding successes, its impact is most significantly manifested in the creation, maintenance, and continuous expansion of strong portal communities over many years. this success has been due both to symbiota's integrated and stable web-based cms and the ability to effectively promote and support self-motivating collaborations at various regional and taxonomic scales. the balance struck in establishing fixed practices for data integration while remaining responsive to specific needs of research teams is also important in this context. symbiota maintains its niche in the biodiversity informatics realm through extensive collaborations and integration of functionalities developed by other projects, thus allowing its core modules to interact functionally and share data with many other systems."
"in what follows, we concentrate on symbiota's overarching principles, functions, and current and future applications. more detailed information on the functionality of particular modules (such as the interactive keys) will be treated elsewhere. we begin with reviewing the history of symbiota's origins which has shaped the overall design with regards to conceptual, technical, and sociological aspects. we then discuss modules for managing specimen and/or observation occurrences, biotic inventories, and identification keys each from the perspective of general portal users and contributors. we close with an overview of symbiota's current use and acceptance. throughout, we attempt to explain why symbiota has been embraced by its user communities, and lay out design and functionality trade-offs with relevance to larger-scale trends in biodiversity informatics [cit] ."
"collaboration eases individual efforts and leads to enhanced information quality. symbiota's primary focus is on permanently vouchered occurrence records of high scientific quality and accountability and on providing the tools to collaboratively manage this information. the platform has a modular framework for publishing biodiversity information; i.e., natural history collection occurrence records or observations, taxonomic information, images, species profiles, and taxon character and character state information in interactive keys for identification. access to modules is achieved by emulating the functionality of a modern content management system (cms). the system promotes a positive feedback loop that includes: (1) making data public instantaneously which can serve to expose errors; (2) using web messaging to alert responsible parties to such errors; (3) using web-based editing tools and workflows that allow such errors to be resolved as they are identified; (4) redirecting data repairs back to a source collection's internally used platform; and (5) rendering repairs permanent at the broader scale with the subsequent data update. in short, symbiota leverages a themed, collaborative biodiversity data mobilization approach towards improving the quality of individual collections' data [cit] )."
"the history of pre-symbiota development dates back to early digitization efforts and local solutions for database supported collections management. early technical collaborations involved the hypersql project [cit], the specimen management system for california herbaria (smasch; [cit] ), the central arizona phoenix long-term ecological research site (cap-lter; http://caplter.asu.edu/; [cit] ) and its information management system, and particularly the southwest environmental information network (seinet; [cit] . the user community -e.g., collections managers, taxonomists, ecologists, data entry personnel, programmers, informaticians, and students -have driven much of the overall software design philosophy and implementation."
"finally, biotic inventories are products generated by experts or expert teams and may integrate information entered, edited, and maintained in all other modules towards a regional flora, fauna, or both. dynamically generated as well as static species checklists may be included. static species lists give researchers complete and continuous control over species composition, taxonomic placements, region specific comments, and voucher assignments. one of the benefits of a virtual flora or fauna environment is the ability to establish all-inclusive species lists as well as a multitude of smaller lists covering more limited regions within the overall area of study."
"sustaining and growing this role is a challenge with scientific, technical, and socioeconomic dimensions that will remain relevant as long as the primary sources of support are competitive innovation and research grants. like any developing biodiversity data platform, symbiota is bound by the requirement to invest into continuous, adequate information technology and personnel infrastructure to remain useful [cit] . with the user community growing, symbiota must develop suitable business models that will allow portals to continue providing services by achieving sufficient revenue for sustaining the underlying infrastructure from the hub level down to the individual contributor [cit] . for the underlying infrastructure this may include incorporating more 'off-the-shelf' software components the will simplyfy and distribute code maintenance. many software solution have become available during the development of symbiota which will have to be evaluated for their ease of integration, e.g., user management and security, certain modules from other content management systems."
"in its current realization, symbiota is designed to support and promote grassroots biocollaborations that work towards efficient data mobilization, improving data quality, and describing biodiversity in the form of virtual floras and faunas. of particular importance is the emphasis on mobilizing information in order to address specific research questions, where the primary data are distributed among numerous collections. collections are regarded as scientifically interdependent. the need to address shared research objectives takes primacy in software development over the more traditional design focus on customizable collection administration (e.g., handling in-house locations and curation trajectories of physical resources or processing loans). although the focus on strengthening small grassroots collections collaborations still characterizes symbiota's primary niche in the biodiversity data environment today, its modular design has enabled symbiota to nimbly respond to changing needs and requirements, now supporting entire specimen digitization and collections management workflows plus extensive data exchanges with other systems."
"although the collaborative approach is emphasized, portals maintain the provenance of information and content authorship. all occurrence records and images in symbiota are indelibly tagged to their source collection, via a collection code and/or specific icon. each collection, in turn, maintains a separate portal identity and homepage that provides a summary of its holdings, members, additional contact information, and collection statistics -number of specimens, percentage of georeferenced specimens, images, number of species, etc. (fig. 3) . modularization facilitates customization while maintaining data integrity. symbiota strikes a balance between allowing portal communities to acquire distinct identities and functions while ensuring database integrity and consistency. the aim to network and integrate biodiversity data meaningfully sets limits to the degree to which portal configurations can vary. these limitations are most apparent in the format of the single occurrence data table (fig. 4 ) which varies minimally across portals and only in data fields not mandated by the darwin core. the occurrence data table is furthermore linked to a portal's taxonomic thesaurus; i.e. a continuously updated, (yet also) unifying reference classification with valid and synonymous names that subtends all taxonomically based search functions and output displays. contributors are not required to conform to conventions that are in flux (e.g., taxonomy), however the system encourages movement toward more unified information alignment. while joining a symbiota portal requires a pragmatic acceptance of these relatively stable and constrained data formatting and taxonomic content conventions, the prospect of wide-ranging collaboration also creates a motivation to depart from an individual collection's 'optimal' solution regarding (e.g.) data scan portal homepage for the arizona state university hasbrouck insect collection, which is being managed \"live\" [cit] . formats, content, and display options. in short, symbiota is not designed to facilitate 'limitless customization' of underlying database tables or functions, instead mandating a level of homogeneity that yields consistent information output across collections and portals and promotes best practices."
"the proposed correction process could be modified to avoid failure in some cases. for example, the entire block could be checked so that multiple errors in the same column are corrected. also, the order of arrival of column errors could be recorded and used to identify errors in cases where the second error hits a word after the word has been checked by the correction process but before the process for identifying the first error is complete. those modifications would provide correction in a few cases but at the expense of increased complexity and correction time. hence, they are not considered in this work. so far, the discussion has focused on a single block but a memory will normally consist of many of such blocks. during the correction process, another error may occur in a different block. in this case, correction of the second error can only start when the correction of the first completes. this complicates estimation of the correction time as it now depends on previous error arrivals. this will be discussed in more detail in the following section."
"the cms approach enables assignment of specific user permissions to share the tasks of data build-up and management of central resources. for instance, designated taxonomy coordinators may be responsible for the taxonomic thesaurus (see details below) to retain community-level acceptance and assure regular updates fostering the collaboration between taxonomic and regional experts. symbiota also facilitates extensive data exchange options with other systems. for instance, collections record data may routinely be provided to the global biodiversity information facility (gbif), taxon descriptions can be exchanged with the encyclopedia of life (eol), and data may be synchronized with local management systems."
"content pollution is an attack, in which an attacker injects fake content into the cache of router. specifically, we can divide this attack into two types. firstly, an attacker attempts to compromise the integrity of the content by injecting or modifying the content block of the data packet transmitted in ndn, which make consumer receiving invalidity data packet. secondly, the attacker attempts to modify the name of the data packet in the network, but the content block of the data packet is correct and has not been modified. in this situation, the attacker is to make consumers can't retrieve the required content in order to achieve a denial of service attacks, or to make consumer getting irrelevant data packet, because the name and content do not match. the purpose of this attack is that legitimate nodes can't identify these faked content in data packets. due to the propagation mode of the ndn, these polluted content packets will pollute the information flow of the whole network."
symbiota provides attractive taxon profile pages with natural language descriptions as well as images for each taxon. an occurrence map generator utilizes information from the collections records to dynamically produce google distribution maps. links to external resources (such as the encyclopedia of life) and google searches for additional web resources are supported.
where the second term is the probability that the second error falls in the same block as the first and therefore causes a failure. from eq. (3) the mean events to failure (metf) can be calculated as
"this means that the approach based on bics is effective in terms of reducing the number of read cycles used for protection, leaving more bandwidth for data operations and consuming less power. in figure 12, the ratio r is depicted for several values of the block size b. it can be seen that, for the bics approach, the smaller the block size is, the fewer read cycles are required since less words in the faulty block need to be checked in order to identify the error."
"for many years, soft errors have been a major concern for circuits that operate in harsh environments, such as space [cit] . due to technology scaling, soft errors are also becoming an increasingly important factor in terrestrial applications [cit] . one type of soft error is the single event upset (seu) [cit] . these errors cause the value of a register or storage element to change. when an seu affects a memory device, the error may lead to system failure. given the broad use of memories in electronic systems, their reliability is of major concern. consequently, the effect of seus has been widely studied in the literature."
"rsnc-based multisource content delivery in ndn, data chunks for a content item can be received from multiple sources. there are two ndn packet types: interest and data. the header of interest packet contains three fields: content name, selector and nonce. in our model, the selector needs to carry the constrain information, including generation number, the set of the original block indices and the required number of linearly independent coded blocks. data packet contains four fields: content name, signature, signed info and content. the signed info needs to carry the coefficient of the linear combinations and generation number. the content carries the original/coded content block. for convenience, major information of interest packet and data packet show as follow:"
"for the markov model to be applicable, the distribution of arrival times and correction durations should be exponential. in our case this is true for arrival times, as a poisson distribution has been assumed. however, it is not valid for correction durations. these are uniformly distributed between the best and worst case, as discussed before. therefore, the markov model only provides an approximation that can be used to obtain an estimate of the mttf in cases for which the simple model is not valid."
"in this section we describe functionality that any user without login credentials may access in a symbiota portal [cit] b) . a simple name/email account creation and login process that requires no human approval will grant users additional permissions to comment on records, maintain their own private or public species lists, and contribute to data transcription efforts."
"the last type of data collected in this study is related to the interaction and collaboration that occurred between students and their collaborative friends. table 3 shows how students used instagram for collaborative language learning purposes. from table 3, it is shown that the students interacted with their group members to discuss content shared by each student. for example, for paraphrasing sentences, they uploaded the sentence to get feedback from their collaborative friends. this shows the interaction between the content sender and his collaborative partner through instagram social media happened."
"meteorological data used as forcing input for the teb surface model come from the nancy weather the first step in our experimental study is to assess the magnitude of the traffic impact on the road surface temperature. figure 7 indicates the rst of an area without traffic and the one subjected to traffic. it is noted that outside peak hours between 8 pm and 6 am rst curves merge for the two zones. this reflects the reduced traffic flux input. however, during the day, we found that the rst of the area subjected to traffic is greater by 1 to 3°c with respect to the non-circulated one. the higher 450 the traffic (especially during peak hours), the larger the gap between the two rsts. the preliminary result of this experimental study confirms those reported in the literature [cit] . firstly the rst differences do not only exist between an urban configuration and a rural one. the rst is also greater in a zone subjected to traffic with respect to another one that is traffic-free. this was observed in a full urban configuration. there is a clear relationship between 455 hourly variation of thermal traffic contribution ( figure 3 ) and hourly rst variation too."
"(6) energy constraint. the energy consumed by satellite activities can be expressed as a correlation function of observation time, sensor side swing and on and off processes. the consumption cannot exceed the maximum limit p j . p j is defined as the energy consumed per unit time of observation, q j is the energy consumed per unit angle of side swing, and o j is the energy consumption associated with turning on and off the machine:"
"having investigated students' knowledge and experience using instagram for mcll, this study sought to explore the content that they had produced and shared as shown. table 2 shows that the students uploaded and shared their content to meet the requirement of mcll. the first type of content is related to the english structure. they selected their sentence and took a photo of it. their content was shared on their own account. in the caption section, they invited their peers to analyze sentence structure available in their shared content."
"t ime coincidence resolution is one of the most important aspects of pet systems. traditionally, finer resolutions have allowed a tightening of the coincidence window for event acceptance, yielding a better noise equivalent count rate (nec) for cleaner reconstructed images. the current surge in popularity of time-of-flight (tof) pet systems [cit], where time difference is used to estimate the radiotracer position along the line of response, imposes a much more stringent limitation on timing resolution. a coincidence resolution of 600 ps or better is estimated to be necessary for a modern tof detector [cit] . coincidence is given by the difference between timestamps assigned to single gamma events on different acquisition boards in the system. naturally, synchronization errors between timestamping boards need to be well below the coincidence resolution, since any discrepancy between reference times on acquisition nodes is directly reflected on the measured difference. hence, an accurate system-wide synchronization scheme is mandatory."
"that conflict with the current mission t', makes the first mission in the conflict set as the current conflict mission t, and re-searches its available time window. if the current conflicting mission t can be moved to a new window, mission t' is inserted into the current time window, and the conflicting mission t is moved to another time window for execution. if the current conflicting mission t cannot be moved to a new window, then the process proceeds to step 4."
"mobile assisted language learning (mall) has been a burning issue among foreign language teachers especially efl teachers. mall opens an opportunity for teachers who are digital immigrants, to explore smartphone features for their students in learning a foreign language. language teachers have found possible benefits from mobile devices compared to a fixed desktop computer in boosting students' motivation and performance. [cit] focused on possibilities, benefits, challenges, motivation and the effect of mobile devices used to support foreign language learning. [cit] regarding student perceptions, the use of several tools for language learning, and the dominance of english for foreign language learning using mobile devices [cit] ."
"fuel consumed by the vehicle is transformed into different types of energy necessary to operate 300 the vehicle. most is transformed into kinetic energy for the vehicle to run and electrical energy for the battery and all the electric components of the vehicle. the other portion of energy producted by vehicle is transformed into heat flux generated by the engine and the exhaust system. [cit] assessed the heat flow generated by the engine s m (w m −2 ) and exhaust system e ex (w m −2 ), explained by the following equations:"
"the advancement of information and communication technology (ict) has inspired many foreign language teaching experts and practitioners to develop language learning and teaching like flipped learning [cit] ). an ict like a smartphone offers students benefits that are available from its features and applications facilitating language learning both in the classroom and beyond [cit] . on the part of students, direction and training on the use of mobile devices to support their language learning process need to be addressed. [cit], when smartphone use for language learning is properly tailored, students may reap many features and benefits to support english language learning both in the classroom and beyond its wall."
"concerning research question three, this study found that students interacted and collaborated with their peers. using instagram, they produced content to accomplish tasks in mcll. a social media platform has the power to facilitate interaction and collaboration among students to complete the task [cit] . collaboration occurred through instagram. [cit], social media provides opportunities for students to conduct mcll. english learning needs to use several approaches that enable learners to produce authentic foreign language according to the needs, time and real context of life. the design of task-based language learning allows students to simultaneously use language while completing tasks."
"in contrast to the traditional scheduling model, dynamic emergency scheduling is mainly for non-periodic missions with uncertain expected completion and arrival time. under the conditions of satisfying the satellite observation constraints and mission requirements, this paper builds a multi-objective mathematical programming model for multi-satellite mission scheduling under emergency conditions, so as to maximize the objective of mission scheduling and optimize the observation plan."
"our study aimed at describing this traffic effect during the winter season on the pavement energy balance. this involved integrating a theoretical traffic description into the teb numerical model dedicated to an urban configuration, and then to quantifying how much the traffic energy input af-95 fects the rst both on the basis of field experimental measurements (weather, traffic) and numerical experiments. the town energy balance (teb) model aims to the parametrize the interactions between the town and the urban atmospheric canopy, and is valid for a grid mesh larger than a few hundred meters."
"students not only produced and shared content but also showcased it in a multimodal way. they shared content in the form of video, images, and captions. such content can be called multimodal. their content was designed by combining the sharing of file types such as audio and video texts that can be studied by language learners [cit] argue that to produce the content, students needed to spend their effort, creativity, and other literacies and combine them to apply multi-literacies to produce a particular content using social media."
"the physical processes involved in modeling the road surface energy balance by the teb model are summarized in figure 2 . in this configuration, the road surface energy balance is expressed by the following equation:"
"according to guibet [cit], the nhc (j kg -1 ) is equal to 42700 for gasoline and 42600 for diesel. the fuel density fuel (kg l -1 ) is equal 0.775 for gasoline and 0.845 for diesel. the average fuel consumption fe (km l -1 ) depends on the type of fuel and on the type of traffic. in the study made by colombert [cit], fe is of the order of 8.5 km l -1 (this includes among things 205 others over-consumption due to air conditioning: 3.1 l.100 km -1 for gasoline cars in the urban cycle and 3.2 l.100 km -1 for diesel ones). according to the values from the literature [cit], an average q v value of 3903 j per vehicle travel distance was selected, which corresponds to an energy per second for a given average vehicle speed. [cit], the instantaneous flux of heat generated by traffic can be 210 evaluated by the following equation:"
"this equation is valid for an extended temperature range [cit] . α tp is the heat transfer coefficient between the tire and the road surface (w m −2 k −1 ), t t is the tire temperature (k) 260 and rst the road surface temperature (k) as mentioned above. [cit] showed that the tire temperature depends on the ambient air temperature and the vehicle velocity. for a velocity lower than 70 km h −1, the tire temperature is expressed by the following equation:"
"based on observations, this study obtained some data concerning students' participation using instagram. students took the course and they were introduced to the notion of instagram use for language learning. focus group discussion was also conducted to investigate their knowledge of instagram and the use of it in their daily life. having undergone a training session, they also learned to use instagram to support mcll and finally worked with their group to accomplish tasks. it is shown in table 1 . table 1, it can be seen that the process of students' participation in using instagram to accomplish mcll underwent several stages, each with its own activity on the part of students. when it came to discussion of the possibility of using instagram to facilitate language learning, there were several findings. the notion of instagram use for language learning was not familiar to most of the students. they were aware that they could access several sources for english learning on instagram. they mentioned several newspapers, tv stations and influential people saying that instagram had benefits for language learning. on the other hand, they did not have the experience of how instagram could be used to support their classroom activities."
"during the simulation period [cit] /08/24 00:00 [cit] /08/24 14:00), the satellite parameters were imported from stk, and the visible time window can be obtained by visibility analysis (table 10 ). for the initial scheduling plan, the priority of each initial mission was calculated. then, to maximize the objective function in section 2.3.2, the hybrid genetic tabu search algorithm was used to solve the problem. by comparing several experimental results, we choose the following settings for this scenario. the population size of genetic algorithm was set to 50, the crossover probability was 0.6, the mutation probability was 0.1, the maximum number of iterations of genetic algorithm was 300; the length of tabu table was 10, the neighborhood size was 10, and the maximum number of iterations of tabu search was 200. then, the initial scheduling plan was obtained, as shown in table 11 ."
"analysis of the rst_teb_a2 shows that the rst forecast is improved by 2°c to 3°c with respect to rst_teb_ic. this improvement primarily reflects the impacts of traffic on the rst and also that the configuration with which the traffic was introduced into the teb model seems more appropriate for the case of winter season. although the experiments were conducted above freezing, rst is still underestimated and might lead to false alerts with respect to ice occurrence. this could 545 be critical in the early commuting hours of the day, and some work is still needed to improve the mitigation of road hazards due to iced roads."
"a mobile phone is closely related to social media applications for youth, especially students. social media is the most prominent category of applications used by teenagers who are digital natives glued to their gadgets and who spend most of their time navigating social media [cit] . it is an application that can both be used to interact and share content at the same time with other users. [cit] stated that this platform attracts language education experts who are interested in conducting studies on the potential and benefits of social media for foreign language learning. the top three social media applications which are used worldwide by millions are facebook, twitter, and instagram."
"journal on english as a foreign language, 9(1), 87-106 copyright © 2019 [cit] -1657 e-issn 2502-6615 learning process in translation courses [cit] . one such effort is to offer social media for mcll. in this study, students used instagram not only for searching content from other accounts, but also created and shared their own content suitable to fulfill their language learning needs. this finding confirms a previous study by özdemir (2017) that suggests social media can be tailored to support language learning."
"fixed phases. moreover, the phase relationships are different each time the system is reset. hence, an additional step is required in order to correct this mismatch. instead of phase shifting the clocks, we act directly on the timestamp counters: a fractional part is added to the local timestamp, so as to account for the phase differences. every module runs its own timestamp counter, even concentration modules. a timestamp counter updates the integer part as usual, but the fractional part is managed only by the synchronization algorithm. by adding the fractional part to the event timestamp values in the acquisition modules, the effect of varying adc sampling clock phases is compensated when computing event time differences."
"to introduce the energy provided by the traffic in the teb model, we should distinguish between the sensible and latent heats. [cit], q traffic was then 220 partitioned into sensible and latent heats, respectively represented by the following equation:"
"the radiative heat flux r v (w m −2 ) emitted downward from the bottom of a vehicle has been studied by several authors [cit] . these studies reported that radiant heat from the bottom of a vehicle significantly affects the heat balance on a road surface, and may be evaluated by the stefan-boltzmann law:"
"trigger signal delay from the digitization point to the timestamping algorithm block may be different for each acqui sition module and hence must be taken into account in order to avoid timing errors for coincidence detection, moreover, this delay may contain non-deterministic components such as adc delay and fpga transceiver latency, unless a specific deterministic latency mode is selected for the transceiver. in order to compensate for this effect, an analog linear ramp gen erator with digitally controlled charge and discharge signals is implemented on board and sampled by an adc channel, with the goal of estimating the delay."
"the subjects were 110 second-year students in the english education department, faculty of tarbiyah and teacher training, universitas islam negeri (uin) sunan gunung djati bandung. there were three classes and all students took the translation ii course. this course was aimed at equipping them with the theory, knowledge, and skills to translate indonesian text into english."
"different values of fj from 0 to 700 ps were measured. the reason for this variation is currently under investigation; one possible explanation would be that the constant transceiver latency components in (7) were not, in fact, constant across different stratix iv gx devices or across different transceivers within the same device. this would imply the need of a one time calibration of the system for the measurement of fj."
"the comparison between field measurements in nancy and simulation results of t air with the teb model in its initial configuration (ic) is illustrated in figure 8a . at nighttime, there is no traffic in 475 rue charles iii, and teb provided results in good agreement with field data."
"in urban areas, besides meteorological parameters, the rst is also influenced by the buildings configuration (percentage of buildings, building heights, widths of roads, type of materials used, etc.). specific configurations where buildings are present everywhere in an urban environment, or totally absent, though not applicable in all urban, were tested to evaluate the sensitivity of the teb 565 model to this parameter. the results are shown in figure 12 . it is found that without building the rst decreases by 0.5°c, especially at night. this can be explained by the nature of the building materials that store heat during the day and restore it at the night along with the absence of a radiative well created by buildings. in the absence of buildings, the heat transfer phenomenon is absent."
"this study falls into the category of research on mall trends in efl teaching and learning [cit] . it investigated students' experience in using instagram for mcll in an islamic higher education context. [cit] showed that in islamic higher education institutions, a mobile phone is a daily need for students and if properly tailored, it can support english language learning. students in this study agreed that their mobile phone was the most frequently used gadget in their daily life. in their opinion, compared to other gadgets, a mobile phone excels in term of communication and information delivery [cit] . they also agreed that a mobile phone offered them easy access to language learning, anywhere or anytime."
"we remark that the delay value d contains not just the considered delay from adc to timestamping logic, but also the propagation delay from the fpga's delay estimation logic to the analog ramp circuit and to the adc input; however, these additional components can be considered to be equal in identical acquisition modules, so they are canceled when computing timestamp differences."
"obserangle(t i,s j,tm) the observation angle, which can be calculated from the geographical position of mission t i, the path of satellite s j and the time tm."
"the satellite mission scheduling is in reality a multi-objective optimization problem with multiple constraints. during emergency response, it not only requires that the completion rate of key emergency missions is high and the mission revenue is large, but also needs to ensure a high scheduling efficiency for each mission to optimally use satellite resources. based on these consideration, we use a linear weighted sum method to build optimization objective function for solving this model:"
"two experiments were then conducted. they consisted in continuously monitoring all the parameters described above over a period of up to 48 h on the same locations and on two distinct dates, and with a variety of weather situations corresponding to an approaching winter."
"as indicated before, the teb model provides an average rst and does not distinguish between an area subjected to traffic and another one that is not."
"after all links are established, the main clocks in every module have exactly the same frequency jclb but different, clock recovery circuit using an external pll with vcxo. the pll loop is closed after the recovered clock from the transceiver becomes available."
the fpga continuously computes the linear regression coefficients corresponding to the last m samples from the ramp signal y [n]. this can be accomplished in an efficient way by iteratively computing the first two moments of the sample interval as
"r nl (w m -²) and r ns (w m -²) are respectively the net of long and short wave radiation received by the road surface. r ld (w m -²) is the downward long wave radiation, r lu (w m -²) is the long wave upward radiation, r sd (w m -²) is the downward short wave radiation and r su (w m -²) is the upward 145 short wave radiation."
"guided by the application requirements of emergency observation, this paper focuses on the key technologies of the multi-satellite dynamic scheduling problem and employs the proposed mathematic model and algorithm in an automatic mission control system for constellations of satellites. the overall objective of this study is to design a mission-oriented satellite scheduling architecture under emergency conditions based on the following goals: (1) to design priority evaluation criteria for emergency missions. the seven impact factors that influence the mission priority, including imaging mission level, type of observation images, visibility of target to satellite, execution urgency degree, type of mission, mission conflict degree and revenue of imaging mission, are taken into account to obtain the priority level of mission, which is then used as heuristic information for solving constraint scheduling problem; (2) to analyse the main constraints of satellite scheduling and the objective function in detail. each term of the objective functions is maximizing the mission priority, maximizing the mission revenue and minimizing waiting time for missions that require an urgent execution time. then, the hybrid genetic tabu search algorithm is used to obtain the initial satellite scheduling plan, so as to avoid useless iterations when close to the best solution in the late stage of algorithm implementation; and (3) to develop a dynamic scheduling algorithm based on mission priority to solve the scheduling problem in case of newly arrived missions. in this approach, newly arrived missions are added into the initial scheduling plan to obtain a new scheduling plan. the validity of the algorithm is verified through experimental simulations, and the feasibility of the emergency observation mission scheduling model is verified based on a model performance evaluation function."
"considerable effort has been devoted to meteorological forecasting of these adverse weather conditions, particularly for road freezing conditions [cit] . to forecast rst, winter maintenance operators rely on numerical 30 models. improvement of these models consisted in producing a forecast for a full network by incorporating the influence of both meteorological and geographical parameters. however, traffic has so far been a challenging parameter to include in rst forecasts [cit] . in the present study, we will be interested in taking into account the impact of traffic in modeling the rst. a short literature review on the thermal effect of the traffic will be presented to identify and to quantify 35 these impacts. a model dedicated to an urban configuration was chosen. the heat fluxes associated with traffic were investigated in details for their introduction into this model. the modification in the energy balance caused by the presence of vehicles was then evaluated. compared with initial traffic implementation in the model, two different approaches were considered. the first consisted in improving the evaluation of the heat flux released by traffic. the second was based on an explicit 40 representation of traffic within the model. forecasts and field results will be compared and discussed."
"the teb model simulates an average rst. it does not distinguish between an area impacted by passing vehicles and another one without traffic. in order to compare the results provided by the teb model with field data, we calculated a weighted average rst. in the following text, the measured road surface temperature rst_measured corresponds to this weighted average rst according to the 460 following relationship:"
"this architecture is indefinitely scalable and admits an arbi trary number of detectors, as long as the data link bandwidth is enough to support the transmission of all coincidence data at the top level. since all modules are identical copies of one of two different designs, all hardware is fully reusable in the case of system expansion (increase in the number of detector modules) or any other topology change."
"traffic also impacts the energy balance by an intermittent interruption of the radiative flux towards the surface of the road. this phenomenon is called vehicle shield and depends on the traffic parameters. vehicle shield firstly prevents the incident solar radiation from reaching the surface of the road. it consequently leads to a loss of energy on the surface energy balance, and secondly it blocks the radiation emitted by the road surface. this physical traffic process can be evaluated by 320 a shield effect coefficient c shield (dimensionless number). [cit] and can be defined by the following expression: traffic influences the heat transfer between the road surface and the surrounding atmosphere by increasing the aerodynamic resistance of air. this process has been studied by several authors and different approaches were used to evaluate it [cit] . [cit] illustrated by the following 330 equation:"
"for youths and teenagers, the most widely used gadget is a mobile phone. compared to other devices, a mobile phone caters to their need to search for information and communicate with their peers. [cit], this tool helps them to learn reading and grammar as it provides a great experience in their language learning. it not only helps students to accomplish the task of searching and communicating with friends in a foreign language for an educational purpose, but also serves as an additional tool to a laptop device while they complete this simple task [cit] . furthermore, its existing applications benefit students by helping them deal with language components. in other words, students have a powerful device to navigate and explore and to support english learning-related activities [cit] ."
"students need to improve their skills that are crucial in the 21 st century for them to strive to meet future challenges. one of the skills they need to harness is collaboration. given the rapid development of mobile phone use and internet expansion, the notion of online collaboration in language classes has attracted researchers and practitioners. to achieve online collaboration in a language class, a teacher may assign tasks or a project to be accomplished by students individually or in a group. the assignments and project groups can be done online using their mobile phone to provide opportunities for students to conduct collaborative language learning online [cit] . in the context of english language learning, teachers need to use several approaches that enable language learners to produce target language in order to accomplish tasks and projects. language, 9(1), 87-106 copyright © 2019 [cit] -1657 e-issn 2502-6615 91 in a collaborative language learning context, students should be assigned a task allowing them to collaborate to accomplish it. a task or project can be designed for language learners to accomplish using their mobile phone. students may use their mobile phone to both communicate with their peers using an application and to accomplish the task at the same time through their participation and interaction online. [cit], these practices have been growing in language classes and published in various journals under the scope of mobile collaborative language learning social media has the power in terms of features to offer both language teachers and learners the chance to implement mobile collaborative language learning (mcll). language learners can explore social media features to communicate with their peers and interact to create a particular content meeting the requirement of a particular task given by their teacher. their interaction using social media can boost their performance to produce a completed task and accomplish mcll [cit] . in addition, social media platform has the power to facilitate interaction and collaboration among students to complete the task in mcll [cit] ."
data were collected by giving participants tasks to accomplish and then observing their activities. there were 4 types of tasks. each was designed for instagram and aligned to the objectives of the translation ii course. this study adopted research steps for online collaboration [cit] .
"in this section, we have conducted several numerical experiments to verify the effectiveness of the proposed method. for the model, 24 scenarios were setup and the number of initial missions contained in these scenarios changed from 25 to 200, with an increase step of 25. the corresponding number of newly arrived missions changed from 5 to 75, with an increase step of 10. simulation experiments were conducted in scenarios with 3, 4 and 5 satellite resources. the frequency of the insertion, reallocation, replacement and deletion operations for emergency missions in different scenarios was counted to obtain figure 5 ."
"step 1: initialize the parameters. the missions in the new mission set t' are sorted from high to low according to mission priority, so that the mission with the highest priority in t' is the current scheduling mission t' with available satellite resource set s."
"the algorithm can be refined by taking the skew between both half-links into account. let us split each half-link's data path latency into components, as shown in fig. 6 . the full data path between the synchronization protocol's digital logic standard two-frame synchronization procedure. local arrival and departure timestamps are recorded and then used to compute the round-trip time."
"with respect to research question one, the findings of this study show that students could participate in mcll for the translation ii course. they had the ability to use instagram to support the educational need to complete the course tasks. this is in line with previous research findings on students' ability to use instagram [cit] both in the classroom and outside the classroom in order to exchange information for language learning purposes [cit] . various efforts need to be made to improve students'"
"(1) imaging mission level. in the process of emergency response, different emergencies can be divided into four categories: natural disaster, accident disaster, public health incident and social security incident. according to the nature, severity, controllability and range of influence, it can be divided into four levels (i-iv), and the emergency response mechanism decreases from level i to level iv. therefore, the corresponding imaging mission level is divided into four levels, and the missions with high level of this factor are given high priority."
"where n is the total number of actual scheduled missions after scheduling plan adjustment, n' is the total number of missions, num(t c ) is the number of missions that the initial scheduling plan changes due to the newly arrived missions in dynamic scheduling, and num(t q ) is the number of initial missions."
"this research applied qualitative methods to investigate instagram use to support mcll among students of an english education department in an islamic higher education context. case study research design was used to explore the student's experience. specifically, the study aimed to investigate their instagram use, the content produced and shared, and interaction and collaboration in order to accomplish mcll."
"where d + i and d − i are the positive and negative distances between mission t i and the ideal solution i, and n is the number of impact factors."
"all the references quoted above are related to the winter season and show that traffic has a significant effect on the rst, especially near traffic signals and/or on roads with a high density of traffic."
"the bibliographic quoted above in the state of the art section indicates that traffic has a significant effect on rst. our interest is then to integrate traffic parameters in modeling the road surface en-180 ergy balance and to evaluate the effects of these energy inputs of traffic on the rst. to do so, two approaches were then considered."
"this study found that students produced and shared content calling for peer response, which ensured interaction took place. for example, a student shared content asking his peers to provide comment where paraphrasing should be addressed. this ensures interaction and it invites peer feedback. [cit], continuous interactions can be increased and these comments are good indicators of online collaboration. such interaction using social media can boost students' performance to produce a task [cit] . in addition to interaction among peers, this study found students collaborated. their collaboration could be seen from the number of comments made about particular content. collaboration happens when interaction is plural. this is to say that the sender of the message gets more peer feedback from other colleagues and in addition the sender sends feedback to maintain interaction. [cit], more interaction brings more chance for collaboration. a large number of comments from a collaborative friend helps the task gets done right away. the contribution of many comments will give input to students completing their tasks [cit] . in addition, providing comment and receiving comment in an online interaction may support students' independence and reduce teachers' dominance in the classroom [cit] ."
"(2) type of observation image. satellite scheduling for emergency missions involves multiple types of remote sensors, thus different images can be selected by users according to their needs. therefore, the types of remote sensing images can be employed for calculating the priority level of"
"synchronization of all acquisition modules with sub nanosecond resolution directly over data links is one of the key components behind the proposed architecture. in our previous paper [cit], the general theory behind this method was exposed, as well as implementation details for xilinx fpgas. here, we summarize the most important aspects and explain the differences in implementation when using fpgas from altera."
"unfortunately, the time difference distribution was not cen tered around o. the mean value fj was always fixed for a given board setup, but it depended on the choice of two boards used (out of 3 available) and also on the choice of inter-module link in each board (each board has two available connections)."
"(7) satellite payload action constraint. for sensor s j of satellite s j, the time interval of continuous observation missions cannot be less than the time of side swing adjustment and the time of stability after a side swing. v j is the speed of the side swing, which is simplified to a constant speed, and ts j is the time of stability after a side swing:"
"the improvement provided by this first approach is to have the traffic heat input variable according to urban traffic characteristics (volume, vehicle velocity and density). the greater the traffic, the lower the speed, and the larger its energy input. therefore, the heat flux generated by the traffic would no longer be considered as a constant throughout the whole period of the simulation. in addition, this approach allows us to test the teb model sensitivity to the variation of the traffic heat 195 inputs."
"at this point, the logic waits for m cycles until a linear fit y [n] � aln + ao of the ramp waveform is obtained. the measured delay is then computed as"
"in the following paragraphs, we have attempted to summarize the different approaches founded in the literature and which have been analyzed in order to identify and to evaluate the different thermal traffic processes. once the physical phenomena have been identified, a choice was made on the equations used to describe them, and their adaptation for their integration into the teb model."
"this configuration was not adapted to a specific rst forecast. the traffic energy input is not only involved in calculating the total heat flux generated by the urban canyon, but it also affects the road energy balance. furthermore, this heat input is not constant and depends on the traffic characteristics (volume, vehicle velocity and the daily distribution density)."
"due to the limitation of satellite resources and different users' needs for remote sensing data, the priority level of emergency missions varies. by analyzing the application requirements of emergency missions, the following seven impact factors are considered in setting the mission priority (table 1) ."
"to lower level concentrators and/or adquisition modules fpga (altera ep4sgx 110) i.e. the time when the ramp takes value b, using the charge signal trigger as the time origin. this measurement is repeated continuously, and the moving average of the last 256 delay measures is used as the valid delay estimation. this delay value is subtracted from the timestamp for all detected single events."
the top concentrator module implements a gigabit ethernet connection for communication with external processors. this is used for the transmission of detected coincidences and for configuration and control commands. the fpga in each module contains a system on chip (soc) with an embedded nios ii processor which handles these commands and relays them to lower level modules whenever appropiate.
an experimental study was conducted to quantify the anthropic energy flux of traffic impact on rst in the winter season. it indicated an rst increase by 1°c to 3°c with respect to the absence of traffic. additional work was undertaken to evaluate to what extent an accurate description of traffic might improve the teb numerical model when dedicated to rst simulations. two approaches to traffic integration in this model were detailed and tested.
"is generated for some amplitude a and time shift k, and its zero-crossing point is computed by linear interpolation on the clock interval where b [n] changes its sign. by using either"
"in a focus group discussion, students discussed what features available in instagram could support language learning. they learned how to integrate instagram for mcll in their translation course. they agreed that video and photo content offered by instagram might facilitate language learning. in addition, the caption feature could improve writing skills. they were ready to use those features of instagram to conduct group work and produce content to accomplish mcll."
"gain, offset, diff. amplitude or charge signals, we obtain digital implementations of the cfd [cit] and arc (amplitude and rise compensated) [cit] methods, all data from a detected event is collected into a 160-bit frame containing a timestamp with 1.6 ps resolution and sent upstream to a concentrator module,"
"the storage capacity of satellite s j, which is occupied by the observation mission t i arranged between any two data transmission windows w 1 and w 2 . it can be calculated from the planned observation mission ti and the speed of reading and writing v j of satellite s j ."
"a 22 na point gamma-ray source was placed between both detectors, at 5 mm and 690 mm distance, respectively. a pmt detector was placed on the far side, used primarily for electronic colimation: coincidences where the event on the far detector fell outside of the center region were filtered away. on the close side, pmt and sipm detectors were tested. the close detector was mounted on a translation table in order to have the gamma source imping on different, known positions on the crystal. the test setup is shown in fig. 9 . coincidence detector (pmt) fig. 9 . detector setup for coincidence measurements."
"(1) imaging mission level. in the process of emergency response, different emergencies can be divided into four categories: natural disaster, accident disaster, public health incident and social security incident. according to the nature, severity, controllability and range of influence, it can be divided into four levels (i-iv), and the emergency response mechanism decreases from level i to level iv. therefore, the corresponding imaging mission level is divided into four levels, and the missions with high level of this factor are given high priority."
"for the impact factors of mission priority, the above qualitative impact factors should be quantified in the calculation model. we used technique for order preference by similarity to ideal solution (topsis) [cit] to perform a synthetic calculation and obtain quantitative indicators of mission priority. the implementation of our method is as follows:"
"due to the limitation of satellite resources and different users' needs for remote sensing data, the priority level of emergency missions varies. by analyzing the application requirements of emergency missions, the following seven impact factors are considered in setting the mission priority (table 1) ."
"for position resolution, coincident events were energy fil tered around the photopeaks, and electronic colimation was applied by using only events that were detected less than 15 imn away from the fov center on the far side. an appropiate time coincidence window was applied so as to remove random coincidences. figure 10 shows the 2d histogram of detected event position for both detectors. for pmt, the five positions are clearly separated, and a spatial resolution of 2.7 mm and 2.6 mm in different axes is obtained at the center. for sipm, the image is noisier and the points are blurred but still distinguishable; the measured resolutions at the center are 4.4 mm and 3.9 mm."
where x max is the maximum value in the column that contains x ij and x min is the minimum value in the column that contains x ij .
the infrared radiative flux emitted by the lower (f ir_veh_inf ) and upper (f ir_veh_sup ) parts of the vehicle are thus evaluated in the following way:
"types of online interaction can be defined more specifically. first, participation refers to a message sent in a discussion without any peer response. second, the interaction can occur when a sent message gets feedback from other peers. in this stage, there is no response to the feedback from the sender of the message or from other colleagues. the last sequence, collaboration, happens when interaction is plural. [cit], it is said that the sender of the message gets more peer feedback from other colleagues and in addition, the sender sends feedback to maintain interaction. mcll using social media, particularly instagram, has been reported by language researchers. instagram is popular among social media users including students and is easy to integrate to their language learning. this platform makes it possible to create, send and share content among students. [cit] mentions that this application serves as a mobile learning tool that facilitates students' learning of english, especially writing skills, even when they are on vacation furthermore, for the purpose of learning english, this social media plays a pivotal role in nurturing an online educational environment which assists students in learning english language skills [cit] . in addition, it has several features that can be geared toward english learning. the video feature may facilitate speaking skill learning and the comment thread can be designed for writing skill learning [cit] ."
"in this section, we have conducted several numerical experiments to verify the effectiveness of the proposed method. for the model, 24 scenarios were setup and the number of initial missions contained in these scenarios changed from 25 to 200, with an increase step of 25. the corresponding number of newly arrived missions changed from 5 to 75, with an increase step of 10. simulation experiments were conducted in scenarios with 3, 4 and 5 satellite resources. the frequency of the insertion, reallocation, replacement and deletion operations for emergency missions in different scenarios was counted to obtain figure 5 ."
"among the interaction coefficients mentioned above, the one between snow and road occurs only in the presence of snow on the road. however, at this stage, the road surface was considered cleared of snow. therefore this coefficient will not be taken into account in the following calculation. the 135 interaction coefficients involved in the calculation of net radiation at the road surface are described by the following equation."
t air is the ambient air temperature (k) and v veh is the vehicle velocity (km h −1 ). the heat transfer 265 coefficient α tp between the tire and the road surface (w m −2 k −1 ) [cit] and is defined by the following relationship:
"according to the first hypothesis of integration of traffic impacts, the heat flows through the engine and the exhaust system are added to the turbulent heat flux of the urban canyon, which influences the road surface energy balance. this is reflected by means of the following equations:"
"language researchers have not paid special concern to instagram and its use to promote mcll in indonesia, especially in an islamic higher education context. this research tries to fill the void of literature on mcll with the assistance of instagram among students in islamic higher education. it is an effort to seize the benefits of mobile-assisted language learning (mall) which is geared toward nurturing students' skills; namely, communication and collaboration, both crucial in the 21 st century. other benefits include preparing students to be able to integrate technology to enhance their language learning and nurturing them to produce content which is suitable for language learning purposes."
"all connections between modules are purely digital, full duplex data links with embedded clock. each link's ends are regarded as master or slave according to the global system hierarchy. the links serve three different purposes:"
"in emergency observation, there are many factors that need to be considered to maximize a total reward of the observed targets, such as the priority level of missions and the total benefit gained from acquiring images. furthermore, in contrast to daily scheduling, the resource allocation for real-time missions as an important feature of this problem also need to be addressed. in the context of a large number of point target imaging missions with different priority levels and newly arrived missions, we develop a multi-satellite dynamic mission scheduling model based on the mission priority under emergency conditions. the algorithm includes the following steps: (1) analysing the factors that influence the mission priority and calculating the priority for emergency missions; (2) analysing the constraints of satellite scheduling and building an optimization objective function; (3) obtaining the initial scheduling plan with the hybrid genetic tabu search algorithm; (4) using dynamic scheduling algorithm tries to insert newly arrived missions to the initial scheduling plan; and (5) evaluating the scheduling plan results. the flow chart of this model is shown in figure 1 . influence the mission priority and calculating the priority for emergency missions; (2) analysing the constraints of satellite scheduling and building an optimization objective function; (3) obtaining the initial scheduling plan with the hybrid genetic tabu search algorithm; (4) using dynamic scheduling algorithm tries to insert newly arrived missions to the initial scheduling plan; and (5) evaluating the scheduling plan results. the flow chart of this model is shown in figure 1 . figure 1 . flow chart of a multi-satellite dynamic mission scheduling model based on mission priority under emergency conditions."
"within the same context of this study, further work will be undertaken to analyze the sensitivity 605 of the teb model to these different physical processes of traffic, and on the basis of additional field data currently available. the objective is to assess the contribution of each traffic process in improving the rst modeling according to the traffic parameters and the variation of atmospheric stability. these thermal traffic impacts should also be coupled with the road surface water balance of the teb model to identify and further quantify the influence of the presence of water in its various 610 forms (liquid and solid (ice and snow)) on the rst modeling. furthermore, the energy absorbed by vehicles has been so far added to the road surface, which was consistent with winter situations and traffic profiles used. so as to extend the approach to other seasons, a detailed description of energy absorbed by passing vehicles will have to be considered."
"step 5: perform the delete operation. when steps 2, 3 and 4 are not satisfied, the new mission t' is deleted and the resource scheduling process is abandoned."
"they also indicated that making an accurate evaluation of the traffic heat input on rst is relatively difficult, firstly because of the plurality of the impact processes, and secondly because of the change of heat input according to these parameters (traffic density, vehicles speed, road topographic profile and atmospheric stability, etc.). [cit] showed that the temperature in the vehicle-85 passage area was approximately 3°c above that in the non-vehicle-passage area during a sunny winter day. [cit] reported that the rst under vehicles waiting at traffic signals was 3 to 4°c higher than that nearby. some experiments with a thermal mapping vehicle indicated that traffic has a significant effect on rst [cit], especially in traffic light areas and/or on roads with high traffic density."
"this study investigated students' experience in using instagram to participate in mcll; to produce and share content produced, to interact and collaborate to produce content to accomplish tasks. the findings of the study will be organized into three sections to answer the research questions."
"vehicle-induced turbulence may also be an important factor in modifying the energy exchange between the air and the road surface in urban areas, especially under conditions of low wind speeds 270 which are typical for the urban canyon. the turbulence generated by passing vehicles promotes forced convection between the road surface and the surrounding atmosphere. this physical process has been studied by several authors [cit] . [cit] have defined an approach to assess the vehicle sensible heat flux s va (w m −2 ) due to vehicle-induced turbulence, removing energy from the pavement for a transfer to the urban 275 canyon. their approach consisted in defining a heat transfer coefficient α s (w m −2 k −1 ) between the road surface and the surrounding atmosphere, depending on the vehicles velocity."
"the next step in our study, and in the first one in the evaluation of the teb parametrization was to check the ability of teb to simulate the air canyon temperature in a street without traffic. as 470 indicated in the literature, some experiments have been conducted over circulated and non-circulated zones [cit] . teb has already been validated to simulate the air canyon temperature for a street without traffic, or with heat flux from traffic neglected [cit] ."
"the typical synchronization method for pet and other high energy physics (hep) readout systems is the use of a clock tree, using zero-delay clock buffers and distributors to span the whole system. however, this approach has several drawbacks related to the use of cabling for clock distribution. all clock cables for a given tree stage have to be matched in length in order to obtain a fully balanced clock tree, which can be difficult when the number of acquisition nodes to be synchro nized is large, forcing additional global timing calibration. further problems arise when there is a large distance between system nodes [cit], as fluctuating operating conditions such as temperature cause a variation of the delay of long cables or fibers. hence, clock trees work best in a cableless environment, when all timestamping electronics are placed within the same crate or otherwise constrained, with clock distribution being implemented through controlled backplane connections. this condition severely limits the system scalability and the mobil ity of the detectors, often hardwiring the maximum amount of supported detectors and forcing hardware redesigns for system expansions or whenever the detector topology is changed."
"the circuit from fig. 4 is implemented in all system modules, using a national semiconductor lmk02000 pll and clock distributor [cit] and an external vcxo with a nominal 156.25 mhz frequency. at the top node, the pll's switch between the charge pump and the loop filter is kept open (tristate output), so that the vcxo control input stays at a constant bias value and the circuit works as a regular oscillator and clock distributor, feeding the transceiver's reference clock input. at a slave node, the pll is initially configured in that same way; however, once the recovered clock is stable, the pll loop is closed and the vcxo output eventually converges to a jitter-filtered copy of the recovered clock. the current functioning of the transceiver, including clock recovery, is not affected during the pll transient phase because its reference clock suffers only very small variations while maintaining its nominal value. after pll convergence, the half-link from slave to master is established, and the filtered recovered clock is used for local logic and for sampling in the case of an acquisition module."
"a. acquisition module figure 2 depicts a simplified diagram of the contents of a single acquisition module. each module contains one gamma sensor, consisting of a scintillating crystal coupled to a pho todetector unit, which can be either a pspmt or an array of sipm devices."
a system architecture for pet has been proposed that is based on synchronization over data links. the architecture is fully modular and scalable and the same circuit boards accept both pmt and sipm based photodetectors. prototype boards have been designed and the architecture has been successfully tested for a small-scale pet system with two detectors. the design and validation of a full-scale coincidence detection system using several concentrator modules remains pending.
"author contributions: j.c. and x.z. are the directors of the corresponding contents. j.c. and x.z. designed the methodology and performed the experiments. j.c. completed data processing, formal analysis and the manuscript. x.z. checked the writing and completed the manuscript polishing."
"where n is the total number of actual scheduled missions after scheduling plan adjustment, n' is the total number of missions, num(tc) is the number of missions that the initial scheduling plan changes due to the newly arrived missions in dynamic scheduling, and num(tq) is the number of initial missions."
the rst_teb_a2 profile indicates that this approach took the heat inputs generated by traffic more properly into account. we also found that heat input peaks of the traffic during rush hours were obtained with better agreement with respect to field measurements.
"in the test scenario, mission request is generated by a random uniform distribution in worldwide. for each request of emergency mission, the following attributes are included: observation duration, geographical location, imaging mission level, observation image type and type of mission. in this paper, 25 initial observation point targets were randomly generated, as shown in figure 2, and the generated mission types are shown in table 5 . using the same method, we generated another five as the newly arrived missions to test the dynamic scheduling algorithm (table 6 ). [cit] /08/24 00:00 [cit] /08/24 14:00 (universal time coordinated), and the imaging duration was varied from 60 to 240 s. for satellite resources, three earth observation satellites were set up in this experiment. these satellites were taken from the satellite database of stk software developed by agi of the united states. the orbit parameters are shown in table 7 and were generated calling the orbital generation tool of stk according to the general orbit type of earth observation satellite."
"with the growth of ict and internet use among language teachers and researchers, the issue of online participation and multimodal content sharing come into existence. [cit], the development of web-based learning can support face-to-face learning and also improve students' online interaction skills. its development brings more advantages of an opportunity for students to produce content in a digital form which provide a wider autonomous space for language learners. the content and product students create can be in so-called multimodal formats. the term multimodal is an emerging one referring to the sharing of various file types such as audio, video, and texts. moreover, multimodal content can be created by language learners to accomplish their learning activities and the products can be studied by language researchers [cit] ."
"at this point, the process proceeds to step 1 until all new missions in the set t' are scheduled; otherwise, the satellite resource number plus 1 and the process proceeds to step 2."
"the resulting analog signals undergo a shaping and anti aliasing filtering stage using a second-order filter before being digitized by free-running adcs. a digitally controlled offset voltage is added to each channel in order to push the signal baseline as close as possible to the edge of the adc input range, so as to maximize the dynamic range of detectable pulses. ad9239 12-bit converters from analog devices [cit] are used with a 156.25 msps sampling rate. these are quad channel adcs with serial outputs, which help reduce the number of board components and digital traces, simplifying board layout and reducing signal integrity issues. a total of 10 adc channels are used: eight for amic outputs, one for an additional fast trigger output from the detector (used for the last dynode signal from pspmts), and one for adc delay calibration."
"in the context of social media use for language learning, the interaction between users can be categorized into several types. participation, interaction, and collaboration are the types that may appear in social media in the context of english learning. in its simple explanation, the three types seem like a sequence for interaction. students should participate first to show to other users their presence. their presence can initiate other users to interact for educational purposes. the collaboration stage can happen when students have gone through the first two stages, that is participation and interaction [cit] ."
"in this section, we have conducted several numerical experiments to verify the effectiveness of the proposed method. for the model, 24 scenarios were setup and the number of initial missions contained in these scenarios changed from 25 to 200, with an increase step of 25. the corresponding number of newly arrived missions changed from 5 to 75, with an increase step of 10. simulation experiments were conducted in scenarios with 3, 4 and 5 satellite resources. the frequency of the insertion, reallocation, replacement and deletion operations for emergency missions in different scenarios was counted to obtain figure 5 . it shows that for the same number of missions, as the number of satellite resources increases, the number of insertion operation notably increases, and the number of deleted missions decreases, which reduces the adjustment to the initial scheduling plan. the number of reallocation operation performed for missions is generally higher than that of replacement operation, which reduces the probability of deleting the initial missions."
"the most popular social media applications have been investigated by language researchers for their promising benefits in language learning. facebook, according to özdemir (2017), has been investigated for its ability to promote intercultural communicative competence. this social media has inspired language researchers and teachers to integrate its power in foreign journal on english as a foreign language, 9(1), 87-106 copyright © 2019 [cit] -1657 e-issn 2502-6615 90 language learning since it provides space for students to participate and interact for the purpose of language development [cit], and supports students in online interaction by expressing their social presence and interacting to discuss taught material in the classroom [cit] . other applications which have gained more and more users because of their useful features are twitter and instagram. the former can be designed to facilitate english language learning using the feature of micro blogging [cit], and to trigger better learning discussions among students beyond a classroom because they can share material, interact and reflect on their foreign language learning [cit], the latter can be tailored to help students learn listening comprehension since it features video content to share among users."
"the purpose of emergency observation mission scheduling is to determine the execution order of the missions and reasonably allocate satellite resources and execution time for each mission. as some new emergency missions are added, the complexity of the problem is further increased. because there are many constraints in the multi-satellite scheduling problem, and the arrival of new missions will affect the mission set, satellite resource set, time window and constraints in the initial model, thus influencing many scheduled missions. therefore, this paper proposes a dynamic scheduling algorithm based on mission priority to solve the scheduling problem of newly arrived missions. in this model, the initial schedule is obtained through a hybrid genetic tabu search algorithm. when new missions arrive, based on the initial scheduling results, the satellite that matches the new mission request is searched in the satellite resource set, the operations including insertion, reallocation, replacement and deletion are performed to execute the new missions with high priority. we design this method to ensure that newly arrived missions can be quickly scheduled under emergency conditions and minimize the change of initial scheduling plan."
"(9) assess the merit of the candidate solution based on the fitness function and whether it satisfies the aspiration criterion. if the criterion is satisfied, it is assigned to the current solution and the best solution found-so-far, the tabu length is set again, and the tabu length of other elements is updated. otherwise, the sub-best solution is taken as the current solution and added to the tabu table to replace the object that first entered the tabu"
"based on the objective of this study, there were several questions it sought to explore: 1) how do students use instagram to participate in mcll?; 2) what content is produced and shared by students on instagram?; and 3) how do students interact and collaborate with their group in an attempt to accomplish mcll?"
"step 6: determine whether the available satellite resources are traversed. if the available resources have been traversed, the solution corresponding to the maximum value in the evaluation set is selected as the final solution, and the scheduling of current mission t' is completed."
"prototype circuit boards were implemented in order to evaluate the performance of the proposed daq architecture for a small pet system with two detectors. the family of boards used in the tests is pictured in fig. 8 . each acquisition module prototype is formed by two boards: an adquisition board with 9 analog inputs (8 general ones and 1 for a fast trigger signal), and an analog front-end board with amic devices. two different front-end boards were designed, with one and four amics, that can be used with photodetectors with 64 and 256 output channels, respectively. the acquisition board has two inter-module links, so it can be used as a small concentrator module with two downlinks, as well as a mixed acquisition/concentration module. a total of three acquisition boards were built for testing."
"the mission defined in this paper refers to the point target that can be observed by the satellites in a field of view. considering multi-resource constraints, we established a mathematical model to describe the scheduling problem. in the observation scheduling stage, the main constraints are as follows. the parameter and label definitions in the constraints are listed in table 2, and function definitions are shown in table 3 . table 3 . function definitions in the constraints."
"gigabit rate data transmission between fpgas is usually implemented using embedded high-speed transceivers and self-synchronous signaling, where the transmitter data clock is embedded in the data signal. the clock is recovered at the receiver using a pll-based clock recovery unit (cru) and then used to sample and decode the incoming data stream. the transceiver is seeded by an external clock which is used both for transmission and as a reference for the cru."
"these sipm devices have an active area of 1 x 1 mm 2 but were soldered on a rectangular grid with 3.00 mm x 3.05 mm separation, so the effective scintillation area is only 10% of the total. a pyramidal frustum lyso crystal was used in this case."
"the constants 1/3 and 2/3 correspond to the portion of the road without traffic and the one subjected to traffic respectively. these values are consistent with the numerical description of the second approach, 1-z traffic and z traffic respectively. therefore, in the text that follows, the results of teb 465 model on rst will be compared to rst_measured. its variations with time for the first experiment are illustrated in figure 7 ."
"before releasing the initial scheduling plan, five newly arrived emergency missions to be allocated were added before the missions were executed. after the priority calculation, the insertion, reallocation, replacement and deletion for newly arrived missions were performed. the scheduling plan of newly arrived missions was then obtained, as shown in table 12 . in the initial scheduling plan, missions t4 and t17 were not allocated satellite resources due to the limitation of the time window, objective function maximization or some other constraints. after the new missions were added, the scheduling plan shows that mission t27 was directly inserted into the initial scheduling plan of satellite s3, mission t26 was inserted into the initial scheduling plan of satellite s3 through the reallocation operation, the affected mission t10 was scheduled again by satellite s3, and the scheduling time was from 12:18:23 to 12:19:38. missions t29 and t30 were respectively scheduled by satellites s2 and s1 through the replacement operation, and the affected missions t21 and t15 were deleted. mission t28 has a small mission priority and was not allocated satellite resources under the condition that the global objective was maximized. figure 3 illustrates the statistical results of completed missions under the scenario with 30 missions. it can be seen that a total of 25 missions were completed after the dynamic scheduling due to the arrival of new missions. the missions with high priority (priorities level of 7, 8, 9 and 10) were all executed. some missions with low priority (priorities level of 1, 2, 5 and 6) were not executed, because these missions were not allocated satellite resources during the initial scheduling or were replaced or deleted by a mission with higher priority in the dynamic scheduling. overall, the proposed method guarantees a high execution rate for high-priority missions. the scheme change rate is 12% by calculating the model performance evaluation, so it is very helpful to guarantee the stability of the initial scheduling plan. figure 4 shows that among the 25 executed missions, the missions with priorities higher than 6 account for 28% of the total number of completed missions, achieving a satisfactory scheduling effect."
"photodetector outputs are sent to amic [cit], an integrated analog front-end that converts 64 detector signals into 8 analog outputs, each of which is a weighted sum of the 64 inputs with digitally programmable coefficients. amic can thus be used as a replacement for a resistor network used as a charge division circuit for anger logic [cit], benefiting from a higher bandwidth due to integrated preamplifiers, and the capability for automatic correction of photodetector channel gain mismatch by fine adjustment of weighted sum coefficients [cit] . additionally, it can be used to obtain the first few statistical moments of the light distribution in a continuous scintillator [cit], from which event position can be extracted. in particular, the second moment contains information on depth of interaction within the crystal [cit] . the newest version of the asic, amic2gr [cit], is compatible with both pmt and sipm-based detectors, so they can be used interchangeably by defining a common connector. additionally, the amic architecture is fully expandable and allows the readout of detectors with more than 64 outputs by using several instances of amic and adding their corresponding current outputs together [cit] ."
"is the thermal conductivity of the first road layer, rst its temperature (k), rst 2 is the temperature of the second road layer (k), d 1 is the thickness of the first road layer (0.001 m, as mentioned above) and d 2 that of the second road layer (0.01 m)."
"(5) storage capacity constraint. for satellite s j, the storage capacity of the observation mission between any two adjacent data transmission windows w 1, w 2 cannot exceed the storage capacity m j of satellite s j :"
"the parameter z traffic was integrated in the model to take into account the portion of the road affected by traffic. the sensitivity test of the teb model to this parameter z traffic, was conducted."
"in each node has to be considered, because that is where the timestamps from (5) are assigned. the latency components are tt x for the transmitter, t p for external transmission lines (board traces and cables) and trx for the receiver. an additional term d.t.p is needed to account for the phase change between the receiver clock domain (recovered clock) and the local clock domain at the receiver node, used for protocol logic and transmission. we obtain"
"the test setup used for evaluation consisted of two acquisi tion modules, one of them working as a concentrator as well and acting as the master in the module hierarchy. a continuous slab of 10 mm deep scintillating crystal covered by black epoxy was placed in each module, with a 49 x 49 nun 2 area coupled to a photodetector using optical grease. two different types of photodetector unit were evaluated:"
"c p is the specific heat capacity (j kg -1 k -1 ), ρ air is the air density (kg m -3 ), rst the road surface temperature (k), t lowcan is the temperature of the lower limit layer of the urban canyon (k), and thus corresponds to the air temperature at a high of 2 m. l v is the latent heat of liquid water evap-155 oration (j kg -1 ) q sat_road is the specific humidity in the road surface (g kg -1 ), q canyon is the specific air humidity (g kg -1 ), r road is the aerodynamic resistance of a dry road, r road_wat is the aerodynamic resistance of a wet road, and ac road, ac road_wat are the aerodynamic conductance for dry and wet road respectively."
rst_with_traffic. this confirms that the physical description of the traffic impacts process is suitable 560 for the traffic integration in the teb model for the winter season.
"a veh_sup is the albedo of the upper part of vehicle, it depends on the color of his painting and an average value was chosen as equal to 0.75 (dimensionless), a veh_inf is one of the lower parts of vehicles. the heterogeneity of the lower parts of vehicles bodies is neglected and an average value of 0.057 was selected (average between that of steel (0.075) and aluminum (0.039)). to the long wavelength flux of the atmosphere r lu . the shield effect caused by passing vehicles will decrease the radiative flux of the road surface. based on these assumptions, the road surface energy balance is written in the following form:"
"data were collected from participants several times as the course proceeded. there were four tasks that students needed to accomplish. they used their mobile phone to produce content and shared it using their instagram account. in addition, they interacted and collaborated on the content to accomplish the tasks. in a nutshell, these activities demonstrated their experience in mcll."
"the integration of traffic in the teb model according to the first approach (a1) and based on a variable heat flux into the canyon with time did not improve rst forecasting, the gap between simulations and measurements is 3 to 4°c. this approach can be used to evaluate the global anthropogenic heat flux in the urban canyon, and is not meant for rst urban simulation. the results of the second approach (a2), consisting in an accurate description of energy contributions of traffic, were 580 consistent with the experimental study as well as with the literature review. they indicated that the traffic increased rst by 1°c to 3°c and this increase depends on traffic conditions (vehicle velocity, traffic density and traffic impact area). some teb model sensitivity tests showed that the traffic impact area affects the rst forecast. if this area is large, the thermal traffic flows are great which results in an increase of the rst. the presence or absence of buildings also influenced modeling 585 of rst. validation was also successfully obtained with the air temperature. these results were obtained in some winter situations not considered as critical. rst is still slightly underestimated in this second approach, and could therefore trigger false alerts of ice occurrence on pavement. to obtain a better forecast for rst with the teb model, it is necessary to properly define the configuration of the urban environment. it should be noted that the integration of traffic in the teb model according 590 to this second approach significantly improved the rst forecast in the winter season. however, there is still a difference of 0.5°c to 1°c between the measurements and the teb-simulated rst. this can be explained either by the error that can be assigned to the measurement devices, or because the physical description we used for the process of traffic impacts still needs improvement, or by the existence of certain road parameters that have not yet been introduced into the rst forecast with the 595 this model."
"dynamic emergency scheduling of earth observation satellites is of great significance for the efficient use of satellite resources to obtain ground image data under emergency conditions. this paper studies the satellite imaging scheduling problem under emergency conditions and proposes a set of reasonable mission priority calculation method based on seven impact factors. considering the constraints in the scheduling process, a multi-objective mathematical programming model of multi-satellite dynamic emergency scheduling based on mission priority is established. this model can be used to solve the scheduling problem of newly arrived emergency missions. in the simulation experiment, complex scenarios with different mission types and different satellites were set, and the priority levels of different missions were obtained with a calculation model of mission priority. it was also used as the heuristic information to solve the constrained scheduling problem. in the multi-objective optimization, the hybrid genetic tabu search algorithm used in this paper effectively overcomes the disadvantages of the weak local search ability of the genetic algorithm and strong dependence on the initial solution of tabu search. therefore, this hybrid algorithm avoids a large number of useless iterations when approaching the best solution in the late stage, thereby improving the efficiency of the algorithm. in the simulation scenarios with different numbers of initial missions and newly arrived missions, the analysis results show that the probability of executing high-priority missions increases because the model considers the calculation of mission priority. thus, a reasonable initial scheduling plan for mission request can be established under emergency conditions, and the effect of satellite resource scheduling can be improved. in case of newly arrived emergency missions, a feasible solution with low time cost can be given by using dynamic scheduling algorithm based on the mission priority. overall, the proposed model yields high scheduling revenues and low scheduling costs, maintains the stability of the initial scheduling plan under the condition of fast scheduling, so it is suitable for solving multi-satellite dynamic mission scheduling problems."
"students applied their new knowledge and skills. they worked on several tasks given to them. using instagram with their group, they interacted and collaborated on a variety of content. they produced content to meet the demand of mcll."
"in the test scenario, mission request is generated by a random uniform distribution in worldwide. for each request of emergency mission, the following attributes are included: observation duration, geographical location, imaging mission level, observation image type and type of mission. in this paper, 25 initial observation point targets were randomly generated, as shown in figure 2, and the generated mission types are shown in table 5 . using the same method, we generated another five as the newly arrived missions to test the dynamic scheduling algorithm (table 6) . the process proceeds to step 1 until all new missions in the set t' are scheduled; otherwise, the satellite resource number plus 1 and the process proceeds to step 2."
"the model evaluation function defined in section 2.5 was performed to evaluate the scheduling results for 24 different scenarios, and the computational complexity was measured using elapsed cpu time (table 13 ). the evaluation indicators of the scheduling plans indicate that for the same number of satellite resources, as the number of missions increases, the mission completion rate and priority execution rate exhibit decreasing trends. when the number of missions is less than 100, the mission completion rate and the priority execution rate are both greater than 0.8 in different scenarios, achieving a better scheduling effect. in the scenario with 25 initial missions, five newly arrived missions and five satellites, the maximum value of the evaluation function is 11.40. because the numbers of insertion and reallocation operations in this scenario are 3 and 2, respectively, the scheme change rate is small. however, when the number of missions is more than 100, the results for each evaluation indicator are poor. we can see that the running time of initial scheduling generally increases with the number of missions, with a maximum of 564.72 s. in contrast to the initial scheduling, the computation time for the dynamic scheduling are quite short, because the satellite resources are allocated based on initial scheduling and mission priority. however, the running time increases significantly with mission size because more different operations are needed in the iteration. it is worth mentioning that the mission priority execution rate is always larger than the mission completion rate, this phenomenon illustrates that the proposed method increases the execution probability of high-priority missions. the above results show that the proposed model is practicable and can meet the need of real-time processing with low time cost, especially for solving small and medium-scale problems. however, due to the mission priority set in this paper, the resource allocation strategy may be unfair, so some processes may wait for a long time. from the mission completion rate in table 13, some missions were delayed the execution process because of their low priority or low revenue, which may lead to the problem of starvation. this situation could be solved by increasing the number of satellite resources, so as to increase the chance of each mission being executed. in addition, it can adjust the weight allocation of the optimization objective function, so that as many missions as possible can be executed before deadline."
"adc outputs are read by a stratix iv ep4sgx110 fpga [cit] . the high sampling rate forces the use of embedded gigabit-speed transceivers for the serial signals. a channel alignment procedure is necessary after adc frames are re ceived and decoded, because the latency of the transceiver and frame decoding logic for each channel may be different. to do so, the adcs are programmed to force their outputs to show a transition between two fixed values; data channels inside the fpga are then selectively delayed in order to have the transitions occur at the same clock cycle."
"second, content was produced and shared by students to accomplish tasks in mcll. there were several types of content produced and shared in their instagram account namely: word meaning, sentence structure analysis, third, interaction and collaboration in instagram were carried out by students to accomplish tasks in mcll. they used instagram thread comments to seek further comments and interaction with their peers in order to accomplish tasks in mcll. their interaction and collaboration can be seen from the comment thread in their instagram accounts which was open to access by other users to read and comment; the contents produced and shared by students met the criteria of mcll. this study is limited by the method used so the findings may not be generalized for other contexts. further studies should address this issue using a different method. the more studies conducted on mall issues the better for future efl teaching that will prepare students to achieve 21 st -century skills."
"in practical applications, the constraints of satellite mission scheduling are very complex. in some cases, manual intervention is needed for obtaining the current satellite scheduling plan, such as the collection of mission request lists, the selection of satellite resources, and the weight setting of optimization objective function. in the emergency situation, the operators may need to adjust the scheme according to the latest planning configuration before it releases, including deleting observation missions and adding new missions. however, finding the balance between manual intervention and automation is a perennial challenge. in future work, if some manual intervention can be transformed into decision-making conditions of the scheduling system through learning mechanism, it will be more capable to meet the demand. furthermore, we will analyze more complex scheduling problems on this basis and further refine the model. additionally, more constraints will be considered to make the scheduling problems more comprehensive."
"energy and time resolution were measured only at the center point. for each event, energy was estimated as the maximum detected amplitude value of the sampled trigger signal. energy resolution was measured by histogramming energy measures and fitting a gaussian curve around the photopeak. fwhm resolutions of 27% for pmt and 31 % for sipm were obtained. similarly, time coincidence resolution was measured by histogramming timestamp differences and fitting a gaussian curve around the peak. the result is shown in fig. 11 . the constant random background is clearly visible, and fw hm resolutions of 3.9 ns for pmt and 10.4 ns for sipm are obtained."
"in this section, we have conducted several numerical experiments to verify the effectiveness of the proposed method. for the model, 24 scenarios were setup and the number of initial missions contained in these scenarios changed from 25 to 200, with an increase step of 25. the corresponding number of newly arrived missions changed from 5 to 75, with an increase step of 10. simulation experiments were conducted in scenarios with 3, 4 and 5 satellite resources. the frequency of the insertion, reallocation, replacement and deletion operations for emergency missions in different scenarios was counted to obtain figure 5 ."
the sensible s * a (w m -²) and latent l * (w m -²) heats in the presence of traffic on the road are respectively written:
"the main purpose of concentrator modules is to detect coincident gamma events and to keep its child nodes (acquisi tors and lower level concentrators) synchronized. a simplified scheme is shown in fig. 3 . each concentrator has a number of links to lower level nodes where single event frames are received in chronological order and stored in fifos. the coincidence detection engine continually compares the timestamps of the first available event from each fifo, and checks whether the timestamp difference between the two oldest visible events is within the selected time coincidence window. if so, both events are registered as a coincidence; if not, the oldest one is discarded as a random event."
"synchronization of timestamps is done on a link by link basis, following the same hierarchy as frequency propagation. in each master-slave data link, the master timestamp's frac tional part remains fixed and the slave's is updated. the top module's timestamp is taken as a reference and its fractional part is fixed at zero."
"in a previous paper [cit], we proposed a synchronization scheme over data links that was able to achieve state-of the-art synchronization resolution in order to overcome these issues. the fact that each link is independently self-calibrated eliminates the necessity of cable length matching and allows automatic compensation of long cable delay variations. free dom of placement for the individual boards is thus guaranteed. a modular, scalable daq architecture was also proposed based on this synchronization method. for this paper, a first working prototype of said system architecture has been built and evaluated. the system has been designed and tested with pet applications in mind, but it must be noted that the proposed architecture is valid for general hep readout applications where several trigger levels are needed."
"the second kind of content that students produced and shared refers to word meaning or definition. for this, they shared a photo containing an underlined word from a particular sentence. in the caption section, they asked their friends to contribute to the word's definition. the types of content shared to show how students used instagram to fulfill mcll. shared content allowed interaction and collaboration to happen among students. this happened because there was a need to collaborate to jointly solve the english sentence problem in terms of its structure. similarly, with word meaning, interaction and collaboration occurred among students to solve the problem. this content clearly shows how instagram provided space for students to create and share images and captions for the purpose of language learning. it goes without saying that the main function of instagram is photo or images sharing and students used this function for language learning-related content."
"this study was aimed at exploring instagram use among students to participate in mcll in a translation course. having discussed the findings above, this study offers several conclusions. first, students were able to use instagram to participate in mcll. their knowledge on instagram uses helped them to integrate social media for the purposes of language learning. they showed their ability to bring instagram to the next level, i.e. for mcll purposes."
"(2) start-up time constraint. the start-up time must be between the minimum start-up time mint j and the maximum start-up time maxt j of sensor s j of satellite s j . the time interval between two switches cannot exceed the preset minimum time interval minperiod j . ∀i, j, tm :"
"the number of on-off switches for satellite s j, which can be calculated from the start-up time and shutdown time of satellite s j ."
traffic integration results using this second approach (a2) are illustrated in figure 9 . this compares the variation with time of rst for a traffic integration in the teb as in the initial configuration and according to (a2) approach for both experiments. rst results with the (a2) approach (rst_teb_a2) are closer to the field data than the initial configuration. the difference between 535 field and calculated rst is nearly 0.5°c on average. rst variations reflect those of q traffic (figure 3) and their amplitudes (3°c figure 9a; 6°c figure 9b ) are consistent with field measurements.
"social media has the ability to facilitate online participation, interaction and collaboration on the part of language learners. these three types of communication can be demonstrated by their participation in social media. moreover, their high interaction with other students signals a strong effort on the part of students to complete a group project or task. also, their interaction can be seen from comment and messages available in their thread and these data are open to access and exploration by language teachers [cit] ."
"another validation of the (a2) approach involved comparing the air temperature measured onto the vehicle in the street with the forecast one obtained with teb. air temperature measurements are obtained at a height (1.8 m) and conditions (generation of a continuous laminar air flow on the probe) 550 compliant with those at which teb is provides its results (2 m). results are presented in figure 10, and indicated good agreement between the forecast and the measurement in both experimental cases."
"regarding research question two, students exhibited an ability to create content and interact with their peers to collaborate on particular content. their interaction and collaboration were made possible by instagram. social media can be used by students to interact and share content among their peers. this finding is in line with previous studies' claims that social media offers an opportunity for language learning [cit], potential and benefit for the purpose of language development [cit], and intercultural communicative competence learning (özdemir, 2017) ."
"this section presents the dynamic scheduling algorithm which tries to insert newly arrived missions into the initial scheduling plan. the newly arrived missions are sorted by priority, from high to low, and scheduled one by one. the scheduling operations are applied to each new mission until one of the operations is successful. the operations are: insertion, reallocation, replacement, and deletion. the algorithm stops when all new missions are attempted to be scheduled, and an ideal dynamic mission scheduling plan is obtained by iteration according to the proposed method. the details of the scheduling operations are as follows:"
"the multi-satellite dynamic emergency scheduling model based on mission priority not only requires a high mission completion rate and high total revenues, but also needs to ensure a better scheduling period for each mission and a high execution rate of missions with high priority. according to the characteristics of the model, three indicators are defined to evaluate the model performance, including the mission completion rate (mcr), mission priority execution rate (mper), and scheme change rate (scr). the evaluation function of the model is given in equation (16):"
"rst and atmospheric measurements were obtained using a vehicle parked in the selected street with an on-board data acquisition system (figure 6a ). the instruments were primarily devices dedicated to meteorological parameters (t air, relative humidity, wind direction and speed). they were installed on the roof of the vehicle, and data collected every 2 s. a radiometer and an infrared camera were dedicated to rst without and with traffic respectively. the radiometer was installed in a compart-415 ment at controlled temperature, attached to the front bumper of the car, also with measurements every 2 s. the infrared camera was installed in a compartment on the vehicle roof. thermal images of the pavement submitted to traffic were taken every 60 s. an illustration of instruments is given in figure 6b . traffic data for the selected street were obtained from the appropriate department in nancy."
"the system has been shown to work with both pmt and sipm based photodetectors; in particular, the ability to work with arrays of 256 sipm has been proved. the performance of both detectors can be compared but the conditions were not the same, either: the scintillator area coverage for the sipm detector was only 10% of that of the pmt, so worse results are to be expected. using only the simplest configuration for the front-end and digital algorithms (amic as a cog network; lack of gain calibration; event detection by fixed threshold crossing; basic, fixed dcfd for timing), we have obtained decent results for all measured specifications, with the exception of pmt-pmt energy resolution. we expect to obtain much better resolutions just by optimizing these digital algorithm parameters."
"step 2: perform the insertion operation. this step involves traversing every available satellite resource of s for mission t' to determine whether the visible time window of the current satellite resource s' is available and whether there is a conflict between missions t' and the scheduled mission in the current time window of satellite s'. if there is no conflict, the mission t' is directly inserted into the current time window. if the new mission cannot be inserted, mission t which conflicts with mission t' is placed in the mission conflict set t\", and the process proceeds to step 3."
"115 ∆z s is the thickness of the first layer of the road surface. (ρc) road is the volumetric heat capacity of the road surface layer (j m -3 k -1 ), t is the time (s), g is the conductive heat flux across the bottom of the road surface layer (road surface heat flux, w m -²), r n is the net radiation flux (w m -²), s a is the sensible heat flux associated with natural wind (w m -²) and l is the latent heat flux associated with phase transition of water (liquid-vapor, and liquid-solid) (w m -²). we chose a very low thickness 120 value (∆z s equal to 0.001 m) so that its temperature reflects the rst. this give a quick response of the road surface temperature to heat flux changes without thermal inertia."
"the genetic algorithm and tabu search are common algorithms for solving combinatorial optimization problems [cit] . the genetic algorithm is a highly parallel, stochastic and adaptive optimization algorithm based on the biological evolution and selection mechanism, and it has strong robustness and high global search capability. however, in the late stage, the convergence speed and the computational efficiency are low, and it is easy to converge to a local optimum. tabu search is a neighborhood search algorithm that prohibits the repetition of previous work and avoids the local optimum. however, tabu search also has some limitations, such as strong dependence on the initial solution, serial operation for only one solution, and low search efficiency."
"2 state of the art and objective of the study accumulation of snow or ice on roads generates hazardous traffic conditions. several models exist and are based on forecasts of the road surface status. the heat flux associated with passing vehicle was partially taken into account by some models (icebreak [cit], icemister imal wind speed of 5.14 m s −1 in daytime, and 2.57 m s −1 at night and during holidays seasons. in such cases, only specific physical processes associated with traffic are considered as relevant, while others one are neglected. none provided nor analyzed the relative importance in terms of the energy fluxes of these processes related to the presence of vehicles."
"the proposed daq arquitecture is outlined in fig. 1 . the system is divided into a front-end section and a back-end section, and each of them is formed by an arbitrary number of identical modules with no constraints on physical location with respect to each other, i.e. the only placement restriction is given by the particle detectors at the front-end. the front end section consists of acquisition modules, which contain the photodetectors and perform analog conditioning of detector signals, digitization, single event detection and position and timestamp extraction. the back-end section contains concen trator modules, which collect data from several adquisition modules and perform time coincidence detection. data from several concentrators can themselves be collected by a higher- level, identical concentrator. all modules are connected form ing a hierarchic tree, with the top node being responsible for the transmission of all aggregated data to the external processor that handles image reconstruction."
"based on the characteristics above, a hybrid genetic tabu search algorithm is designed in this paper to allocate satellite resources to the mission set. the algorithm is used to solve the combinatorial optimization model and obtain the initial scheduling plan. the core concept of this algorithm is as follows: (1) the genetic algorithm is used for large-scale searches in the global space. the initial population traverses most of the solution space in parallel. at the end of iteration, the result is stable in the region where the solution space is better; and (2) starting from each individual in the optimization region, the tabu search is applied in a small-scale search of the local space to delay or avoid the genetic algorithm converging to a local optimum, thereby improving the ability to search for the best solution. the process of the hybrid genetic tabu search algorithm is shown in the following processing chain (table 4) : table 4 . processing chain for the hybrid genetic tabu search algorithm."
the ( * ) symbol denotes surface parameters impacted by traffic. the constant 0.22 [cit] for the tire frictional processes ( table 2 ). the net radiation impact on traffic r * n is expressed by the following equations:
"to identify the most appropriate approach to implement traffic in the teb, some experiments were conducted. they consisted in rst measurements on pavement zones subjected and not subjected to traffic. the experimental zone was located in rue charles iii (nancy-france), having a canyon 405 configuration consistent with teb, with a width around 12 m ( figure 5 ). this street is straight, orientated slightly north of west-east, and consisting of one non-circulated lane, nearly 3 m wide, and two circulated lanes to give a total width of nearly 9 m, and with a one-directional vehicle flow going east."
"our goal is to use the exact frequency of the recovered clock for the local logic at the slave node as well as for data transmission in the opposite direction (slave to master). however, the transmission clock is the same as the reference clock which is needed to recover the desired clock frequency in the first place; hence, a special clocking circuit is required if we want to use the same transceiver for both half-links."
"step 4: perform the replacement operation. this step assesses the priority of mission t' and the current conflicting mission t. if the priority of mission t' is higher than that of mission t, the current conflicting mission t is deleted, and then the new mission t' is inserted into the visible time window; otherwise, the mission replacement is unsuccessful, and the process proceeds to step 5."
"the multi-satellite dynamic emergency scheduling model based on mission priority not only requires a high mission completion rate and high total revenues, but also needs to ensure a better scheduling period for each mission and a high execution rate of missions with high priority. according to the characteristics of the model, three indicators are defined to evaluate the model performance, including the mission completion rate (mcr), mission priority execution rate (mper), and scheme change rate (scr). the evaluation function of the model is given in equation (16):"
"it is worth to clarify the effect of choosing some range of scale coefficients at this point. as mentioned above, autocor- relation sequences derived from musical signals are typically characterized by the period defined by the tatum of the piece. in fig. 6, three pulse trains, as a simplified model for such type of autocorrelation sequence, are reconstructed using the complex scale coefficients smaller than . the pulse train has a length of 5 s and a period length of 100 ms, and it was sampled at a sampling period of ms. it can be seen that by using more scale coefficients for the reconstruction, the approximation of samples at large time values gets improved. this is caused by the type of the base function applied in the scale transform as denoted in (2): functions are chirp functions for which the period is increasing as time increases. this increment is realized faster for small scale values. thus, the base functions of will match the period of the pulse train earlier in time than the base function of, if"
"the above search process occurs in a period of . with the implementation of the flow control strategy and the decrease in passenger flow, the above process can be repeated. reducing or replacing the current flow control node can formulate the flow control program for subpeak conditions."
"for a fixed inter-packet time, the mcdr-based mac protocol has almost the same energy regardless of elapsed time since its margin size is determined by the inter-packet time. as shown in figure 7, when the interval is 1 min, mcdr(0.1) consumes 86% less energy compared with mcdr. when the interval is 1 h, mcdr(0.1) consumes almost 90% lower energy than mcdr as expected since mcdr(0.1) has a margin size 10 times smaller than that of mcdr."
"transit network. according to the urban rail transit network topology and passenger flow characteristics, the controllable model of the urban rail transit network is presented iṅ"
"the first experiments explores the influence of tempo deviations within the classes. since for the midi files the tempo information is given, experiments could be conducted with the tempo of the pieces changed in a deterministic way. for this, from the data in d3 the global tempo mean value has been computed. then, all pieces have been assigned this tempo mean plus a uniformly distributed noise. this noise has been varied in steps of 5% from 0% up to 85%. for 0% noise all pieces share the same tempo, and no scaling effects the autocorrelations. at 85% noise level noise level the global mean of about 87 bpm results in a possible tempo range from 13 to 161 bpm. in order to compensate for the noise introduced by the randomly changed tempo for each noise level the experiment has been rerun ten times, and the mean accuracies of the ten runs are reported. computing the mean svm-accuracy for the noise free case leads to an accuracy of 82.9%. the small difference to the accuracy of 82.3% (as shown in table ii ) in presence of the original tempo variance of the data proves the robustness of the proposed method to this variance. increasing the noise level leads to an almost linear decrease in classification accuracy. however, at the largest tempo noise level of 85% the accuracy is still 73.2%. this confirms that the theoretical properties of the scale transform make the features robust to large tempo changes in practice as well."
"there are a number of techniques currently available in multifocal multiphoton micrcoscopy to generate multiple scanning points. these include the use of nipkow spinning disks [cit], cascaded beamsplitter arrays [cit], microlens arrays [cit] and diffractive patterned illumination approaches [cit] . these approaches restrict flexibility in relation to light efficiency (number of beams) or the use of patterned illumination. one technique utilizing a holographic pattern projected onto a diffractive spatial light modulator (slm), (developed originally for use in optical tweezing applications [cit] ) is gaining a wider acceptance in multifocal multiphoton beam scanning implementations [cit] ."
"this paper proposed an optimization method for flow control of urban rail transit based on the state-space equation and the driver node-matching algorithm. in addition, the characteristics of the optimization control of this method were analyzed by taking the beijing rail transit network as an example. the conclusions are as follows."
"the second experiment explores the way accuracy might get affected when dealing with real audio signals of turkish music instead of the midi signals as contained in d3. for that purpose, the functionality of the midi toolbox [cit] for the synthesis of an audio file from a midi has been used. the synthesis locates shepard tones [cit] of constant intensity wherever an onset is listed in the midi file. thus, computing an oss from the signals synthesized in this way results in almost constant onset strengths amplitudes at the locations of the note onsets. the accuracy clearly decreased to 63.5% (from 82.3%), again using svm on stm features at . it was investigated if this decrease is caused by the flat characteristic of the oss that does not allow the differentiation between strong and weak onsets. for this, audio files were synthesized using the timidity 2 software, which uses the velocity information contained in midi file, which means that onsets have varying strength. a standard piano sound has been used for synthesis. in the same experimental setup, using svm on stm features at, an accuracy of 77.4% was obtained. in another experiment, the durational accent type used in the oss computation from the midi files was replaced by flat accents. this means that impulses of constant height were positioned at the location of all note onsets. indeed, removing the information about the intensity of the onset leads to the accuracy of 68.7%, and it can be concluded that the weighting of an onset according to its strength is a crucial information. thus, it can be assumed that this method will work comparably well when applied to real world audio, which contain the full range of dynamics that characterizes human performance."
"the time represented by a node's clock when the reference node time is given. if the current reference node time is t k, the relative time of node a is denoted by c a (t k )."
"abstract-as a special case of the mellin transform, the scale transform has been applied in various signal processing areas, in order to get a signal description that is invariant to scale changes. in this paper, the scale transform is applied to autocorrelation sequences derived from music signals. it is shown that two such sequences, when derived from similar rhythms with different tempo, differ mainly by a scaling factor. by using the scale transform, the proposed descriptors are robust to tempo changes, and are specially suited for the comparison of pieces with different tempi but similar rhythm. as music with such characteristics is widely encountered in traditional forms of music, the performance of the descriptors in a classification task of greek traditional dances and turkish traditional songs is evaluated. on these datasets accuracies compared to non-tempo robust approaches are improved by more than 20%, while on a dataset of western music the achieved accuracy improves compared to previously presented results."
"the most important process of a wsn mac protocol for an ultra-low duty-cycle environment is to estimate when the rx node wakes up without consuming much energy. however, as mentioned earlier, in practice, this is difficult to accomplish because the clock drift of a node changes according to various factors. 11, 12 in this section, experimental scenarios are presented and the measurement results are analyzed for the relative clock drift based on various measurement intervals. the packet reception and transmission times at mac layers of the tx and rx nodes are measured, respectively, and the relative clock drift from these measurements is calculated."
"based on the comparison of the passenger flow od data for the early peak (7:00-9:00) and ordinary periods, we selected a threshold of 69. the matrix can be transformed as shown below:"
"we performed an sinr analysis of the uplink and downlink transmissions and extensive simulations for the performance evaluation of the network throughput and fairness among clients. further, we compared the performance of pocmac with that of the csma/ca-based half-duplex scheme and the full-duplex scheme without power control. in addition, we conducted software defined radio (sdr)-based experiments to evaluate the performance of the proposed transmit power adjustment scheme described in section iii-c in an office environment."
"using the control frames and headers, the ap collects the inter-client interference information from the rx, calculates the transmit powers for itself and the tx based on the collected information, and then informs the tx of the transmit power for the uplink data transmission. fig. 7 shows an example of the operation of the tx, rx, and ap. during the first transmission period, c1 wins the contention against c3 and c5, and c1 is the tx that transmits to the ap. the ap broadcasts a cts-u control frame, which is an acknowledgement to c1, and includes the information that it wants to transmit a data frame to c2 or c4. from the contention for the receiver selection, which has been described in section iii-b, c4 is determined as the rx, and it then transmits a cts-d frame with the inter-client interference information to the ap. using the estimated and collected information, the ap calculates the optimal transmit powers for the tx and itself, and then, it starts a downlink data transmission that is used to inform the tx of its transmit power. then, c1 can start an uplink data transmission with the instructed transmit power. finally, c4 transmits an ack-d frame to the ap, and the ap also transmits an ack-u frame to c1. the next transmission period will start after a distributed inter-frame space (difs). the detailed procedures of the tx, rx, and ap under the proposed pocmac protocol are described as follows."
"(3) to address the uncontrollable situation of the morning peak hour of the passenger flow network, an optimization method of flow control of the urban rail transit network was proposed based on driver node matching. the minimum set of driver nodes was identified by an intelligent search algorithm, which can obtain the specific flow control stations that can be controlled by the passenger flow network."
"however, the relative clock drift, which occurred between the rx and tx nodes, is our focus. the experiments were performed to observe the relative clock drift characteristics of nodes located in the outdoor environment. in the experiments, three node types such as telosb, 22 micaz, 23 and hmote 24 were used. for every node type, 11 nodes were used."
"this paper presents development of a multifocal multiphoton imaging system for high-frame rate flim imaging. we have demonstrated that multifocal multiphoton imaging with a 2d spad array is practical and offers significant speed advantages compared to single beam laser scanning or a widefield flim microscopes. with this implementation of mm-flim, the minimum acquisition time for a homogeneous image scales as where n 2 is the number of pixels in the image, t is the pixel dwell time to acquire the required 1000 photons for a satisfactory single exponential fit [cit] and n is the number of beams. at the maximum count rate per pixel for the current system (500 khz) and 64 beams, the minimum frame time is 2.05 s for a homogeneous sample, which is a drastic improvement when compared with 131 s required for a single beam scanning system. often in flim imaging spatial resolution is sacrificed for temporal resolution and applying a 3 x 3 spatial binning reduces this acquisition further to 228 ms in the mm-flim. one can also fit with a lower number of photons to obtain faster acquisition rates (500 ms acquisitions) although this will affect the accuracy of the lifetime determination ( figure 5) . we have demonstrated long-term observation of living biological specimens with diffraction limited imaging performance with no significant degradation in signal to noise. this is due to the lower photon flux of excitation in each beamlet compared to a single beam scanning system [cit] ."
it adopts a hybrid approach similar to wisemac to remove the periodic synchronization overhead. it uses an adaptive estimation algorithm to find an exact error margin to improve the energy efficiency. the estimation algorithm exploits the relation between error size and synchronization interval from figures 3(a) and 4.
"dt n : t n à t nà1 : m n : margin size. e n : error size of the estimated wakeup time, that is, t ã n à t n t: the wakeup interval time of rx node in terms of rx node's clock."
"the mm-flim microscopy system is shown schematically in figure 1 . for multifocal beam generation, a phase-domain spatial light modulator based, arbitrary pattern holographic projection system was implemented, using techniques originally developed for optical tweezing [cit] and single molecule imaging with fluorescent correlation spectroscopy [cit] . horizontally polarized laser light from a ti:sapphire chameleon ultra ii laser was expanded and collimated to overfill a near-ir (nir) optimized slm (boulder non-linear systems inc.). using a doubly weighted gerchberg-saxton (dwgs) iterative phase retrieval algorithm [cit], the appropriate holographic phase pattern was calculated and projected onto an slm, which was positioned in the fourier plane. coupled with the incident laser light, the required laser beamlets were generated experimentally at the focal plane. a blazed grating phase pattern was convolved with the beamlet phase pattern to efficiently project the beamlet pattern into the first order diffraction pattern. the patterned nir light was then optically relayed through a set of xy galvanometer scanners and transmitted through a long-pass dichroic filter onto the back aperture of a x40 1.3n.a. plan fluor oil immersion microscope objective [nikon instruments ltd] such that the back focal plane was conjugate to the slm. due to issues associated with projecting a square pattern onto a circular aperture, the beam size was set so that it slightly under-filled the pupil plane of the objective. therefore the multifocal microscope system functioned with an effective excitation n.a. of 1.2. since the focal plane was conjugate with the image plane of the slm, the 2d beamlet array was re-created at the focal plane and was raster scanned across the sample in beam scanning mode (using the galvanometer scanners). two photon excited fluorescence was then collected from the focal plane by the objective, descanned by the galvanometer scanners and re-imaged via a dichroic mirror onto the back aperture of a x10 0.3n.a. plan fluor objective [nikon] and focused onto the megaframe spad array."
"as understanding the exact characteristics of sources of uncertainty is essential to finding the fundamental solution to these problems, there have been many related research efforts. these mainly analyzed the characteristics with only a few fixed measurement intervals because their targets are periodic synchronization protocols. with limited previous work, it is difficult to discover the comprehensive characteristics of uncertainty sources."
"the measurement results for the relative clock drift are given in figure 2 with respect to some selected timestamp message intervals. figure 2 shows that gaussian distribution works only for large intervals against the previous study results that claim gaussian distribution of the relative clock drift size. 12 if the interval is smaller than or equal to 1 min, the distribution of the relative clock drifts exhibits a very different form."
"however, the linear regression and pll require long-precision floating point numbers and a large memory size, which can be a burden in low-cost wsn nodes. moreover, the bounded-drift model can be used for both aperiodic and periodic synchronous protocols, while linear regression and pll are usually used for periodic synchronization protocols."
"most wsn nodes adopt a clock source such as a 32 khz-oscillator. thus, clock errors due to the coarse granularity always exist and the size is small but fixed regardless of the interval size. moreover, a packet is generally handled by a specific task for transmission in a wsn node. if the interval size decreases, the more frequent packet transmission causes higher processor load resulting in increased scheduling delay and variations. 11 thus, these errors caused by coarse granularity or scheduling overhead dominate the overall relative clock drift when the interval is small."
"synchronous mac protocols require many control packets and consume a large portion of the wakeup period for synchronization. 2, 15 each node can send packets to destinations without worrying about clock drift and thus easily support control plane functions of the upper layer such as route recovery."
"synchronous mac protocols such as s-mac 2 and t-mac 3 are computationally expensive and consume a significant amount of energy in exchanging control packets between neighbor nodes to ensure synchronization of nodes according to a sleep-wake timetable. in this way, the transmission (tx) node wastes very little energy during its transmission period because it knows the exact wakeup schedule of each neighboring node. nonetheless, synchronous mac protocols waste energy due to the large synchronization overhead, especially for low-traffic applications. these problems get exacerbated when individual clocks have random amounts of drift depending on various environmental factors."
"each configuration of the considered protocols is shown in table 1 . the field of ''stored variables'' indicates the total number of variables maintained at each rx node to estimate the margin size. since the other protocols except for the mfi-based protocol require very small sizes of memory, they have no difficulty in supporting any wsn node platforms."
"when the interval is fixed, the mfi-based mac protocols, that is, mfi(2) and mfi(1.2), show almost the same performance regardless of r. only one subrange of the interval is used in this case so the energy consumption is independent of the number of subranges. when the packet interval is 1 min, widemac consumes 84% less energy than mcdr and almost the same energy compared to the other adaptive algorithms with the same safety factor. when the interval is 1 h, it consumes 5% less energy compared to mfi. for the fixed intervals, the margin size of widemac is set to is an exponentially weighted average of given input estimation errors. this helps the estimation based on history of (dt n, e n ) pairs to be reasonably accurate."
"when the inter-packet time is 1 min, rats shows a 5% failure rate which is the highest compared to other protocols. although rats adaptively calculates the window size and synchronization interval, it also relies on pre-defined configuration values such as d and e max . these values are dependent with the environment which sensor nodes are installed in. thus, they cannot provide an optimal performance in the different environment. table 3 shows the total packet delivery ratio for each protocol. since packet delivery ratio is proportional to packet transmission delay and energy consumption, it is a very important factor. as shown in the table, widemac achieves a very high delivery ratio. although it does not aim at periodic data transmission, it shows that widemac can support a low energy consumption and high delivery ratio simultaneously."
"the optimization problem attempts to maximize the minimum sinr of the uplink and downlink transmissions while satisfying the sinr constraints. to effectively solve this max-min optimization problem, (13) is rewritten as a linear programming problem by introducing a new variable k. then, the linear programming problem for (13) can be expressed as follows."
the kalman controllability rank is used to determine the condition. the determination model of the controllability of the urban rail transit network is given by
"node matching. in this paper, the driver node matching method is improved based on the hopcroft-karp algorithm [cit] . if there is no shared head node or tail node on all edges of the network, the network achieves maximum matching. if the network does not match exactly, the value of the driver node is equal to the number of nonmatching nodes. for a nonmatching node, a given input signal can reach all matching nodes to control the entire network. even with different initial searches of matched edges, the number of minimum driver nodes is fixed. for the network of rail transit passenger flow, it is necessary to identify the driver nodes of the flow control station to optimize the passenger flow network and enable network control. this paper proposes an optimization method of flow control based on driver node matching according to the network topology and the passenger flow characteristics of urban rail transit. the method is described as follows."
"combining the network structure with graph theory, the matching method of the node and edge is used to determine whether the network can be controlled. if the input signal can reach all paths by input signals 1, 2, and 3, the system is controllable (figure 1 ). how can an uncontrollable network be converted into a controllable network, and how can the minimum input signal be determined? this is a matching problem. the nodes in the network can be matched by the binary graph method. a matching edge means that any two directed edges do not have a common vertex (head or tail node). the matching node is the head node of the matching edge."
"for a given dataset, all pairwise dissimilarities between songs are computed using the measures described in section iii-b. this results in dissimilarity matrices, having values close to zero whenever two pieces are found to be similar. in order to determine the accuracy of the proposed rhythmic similarity measure, the accuracies of a -nearest neighbor (knn) classification will be determined. for this, each single song will be used as a query for which a classification into one of the available classes is desired, i.e., a leave-one-out cross validation is performed using the computed dissimilarity matrix as an input. the value that determines the number of neighbors is varied in the interval, and the best accuracy achieved by varying is then reported. in order to determine if these accuracies are over optimistic, the knn accuracies will be compared with results achieved using a fisher lda classifier and a pairwise svm classification using a linear kernel. for svm, the implementation included in the weka software [cit] has been used without any parameter changes. both lda and svm classifiers are evaluated using leave-one-out cross-validations."
"in order to evaluate the relation between the proposed distance measure and the way human subjects perceive rhythmic similarity on the used data, a listening test was conducted. for this test, 11 subjects were asked to judge the similarity measurements performed on d2 which lead to the optimum classification performance for this dataset in section iv-a ( for lda). each subject was asked to decide which of two comparison samples was rhythmically closer to a reference sample. a total amount of 25 reference samples were randomly chosen from d2 and presented to each subject. one of the comparison samples was the closest to the reference according to the proposed rhythm similarity measurement, while the other was the sample which was positioned in the middle of the ranked list of samples produced by the suggesting method as being similar to the reference sample. the subjects could decide for one of the two samples being closer, or they had the possibility to state that both comparison samples are equally close to the reference. all subjects had practical experience in all style of dances present in the dataset (cretan dances). they were informed that all music will be traditional cretan dances, but not exactly which type of dances. furthermore, they were asked not to restrict their judgement on the recognition of the class, but to concentrate on judging rhythmical similarity, independently of the class affiliation. the result is shown in table v, and it can be seen that in 64% of the cases the proposed measurement agrees with the listeners' judgements. in only 16% of the cases, the proposed measurement contradicted the listeners' opinion. no difference regarding the similarity of the two comparison samples was perceived in 20% of the cases. these results prove that apart from the objective verification of the proposed method in the classification task, the method is characterized by a high correlation of the way subjects perceive rhythmic similarity."
"the degree value of the passenger flow network of urban rail transit reflects the accessibility of network nodes. a larger value indicates more transfer choices for a station. conversely, a smaller degree value indicates weaker accessibility of a station. passengers may need to make a high number of transfers before arriving at their destination."
"the empirical results show that widemac reduces the energy consumption by 90%, when compared to maximum clock drift based protocols, and provides a stable and fast margin estimation, when compared to other adaptive protocols. they also confirm that widemac supports a wide range of duty cycles without performance degradation. therefore, the proposed scheme is widely applicable and suitable for any application that requires very low energy consumption."
"for d2, table iii shows a considerable advantage for the proposed scale distance measure, which achieves an accuracy of 76.1% with a confidence interval of 6.2%: on this dataset it outperforms the cosine measures by 21.8/31.4 percentage points."
". this leads to an interesting interpretation: fixing the maximum lag of the autocorrelation results in a vector of a given length, and increasing the number in the stm descriptors equals to giving more weight to higher lag values within this vector."
"in this research, the characteristics of sources for uncertainty were extensively measured according to various measurement intervals. through analyzing the relation between the clock drift size and the measurement interval, it was found that the ratio of the synchronization error to the synchronization interval can be modeled by a linear function without serious errors. using this result, a new wsn mac protocol named widemac was designed to use two estimation algorithms that estimate the wakeup time and the upper bound of the total timing error."
"(2) according to the structure controllability model, this paper constructed a method of controllability determination for an urban rail transit network. this paper proves the theory of network controllability by taking as an example the passenger flow of the beijing urban rail transit network. the passenger flow network is controllable when the number of driver nodes is 25.3% of the entire network."
"we carry out extensive simulations to evaluate the throughput performance of pocmac using matlab. for our simulations, we consider a single-cell system with an ap having fullduplex capability and its associated clients with backlogged user datagram protocol (udp) packets. we use a disk region with a radius of 100 m, where the ap is located at the center of the region, and all clients are randomly distributed within the region. the transmission rates are set to 6 mbps for the control frames and 54 mbps for the data frames, and the overhead for each control frame is less than the difs duration. each tx attempts to transmit as many udp packets as possible. the reported values for the simulation results represent the average of 1,000 transmission sessions. we compare the throughput performance of pocmac with that of full-duplex without a power control scheme and that of the csma/ca-based halfduplex scheme, which allows only a single transmission at a time. in contrast to section iv-a, because a number of clients that can be considered as the rx are distributed in the region, we can confirm the effect of receiver selection by the rssb contention mechanism on throughput performance improvement. the parameter values used in the simulations are listed in table i . fig. 11 shows the average throughput with respect to α in the ap. in the half-duplex case with csma/ca, there is no change in the throughput performance because it is not affected by the suppression level of self-interference cancellation. on the other hand, pocmac achieves a higher throughput performance than the other schemes over the entire range of α, and the throughput is saturated after α is greater than 65 db. when α is greater than 65 db, the throughput performance levels off because the ap already uses its maximum transmit power and the selfinterference is sufficiently suppressed with the high value of α. the transmit power is already saturated with its maximum value, and the throughput performance cannot be further improved. pocmac without the rssb contention mechanism shows a lower throughput performance than that with the rssb contention mechanism. this implies that the rssb contention mechanism contributes to the improvement in the throughput performance."
most existing research calculates the absolute clock drift from the difference between each node's clock and gps time. 12 these experiments are useful to obtain the characteristics of the clock drift incurred by the oscillator or crystal.
"for high precision multiphoton flim, time-correlated single photon counting (tcspc) is unparalleled in its measurement accuracy [cit] . in terms of imaging speed, tcspc is fundamentally limited with respect to photon counting rate, since the stochastic nature of the emission process requires that the detection rate is much less than one photon per excitation event to prevent inaccuracies in lifetime determination [cit] . consequently, acquisition times for laser scanning flim are in the order of minutes, whereas many dynamic biological events occur on significantly faster timescales [cit] . in order to overcome this limitations, parallel signal acquisition using arrays of laser beams with either photomultiplier arrays [cit] or time-gated camera detection systems [cit] have been employed. however, in implementations to date, accurate determination of fluorescence lifetime with large numbers of channels in parallel is limited either due to cross-talk in multi-anode pmt's [cit] or subject to systematic error due to measurement methodology in camera systems [cit] . for a single beam scanning system, count rate limitations can be abrogated by parallel detection in a multi-anode pmt [cit] . whilst, significantly increasing frame rates, this is at the cost of significant pulse pile-up [cit], leading to count rate dependent errors in the observed fluorescence lifetime. furthermore, the count rates reported for in vivo imaging with a low axial and lateral resolution are unobtainable in high na lenses without excitation saturation and significant phototoxicity and photobleaching. thus, the ability to quantitatively measure complex biological events with high temporal resolution remains a significant challenge."
"two more experiments have been conducted to evaluate the robustness of the proposed method. for these experiments the svm classification that resulted in the best accuracy of 82.3% on the midi data has been used, which means that all scale coefficients until have been used in the stm (see table ii ). again, only the melody lines have been included in the oss computations, while the percussive instruments have been left out."
"integral non-linearity (inl) -variations in the temporal bin size between detectors as a result of the cmos fabrication process (see figure 4) . this was compensated using a rational resampling technique. this technique incorporated first interpolation and decimation to change the sampling rate by a rational factor, thereby rescaling the temporal data taken from each detector into the same time bin size. data was saved and subsequently analyzed using tri2 lifetime analysis software [cit] ."
"(1) degree. the degree is defined as the number of connections between node and other nodes. the greater the degree is, the more connections between nodes and the more important the nodes are in the network. the degree is given by"
"the control method of this paper relies on the basic theory of macroscopic analysis of the whole network controllability state and reveals the relationship between driver nodes and controllability. an optimization method for flow control of urban rail transit is proposed based on the controllability of a large-scale network. this method does not have the disadvantages of current research methods that focus on flow control of a single station or local area. this method is highly suitable for the overall optimization of a passenger flow network with complicated operations. however, specific flow control measures are not addressed. to ensure the safe and efficient operation of the urban rail transit system, important directions for future research include determination of the flow control intensity and measures based on the optimization of the flow control stations and realization of the coordinated optimal control of the macroscopic system and micro individual systems."
"increases, and as a result, the probability that the client with a small contention window size can access the wireless channel first, increases. using the rssb contention mechanism to determine the receiver, pocmac enables the candidate client that can maximize full-duplex capability to have a higher probability of receiving the downlink transmission from the ap. during this contention, collisions among candidate clients may occur if more than two clients choose the same back-off number. in this case, pocmac fails to select the rx for downlink transmission, and the tx that successfully transmitted rts to and received cts from the ap performs the half-duplex uplink transmission."
"second, to maximize the network throughput performance, we formulate an optimization problem for calculating the optimal transmit powers of the ap and client. the optimization problem is then solved as part of the proposed mac protocol, namely, power-controlled mac (pocmac), to coordinate the uplink and downlink transmissions. by adjusting the transmit powers of the ap and transmitter, the inter-client interference is minimized, and higher sum throughput of uplink and downlink transmissions can be achieved. pocmac uses additional short control frames for selecting a receiver and an acknowledgement frame for completing full-duplex transmissions. in spite of the increased overhead due to the control frames, the proposed mac protocol can achieve a high performance gain because the ap can additionally transmit a long data frame while it is receiving an uplink transmission."
"wireless sensor networks (wsns) have an attractive emerging technology that can support a myriad of services involving environmental monitoring, sensing, and data collection. in particular, wsns are playing an increasingly important role in monitoring applications such as automatic metering reading or advanced metering infrastructure. 1 since it is prohibitively expensive or even infeasible to replace batteries of wsn nodes in many environments, it is important that the network remains operational for a long period (i.e. have a long ''lifetime''). given the unique traits of monitoring applications, a wireless sensor media access control (mac) protocol should be designed differently from other ad hoc or wireless mac protocols, explicitly considering its particular characteristics."
"a description of the rhythmic content of a piece of music based on the scale transform was proposed. this description is robust to large tempo variations that appear within a specific class and to large tempo overlaps between classes. using simple distance measure and classifier techniques, the descriptor vectors can be used to classify the samples with high accuracies. the approach is computationally simple and has no need of any tempo or meter estimation which might be desirable for certain kinds of music signals. based on mutual information criteria, a method was proposed for choosing a feature set that is optimal for the classification task. the relation between autocorrelations sequences and the riemann zeta function in scale domain was explored, while a discussion of the signal reconstruction by applying inverse transform enabled to gain valuable insight into the relation between variables in scale and in time domain. the inclusion of the traditional turkish dataset provided us with a potential starting point for a detailed study of rhythmic characteristics of turkish traditional music. the suggested measure provides a simple and efficient tool for the description and comparison of rhythm content, especially applicable to music with little or no percussive content and strong tempo variations."
"the plasmids encoding human egfr (with the (a206k) dimerization-deficient mutant) and human erbb2 tagged at the c-terminus were constructed by inserting the cdna for the respective receptor into a modified version of the pegfp-n3 vector (clontech) in which the ala206 to lys mutation had been incorporated into the egfp sequence. the plasmid encoding human erbb3 tagged at the c-terminus with mrfp was constructed by inserting the erbb3 cdna sequence upstream of mrfp within a modified pcdna3.1 vector. the plasmid encoding grb2-mcherry was a gift from dr perani and prof peter parker (king's college london, uk)."
"in this section, experimental results for the proposed mac protocol are presented. to evaluate the performance, four types of mac protocols including widemac were implemented using telosb 22 and tinyos 26 as follows."
"in the optimization, the transmit powers are assumed to be continuous values. however, in most communication systems, they are incrementally increased/decreased by a fixed integer value. for example, in the communication devices used for our experiments, the transmit powers are discrete integers in the range of [−12, 19] dbm. in practice, the optimization can be reformulated as an integer programming problem."
"mcf7 breast carcinoma cells were grown in dmem medium supplemented with 10% fetal bovine serum (fbs), penicillin g (100 u/ml)/streptomycin (100 mg/ml) and 1% l-glutamine, and cultured in an atmosphere containing 5% co2/95% air (v/v). for imaging, cells were plated into 3.5 cm glass-bottom culture dishes (paa laboratories) 24 hours prior to transfection. for egfr-egfp/grb2-mcherry and her2-egfp/her3-mcherry co-expression studies cells were co-transfected using fugene 6 (promega, madison,wi, usa) with 1.5 µg and 3 µg of donor and acceptor plasmids respectively. following transfection, cells were maintained at 37°c in a humidified 5% co2 atmosphere for 24h to allow expression of exogenous proteins. for live cell imaging, cell culture media was replaced with phenol red-free optimem (gibco), supplemented with 10% fbs and 25 mm hepes. the microscope body was equipped with an environment chamber (solent scientific ltd, uk) enabling cell cultures to be stably maintained at 37°c for the duration of imaging experiments. human recombinant egf and neuregulin were purchased from peprotech, new jersey, usa."
"implies that client i has a high rss from the ap and low inter-client interference by the tx; therefore, we use the cw rssb,i of client i for providing a higher reception probability to the client with a high rss from the ap and low inter-client interference. thus, cw rssb,i decreases as"
"single-photon avalanche photodiodes (spad) array cameras developed for microscopy and spectroscopy techniques have been designed utilizing either custom or standard cmos technologies [cit] . whilst recent studies have shown the use of custom cmos in a parallelized 8 x 1 spad array for use in single molecule fluorescence correlation spectrocopy [cit], there are issues in scalability due to power requirements (6 w for 8 spads) [cit] . standard cmos spad arrays are better suited for creating large, power efficient (100mw for 1024 spads) and affordable spad array cameras while maintaining good (50ps) timing resolution. in previous studies, the 32x32 megaframe spad array has been presented where each individual pixel contained separate timing circuitry [cit] . due to the relative size of the timing circuitry and associated logic, the active region of each pixel is ~1%. hence when operated as a 32x32 pixel widefield camera, due to the low fill factor there is a low collection efficiency [cit] ."
"in the current architecture, up to 0.5m time-stamp/s are delivered to the field programmable gate array (fpga) for each pixel. each timestamp is 10 bits long resulting in 56 ns dynamic range covering the needs for most fluorophores and light sources commonly used in microscopy. the fpga circuit board (opal kelly xem3050) contains the spartan 3 fpga from xilinx (xc3s4000) and provides a usb2 interface to the host pc. the test software included with the fpga circuit (pipetest.exe, frontpanel 4.2.5, opal kelly) board can perform ~32mb/s streaming to the host pc. the firmware was written in verilog using xilinx's ise design tools, system edition, version 13.2. fpga board and megaframe camera are interfaced via a custom motherboard printed circuit board (pcb). the motherboard pcb provides the inputs to laser pulse and laser scanner synchronization. laser pulse synchronization signal is fed directly into megaframe camera for reverse start-stop tcspc timing. laser scanner synchronization signals are interfaced to the fpga."
"however, pocmac can overcome this problem in the same situation. as shown in fig. 9, the sinr uplink and sinr downlink of pocmac increase and then converge as the distance between the tx and the rx increases. except when the distance between the tx and the rx is very small, both the sinrs of pocmac are higher than the sinr threshold. when the distance between the tx and the rx is small, the inter-client interference caused by the tx significantly reduces the sinr at the rx, and the sinr uplink and sinr downlink of pocmac then have low values. however, if the sinr downlink at the rx is lower than the sinr threshold, the rx cannot receive the data transmission from the ap owing to inter-client interference. in this case, pocmac operates through half-duplex communication after receiving the cts-d frame. then, the sinr uplink obtained is equal to that of full-duplex without power control during the period."
"( ) is the passenger flow state level of station in time . according to the national standard subway design specification (gb50157 [cit] ) of china and the transit capacity and quality of service manual (2nd edition) of tcrp, a single facility is divided into four levels (table 1) ."
"matrix is an input matrix that consists of flow control stations. the flow control stations are shown in table 3 . the matrix removes the evening peak flow control stations, such as wudaokou, liangmaqiao, jintaixizhao, chaoyangmen, fuxingmen, yonganli, fengtaikejiyuan, and dongwuyuan."
"we selected the tiantongyuanbei, tiantongyuan, tiantongyuannan, and lishuiqiao stations of line 5 of the beijing subway (figure 4(d) ). the passenger flow network of the four station nodes is converted into a binary graph (figure 4(e) ). the maximum matching edge of this small network is shown as the red matching edge in the binary graph (figure 4(f) ). v 2, v 3, and v 4 are the matching nodes, and v 1 is the nonmatching node. therefore, the driver node is v 1 ."
"to analyze the effect of inter-client interference between the tx and the rx, we perform a simulation for the sinr of the uplink and downlink transmissions in the ap with respect to the distance between the tx and the rx. as shown in fig. 8, we configure the scenario for this simulation as follows. the positions of the ap and tx are fixed, and the tx and rx are located at a distance of 100 m from the ap. only the rx moves to the opposite side of the tx along a semicircleshaped trajectory. thus, the tx and rx are at the same distance from the ap, and only the angle between the tx and the rx changes. with this topology, it is not necessary to select the rx because there are only one tx and one rx, and thus, the rssb contention mechanism is disabled. fig. 8 . network scenario for varying distances between the tx and the rx. fig. 9 . sinr uplink and sinr downlink with respect to the distance between the tx and the rx when the suppression level of self-interference cancellation is 70 db. fig. 9 shows sinr uplink and sinr downlink with respect to the distance between the tx and the rx when the suppression level of self-interference cancellation is 70 db. in the case of full-duplex without power control, sinr uplink does not change as the distance between the tx and the rx increases, because it is not affected by the position of the rx. in contrast, sinr downlink increases as the distance between the tx and the rx increases. however, when an sinr threshold is required to successfully receive the data transmission (e.g., we use an sinr threshold of around 6 db in the simulations for the throughput performance), the rx cannot receive the downlink transmission owing to the low sinr downlink in almost all positions. thus, full-duplex without power control cannot utilize full-duplex capability in this case."
"for the proposed similarity measure, there are mainly two critical parameters: the length of the maximum lag considered in the autocorrelation and the numbers of coefficients of stm in (8) . the influence of these parameters will be explored by computing the accuracies in a grid search of these two parameters. for each dataset the optimum number for the maximum lag will be determined, and the effect of varying the number of scale coefficients will be explored."
"in order to compare accuracies lifetime determination techniques for image acquisition of live cells at high speed, data sets of mcf-7 human carcinoma cells transfected with egfp were imaged for 500 milliseconds and 5 seconds per frame ( figure 5 ). levenburg-marquardt (l-m) lifetime fitting (which we normally use to fit tcspc data) was compared with rapid lifetime determination (rld) [cit], which is commonly used for analysis in gated image intensifier (goi) based flim systems. these data demonstrate that l-m fitting with tcspc data, even at low photon numbers gives a much more accurate determination of the lifetime per pixel than rld. it should be noted that this comparison is at poisson limited signal/noise which is superior to goi performance. we also show that frame rates in excess of 2 hz are readily achievable with the system, without the limitations imposed by goi based systems [cit] . mcf-7 human breast carcinoma cells were transiently transfected with egfp only and datasets were acquired for a 7x7 array for 500 milliseconds and 5 seconds. in a 256x256 dataset, a maximum of 200 [cit] photons were collected per pixel at 500 milliseconds and 5 second acquisitions respectively. in order to simulate the analysis of time gated camera to compare with levenburg-marquardt (l-m) fitting using tri2 we used the 2 gated rapid lifetime determination (rld) approach for single exponential decays with 2 ns gate size. lifetime images are displayed for both techniques with no binning and 5x5 circular binning for both 500 milliseconds and 5second acquisitions and their histograms compared. l-m clearly outperforms the rld in all data analysis performed giving a much more accurate determination of the lifetime."
"for the small interval, all the adaptive algorithms need almost 60-70 h to obtain stable margin values as shown in figure 7 (a). it implies that infrequent but abrupt changes in timing information occur. for medium and large intervals, that is, for 30 min and 1 h, they require 50 and 40 h until margins converge, respectively. it is obvious that the larger interval generates the larger margin size. since the increased margin is less affected by the abrupt change in clock drifts, the convergence time can be reduced. all the protocols experience similar failure rates regardless of interval size. thus, failure rates only occur when the inter-packet time is 1 min as shown in figure 8 (a). as shown in the figure, only mcdr always estimated correctly without any failure during evaluation. other protocols except for rats failed to estimate the wakeup time only one time but no estimation failure happened after they obtained converged margin sizes. mfi(1.2) and mfi(2) use only one sub-range if the inter-packet time is fixed so they should provide the same performance."
"in summary, pocmac includes (i) a contention-based receiver selection scheme to provide a higher reception opportunity to clients with low interference and (ii) a transmit power adjustment scheme for computing the optimal transmit powers of the ap and tx. pocmac needs to collect the channel gains, such as g t x,ap, g ap,rx, and g t x,rx, and these channel gains can be obtained when control frames are exchanged between the ap and clients. for example, the channel gain from the tx to the ap (g t x,ap ) is obtained when the tx transmits a control frame to the ap for a transmission request. new control frames designed for pocmac will be explained in detail in section iii-d. thus, the ap and rx can successfully decode signals without significant interference, and therefore, the ap can fully exploit full-duplex capability for higher throughput performance. pocmac performs the following key functions: 1) collecting information on the interclient interference between the tx and the rx, 2) determining the rx from among the clients, and 3) calculating and notifying the optimal transmit powers for the ap and tx."
"when widemac initializes or loses synchronization, it operates similar to x-mac 4 that uses a smallsized packet called preamble to find the wakeup time of the rx node. the rx node replies with a preamble acknowledgment (ack) and waits for a data packet. if the tx node receives the ack, it sends a data packet to the rx node. otherwise, the preamble will be retransmitted."
"the fourth row of table iii ( i.e., for ) shows the accuracies that can be achieved when the tracks containing percussive accompaniment are also included in the computation of oss. the accuracies are then in general improved, since the percussive accompaniment is typically the same for one specific usul. the relatively high values in the third row clarify the information about the usul that is contained solely in the melody line of the composition. as the difference between the best accuracy in the third row and the best in the fourth row is only 7.9 percentage points, it can be concluded that this relation between the melody and the usul is very strong."
"where f is a proportional constant linked with the communication energy consumed in 1 s. therefore, it can be concluded that the maximum relative clock drift rate determines the average energy consumption of synchronization protocols. the experiments were performed for 14 days in an outdoor environment. experimental results only for telosb are presented because those for micaz and hmote show similar qualitative results."
"the beam generation technique offers significant flexibility in terms of near-field resolution control as well as having complete programmable control of the number and positioning of each beamlet. the excitation beamlet array generated fluorescence in the microscope focal plane via a two photon absorption process and was back-projected through the system to the detection plane. for parallelized tcspc detection, each fluorescence image of the beamlet array was focused onto a single spad of a detector array (figure 2a & b) . to ensure optimal efficiency of detection, the fluorescent beamlet projected onto the detector aperture was significantly smaller (1.8 µm fwhm) than the active area of the spad (6 µm dia.). the field of view of the microscope was determined by the magnification between object and detection planes, with beamlets separated in the detection plane by the detector spacing, 50 µm. with a 40x objective (nikon plan fluor oil immersion 1.3 na) the field of view was 100 m for an 8 x 8 beamlet array. precise alignment and matching of the beamlet spacing and angular orientation onto the detector array was performed to ensure a fill factor of 100%. the dwgs algorithm [cit] was implemented (incorporating direct feedback from the detector array) to compensate for poor intensity fidelity between beamlets [cit] leading to significant improvement in uniformity of the beamlet intensity variation."
"further to imaging egfr/grb2 signalling dynamics in living cells mm-flim was appied to the analysis of ligand-dependent receptor heterodimerisation. we examined the interaction between the egfr family members her2 and her3, given the clinical significance of this heterodimer pair. her3, which lacks intrinsic tyrosine kinase activity, can form competent signalling complexes through heterodimerisation with other egfr family members following activation by its ligand neuregulin. as a potent driver of the pi3k/akt-dependent cell survival pathway this novel kinase-deficient egfr family member is gaining greater attention as a potential therapeutic target in cancer [cit] . in particular, the role that the her2/her3 heterodimer plays in tumourigenesis and treatment resistance in breast cancer has become the focus of intense study. in her2-amplified breast cancers, it is the her2/her3 heterodimer which is largely responsible for driving tumour growth. poor response to her2-targeted therapies (for instance trastuzumab) in these patients is believed, in part, to be associated with increased expression of her3 [cit] ."
"we carry out simulations to verify the fairness performance of pocmac. in the simulations, we use the same simulation setup and parameters as described in section iv-b and table i, respectively, and we use jain's fairness index [cit] to evaluate the fairness in throughput among clients. fig. 13 shows the throughput fairness with respect to α when the number of clients is 30. it is observed that pocmac achieves the highest fairness index among all the schemes in our tests. with pocmac, the fairness index is higher than 0.97, which implies that the clients have almost the same transmission opportunity on a long-term basis. it is also observed that the value of α does not affect the fairness performance. it is worth noting that pocmac does not provide only clients near the ap with more opportunities, because the rx will be determined by the signal strengths from the tx as well as that from the ap. as a result, the clients can get almost the same opportunity to transmit, thus enhancing the fairness performance, as shown in fig. 13 ."
"on the contrary, the maximum relative clock drift rate decreases rapidly with the interval in figure 3 (b). equation (3) and figure 3 (b) imply that periodic synchronization may not be optimal in terms of energy consumption. it is advantageous to perform synchronization when the tx node has a packet for saving energy."
"to measure the channel information and sinr of the uplink and downlink transmissions, we set up the warp hardware as shown in fig. 15 . as described in section iv-a, the positions of the ap and tx are fixed, and only the rx moves from a to d. the tx and rx are located on the floor, and the ap is located at a height of 4.1 m from the floor. while the tx sends a signal for uplink transmission to the ap, the ap simultaneously sends a signal for downlink transmission to the rx. at each rx position, the ap measures the sinr of the uplink transmission sent by the tx, and the rx measures the sinr of the downlink transmission sent by the ap. table ii lists the experiment results obtained with and without the power control scheme of pocmac. for fullduplex without power control, there is no significant change in sinr uplink for the different rx positions because the positions of the ap and tx are fixed. however, sinr downlink decreases significantly as the rx approaches the tx, because the signal sent by the tx increasingly interferes with the reception of the downlink transmission in the rx. therefore, because the sinr threshold for successful reception is 6 db, full-duplex communication is not possible at any of the rx positions. the power control scheme of pocmac, as described in section iii-c, can overcome this problem. when the rx is located at b, c, and d, the inter-client interference caused by the tx is reduced by adjusting the transmit powers, and as a result, sinr downlink increases significantly. in addition, full-duplex communication is possible when the position of the rx is b, c, and d because the sinr threshold is 6 db. when the rx is located at a, even if the ap uses the maximum transmit power of 5 dbm and the tx uses the minimum power of −12 dbm, it cannot overcome the high inter-client interference caused by the tx owing to the short distance between the tx and the rx."
"to verify the performance of the proposed mac protocol, we analyze the signal-to-interference-plus-noise ratio (sinr) of uplink and downlink transmissions with respect to the interclient interference. further, we compare the throughput and fairness performance of the proposed mac protocol with other schemes through extensive simulations. in the simulations, the proposed mac protocol enhances the throughput performance by around 180% as compared to an ieee 802.11-based halfduplex system, and by around 145% as compared to a fullduplex system without any schemes such as power control and receiver selection. in addition, we conduct software-defined radio-based experiments to confirm the power control effect of the proposed mac protocol. the simulations and experiments show that the proposed mac protocol achieves better performance than other schemes in terms of both throughput and fairness by effectively managing the inter-client interference."
"for live cell imaging experiments, mcf-7 cells were transiently transfected with expression constructs encoding egfp-tagged her2 (her2-egfp) and mrfp-tagged her3 (her3-mrfp). cells were imaged every 15s for 40 minutes with the addition of neuregulin (50 ng/ml) occurring at the 10 minute time point. following expression, her2-egfp and her3-mrfp both exhibited a distinct pattern of spatio-temporal localization within mcf-7, which would be extremely challenging to reveal using convention lifetime imaging techniques."
"this paper analyzes the topological characteristics of an urban rail transit passenger flow network. then, a controllability model of the passenger flow network is constructed based on traditional control theory. an improved controllability determination method for uncontrollable networks is proposed, and the minimum number of driver nodes in the controllability passenger flow network is calculated. the method of flow control optimization is built based on driver node matching, and the specific flow control station set for controllability of the passenger flow network is presented. the method is validated based on actual passenger flow data for the beijing subway network. in the actual flow control process, the passenger flow will change. the set of flow control stations is obtained at different time periods. when the passenger flow is relatively stable, the flow control stations tend to be fixed."
"the newly added flow control stations are transfer stations that have 4 or 5 degrees. these transfer stations have large passenger flow, and the internal structures are highly complicated. among these stations, xizhimen is a transfer station with three subway lines and, consequently, complicated transfer of passenger flow. flow control can be implemented inside the station or in security. the proposed method in this paper is repeated. when the flow control measures are implemented after four time cycles, two flow control stations there may be a subpeak state, and the implementation of flow control measures should be a dynamic process."
"the energy consumption of the rx node is mainly defined by the wakeup time interval. however, the energy consumption of the tx node is affected by many factors such as duty cycle and traffic rate. therefore, the total energy consumption of the tx node is measured every 10 h for performance evaluation. experiments were performed with six pairs of rx and tx nodes in the same outdoor environment for 100 h, where tx nodes send packets to rx nodes according to the four inter-packet time distributions, respectively, for each experiment shown in table 2 ."
"we selected the junshibowuguan and wanshoulu stations of line 1 and the xidiaoyutai, gongzhufeng, and lianhuaqiao stations of line 10 of the beijing subway (figure 4(g) ). the passenger flow network of the five station nodes is converted into a binary graph (figure 4(h) ). the maximum matching edge of this small network is shown as the red matching edge in the binary graph (figure 4(i) ). v 2, v 3, and v 5 are the matching nodes, and v 1 and v 4 are the nonmatching nodes. therefore, the driver nodes d are v 1 and v 4 ."
"in each experiment, the preamble packet size is 8 bytes and its transmission interval is 0.26 ms. 4 since the packet retransmission is disabled, the packet is dropped if wakeup time estimation is failed. all the algorithms except for the mcdr-based mac protocol use the recursive estimation algorithm to find the wakeup time of the rx node."
"due to their large size, fast speed, and safety, urban rail transit systems have become the backbone of city transportation. in recent years, the volume of passenger flow has increased rapidly. congestion of passenger flow is very high, especially during the morning and evening rush hours, which is a severe challenge for the operational safety of urban rail transit. with network integration of urban rail transit, traditional passenger flow control methods cannot accommodate the increasingly large-volume, line-intensive, complex organizational conditions in modern transit systems. the strategy of flow control optimization for urban rail transit network controllability provides a new perspective for network control."
"after the tx node discovers the wakeup time of the rx node, widemac does not send preambles when a packet transmission request arrives. instead, it estimates the wakeup time and the timing error size for the rx node. after the estimation, it sends preambles only within the estimated range of the wakeup time for the rx node. therefore, it can significantly decrease the energy consumption."
"wakeup time estimation algorithms in synchronization protocols can be broadly classified into three types, that is, linear regression, phase-locked loop, and bounded drift model-based approach. 16 linear regression finds the clock speed ratio s and the time offset d to minimize the difference between the fitted value and the sampled one as follows 17"
"we also perform another set of simulations to evaluate the sinrs of the uplink and downlink transmissions in the ap with respect to the suppression level α used for self-interference cancellation in the ap. to analyze the sinrs caused by the change in α, we use the same topology as that shown in fig. 8 and set the angle between the tx and the rx to 90"
"when the ratio of driver nodes is close to 0.253, the number of driver nodes is 65, and the beijing urban rail transit network is controllable during early peak. from the perspective of network controllability, it is suggested that the beijing urban rail transit increase the number of driver nodes to improve controllability and optimize the flow control strategy. we attempt to retain the original finite-flow station during the entire deletion process, and the updated topology diagram of flow control stations in beijing urban rail transit is shown in figure 6 ."
"oscillators or crystals that run individual clocks have different clock speeds within a margin of error that depends on the cost of each oscillator. furthermore, the clock speed of each node can change due to clock drift that may depend on various environmental factors. this results in a serious problem for sensor communications since the tx node does not exactly know when the rx node wakes up."
"the clustering coefficient of the urban rail transit network reflects the connection of transfer passenger flow between stations. the larger the aggregation coefficient is, the higher the connection degree between stations is."
"( according to the overall congestion risk assessment results, the value of ( ) is 1, 2, 3, 4. actual passenger flow demand per unit time. the larger the flow control rate is, the larger the flow control intensity is."
"in (17) and (18), p t,ap and p t,t x are increasing functions of k. by gradually increasing k, we can find the optimal transmit powers p * t,ap and p * t,t x that satisfy the constraints in (14) . as shown in fig. 4, the red line is a set of points (p t,t x, p t,ap ) that satisfy both (17) and (18). the optimal point (p * t,t x, p * t,ap ) is obtained by increasing k. the optimal transmit powers solve the linear programming problem to maximize the sinrs of the uplink and downlink transmissions."
"the dataset of turkish music, d3, consists of six different classes of rhythm, but unlike the other two datasets, the classes are not related to specific dances. the musicological term used for the different types of rhythm in this music is usul. each usul specifies a rhythmic pattern that defines the temporal grid for the composition. these patterns can be of various lengths from 2 up to 124 beats. the six usul in d3 have lengths from 3 up to 10: aksak (9/8), curcuna (10/8), düyek (8/8), semai (3/4), sofyan (4/4), and türkaksagi (5/8). these short usuls were chosen, because no sufficient number of songs with longer usuls were available to the authors. according to table i, the tempo variances within each class are much bigger than in d1 and d2. this is because samples in d2 are connected to specific dance movements which puts a natural constraint to the range of tempo variations. most of the samples in d3 are not dance music and as such, their tempo can vary in a much wider range. thus, features for the description of the rhythmic content have to be robust to these changes. in order to acquire the samples, the teaching software mus2okur [cit] was used, resulting in a collection of 288 songs, distributed among the six usul as shown in the last row of table i . the software gives a list of songs for a chosen usul, which are then exported to a midi file. thus, the data in d3 is available in form of symbolic descriptions, which means that their onset times can be read from the description. the midi files contain the description of the melody lines, usually played by only one or two instruments in unison, and the rhythmic accompaniment by a percussive instrument. as this content is separated into different voices, the rhythmic accompaniment can be excluded. this enables to focus on the relation between the melody of the composition and the underlying usul. to the best of our knowledge, such a study on usul has not been conducted before."
"for further investigation, figure 3 shows the measured maximum relative clock drift and rate according to the message interval. the line marked with empty circles represents the mcdr (u) 3 2 and shows the margin for the relative clock drift rate used in bounded drift model-based protocols."
"transit network. [cit], we extract the connections between stations in the urban rail transit passenger network. the beijing urban rail transit passenger flow network has 269 stations and 18860 edges, and the average degree is 70.1."
"rats protocol requires explicit synchronization which consumes additional energy. as the inter-packet time increases, the overall energy consumption of rats decreases. however, due to the overhead of explicit synchronization, the performance of rats can be degraded in terms of the high estimation failure and low packet delivery ratio even if it consumes the same energy compared to competitors."
"in contrast, asynchronous mac protocols such as x-mac 4 and ri-mac 5 do not waste energy on periodic synchronization. however, their main drawback lies at the other end of the spectrum. the tx node spends considerable energy to find the wakeup time of each neighbor when it has a packet to transmit. hence, they could waste a significant amount of energy, especially in low duty-cycle environments. to mitigate this problem, hybrid approaches such as scp-mac 6 and wisemac 7 have been introduced. however, they do not fully exploit the characteristics of sources for uncertainty, thus their improvements are limited."
"these localisations accumulated at the cell periphery and within a perinuclear compartment, the latter of which was far more pronounced in the case of her3-mrfp (figure 11) . the diffraction limited spatial resolution afforded by the mm-flim system enabled lifetime changes within these discrete subcellular compartments to be monitored over the course of imaging experiments and regions of interest were selected such that lifetime changes within these compartments could be assessed ( following the addition of neuregulin, time-lapse imaging revealed a gradual fall in the lifetime of the her2-egfp donor specifically within the perinuclear compartment (from 2.21 +/-0.02ns to 2.15 +/-0.02ns) due to accumulation of vesicles with short lifetimes whilst donor lifetime at the cell periphery remained relatively constant (figure 13) . these data constituted a fret efficiency increase from 3.1  1.3% to 5.7  0.9% after neuregulin addition. the fall in donor lifetime, indicating an accumulation of her2/her3 heterodimer within a perinuclear compartment, was detectable within the first 100s of ligand addition and stabilised after approximately 7 minutes, remaining low for the remainder of the observation period. interestingly, the lack of any detectable change in donor lifetime at the cell periphery suggests that newly formed her2/her3 heterdimers are rapidly internalised and targeted to subcellular compartments following neuregulin stimulation and therefore active receptor complexes are predominantly intracellular. a basal constitutive association between her2 and her3 was also observed within the perinuclear compartment but this was significantly increased following addition of ligand (figure 14) . the donor lifetime of control cells expressing her2-egfp alone (2.28  0.02ns) remained unchanged throughout the course of time-lapse experiments."
"the degree distribution of the passenger flow network of beijing urban rail transit is shown in figure 3 with the power law distribution and is thus a scale-free network. the station strength indirectly reflects the service capacity of the station. the cumulative strength distribution for the beijing urban rail transit passenger flow network is shown in figure 3(c) . according to the statistics, 4.5% of the station strength is greater than 15 000, and 88.2% of the station strength is smaller than 10 000. these results indicate that the intensity distribution of the station node is extremely uneven. the stations carrying large passenger flow are usually transfer nodes, which enhances the accessibility of the network. the average clustering coefficient of the beijing urban rail transit network is calculated to be 0.563, indicating high clustering. the cumulative distribution of the clustering coefficient for the beijing urban rail transit passenger flow network is shown in figure 3(d) . each station has a high connection degree with adjacent stations. when the clustering coefficient of the station is 1, the station has a low degree value. therefore, the relationship between the degree value and the clustering coefficient is negative. the average distance of the urban rail transit network indirectly reflects the transfer times of passengers. the shortest distance distribution of the beijing urban rail transit passenger flow network is shown in figure 3 (e). the average shortest path length of the passenger flow network is 1.69. thus, passengers reach their destinations on average by 1.69 times. the transfer rate is up to 84% within three times."
"however, full-duplex without power control does not lead to a significant performance improvement as compared to the performance of the csma/ca-based half-duplex scheme. in this case, even if α increases, the sinr of the downlink transmission at the rx does not change, as explained in section iv-a, while the sinr of the uplink transmission from the tx increases. note that without proper power control, the sinr at the rx cannot be adjusted to be higher for successful packet transmissions in these simulations. the simulation results indicate that pocmac can improve the throughput performance by around 180% as compared to the csma/ca-based halfduplex scheme, and by around 145% as compared to fullduplex without power control. fig. 12 shows the average throughput with respect to the number of clients when α is fixed at 60 db. it can be seen that the average throughput of all the schemes increases until the number of clients becomes 15. when the number of clients is greater than 15, the throughput performance of all the schemes is degraded because the frequency of collisions involving rts frames increases as the number of clients increases. even though the average throughput performance is degraded owing to the number of clients, pocmac exhibits the best performance over the entire range."
"phase-locked loop (pll) is originally designed to maintain the local oscillator in the same phase and frequency as the reference signal by applying a correction signal to the oscillator. the input of pll is the difference between the actual sample and the fitted value. it detects its slope and offset by using the proportionalintegral (pi) controller. pll needs less memory than linear regression, but requires a longer time for convergence. 18 the bounded drift model-based estimation algorithm is one of the simplest algorithms, but very practical in terms of implementation. in this model, the clock drift rate is assumed to be bounded by the maximum clock drift rate (mcdr) denoted as u. the upper bound of the synchronization error can be given as the product of the mcdr and the synchronization (or packet transmission) interval. therefore, the margin size m(á) for the error is usually set to"
"in addition, from (5) and (6), we define the sum rate r sum as the achievable sum rate of the uplink and downlink transmissions at the ap as follows."
"in this paper, an optimization method for flow control based on driver nodes is proposed to optimize an uncontrollable passenger flow network. the ratio of driver nodes to the nodes during early peak is shown in figure 5 ."
"previous research on network controllability has mainly been based on the general characteristics of complex networks. in recent years, a few studies have examined controllable analysis of real networks. however, these studies have mainly focused on the analysis of complex topological properties. there is no specific strategy for the optimization of network controllability. most studies have ignored the function attributes of the nodes and edge weights in the actual network, making it impossible to propose effective control methods and coping strategies for specific issues."
"since each term in equation (7) is expressed in a recursive form, the computational cost is very small. the overall procedures for the relative clock speed estimation is presented as follows ( figure 5 )."
"a simple solution to this is to use a global positioning system (gps) module or a very accurate clock (e.g. an atomic clock). however, these solutions are too expensive or energy inefficient. for example, a gps device is not only expensive but also consumes about 30 to 360 times more power than a radio chip. 9 currently, a more practical approach is to use a synchronization protocol to predict and adjust each clock experiencing drift."
distance between nodes and is defined as the number of edges of the shortest path between two nodes. the average path length of network is defined as follows:
"liu [cit] and applied the judgment of the state-space equation of control theory to network controllability for the first time. in addition, the directed network was transformed into a binary graph, and the maximum matching was calculated. liu's research represented a new starting point for network controllability and laid the foundation for subsequent studies by others. a great deal of subsequent work has begun to focus on the impact of network topology on the controllable performance of network structure [cit] . based on liu's research, the relationship between the controllability and energy consumption of different types of networks was analyzed from the perspective of energy consumption [cit] . nepusz [cit] converted the network to an edge-based model by considering the dynamics of the edges of the network. lombardi [cit] applied a controllable matrix to the network. the value of the matrix element was the path gain from the input signal to the node. [cit] evaluated changes and control costs of network controllability under cascade failure conditions. the number of driver nodes of a random network and scale-free network were calculated in cascade failures. a minimum structure perturbation method was proposed to optimize the controllability of the network [cit] . the minimum number of edges required for controllable optimization was equal to the minimum number of conversion edges, and a network with positive correlation facilitated optimal control."
"one of the most effective ways to increase the lifetime of a wsn node is to put it into sleep mode periodically but this causes an additional problem, that is, how to find the wakeup time of the reception (rx) node. to solve this problem, there exist two approaches that are classified into synchronous and asynchronous types."
"wireless monitoring is a very important application for wsns. however, existing wsn mac protocols do not meet the energy needs of such an application. in this article, various clock drift characteristics were extensively analyzed through experiments and an ondemand synchronous mac, called widemac, was designed to carefully exploit the real clock drift characteristics. this scheme adopts two estimation algorithms to keep track of the wakeup time of the rx node. these algorithms provide a precise estimation of both the relative clock drift and the total timing error bound, which results in significant transmission energy savings at the tx node."
"to experimentally evaluate the impact of power control by pocmac, we implemented a power control scheme for pocmac on the wireless open access research platform (warp) [cit] . we conducted experiments on a warpnet experiment framework using warp v3 hardware. the warpnet experiment framework supports real-time experiments and allows the physical layer processing and hardware control to be executed by the warp hardware. all the warp hardware devices are connected to a host pc via ethernet, and the host pc can control the operation of the warp hardware. we use a carrier frequency of 2.4 ghz and an ofdm physical layer with qpsk modulation. the initial transmit power and the maximum transmit power are set to 0 dbm and 5 dbm, respectively. based on the specifications of the warp hardware, the minimum transmit power is limited to −12 dbm and the transmit power is adjustable in increments of 1 dbm. the main objective of the experiment is to verify the sinr performance of the uplink and downlink transmissions by the transmit power adjustment scheme, as described in section iv-a."
"the transmit power control that determines the transmit powers of the ap and tx should 1) facilitate successful simultaneous uplink and downlink transmissions, and 2) enable each transmission to achieve the maximum sinr value. fig. 4 shows the feasible region and optimal point of transmit powers with respect to the transmit power of the ap and tx. the blue line is the upper bound that satisfies (11), and the green line is the lower bound that satisfies (12) . the gray-shaded area is the feasible region that simultaneously satisfies both (11) and (12), i.e., the transmit powers of the ap and tx in the region ensure the feasibility of simultaneous uplink and downlink transmissions. within the feasible region, we need to find the optimal transmit powers of the ap and tx to maximize the sinr values of the uplink and downlink transmissions. therefore, we formulate an optimization problem for the transmit powers of the ap and tx as follows"
where is the number of network nodes. the average path length of the urban rail transit passenger network reflects the number of passing stations from the origin to the destination. this parameter is an indicator of the connectivity of the passenger flow network of urban rail transit. the input-output model of a linear time-invariant system can be represented as follows [cit] :
"the model of the passenger flow network must be built based on the rail infrastructure line. we define the station as a node and the rail connecting two adjacent stations as an edge. the nodes and edges constitute a physical network of urban rail transit. we then superimpose passenger flow on the orbital transport physical network, which can be extended to a passenger flow network of urban rail transit. the station is defined as a node of the network. if there is passenger flow between two stations, there is an edge between the two stations. the transferred passenger flow is the weight of the edge. this is the passenger flow network of urban rail transit. a complex network generally has a high number of nodes, a large degree of distribution, high concentration, and so on. the passenger flow has the characteristic of strong fluidity. therefore, the whole network cannot be controlled effectively solely by determining the flow control node from the aggregation number of passenger flow. this paper analyzes the basic indices of the passenger flow network, thus laying the foundation for study of the controllability of the passenger flow network."
"to maximize k, which is a quantity dependent on sinr uplink and sinr downlink, the optimal solution should make sinr uplink and sinr downlink equal to each other. then, the inequality sinr constraints can be expressed as a function of k as follows:"
"where t i and t i are times represented by tx and rx nodes, respectively. least squares or generalized least squares are usually used for linear regression. more samples help more exact estimation but require a larger amount of memory."
"after the network operation of rail transit, the change and rule of passenger flow are more complicated than those of the single or simple network structures due to the greater number of flow and transfer opportunities. an urban rail transit network is a control system. the external flow control measures are the input signals, and the od passenger flows of the network are the state variables. under normal conditions, urban rail transit networks are within the controllable range. however, in the morning and evening peak hours or under large passenger flow conditions, due to the reduced levels of path service and the mismatch between passenger flow and section capacity, the entire network is in disequilibrium. at the system level, this is an uncontrollable state. to maintain the system in a state of controllability, corresponding flow control measures are taken to reduce flow aggregation."
"fluorescence lifetime imaging microscopy (flim) is a powerful technique for high resolution imaging of the functional spatio-temporal dynamics in situ. förster resonance energy transfer (fret) is, by far, the most extensively studied technique for observation of protein-protein homo-and hetero-dimer interactions in intact cells [cit] . for intermolecular fret, a key benefit of performing donor flim (when compared to intensity based ratiometric techniques), is that fluorescence-lifetime measurements of donor emission are independent of acceptor concentration and is thus suited to imaging studies in intact cells [cit] . multiphoton microscopy confers additional advantages in terms of inherent three dimensional sectioning and enhanced depth penetration for in vivo imaging [cit] . however, the data acquisition rate for flim is a significant limitation in current implementations of laser scanning microscopy."
"where j á j is the absolute value operator. the maximum relative clock drift determines the margin size, that is, the upper bound for the synchronization error. for example, if the margin size is set to a larger value than the maximum relative clock drift, the tx node can synchronize with the rx node without failure. in addition, the mcdr is a very important factor as it mainly affects the energy consumption of the synchronization protocols. if the synchronization energy consumption is defined as the energy wasted to learn the wakeup time of the rx node, the estimation error in the clock drift rate directly affects the energy consumption for synchronization. intuitively, synchronization energy consumption is proportional to the maximum relative clock drift rate (energy consumed by actual data transmission can be ignored because the traffic generation rate is low in wsn). then, the total synchronization energy consumption for a given duration t (in j/s) can be expressed as"
"the key challenge for the full-duplex mac is to coordinate multiple simultaneous transmissions, which are made possible by the new in-band full-duplex capability. fig. 1 shows the key challenge of inter-client interference with simultaneous up/downlink in a network environment, when the uplink and downlink flows involve different clients. the gray circle in fig. 1 denotes the transmission range of the full-duplex access point (ap). assume that client 1 (c1) wants to transmit a packet to the full-duplex ap, and that the full-duplex ap wants to transmit a packet to client 4 (c4), as shown by the black line in the figure. in this case, the signal transmitted from c1 can interfere with other clients, particularly c4, which intends to receive the signal from the ap. if c1 is located close to c4, and the signal transmitted from c1 is very strong, c4 cannot receive the signal transmitted from the ap owing to the interclient interference caused by the signal of c1."
"one of the key shortcomings of the aforementioned studies is that the leveraging of full-duplex ap capability is not supported when all flows are half-duplex, which is possible in practical wireless network environments supporting devices that do not have full-duplex ability. in other words, the mac protocols proposed in previous studies can be applied only to a wireless network where all the clients in the network have full-duplex capability. however, in this paper, we consider a new mac protocol design to achieve the maximum performance gain in practical wireless networks when only the ap has full-duplex capability."
"after the rx is determined by the rssb contention mechanism, the ap calculates the optimal transmit powers for itself and the tx using the information about the received powers from the tx and rx, the inter-client interference from the tx to the rx, and the self-interference in the ap. this transmit power adjustment scheme can reduce the inter-client interference and prevent collisions at the rx. in section ii, we stated the conditions required for successful uplink and downlink transmissions in (7) and (8), i.e., for successful transmissions, the sinr of the uplink and downlink transmissions should be higher than the sinr threshold γ. from section ii, the conditions can be rewritten as (11) and (12) with respect to the transmit powers of the ap and tx."
"in asynchronous mac protocols, a tx node should find an rx node's wakeup time before packet transmission by hearing or sending beacon packets repeatedly over one sleep period. 14 x-mac 4 and ri-mac 5 are famous protocols that belong to this category."
"the remainder of this paper is organized as follows. section ii presents the system model for an in-band full-duplex wireless network. section iii describes a new mac protocol that identifies the receiver with low inter-client interference and controls the transmit powers of the ap and transmitter. to obtain the optimal transmit powers, we formulate an optimization problem to maximize the uplink and downlink capacity based on the channel gain information of the ap and clients. then, we design the mac protocol to coordinate the uplink and downlink transmissions to fully exploit full-duplex capability. section iv presents the performance evaluation of the proposed mac protocol. finally, section v summarizes our findings and concludes the paper."
"we conducted another set of experiments with a random topology in which the tx and rx are located in 30 random positions. for each pair of tx and rx positions, we measured the sinr and evaluated whether full-duplex communication is possible using pocmac. fig. 16(a) shows the changes in the received powers for the uplink and downlink transmissions when the power control scheme of pocmac is applied. under the power control scheme of pocmac, full-duplex communication is possible at 20 positions (depicted as red circles) out of the 30 positions, while it is possible only at 8 positions (blue squares) without power control. this is because the power control scheme of pocmac reduces the interclient interference caused by the tx, as shown in fig. 16(a) . note that the gray level of arrows indicates the amount of change in inter-client interference due to the power control scheme of pocmac. as shown in fig. 16(b), the power control scheme of pocmac can reduce the inter-client interference by around 10 db, on average. the experimental results indicate that the power control scheme expands the geographical region where full-duplex communication can be achieved."
(3) clustering coefficient. the clustering coefficient reflects the node aggregation of the network. it assumes that the number of connection edges of node is . the maximum number of edges of node number is ( − 1)/2. the clustering coefficient of node is defined as follows:
"the determination of the minimum input for a directed network can be converted to a maximum matching problem to solve the network (figure 2) . according to the system discrete dynamics in nature and society 5 structure (figure 2(a) ), the edges between nodes correspond to state matrix in the equation (figure 2(b) )."
"(1) the analysis of the characteristics of the complex network based on the passenger flow indicated that the passenger flow of the beijing urban rail transit network has a \"scale-free\" characteristic."
"clients that want to transmit data frames to the ap should first transmit an rts frame to the ap. to transmit the rts frame, all the clients have to perform a back-off mechanism to avoid collisions before transmitting the rts frame. each client chooses a random back-off number within the range of 0 to (cw min ), where cw min is an initial contention window size, and then attempts to transmit the rts frame after waiting for a random time period. if more than two clients choose the same back-off number, rts frame collisions will occur. in this case, the clients perform the back-off mechanism again, and the contention window size is doubled. this is called a binary exponential back-off mechanism, and it is used in carrier sensing multiple access with collision avoidance (csma/ca) for ieee 802.11."
"the wakeup time estimation algorithm determines the total energy consumption of wsn mac protocols, thus, it should be carefully designed according to the requirements of upper layers such as node lifetime and a sleep period. according to the way to obtain the wakeup time, mac protocols can be classified into three types: asynchronous, synchronous, and hybrid."
"in figure 3(a), the size of the maximum relative clock drift increases with the interval. it is obvious that it has more chances of the environmental change such as temperature and humidity occurs if the interval is larger. however, it is almost fixed for intervals less than 1 min due to the errors caused by clock granularity and scheduling delay."
"note that the self-interference signal could be canceled by subtracting the canceling signal from the received signal [cit] . then, the received signals y rx and y ap can be written as"
"as clock accuracy and characteristics are important for a wide variety of applications, there have been many researches in this area. these studies have found that clock drift can be caused by various environmental factors such as temperature and humidity, as well as software and hardware architectures. 10, 11 they also found that the distribution of the clock drift rate follows gaussian. 12, 13 however, most previous research focused on the clock drift at a fixed measurement interval because the goal was to develop periodic synchronization protocols. thus, these results cannot show the relation between the clock drift and the measurement intervals, which is important in the context of event monitoring applications."
"note that the ap starts the downlink data transmission to the rx earlier than the uplink data transmission from the tx. there are two reasons for this. first, the ap has to notify the tx of the optimal transmit power using an ha frame of the downlink data frame before the tx starts the uplink data transmission to the ap. second, for effective self-interference cancellation, the ap needs to nullify the self-interference caused by the signal that the ap is transmitting. when the ap starts the downlink data transmission, it can accurately estimate the gain of its own self-interference if there are no other signals. with this estimate, it begins the self-interference cancellation, and the self-interference is then canceled out and stabilized at the noise level. this approach, which makes the ap transmit before receiving, can cancel the self-interference more effectively than in the opposite case [cit] . when the length of data frames for the uplink and downlink transmissions is the same, two transmissions cannot be simultaneously terminated owing to the delayed uplink transmission. even though the uplink transmission is delayed for the transmission time of the hc frame, the delay is around 2 μs when the data transmission rate is 54 mbps; because it is shorter than a short inter-frame space (sifs) time, collisions due to the transmission of the ad frame do not occur."
"in figure 10, for the small interval, 1 min, widemac shows the second best performance except for mcdr(0.1) which has a very high estimation failure rate, and for the large average interval, 1 h, it outperforms all the other protocols except rats which shows the worst estimation failure rate. rats shows the worst performance for the small interval due to non-optimal parameters and explicit synchronization. for medium and large intervals, it tends to generate very small margins. therefore, it shows a poor packet delivery ratio although it consumes less energy compared to other protocols as shown in table 4 . table 4 shows that widemac obtains the low delivery ratio for large interval. however, packets were lost only before the margin size converged, while other adaptive protocols suffered from packet drop after converged. when the average interval is 1 h, the margin size of widemac fluctuates as in the early stage of the experiment until the elapsed time is 30 h. this happens because widemac requires a large number of (dt i, e i ) pairs in estimating the margin. if it does not have enough information for estimation, it uses a pre-defined margin size that leads to high energy consumption. however, after having a sufficient number of (dt i, e i ) pairs, it consumes a very small amount of energy. therefore, the initial overhead can be ignored in the overall performance. thus, only widemac can show 0% estimation failure rate after the margin size is converged for all intervals as shown in figures 9 and 10 ."
"in this paper, we consider wireless networks in which an ap is capable of full-duplex communication and clients only use half-duplex transmissions. to overcome the challenge of interclient interference shown in fig. 1, we propose two concepts: distributed interference measurement to modulate access probabilities, and distributed power control to maximize the resulting throughput. first, we propose the use of a signal-strength-based back-off mechanism to provide a higher reception opportunity to the client with a low inter-client interference. using this mechanism, the client with the lowest inter-client interference has the smallest contention window size and is eventually selected to receive a downlink transmission from the ap."
"2) the second difference is that the windowed computation of the autocorrelation as defined in (6) has been found to cause problems. this is related to two facts: osss derived from midi data are much more sparse than osss derived from waveform data, as the onsets are discrete impulses of varying height. furthermore, the tempo of pieces in midi format remains absolutely constant. no noise is induced by the way humans play musical instruments, which can cause the peaks in oss to deviate from the position determined by the meter. because of that, one sample autocorrelation is obtained using the whole onset strength signal as input. the autocorrelation is then transformed into scale space by using (4), resulting in the stm descriptor for a midi signal."
"during the +δ statistical period, ( +δ ) is the number of passengers in station of the line and is equivalent to the number of people in the station plus the difference between people inbound and outbound plus the difference in transfer passenger flow, which can be shown as follows: passenger flow control is mainly the control of inbound passengers. there is no fundamental reduction in passenger demand, and the distribution of passenger flow demand is adjusted. the passenger flow of the network achieves a relatively stable state of time and space distribution."
"on the other hand, in the case of full-duplex without power control, sinr downlink does not change, while sinr uplink gradually increases as α increases. thus, because the high α enables the ap to suppress the self-interference caused by the signal that it is transmitting to the rx, sinr uplink can be increased. however, even if the self-interference is almost completely suppressed by the high α, it cannot increase the sinr downlink without power control. because the sinr threshold is 6 db, the ap cannot perform full-duplex communication for all values of α with this topology."
the example shows that there is no cross road loop or straight line and that there are relatively few required driver nodes. trifurcated and cross structures are very common among subway passenger flow networks. these structures usually have a high number of nonmatching nodes and require a high number of driver nodes. table 2 .
"in this section, we provide the necessary background of scale transform for supporting our suggestions. then, we describe the suggested method of measuring rhythmic similarities in music by distinguishing the cases of music representation by an audio waveform and by the midi format. more specifically, the necessary background will be provided in section ii-a, and thereafter in sections ii-b and ii-c the different demands of the waveform and the midi data will be addressed. section ii-d gives further information about characteristics of the proposed features."
"generally, the amount of energy consumption is proportional to the packet transmission rate and the wakeup interval. since an asynchronous mac protocol does not use a synchronization algorithm, its implementation is very simple, but it consumes a higher energy for transmission than a synchronous mac protocol. hence, it is effective only for applications with infrequent packet generation and a high duty cycle."
"where sinr uplink and sinr downlink are the sinrs of the uplink and downlink transmissions, which correspond to sinr ap in (5) and sinr rx in (6), respectively."
"we consider an in-band full-duplex wireless network that consists of an ap with full-duplex capability and multiple clients without full-duplex capability. note that the ap is easily equipped with elaborate antenna techniques and signal processing modules for self-interference cancellation while mobile clients with full-duplex capability are being incrementally deployed. therefore, we consider a full-duplex wireless network where the ap can simultaneously transmit and receive signals and the clients can either transmit or receive at a given instant in time. in this paper, a transmitter (tx) refers to the client transmitting signals to the ap, and a receiver (rx) refers to the client receiving signals from the ap. fig. 2 shows the system model for a wireless network with a single full-duplex ap, one tx, and one rx. note that the single ap is depicted as two separate components for transmitting and receiving signals. the full-duplex ap and tx transmit the signals x ap and x t x, and the rx and full-duplex ap receive the signals y rx and y ap, respectively. g i,j is the channel gain from client i to client j. because a full-duplex ap transmits and receives a signal simultaneously, the transmitted signal of the ap is fed back to the receiving rf chain of the ap, and it interferes with the signal reception at the ap. thus, there exists a channel from the transmitting rf chain to the receiving rf chain of the ap, and this channel is modeled as the self-interference channel gain g ap,ap . the channel gains are modeled as complex gaussian random variables with zero mean, and they are assumed to be constant over the duration of each transmission. as shown in fig. 2, the canceling signal for self-interference cancellation in the full-duplex ap is modeled as τ · g ap,ap · x ap, where g ap,ap is the channel estimate of g ap,ap, and τ is a cancellation coefficient. here, τ is determined by the degree of self-interference cancellation, which depends on the analog and digital cancellation techniques for full-duplex communication."
"we proposed a full-duplex mac protocol to provide greater reception opportunities to clients with low interference and to reduce the interference between uplink and downlink transmissions at the ap. for a given uplink transmission from a client to the ap, a client that can achieve high sinr in spite of the simultaneous uplink transmission may have a greater chance of being selected as the downlink client under the proposed rssb contention mechanism. to maximize the uplink and downlink sinrs, an optimization problem was formulated, the optimal solution of which determines the transmit power of the ap and the uplink client. we defined control frames and header structures to implement our protocol, pocmac. the performance of pocmac was evaluated under various simulation configurations with regard to the sinr, throughput, and fairness. in addition, sdr-based experiments with warp were performed in a real wireless communication environment. the simulation and experiment results confirmed the excellent performance of pocmac."
"where dt is the inter-packet transmission time. if s and d are successfully obtained, the optimal margin size can be calculated using v(á). for simple pak explanation, some notations are defined assuming a pair of rx and tx nodes (this assumption can be generalized easily). t n : the earliest wakeup time of rx node to receive a packet from tx node in terms of tx node's clock when tx node has the nth packet to be transmitted to rx node. t ã n : estimated t n in terms of tx node's clock. dt n : t n à t n à 1. t n : t n in terms of rx node's clock. t ã n : t ã n in terms of rx node's clock."
"the energy consumption of the tx node in the bounded-drift model is determined by the mcdr and not by the sleep period. as the amount of the total energy consumption is inversely proportional to the sleep period, the network lifetime can be prolonged by lowering the duty cycle."
"to demonstrate the dynamic imaging capability of the system we chose to image two critical steps involved in the signalling of receptor tyrosine kinases, namely, (1) receptor dimerization and (2) receptor recruitment of adaptor protein. in both cases we chose to image signalling components of the epidermal growth factor receptor (egfr) family of receptor tyrosine kinases and all experiments were conducted in live human breast carcinoma cells."
"in full-duplex communication, we need to select a receiver for the downlink transmission and a transmitter for the uplink transmission. as described above, the performance of uplink and downlink transmissions during full-duplex communication is highly dependent on the rss between the ap and the rx and that between the tx and the rx. we propose a received-signalstrength-based (rssb) contention mechanism for selecting the receiver for the downlink transmission. note that the contention mechanism for uplink transmissions is nearly the same as ieee 802.11 dcf with rts/cts handshake, and an advanced mechanism that adaptively adjusts the contention window size can be applied for further performance enhancement."
"epidermal growth factor (egf)-dependent activation of egfr represents the archetypal mitogenic signalling pathway for receptor tyrosine kinases, coupling extracellular growth factors to ras/mapk activation [cit] . in addition to its role in cell proliferation, egfr activation can promote cell survival and differentiation and also drive the profound remodelling of the cell cytoskeleton, inducing migratory and invasive phenotypes in many cell types. the adaptor protein grb2 is likely to represent a key node coupling egfr activation to this diverse range of biological outputs as it provides a molecular bridge between active receptor and an array of downstream signalling protein classes including exchange factors, phosphatases, ubiquitin ligases and cytoskeletal remodelling factors [cit] . grb2 is recruited to activated receptor via its src homology 2 (sh2) domain which targets specific peptide consensuses flanking a critical central phosphotyrosine residue within the receptor's intracellular domain. grb2 serves to establish receptor-associated signalling scaffolds through which a downstream array of enzymatic pathways are rapidly activated: indeed ras activation, which occurs following the grb2-dependent recruitment of the exchange factor sos to ligand-activated egfr, is maximal within two minutes of addition of egf to cultured cells [cit] . visualisation of egf-induced egfr/grb2 association (which cannot be performed in a conventional signal beam scanning flim) represents an interesting challenge for testing the capabilities of the mm-flim system."
"with the in-depth study of controllability of complex networks, many studies have applied control methods to the control judgment and optimization of real networks. meng [cit] studied the controllability of a railway train service network and defined the driver nodes based on immune transmission and cascade failures. an improved theoretical model for the control of complex network and a dual graph of train service network were constructed. ravindran [cit] identified driver nodes with a maximum matching algorithm and classified the nodes. the key regulatory genes in the cancer signal network were identified by controllable analysis. the topology and controllability of the u.s. power grid were analyzed by li [cit], and a new method was proposed to quantify the probability of the intermittent node becoming the driver node."
"the edges between nodes are directed, and the direction is the same as the transport direction of passenger flow. the passenger flow data used in this case are the early peak passenger flow data of the beijing subway network on december 14, 2015. according to the passenger flow od data, the state matrix a is 18 860 * 18 860."
"when the average packet interval is 1 min, mfi(1.2) uses a very small amount of energy, but its estimation failure rate fluctuates and becomes higher compared to other adaptive algorithms. although a smaller size of each sub-range leads to a more exact margin estimation, it requires a lot of (dt i, e i ) pairs. therefore, this leads to a longer convergence time in terms of the margin size. on the contrary, mfi(2) shows a very low estimation failure rate but consumes 30% more energy than mfi(1.2)."
"in addition, fig. 14 shows the throughput fairness with respect to the number of clients when α is 60 db. as the number of clients increases, the average inter-client interference between clients increases because the distance between clients becomes shorter. hence, the fairness index for all the schemes is degraded as the number of clients increases. however, pocmac achieves a better fairness performance in throughput among clients than the other schemes, because the rssb contention mechanism can select the rx with low inter-client interference even in high inter-client interference situations."
"differential non-linearity (dnl) -cyclic intensity variations in time due to phases generated by the four delay-element oscillator (see figure 3) . this was corrected individually for each pixel. the four delay-element oscillator in the pixel has 8 phases (essentially four positive transitions round the ring followed by four negative transitions). this results in a histogram with a repeating pattern of 8. by reducing the bin size to 8 for a measurement taken with a temporally uniform illumination source, one can determine appropriate correction factor to scale/multiply to each bin number."
"in fig. 2, a periodicity spectrum of a cretan dance sample of the class siganos is shown in bold lines, while the periodicity spectrum of its time scaled version is depicted in dotted lines. the scaled version was obtained using the audacity 1 software, by applying the included plug-in for changing tempo of an audio file with a scale factor of . the scaling in the frequency domain representation can be recognized in fig. 2 . the immediate computation of a pointwise distance between the depicted periodicity spectra is affected by the time scaling caused by the different tempi."
"the remainder of this article is organized as follows. in section ''related work,'' related work is briefly described, and the measurement results for the relative clock drift variation are shown in section ''relative clock drift analysis.'' in section ''widemac protocol,'' the proposed protocol is described in detail. evaluation results are presented in section ''performance evaluation'' by comparing the performance of the proposed mac protocol with those of existing state-of-the-art protocols. in section ''conclusion,'' the main results of the article are summarized and the conclusion is presented."
"it supports aperiodic transmission as well as periodic transmission. against most existing protocols that suffer from significant energy consumption in the ultralow duty-cycled environments, widemac shows extremely low energy consumption even in the very low duty-cycled environments. thanks to these features, it can support various applications such as infrequent event detection or monitoring with very long transmission intervals from seconds to hours."
"the megaframe was operated in either time correlated (lifetime) or time-uncorrelated modes (intensity). on-pixel tdcs generated raw time-correlated data, which was postprocessed to generate an image. due to the small size of the spad active area (6 µm diameter, ~28 µm 2 ), the focused beamlet array required careful alignment for which a protocol was developed [could add spie ref]"
"whilst wide-field fluorescence lifetime imaging is possible at up to 30 frames/s with gated image intensifiers [cit], this is not practical in a biological setting due to sample limitations (i.e. excited state fluorophore saturation), significant imaging artefacts and excitation photon flux that may be damaging to cells [cit] . moreover, the mm-flim system described here, provides a platform for future improvements in speed and signal-to-noise by increasing the number of beams or improving the sensor spatial resolution with lower pitch, higher fill factor spads. to demonstrate the capabilities of the system we have performed quantitative imaging of protein-protein interactions of the egfr family signalling network within living cells at unprecedented spatial and temporal resolutions for time-domain flim. we show that the grb2 adaptor protein is recruited to egfr receptor within 75 s of cell stimulation with egf, an observation that would be challenging with conventional tcspc systems. in addition, we are able to visualise the rapid heterodimerisation of her2 and her3 receptor tyrosine kinases in response to cell stimulation with the ligand neuregulin. we observe the gradual accumulation of these her2/her3 receptor complexes within a discrete perinuclear compartment highlighting the spatial resolution of the mm-flim system. we believe this technology has the potential to transform functional flim/fret imaging to enable dynamic spatio-temporal protein interactions to be studied quantitatively for the first time in situ."
"(7-2) otherwise, the ap determines that the downlink data transmission has failed, and then, it transmits the ack-u frame with '0' acknowledgement bit. (8) after transmitting the ack-u frame, the ap returns to the initial state."
"comparing the measures based on the scale transform (i.e., using euclidean distance and using cosine distance) we see that indeed outperforms . this was expected, because of the normalization factor in (1) (i.e., ) is unknown, and this affects the magnitude of vectors being compared, but not the angle between them. compared to, the distance derived using dynamic periodicity warping [cit], the advantage of regards accuracy as well as computational: while in dpw there is the need to compute a warping path using dynamic programming, the most time consuming operation in the scale distance measure is the scale transform which is performed using a matrix multiplication."
"based on the congestion risk assessment standard for beijing urban rail transit stations, the risk level is classified as heightened risk, high risk, general risk, and risk free according to the evaluation score. given the limited and unobstructed equipment and facilities, the risk level has little effect on passenger travel and station operation and is not the basis for classifying the risk rating. therefore, the congestion level of the whole station can be obtained from the number of survey points, such as platforms, passages, security, up and down staircases, and entrances. the specific principles are as follows:"
"the resulting flow control scheme is shown in figure 6 . the three red nodes are the newly added flow control stations: jianguomen, chongwenmen, and xizhimen. the two blue nodes are the original flow control stations that were deleted."
"the node strength is the sum of the node weights. the node strength of the passenger flow network of urban rail transit reflects the passenger demand of the station. the greater the node strength is, the larger the passenger flow of the station is."
"in this section, widemac is proposed to achieve a very low energy consumption and support a wide range of duty cycles. widemac uses simple estimation algorithms and requires a very small size of memory. due to these features, it can be implementable on most existing node platforms. it efficiently supports very long traffic intervals, that is, seconds to hours, and the overall energy consumption decreases with the interval."
"in this experiment, the tx node issues packet transmission requests according to the exponential distribution. since the packet transmission interval is not fixed, it is possible to investigate the performance of protocols in more realistic environments. two exponential distributions were used and their average intervals are 1 min and 1 h, respectively. as shown in figure 9, the results are quite different from the fixed interval cases. when the average interval is 1 min, mcdr(0.1) consumes the smallest energy as shown in figure 9 . however, it cannot deal with abrupt changes in clock drift. this leads to high estimation failure as shown in figure 10 (a) that shows the estimation failure rate of each algorithm. the average failure rate in mcdr(0.1) is 1.7% which is much higher than 0.4%, 0.1%, and 0% for mfi(1.2), mfi(2), and widemac, respectively. the failure rates of all the algorithms except for mcdr(0.1) and rats converge to 0%, but only widemac achieves 0% estimation failure rate for all experiment time. when the average packet interval is 1 h, mcdr(0.1) significantly consumes energy more than other adaptive algorithms. this shows a limitation in using a constant margin regardless of traffic intervals and clock drifts."
"this model type typically describes the average behavior of a system (eg, populations or subpopulations) without taking into account stochastic processes or chance events in single entities (eg, individuals). hence, such models are typically applied to situations with a large number of individuals where stochastic variation becomes less important and heterogeneity can be accounted for using various subpopulations. the parameters of a deterministic model are typically fixed, and a simulation always produces the same result. deterministic models are typically easier to calibrate to data than stochastic models."
"in a discrete-time model, cycle length represents the interval from one time point to the next, for example a specific number of days, weeks, months, or years."
"a type of model where the parameters, variables, and/or the change in variables can be described by probability distributions. this type of model can account for process variability by taking into account the random nature of variable interactions, or can accommodate parameter uncertainty, and so may predict a distribution of possible health outcomes. considering process variability can be particularly important when populations are small or certain events are very rare. stochastic models are often simulated using monte carlo methods."
the array is composed of two identical patches separated by half wavelength distance of 34mm. the overall dimension of array is 7776mm 2 . with and without isolating structure array is shown in fig. 1 . the substrate thickness is taken 2mm and height of both ground plane and patches are kept 0.787mm. the isolating structure is composed of three dumb bells with two m shaped structures. the central dumb bell is surrounded by m shaped structures followed by smaller dumb bells. the distance between all dumb bells is kept 2.5mm and in that distance m shape is introduced. both isolating structures are shown in fig. 2(a) and (b) respectively. the edge to edge distance is kept 20mm and center to center is kept 34.5mm. a 1.9mm whole is also made in ground plane to connect sma connector inner probe to the patches.
"this is a dynamic model where time is treated as a continuous variable (in contrast to a discrete-time model), meaning that the state or value of all other variables (or health outcomes) can be calculated for any time point of interest."
"in last 4-5 decades, chaos theory has been developed by the efforts of many researchers by analyzing the behaviour of dynamical systems that are deterministic in nature but fully unpredictable. these are behaviourally random look like systems. chaos theory has many applications in the field of physics, mathematics, chemistry, engineering etc [cit] . equally chaos is also used in random number generation [cit], hash function [cit] and cryptography [cit] etc. chaotic cryptography is nowadays thrust area of research in data security (see, for instance, [cit] ."
"calibration is the process of adjusting model parameters, such that the model output is in agreement with the data that are used for model development. 22 the aim of calibration is to reduce parameter uncertainty in order to achieve high model credibility."
"the matching impedance is set to be 50ohm. the overall dimensions of dumb bell and m shaped isolating unit are given in tables i and ii. ma, mb, mc, md, me are the dimensions of m shape isolating structure, whereas da, dc are the dimensions of central dumbbell while db, de are the dimensions of side dumbbells respectively. df, dd, dg are the separating distance among isolating elements. patterns are introduced in efforts to change the electromagnetic conduction properties of structures. these structures relative to size of wavelength acts usually as block filers for certain number of frequencies and can be considered as lc network. surface wave's interference reduction was seen with insertion of isolating structure."
these are a class of computational methods that are based on random sampling. monte carlo methods are typically used to simulate stochastic models and are computationally intensive.
"equations that describe the change of a dependent variable, with respect to an independent variable, based on differential calculus. for example, ordinary differential equations can be used to describe the increase and decrease of infected individuals in continuous time resulting from acquisition or clearance of infection. ordinary differential equations are typically used for deterministic and compartmental models."
"the credibility of a model refers to judgments about the degree to which the model provides trustworthy results. several dimensions of credibility have been described, including validity, design, data analysis, reporting, interpretation, and conflicts of interest. 16"
"variables describe model elements such as exposure risks, interventions, or health outcomes that can vary between settings or over time. the value of a dependent variable (eg, number of infected individuals) changes in relation to an independent variable (eg, time)."
"a range of techniques used to test the impact of the assumptions made about the parameters. the analysis can be done by changing one parameter (one-way, univariate), or simultaneously changing several parameters (multi-way, multivariate). the parameters selected for sensitivity analyses are thought to have an impact on the outcome of interest. in a deterministic sensitivity analysis, a parameter is assigned a limited number of values, while in a probabilistic sensitivity analysis, each parameter is assigned a probability distribution, and parameter values are randomly sampled from these distributions."
"from table 4 and figs. 5 and 6, it can be observed that proposed scheme require some more time to perform encryption-decryption operations from table 4 and fig. 5, it can be observed that the encryption time of proposed scheme is less than 1.5 times of the previous schemes and sometimes it is nearer to the previous schemes. from table 4 and fig. 6, it can be observed that time for decryption process is about 1.4 times of the previous schemes, so decryption time is slightly more than previous schemes. proposed scheme has two two-step logistic map, one for encryption and another for updating the initial condition and the table entries. the proposed system has 160 bit key that provide 2160 different key combinations. it resists the proposed cryptosystem against brute−force attack. the experimental results shows that the proposed scheme require some more time for encryption and decryption of data then baptista\"s cryptosystem, wong\"s dynamic look-up table cryptosystem and improved dynamic look-up table cryptosystem. the comparisons based on hamming code distance and edit distance shows that the proposed cryptosystem have more avalanche effect than baptista\"s cryptosystem, wong\"s dynamic look-up table cryptosystem and improved dynamic lookup table cryptosystem. updating the initial condition after each encryption makes it secure from known initial condition and control parameter i.e., known partial key. the control parameter is also fixed in this scheme, so map behaviour is fixed in all states. this resists proposed scheme against behaviour analysis attack and partial key recovery attack."
"in a parsimonious model, descriptive or predictive, the number of assumptions, parameters and variables is minimized. parsimonious models are often relatively simple, but they can also become more complex if they achieve the right balance between complexity and explanatory power."
"alvarez et. al. [cit] examined encryption schemes of wong\"s [cit] look-up table and observed that the attacker can easily get the next position of the symbols without knowledge of the exact current value of x [cit] ."
"the development of methods for incorporating mathematical modeling studies into evidence syntheses and clinical and public health guidelines is still at an early stage. systematic reviewers and guideline developers struggle with questions about whether and how to include the results of mathematical modeling studies into a body of evidence. the review of mathematical modeling studies predicting drug-effectiveness from rct data identified 12 studies using four different modeling approaches. 7 because of the varying use of key terminology between studies, and because certain terms can have different meanings in the literature, it was necessary to describe in the review each modeling approach in detail to illustrate the differences between them. this effort highlights an important reason for the challenges in summarizing the results of mathematical modeling studies."
"discard left 16 bit y 0 and add the key ku 3 and ku 4 at the last of new x 0 . then get float initial condition y 0 from it for second map. g) updating parameter will be updated as (15), (16), (17) and (18)."
"alvarez et. al. [cit] examined encryption schemes of baptista [cit] . they found three types of cryptanalysis attack on baptista\"s cipher: one-time pad attacks (chosen plain text), entropy attack and key recovery attacks [cit] . they proved that it was a weak cryptosystem."
"in section ii, baptista\"s algorithm and concept of dynamic look-up table in context of baptista\"s algorithm has been given. also, two-step logistic map has been described. in section iii, improved algorithm has been proposed in section iv, comparison of improved algorithm with previous schemes has been proposed. finally, the paper has been concluded in section v."
"further, to enhance the security of dynamic look up table to make baptista\"s algorithm more secure, they picked up third, fourth and fifth digit of the current value of x parameter for updating the dynamic lookup table entries [cit] . then i th indexed entry will be swapped with j th entry using (3)."
"once the mathematical modeling studies have been broadly characterized, and their purpose has been determined, it is important to gain a better understanding about some of the terms used to describe the technical aspects of the model used in a study. for example, has heterogeneity among different individuals been incorporated, or what simulation methods were used to obtain the model results?"
can be used to assess effectiveness of interventions where randomised controlled trials are not possible. evidence primarily relies on the quality of the primary data.
"a dynamic model contains at least one time-dependent variable. 11 this type of model is used to describe and predict the course of health outcomes (eg, infection incidence) over time when, for example, the exposure risk (eg, infection prevalence) also changes over time."
a time horizon denotes a chosen time at which point the effect of an intervention will be evaluated. the time horizon should reflect the health outcomes and the relevant intermediate and long-term effects of an intervention. 1
"from figures it is cleared that existence of dgs have not altered radiation patterns. only the patterns are marginally propped since as of asymmetry in the plane from which it is evident that not only proposed structure successfully blocked the surface waves but also mends the patterns. a fresh dgs design is presented in this study to improve isolation enhancement between mimo antennas. a very simple structure is designed comprising of two shapes m and dumb bells using computer simulation technology software. by insertion of proposed patterns reduction of mutual coupling was achieved up to more than 20db and was reduced to sum of -42db also improving 20mhz of bandwidth. the antenna showed good performance parameters results such as gain, directivity, miss match losses with minor alteration in radiation patterns. the isolated embedded structure antenna can be used for satellite services and for radio altimeter applications."
"from table 2, we can see that proposed scheme has also more hamming code distance than the previous one baptista type cryptosystems with small change in key."
"in last 2-3 decades, many different chaotic encryption schemes have been proposed. the most used symmetric cryptosystem based on ergodic property of chaotic maps was given by m. s. baptista [cit] . the purpose of this paper is to propose the use of two-step logistic map to enhance the security of baptista type cryptosystem."
"a term describing processes for assessing how well a model performs and how applicable the results are to a particular situation. 26 there are five main types of validation: face validation (subjective expert judgment about how well the model represents the problem it addresses); internal validation (internal consistency, verification, and addresses whether or not the model behaves as intended and has been implemented correctly); cross validation (convergent validity, model results are confirmed by other models); external validation (model results predict outcomes obtained in a real world setting or in a data set different from the one used for model development); predictive validation (model-predicted events are later corroborated by real-world observations)."
the following list includes some of the most frequently used terms in mathematical modeling studies in various fields of epidemiology. the terms in section 3.1 will help in assessing the technical aspects that relate to model development and structure. the terms in section 3.2 are related to model calibration and validation.
"in mathematical modeling studies, assumptions typically relate to the structure of the model and the supposed interrelationships of model variables. an important assumption in infectious disease models concerns the way in which individuals have contacts with each other. to assess the effectiveness of past interventions or explain previous events and learn from them."
"the proposed antenna work in future can be extended for circular polarized antennas that are suitable for avionics, aerospace and satellite applications. also this work can be extended to enhance isolation among mimo elements by increasing dgs elements."
"by using two-step logistic map, factor b also can be a part of the key by which key space will increase the security of such type of chaotic cryptosystems. so in the proposed scheme, we shall use a two-step logistic map to enhance the capabilities of chaotic cryptosystem. [cit] as (5) and (6) first for encrypting data and second for updating the lookup table."
this type of dynamic model treats time as a discrete variable (in contrast to a continuous-time model) and other variables (or health outcomes) can only change at specific time points.
"a parameter is a quantity used to describe the interrelationships between model variables. for example, parameters can describe how long different individuals reside in different health states, or how likely they are to transmit a disease to another person. there are different methods to specify the value of parameters. mathematical modelers can either choose theoretical values based on specific assumptions, or set the values based on literature reviews or model calibration."
"a markov model assumes that the future state of variables depends only on the current state, but not the previous states, of variables. for example, in a discrete-time markov model, the number of new infected individuals is calculated based on the total number of infected individuals at the previous time step."
"e) rest 32 bits of key will be used as updating parameters as (11), (12), (13) and (14) . (14) f) update the key after encryption of a block of data as discard left 16 bit x 0 . then add keys ku 1 and ku 2 at the last of new x 0 . now get the float initial condition x 0 from it and add with second logistic map output. this will be new initial condition for first map."
"in addition to providing a useful common terminology for public health specialists and mathematical modelers, the description of different model types and other terms defined in the glossary facilitate interpretation of the results of mathematical modeling studies and inform their incorporation into the guideline development process. as a first step, one needs to identify whether a particular research question, eg, the evaluation of public health programs, long-term effectiveness or comparative effectiveness, can be investigated using a model. next, it will be necessary to assess whether existing mathematical modeling studies are appropriate to inform or support a research question or recommendation. we identified four comprehensive frameworks of good modeling practice. 28 these frameworks cover items such as relevance, conceptualization of the problem, or model structure. questions such as whether the model population is relevant, the variables represent the desired health outcomes, the necessary heterogeneity is taken into consideration, the time horizon is appropriate, or the assumptions justified can help in the assessment of mathematical modeling studies. other items concern validity or consistency, ie, the performance of the model according to its specifications. the model should also consider uncertainty with regard to the structure, parameters, and methods. finally, credibility, which takes a number of these items into account, can then be used as the central concept for guideline developers to address the appropriateness of a mathematical modeling study for providing evidence for a specific question, 29 as illustrated in the following example. prevention of tb transmission in hospitals, and particularly of multidrug-resistant tb, is essential in all countries and requires a combination of strategies. predicting the spread of tb in a hospital and the surrounding community, and how alternative methods of control might limit the emergence of resistance, are complex nonlinear processes. it is, however, ethically and logistically impossible to conduct rcts to examine the efficacy of these strategies. mathematical modeling studies that use observational evidence can therefore play an important role in deciding which strategies are likely to be the most effective. the who guideline development group for tb infection control in health care facilities, congregate settings and households assessed systematic reviews of the evidence, which included mathematical modeling studies. 3 one mathematical modeling study that the guideline committee considered, investigated the effects of several different control measures on the spread of extensively drug resistant (xdr) tb in a community in south africa. 10 the model described the transmission of tb in a complex system that included variables representing or contributing to: both the hospital and the surrounding community; different tb health states such as susceptible, latent, infectious, and recovered; drug resistance; hiv infection; and the effects of different control interventions alone and in combination. hence, the study considered the transmission setting that was of relevance to the guideline, and the model structure included the desired health outcomes and variables. the authors used a mechanistic approach to make explicit the way in which stages in the transmission and natural history of tb are related. a deterministic, compartmental model, using ordinary differential equations to describe the transitions between different health states in a dynamic way was appropriate because it allowed the right balance between complexity and tractability. key parameters that described the natural history, such as rate of natural clearance and rate of relapse, were based on the literature, and their influence was assessed in an uncertainty analysis. parameters such as the transmissibility coefficient were calibrated using longitudinal data of individuals in a south african community, where data on tb were collected. the model outputs provided quantitative predictions about the percentage reduction in xdr-tb cases over a reasonable time horizon. external validation of the model was performed using cross-sectional data with information on the prevalence of tb and of drug resistance and the proportion of resistance cases in people with hiv infection. in summary, the mathematical modeling study covered many of the critical items, and we would conclude that the study has a high credibility."
"this describes the approach of solving a mathematical model using either deterministic or stochastic (see monte carlo methods) simulation techniques to iteratively calculate the model variables, which are often time-dependent, for a specific set of parameters. iteratively calculating the model variables means updating the population characteristics at each time point based on the simulated population characteristics at previous time points. computational solutions are used when the model is too complicated for deriving an analytic solution."
"this is a stochastic model representing individuals as discrete entities with unique characteristics. an individualbased model can be useful to accommodate heterogeneity in a given population. individual-based models are also often referred to as agent-based or micro-simulation models. while individual-based models can provide more realistic representations of a system, they can be difficult to parameterize because they require much more detailed knowledge, or assumptions, of how variables interact. the stochastic nature of these models makes them computationally intensive and challenging to calibrate."
"in mathematical modeling studies, this typically describes the differences among individuals, or the variability across parameter values for a specific group of individuals, because of their demographic, biological, or behavioral characteristics."
"the proposed method has been tested with only text information. it has a future work that it can be implemented for other types of information like images, video etc. further, it can be used with some more feedback factors by making the new iterated value based on more than two previous values. such type of mechanism can also be developed for making a chaotic hash function"
"compared with natural ventilation, the authors found that mechanical ventilation alone would reduce xdr-tb cases by 12% (range 10%-25%). the use of respiratory masks by health workers would prevent 2% of all tb cases, but nearly two-thirds of xdr cases in hospital staff. the guideline development group considered this study, together with other observational and modeling studies identified through the systematic review. even though the summarized evidence for the use of ventilation systems and particulate respirators was weak, indirect, and of low quality, the studies suggested that these interventions are favorable for tb infection control."
"before one starts to assess and compare the results of different mathematical modeling studies with each other, it can be helpful to fit them into a larger picture. experts in systematic reviews and guideline developers need to be able to sort out which modeling studies are likely to help them draw a conclusion, formulate a recommendation, interpret the findings of another study, or understand the clinical or pathological background to a problem. mathematical modeling studies can be characterized using several dichotomies that help to describe broad aspects, such as the scope and approach. table 1 provides a list of some important model dichotomies, together with a brief definition, an example, and their relevance to systematic reviews and guideline development. a fundamental distinction can be made between mechanistic and phenomenological models. mechanistic models use mathematical terms to describe the real-world interactions among different model variables. the parameters governing these models typically have a physical, biological or behavioral interpretation. infectious disease models, for example, can describe the movement of individuals within hospital wards, and how infections are transmitted upon physical contact between a susceptible and an infected person. 10 these models have the advantage that specific interventions, such as infection prevention through quarantine or isolation, can be explicitly implemented. phenomenological models, on the other hand, describe the relationships among different model variables, consistent with fundamental theory, but not derived from first principles. hence, this type of model does not attempt to describe or explain why and how certain model variables interact, but instead, focuses on the functional relationship that best describes an observed phenomenon. statistical models, such as regression models, are typically phenomenological and describe the statistical relationship or association between different model variables."
state-transition models assume that individuals can be in different (health) states and move (transition) between them. 21 they are typically described using the framework of either markov models or individual-based models.
"a type of deterministic or stochastic model where individuals that share the same characteristics, on average, are being grouped into a single population or several subpopulations. in contrast, an individual-based model treats every individual as a single entity that can have unique characteristics."
(18) h) update the look up table via shuffling the entries and get the new associated intervals for the symbols based on second map value by iterating it number of times the symbols previous position and j th entry will be swapped with i th entry using (19).
"where x is the current value and n is the number of entries in table. other process will be same as original baptista\"s [cit] chaotic cipher."
"where e is interval, e2 is the current value of initial parameter of second map and n is the total number of possible symbols in plain text. the associated interval will be [xmin + i * e + e2, xmin + (i + 1) * e + e2)."
"increasing herpes zoster incidence after mass childhood vaccination against varicella. can provide a rationale for considering a particular intervention. in the absence of data, results need to be critically evaluated in light of modeling assumptions."
"a predictive model can forecast future events, such as the course of an epidemic in a given population under different scenarios, whereas a descriptive model describes and/or explains previously observed phenomena, such as the effectiveness of past interventions. quantitative models provide a numerical estimation of an intervention effect on model variables, and therefore depend on highquality data to inform the model parameters. qualitative models are usually relatively simple models that only provide insights into the direction of an effect, but not its precise magnitude. nevertheless, they can be used to thoroughly investigate the interrelationships between model variables and the influence of specific parameters on health outcomes (also see analytic solution). qualitative models can also be useful to explore the potential for unintended consequences of interventions beyond the direct intended effects that might have been observed in rcts. finally, an important model dichotomy distinguishes between what drives the results of mathematical modeling studies. most mathematical models incorporate a combination of some underlying theory, model assumptions, and data. the results of a theory-driven model are primarily based on a priori knowledge or assumptions about specific interrelationships, such as the effectiveness of a particular intervention, and are not directly inferred from data. data-driven models infer their results primarily from data, and are not driven by theory or assumptions that are not well supported."
this term refers to mathematical models that synthesize available evidence to estimate health outcomes and guide decision making. the term is typically used in health economic analyses.
"this model type stratifies the population into different compartments, such as different health states. compartments are assumed to represent homogeneous subpopulations within which the entities being modeled-such as individuals or patients-have the same characteristics, for example the same sex, age, risk of infection, or death. the model can account for the transition of entities between compartments (see state-transition model)."
"where n is the total number of possible symbols in the plain text. rest of the process will be same as baptista\"s algorithm [cit] ."
section i covers introduction area. section ii covers single element and array configuration and structure of novel design. section iii covers the discussed results and last conclusion. ease of use
"i) in proposed scheme, the lookup table will be as fig.1 . this is the fully dynamically updated look-up table, which will be the part of security. in this lookup table, there will be n + 1 interval where n is the total number of possible symbols in plain text. so the value of e will be calculated as (20)."
"relates the health outcomes directly to the model parameters using mathematical formulae. models that can be solved analytically are usually simple models, while more complex models typically require a computational (numerical) solution."
"in a static model, all variables are independent of time and constant. a static model typically describes the equilibrium of a system, and relates the model variables for a particular time point only. in contrast to dynamic models, this type of model cannot take into account timedependent changes of exposure risks or health outcomes. decision-tree models are static models."
"the proposed scheme has been implemented and tested using java development kit (jdk 1.7.0) on windows platform. system configuration consists of intel pentium dual core (1.86ghz), cpu with graphic and 2gb ram. this scheme with existing schemes: baptista\"s cipher, dynamic look-up table cipher (dlt) and improved dynamic lookup table cipher (idlt) has been compared using similar conditions and plaintexts. then the results are compared to see the performance of proposed scheme. the first two comparisons shows the avalanche effect that if small number of bits are changed in plain text or the keys then it should show the large number of bits change in cipher text"
"in a collaborative environment, it is easier to share findings and communicate ideas than on a single-user desktop computer, but it may be more difficult to perform individual analyses due to interference [cit] . consequently, analysts may continuously switch back and forth between desktop and collaborative workflows (see figure 1(c-d) ). based on this model, we improved the ipca and ipca-ce applications to support this continuous analysis process. we suggest that providing support for managing and sharing findings is as important as providing useful analysis tools because the users' end goal is to discover evidence that supports their hypotheses."
"5.1.1 participants. participants were ma students of linguistics at the chinese university of hong kong who received a monetary reward for their participation. they were native speakers of mandarin and reported no auditory problems. the c group was comprised of 20 adults (all women, mean age 23.1 ± 1.5 years, age range 22-27), whereas the v+t group was comprised of 19 adults (17 women, mean age 24.0 ± 1.6, age range 22-27). one additional participant in the v+t group had a score of more than 2.5 standard deviations below average in the test and, therefore, was not included."
"(we study a larger number of predicates in section 5.6.) 2. instacart: this dataset is the sales records of an online grocery store [cit] . we use their orders table, which contains 3.4 million sales records. here, the queries ask for the reorder frequency for orders made during different hours of the day. answering these queries involves predicates on two attributes: order_hour_of_day and days_since_prior. 3. gaussian: we also generated a synthetic dataset using a bivariate dimensional normal distribution. we varied this dataset to study our method under workload shifts, different degrees of correlation between the attributes, and more. here, the queries count the number of points that lie within a randomly generated rectangle. for each dataset, we measured the estimation quality using 100 test queries not used for training. environment all our experiments were performed on m5.4xlarge ec2 instances, with 16-core intel xeon 2.5ghz and 64 gb of memory running ubuntu 16.04."
"a finding from visual analysis procedures may be represented as a screenshot, which shows what was found during the analysis, and may include an annotation, which explains in more detail what the screenshot represents. in ipca and ipca-ce, both screenshots and annotations are used to manage users' findings. the applications provide two methods for providing annotations: text and drawing. text-annotation is an indirect approach for explaining the details of a user's finding. drawing-annotation allows users to directly indicate important elements or features visually on-screen. in ipca, both methods of annotation are performed using a keyboard and mouse. however, in ipca-ce, annotations needed to be supported differently because all interactions are initiated by finger touches. therefore, a virtual keyboard is displayed for text-annotation, and a drawing tool is used for drawing-annotation. figure 4 shows examples in which users utilize these annotation tools to indicate their findings in ipca and ipca-ce."
"in this paper, we study the query-driven selectivity estimation (problem 1) as a standalone problem, which is not tied to any specific dbms. however, any query-driven selectivity estimation technique (including ours) can be integrated into a dbms using much of their existing infrastructure. most dbms systems contain the module that computes actual selectivities, the module that computes selectivity estimates, and the api to store metadata in its system catalog. for example, apache spark already collects actual selectivities in the filterexec class (defined in basicphysicaloperators.scala) [cit] . although spark currently reports this selectivity to the user only at query time, it be modified to also store the observed selectivities in its metastore (which is equivalent to its system catalog). the produced selectivity estimates can then be used in the filterestimation class (defined in filterestimation.scala). the previous work proposes a similar integration strategy but for ibm db2 [cit] and microsoft sql server [cit] ."
the cwlc algorithm was used to learn to classify the novice-expert dataset. cwlc was applied on the generated dataset using 10-fold cross-validation on the entire dataset. the initial results with all the features are presented in table 1 . further experiments were conducted with feature selection to ascertain if the classifier results could be improved. feature selection techniques are commonly applied in data mining problems to extract the most discriminative features to reduce the feature size as well as to improve classifier performance. correlation-based feature subset selection (cfssubseteval) technique provided by weka (we used the default values for the parameters) was utilized; this technique evaluates the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them [cit] .
"spoken languages use consonants and vowels to form words. they also use pitch modulations, which can occur at the level of the phrase (intonation) or of the word (lexical tone). whereas variations in intonation are used to convey phrasal prosodic structure or emotion, tonal variation is also used by some languages to signal lexical contrasts. cantonese is an example of these tone languages, where the syllable [ji] means either doctor (醫), chair (椅), idea (意), child (兒), ear (耳), or two (二), depending on the tone with which it is produced. although the processing of lexical tone is becoming a prolific field of inquiry (e.g., [cit], our knowledge of tone languages and how tone modulates speech perception is still limited because of a lack of data from tone languages."
"based on understanding existing literature of collaborative environments, we suggest that users' analyses should not be isolated in one environment (i.e. desktop or collaborative environments). an individual user performs a data analysis and compiles a list of findings in a desktop environment. when enough interesting results are found, the user meets with other analysts in a collaborative environment to discuss and share these findings. after sharing findings with each other, the users then work together interactively to perform a collaborative group analysis. afterwards, the users then take the findings from the collaborative analysis back into a single-user setting for individual analysis and validation. this process then continuously repeats."
"5. autohist: this method creates an equiwidth multidimensional histogram by scanning the data. it also updates its histogram whenever more than 20% of the data changes (this is the default setting with sql server's auto_update_ statistics option [cit] ). 6. autosample: this method relies on a uniform random sample of data to estimate selectivities. similar to autohist, autosample updates its sample whenever more than 10% of the data changes. we implement all methods in java."
"this low-level change to the rendering mechanism affects the implementation of several features in ipca-ce. for instance, a screen capture of a user's hand-drawn annotations can no longer be accomplished by copying a rectangular frame buffer because the portal might not be axis-aligned. the operation now requires two steps, one to render the portal in an axis-aligned manner using an additional back buffer, and the second operation to capture the image."
"the rendering mechanism for ipca-ce differs significantly from the desktop version since ipca-ce needs to support multiple interfaces for multiple users. while ipca utilizes a single opengl context, ipca-ce needs to create multiple \"virtual\" contexts, one for each of the interfaces. the general architecture of the ipca-ce interface is based on the pad++ metaphor [cit], in which each interface is called a \"portal\"."
"the mixture model and a uniform distribution, rather than maximizing the entropy of the model (as pursued by previous work [cit] ). as described above, directly computing the quality of a mixture model involves costly integrations over 2 m distinct ranges. however, when minimizing the l2 distance, the 2 m integrals can be reduced to only m 2 multiplications, hence greatly reducing the complexity of the model quality evaluation. although minimizing the l2 distance is much more efficient than maximizing the entropy, these two objectives are closely related (see appendix a for a discussion)."
"in traditional mouse-based interaction, the user can move a mouse over a visual element to highlight the element (such as to display its label) without the use of the mouse buttons. in a multi-touch environment that utilizes infrared refraction and reflection to detect a user's touch, there is no way to discern the difference between mouse-over and mouse-drag (holding down a mouse button and moving the mouse) because both operations require the user's finger to be touching the surface of the table ."
"where д z (x) is the z-th simpler pdf and h(z) is its corresponding weight. the subset of the data that follows each of the simpler pdfs is called a subpopulation. since the subpopulations are allowed to overlap with one another, a mixture model is strictly more expressive than histograms. in fact, it is shown that mixture models can achieve a higher accuracy than histograms [cit], which is also confirmed by our empirical study (section 5.5). to the best of our knowledge, we are the first to propose a mixture model for selectivity estimation. 2 challenges using a mixture model for selectivity learning requires finding optimal parameter values for h(z) and д z (x); however, this optimization (a.k.a. training) is challenging for two reasons. first, the training process aims to find parameters that maximize the model quality, defined as ∫ q(f (x)) dx for some metric of quality q (e.g., entropy). however, computing this integral is non-trivial for a mixture model since its subpopulations may overlap in arbitrary ways. that is, the combinations of m subpopulations can create 2 m distinct ranges, each with a potentially different value of f (x). as a result, naïvely computing the quality of a mixture model quickly becomes intractable as the number of observed queries grows."
"collaboration in real-world analysis can be regarded as a process of working together or sharing decision-making to develop a joint strategy or answer for the given complex tasks. since collaboration is beneficial for solving complicated tasks, domain experts often work together to solve analytical problems in a collaborative setting [cit] . however, it has been known that real-world analysts typically perform both individual and group tasks, and as a result must frequently transition between single-user and multi-user collaborative workflows during the course of their analysis [cit] . despite this fact, most visual analytics solutions have been designed either as standalone single-user applications or as purely collaborative systems, and very few analytical tools have been developed that cohesively support both activities."
"with the system, a single user performs an analysis with ipca ( figure 3a ) and multiple users collaborate with the extended collaborative application (ipca-ce) ( figure 3b ). since the collaborative application is an extended version of the desktop application, both (5) are shown in both applications. buttons and menus (6) in ipca were converted to several touchable buttons (6 and 7). in ipca-ce, the sliderbars can be expanded and collapsed by pressing the toggle button (8), and tab buttons (9) were added to access users' findings and annotations. both applications support manipulating and changing the scale of the projected data items."
"ipca ( figure 2a ) is designed (1) to help the user understand the complicated blackbox operation of principal component analysis [cit] and (2) to allow the user to analyse complex data sets interactively [cit] . specifically, it focuses on assisting the user in better understanding and utilizing pca for analysis by visualizing the results of principal component analysis using multiple coordinated views and a rich set of user interactions. the application is designed primary for use on a standard desktop computer. since our primary goal of this study is to design a collaborative visual analytics system to support a continuous analysis process, the system has been upgraded to support annotation techniques (see section 5 for detail)."
"this section describes how to compute the weights w z of quicksel's subpopulations. for training its model, quicksel finds the model that maximizes uniformity while being consistent with the observed queries. in section 4.1, we formulate an optimization problem based on this criteria. next, in section 4.2, we present how to solve the optimization problem efficiently."
"when reporting training time, we include the time required for refining a model using an additional observed query, which itself includes the time to store the query and run the necessary optimization routines."
"since the exact processes for sharing ideas and analysis findings is still unclear, there is much work to be done in understanding the knowledge sharing process in collaborative environments. in the future, it will be necessary to perform an expert evaluation of sharing analysis results between the single-user and collaborative environments. these results will provide guidelines for designing visual analytics systems that accurately reflect the analytical processes carried out by real-world experts."
"as explained in section 1, we use the term scan-based methods to refer to techniques that directly inspect the data (or part of it) for collecting their statistics. these approaches differs from query-based methods which rely only on the actual selectivities of the observed queries."
"a novice-expert overlay model is a process which consists of three components: a) the user interactions of novices captured during problem solving; b) an expert solution path; and c) the feedback delivery system. in order to adapt the nature of the feedback to the specific needs of different novice physicians, bioworld compares the evidence items identified as pertinent by novices and experts with the aim of highlighting similarities and differences in the context of the feedback palette."
"about 10 participants joined to the study (eight male and two female). four participants were undergraduate students, five were graduate students, and one was a faculty. each participant was asked to solve the five task questions, which were:"
"since interaction is somewhat related to users' reasoning [cit], we can assume that our participants' performance in using ipca is attributed solely to the interface design and the set of interactions. we believe that interaction plays a significant role in solving analytical task questions by understanding pca deeply. unfortunately, we are not able to isolate the specific factor(s) that plays a major role in determining the participants' performance due to the multiple coordinated views, the interactions, or others. we simply believe that the interactions play a significant role in that the user' direct and continuous manipulation with pca is rewarded with immediate visual feedback."
"in light of both the advantages and limitations of collaborative environments, we suggest that users' analyses should not be isolated in one environment. as shown in section 3, the developed informal model for the general analytical process should be supported and maintained to allow analysts to switch back and forth between single-user and collaborative workflows. an individual user performs a data analysis and compiles a list of findings in a desktop environment. when enough interesting results are found, the user meets with other analysts in a collaborative environment to discuss and share these findings. after sharing findings with each other, the users then work together interactively to perform a collaborative group analysis. afterwards, the users then take the findings from the collaborative analysis back into a single-user setting for individual analysis and validation. this process then continuously repeats."
"with the ipca and ipca-ce applications, analysts can perform their analytical tasks by switching back and forth between sign-user desktop and collaborative visual analytics environments. as a result, the system parameters and analytical findings for these tools are tightly integrated so that analysts may seamlessly transition without losing track of the analysts' analysis process. to support the sharing of analytical results (i.e. findings), which many experts consider it to be the purpose of collaboration [cit], we defined an xmlbased format for managing the findings from analyses, which can \"follow\" the user as they transition between single-user and collaborative contexts. in a single-user desktop environment, we performed a user study to see the effectiveness of ipca for solving analytical tasks, and creating and sharing findings in a single-user desktop environment. to understand how participants cooperate and share their findings for solving analytical tasks in a collaborative environment, we conducted an additional user study with a varying number of available workspaces in a collaborative environment. this paper is organized as follows. first, we discuss the previous research in collaborative visualization that is relevant to our approach. in section 3, we represent our viewpoint to support analysts' continuous analytical processes in different environments. in section 4, we provide an overview of our visual analytics tools and outline the technical challenges involved with extending an existing application for deployment in a collaborative environment. next, in section 5, we then describe the improvements made to our applications to support the sharing of analytical processes. in sections 6 and 7, we report the user study that was performed to understand users' analytical processes in a desktop and collaborative environment, respectively. finally, in section 8, we discuss future work and conclude the paper."
"supported queries similar to previous work [cit], our technique supports selectivity estimation for predicates with conjunctions, negations, and disjunctions of range and equality constraints on numeric and categorical columns. in other words, we currently do not support wildcard constraints (e.g., like '*word*'), exists constraints, and any constraints. in practice, often a fixed selectivity is used for unsupported predicates, e.g., 3 .125% in oracle [cit] ."
"bioworld is a computer-based learning environment designed as a cognitive tool to train novice physicians in diagnosing virtual patients and receiving timely feedback [cit] . ongoing feedback is provided to learners pertaining to their prioritized list of evidence items that supports their diagnostic process as it compares to evidence taken by an expert to solve the case. in an attempt to tailor the content of the feedback to the specific needs of different novices, bioworld relies on a novice-expert overlay approach to assess learning during problem solving [cit] ."
"segmentation was tested by means of a two-alternative forced choice task presented at the end of the listening phase. this test comprised 24 items contrasting bisyllables with either a consonant-tp of 0.75 (for the c group) or a vowel-tp of 0.75 (for the v group), against bisyllables with the relevant tp of 0.25 or lower. to make sure that we assessed tp extraction and not just memory for specific chunks of syllables, we used only bisyllables that never appeared in the speech streams for the test."
query-driven techniques create their histogram buckets adaptively according to the queries they observe in the workload. these techniques can be further categorized into two based on how they compute their bucket frequencies: errorfeedback histograms and max-entropy histograms.
"finally, we tested the generalizability of these results to tone languages other than cantonese. our fourth experiment collected data from speakers of mandarin, a tone language sharing many properties with cantonese although the two languages are mutually unintelligible."
"the procedure was similar to that of experiment 1, except that instructions to participants were presented on the computer screen both in mandarin and in english."
a mixture model is a probabilistic model that expresses a (complex) probability density function (of the population) as a combination of (simpler) probability density functions (of subpopulations). the population distribution is the one that generates the tuple x of t . the subpopulations are internally managed by quicksel to best approximate f (x).
"collaborative visualizations have a long history. [cit] described four general reasons why collaborative visualization is compelling as (1) experts' knowledge can be available any time and at any place, (2) this expertise can be transferred to others, improving the local level of knowledge, (3) based on the supported accessibility, visualization products can be reviewed and modified as they are produced, reducing turn-around time, and (4) remote accessibility can reduce the need to relocate the expertise physically."
"overall, four participants mentioned that although a collaborative environment is useful to share ideas and findings, it is difficult to perform visual analysis simultaneously. for example, in the double and multiple workspace conditions (see figure 8b and 8c), one participant pointed out that he felt like he was racing his partner to discover more findings. participants suggested that they would also like to have an isolated workspace (such as a desktop environment) for solving analytical problems. additionally, though all recruited participants were healthy and of average fitness, most participants mentioned that standing in front of the multi-touch table for long periods of time was difficult due to fatigue."
"the greedystepwise forward search technique was employed through the space of attribute subsets to search for the most discriminative feature set and rank them based on their individual predictive ability. the process selected 31 top features. a new dataset was generated using these 31 features, and cwlc was applied to this dataset. having applied the aforementioned feature selection technique to our dataset, there was a marked improvement in the classifier's results (from 89.19% to 97.30%) when the selected features were used in novice-expert classification. the improved classification results are presented in table 2 . in this study, the case summaries written by experts and novices in the bioworld system were analyzed. the use of machine learning in augmenting the current novice-expert overlay model in the system was considered. the current study used the confidence-weighted linear classier for the noviceexpert classification task. the results from this study support the idea of employing text classification in augmenting the novice-expert overlay model in bioworld. the implications of the current study are promising in that the study demonstrated via the experimental results the efficacy of confidenceweighted linear classifier in the novice-expert classification task. the cwlc shows significant promise; with the classifier reaching an accuracy level of 97.30% after the feature selection techniques were incorporated. going forward with the effort towards augmenting the novice-expert overlay model, several opportunities for improving the bioworld system are available. future work will explore other classifiers useful in text mining tasks. a research problem that deserves attention is the automatic scoring of text, i.e., the case summary in our problem space. future studies will explore this line of research."
"contrastive lexical tone is a constitutive element of many languages, but the impact of its presence on linguistic computations is not yet well understood. we have taken a first step in this direction by investigating statistical segmentation processes in cantonese. our data show that cantonese speakers failed to segment streams with tps carried by consonants, and that tones boosted cantonese speakers' performance in segmentation based on vowels. [cit], adding to the evidence for the differences in linguistic processing between tone and non-tone languages (e.g., [cit] ) . we have also shown that the ability of vowels and tones together to sustain tp computations appears to be specific to tone languages, as it is also present in mandarin speakers but not (or, at least, present to a lesser degree) in speakers of a non-tone language tested with the same materials. altogether, our study contributes data to the understanding of tone languages and of how speakers of these languages process speech streams. these data highlight differences between non-tone and tone languages, including aspects that were previously thought to be universal."
"in this section, we compare the end-to-end selectivity estimation quality of quicksel versus query-driven histograms. we have summarized the main results in table 3 . the table reports that, for both dmv and instacart dataset, quicksel was significantly faster for the same accuracy, and significantly more accurate for the same time limit. specifically, we gradually increased the number of observed queries provided to each method from 10 to 1,000. for each number of observed queries, we measured the estimation error and training time of each method using 100 test queries. these results are reported in figure 3 . given the same number of observed queries, quicksel's training was significantly faster (figures 3a and 3d ) while still achieving comparable estimation errors (figures 3b and 3e) . we also studied the relationship between errors and training times in figures 3c and 3f, confirming quicksel's superior efficiency (stholes, isomer+qp, and querymodel are omitted in these figures due to their poor performance). in summary, quicksel was able to quickly learn from a large number of observed queries (i.e., shorter training time) and produce highly accurate models."
"3.1.1 participants. the participants in experiment 2 were also undergraduate students of linguistics at the chinese university of hong kong and received course credit for their participation. as in experiment 1, they were cantonese-english bilinguals who reported cantonese to be their dominant language, as well as having no auditory problems. we tested a group composed of 19 adults (14 women, mean age 19.4 ± 1.6, age range 18-24). one additional participant was not included in the analysis because his/her score in the test was more than 2.5 standard deviations below average."
"limitation 2: non-trivial bucket merge/pruning given that query-driven histograms [cit] quickly become infeasible due to their large number of buckets, one might consider merging or pruning the buckets in an effort to reduce their training times. however, merging or pruning the histogram buckets violates the assumption used by their optimization algorithms, i.e., iterative scaling. specifically, iterative scaling relies on the fact that a bucket is either completely included in a query's predicate range or completely outside of it. 7 that is, no partial overlap is allowed. this property must hold for each of the n predicates. however, merging some of the buckets will inevitably cause partial overlaps (between predicate and histogram buckets). for interested readers, we have included a more detailed explanation of why iterative scaling requires this assumption in appendix b."
"our data for the experiments come from the bioworld system. the case summaries were extracted from the log files generated by bioworld. the data for the novice-expert classification problem included a total of 74 case summaries, with 60 cases written by novices and 14 cases written by experts."
"data from experiments 1 and 2 raise the issue of whether any acoustic cue correlated with vowels (not necessarily tones) would enhance segmentation. in this view, tones would have no special status other than improving vowels' acoustic saliency. while our data does not deal directly with this issue, they may provide a partial answer. first, a purely acoustic account should generalize similarly across languages. however, while russian speakers extracted tps from tone-bearing vowels significantly above chance level, their accuracy was much lower than that of speakers of cantonese and mandarin (55.4% vs. 68.4% and 70.0%, respectively). second, it needs to be pointed out that tones have a complex, multidimensional acoustic realization that differs in quality (e.g., raising, falling, flat, dipping), in contrast to other dimensions such as amplitude or vowel length that differ only in quantity (e.g., higher/lower, longer/shorter). this asymmetry resembles that between consonants and vowels [cit], and deserves further research."
"our findings constitute strong evidence that the current formulation of the cv hypothesis [cit] needs to be modified in order to include tone languages. [cit] on the basis of the following cross-linguistic observations: (a) consonants tend to outnumber vowels; (b) in many languages, consonants tend to disharmonize within words whereas vowels tend to harmonize; (c) consonants are perceived categorically while vowels are not; and (d) sequences of consonants (but not vowels) may constitute morphological roots in some languages. none of the observations in their review included tone languages and, hence, the possibility of a functional specialization for lexical tones and their effect on the linguistic division of labor between consonants and vowels were not discussed."
"synchronous sharing in the collaborative environment provides users with the capability of viewing others' findings to gain the understanding of collaborators' analytical processes (see figure 1b) . this type of sharing allows users to work together simultaneously to find new analytical results. this is analogous to traditional procedures for sharing analysis results, such as preparing presentation slides or written reports to present and discuss in a group meeting. this is accomplished in ipca-ce by dragging findings directly from one workspace to another (see figure 6c ). using this direct passing operation, collaborators can easily become aware of each other's analytical processes and results, although they each still maintain an individual workspace for performing their analyses. since this sharing operation should be subject to the agreement of the collaborator, a confirmation window is displayed to ask for permission to accept the finding being shared by another."
"second, the outer optimization algorithms are often iterative (e.g., iterative scaling, gradient descent), which means they have to repeatedly evaluate the model quality as they search for optimal parameter values. thus, even when the model quality can be evaluated relatively efficiently, the overall training/optimization process can be quite costly."
the proof of this theorem is in appendix c. the implication of the above theorem is significant: we could reduce the problem of o(2 n ) complexity to the problem of only o(n 2 ) complexity.
"in the development of the collaborative system, it is important to use two concurrent processes, one of which utilizes individual threads. the multi-touch engine runs in its own process, which detects finger touches on the table and sends input event messages to the client application via tcp/ip. in the ipca-ce process, the input thread receives messages from the multi-touch engine and adds them to a queue. the content of each of these messages is relatively simple, but describes the position of a user's touch, as well as the state of the touch (finger-down, finger-drag, etc.). a separate thread then processes the queue to determine its relevance to updating the ipca-ce interface. for instance, multiple (false) touches will be condensed into one single touch by this thread to reduce unnecessary computation. finally, the rendering thread receives update requests based on the processed queue. it then renders the visual interface and, if necessary, performs principal component analysis on the underlying data."
"1. stholes [cit] : this method creates histogram buckets by partitioning existing buckets (as in figure 1 ). the frequency of an existing bucket is distributed uniformly among the newly created buckets. 2. isomer [cit] : this method applies stholes for histogram bucket creations, but it computes the optimal frequencies of the buckets by finding the maximum entropy distribution. among existing query-driven methods, isomer produced the highest accuracy in our experiments."
"workload shifts we also studied quicksel's accuracy under situations where the query workload shifts over time. here, we generated the gaussian dataset with correlation 0.5. we generated queries with different predicates, with each predicate being a different rectangle in the 2-dimensional space. 10 we simulated three scenarios of workload shifts by modifying these rectangular predicates. first, we created a random-shift workload by choosing random rectangles in the space. we also created a sliding-shift workload by gradually moving the rectangles from the left-tail of the normal distribution towards the right-tail. third, we created a no-shift workload by using the same rectangle for all queries. in each scenario, we first trained quicksel on the first 10 observed queries (i.e., sequence numbers: 1-10) and measured the accuracy on the next 10 observed queries (i.e., sequence numbers: [cit] . then, we trained quicksel on the first 20 observed queries (i.e., sequence numbers: 1-20) and measured the accuracy using the next 10 observed queries (i.e., their sequence numbers: [cit] . we kept increasing the number of observed queries until we reached 1,000 observed queries for training."
"kernel density estimation (kde) kde is a technique that translates randomly sampled data points into a distribution [cit] . in the context of selectivity estimation, kde has been used as an alternative to histograms [cit] . the similarity between kde and mixture models (which we employ for quicksel) is that they both express a probability density function as a summation of some basis functions. however, kde and mm (mixture models) are fundamentally different. kde relies on independent and identically distributed samples, and hence lends itself to scan-based selectivity estimation. in contrast, mm does not require any sampling and can thus be used in query-driven selectivity estimation (where sampling is not practical)."
"indicator function that returns 1 if its argument is true and 0 otherwise s i the selectivity of p i for t (p i, s i ) i-th observed query n the total number of observed queries f (x) pdf of the tuple x (of t )"
"conversion one: quadratic programming we can solve problem 2 efficiently by exploiting the property of the distance metric of our choice (i.e., mse) and the fact that we use uniform distributions for subpopulations (i.e., umm). the following theorem presents the efficient approach."
"asynchronous self-sharing occurs in most single user desktop applications (see figure 1a) . in ipca, the user can continuously create findings and track the history of their analysis by viewing the previously created findings (see figure 6b ). previously saved findings can be displayed, allowing the user to track the history of the analysis and continuously update previous findings based on recent results."
"as in the previous work, we focus our presentation on predicates on a single relation. however, any selectivity estimation technique for a single relation can be applied to estimating selectivity of a join query whenever the predicates on the individual relations are independent of the join conditions [cit] ."
"we found that the single desktop application (i.e. ipca) is good for understanding data, identifying anomalies, and sharing their findings. by analysing log files, we found that most participants tend to create findings and track the history of their analysis by checking the previously created findings. they created about three ± two findings on average when completing each given task. participants commented that the supported annotation techniques in ipca are useful and efficient to manage and share their findings. however, we found that they prefer not to share findings since ipca does not support a direct sharing mechanism over the network. from this study, we found that single desktop environments have advantages for providing private working environments that help users concentrate on solving analytical problem themselves without getting interrupted by other people. however, this approach includes a limitation as:"
the solution w * to problem 3 can be obtained in a straightforward way by setting its gradients of the objective (with respect to w) equal to 0:
"challenge solving problem 2 in a naïve way is computationally intractable. for example, consider a mixture model consisting of (only) two subpopulations represented by g 1 and g 2, respectively. then, ∫"
"in this section, we compare the effectiveness of quicksel's model to that of models used in the previous work. specifically, the effectiveness is assessed by (1) how the model size-its number of parameters-grows as the number of observed queries grows and (2) how quickly its error decreases as its number of parameters grows. here, we use quicksel's default setting whereby its number of parameters increases linearly with the number of observed queries, deferring the analysis of its non-default setting to section 5.6."
"we use two real datasets and one synthetic dataset in our experiments, as follows: 1. dmv: this dataset contains the vehicle registration records of new york state [cit] . it contains 11,944,194 rows. here, the queries ask for the number of valid registrations for vehicles produced within a certain date range. answering these queries involves predicates on three attributes: model_year, registration_date, and expiration_date."
", where i (·) is the indicator function. a pair (p i, s i ) is referred to as an observed query. 5 without loss of generality, we assume that n queries have been observed for t and seek to estimate s n+1 . finally, we use f (x) to denote the joint probability density function of tuple (that has generated tuples of t )."
"our second experiment explored more directly the role of lexical tones in segmentation by introducing tonal variation in our speech streams. more specifically, we investigated whether cantonese speakers' performance is enhanced if tps are carried by vowels and tones together, rather than just vowels alone. to do this, we presented a new group of cantonese speakers with speech streams where vowels were paired with tones in a one-to-one manner (see methods)."
"participants were tasked with solving three virtual patient cases in bioworld on an individual basis for a total duration of 2 hours. while solving the patient cases, participants also engaged in thinking aloud. the three virtual patient cases were amy, cynthia, and susan-taylor. the correct diagnosis for the cases amy, cynthia, and susan-taylor was diabetes mellitus (type1), pheochromocytoma, and hyperthyroidism respectively."
"there is an extensive body of work on selectivity estimation due to its importance for query optimization. in this section, we review both the scan-based methods (section 7.1) as well as the query-driven ones (section 7.2). quicksel belongs to the latter category. we also discuss the connection to selftuning/self-driving databases. finally, we briefly overview the selectivity estimation in commercial dbms (section 7.3). we have summarized the related work in table 4 . scan-based histograms"
"with regards to overall preferences for solving problems, four participants preferred double workspaces, and eight participants preferred multiple workspaces. no participants indicated a preference for the single workspace condition. these preferences are consistent with our results indicating that they were able to discover fewer findings when using a single workspace. however, when asked which condition they preferred for communicating and sharing ideas, half of the participants preferred a single workspace, with the remaining six split evenly between the multiple and double workspace conditions. this indicates that a single shared workspace may be effective for communicating findings since users are looking at the same visual representation of the data. when reviewing participants' qualitative feedback, we found that most participants pointed out that the multi-touch table interactions were helpful in understanding the data. however, the overall feedback we received for this collaborative visual analytics application was very positive."
"our approach first, to ensure the efficiency of the model quality evaluation, we propose a new optimization objective. specifically, we find the parameter values that minimize the l2 distance (or equivalently, mean squared error) between 2 our mixture model is also different from kernel density estimation (kde) [cit], which is constructed by scanning the actual data, rather than analyzing observed queries."
"there are many metrics that can measure the distance between two probability density functions f (x) and д 0 (x), such as the earth mover's distance [cit], kullback-leibler divergence [cit], the mean squared error (mse), the hellinger distance, and more. among them, quicksel uses mse (which is equivalent to l2 distance between two distributions) since it enables the reduction of our originally formulated optimization problem (presented shortly; problem 2) to a quadratic programming problem, which can be solved efficiently by many off-the-shelf optimization libraries [cit] . also, minimizing mse between f (x) and д 0 (x) is closely related to maximizing the entropy of f (x) [cit] . see appendix a for the explanation of this relationship."
"speech streams were built in a similar manner to those of the v group in experiment 1, but this time each syllable was recorded with a specific tone. we paired all e vowels with t3 (mid-level tone), all i vowels with t1 (high-level tone), all u vowels with t4 (falling tone), and all y vowels with t2 (high-rising tone). the recording was done by the same phonetician who recorded the material for experiment 1. as before, we made sure that none of the resulting syllables were attested in cantonese. syllables were then digitally modified to have a length of 450 ms and the pitch contours presented in figure 2, and later concatenated into speech streams as in the previous experiment."
"throughout this article, we have investigated the role of consonants, vowels, and lexical tones in the computation of statistical regularities present in artificial speech streams, with an emphasis on the performance of speakers of tone languages. our data showed that cantonese speakers failed to segment speech streams where the relevant tps were carried by consonants alone. in contrast, they performed above chance level when tps were carried by vowels, with a substantial improvement when vowels were paired with tones. the consonantal results stand in sharp contrast to previous evidence from non-tone languages, where test participants showed successful segmentation when consonants were statistically relevant [cit] . this suggests that tonal information provided strong cues for cantonese speakers to extract the statistical cues present in the streams, beyond those signaled by vowels alone."
"the illustration of users' processes of solving complex problems and sharing analysis results (findings) has been presented as a poster [cit] at the visualization conference [cit] . visualization experts' opinions were positive to our idea of preserving users' continuously analysis process in different environments instead of isolating them in one environment. some of them commented that providing two visual analytics tools is good for performing collaborative and single-user analytical processes, especially when solving difficult analytical problems."
"from this study, we found that collaborative environments have advantages for solving analytical problems, especially when sharing ideas and findings. however, this approach has several limitations, such as:"
"ipca and ipca-ce applications support a continuous analysis process that permits analysts to switch back and forth between desktop and collaborative environments. users can export findings and system parameters back and forth between applications, allowing them to transition from single-user to collaborative contexts without losing track of their current analytical process. as shown in figure 1, we defined that sharing findings between the two different environments is performed as asynchronous transitional sharing processes. however, synchronous sharing between the two environments can be supported depending on how applications are designed. in such case, it is important to support isolating users if they want to work themselves. the two applications are available publicly online at the url http://www.knowledgeviz.com/ipca/."
"a more comprehensive approach would include several types of user interactions during the analysis performed by the learner model. in past studies, our examinations have included other types of user interactions that have been shown to be critical to diagnostic performance, including help-seeking, information-seeking, diagnostic laboratory testing, and case summary writing [cit] . the evidence from the investigations of the case summaries suggests that there are significant differences between case summaries written by novices as opposed to experts. the linguistic features extracted from the case summaries have been used successfully to discriminate between the type and correctness of case summaries. however, there is a pressing need to build text classification algorithms that perform efficiently within the server database. this paper addresses this issue by evaluating the performance of confidence-weighted linear classifiers, an online learning method for natural language processing (nlp) problems. in particular, this research seeks to address the following questions: a) what is the accuracy of confidenceweighted linear classifiers in the recognition of case summaries written by novices and experts?; and b) how does the addition of a stepwise feature selection algorithm affect the accuracy of the text classification model? ii. the context: bioworld bioworld ( fig. 1) is an intelligent tutoring system for the medical domain that is designed to support novice physicians in practicing diagnostic reasoning skills while receiving feedback [cit] . the system was created using a cognitive apprenticeship framework [cit] where learners practice realistic clinical reasoning tasks and are scaffolded in the context of their learning with expert models. in bioworld, novice physicians learn clinical reasoning by diagnosing virtual patient cases by identifying relevant evidence/symptoms, ordering labtests, seeking help via the embedded library, and reasoning about the nature of the underlying disease [cit] ."
"in this section, we first define relevant notations in section 2.1 and then formally define the problem of query-driven selectivity estimation in section 2.2. next, in section 2.3, we discuss the drawbacks of previous approaches. table 2 summarizes the notations used throughout this paper."
"bioworld employs a novice-expert overlay system [cit] to assess clinical reasoning during learning. the system compares similarities and differences in learner solution path against an expert solution path. for instance, after novices submit their final hypothesis, learners are able to compare their solution with an expert's solution. along with a comparison of the novice-expert on diagnosis and evidence, the system also provides a detailed explanation of an expert's reasoning. however, the current version of this user model omits an important aspect of the novices' clinical reasoning, i.e., the final written case summary. the written case summaries contain highlights apropos the symptoms, vital signs, and labtests that were germane to diagnosing the patient case. in order to address this gap and to augment the current novice-expert overlay model, this paper represents the first steps in exploring the efficacy of text categorization algorithms in differentiating between novice and expert case summaries written in bioworld to augment the current novice-expert overlay model in bioworld."
"although many useful visual analysis applications have been developed to assist users in understanding complicated relationships in large data sets, they are mostly limited desktop applications designed for single users. collaborative visual analytics environments have also been developed, which allow users to work together to solve complex analytical problems. however, on their own, neither of these two modalities entirely reflects the continuous analytic processes carried out by real world experts. in this paper, we presented an integrated visual analytics toolset composed of a single-user desktop application and a collaborative touch-table system. based on the consideration of sharing findings, we presented an informal model for the general analytical process that occurs as analysts switch back and forth between single-user and collaborative environments. by following this model, both the desktop and collaborative applications support this continuous analysis process."
"1. we propose the first mixture model-based approach to selectivity estimation (section 3). 2. we propose a new optimization objective, namely minimum difference from the uniform distribution, for efficient training of a mixture model (section 4.1). 3. we show that the min-difference problem can be reduced to a quadratic program and present an efficient strategy for solving it (section 4.2). 4. we conduct extensive experiments on two real-world datasets to compare quicksel's performance and stateof-the-art selectivity estimation techniques (section 5)."
"recall that both g z and b i are represented by hyperrectangles. thus, their intersection is also a hyperrectangle, and computing its size is straightforward."
"4.1.1 participants. participants were undergraduate students or young professionals in areas related to mathematics and computer science from lipetsk, russia, and received a monetary reward for their participation. all of them were monolingual speakers of russian and reported no auditory problems. the c group was comprised of 24 adults (14 women, mean age 23.7 ± 3.6 years, age range 18-30), whereas the v+t group was comprised of 24 adults (12 women, mean age 24.4 ± 4.5, age range 18-32). two additional participants were tested but not included because they failed to comply with the instructions in the test."
"patient has elevated t3, t4; low tsh, and elevated thyroid stimulating antiglobulin. this is very suggestive of hyperthyroidism due to an autoimmune process. listed symptoms (anxiety, weight loss, elevated hr, bp, tremor, sweating) all support this diagnosis."
"our goal our goal is to develop a new framework for selectivity estimation that can quickly refine its model after observing each query and, thereby, produce increasingly more accurate estimates over time. we call this new framework selectivity learning. in particular, we focus on designing a low-overhead method that can scale to a large number of observed queries without requiring an exponential number of buckets."
the bendy inequality sign (≽) means that every element of the vector on the left-hand side is equal to or larger than the corresponding element of the vector on the right-hand side.
"text-annotation (red arrows) contains data inputted using either a desktop keyboard (ipca) or a virtual keyboard (ipca-ce). drawing-annotation (blue arrows) allows users to directly annotate onto the display using a drawing tool controlled by a mouse (ipca) or finger touches (ipca-ce). users' findings can easily be recreated with the parameter sets, it is an important and useful feature for sharing findings with others. each finding is identified by the user's name and a timestamp. figure 6 shows how findings are managed. in ipca, all findings are listed chronologically in a separate window, and the user is provided with buttons for updating or deleting findings in the list (see top of figure 6a ). a finding is created using the current view and annotations and added to the list by selecting a button in the main window. in ipca-ce, however, the differences in display and interaction require that findings be managed differently than the desktop applications. findings are managed within a tabbed window activated by a button above each workspace (see bottom of figure 6a ). each finding is represented as a screenshot thumbnail identified with the user's name and timestamp. findings are created by touching a capture button in the workspace, and findings can be moved into the workspace for updating via a simple drag-and-drop operation. since multiple users can use ipca-ce simultaneously, each user has their own storage space (i.e. directory or folder) to manage findings."
"altogether, our findings show that speakers of the tested tone languages differ in terms of their sensitivity to statistical cues carried by consonants, but that they both benefit to a large extent from similar tp cues carried by tone-bearing vowels. tone-bearing vowels allow speakers of both languages to extract statistical words from artificial speech streams more successfully than speakers of a non-tone language, suggesting that the mechanisms supporting statistical segmentation in speakers of a tone language differ from those used by speakers of non-tone languages. we conjecture that speakers of non-tone languages may process statistics on tone-bearing vowels by means of general auditory mechanisms only [cit], whereas speakers of tone languages may further activate language-specific mechanisms boosting tp computations."
"although data analysis is often considered to be a stand-alone task, previous research has shown that analysis of empirical data in collaborative environments is important and should be considered when developing visualization applications [cit] . while collaborative analytics can occur in a variety of interaction modalities, we focus specifically on collaboration using a multi-touch table. specifically, a complete visual analytical system is designed for solving real-world tasks ought to have two integrated components: a single-user desktop application and an extended system suitable for a collaborative environment. specifically, an existing single-user desktop analytical tool for exploring data using principal component analysis (called ipca [cit] ) is adapted into a collaborative touch-table application (called ipca-ce [cit] ). extending an existing desktop application into a collaborative touch-table environment introduces unique technical challenges. the inherent differences between mouse and touch-based interaction require that the user interface must be redesigned, and limitations may also be imposed by factors such as system performance. however, despite the costs of converting an existing application into a new interaction modality, we believe that supporting both single-user and collaborative work in an integrated fashion provides important benefits for real-world analysis."
"since existing buckets may split into multiple ones for each new observed query, the number of buckets can potentially grow exponentially as the number of observed queries grows. for example, in our experiment in section 5.5, the number of buckets was 22,370 for 100 observed queries, and 318,936 for 300 observed queries. unfortunately, the number of buckets directly affects the training time. specifically, iterative scaling-the optimization algorithm used by all previous work [cit] -the cost of each iteration grows linearly with the number of variables (i.e., the number of buckets). this means that the cost of each iteration can grow exponentially with the number of observed queries."
"the results are plotted in figure 7b . as expected, the errors were highest under random-shift. however, the error still decreased as quicksel observed more queries; after 100 queries, the relative error was only 1.2% for the random shiftworkload. the errors were lower in general for the other workloads."
"overall task completion time and their findings were evaluated by analysing the recorded interaction logs and the captured findings. after solving each task question, a post-task questionnaire was given for tracking their personal opinions about the task and the tool for solving the task. at the end of the study, their personal factors of ease of use and usefulness of the system were asked using a 5-point likert scale, with higher numbers corresponding to more positive ratings. in addition, they were asked to provide their personal qualitative feedback about the application."
"as stated in section 1, we address this problem by employing a mixture model, which can express a probability distribution much more effectively than query-driven histograms."
"in here, we developed an informal model for general analytical process that should be supported and maintained to allow analysts to switch back and forth between singleuser and collaborative workflows. as illustrated in figure 1, we believe that analysts perform four distinct sharing processes: (a) asynchronous self-sharing in the desktop environment, (b) synchronous sharing in a collaborative environment, and (c, d) two asynchronous transitional sharing processes between the desktop and collaborative environments. all four sharing processes can be supported by passing the finding parameters between users and applications. processes (a) and (b) form continuous loops within each environment, while processes (c) and (d) form a global loop through which the entire analysis process iterates over time. by providing support for all four sharing processes, we form an integrated visual analytics system which reflects the analytical processes carried out by real-world experts."
"the ethics committee of the international school for advanced studies (sissa) in trieste, italy, approved the protocols of all the experiments of this study. [u], and [y] respectively). each syllable in the set was spoken by a phonetician-a native speaker of hong kong cantonese-with a low-level tone (t6, see figure 1 )."
"estimating the selectivity of a query-the fraction of input tuples that satisfy the query's predicate-is a fundamental component in cost-based query optimization, including both traditional rdbmss [cit] and modern sql-onhadoop engines [cit] . the estimated selectivities allow the query optimizer to choose the cheapest access path or query plan [cit] ."
"the bioworld system generates log files of user actions. there are three types of performance metrics in the log files, namely, diagnostic efficacy (e.g., accuracy, count of matches with experts, and percentage of matches with experts), efficiency (e.g., number of tests ordered and time to solve the case), and affect (e.g., confidence). information saved in the log-file included the attempt identifier (participant and case id), a timestamp, the bioworld space (e.g., chart), the specific action taken (e.g., add test), and details in relation to the action (e.g., thyroid stimulating hormone (tsh) result: 0.2 mu/l). along with the aforementioned data elements, the case summaries written in the bioworld system are also logged in the log files."
"in this article, we test whether the cv hypothesis can be extended to a language with lexical tones. our first research question was whether speakers of a tone language are sensitive to tps carried by consonants in the same way as speakers of non-tone languages. we explored this question by presenting native speakers of cantonese with flat-tone speech streams carrying tp cues either only on consonants or only on vowels. our second question involved the effect of tonal variation in the speech streams and, in particular, we investigated whether the computation of tps on vowels and tones coupled together would be improved with respect to vowels alone. we then further explored the generalizability of cantonese speakers' data by testing speakers of mandarin and russian (a tone and a non-tone language, respectively) with two conditions that we observed to be critical. to our knowledge, this is the first study using the tp-based speech segmentation paradigm to assess the cv hypothesis with speakers of tone languages."
"creating a collaborative tabletop visual analytics system based on a single-user desktop counterpart is not without technical challenges. these challenges involved with the transitioning of a single-user, single-touch system to a multi-user, multi-touch environment include performance limitations, differences in rendering mechanisms, and differences in user input modalities. here, we present the most significant challenges in detail and describe our solutions for overcoming them."
"max-entropy histograms max-entropy histograms [cit] find a maximum entropy distribution consistent with the observed queries. unfortunately, these methods generally suffer from the exponential growth in their number of buckets as the number of observed queries grows (as discussed in section 2). quicksel avoids this problem by relying on mixture models."
"together, ipca and ipca-ce form an integrated toolset which allows analysts to switch back and forth between the two visualizations on separate hardware without losing track of their current analysis tasks. while single-user analysis could technically be performed on the touch-table using the collaborative application, this might not be as effective and productive as using the standalone desktop application. since experts often prefer to work alone and switch their analysis process into a collaborative group activity only when necessary [cit], it is important to provide both applications, using hardware appropriate for the type of interaction required by each. since ipca and ipca-ce applications support a continuous analysis process that permits analysts to switch back and forth between desktop and collaborative environments, users can export findings and system parameters back and forth between applications, allowing them to transition from single-user to collaborative contexts without losing track of their current analytical process."
"as will be presented in section 4.2, quicksel's training involves the computations of the intersection size between two subpopulations, for which the essential operation is evaluating the following integral:"
"initially, before any query is observed, we can conceptually consider a default query (p 0, 1), where all tuples are selected and hence, the selectivity is 1 (i.e., no predicates)."
"data dimension lastly, we studied the effect of data dimension (i.e., number of columns) on error. here, we generated the datasets using multivariate normal distributions with different dimensions. for each dataset, we used three methods-autohist, autosample, and quicksel-to produce selectivity estimates for the predicates on all dimensions. autohist used 1000 buckets, autosample used 1000 sampled rows, and quicksel used 1000 observed queries. figure 7d shows the result. the error of autohist increased quickly as the dimensions increased, which is a well-known problem of multidimensional histograms. in contrast, the errors of autosample and quicksel were not as sensitive to the number of dimensions of the data. the robustness of quicksel is due to the fact that its internal model only depends on the intersection sizes of the query predicates (not on their dimensions). among these methods, quicksel was the most accurate."
"however, unlike pad++, the portals in ipca-ce are not always axis-aligned. since a multi-touch table is inherently without orientation, we wanted the ipca-ce interface to be usable by all users standing around the table, regardless of their positions. to that end, each of the portals needs to be rotatable on demand so that it can be appropriately oriented to its user's position around the table. the rendering mechanism for each ipca-ce portal is therefore based on a hierarchical structure of geometries (such as a line, a dot, or a polygon), each referenced by the coordinates of its parent portal. as the user rotates or resizes a portal, each geometry will update its global coordinates to reflect the change."
"with a few exceptions, multi-threaded operation is not usually required for desktopbased visual analytics applications since all interactions are based on single mouse input. however, in a collaborative environment, multiple processes and threads are necessary to manage the display and listen for incoming touch events. by utilizing a multi-process and multi-threaded architecture, ipca-ce becomes significantly more complex than ipca. however, this architecture allows the application to take advantage of a multi-core cpu to support non-interrupted real-time interactivity for multiple users."
"the case summaries were extracted from the log files generated by the bioworld system. the methodology employed in our study is displayed in fig. 2 . in our study, the weka [cit] toolkit (fig. 3) was used to run our experiments. weka is a comprehensive workbench for machine learning algorithms for data mining tasks ranging from data preprocessing to classification. the preprocessing filter stringtowordvector provided by weka was used to extract feature vector from the summary texts. the default values were used except for the parameter that converted texts into lowercase. stemmer is commonly used to reduce the feature size in text classification problems where the feature size tends to be in the order of tens of thousands. because our dataset is fairly small in terms of both the size of each summary as well as the total number of summaries provided by expert and novices in the field, the stemmer was not utilized. stopwords were not removed; they were kept as the part of feature vector. using the aforementioned preprocessing technique, 823 word tokens were extracted as features. the resulting arff file format was then converted into libsvm format supported by cwlc tool out of the box."
"observe that w * is expressed in a closed form; thus, we can obtain w * analytically instead of using iterative procedures typically required for general quadratic programming."
"it is important to point out a limitation of our experimental approach. our materials were designed so as to provide informative statistical cues only between a single type of unit, keeping tps between the other units and between adjacent syllables equal to 0.5. on top of this, we used only syllables unattested in cantonese to avoid any semantic confusion. these strong constraints limited the number of available combinations between vowels, consonants, and tones in a manner that made it extremely difficult to manipulate tps for vowels and tps for tones independently. as a result, the question concerning whether tones alone are sufficient carriers of statistical information is still open. still, our data demonstrate that the presence of lexical tone in a language modulates the computation of statistical cues."
"transitional sharing from the collaborative environment back to the desktop application (see figure 1d) has not been considered in many previous collaborative visual analytics systems. however, we believe this is an important method for users to further investigate interesting results that were shared during the collaborative session. thus, it is also possible to export the findings of collaborative analysis from ipca-ce as an xml file for transmission over the network to ipca. to understand the sharing processes, we performed user studies in two different environments. in following sections, we described how we performed the studies and what we found."
"in recent years, text mining has become an important means of knowledge-based discovery and has assumed a central method with multifarious potential applications in varied fields ranging from commerce to education. the field of educational data mining has significantly grown and data mining has been applied with much success on educational data [cit] . when investigating the potential for augmenting the novice-expert overlay model in bioworld, one natural idea was to consider the use of machine learning techniques for this task. although a survey of the literature will yield a gamut of machine learning algorithms for solving text classification problems, the scope of this study has been limited to an online algorithm. since our study is an initial attempt for augmenting the novice-expert overlay model in the bioworld system, as such, for the purpose of simplicity, the constrained nature of our study is appropriate. future extensions of this study will explore other algorithms."
"with that of an earlier predicate, they split the bucket(s) created for the earlier one into two or more buckets in order to ensure that the buckets do not overlap with one another. figure 1 shows an example of this bucket splitting operation. 2. training: after creating the buckets, query-driven histograms assign frequencies to those buckets. early work [cit] determines bucket frequencies in the process of bucket creations. that is, when a bucket is split into two or more, the frequency of the original bucket is also split (or adjusted) such that it minimizes the estimation error for the latest observed query. however, since this process does not minimize the (average) error across multiple queries, their estimates are much less accurate. more recent work [cit] has addressed this limitation by by explicitly solving an optimization problem based on the maximum entropy principle. that is, they search for bucket frequencies that maximize the entropy of the distribution while remaining consistent with the actual selectivities observed. although using the maximum entropy principle will lead to highly accurate estimates, it still suffers from two key limitations."
figure 1: bucket creation for query-driven histograms. p 3 is the range of a new predicate. the existing buckets (for p 1 and p 2 ) are split into multiple buckets. the total number of buckets may grow exponentially as more queries are observed.
"one method for overcoming this inherent difference is to use a multitude of gestures to describe each possible operation. however, given the number of features in ipca-ce, creating a gesture for each feature will inevitably confuse the user and steepen the learning curve, thereby lowering the usability of the system. we therefore take a \"lowtech\" approach by creating buttons along the borders of each portal (see figure 2b ). the majority of the features in ipca-ce can be performed by activating or toggling these buttons. however, for the user's convenience, a few popular gestures that have been widely adopted by multi-touch devices (such as the iphone) have been incorporated into the ipca-ce interface. these gestures include resizing, zooming, and rotation by using two fingers simultaneously. during our user study, we observed that these gestures were intuitive for participants and did not introduce usability concerns during their analyses."
"mark and kobsa [cit] performed an empirical study to understand the differences between group and individual behavior within collaborative information visualization environments. they found that a group solves the given questions more accurately and spends less time doing so. however, it is still unknown what features should be supported within a collaborative data analysis system on a touch-table in order to reliably gain these benefits. ma [cit] discussed existing web-based collaborative workspaces in terms of sharing high-performance visualization facilities, visualizations, and findings, and noted that sharing visualization resources will eventually provide support for collaborative workspaces. despite the numerous collaborative visualization systems that have been developed, it is still unclear how these systems should be designed, though some guidelines have been suggested. heer and agrawala [cit] provided design considerations for asynchronous collaboration in visual analytics environments. additionally, in a review of existing applications in terms of controlling the visualization, johnson [cit] outlined challenges and suggested guidelines for the design of synchronous collaborative visualizations. however, none of these guidelines address how analytic processes might be extended across both single-user and collaborative modalities, as we focus on in this paper."
"sampling sampling-based methods rely on a sample of data for estimating its joint distribution [cit] . however, drawing a new random sample requires a table-scan or random retrieval of tuples, both of which are costly operations and hence, are only performed periodically."
"we investigated our first research question by presenting cantonese speakers from hong kong with artificial speech streams where statistical cues were carried either only by consonants (c group) or only by vowels (v group). to make our results comparable with previous data from non-tone languages (see table 1 for a short summary of relevant data), we used only syllables produced with a low-level tone (t6; see methods). to avoid any confusion due to syllable frequency in cantonese, all syllables used in the streams were unattested in the language (bearing this specific tone)."
"findings are stored in an xml format (see figure 5 ) similar to the p-set model [cit] . however, since our design philosophy does not require us to track all of a user's exploration procedures, we simply describe each finding with parameter sets. the parameter sets are similar to the sets defined in the p-set model, though defined specifically for our visual analytics system. sets represent interactive operations (such as selection and deletion), view, sliderbar control, text-and drawing-annotations, and a final result. since"
"we also studied the relationship between the number of model parameters and the error. the lower the error (for the same number of model parameters), the more effective the model. figures 4b and 4d show the result. given the same number of model parameters, quicksel produced significantly more accurate estimates. equivalently, quicksel produced the same quality estimates with much fewer model parameters."
"during the course of the study, we observed that participants continuously communicated their ideas in the single workspace condition because they needed to have an agreement before performing an action in their shared workspace (see figure 8a ). in the single workspace condition, we found that interference [cit] sometimes occurred between collaborators because their desired working areas are overlapped. we also observed that when one participant was interacting with the single shared workspace, the partner would observe these interactions and try to think of new ideas or strategies. this pattern of collaboration is alternated continuously as the participants exchanged roles. however, we also noticed that when partners had different ideas of how to solve a problem, collaborating in a shared workspace became difficult."
"conversion two: moving constraints the quadratic programming problem in theorem 1 can be solved efficiently by most off-the-shelf optimization libraries; however, we can solve the problem even faster by converting the problem to an alternative form. we first present the alternative problem, and then, we discuss it."
"to this end, it is necessary to study a broader set of tone and non-tone languages to understand the elements determining speakers' ability to use statistical cues carried by consonants and vowels. one particularly interesting case is danish: højen [cit] presented data from 20-monthold infants exposed to danish-a language with the peculiarity that vowels outnumber consonants-in a word-learning task, showing that they tend to rely on vowels more than on consonants."
"due to differences between desktop and collaborative environments, ipca-ce is designed to support multi-touch input. from our previous study with ipca [cit], only important and frequently used interaction features were replicated as touchable buttons in ipca-ce. a total of 16 touchable buttons and 2 tab buttons were designed for interaction in ipca-ce: nine buttons for interacting with represented data items ( figure 2b-6 to support users managing and sharing their findings, annotation techniques are added to both applications. a detailed explanation about these annotation techniques is included in section 5."
"today's databases typically rely on histograms [cit] or samples [cit] for their selectivity estimation. these structures need to be populated in advance by performing costly table scans. however, as the underlying data changes, they quickly become stale and highly inaccurate. this is why they need to be updated periodically, creating additional costly operations in the database engine (e.g., analyze table). 1 to address the shortcoming of the scan-based approaches, numerous proposals for query-driven histograms have been proposed, which continuously correct and refine the histograms based on the actual selectivities observed after running each query [cit] . there are two approaches to query-driven histograms. the first approach [cit], which we call error-feedback histograms, recursively splits existing buckets (both boundaries and frequencies) for every distinct query observed such that their error is minimized for the latest query. since the errorfeedback histograms do not minimize the (average) error across multiple queries, their estimates tend to be much less accurate."
"if an analogous finding were demonstrated in adult speakers as well, danish would also constitute a violation of the cv hypothesis. another interesting case is japanese, where pitch accent might, in principle, play a similar role to tones in cantonese and mandarin."
"this section presents how quicksel models the population distribution and estimates the selectivity of a new query. quicksel's model relies on a probabilistic model called a mixture model. in section 3.1, we describe the mixture model employed by quicksel. section 3.2 describes how to estimate the selectivity of a query using the mixture model. section 3.3 describes the details of quicksel's mixture model construction."
"2.1.1 participants. the participants in experiment 1 were undergraduate students of linguistics at the chinese university of hong kong, who received course credit for their participation. they were cantonese-english 2 bilinguals but reported cantonese to be their dominant language, as well as having no auditory problems. the c group comprised 21 adults (16 women, mean age 20.2 ± 2.7 years, age range 18-30), whereas the v group comprised 21 adults (17 women, mean age 20.0 ± 2.2, age range 18-26). two additional participants were tested but not included because they declared english to be their dominant language."
"although supporting collaboration when solving real-world analytical tasks is important, most visual analytics tools have been designed as single-user desktop systems [cit] . since we believe that user-friendly visualizations in a collaborative environment enable users to find results more accurately, we chose to extend a known and useful application to work in a collaborative touch-table environment. multi-touch surfaces support a rich set of interactions that allow multiple users to work together to solve complex analytical problems interactively. we selected the interactive principal component analysis (ipca) application, which has been shown to be an effective and easy to use desktop visualization for analysing data sets and interactively exploring the parameters of principal component analysis [cit] . figure 2 shows a system overview showing ipca and ipca-ce with an analysis of the glass dataset, which is a publicly available scientific result from the uci machine learning repository [cit] ."
"of particular interest to us is how the presence of lexical tone in a language affects speechprocessing biases that have been widely documented in non-tone languages. for instance, tone languages exhibit reversed effects in the processing of consonants and vowels when compared to non-tone languages: it is a well-known fact that when speakers of dutch and spanish are presented with a pseudoword, such as kebra, and asked to convert it into a word by changing a single phoneme, they prefer to substitute a vowel (e.g., producing cobra) rather than a consonant (e.g., producing zebra) (cutler, sebastián-gallés, [cit] ) . [cit] . in the present study, we explore a similar case of processing bias regarding consonants and vowels that has been investigated in several non-tone languages but not, so far, in tone languages, namely that consonants are favored for the computation of transitional probabilities (tps) in speech segmentation."
"to simplify our presentation, we focus on conjunctive predicates. however, negations and disjunctions can also be easily supported. this is because our algorithm only requires the ability to compute the intersection size of pairs of predicates p i and p j, which can be done by converting p i ∧ p j into a disjunctive normal form and then using the inclusion-exclusion principle to compute its size."
"observe that with this approach, we need four separate integrations only for two subpopulations. in general, the number of integrations is o(2 m ), which is o(2 n ). thus, this direct approach is computationally intractable."
"we have demonstrated quicksel's superior accuracy and performance for selectivity estimation; however, we have not studied its impact on query optimization (including access paths and join planning). we plan to integrate quicksel into an open-source dbms in order to study its impact on the overall quality of the query plans chosen by the query optimizer."
"although it has been found that the initial version of ipca is superior to sas/insight for solving analytical questions [cit] from the within-subjects user study, the effectiveness of the updated version of ipca for solving analytical tasks, and creating and sharing findings in single-desktop environments has not been clearly determined. to understand the users' analytical processes as well as the effectiveness of capturing and sharing findings with ipca, we performed a user study. in the study, we asked participants to capture their findings with utilizing text-and drawing-annotations. this study was conducted under the approval (protocol number: 325298-1) by the institutional review board (irb) at the university of the district of columbia."
4. querymodel [cit] : this method computes the selectivity estimate by a weighted average of the selectivities of observed queries. the weights are determined based on the similarity of the new query and each of the queries observed in the past.
"to study how quicksel's accuracy changes based on different degrees of correlations in the data, we used our gaussian workload, generating values with different correlations between columns. in each case, quicksel trained its model using 100 observed queries; the error was measured using the other 100 queries (not used for training). as shown in figure 7a, the errors remained almost identical across all different degrees of correlation."
"then, our goal is to build a model of f (x) that can estimate the selectivity s n+1 of a new predicate p n+1 ."
"although half of the participants are new to visualization, approximately 72% of the participants answered correctly. about 78% of the participants mentioned that ipca was very or somewhat useful for solving the all task questions. from the study, we noticed that most participants spent relatively large amount of time by trying to find correct answers through interaction with dimensions (specifically for the task 3 and 4)."
"to overcome the limitations of sharing ideas, parallelizing efforts, and performing discussion and consensus building in single-user desktop environments (see section 2), numerous studies on collaborative visualization have been performed. however, a limited number of studies have been performed to find limitations in a collaborative environment, such as our multi-touch table, especially when solving analytical problems. based on our understanding of user behaviors when solving complex analytical problems, along with the results of studies of real-world analysts [cit], we performed an additional user study with the collaborative ipca-ce application in single, double, and multiple workspaces. this study was conducted under the approval (protocol number: 09-11-04) by the institutional review board (irb) at the university of north carolina at charlotte."
"we also compared quicksel to two automatically-updating scan-based methods, autohist and autosample, which incorporate sql server's automatic updating rule into equiwidth multidimensional histograms and samples, respectively. since both methods incur an up-front cost for obtaining their statistics, they should produce relatively more accurate estimates initially (before seeing new queries). in contrast, the accuracy of quicksel's estimates should quickly improve as new queries are observed."
"a sample of a case summary written by an expert is presented below: 37 year old female, presenting after starting high blood pressure pill with episodes of palpitations, flushing and sweating. on exam, hypertensive, relative tachycardia. labs revealed: normal tsh, t4, t3, glucose. elevated free urinary cathecolamines but normal total. ct abdo normal."
"our model to overcome the limitations of query-driven histograms, we use a mixture model [cit] to capture the unknown distribution of the data. a mixture model is a probabilistic model to approximate an arbitrary probability density table 1 : the differences between query-driven histograms [cit] and our method (quicksel) ."
"experiments 1 and 2 revealed that speakers of cantonese were unable to use statistical information carried by consonants, in sharp contrast to statistical information carried by vowels and tones together. these results raise two further questions. a first methodological question is whether speakers of a non-tone language would succeed in segmenting the speech streams where tps are carried by consonants. this issue is important to assert the validity of our materials and comparability with previous research. the second question relates to the good segmentation performance by cantonese speakers when using statistical cues carried by tone-bearing vowels, and if this outcome is specific to speakers of tone languages. to investigate these issues we recruited native speakers of russian, a non-tone language whose speakers can distinguish all the syllables in our materials (even if they might perceive different segments than speakers of cantonese). russian speakers were randomly assigned to one of two groups, one that listened to the same materials provided for the c group in experiment 1 (flat intonation and tps carried by consonants), and a second one that listened to the same materials provided for experiment 2 (tps carried by tonebearing vowels)."
"using these ideas, we have developed a first prototype of our selectivity learning proposal, called quicksel, which allows for extremely fast model refinements. as summarized in table 1, quicksel differs from-and considerably improves upon-query-driven histograms [cit] in"
"the application consists of four views: projection view ( pearson-correlation coefficient provides a value between +1 and -1 by measuring the linear correlation between two variables. if the value is close to 1, the two variables maintain a positive correlation. otherwise (close to -1), the two variables preserve a negative correlation. all views are closely connected, such that if the user changes the elements in one view, its corresponding results are updated in other views (brushing & linking). this interactivity allows the user to infer relationships between the coordinated spaces (see the paper [cit] for details)."
"two additional experiments with speakers of mandarin-another tone language-and russian-a non-tone language-shed further light on the generalizability of these results and the nature of the mechanisms involved. both speakers of mandarin and russian succeeded in using statistical information carried by consonants, demonstrating that the ability to use this type of information is not incompatible with the presence of lexical tones in a language. nonetheless, speakers of mandarin and cantonese performed similarly well in extracting tps from tone-bearing vowels. speakers of these two tone languages showed better segmentation of tps carried by tone-bearing vowels than of those carried by consonants. likewise, performance of these speakers in segmenting streams based on tone-bearing vowels was better than that of russian speakers in the same task."
"a total of 12 graduate students participated in the study (nine male, three female). eight participants had limited experience using a multi-touch table, and four of them had no experience. the experiment required two participants to work together to solve a given task. the study used a within-subjects design with three conditions, corresponding to the number of available workspaces (see figure 8 ):"
"for this experiment, we used the speech streams provided for the c group in experiment 1 and those provided for experiment 2. 4.1.3 procedure. the procedure was similar to that of experiment 1, except that instructions to participants were presented on the computer screen in russian."
"likewise, a very closely related system for conducting geospatial analysis utilizes tablet pcs to allow users to issue commands to the collaborative environment [cit] . interestingly, they theorize that their single shared display might discourage individuals from exploring, while providing individuals their own personal interfaces might make them more comfortable exploring their own ideas in private before sharing them with collaborators. indeed, our work builds on this speculation, examines the issues involved, and provides a mature implementation that supports and encourages this behavior. overall, to the best of our knowledge, there is no visual analytics tool has been mainly designed to support both single and collaborative environments. most visual analytics tools are designed to work in a single desktop environment. therefore, they need to be modified or rebuilt to make them work in a collaborative environment. since modifying or rebuilding existing visual analytics tools requires additional time and efforts, web-based technology is commonly adopted to build collaborative visual analytics spaces [cit] . in this paper, we explain how our visual analytics tools (ipca and ipca-ce) are designed to support both environments with emphasizing some technical considerations how to overcome technical limitations of supporting the environments (see section 4.4)."
"in addition, we adopt a non-conventional variant of mixture models, called a uniform mixture model. while uniform mixture models have been previously explored in limited settings (with only a few subpopulations) [cit], we find that they are quite appropriate in our context as they allow for efficient computations of the l2 distance. that is, with this choice, we can evaluate the quality of a model by only using min, max, and multiplication operations (section 3.2). finally, our optimization problem can be expressed as a standard quadratic program, which still requires an iterative procedure."
join selectivity learning accurate join selectivity estimation requires the correlation information between the join keys. we plan to extend quicksel to incorporate these statistics in its mixture model.
"applications support similar user interaction techniques (selection, manipulation, zooming, etc.) to perform interactive data analysis [cit] . therefore, the users can perform a smooth transition with their findings from one environment to another."
"transitional sharing from the desktop application to the collaborative environment represents the process by which the results from a single user's analysis are made public for the rest of the group (see figure 1c) . if the user wants to take their findings from ipca into a collaborative setting, the xml file can be transmitted over the network and imported into ipca-ce."
"to achieve a higher accuracy, the second approach is to compute the bucket frequencies based on the maximum entropy principle [cit] . however, this approach (which is also the state-of-the-art) requires solving an optimization problem, which quickly becomes prohibitive as the number of observed queries (and hence, number of buckets) grows. unfortunately, one cannot simply prune the buckets in this approach, as it will break the underlying assumptions of their optimization algorithm (called iterative scaling, see section 2.3 and appendix b for details). therefore, they prune the observed queries instead in order to keep the optimization overhead feasible in practice. however, this also means discarding data that could be used for learning a more accurate distribution."
"adaptive selectivity estimation [cit] fits a parametric function (e.g., linear, polynomial) to the observed queries. this approach is more applicable when we know the data distribution a priori, which is not assumed by quicksel."
"one of the extended forms of tool life equations takes into consideration the cutting speed v c and-to a lesser degreethe feed per revolution f z . the machining time given the additional effect of feed on the tool life can be defined by t 2, as in eq. (24). equation (26), which defined the tool life t v, takes a different form for this case"
"customer 53990001 is selected from category 2 for the forecasting customer. the mapes during a training period for category 2 are shown in figure 9 when the parameters are set as shown in table 6 . the compared curves of actual load and forecasting load using the proposed method on 18 november for customer 53990001 are shown in figure 10 . the mape for customer 53990001 on 18 november is 10.23%. the compared curves of actual load and forecasting load from 18 to 24 november for customer 53990001 are shown in figure 11 . the mape for customer 53990001 in this week is 10.97%. in figures 10 and 11, the error in sample points of one day is basically average and becomes larger when the curve comes to a peak. it is reasonable because the high or low peak is not reachable in most cases. the network should balance the prediction results for most situations during the training process. according to figure 9, the mape decreases to a steady state as the epoch increases to 200. according to figures 10 and 11, the forecasting curve is close to the actual curve, which proves the availability of the proposed method. the samples are preprocessed by k-means clustering algorithm to form three categories for training. we performed a comparative experiment with variable-controlling approach about clustering. the compared results of customer 53990001 [cit] are shown in figure 12 . the compared mapes of prediction on 18 november for nine customers in three categories from different feeders are shown in table 7 . it can be concluded that the forecasting curve without clustering deviates from the actual curve and that its mape is larger. the reason is that different characteristics of electricity use create a bad effect for short-term load forecasting. the effect reduces when we use corresponding trained networks for different customers. therefore, the performance is generally improved by clustering. the input of proposed network includes d p, w p, t p and p l, which means that the network obtains and fuses the previous load changing process and other environmental information. in this case, we removed the input layer and the following fully connected layers in the network. the comparison results of customer 53990001 with multi-source or only load data input are shown in figure 13 . the compared mapes for nine customers in three categories from different feeders are shown in table 8 . the experimental condition is the same as the one above. it can be concluded that the performance of only using load data is obviously poorer. although the change shape is similar to actual, the curves deviate from the actual curves. correspondingly, the mapes are larger. the reason is that date, weather and temperature are necessary factors to consider during short-term load forecasting processing. people would raise their load on a hot or cold day, even a rainy or snowy day."
"the shortest machining time in the model with neglecting the tool life is achieved for rotational spindle speed equal to n max . the most efficient machining method corresponding to the cutting width a e, rotational spindle speed n max, and other given parameter values-for the model with neglecting the tool life-is presented in fig. 14a . the machining time t 4 (min) conducted in m-passes that accounts for tool life defined as t v or t v, f can be expressed as"
"the vanishing gradient problem can be solved by adding control gates for remembering information in the process of data transfer. in lstm networks, the hidden units of rnns are replaced with lstm blocks consisting of cell, input gate, output gate and forget gate. moreover, the forget gate and input gate are combined into a single update gate in gru neural network. the structure of gru is shown in figure 2 ."
"to increase the stability and economy of power grids, a method for short-term load forecasting with multi-source data using gru neural networks is proposed in this paper, which focuses on individual customers. the proposed structure of the whole network is shown in figure 5 . the real-world load data of individual customers is recorded from dongguan power supply bureau of china southern power grid in guangdong province, china. before training, the customers with load data are clustered into three categories by k-means clustering algorithm to reduce the interference of different electricity use characteristics. then, the environment factors are quantified and put into the input of the proposed networks for more information. the gru units are introduced into the network for its simpler structure and faster convergence compared to lstm blocks. the results in figures 12 and 13 show that clustering and multi-source input can help to improve the performance of load forecasting. the average mape can be low as 10.98% for the proposed method, which outperforms the other current methods such as bpnns, saes, rnns and lstm. the improvement is notable (figures 15-17) . in general, the availability and superiority of the proposed method are verified in this paper. in the future, combining with the technique of peak prediction could be a subject worth studying for load forecasting. moreover, since the load forecasting for the customers in all power grid areas is a large-scale task, transfer learning and continuous learning will be considered based on the proposed framework for high-efficiency load forecasting."
"both methods have their advantages and disadvantages. standard threading requires time for tool return to avoid high dynamic movements in the x-axis. rope threading does not need multiple passes of the tool, but its efficiency is often constrained by dynamic tool motion limitations. therefore, a third method is proposed: hybrid threading, which achieves a favorable balance between both methods described above. in this method, the number of tool passes is less than in standard threading but more than one (as in rope threading). as a result, high accelerations of rope threading and the high feed rate in the z-axis of standard threading are both mitigated. detailed tool movements of the three methods are described in section 4."
"software testing is the process of executing the program with the intent of finding the faults. a web application pen test uses unique techniques to expose potential flaws in client/server applications. actually, a web application is a client/server model with the browser as the client. both black box (functional) and white box (structural) testing can be used in the upcoming phases. in this paper, it is proposed to do functional or black box testing of websites due to the following reasons:-1. initially, when a project starts, source code is usually unavailable."
"moreover, the factors of date, weather and temperature should be added into input with quantization. first, the power consumption should be different between weekdays and weekends. the official holidays are also an important factor, so we quantify the date index as shown in table 1, where the index of official holidays is 1 no matter what day it is. similarly, the weather and temperature are quantified according to their inner relations, as shown in tables 2 and 3 ."
the feedforward deduction process for gru units is shown in figure 2 and given by equations (4)- (10) . where u is the number of update gate vector; r is the number of reset gate vector; h is the number of hidden vectors at t time step; h is the number of hidden vectors at t − 1 time step; f and φ are the activation functions; f is the sigmoid function and φ is the tanh function generally; and s t h means the new memory of hidden units at t time step.
"apart from machine acceleration a m, the cutting speed v c is another constraint on the rotational spindle speed n both in the upper and lower bands."
"when the directions of spindle rotations and feed f z are correctly set, the tool does not pass over the whole contour in one spindle revolution, and its value is decreased by f z . the results are presented in fig. 4, which are schematic and may not fully reflect the real relations between dimensions of analyzed objects."
"the v c values change during machining because of dimension h, which must be taken into account if these changes significantly affect the cutting speed v c ."
"substituting eqs. (2) and (3) into eq. (1) and differentiating the result, we obtain eqs. (4) and (6), which define the tool acceleration in the x-axis on arcs defined by radii r 1 and r 2, respectively: a x1 (4) and a x2 (6) . the section that connects both thread arcs defined by radii r 1 and r 2 is tangent to the arcs and is a straight line, which means that tool acceleration in the x-axis in this section is zero, as described by eq. (5) ."
"there are two typical turning methods for thread machining: standard threading and rope threading, as shown in fig. 2 . standard thread machining requires many passes of a tool, and the feed rate is equal to the thread pitch. this method uses a relatively slow spindle speed and a relatively high z-axis feed rate. the tool placement in the x-axis is set to be constant during the pass. the use of many positioning movements after each pass increases machining time. in contrast to the standard threading method, rope threading requires only one pass of a tool, and the feed rate in the z-axis is smaller. however, tool movements in the x-axis must be implemented, and high accelerations are required to ensure efficient machining. for rope threading, suitable dynamic properties of a lathe drive responsible for the x-axis feed are typically the main constraint."
"all entities of a web application must be tested thoroughly. but web testing raises new issues also. one of the key issues is the unexpected change in state when the browser's back button is clicked or direct entry of url in the browser. thus, there is a needa) to test the navigation between pages those are of interest."
the tool life is a factor to be considered when planning the mass production of threads. equations describing it were proposed by f. w. taylor. a tool operating at a higher cutting speed has a shorter lifetime. the minimization of the machining time t 2 accounting for the tool life relative to the cutting speed v c is given by
"resident customers may increase electricity consumption on weekends but business customers may not. these are some obvious reasons why we should consider the environment factors. it can be concluded from the two experiments that the mapes are floating in a certain degree. the maximal mapes of all samples in the conditions of the two experiments are shown in table 9 . the maximal mape without clustering and with only load data is significantly larger than the proposed method with clustering and multi-source data. the maximal mape of proposed method is 15.12%, which is acceptable for load forecasting of individual customers. the performances are good with lstm networks in dealing with time sequence but there are more parameters to train compared with gru neural networks. in the proposed network, the gru layers have 285,300 parameters to train while the lstm layers have 380,400 parameters with the same architecture. the cost time for training with lstm network is about 20% longer than training with gru neural networks in the experiments in this paper. the mapes of network with lstm and gru layer in the same architecture with the same samples in category 2 during the training process are shown in figure 14 . we can conclude that gru neural networks do better in both convergence speed and training time, which depends on the improved single structure of gru units. we also performed the experiments to compare with current methods such as back-propagation neural networks (bpnns) [cit], stacked autoencoders (saes) [cit], rnns [cit], and lstm [cit] . their parameters and structures are set as described in section 3.2. the compared average mapes of these methods, trained and tested with all samples described at the beginning of this subsection, are shown in figure 15 . the specific values of average and maximal mapes are shown in table 10 . moreover, the results of nine customers are shown to validate the better performance of the proposed methods. the mapes for 30 [cit] are shown in table 11 . it can be concluded that the proposed method results in smaller error in both average and maximal mapes. the proposed method performs better compared to the other current methods in most cases for short-term load forecasting in wanjiang area. in detail, the forecasting load curves of customers 37148000 and 53990001 on 30 [cit] based on these methods are shown in figures 16 and 17 . we can observe the closest curve to the actual curve is the proposed method in the results of these experiments. time information is important in short-term load forecasting which the bpnns and saes cannot extract. therefore, they get poorer performance in the experiments. the vanishing gradient problem limits the performance of rnns because of the decreasing perception of nodes. the architecture is simpler and the parameters are fewer in gru neural network compared to lstm networks (section 2.1). therefore, the performances of gru neural networks are better than the other current methods. in general, the availability and improvement of the proposed method are proven by the real-world experiments."
"static analysis of websites is a specialized area covered by dedicated tools like hyperlink tools. these tools are also known as web-spiders. these tools must interact with website and internet (both) in order to find defects. the defects found by these tools include:-a) incorrect route to a wrong page. the point is that if at the design level only, a website security is thoroughly tested then the errors need not propagate to further phases of sdlc."
"in this section, the experiments are described in detail and the results are shown in figures and tables. the specific discussion for results is elaborated after the results and prove the improved performance compared to other methods. the data for experiments are recorded in section 2.2."
"using v c definition and eqs. (26) and (27), we obtain the optimum rotational spindle speed n opt t v when the tool life is taken into account"
"the time in which the center of the tool nose travels from point a 1 to point b 1 can be determined by dividing the travelled section along axis z′, by constant speed v z′ given by eq. (7)"
"web applications testing is a specialization of software testing. testing a web application is challenging, not only for mission critical tasks it carries out but also for factors like security, the scalability and the high dependency of the outputs on the web browser and on the way a user interacts with the web browser. a few of recent studies are stated below."
"the contour is made up of arcs linked with a straight line tangent to each of them. the thread pitch is defined by p, and the thread depth is defined by h."
"where w is the weight; a is the sum calculated through weights; f is the activation function; s is the value after calculation by the activation function; t represents the current time of the network; i is the number of input vectors; h is the number of hidden vectors in t is time; h is the number of hidden vectors in t − 1 time; and o is the number of output vectors. similar to conventional neural networks, rnns can be trained by back-propagation through time [cit] with the gradient descent method. as shown in figure 1, each hidden layer unit receives not only the data input but also the output of the hidden layer in the last time step. the temporal information can be recorded and put into the calculation of the current output so that the dynamic changing process can be learned with this architecture. therefore, rnns are reasonable to predict the customer load curves in power grids. however, when the time sequence is longer, the information will reduce and disappear gradually through transferring in hidden units. the original rnns have the vanishing gradient problem and the performance declines when dealing with long time sequences."
"the theoretical surface roughness depends on the cutting width a e, geometry of the cutting edge, and the machined profile. the cutting width a e, defined as the distance obtained by projecting tool points tangent with the machined surface profile on the z-axis (fig. 8a), should be constant for a given set of machining parameters. in rope threading, the defined cutting width a e is equal to the feed per revolution f z ."
"this finding means that if inequality (38) is satisfied, it is worth increasing the number of passes as much as possible; hence, standard threading is the most efficient method. if the inequality is not satisfied, the most efficient machining is conducted in one pass, corresponding to rope threading, and each additional pass increases the machining time."
"gated recurrent unit neural networks are the improvement framework based on rnns. rnns are improved artificial neural networks with the temporal input and output. original neural networks only have connections between the units in different layers. however, in rnns, there are connections between hidden units forming a directed cycle in the same layer. the network transmits the temporal information through these connections. therefore, the rnns outperform conventional neural networks in extracting the temporal features by these connections. a simple structure for an rnn is shown in figure 1 . the input and output are time sequences, which is different from original neural networks. the process of forward propagation is shown in figure 1 and given by equations (1)-(3)."
"software testing is both an art and science. the security aspect of testing can be introduced right from the start of software development life cycle (sdlc) -right from requirements analysis phase far architecture and design, coding, software testing, quality production, to the operation and maintenance [cit] . weaving security into the web application delivery lifecycle does require a synergistic approach that incorporates people, process and technology (ppt)."
"the tool path in the x-axis for one spindle revolution at constant speed is presented in fig. 3a as the tool path along a profile in the z′-axis. when the thread is rotating in the spindle, the theoretical profile of the xz-plane cross section appears to move along the z-axis (in the area constrained by the thread length). to describe that movement relative to the tool movement in the z-axis, the z′ axis is introduced. figure 3a shows the relative movements conducted by the tool. the relative speed of this motion is designated v z′ . depending on the directions of the spindle and feed motion of the tool in the the z′-axis corresponds to the angle of spindle revolution φ or time t given a constant spindle speed. the vertical axis in fig. 3a defines displacements in the x-axis direction of the lathe. the tool path is separated from the thread contour by the value of the nose radius. considering section a 1 a 2, the tool accelerates in the x-axis along arcs a 1 b 1 and b 2 a 2 . in section b 1 b 2, the tool moves along the x-axis with a constant speed. further tool positions during machining replicate or flip this basic movement relative to the x-axis. the relation between the vectors v z′ and v x is presented in the fig. 3b ."
"the detailed structure of whole network are shown in table 5 . the parameters of the network are set as shown in table 6 . the structure and parameters are set for better performance according to the multiple experiments for customers in wanjiang area. the \"rmsprop\" optimizer is chosen for its better performance in recurrent neural networks. the parameters can be adjusted for the different practical situations. in this paper, the number of epoch is set to 200 for the proposed method and can be adjusted for the compared methods. the training is stopped when the error decreases to a steady state."
"a website is a collection of related web pages. in this paper, static analysis is therefore, emphasized. it includes:-a) size of web pages. b) overall structure or site map of the website."
"4. since the errors found during testing have their root cause to coding, which depends on design and requirements analysis? it is therefore, very right to go back to design phase and find out the root cause of software failure."
"rotational movement of a workpiece can be idealized as a tool moving linearly along a thread contour because the analyzed rope thread is a single-start thread form, i.e., the pitch and lead are equal."
"the curves plotted in fig. 8b can be used to determine the cutting width a e for a given nose radius to obtain a required roughness class. fig. 4 the influence of f z on the tool movements in z′-axis. the following colors refer to the online version: blue-thread helix; orange-part machined in one spindle revolution; green-part that had not been machined equal to f z /p; p-thread pitch. a illustrative model. b presentation in xz′-plane note that in the case of high cutting width values, the thread profile may not be achieved ( fig. 9 ). therefore, it is not recommended to use high nose radius values for high rz."
"the schematic diagram of proposed framework based on gru neural networks for short-term load forecasting is shown in figure 5 . the individual customers are clustered into a few categories for more accurate forecasting. the samples are recorded from the categories where the customer to be predicted locates in. the load measurement data of individual customers in one day is extracted as a sample for short-term load forecasting, noted by p. the dimension of p is 96 with the 15 min sampling period. then, the samples are reshaped into two-dimension for the input of gru neural networks. considering the influencing factors date d, weather w and temperature t, date d p, weather w p and temperature t p on the forecasting day are added to the another input of the gru neural networks. considering the general factor of date, the prediction interval is set to seven days. therefore, the load measurement data p l on the day in the last week from the forecasting day, d p, w p and t p, are recorded as the overall input. the load measurement data p p on the forecasting day are recorded as the output, whose dimension is 96. therefore, the input x and output y of samples are given by equations (14) and (15) . the features from gru neural networks and fully connected neural network are merged with the concatenating mode and passes through batch normalization and dropout layer to avoid overfitting and increase the learning efficiency. the principle is that batch normalization can avoid the gradient vanishing of falling into the saturated zone, and that the better performance in fixed combination is avoided when random neurons do not work in a dropout layer. then, two-layer fully connected neural network are added before the output for learning and generalization ability. with training by back-propagation through time, the whole network implements the short-term load forecasting for individual customers. the structure can be extended if there is more information in the practical situation. the basic theory is also acceptable for medium-term load forecasting and long-term load forecasting, but different influence factors should be considered and the model should be changed with different input, output, and inner structure for good performance."
"according to figure 2, the new memory s t h is generated by the input x t i at the current time step and the hidden unit state s is not related to new memory, the reset gate can completely eliminate the information in the past. the update gate determines the degree of transfer from s"
both passive and active testing can be performed [cit] . passive means that tests do not disturb the natural operation of protocol. the records of the observed events are called as a trace. this trace will be compared to security properties derived from the standards. passive testing can be applied on a system in its real context (with real users) whereas active testing is performed in a simulated environment with simulated/ emulated system users.
"load forecasting is an essential part for energy management and distribution management in power grids. with the continuous development of the power grids and the increasing complexity of grid management, accurate load forecasting is a challenge [cit] . high-accuracy power load forecasting for customers can make the reasonable arrangements of power generation to maintain the safety and stability of power supply and reduce electricity costs so that the economic and social benefit is improved. moreover, forecasting at individual customer level can optimize power usage and help to balance the load and make detailed grid plans. load forecasting is the process of estimating the future load value at a certain time with historical related data, which can be divided into long-term load forecasting, medium-term load forecasting and short-term load forecasting according to the forecasting time interval. short-term load forecasting, which this paper focuses on, is the daily or weekly forecasting [cit] . it is used for the daily or weekly schedule including generator unit control, load allocation and hydropower dispatching. with the increasing penetration of renewable energies, short-term load forecasting is fundamental for the reliability and economy of power systems."
"to obtain the maximum efficiency, the time t 1 (min), described by eq. (18), should be as short as possible. given a constant length l (m) of a machined thread, time reduction is equivalent to maximizing the efficiency function defined as t −1 1, as shown in eq. (19)"
"since a website is defined in terms of objects (i.e. a link, a command button, a list box on a web page, a message, an image, a downloadable file, audio or another website itself). so the objective is to test these objects [cit] . practically speaking, an application with broken links, missed images and slow download will not attract customers."
"in order to satisfy users requirements and make web applications easy to use, usability testing is carried out [cit] . usability is embodied in such aspects as navigation, graphics and appearance. the navigation should be concise, clear and uniform. the graphics should have definite intentions. the correctness, precision and relativity of the graphical information also should be guaranteed. the main objective is to check whether the pages are compact, consistent and contrastive."
a contour of a male thread based on iso 10208:1991 is presented in fig. 1 . the standard gives different thread diameters measured in d. the dimension values are shown in table 1 .
web application security consortium (wasc) is another standard that classifies the vulnerabilities to a web application. there is some obvious overlap with owasp model. both owasp and wasc items can be interwined together to provide the best benefits to the client base.
"when the a e value increases, the thread profile may be increasingly flattened because of the material that is not machined. therefore, the profile accuracy can be added as an additional constraint."
"similar considerations as those for t 3 (eqs. (34), (38), and (39)) can be made for t 4 regarding the benefits of increasing the number of tool passes. the most efficient machining method, for the model which account the tool life and for the given parameters is presented in fig. 14b as a function n opt of a e ."
"in this section, the methodology is proposed for short-term load forecasting with multi-source data using gru neural networks. first, the basic model of gru neural networks are introduced [cit] . then, data description and processing are elaborated. the load data are clustered by k-means clustering algorithm so that the load samples with similar characteristics in a few categories are obtained. this helps improve the performance of load forecasting for individual customers. in the last subsection, the whole proposed model based on gru neural networks is shown in detail."
"another standard, osstm, is for unprivileged security testing of an application. it provides a all these standards do vulnerability analysis, we need to couple our findings with that of a finite set of criteria given by above standards, gathered overtime [cit] . without a trained eye and a deep system. it includes many other items like information, internet and communication. also there exists a common criterion that defines evaluation of security properties of it products and systems. here, applications are defined as 7-levels ranging from eal 1-7. common criteria 2.1 are now iso 15408."
"after substituting eqs. (11) and (12) into eq. (10), we obtain the average acceleration equation in the x-axis on the arc defined by radius r 1"
"during design phase, since a website design is available, so test websites for its usability i.e. its navigation and data flow between components, tiers or systems."
"1. trained samples are formed by clustering to reduce the interference of different characteristics of customers. 2. multi-source data including date, weather and temperature are quantified for input so that the networks obtain more information for load forecasting. 3. the gru units are introduced for more accurate and faster load forecasting of individual customers."
"locating a pin in a haystack means locating defects in both workable and non-workable code. this activity is so much difficult that in spite of the fact software tester's claim that the testing is complete yet the errors escape out. and if you will not locate and debug these errors then the customer will do it. testing involves some amount of cost, effort and time. the objective is to do testing using optimal resources."
"for an angle β 2 defined as tool angular position from point b 2 on an arc defined by radius r 2, the equation can be determined as"
"is almost completely passed to s t h . if s t u ≈ 0, s t h is passed to s t h . the structure shown in figure 2 results in a long memory in gru neural networks. the memory mechanism solves the vanishing gradient problem of original rnns. moreover, compared to lstm networks, gru neural networks merge the input gate and forget gate, and fuse the cell units and hidden units in lstm block. it maintains the performance with simpler architecture, less parameters and less convergence time [cit] . correspondingly, gru neural networks are trained by back-propagation through time as rnns [cit] ."
"to ensure proper threading, the accelerations a r1 and a r2 should not exceed the acceleration value a m specified for a given machine. it was assumed that machining is most efficient when the greater value from a r1, a r2 is equal to"
"the custom of electricity use and characteristics of load curve are different among the different categories of customers such as industrial customers, residential customers and institution customers. the different characteristics would affect the performance of forecasting. training forecasting networks with each customer separately would be a huge computation and storage problem. therefore, in the proposed method, the load curve samples are divided into certain categories using k-means clustering algorithm. samples with similar characteristics form a certain category, which form the input of gru neural networks for the corresponding customers. k-means clustering algorithm is a simple and available method for clustering through unsupervised learning with fast convergence and less parameters. the only parameter, k, number of clustering category, can be determined by elbow method with the turning point of loss function curve."
static analysis means testing system properties by code inspections without executing them. static techniques for source code analysis allow us to proactively eliminate or neutralize security bugs before they are developed or exploited [cit] . also if a program and its security properties are modeled then model checking techniques will help to identify whether there is any state that violates the desired security goal.
"before the short-term load forecasting using gru neural networks, the load curves of individual customers are clustered to different categories for samples with k-means clustering algorithm. the parameter k is selected as 3 by elbow method. there are 746 customers in the wanjiang area in dongguan city. the load measurement data should be processed with 0-1 standardization to the same scale for clustering to reduce the impact of different magnitudes and dimensions. the clustering is done for 10 times with load curves in 10 days for the individual customers. the clustering results are obtained with the average results in 10 days and the number of each clustering category is shown in table 4 . the standardized curves for 30 selected customers in three categories on a weekday are shown in figures 6-8 . as can be seen in figures 6-8, different customers have different characteristics of electricity use. according to figure 6, there are two electric peaks in a day. the evening peak is higher than the noon peak. the classic representation of this characteristic in figure 6 is residential customers. different from figure 6, figure 7 maintains the peak from 9 a.m. to late at night except noon. they are the general load curves of industry and business customers. in figure 8, there are two electric peaks in the morning and afternoon. it should belong to the government and institutional customers. even though a few customers have differences with the overall curve, this is the best clustering for them and it does not influence the overall performance greatly. with the clustering of individual customers, the networks can be trained with samples in the same category according to the customer to be predicted, so that the interference of electricity use characteristics can be reduced."
"in general, the proposed method uses the clustering algorithm, quantified multi-source information and gru neural network for short-term load forecasting, which past research did not consider comprehensively. the independent experiments in the paper verify the advantages of the proposed method. the rest of the paper is organized as follows. the methodology based on gru neural networks for short-term load forecasting is proposed in section 2. then, the results and discussion of the simulation experiments are described to prove the availability and superiority of the proposed method in section 3. finally, the conclusion is made in section 4."
"the paper presents a thorough analysis of the noncircular turning process on the basis of the smooth-contour iso 10208:1991 male rope thread. the optimization process results in the selection of the number of tool passes, which defines the threading method and the cutting parameter values. in the case of rope threading or hybrid threading, the increase in the tool nose radius value increases the required tool accelerations in the x-axis. relative to standard threading, the processing time is reduced for a smaller number of passes, but the cutting speed can be constrained by the machine tool acceleration capabilities. surface roughness is considered along with an indication of the issue related to geometrical errors associated with excessively high cutting width values. the impact of tool life modelling is explored. the diameter of the machined thread also has an influence on the obtained values. ultimately, algorithm-based selection of a threading method and cutting parameters is enabled. on the basis of the input parameter values and the model choice, the machining method can be selected along with the values of cutting parameters such as the number of passes, rotational spindle speed, tool nose radius, and feed per revolution in the z-axis. the corresponding analysis shows the complexity of the machining of the rope threads mainly focusing on tool kinematics as a function of cutting parameters and tool geometry. note that the analysis does not account for all phenomena that may influence the process; for instance, dynamic cutting forces are not taken into account."
"another issue related to the optimization of the turning process is the surface roughness, which depends primarily on the tool geometry and preset feed rate [cit] . to formulate the efficiency optimization issue for the rope threads, it is required to analyze the kinematics of the rope threading method, which is different from the classical turning. the analysis was made in the present paper. to optimize the machining process of the rope threads, the present paper describes the kinematic analysis of the rope threading process and presents the optimization taking into account the desired surface roughness, relationship between the nose radius and maximum tool acceleration of the tool motion, and (optionally) the chosen tool life model. the issue of possible shape errors was also discussed. furthermore, next to the mentioned standard threading and rope threading, the paper puts forward a new hybrid threading method, which is a compromise between many tool passes characteristic for standard threading vs. singlepass machining. obtained results and the algorithm of the machining efficiency optimization, for the given constraints and described tool life models, are presented. this paper is organized as follows. in section 2, the theoretical background of the issue is presented. performance optimization of rope threading without considering the tool life and with two tool life models is shown in section 3. in section 4, the machining methods, rope/standard/hybrid threading, are analyzed. in section 5, an algorithm for selecting cutting parameters and threading method is presented. the calculated efficiency results are shown in section 6. conclusions are given in section 7."
"sql injection occurs when a user enters inputs through forms to the background (server side) database in order to generate sql query statements. to find all injection points i.e., the attack points of the application, a complete scan of an application is to be done. to scan an application, the basic structure of the application must be extracted. it can be expressed as a tree whose root node is the front page of a website. the child nodes of a node represent its link points. a tree scan is performed with breadth-first search. then test cases are generated and run. test results are collected and test reports are generated."
the time in which the center of a circle in tool nose travels from point b 2 to point a 2 can be determined by dividing the travelled section along z′-axis by the speed constant v z′ given by eq. (7)
web applications are developed using spiral model. they have shorter delivery schedules than normal conventional it projects. web projects are developed often in the range of 1 to 3 months.
"interactions between dynamic pages can be carefully tested. however, it is impossible to test all interactions [cit] . a tradeoff exists between test coverage and test effort. so, at least pairwise interactions i.e. between two pages are the need. if a page, p1, could reach another page p2 then there must exist one test sequence in which both p1 and p2 are visited in the given order. so, we need to capture the navigation structure of the application under test (aut). hence, an abstract model of aut is desired. test sequences are generated from this model."
"the open web application security project [cit] (owasp) experts list standard top-ten threats to web security. this provides a common reference point for developers, consumers and vendors. it helps to build and test secures web applications."
"when considering the effect of the cutting speed and feed on the tool life, the optimum rotational spindle speed that does not exceed the permissible acceleration is given by 4 analysis of machining methods: rope/standard/hybrid threading"
"accelerations on arcs vary. the tool speed at each point is the resultant of the speeds v x and v z′ and is tangent to the tool path. therefore, the relations between speed vectors are defined by angles β 1 and β 2 which are described by β in eq. (1), according to fig. 3b . the speed in the x-axis can be given by"
"absolute accelerations on each arc are presented in fig. 5 in the time domain. due to jerk limitation in each feed drive system, the profile can never actually be perfectly copied due to sudden acceleration increases at the beginning and the end of each arc. it is therefore assumed that the acceleration value of each machine a m can be defined as the average acceleration on a given arc. the average accelerations on sections a 1 b 1 and b 2 a 2 are equal to the average acceleration along the whole length of arcs r 1 and r 2, respectively."
"all work which has been done in this field is about defining the description semantics of object constraints language (ocl) or the programming or modeling languages. for that, metamodeling became in the beginning of this decade has a widely useful tool to describe the (abstract) syntax of modeling languages."
"these two approaches have a good manner to evaluate ocl constraints in a formal and non-ambiguous method, but they still have some disadvantages, first disadvantage is this gap between ocl\"s official syntax definition which is given as metamodel, and the ocl\"s syntax which is given in structural induction. second one which is the main drawback is the understandability."
"due to chosen internal construction of convolutional layers and the final detection layer (in which yolo smartly combines it probability maps), it is unable to robustly detect objects that are small compared to the image dimensions. this happens quite frequently in our promotion board detection case, resulting in lower average precisions. once the object instances cover more than 20-30% of the image (e.g. the product box case), this is no longer an issue. the grid based region proposal system in yolov2 forces neighbouring detections together in a single detection box, dropping the average precision. the architecture has a non-maxima suppression parameter that can be tweaked for this, but initial experiments show this does not resolve all issues."
"in this paper we are focusing on behavior model transformation to push the wheel of behavior model transformation development, and to be aware about some aspects that we can contribute on to participate in these developing. these aspects are semantics of behavior models, completeness of platform independent model (pim), model composition, and supporting languages for mapping of behavior models."
"the dimension of time implicates special characteristics which cause specific data quality problems. thus, a catalog of data quality problems focusing specifically on time-induced problems yields benefits. in spite of its length, we do not claim our categorization of time-oriented data problems to be complete. however, we provide a collection of numerous problems from real life projects which constitutes an important basis for further research. moreover, we integrated this collection with existing taxonomies of general data quality problems to provide a categorized and unified work of reference. this reference comprises several important aspects that need to be considered when dealing with the quality of time-oriented data."
"our application of promotion board detection proves that the deep learned object detection pipeline is valid for industrially relevant cases, using only a very limited training set and thus minimal manual effort in labelling, yielding models that achieve moderate average precisions. at the same time we conclude that not all applications need a model with a 100% average precision, in order to do its job, like in the case of automated eye-tracker data analysis."
"several drawbacks have kept deep learning in the background for quite the while. until recently deep learning had a very high computational cost, due to the thousands of convolutions that had to process the input data from a pixel level to a more content-based level. on high-end systems, reporting processing speeds of several seconds on vga resolution has long been the state-of-the-art. this limited the use of these powerful deep learning architectures in industrial situations such as real-time applications and on platforms with limited resources. furthermore deep learning needed a dedicated and expensive general purpose graphical processing unit (gpgpu) and an enormous set of manually annotated training data, both things that are almost never available in industrially relevant applications. the available frameworks for deep learning lacked proper documentation and guidelines, while pre-built deep learning models were not easily adaptable to new and unseen classes."
"once the average loss-rate no longer drops, we halt the training process. an example of the loss rate tracking for one of our transfer-learned models can be seen in figure 3 . in general we notice that a loss rate just below 0.5 seems optimal for any model, which is transfer-learned from the given yolov2 pre-trained architecture. for both models we run the training overnight halting the training at 30.000 iterations. we evaluate the precision, recall and average precision performance of each detection model at 1.000, 5.000, 10.000, 15.000, 20.000, 25.000 and 30.000 training iterations."
"examples of generic packages or packages of untrained brands, yielding low probabilities compared to actual trained brands can be seen in figure 8 . knowing that actual trained classes trigger detections almost always above a probability of 85% ensures us that there is still a margin for defining the most optimal threshold."
a catalog of general data quality problems which integrates different taxonomies draws a comprehensive picture of problems that have to be considered when dealing with data quality in general. it serves as a reference when formulating integrity constraints or data quality checks.
"a raster interval is a unit of time that constitutes a raster: 'hour', 'day', 'week', or 30'. in exceptional cases raster intervals may also be of uneven length, such as for the temporal unit 'month'."
"before we move on to qualitative and quantitative evaluation of our object models in section 6, we first elaborate on how we built object detection models for our specific industrially relevant applications."
"an interval is a portion of time that can be represented by two points in time that denote the beginning and end of the interval. alternatively, intervals can be modeled as start time (i.e., a point in time) in combination with its duration (i.e., a given number of seconds, minutes, hours, etc.), or as duration in combination with end time [cit] . for instance, 08:00-09:00; 08:17-17:13; 8:17+50'."
"for this paper we created two datasets used for training and evaluating our deep learned object detection models. in both cases we provided manual annotations, allowing for a precision-recall based evaluation."
"if your industrial application concerns a class that is previously trained on any given public dataset (e.g. pedestrians, cars, bicyclists), these algorithms provide off-the-shelf pre-trained detection models. however, when your object class does not occur in any of the pre-trained networks, one needs a huge dataset and a lot of processing time to come up with a new deep learned model. that is why [cit] investigated the transferability of deep learning features and discovered that the first convolutional layers of any deep learning architecture actually stick to very general feature representations, and that the actual power resides in the linking of those features by finding the correct convolutional weights for each operator. this allowed applying the concept of transfer learning [cit] onto deep learning. by initializing the weights of the convolutional layers that provide a general feature description, using the weights of any pre-trained model, and by using a small number of annotated application-specific training samples to update the weights of all layers for the new object class, one can learn a complete new model. combined with data augmentation [cit], a small set of these training samples can introduce enough knowledge for fine-tuning an existing deep model onto this new class."
"many industrially relevant applications do not focus on existing classes from academically provided datasets. gathering tremendous amounts of training data and providing manual labelling, is a timeconsuming and thus expensive process. luckily transfer learning [cit] provides possibilities in this case. given a limited set of manually annotated training samples, it retrains existing off-the-shelf deep learning models and adapts the existing weights so that the model can detect a complete new object class. we apply transfer learning in two applications, while focussing on using a small set of training data for the new classes (to prove we do not need huge datasets for this task) and simultaneously ensuring real-time performance. the first industrial relevant application handles the detection of promotion boards in eye-tracking data, allowing companies to analyse shopping behaviour, as seen in figure 1(left) . the second application handles the detection and recognition of warehouse products for augmented advertisement purposes (as seen in figure 1(right) ). specifically we look at a generic product box detector, followed by a multi-class 14-brand-specific detector."
"for our general single-class product box detector, we perform transfer-learning for 5.000 iterations, while our fourteen-class brand detector is trained for 15.000 iterations. both trainings lead to a convergence in loss-rate, just below the 0.5 threshold, previously indicated as a good stopping point. figure 6 gives us an overview of the generated precision-recall curves for all the transfer-learned models. at the bottom left part of the figure we observe the performance on the complete validation dataset (so all fourteen brands together). our general single-class product box detector is able to robustly detect every single object instance in the large validation set, obtaining an average precision of 100%. the 14-class object detection model is as promising, obtaining an overall average precision of 99.87%, directly providing the correct brand label in combination with an accurate localisation."
"in the course of investigating data quality problems from real-life projects, we realized that the kinds of problems that are subject of this paper (i.e., wrong, duplicated, missing, inconsistent data, etc.) are not the only ones that need to be identified and resolved. tasks, like checking the credibility of data entries that cannot easily be categorized as 'wrong', or transforming the data table into a specific format that is suitable for further processing steps are strongly linked to the process of data cleansing and need special consideration. also, a relevant number of problems occur as a consequence of cleansing/transforming the data set, thus such dirtiness might be created by the process itself."
"moreover, raster intervals may have attributes attached such as 'weekday', 'holiday', 'opening hour', 'working hour', 'school day', or 'christmas season'. consequently, there are attributes that can be calculated (e.g., the attribute 'weekday') and attributes that require further information (e.g., the attribute 'holiday')."
"we introduce some terms on different types of time-oriented data in section 3.1 before we continue with a detailed description of our main contribution-the categorization of dirty time-oriented data in section 3.2. in section 4 we provide a short outlook on further work we have planned to carry out in this area, and we sum up the main results of our work in section 5."
"keeping these changes in mind, we transfer learn a new object model for our new object class. we allow all weights to update (including the pre-assigned weights) and thus to converge towards the optimal solution given our new training data. we also use data augmentation to ensure we enrich the small application specific annotated training set."
qvt (query/views/transformation) is the omg standard language for specifying model transformations in the context of mda. it is regarded as one of the most important standards since model transformations are proposed as major operations for manipulating models. [cit] . the three concepts that are used in the name of the qvt language as defined by omg documents are: [cit] .
"qvt languages are arranged in a layered architecture shown in figure 4 . the languages relations and core are declarative languages at two different levels of abstraction. the specification document defines their concrete textual syntax and abstract syntax. in addition, relations language has a graphical syntax. operational mappings is an imperative language that extends relations and core languages. relations language provides capabilities for specifying transformations as a set of relations among models. core language is a declarative language that is simpler than the relations language. one purpose of the core language is to provide the basis for specifying the semantics of the relations language. the semantics of the relations language is given as a transformation relationstocore. this transformation may be written in the relations language. sometimes it is difficult to provide a complete declarative solution to a given transformation problem. to address this issue the qvt proposes two mechanisms for extending the declarative languages relations and core: a third language called operational mappings and a mechanism for invoking transformation functionality implemented in an arbitrary language (black box implementation)."
"a raster can be defined as a fragmentation of time without gaps consisting of raster intervals (usually with same lengths). for example, a 30' raster interval that is typically aligned with coarser time units: 00:00-00:30; 00:30-01:00; ..."
"our second application is the robust detection and classification of warehouse product packages for augmented advertisements. in this application it is first of all our task to robustly locate the packages in any given input image. furthermore we want to classify each product box in the same, single run of our deep learning classifier, and return its brand."
"for our promotion boards, we look into deep learning with two specific reasons: 1. we want to analyse complete eye-tracker experiments by linking object detections to eye-tracker data, removing the need for manual analysis. 2. we want to reduce the amount of annotations as much as possible without losing too much average precision on the obtained object detectors. we build two separate object detection models for this application. first we build a single-class detection model, able to detect the small red promotion board as seen in figure 2 (left). however, since the same shop also contains a second board of interest, seen as the large red promotion board in figure 2(right), we explore the possibilities of building a two-class detector, able to detect and classify both signs at the same time and in a single run."
"latter being defined by either two points in time (i.e., start and end of the interval), by its start (i.e., one point in time) and its duration, or its end and its duration (as defined in sec. 3.1). besides the temporal units, we especially consider time-dependent values (e.g., all events at a given point in time, all events within a given interval). with respect to these categories we outline which data quality problems arise for which data type (indicated by bullets in tab. 2-4). the first two columns of the tables reflect the general categories derived from existing taxonomies. the third column gives descriptions and examples of specific time-dependent quality problems for each category."
"finally the application of product package detection and classification tries to push the limits of deep learning and model fine-tuning on industrial cases. by using again very limited datasets (keep in mind some classes have only 15 labelled image samples) we achieve a perfect solution for our application, achieving a 100% average precision when it comes to generic box detection. furthermore if we directly incorporate the classification in the detection pipeline, we achieve a remarkable 99.87% average precision. this perfectly proves that deep object detection could be a valid solution to several industrial challenges that are still around nowadays."
"other interesting studies on data quality include [cit] who present a list of themes and keywords derived from papers of the last 20 years of data quality research, madnick and wang [cit] who give an overview of different topics and methods of quality research projects, and neely and cook [cit] who combine principles of product and service quality with key elements (i.e., management responsibilities, operation and assurance cost, research and development, production, distribution, personnel management, and legal function) of data quality across the life cycle of the data. however, none of these approaches systematically builds a taxonomy of data quality problems."
"-means to specify automatic transformations, and -information visualization [cit] methods for interactive manipulation of the whole dataset as well as of selected entries. 5. supports the management of various versions and corrections/partial updates of the dataset."
"the remainder of this paper is organized as follows. in section 2 we discuss related work and highlight the state-of-the-art in deep object detection. section 3 discusses the collected dataset for training and validating the object detection models. this is followed by section 4 discussing the selected deep learning framework and model architecture, used for transfer learning. section 5 handles each application case in detail. in section 6 we discuss the obtained average precisions and execution speeds. finally we conclude this paper in section 7 and provide some future work in section 7.4."
"in this section we want to discuss some restrictions of the current yolov2 framework, shed a light on some issues that still arise when using our models and finally draw some conclusions on our suggested approach. finally we suggest ways for further improving the current pipeline."
"finally every augmented training sample is randomly flipped around the vertical axis in the darknet data augmentation pipeline. while in most cases this helps to build robustness inside the detector, there are cases where this can worsens the actual model. we therefore suggest training models without these random flips, and comparing them to our already obtained results, to investigate their actual influence."
"for instance, rastered time-oriented data may have distinct errors. on the one hand, the raster itself may be violated (e.g., a data set rastered on an hourly basis which contains an interval of minutes). on the other hand, the attributes of rastered intervals may indicate incorrect data values (e.g., sales values outside opening hours), or the values within the rastered intervals may violate some constraint such as 'each rastered interval must contain a value greater than 0 for a given data attribute'. in addition, a further type of data has to be considered when dealing with quality problems of time-oriented data, namely time-dependent values such as 'sales per day'."
"from a methodological perspective, we applied an iterative mixed-initiative approach combining a bottom-up grounded theory procedure [cit] with a top-down theory-centric view. on the one hand, our work gathered, modeled, and coded iteratively a number of time-oriented data quality problems that appeared in our real-life data analysis projects. these projects led to a large collection of examples of time-oriented data quality problems in diverse industry sectors and diverse kinds of data. on the other hand, we analyzed, compared, and merged the existing taxonomies discussed above that aim to model dirty data aspects (see sec. 2 and tab. 2-4). in the course of integrating the time-oriented data quality problems with the categorizations of general data quality problems, we re-arranged, refined, extended, and omitted some categories according to our needs and practical experiences. we kept the concept of categorizing data quality problems into problems that occur when the data set stems from a single source and those that occur when two or more data sets need to be merged from multiple sources. single source problems may of course occur in multiple source data sets too but the provided list of multiple source problems focuses on problems that specifically emerge when dealing with data sets from multiple sources (as mentioned by rahm and do [cit] ). moreover, we excluded some categories of quality problems which do not relate to any time-oriented aspect such as 'inconsistent spatial data'."
"as framework we decide to use darknet [cit], a lightweight deep learning library, based on c and cuda, which reports state-of-the-art performance for object detection using the yolov2 architecture. moreover, it reaches inference speeds up to 120 fps and integrates well with our existing c++ based software platform (using opencv3.2 [cit] ). the framework does not require explicit negative training samples but rather uses areas in the image that are not labelled as object. we build our customized version of the framework, which is publicly available 1 and allows integrated evaluation of trained models using the precision-recall metric."
"the packages all have general product box properties, while the print on the box distinguishes the brand. therefore we start building a robust product package detector, able of localizing packages with a very high average precision in newly provided input images. this can easily be followed by a small classification network deciding which brand we have. however, doing this in a single run per image is our ultimate goal once we succeed in building a robust product pack detector, we try pushing our luck, so we generate a 14-class brand detector, which is able to both localize and classify the product packs in a single run, completely removing the need of a separate classification pipeline."
"the small difference in average precision between both models is caused by brandc v1, which is unable to reach a class-specific average precision of 100%. we discovered four samples of the brandc v1 with a wrong label, which under data augmentation seems to have a significant influence. before adjusting these labels, we obtained a 93.97% average precision for this class, while afterwards we obtain a 95.66% average precision."
"another important issue of data cleansing is the transformation of the given data table into a table structure that is suited for subsequent processing steps, such as splitting/merging of columns, removing additional rows (e.g., summary rows and comments), or the aggregation of temporal tuples into rastered intervals. a couple of software tools exist to aid this transformation [13, [cit] . however, further research is needed on which kinds of transformations should be supported and how to support them most efficiently as well as how to organize the management of the various versions and updates."
"dirty data leads to wrong results and misleading statistics [cit] . this is why data cleansing -also called data cleaning, data scrubbing, or data wrangling -is a prerequisite of any data processing task. roughly speaking, data cleansing is the process of detecting and correcting dirty data (e.g., duplicate data, missing data, inconsistent data, and simply erroneous data including data that do not violate any constraints but still are wrong or unusable) [cit] . dirty data include errors and inconsistencies in individual data sources as well as errors and inconsistencies when integrating multiple sources. data quality problems may stem from different sources such as federated database systems, web-based information systems, or simply from erroneous data entry [cit] ."
"kermeta is a meta-language for specifying the structure and behavior of models; it has been also developed as a core language for model driven engineering (mde) platform. kermeta is an executable metamodelling language implemented on top of the eclipse modeling framework (emf) within the eclipse development environment. figure 5 show three main windows in kermeta graphical interface, which the first one is the metamodel using class diagram(which is a subset from uml class diagram mof metamodel), the second widows is the kermeta code to describe the class diagram, and the last one is the summarization for the class diagram."
"we reckon our models seem to be underperforming, seeing their optimal configuration strands at about 65% precision for a 50% recall. this can be due to motion blur in the eye-tracker data in the validation set. the scales of the annotated validation data also differ from the training data, leading to unlearned object dimensions. finally yolov2 has problems with detecting objects that are relatively small in comparison to the image dimensions, while this is not an issue for a human annotator."
"if we trace the development of mda approaches, we will find that most researchers had gave their attention on structural aspects of platform specific model (psm) level and in generating code, but they had gave less attention to the platform independent model (pim) level and the behavior of the modeled applications. we did not find a lot of work about this aspect, but we only aware of one paper, which present an mda based that incorporates behaviour modelling at the pim level in the development of a specific category of applications [cit] . in this paper they mentioned that behavior pim can be divided to more than one layer of abstraction, the first one is more independent than the other layers, while the deep one can be near more to psm."
"in this paper we have investigated different approaches of categorizing data quality problems. we have examined a number of relevant taxonomies of dirty data and carved out their similarities and differences. furthermore, we have focused on the data quality problems that occur when dealing with time-oriented data, in particular. we have derived a number of time-oriented data quality problems from our experience in numerous projects in different industry sectors and we merged the results of the literature review of existing taxonomies with our practical knowledge in dealing with time-oriented data."
"kermeta is a language for specifying metamodels, models,and model transformations that are compliant to the meta object facility (mof) standard [cit] . the objectoriented meta-language mof supports the definition of metamodels in terms of object-oriented structures (packages, classes, properties, and operations). it also provides modelspecific constructions, such as containments and associations between classes [cit] ."
"before we actually present the taxonomy of quality problems, we introduce some terms on different types of time-oriented data. the categorization originates from the observation that checking the data for given problems turns out to be different for these distinct types of time-oriented data."
"figure 6 [cit] shows the base model slice which is composed of a set of base models. similarly, an aspect model slice is composed of a set of aspect models. base models are written in standard uml. aspect models are written in the mata language and are defined as increments of the base models or other aspect models. each aspect model describes the set of model elements affected by the aspect (i.e. the joinpoints) and how the base model elements are affected."
we proved that using deep learning for industrial object detection applications is no longer infeasible due to computational demands and unavailable hardware. by using an off-the-shelf deep learning framework and by using transfer learning we succeed in building several robust object detection models on an affordable general purpose gpu.
"we can only guess towards the actual reasons, but we are convinced that one of the issues could well be the sampling of the actual eye tracker video material, which contains huge amounts of motion blur, especially when people tend to move their head fast, for a quick look down the isle. better eye-tracker hardware could be a possible solution here, or replacing the validation data with only clear eye-tracker data, removing the actual motion blurred evaluation frames. we also reckon our model will never be able to detect motion blurred advertisement boards, since we never used them as actual training data. adding them to the actual training data might also improve the generalization capabilities of the deep learned model."
first of all we would like to do a deeper study of our promotion board detection case and how we can improve the moderate average precision rates obtained.
"the first dataset exists of several videos of an eyetracker experiment in a belgian warehouse. data is collected by costumers, approached at the entrance and asked to go shopping wearing an eye-tracker, without explaining them the goal of the experiment. two classes of promotion boards (small sign and large sign) are manually annotated in each frame of the eye-tracker videos. as training data we use respectively 75 and 65 frames containing the promotion boards. as validation data, we use 420 and 960 frames respectively, containing both frames with and without the actual promotion boards. the second industrial relevant dataset contains 376 images of product packages. in each image the location of the package is manually annotated using a rectangular bounding box. in total 14 brands are included, allowing us to both provide labels of a product box and the associated brand, useful for training multi-class object detection models. a separate validation set is collected, existing of twentysix fifteensecond videos (@30fps), containing different views of similar product boxes. table 1 contains a detailed overview of the specific amounts of training and validation samples per brand."
3. we initialize the weights of all convolutional layers except the last one and leave the weights of the final convolutional layer and the detection layer uninitialized. this allows us to learn class specific weights for these deciding layers.
"this section discusses in more detail the achieved results with each trained object detector. furthermore it highlights some of the issues we have when training these object detection models and discusses some of the limits of using the yolov2 architecture. since all our models are fine-tuned from the same yolov2 architecture, we have an equal storage size of 270mb, while the gpu memory footprint of the same model equals 450mb."
"initially we see an increase in average precision when raising the number of iterations for each model. however, once the average precision (calculated on the validation set) starts dropping, we select the previous model as best fit, in order to keep the model that generalizes best on the given validation set. for the single-class detector this means a model of 15.000 iterations at an average precision of 59.98% while for the two-class detector the 30.000 iterations model at an average precision of 59% performs best. looking at these curves it seems that the two-class model might still be converging to an optimal solution and thus continuing training with more iterations might be a feasible option."
"specifically, we presented an integrated and consistent view of general data quality problems and taxonomies. thus, we provided a useful catalog of data quality problems that need to be considered in general data cleansing tasks. in particular, we provide categorized information about quality problems of timeoriented data. thus, we established an information basis necessary for further research on the field of dirty time-oriented data, and for the formulation of essential quality checks when preprocessing time-oriented data."
"there is a lot of transformation languages working as a tool to make the transformation operation full automated, we have chosen a three types of these languages depend on some criterion. first one is query, view, transformation (abbreviated by qvt) which is most standardized, which is sponsored by object management group (omg). the second one is kermeta (abbreviation of kernel metamodel), it is domain specific language, it is building basically on object oriented programming, and it can be plugged on eclipse. the third one is mata (abbreviation of modeling aspects using a transformation approach), from its\" name we can see that it is building on aspect oriented programming. now we need to take each language individually, and focusing the light on some its\" features, and making technical comparison."
"since convolutional neural networks process image patches, doing a multi-scale and sliding-window based analysis takes a lot of time, especially if image dimensions increase. combined with the fact that deeper networks achieve a better detection accuracy, deep networks like densenet201 and inceptionv3 can easily take a couple of seconds for vga resolution. to increase execution speed, academia introduced the concept of region proposal techniques. these are fast lightweight filters that pre-process images, looking for regions that might contain objects we are looking for. instead of supplying the classification pipeline with millions of candidate windows, the region proposal algorithms reduce this to several thousands of windows, clearly reducing processing time."
"in order to converge to an optimal configuration, deep learned models need large amounts of annotated train-1 https://gitlab.com/eavise/darknet ing data and a lot of processing time. for the suggested yolov2 architecture, this is also the case. the default network configuration uses 800.000 floating point operations per given image. to optimize the weights assigned to each operation, one literally needs multiple thousands of training samples. for industrial applications this is not manageable, certainly if for each new object detection model, you need new and annotated training data. transfer learning bridges the gap here. for our detection models we start from a yolov2 [cit] dataset with pre-trained weights. to be able to apply transfer learning onto a new object class several adaptations have to be made to the network."
"by adding a class that contains all non-standard boxes we basically want to force the deep learning network to learn all product brands, and on top of that an extra generic box detector. we add extra t. raining data, collected from 12 household rectangular boxes and started training again. figure 7 shows that the initial iterations show a promising drop in loss rate, however, at a certain moment we literally experience a training loss rate explosion, and the model is not able to recover from this. to be able to recover from this, a possibility might be to lower the learning rate from the moment the loss-rate increases again, since the giant loss-rates clearly show the architecture is on the wrong track looking for the optimal solution, given the multi-dimensional error surface."
"in preparing our taxonomy of dirty time-oriented data and data quality problems we start with a review of some general taxonomies. more specifically, we look at the general partitions used in this research (e.g., single-source problems versus multi-source problems), but are especially interested in the 'leafs of the taxonomies', i.e. those types of possible errors that are specific enough to be covered by a specific test (e.g., duplicates, missing values, contradictory values). the leafs mentioned in those general taxonomies are summarized in an overview table (see tab. 1)."
"mata takes a different approach to aspect-oriented modeling (aom) since there are no explicit join points. rather, any model element can be a join point, and composition is a special case of model transformation. the graph transformation execution engine, agg, is used in mata to execute model compositions, and critical pair analysis is used to automatically detect structural interactions between different aspect models. mata has been applied to a number of realistic case studies and is supported by a tool built on top of ibm rational software modeler."
"to start with, we give an outline and summarization of taxonomies of general data quality problems in section 2. in section 3 we take a closer look at the different types time-oriented data that demand special consideration."
"we need a way of defining when our training algorithm reaches an optimal solution. at the same time we need to avoid overfitting the model to the training data, keeping a model that generalizes well to never seen before data. in order to check the training progress we continuously monitor the loss-rate on the given training data. this is monitored for every new batch of samples provided to the training algorithm, together with the average loss-rate over all batches."
"2. we adapt the amount of anchor ratios to our class specific problem. this ensures that in the detection layer, prediction boxes are generated fitted in a ratio that agrees to our actual fine-tuning data from our new object class."
"the majority of types of dirty data require intervention by a domain expert to be cleansed [cit] . thus, a combination of means for transforming the whole dataset at once with means for interactively investigating the data problems and manipulate single table entries or groups of table entries seems to be a promising solution. since the sight is the sense with the highest bandwidth we believe that visualization is a good way to communicate a compact overview of the 'dirtiness' of the data as well as to point the user to those cases of data quality problems where manual interaction is needed. moreover, we plan to realize an interactive information visualization [cit] prototype that allows for direct manipulation of the data set. this would not only facilitate the task of cleaning the data but it would also provide immediate visual feedback to user actions."
"for mda approach there is still no agreement on how behavior aspects should be supported. there are a lot of effort has been done on model mapping and transformation from pims to psms in many application domains. much works which have been done using mda approach give attention on behavior aspects in psms. in this paper, we will provide a good monitoring for behavior model mapping either using vertical mapping (refinement) or horizontal mapping (from pim to psm)."
"finally deep learning separates training data based on a combination of convolution filters, trying to find an optimal solutions. however we do not tell it how it should do its task or which features to ignore, which sometimes has undesired effects. e.g. in the case of our generic product box detector, when presented with a similar rectangular shape (e.g. a box of cereals or sugar) the model also triggers a detection. this behaviour is expected since a product package is first of all a rectangular box."
"the generated taxonomy serves as important basis for further planned initiatives to support time-oriented data quality issues. specifically, we plan to develop a prototype that 1. checks time-oriented data for these kinds of quality problems, 2. generates a report about the problems found, 3. visualizes the 'dirtiness' of the data set and its progress, 4. provides tools for data cleansing:"
"caution should be used here. this only works when using a limited set of hard negatives samples in relation to the amount of training samples per class. adding too much hard negative samples increases the risk of generating training sample batches that contain almost no actual object annotations, which forces to learn the model to detect nothing at all."
"model composition is a technique, which used with behaviour models for building bigger models from smaller models, thus allowing system designers to control the complexity of a model-driven design process. but many these model composition techniques are themselves very complex because they compose the internal member of participating models in non-simple manner."
"these issues no longer exist, and deep learning made a major shift towards usability and real-time performance. with the rise of affordable hardware, large public datasets (e.g. imagenet [cit] ), pre-built models (e.g. caffe model zoo [cit] ) and techniques like transfer learning, deep learning took a step closer towards actual indus-trial applications. together with the explosion of stable and well documented open-source deep learning frameworks (e.g. caffe [cit], tensorflow [cit], darknet [cit] ), deep learning opened up a world of possibilities."
"(a) start/end of non-rastered intervals (non-rastered points in time) (b) duration of non-rastered intervals 3. rastered points in time 4. rastered intervals (i.e., start+end, start+duration, or duration+end):"
"a second attempt is made by adding hard negative training images, images containing generic unannotated box shapes, without changing the number of classes to learn. the architecture will force the learning process to try and make the response of those hard negative images zero and thus hopefully improve the ignoring power of the model for generic packages. this approach works and yields a transfer-learned model with a low loss-rate. the hard negative data is not removing the generic box detection properties from the model, which it still needs for detecting the actual brands, but when looking at the probabilities for the generic box detections, we clearly notice a larger gap in probability between object class instances and generic box detections."
"to remove the effect of other rectangular packages triggering the product package detector, we investigate the detection probability range. given a set of product packs and some random packs with other aspect ratios, we noticed that there is a large probability gap between known and unknown packages. simply placing a probability threshold of 65% already results in the removal of all detections of non-known product packages and brands. however we are interested in other possible solutions to this problem, that do not include setting a hard threshold on the probability output, since unseen data could screw up this approach and trigger detection probabilities for desired objects that are lower than the given threshold."
"note that an aspect model can only be defined as an increment of a model of the same type; for example, sequence diagram aspects can extend base sequence diagrams but not base state diagrams."
"a query is an expression that is evaluated over a model. the result of a query is one or more instances of types defined in the source model, or defined by the query language. view: a view is a model which is completely derived from another model (the base model). there is a \"live\" connection between the view and the base model. transformation: a model transformation is a process of automatic generation of a target model from a source model, according to a transformation definition."
"in our experience the detection model generally fine-tunes over multiple thousands of iterations, providing a batch of 64 samples at each iteration to the model for updating the weights and reducing the loss rate on those given samples. if our model trains for 10.000 iterations, the model needs more than 640.000 training samples. given we only have around 400 annotated training samples available this would mean that each sample is shown over a thousand times to the network, risking to overfit the model to the training data. data augmentation applies specific transformations to the image to generate a large set of data from a small set of training samples, effectively reducing the number of times a single sample is shown to the network and thus avoiding overfitting."
"when dealing with the detection of errors in time-oriented data there are special aspects to be considered. time and time-oriented data have distinct characteristics that make it worthwhile to treat it as a separate data type [cit] . examples for such characteristics are: time-oriented data can be given either for a time point or a time interval. while intervals can easily be modeled by two time points, they add complexity if the relationship of such intervals are considered. for example, allen [cit] describes 13 different qualitative time-oriented relationships of intervals. also, intervals of validity may be relevant for domain experts but might not be explicitly specified in the data. when dealing with time, we commonly interpret it with a calendar and its time units are essential for reasoning about time. however, these calendars have complex structures. for instance, in the gregorian calendar the duration of a month varies between 28 and 31 days and weeks do not align with months and years. furthermore, available data may be measured at different levels of temporal precision. given this complex structure of time, additional errors are possible and correspondingly a specific taxonomy is helpful in addressing these issues."
"the central idea of the omg\"s model driven architecture (mda) is that developer should be used to develop models, not programs. that is not to privilege a graphical over a textual programming, but rather to make the developer to be enabled to work at as a high level of abstraction as is feasible. the general scenario of mda is a single platform independent model (pim) might be created and transformed, automatically, into various platform specific models (psms) by the systematic application of understanding concerning how applications are best implemented on each specific platform. the omg\"s queries, views and transformations (qvt) standard [cit] defines languages in which such transformations can be written [cit] . depending on four aspects, we can trace the progress in development of behavior models mapping and transformation. first aspect is about the semantics of behavioral (operational) models, second aspect is the completeness of behavior platform independent model (pim), beside the languages that are working on the field of behavior model mapping and transformation as a third aspect, and the last aspect which is about the new trends for model composition."
"the constructor must specify the name of the driver to be referred and the producing event type. an initial function, init, is called at starting state. a getvalues will repeatedly run until interrupted in the separate thread. an execute function will be activated when producing the events by inputting an array of memorized values and their corresponding type. a getdefaulttype function is to refer the output type."
"in the theoretical scenario, we assume six types of nodes to produce the data flow as shown in table 2, where (t x ) is the benchmark time to complete task t x for one input. the fixed and blank nodes will be arranged in square grid topology while the mobile nodes will be moving around all covering areas by random-waypoint mobility model with the random speed from 2 to 20 m/s as shown in fig 17 . the benchmark ( ) is computed as shown in the equation (1) . in this simulation, we set cpi and clock rate as 2.5 and 500 mhz, respectively."
"where i dc and i ac are the amplitude of dc-bus current and ac output current; ϕ 1 is the power factor angle, and ω is the angular frequency. assuming a lossless mmc system, the relationship between i dc and i ac can be derived [cit] as"
"toward the world of smart things, superabundant flows of information require processing to discover the hidden meaning behind. [cit] s, cloud computing comes a long way as a promising solution to handle the exponentially rapid growth of information. pushing away the heavy computation, such as video stream processing, to the cloud server has become common sense nowadays. meanwhile, the bottleneck problems due to the limitations of network links and security issues of cloud services significantly cause a high concern."
"the modular multilevel converter is an emerging and attractive voltage-source converter (vsc) topology for highvoltage direct current (hvdc) transmission systems due to its modularity, scalability to different voltage levels, high output quality and no high voltage dc-link capacitor [cit] ."
"thirdly, we conduct multiple levels of experiments to study and evaluate the proposed method. the most basic layer is a unit test on the computation module. we test on intel edison, a tiny computer module designed for iot devices. in the aspect of distribution logic, we perform a preliminary logical experiment in the theoretical scenario and a large-scale simulation in the nursing-home use case comparing with centralized and naïve distributing approaches. we also implemented the edgecep prototype and deployed it into intel edison for a smart room application enabling the advanced batman protocol [cit] for reliable ad-hoc network connection."
"in the normal operation of the mmc, the capacitor voltages among sms are balanced in the steady-state, which can be achieved by various voltage balancing control methods [cit] . therefore, it is reasonable to assume that the increase and decrease of the sm's capacitor voltage (charged by d 2 and discharged by t 1 ) are equal during one fundamental period, and the relationship can be expressed as"
"to affirm our broker design, we develop two simple engines, one with our proposed relation-based processing component and the other with the jess rule-engine [cit] . both engines are installed on the same intel edison board and run a simple aggregation task finding an average of temperature data within last x seconds defined as following: as expected, with a variation of iteration numbers from different range x, the result in fig. 11 shows that a pure rulebased approach needs extremely-expensive computation cost for a high number of iterations. meanwhile, the cost of the combination approach remains low even if the number of iterations is high. at 1500 iteration numbers, an average computation time of rule-engine approach is 16.9 times higher than the relation-based approach."
"we compare our proposed decision algorithm, flow optimization, (fo) with another two greedy algorithms for a node to choose the offloading node when it reaches the computation capability limit. the first method is to choose the broker with highest computation power left (hc). the second method is to choose the random broker node (rn)."
where ∆u +/− is the voltage increase/decrease during one fundamental period; c i is the capacitance of the i th sm. the relationship in (6) can be further simplified as
"the organization of the paper is as follows. the first section gives the background of complex event processing (cep). the second section focuses on the task distribution on cep. summary of the main contributions is also here. the third section describes the proposed module-based event stream processing engine (edgecep). the fourth section is about the proposed task-distribution framework, problem definition, and solution. the last section reports and concludes the experimental results."
"a. task distribution problem definition as defined above, task represents composite definition and subscription. subscription tasks are always active while definition tasks will be active only when any subscription tasks refer to them. a distribution plan is composed of (i) the assigning matrix, designating which processing task is going to be active on which broker node, (ii) the offloading matrix, showing paths from the stream-source broker to the corresponding assigned broker, and (iii) the notifying matrix, showing paths from the processor to the destination actuator. in this part, we formally give a problem definition to minimize resource utilization on edge networks concerning the total flow volume from sources to destinations of all active tasks under resource and consistency constraints."
"with different producing rates of the source nodes, including stair, in-building, mobile and monitor, total flow volumes per time slot are summarized in table 1 . the results confirm that available knowledge, reflected by the radius of the vicinity group, affect the distribution decisions as well as the total flow volume."
"this paper has introduced a self-organized task distribution framework for module-based event stream processing (edge-cep). the edgecep is a general complex event processing engine that combines the advantage of an event-dependent specification, along with efficient tuple-based processing by pseudo-source mechanism employing publish-subscribe and content matching techniques. we newly define a supportive event-specification language enabling relational operations. we introduce an optimization problem of the task distribution plan. the proposed framework applies tabu search with a flow-based greedy move to find the sub-optimal solution. the solution computation is periodically executed independently at each node and shared with other necessary information for others to compute the plan. we observe and evaluate the proposed system in many levels and experiments including large-scale simulation and real-environmental deployment. the simulation in the theoretical scenario shows that the proposed flow optimization outperform the trivial algorithms. the average delivering time is almost 1 and 0.5 seconds faster than choosing the highest computation power left and a random broker, respectively, in dense networks. the proposed method can process almost 2-times results more than the others at the end of the simulation. furthermore, it can reduce total packets 6.6 times from the centralized approach in the practical nursing home scenario. we successfully deployed a prototype engine over an ad-hoc wireless sensor network composed of intel edison modules in the real environment. the running result presents decreasing communication cost in general."
"to address above problems, some research efforts have been made. an explanation about the loss unbalance for both conduction loss and the switching loss is detailed, and a two-dimension sorting method is proposed for a balanced junction temperature behavior [cit] . experiment validations and reliability assessment are further conducted on a down-scale"
"we assume five tasks to process the input from each fixed and static nodes. the reduced ratio and instruction count depend on the data size that they produce as shown in table 3 . to control the total flow affected by each task, we randomly assign a type to each node by considering the evenly total flow generated. thus, the minimum numbers of each type are 50, 25, 1, 25, and 1, respectively, to generate equal 500 kb/s. destinations for the tasks are random for each running. we set the window time of assignment recalculation and simulation time as 5 and 30 minutes, respectively."
"gird dc power fig. 1 . circuit configuration of a typical three-phase mmc. (i dc is the dc-bus current, iac is the ac output current, and i dif f is the differential current.) bench [cit] . various active thermal balancing methods based on the circulating current are explored for component-level power loss balancing, but the effectiveness is limited [cit] focuses on the submodule-level power loss balancing integrated with the capacitor voltage balancing, but the loss model for mmc with a large number of sms is computationally burdened."
"a typical circuit configuration of three-phase mmcs is presented in fig.1 . the mmc is composed of three phases, which can be divided into the upper arm and the lower arm. each arm includes n series-connected sms, and an arm inductor to restrain the circulating current within the phase leg. half-bridge sm with four semiconductors is adopted in this paper. generally, the arm current can be divided into two parts, namely the common component i comm and the differential component i dif f [cit] . a dc bias is an essential part of the differential component for the active power transfer, and to maintain the sm's voltage at the rated value. in addition, harmonic currents can be injected into the differential current to achieve certain objectives as well [cit] . if the dc bias differential current is only considered here neglecting other harmonics, the upper arm current i up of phase a can be expressed as"
"switching profile system parameters in the simulation is conducted based on the prototype in fig. 7 . fig. 8 shows the circuit scheme where two full bridge converters are used. one is used as the device for control (dfc) regulating the inductor current to track the current profile from simulation. another one is divided into the dut and the auxiliary half bridge. the dut is controlled by the switching profile. besides, two capacitors (0.22 mf and 0.26 mf) and two modulation methods (psc and nlm) are used in this paper for validation. other parameters are listed in table i . fig. 9 shows the experiment waveforms of the arm current, the on-state voltages of both igbt and diode, and the gate signals under the condition of nlm. it can be seen that the arm current is well regulated, and the on-state voltages of both igbt and diode are sampled in two fundamental periods. by using the waveform data exported from the oscilloscope, the accumulated conduction loss of the four semiconductors can be calculated in matlab. the experimental results are compared with the simulation as shown in fig. 10 . the simulated total conduction losses of one sm in one fundamental period are 0.2332 j, 0.2333 j and 0.2316 j for psc, nlm1 and nlm2 respectively with the variation of 0.7%. the average accumulated conduction losses from the experiment are around 0.2464 j with the variation as low as 0.3%. in addition, the conduction loss calculated by the proposed method is 0.2296 j with the error of 1.4% and 6.8% compared with the simulation and experiment results respectively."
"the proposed framework is composed of multiple broker nodes that are collaborating on processing via self-configured and self-organized ad-hoc networks. the global knowledge of user, i.e. subscriptions and event definitions stored in xml format (see example in fig. 7), are supposed to be synchronized. each broker will periodically update its decision and broadcast the info message for every specific window time. the info message contains the information that is necessary for others to calculate the distribution plan, as will be addressed in the problem definition below. we give an example of info message in fig. 8 . in this section, we formulate a task distribution problem and introduce a suboptimal searching algorithm to solve in general as well as procedures to deal with aggregation and task-dependency issues."
"contribution becomes a concerning factor. an example of patient anomaly detection and report is illustrated in fig. 2 . in this example, the heart rate (hr) sensor on the patient's smartwatch, unlike an alway-powered camera, cannot perform complicated processing itself. our idea is to allow datagenerator source, hr sensor, to leave processing task to any capable nodes, like a smart bed, along delivering flow to the destination, doctor tablet. correspondingly, we have newly formulated a flow-based optimizing function over two following assumptions. firstly, all edge devices can provide preferable constraints of computation and communication contribution regarding its battery limitation. secondly, devices will be determined as available when it has the potential to finish the task within a specific deadline. to observe limitation and present advantages of the proposed framework, we have carried out a scaling simulation. additionally, to confirm practicality, we have tested our edgecep prototype with real environmental deployment. we installed the prototyped edgecep to the intel edison running on top of batman-adv mesh networks [cit] ."
"in runtime, there are three subscriptions requested by user 1 as followings: (1) out-range average temperature when some people stay (table 7), (2) out-range average humidity when some people stay (table 8), and (3) face detection when someone comes in (table 9) ."
"different sms share a balanced conduction loss regardless of the operation condition, the modulation techniques and the parameter mismatch related to the capacitor and the semiconductor when the sm capacitor voltages are well balanced. the conclusion is helpful to guide the practical cooling design and the active thermal balanced control of the mmc system, where more attention should be paid to the switching loss. moreover, a computationally light conduction loss estimation method is proposed correspondingly, which depends on the onstate characteristics of the semiconductors and the arm current only without considering the switching transients. the validity of the conclusion is verified through both full-scale and downscale simulation and experiment."
"then, we reduce the density of flow volume by increasing the number of nodes to 200, 500 and 1000. fig. 20 presents the total flow volume after 45 minutes. our proposed algorithm works best in all cases, especially in the dense scenario. in the aspect of constraint violations, the proposed approach significantly reduces the violation count for all cases as shown in fig. 21 ."
"our proposed approach still has several points that need to be improved. there are three of them that i would like to mention here. firstly, the current version of our prototype does not allow users to customize the quality of the service. although users can specify the demands of resource usage limitation, when those demands could not be satisfied, all violations are valued equally. some edge networks prefer communication loss than overload computation. for example, in the sensor networks on pedestrians of a smart city with very-limited battery power, the monitored information is large but insignificant. some may have no problem with computation power supply. still, communication must be reliable such as smart hospital systems. to make a decision, we consider the multicriteria decision analysis (mcda) as one of the potential approaches to the solution [cit] ."
"we start with the minimum 102 nodes and randomize destination set for each of three simulation runs. fig. 18 shows the task results delivered to its specified destination (nresult) after 30 minutes. regardless of the position of destinations, the proposed decision algorithm always provides the higher throughput in term of the number of delivered results. as presented in table 4, although the proposed method fo has events processed after the compared approaches (see time to be processed (t p )), it delivers the highest amount of processing results with the fastest delivering time (t d ). in particular, for the dense scenario, where every node produces one input stream and resources are not sufficient, the distributions of incoming flows recorded at the last window after 45-minutes running of all nodes are illustrated as in fig. 19 . note that, it does not include the self-generated flow. we can observe that our proposed flow optimization (fo) and choosing highest computation power left (hc) can distribute the flow volume across all nodes more efficiently than choosing random broker node (rn). since there is no sufficient resource until some nodes detect a loop and no choice but do the processing, the nodes always forward the large-volume flows."
"inspired by tesla [cit] and cql [cit], we newly define an expressive event specification language combining syntax for relational operations. a general structure is represented as below:"
"to observe the radius factor of local knowledge, we implement java program of the proposed flow-based algorithm and test under the controlled scenario as presented in fig. 12 . we omit the link constraints in this experiment to reduce the complexity of the communication path."
2). different sms share a balanced submodule level conduction loss regardless of the switching transient or the modulation strategies when the capacitor voltage of sms are well balanced.
"in addition to the full-scale simulation validation, a threephase mmc with scaled down system parameters is simulated as well. the arm current contains dc and ac components with the peak value being 4 a and 10 a respectively. unity power factor is used, and the modulation index is set at 0.8. moreover, the same igbt module f4 50r12ks4 from infineon with the experiment is used in the simulation. thermal profiles of both igbt and diode in the simulation are tested through curve tracer b1506a under various temperatures ranging from 25"
"firstly, we design a hybrid module-based event stream processing framework aiming for self-organized iot edge devices over wireless networks. the proposed architecture uses the event-dependent specification like logic-based approaches [cit] but stores and processes with relational databases like general module-based approaches [cit] . it obsoletes the assumption of source knowledge in stream processing while remains the use of relational storage for efficiently processing high-relevant data in a self-organized manner. to achieve this, we especially define a new event specification language to express relational operation, and design and develop a broker-based middleware for its distributed execution together with place-and-play api to support customized devices and functions."
"thirdly, even if edgecep allows users to extend new functions, it still requires re-building to apply them. also, the memory of some devices is so limited that cannot even store a big reference function classes. one possible solution is to load an operation code on-the-fly by keeping just only class-owner node and class property. the code-offloading mechanism has been developed in many ways as found in boinc [cit], and tasklets [cit] ."
"we evaluate our proposed system in four perspectives: (i) unit design comparing the performance of processing between the conventional rule engine and proposed relation-based engine, (ii) task distribution and delivery algorithm in the preliminary experiment, (iii) scalability of distribution solution by largescale simulation in the theoretical scenario and practical nursing home scenario, and (iv) utility of prototype program in the real environment. through all experiments, we fixed the stop condition for searching at 20 rounds."
"at starting state, five atomic definitions and five composite definitions are loaded into the system. atomic definitions are temp, humid, tracking, and video for temperature event, humidity event, tracking event, and video-frame event, respectively. composite definition are avgtemp5mins, avghumid5mins, movein, moveout, and peoplecount2mins for event of average value for last 10 minutes in a specific location of temperature, event of average value for last 10 minutes in a specific location for humidity, event when someone comes in a specific location, event when someone get out from a specific location, event to count people in a specific location for every 2 minutes, respectively."
"for aggregation tasks, we cannot determine the results until knowing all relevant data collected from all brokers. in this paper, we use the term, aggregation task, to stand for such a kind of tasks. in common, the aggregation task needs to perform on a specific node. however, in distributed systems, the aggregation is usually divided into two steps. the first step is to distributedly execute while the second step is to finish at one node. in the similar way, we allow distribution by pre-post specification, mentioned in section iii-a. in the similar way, we allow distribution by pre-post specification, mentioned in section iii-a. to decide the header node for the second step, we exploit a blind bid protocol that leads to only one agreement by evaluating the values of all candidates from topology and identification. we suppose to produce the aggregation result periodically. the header node will make a request and wait for all replies with a specific timeout to execute post-processing."
"where p com1 and p com2 are the common conduction loss components for all sms, they are unrelated to the switching actions; ∆p i is the specific conduction loss component of the i th sm; k is a constant related to the arm current. ∆p i can be estimated by p (k), whose minimum value can be achieved when (10) holds."
"the rest of this paper is organized as follows: section ii gives the introduction of the basic operation principles of the mmc. in section iii, the proposed submodule-level conduction loss estimation method is introduced followed by the full-scale simulation validation in section iv. experimental validation based on a down-scale test bench is conducted and describes in section v. section vi gives the conclusions."
"in this experiment, we use the java program from subsection v-b and deploy the plan results in the scenargie simulation with a smart nursing home scenario consists of three buildings: a, b1, and b2, as depicted in fig. 22 . input flows are 1.5kb/min from an environmental sensor, 1.5kb/s from a wearable device, and 6.75mb/s from a video streamer. the simulating system runs four requests: (i) general surveillance sending to a local server (ii) face detection on building a sending to internet gateway (iii) average environmental information sending to an air flow controller for each floor (iv) heart-rate pattern detection sending to the nearest nurse tablet. in this scenario, we first evaluate our offloading-node selection algorithm (flow-based) to nearestto-source heuristic approach (hop-based) and all-to-gateway approach (centralized). results at stable state are summarized in table 5 . the proposed method produces data flow about 7-times lower to the centralized approach. comparing to the hop-based approach, it also outperforms on flow volumes and loss rate. to evaluate our proposed distributed assignment mechanism, we compare its collaborating overhead, a broadcasting message of the node status, to that from the centralized-assigning approach. we summarize results in table 6, where cover flood, guarantee flood, and optimal flood refer to flooding to all nodes, maximum hop to reach the server, and minimum hop that provides the minimal result as a cover flood, respectively. the total number of packets in the distributed approach is higher than the centralization in general. however, the packet size of the centralized approach is much larger respective to the content numbers. furthermore, the simulation scenario also shows that the total size of overhead packets from the centralized assigning is larger than the distributed approach with cover flooding."
"coordinator performs the optimization algorithm to determine a distribution plan that minimize resource utilization while keeping the specified constraints satisfied using statistical record of flows and knowledge about subscription, broker, and networks from content filter, and statistical record of execution from processing and aggregating agents in task processor. the distribution plan is applied to control forwarding behavior of content filter and processing behavior of task processor. the assigning part in task-distribution plans can affect the neighbor decision on local optimization. it sends this assignment part to the neighbors as information as well. the communication flow is depicted in fig. 5 . our approach is not only based on distance heuristics, like nearest to the source or shortest path to destinations but also considers resource constraints and input flow volume (arrival rate). fig. 6 shows an example of a simple distribution plan to cut face images from video stream when someone moved into the interest location. in other words, the collaborating task is to process the subscription given in the above subsection (iii-b.2 content filter). regardless of all constraints, the tasks, movein and face detection, are both assigned by the above heuristics. the particular problem definition and the solution are described in section iv."
"the results of two-hours run with 10-minutes time-window are concluded in table 10 . we notice that the second and third subscriptions, as well as, referred definition tasks are mainly executed at the destination-nearest broker due to aggregation overhead while the first subscription performs at the source-nearest broker. in the aspect of performance, the summarized results also present that the broker 1 and 2 gain 25% higher communication cost due to aggregation cost and information-exchange overhead. at the same time, communication costs on the rest brokers are reduced at least 60% from the base cost, especially video stream from the broker 6 to 4."
"the small errors between the simulations, the experiments and the calculation confirm the balanced conduction loss distribution and validate the effectiveness of the proposed conduction loss estimation method."
"reliability is one of the major concerns for the mmc because of the large cost investment and the large number of semiconductors, which are the weakest components in power converters [cit] . thus, it is necessary to fulfill the full potential of submodules (sms) by posing even thermal stress on the devices. however, unbalanced power loss behavior (componentlevel and submodule-level) in the mmc can lead to various thermal stresses, which brings a challenge to the cooling system design and the converter reliability. the componentlevel unbalance is caused by a dc bias in the arm current when the active power is transferred through the mmc. the four semiconductors (taking the half bridge sm for example) undertake different thermal stress [cit] . submodule-level uneven power loss dissipation mainly results from the parameter mismatch among sms and the low switching frequency for the mmc based on nearest level modulation (nlm) [cit] ."
"there are some cases that more than one subscription require processing on the same event streams. for clearer understanding, we give an example of the system running the three following subscriptions: stranger notification to notify when unrecognized persons enter the house, members' entrance record to keep records when recognized persons, i.e.g, a member of the house, come back and anomaly to detect anomaly behavior in the house. we can construct a directed acyclic graph (dag) to show the dependency of activating tasks, including subscription tasks and referred to definition tasks as shown in fig. 9 . roots are atomic events while leaves are subscription tasks. edges start from referred events (parents) or definition tasks to referring to tasks (children). since the stranger notification and members' entrance record subscription tasks require (or depend on) the face label task, they are both children of face label. in other words, they are sharing the dependency. assigning tasks without considering the dependency might cause these two following problems: (1) early consumption and (2) ambiguous passing. for the former, one task might early consume a required event of other tasks. for example, when a node with the face label stream decides to execute stranger notification task but assign members' entrance record task to a neighbor node. since the face label stream will be consumed for the stranger notification task, members' entrance record task will never be inputted. the latter is how to forward the event that is subscribed by two or more tasks assigned to different nodes."
"considering the equivalence among the six arms in threephase mmc system, the analysis in the following will only take the upper arm of phase a for example. the upper arm current i up is first divided into the positive part i p and the negative part i n respectively for an easy loss calculation."
"edgecep is a hybrid event processing engine that is supportive for fully-distributed collaboration of processing on the edge of networks. referring an architecture shown in fig. 1, the devices with edgecep are considered as brokers. there are only three significant conditions to be a broker: (i) computation power (ii) network connection (iii) programmability. brokers could be a tiny computer module like intel edison or a high-efficiency server. commonly, they are processors. still, if some sensing modules are attached, they are additionally considered as sensors. on the other hand, if they install some actuating modules, they can play actors role as well. all brokers are connecting with a self-organized routing protocol like batman [cit] . some of them may be internet gateways. all brokers collaborate to complete all requests, called subscription. the subscription is composed of the ways to detect, analyze, and generate an output as well as an actuator and an action to be activated. it is written in our newlydefined supportive language and committed by subscribers (system users) and synchronized with all connecting brokers. it will process the corresponding sensing flows and deliver the outputs to the corresponding actuators to drive a specified action in response."
"to allow any devices connected to the edgecep engines, we develop a java api, event driver, for users to interpret the sensing streams from their devices to the event entity. it can refer to the existing definition or form a new one. the connecting brokers must update the new implemented classes to execute. they have to suspend the processing and re-build the edgecep program."
"in this paper, a sysml-based simulation model aggregation framework is first proposed for vegetable seedling propagation system. in the proposed framework, sysml is used to build a system conceptual model in terms of system structures and activities in a hierarchical manner. additive regression modelbased approach is also proposed to estimate parameters for a higher level model. the proposed framework is then demonstrated via one of the largest vegetable seedling propagation system in north america. in the conducted experiments, the simulation model aggregated based on the proposed framework has produced output with same accuracy as that of a detailed simulation model, achieving significant execution speed improvement. our future research works are to study the impact of parameter ranges on the accuracy of the estimation, and to develop algorithms to determine an appropriate aggregative level considering the required accuracy and computational requirements."
"where, yi is the response, xik is the kth predictor for the ith of n observations and i  is an error of the model. where n is the number of simulation output samples. thus, the response yi is subject to normal distribution as follows:"
"one way to balance the execution speed and fidelity is to aggregate the model. there are two primary issues that need to be resolved for simulation model aggregation. one is to narrow down the set of decision variables. the other one is to reduce the total number events during simulation execution. for the first issue, large number of decision variables not only increases the computation time of simulationbased optimization, but also sometimes prevents users from focusing on more significant decision variables. therefore, using sensitivity analysis techniques such as anova to exclude insignificant ones from the set of decision variables and to aggregate several decision variables can help decision makers or optimization programs find the optimal or near optimal decisions more efficiently [cit] . for the second issue, the number of simulation events should be reduced by conceptually aggregating multiple activities into one [cit] . this is often achieved by aggregating model building blocks into one or a few and objects into that of a larger size during the modeling stage. in this way, the simulation provides what would happen \"on the average\" instead of modeling all the detailed activities to look into what would happen \"explicitly\". one important requirement for such aggregation is that the aggregated model must maintain the same behaviors of the low level model. [cit], this paper mainly focuses on resolving the second issue."
"although simulation model aggregation has been addressed in the past, the literature lacks formal modeling approaches to simulation model aggregation. without a formal system conceptual model, the practice of model aggregation is ad hoc [cit] . currently, commonly used languages in systematic and formal information modeling for simulation include express, idef, uml and sysml [cit] . in addition to the properties shared with other formalisms, sysml enables modelers to efficiently capture functional and performance requirements via requirement diagram and to precisely define performance and quantitative constraints via parametric diagram. besides, a common practice for estimating parameters for a higher level model (aggregated model) available in the literature is to mostly use the mean values of lower level model parameters. since the inputs and outputs of a complex system tend to have highly nonlinear relationships, the high level model based on this approach is likely to result in inaccurate outputs, where significant statistical information is lost during the aggregation process [cit] . multivariate nonlinear regression-based approach (e.g. additive regression model) is known to be capable of accurately identifying both linear and nonlinear representation of predictors and responses [cit] . this category of statistical approach can be used to estimate the aggregated model parameters via conducting simulation experiments. since only limited information about the relationships between predictors and responses is available before conducting the regression analysis, obtaining an accurate form of nonlinear regression model is computationally expensive in terms of the required sample size. therefore, an additive regression model with nonlinear partial-regression function can be used as it is also able to accurately estimate the relationships between predictors and responses, but is less computationally demanding. in this paper, a sysml-based simulation aggregation framework is proposed for developing an aggregated simulation model for a seedling propagation system. first, a formal system conceptual model is constructed in sysml to define system structure and activity. the simulation models conforming to the system conceptual model are then constructed using arena software. an additive regression model-based approach is proposed to estimate parameters for the higher level model. the framework is finally demonstrated via modeling one of the largest grafted seedling propagation systems in north america."
"the proposed simulation model aggregation framework involves three main steps (see figure 2 ): 1) system conceptual modeling, 2) simulation modeling, and 3) parameter estimation. in the first step, a sysml-based system conceptual model is developed to describe system structures (e.g. departments, cells and machines/workers), activities (e.g. object loading/unloading, transportation and processing) and parameters (e.g. loading/unloading time and processing time) in a hierarchical manner. in the second step, different levels of simulation models are constructed conforming to the constructed conceptual model. the number of levels varies depending on the structure of the considered system. in this paper, three levels are used as the seedling propagation system can be conceptually described at each of the department, cell, and machine levels (high, medium and low levels). in the third step, different levels of simulation model parameters (e.g. mean value u and randomness w) are used to formulate experiment settings, and then used as simulation inputs for experiments. the simulation inputs (u, w) and outputs (f(u, w)) are both used to construct regression models. finally the regression models are applied to estimate the higher level model parameters (u', w'). this procedure continues until the highest level model parameters are estimated."
"in a grafted seedling propagation system considered in this paper, grafted vegetable seedlings are produced using scion and rootstock seeds via a variety of stages including: 1) seeding, 2) germination, 3) presorting grow on, 4) sorting, 5) pre-grafting grow on, 6) grafting, 7) healing, and 8) post-healing grow on. as depicted in figure 1, both scion and rootstock seeds are first seeded into trays via a seeding machine in the seeding stage. the seeded trays (both scion and rootstock) are then moved into germination chambers in a selected germination area of the greenhouse. the germinated scions/rootstocks are moved into different compartments of the greenhouse and grow on until the cotyledon's full emergence. as soon as cotyledons emerge, the scions/rootstocks are transplanted into different trays via a sorting machine based on their sizes. the sorted scions/rootstocks are moved back to greenhouse for further grow-on before grafting. once the size of scions/rootstocks is suitable for grafting, they are moved to grafting department where they are grafted by grafting workers or grafting robots. the grafted seedlings are then put into healing chambers for healing. finally, the healed seedlings are moved back to greenhouse for further grow-on to reach the size that vegetable producers require."
"the reminder of the paper is structured as follows. in section 2, the seedling propagation system is described. in section 3, the system conceptual model is constructed in sysml. then the additive regression model-based parameter estimation approach is proposed. the proposed framework is demonstrated in section 4 for one of the largest seedling propagation system in north america. in addition, the simulation results obtained from the low level model and aggregated model are compared to the considered sys-tem. in section 5, conclusions and directions for future extensions to the proposed framework are discussed."
"in the real-world problems, many of optimization problems are modeled as dynamic optimization problems, which optima change whole the time. the traditional optimization algorithms are failed in the dynamic environments, because the traditional algorithms cannot track the changes in the environments [cit] . several approaches such as maintenance diversity methods, increase diversity methods, memory-based methods and multi-swarm methods are developed for solving dynamic optimization problems [cit] ."
"as for feature extraction, the most intuitive method would be to take basic characteristics of waveform shapes, such as peak-to-peak amplitudes and timing intervals as features. this simple algorithm has little computation time, but is poor in distinguishing spikes of different clusters especially when the snr is low. other algorithms based on principal component analysis (pca) [cit] and discrete haar wavelet transformation (dwt) [cit] are demonstrated with better sorting performance. haar wavelet transformation is formulated according to eq. 6 and eq. 7,"
"in this paper, due to diversity maintenance and speeding up, the hybrid algorithm based on multi-swarm cellular pso and clonal selection algorithm is proposed. in the proposed algorithm, a simple clustering is used to form sub-population in each cell and clonal selection algorithm is applied for each swarm in each cell to improve solutions."
"where sbestmem l k, the best position in dynamic memory of the cell to which the group belongs and sbestmem l k is the best position in other groups in the cell. as an example, if the best particle in group j of cell k is updated, sbestmem l k is the best position among all the groups in cell k except group j. the forth part of the equation prevents the groups from converging to a local extrema."
"in the following experiments, we demonstrate the results of each module via cuda implementation. two data sets are used for testing (dataset1 and dataset2). fig. 9 shows the neo spike detection results. if a spike is detected, we set a value one to the corresponding position index. for instance, fig. 9(a) shows the original signal data and the corresponding detected spikes in the position indices 1331, 1742, 1988 [cit] with value one. compared to the cpu version, the detected results are identical but less computational power is needed for the gpu version. the results of cubic spline interpolation using cuda implementation are shown in fig. 10 . after interpolation, the spike waves become much smoother. therefore, the interpolated spike contains 64 data points with u being equal to 2. fig. 11 shows the improvement of neuron cluster separation after interpolation. the original neural signals are sampled at 25ksps (left). the spike waveforms are re-aligned after up-sampling to 50ksps through cubic spline interpolation. as observed, the sorting performance significantly increases after interpolation. the pca projection is performed after the wavelet transformation. 64 data points are projected to the two principal dimensions by multiplying the corresponding eigen-vectors. fig. 12(a)(b) show the projected interpolated spikes in the feature space. finally, we apply cpu-based watershed clustering on the feature space for classifying the spikes into 3 categories (fig. 12(c)(d) )."
"clonal selection algorithm (csa) is one of the artificial immune system algorithms, which is inspired from natural immune system mechanisms. clonal selection was proposed by de castro for learning and optimization [cit] . the main features of csa were considered such as affinity proportional reproduction and hypermutation."
"spikes are the signals transmitted between neurons in a form of electronic action potential. through extracellular electrode arrays, each channel records neural signals from multiple neurons. since the knowledge in the firing pattern of neural signals from individual neuron is highly desirable, the identification of each neuron from the recorded spike trains is necessary. this process is commonly referred as spike sorting. currently, spike sorting is mainly used in the research of neuroscience and cortical control prosthesis. it is essential for studying neural activities and sensation in neuroscience research. it provides brain functions that communicate with outside world through cortical controlled prosthesis by spinal cord injured patients and those with the parkinson disease or epilepsy. once the spikes are accurately classified, neural modeling and decoding can interpret groups of spikes into control commands to prosthesis or actuators. since the results of the neural decoding are less significant without an accurate spike sorting, robust sorting performance is a critical issue [cit] . besides, home-care surveillance systems require real-time processing capabilities to react to patients' symptoms or motion intention in time."
"in the experiments, proposed algorithm so called cellular psobased on clonal selection as cpsoc is compared with hibernating multi swarm optimization as hmso [cit], learning automata based immune algorithm as laia [cit], cellular differential evolution as cde [cit], cellular particle swarm optimization as cpso [cit], and adaptive particle swarm optimization as apso [cit] by offline error. for each experiment, the average offline error and standard deviation of 30 times independent running is reported. the results of several dynamics are listed in the table ii, to table v. according to the results of the table ii to table v, the proposed algorithm is better than alternative algorithms relatively."
"where y i and y i are the first and second differentiation of i th cubic polynomials. that is, the first and the second differentiation are continuous on source data points. commonly, the first and second derivative is set to zero as boundary conditions at the endpoints. the differential equation has been simplified into tri-diagonal matrix computation in eq. 4. the coefficient of i th third-order polynomial can be expressed via eq. 5. di indicates the function's first derivative (i.e., y i (0)). once the polynomial is solved, we interpolate spikes by substituting the equal-division points into eq. 2. afterward, the peak alignment is executed for the up-sampled spikes."
"iii. clonal selection algorithm artificial immune system algorithms are bio-inspired algorithms which have been inspired by natural immune system. natural immune system is divided into innate immune and adaptive immune, the algorithms are proposed by researchers, which are modeled based on the latter. moreover, based on several theories for natural immune system, the inspired models are developed into five groups: (1) negative selection algorithms, (2) artificial immune network, (3) clonal selection algorithms, (4) danger theory inspired algorithms, (5) dendritic cell algorithms [cit] ."
"in the proposed algorithm, after each change clonal selection which increases the efficiency of the algorithm is performed for each swarm. the csa is applied to the cbest of each cell. the overall process includes definition of a magnitude and a direction of movement for each dimension determining the magnitude and direction of the search in that dimension. moving in each dimension according to the specified magnitude and direction, the fitness is calculated for the obtained position and if improved, the current position is substituted by the obtained one and if not, the movement direction is reversed in that dimension and a new direction is followed. if the fitness is improved performing the latter action, the update is performed and if not, the magnitude of movement is decreased and the process begins for the next dimension. the whole procedure is performed for all dimensions until in a movement further improvement is impossible in all dimensions and the minimum movement magnitude is reached in all dimensions. the flow chart of proposed algorithm is illustrated as figure 2. in the algorithm, when particles in a swarm converge to a position, the swarm becomes inactive and its particles are used as free particles in finding better solutions in other swarms of the cell or in the neighbor cells. figure 3 depict the running of the algorithm of particles in a 2-d search space."
this project was supported by the gup programming course hosted by national taiwan university. the authors would like to thank prof. wei-chao chen for advising the project and the valuable comments.
"in this section, a gpu-based multi-channel spike sorting design is investigated. we develop a 32 channel spike sorting algorithm on a g80 gpu with cuda implementation and cpu co-design. considering the complexity and data parallelism, the watershed clustering is not suitable for cuda implementation and therefore is implemented using cpu. we benchmarked the processing time on a duocore 2.8ghz cpu for four modules in our spike sorting algorithm (fig. 7) . the result shows that the spike detection and the spike interpolation costs 27.5% and 57.9% computation time, respectively. for feature extraction, it costs 8.8% computation time for wavelet calculation and pca projection. since the eigenvalues are similar for spike data, they are pre-calculated to reduce complexity. for alignment, only 5.8% computation power is needed. in our experiments, the correct rate of classification drops slightly without alignment module for multi-channel spike sorting. we therefore simplify the overall algorithm. the spike interpolation is directly followed by feature extraction. in conclusion, only neo spike detection, cubic spline interpolation and two-dimensional pca projection with haar wavelet calculation are implemented with cuda."
repeat steps 3-6 until stopping conditions is met 3. evaluate the antibodies and calculate their affinity 4. select a number of the highest affinity antibodies of n and cloning these antibodies with respect their affinities 5. mutate all the colonies with a rate proportional to their affinities 6. add the mutated antibodies to the population and reselect a number of the maturated antibodies as memory
"i n c e l l u l a r p s o, a c a i s u s e d f o r s o l u t i o n s p a c e partitioning. ca technique is known as a mathematical model of systems with several simple components which have local communications. using ca and the local rules, an ordered structure may be obtained from a completely random state. two well-known neighborhood structures of von neumann and moore in ca are shown in figure 1.k i i ii k ii k k l bestmem( () ) ( () ) ( () ) ()"
"since the variation of spikes is rapid but smooth and continuous, we choose cubic spline interpolation to reconstruct the spike waveforms. cubic spline interpolation constructs a spike with n−1 piecewise third-order cubic polynomials between n sample points (y1, y2, ..., yn). the polynomial curves are represented by,"
"the rest of this paper is organized as follows: section ii briefly introduced the cellular pso. the proposed algorithm is described in section iii. experimental results of proposed algorithm on moving peaks benchmark as a popular dynamic environment with comparison with alternative algorithms are reported in section iv. finally, section 5 concludes the paper."
"in cellular pso, the search space is partitioned by a cellular automaton (ca). particles in each cell in the ca searches and control its corresponding region according to some transmission state rules of ca. each particle is assigned to a cell based on its position in the space and the search procedure is performed separately for each cell and its neighbors by using the pso. this search method provides enough diversity and provides the ability of following multiple optimum solutions. in addition, neighboring cells communicate information about their best known solutions which results in a more appropriate cooperation between neighboring cells sharing their experiences. this in turn increases the efficiency of the algorithm [cit] ."
"for the proposed method the inertia weight is considered as a random variable between 0.4 and 0.9. the acceleration coefficient is set to 1.496180, the number of particles is 40; the type of neighborhood structure is moore and the size of partition is 5."
"in summary, the neo, cubic spline interpolation and pca with dwt are chosen because of their robustness against noise and benefits to accuracy. for the spike detection, the neo operator is first performed. then, the spike events are detected when the neo value exceeds a threshold at the peaks of the convex curves of the original neural waveforms. signal resolution is increased by cubic spline interpolation in order to raise sorting accuracy. for the feature extraction, the dwt is followed by pca. finally, we use watershed segmentation algorithm to classify the clusters."
"in order to evaluate the proposed algorithm in dynamic environments, several experiments are performed on moving peaks benchmark (mpb). in the mpb, there are some peaks in a multi-dimensional space, where the height, width, and position of each peak alter when the environment changes. unless stated otherwise, the parameters of the mpb are set to the values presented in table 1 [12, [cit] . in order to compare the proposed method with other algorithms, the offline error (oe), provided by the equation below, is used."
"moreover, evolutionary algorithms such as genetic algorithm [cit], differential evolution [cit], artificial immune system [cit] and ant colony optimization were presented for solving dynamic optimization problems."
"the remainder of this paper is organized as follows. in section 2, the algorithms used in the off-site spike sorter for each spike sorting steps are introduced. section 3 states the cuda implementation in detail. section 4 demonstrates the improvement on processing time without degrading the performance on a gpu platform. section 5 concludes this work."
"it is note that, several versions of csa are developed by researchers in the literature, although it is used standard csa in the proposed algorithm. in our proposed method, generally, after partitioning the space into cells, a simple clustering is applied on particles to make swarms in each cell. in during the cellular pso, clonal selection is used to improve the solutions. in the proposed algorithm, the position and the velocity of the particles are updated according to equation (3) and equation (2). where sbestmem k, is the best position in group k, is used as dynamic memory of the group to which particle i belongs. in addition, best particles in each group are updated by using the following equation."
"in this paper, we exploit a home-care surveillance system via introducing a multi-channel spike sorting technique with a gpu datacomputing flow. we propose a spike sorting algorithm for home-care surveillance systems that contact hospitals when the damage is detected from the results of neural decoding, such as perilous action, injured sensation and abnormal neuronal activity."
"vi. conclusion in this paper, an improving cellular pso algorithm proposed by clonal selection algorithm in dynamic environment. the proposed algorithm, speed up the clellular pso algorithm by clustering and improve the quality of solutions by clonal selection algorithm. moreover in the proposed algorithm, it is applied precise search without increasing the number of partitions. the experimental results on the moving peaks benchmark as a popular dynamic environment with comparison with state of the art algorithms shows the relative improvements for our suggestions."
"where f is the fitness function, t is the maximum number of iterations and pbest(t) is the best known global solution found by the algorithm in iteration t."
"tion and subtraction operations are executed for two data points set by set in a spike window. the results are reordered to form a haar wavelet feature. all wavelet waveforms are then reduced to twodimensional feature vectors using pca projection, which is enough to maintain classification accuracy. fig. 6 illustrates the resource mapping flow and the order of data processed in each stage."
"the main algorithm flow for single spike sorting is shown in fig. 3 . spike sorting generally consists of four main parts: the spike detection to allocate the spike events, the spike interpolation and alignment to enhance the signal resolution and to improve accuracy, the feature extraction to compute the dominant features and to perform the dimension reduction, and the clustering to classify the detected spikes into groups according to the extracted features."
"computation time and accuracy are the critical issues for offsite spike sorters. since the classification of spikes depends on the features extracted from the spike waveforms, a better sorting performance usually comes with a higher sampling rate. we introduce cubic spline interpolation to reconstruct waveforms with high sampling rate before feature extraction. however, interpolation increases complexity. we accelerate the sorting procedure through gpu. fig. 2 shows a multi-channel spike sorting system. after amplifying and converting the recorded signals from analog to digital, the digital bio-signal processing technique is applied to find and classify the spikes. in this paper, we implement off-site spike sorters with spike detection, interpolation, feature extraction on gpu, and with classification on cpu. a better sorting performance with high signal resolution could be achieved."
"a traditional ecosystem has been described as \"the complex of living organisms, their physical environment, and all their interrelationships in a particular unit of space\" [cit] . by applying this simple and good working definition to learning; we can describe a learning ecosystem \"as the complex of living organisms in a learning environment (e.g. students, educators, resources), and all their interrelationships in a particular unit of space (can be digital or physical)\" [cit] . in a learning ecosystem it is important to consider the interrelationships of the main actors (students and educators) but also the role of the learning space (both digital and physical). the learning space is by analogy the physical environment in a traditional ecosystem, includes (organisms) information and digital resources like slides, lecture recordings, blog entries and forum discussions; but also physical materials like books, notes and handicrafts, to mention some. the space is where teaching or learning is happening and where such processes and interrelationships are conducted. the interrelationships exist [cit] between the main actors (students and educators), the main actors with the resources (content), and the resources themselves (e.g. recommender systems) [cit], see fig. 1 . those interrelationships shape the quality and value of students' learning experience and our ability to develop teaching approaches enhancing students' dynamics and needs is critical towards the emerging ubiquitous learning era."
"with the visualization of the students' activity using graphs, we reach the conclusion that the use associated with the video assessments provides an important motivation for students to watch the videos and be prepared for the class active learning activities. as we can see from fig. 7, most of the students' stopped the video after reaching into the segment where the information for the last question was found. on the other hand, the \"attractive\" (many views) video segments were identified at the video segments where the presenter was giving the solution of the respective problem."
"note that (21) has a similar form as (15) . therefore, from the optimization problem ðp1þ and theorem 1, we can show that the optimal p $ i to minimize p l in (21) is also proportional to h i, i.e., p"
"by exploring the notion of learning ecosystem, we shown that is applicable to describe and model the main actors, diverse resources and sociotechnical dynamics fig. 9 student's attitudes toward the approach within learning. hence, the concept of ecosystem is an interesting approach which can be applied in learning and give an overview of the different roles, processed as well as learning dynamics. the proposed framework was applied in an introductory course in web technologies. in the empirical study, we investigated students' content navigation, learning performance and attitudes. we also indicated that the main quality of the \"attractive\" information video-segments is the rich and useful amount of transferred information and knowledge, and of course its association with students' assessment. last but not least, we presented students' progress throughout the course using pre-mid-post assessment, and examined their attitudes regarding easiness, usability, usefulness, and acceptance of the course as well as the two positive and one negative emotions."
"significantly reduce the power of its rf transceiver (for idle listening and data transmission/reception), while waiting for the mobile agent to come by for reliable communication. this way, each sensor needs not spend much power to reach distance r; instead, it just needs to wait until the mobile agent gets close enough (much shorter than r) to save its power. in either case, the mobile collector can visit only a single sensor and communicate with the sensor at a given time instant."
"a random walk has been widely used as a means of randomized routing or probabilistic packet forwarding in the data gathering process thanks to its inherent distributed nature and other preferable properties such as simplicity of implementation, scalability, robustness to topology changes, and avoiding critical points of failure (or hot-spot formation) [cit] . the random walk has been also popularly used as a mobility pattern for physical mobile agents collecting information over sensor nodes [cit] ."
"to visualize the effects of other attitudes in students' intention to participate in the course, we divided easy, control, use, enjoyment, excitement and boredom on high and low categories performing median split. we then created a diagram in order to provide a better understanding of the effect. in fig. 10, we can observe that boredom is the only one with a negative effect. overall, fig. 10 clearly exhibits the positive and significant (based on the pearson's correlation) influence of students' attitudes on their intention to participate in similarly developed courses in the future."
"the rest of this paper is organized as follows. section 2 describes the system model employed in this paper. section 3 provides the definition of our performance metric-network loss probability, and its mathematical analysis. we then find the optimal movement strategy of a mobile collector among a class of markovian random-walk movement strategies in minimizing the network loss probability under a simple yet non-trivial setting, and explain how such an optimal movement strategy can be made to work in a distributed fashion in section 4. we further discuss its extension to more general settings in section 5. we provide various numerical simulation results on the performance of our distributed optimal movement strategy in section 6, and finally conclude in section 7."
"a learning ecosystem has been described as a means for orchestrating a variety of learning approaches given by the varied characteristics of learning processes [cit] . learning ecosystems have seen as environments which are \"consistent with (not antagonistic to) how learners learn.\" [cit], focusing on the learning process and take into account learners' characteristics, needs and the potential dynamics and interactions with different actors (students, educators), as well as the learning environment and resources. thus, the concept of learning ecosystem provides an ideal ground to orchestrate multiple tools and practices in the best possible way [cit] ."
"). for random variables u and v, we define a convex order, written as u cx v, if effðuþg effðv þg holds for any convex function f for which the expectation exists."
"active and problem-based learning activities are founded upon a constructivist theory and traditional lecturing derived from direct instruction methods is founded upon behaviorist ideology. active learning is a model of instruction that focuses the responsibility of learning on learners. [cit] s by its appearance on the association for the study of higher education (ashe) report [cit] . during the last years, active learning practices like project-based learning and flipped classroom have gained prominence worldwide, however sometimes we face a lack of consensus on what exactly active learning is. a working definition of active learning, derived from collecting opinions from 338 experts is the following: \"active learning engages students in the process of learning through activities and/or discussion in class, as opposed to passively listening to an expert. it emphasizes higher-order thinking and often involves group work.\" [cit] . although this definition is quite generic, it perfectly portraits the rationale of active learning, without being restrictive. active learning has been deployed in a number of education studies, however a framework describing the learning dynamics is typically not described; the development of a learning ecosystem framework to support active learning, will allow us to better understand and further develop teaching approaches enhancing students' dynamics and needs."
"the development of the framework emphasizes in the generic view of the learning ecosystem, hence it is possible for an individual to apply it to any active learning situation, such as project based learning, or peer instruction. another important aspect is to assert that the interrelationships and interactions with all the organisms, external influences as well as the infrastructures of the learning ecosystem are in principle dynamic (fig. 3 ). this generic view helps to get a better picture about a specific learning situation, and allows educators and practitioners to achieve a more holistic approach for the development of more effective learning. a graphic representation of this definition is shown in fig. 3 ."
"on the other hand, the previous research [cit], closely related to our work in this paper, analyzed how much data can be successfully collected by the agents in time under the mathematical framework of the standard random walks on the grid (torus) [cit] or by directly taking into account the time interval between successive visits of any of the random-walk agents to a sensor node [cit] . their performance analysis was done from a single node's point of view, as the data arrival rate is the same over all nodes. in contrast, we investigate the problem under a general network (graph) setting, while allowing different data arrival rates to sensor nodes. more importantly, our focus is on the design of the distributed optimal movement strategy for such mobile agents among a class of markovian randomwalk movement strategies, as opposed to the performance analysis under the standard random walk strategy."
"as aforementioned, the collected data consists of three different types; therefore, an appropriate data analysis was used for each different set of data. students' video navigation was analyzed with aggregated time series visualizations, in order to identify"
"building upon existing technologies and practices like video-assignments, clickers and micro-project approach, in the next section we propose a learning ecosystem for active learning. in the third section we present an empirical study following the proposed approach, were by collecting diverse-sourced data, we portray students' experience throughout the course of the semester. the last section of the paper draws conclusions and discusses ideas for further research in the area. this study aims to provide a springboard for other scholars and practitioners to further examine the efficacy of this specific blended learning approach. our conceptual framework is a flexible procedure that can be utilized and adapted to meet different needs."
"we want to emphasize that our findings are clearly preliminary with inevitable limitations. as for the internal validity of the empirical study, data are based on a self-reported method, log-files and assessments. other in-depth methods such as semi-structured interviews could provide a complementary picture of the findings through data triangulation. as for external validity, the subjects were computer science majors, which may somewhat limit the generalizability of our results. nevertheless, the insights drawn are not connected with the subjects' background and can be applied on any population. fig. 10 the influence of students' attitude in their intention to participate in the course"
"in order to engage students deeply in the process of learning, they worked with a group project throughout the semester and were asked to apply the obtained knowledge as well as to make progress presentation and get feedback. this gave them the opportunity to be involved in an active learning process. the aim was to engage students more deeply in the process of learning course material by encouraging critical thinking and fostering the development of self-directed learning. active learning affords the opportunity for application and practice, and the asking of questions. during the team micro-projects, students had to find a client and mimic the professional software development process. in particular professional software development projects had the following sequence of phases: requirements, design, implementation and testing."
"given a graph g with n sensor nodes, we consider a class of markovian random walk strategies, each of which can be the movement strategy for a mobile collector traveling over g and visiting each sensor i to collect all of the currently stored data (or its summarized or compressed version) in the buffer of size b. let xðtþ be the location of the mobile collector over g at time t. by definition, the random sequence fxðtþg t!0 is a discrete-time markov chain on a finite state space n with its transition probability given by"
"which is independent of the initial position xð0þ for all i. therefore, from (2), (3), and (8), the network loss probability p l can be written as"
"an initial empirical validation of the proposed framework took place in a public university. the goal of this empirical validation is to provide the first analyticsbased evidence regarding the effectiveness and acceptability of the proposed framework. the early results should not be seen as a rigorous evaluation of the proposed framework, but as reflections rising from a particular case study as well as empirical evidence for further development of the framework."
"our approach emphasizes relationships and interactions related to the information flow as well as knowledge transfer and transformation. similarly with a biological fig. 1 different types of interrelationships (interaction) in digital and physical learning spaces [cit] ecosystem, in a learning ecosystem, individuals can form groups spontaneously and can interact with each other or with learning utilities at the individual or group level. they can also perform specific behaviors in order to contribute to or perturb to the success of the learning ecosystem [cit] . changes in the learning ecosystem conditions (external influences) shape the \"behavior\" of the system and its components. to be successful and to be valuable for the system, each individual and group must adapt to the environmental conditions to find their niches. in order to fit them all together, proper learning infrastructures must also be available (fig. 3) ."
"practical principles and heuristic models that enable actors within the learning ecosystem to understand and shape their learning future, is considered as one of the cornerstones of smart learning environments [cit] . in this research, we presented a learning ecosystem framework and the first captured results of its application. the framework can be put into practice using basic e-learning tools and active learning practices; hence the framework can be used for those interested in incorporating project-based principles as well as flipped classroom or any other active learning approach in their teaching, since it is a flexible procedure that may be adapted to meet different needs."
"the empirical evaluation was conducted in two identical classes (in terms of learning goals, teacher, teaching method and so on), those classes had 510 computer science [cit] students (20-29 years old, 105 females and 405 males) enrolled in the web technology course. the course lasted 12 weeks, and we applied the proposed framework (as described in section 2.2). in addition to students' analytics obtained from the aforementioned systems, we employed a post survey. a total of 73 students' (14.31 %) volunteered to participate on the survey (14 females, 59 males, with mean age 22.38 s.d. 2.50)."
"following the proposed framework (fig. 3), we implemented video-assignments to scaffold the know/believe part, this helped us to assure that students have attained the fundamental knowledge. this is typical in many active learning approaches (e.g., flipped classroom), where students are involved with the learning materials in order to obtain the initial fundamental knowledge. this basic knowledge was made available using video lectures, in addition we employed an integrated assessment to the video lecture (see fig. 4 ) and weekly exercises. these self-regulated, but also well-defined concepts allowed students to better understand the video and reflect about their learning [cit] . upon students' completion of the video lecture and the respective assessments, instructors can access all the collected data, visualize students' activity and progress, and ultimately fig. 4 an example of a video assignment (using video learning analytics system; [cit] ) identify students' interaction with the materials. such information allowed instructor to be well-prepared for the active engagement session by addressing all the misunderstandings and misconceptions of the students (ruipérez [cit] ."
"as to the random walk-based routing for data gathering, the existing research studies have mainly focused on the performance of the following metrics: delay-the time for a random walk (a data or query packet) to reach its destination (a sink node or a sensor having certain information of interest) [cit], and cover time or its partial cover time-the time for the random walk until to visit all or partial set of sensors [cit] . these metrics are suitable for one-shot information delivery or search/query. in contrast, in this paper, we look at the problem of data gathering using the random-walk agent(s) from a different, but important perspective. note that wsns are composed of low-cost, low-power sensors, each of which is equipped with limited buffer/storage space for data. also, the random walk-based data gathering is typically for delay-insensitive applications in which the collected data is mainly used for post-processing or other research studies later. it is thus more important to measure how much data can be collected before it is lost due to limited buffer space, when the sink periodically generates query packets or collector packets moving over the network in a random walk fashion to gather measured data or its aggregated/compressed version from sensor nodes [cit] ."
"now, p l is solely dependent on p i, and the problem of finding the optimal transition matrix p is equivalent to finding the transition matrix p achieving the optimal p (15) . since n p is a global constant, optimizing p l over p p is equivalent to the following problem: for a function fðx xþ : r n ! r,"
"by visualizing students' video usage throughout the semester, we concluded that most of the viewings are happening in three instances. first during the first days of the video release, second during the assignment period where students might need to use some information/knowledge from the video and third before the exam. as we can see in fig. 6 from a typical video usage visualization, the three main peeks have been identified in the release week, the assignment week and during the last week before the exam."
"today, diverse technologies have been applied in a variety of active learning practices. however, isolated use of different technologies does not offer an overview of the different learning dynamics. developing a learning ecosystem framework, which will allow us to describe \"the complex of living organisms\" as well as their interrelationships, will help us to better understand and further develop our teaching approaches. in this work we work towards this direction, by the following twofold contribution, first we present a conceptual framework of a learning ecosystem which can host active learning instruction and second we provide some first analytics-based evidence regarding its effectiveness and acceptability."
"in order to be able to portray students' experience during the course, we employed three different types of measures. a) students' video navigation (collected via a video learning analytics system [cit], log-files), b) students' learning performance/score (collected via the quizzes [cit], log-files) and, c) students' attitudes toward the course (collected via the post survey, see table 1) in particular, video navigation was captured based on students' interaction with the video player. students learning performance was collected in pre-middle-post measures throughout the semester. the survey included multi questions factors of 1) ease, 2) control over the course, 3) intention to participate in similarly made course, and 4) usefulness of the overall teaching approach as well as the single question factors of 1) enjoyment, 2) excitement and 3) boredom. table 1 lists the operational definitions and the number of items (questions) of each of the constructs (measures), as well as the source from which the multi question measures were adopted. we employed a 7-point likert scale anchored from 1 (\"completely disagree\") to 7 (\"completely agree\")."
"on the other hand, when the mobile collector is a physical agent, we assume that the mobile agent can visit a single sensor node (or its proximity) at time t, and then move to one of its neighbors within its detection range r, at time t þ 1, as depicted in fig. 1b . thus, if sensor nodes i and j are within distance r, then there is an edge between them, i.e., ði; jþ 2 e. this model can be a reasonable discretetime/space approximation for the continuous trajectory of a mobile agent (e.g., data mule) over the sensor field [cit] . one critical reason to use the mobile agent(s) for data gathering is due to low energy budget at each sensor. specifically, in the presence of mobile agent(s), each sensor can fig. 1 . illustrating our system model in which each sensor has limited buffer space and data arrival rate to each node is heterogeneous, while a mobile collector, which can be either a query/collector packet or a physical mobile agent, moves over the graph to collect the data."
"the next question is how to construct the corresponding transition matrix p for the random walk (the movement strategy of the mobile collector) in a distributed manner 1 . roughly speaking, the mixing time is the amount of time it takes for the markov chain to converge to its stationary distribution starting from any initial distribution."
"in this section, we present simulation results on the network loss probabilities of our distributed optimal movement strategy under various setting of buffer sizes and the number of mobile agents as well as different data arrival patterns to the network. in particular, we here demonstrate its considerable performance improvement over the standard random walk strategy in which a random walk at the current node moves to any one of its neighbors uniformly at random at the next time slot. as mentioned earlier, the standard random walk strategy has been widely used as a randomized routing for query/collector or data packets as well as the movement strategy for physical mobile agents in the data gathering process [cit] ."
"here, x c and y c are uniformly and independently chosen over ½0; 1. fig. 2b depicts an example pattern of data arrival rates based on (24)."
"traditional lecture style is a common teaching approach in higher education classes; however, the traditional lecture style of teaching can often place students in a passive role, which typically involves students retaining isolated facts that can later be forgotten. [cit] definition, traditional lecture style is a \"continuous exposition by the teacher\"; in a traditional lecture instruction students' activity is limited to taking notes and asking questions to the instructor. over the last few years, instructors have been moving away from the traditional lecture style by implementing more active learning practices, like project-based learning and flipped classroom, and increasing the technology use as a way to extend and enhance students' understanding."
"another important question is related with the intensity of the questions (number of questions per video) in the video assignment. in order to investigate any potential relation between questions' intensity and students' watching behavior; we used activity graphs from video assignments with different intensity. we qualitatively concluded that low questions intensity (very few questions, i.e., 2 or 3 in the video) decreases the number of students' views and increases the early dropouts from the video. very high intensity (too many questions, i.e., more than 10 in the video) also decreases the number of students' views and increases the early dropouts from the video. medium questions' intensity (i.e. 3-10 questions) is found to keep students' interested in the video and also not to frustrate them. figure 8 gives an indicative example of how questions' fig. 6 students' video usage throughout the semester intensity is related with students' watching behavior. we didn't investigate questions' quality, but it is very likely that questions quality and demands (questions need students' to be able to assemble and synthesize instead of merely reproduce the content from video) will also play an important role in students viewing behavior."
"to examine any potential shift in students' performance during the course, we used analysis of variances (anova) between the pre, mid and the post-scores. as we can see from table 2 students' scores were at very high levels in the pre, mid and the post assessment. performing an anova, the results showed no significant difference ( table 2) . as a consequence, there was no shift in students' score throughout the course."
"the degree of students' intention to participate in similarly developed courses in the future. [cit] . as for students' learning performance, we captured students' pre-mid-post assessments scores mapped them in a diagram and employed an analysis of variance (anova), this will allow us not only to capture students' performance toward the course, but also to identify any potential shift during the course. last but not least we used descriptive statistics on students' attitudes towards the course."
"we have developed an analytical framework to evaluate the network loss probability as a performance metric for different markovian movement strategies of mobile collectors moving over a graph (or network) for data harvesting in wsns. under this framework, we were able to find the optimal movement strategy for the mobile collectors under mild conditions so as to minimize the network loss probability. our optimal strategy can be made distributed using only local information via the metropolis-hastings algorithm. we have demonstrated through extensive numerical simulations that our distributed optimal movement strategy remarkably outperforms the standard random walk strategy under various settings of network topology, buffer size, and the number of mobile collectors, as well as heterogeneous and spatially-correlated data arrival patterns. we expect that our reasoning behind the distributed optimal movement strategy can be applicable for the design of markovian random walk-based applications in general networks beyond wsns."
"based again on the proposed framework, the active engagement activities are developed with a main focus on recalling the basic knowledge and then engaging students with active learning and critical thinking processes. the recall part is conducted using a mobile game/quiz at the beginning of the class. the instructor prepared a session with questions related to the basic knowledge, supported with different forms of audio visual materials (e.g., videos) [cit] . the class was equipped with a projector, which was used to display the main screen of the game (see fig. 5a ), and each student used his/her own mobile phone to give the answer to the respective question (see fig. 5b ). at the end of the course, the instructor could download all the collected analytics of the game and explore students' progress and understanding."
"then, we can interpret z i as a reward during a regenerative cycle r i, and thus from the standard renewal theory on the (delayed) renewal reward process [cit], we have"
"there is another set of research work on exploiting physical mobile agents (e.g., data mules) for data harvesting over the sensor field [cit] rather than relying on multi-hop communication over sensor nodes. the reason of using the mobile agents is to save the limited battery power of each sensor and to overcome possible network isolation. in this regard, much research is based on traveling salesman problem, i.e., finding a hamiltonian cycle (a np-hard problem), or its variants, and their solutions are, in general, only globally solvable or require global network information such as sensor location/distance information [cit] . in this paper, we focus on markovian randomwalk movement strategies without such global information. a crucial reason of using the random walk is its distributed nature, as it is desirable or sometimes imperative not to rely on global (or near-global) information."
"ata gathering or harvesting is a generic research problem in wireless sensor networks (wsns)-how to collect observed (or measured) data or information from sensor nodes, and so has been actively studied in the literature. for the data gathering process, a sink (or base station) periodically generates query packets or collector packets to gather certain information of interest from sensor nodes [cit], or each sensor, instead, directly informs the sink node about its observed data or events [cit] . in addition, mobile sinks (agents), e.g., data mules, can move around the sensor field and collect information observed at each sensor node [cit] ."
"the proposed framework was applied in an introductory computer science course, named web technology. like any course, web technology was framed within predetermined external influences (conditions) like learning goals, methods, workload and content. the focus of this course is on the world wide web as a platform for interactive applications, content publishing and social services. by the end of the course students are expected to be able to design and develop web-pages and webapplications; using markup (e.g., html), design (e.g., css) and client-side (e.g., javascript) programming languages. students have to deliver specific biweekly assignments, work with a self-selected group project and take written examination; these three components are also the evaluation criteria. the course materials, digital communication as well as the assignments and project-work are orchestrated from a learning management system (lms)."
"in this section, we formally define the optimization problem in order to find the movement strategy of the mobile collector (random walk) in minimizing the network loss probability p l, and then explain how the mobile collector can adjust its movement strategy to achieve the optimal strategy in a distributed manner."
"behavior model that does not require any history information for the recognition of normal driving and secondary driving tasks. in addition, the importance of driver posture features for the identification of driving tasks is evaluated."
"the data-recording frequency for the head tracker is 30 hz, which is approximately three times greater than the kinect, and therefore, the head tracker yaw angle shown in fig. 5 is the smoothed version of the original signal. finally, the mean error and standard deviation between the calibrated kinect signal and the head tracker for yaw, pitch, and roll angles are 1.93 ± 11.55°, 1.47 ± 5.98°, and 1.44 ± 6.98°, respectively."
"where x i is the permuted i th feature in the feature vector x, b is the number of trees in the rf, ooberr t i is the model prediction error of the perturbed oob sample with the permuted feature x i for tree t, and ooberr t is the untouched oob data sample with permuted variable. the concept of permutation feature importance is that a large importance value indicates the feature, which is influential in the prediction, and permuting the feature value will influence the model prediction. in contrast, a small influential feature will have no or less impact on the model prediction. the predicted feature importance for the 42 driver signals using rf are shown in fig. 7 . from the importance estimation results, the driver yaw angles are extremely important for action classification for all five drivers. to verify the prediction results given by rf, in section iii-a2 proposes another feature evaluation technique called the mic, which uses a completely different method to estimate feature importance."
"as shown in fig. 8, although the prediction results of the two algorithms are not identical, there is some consistency in the results of the two algorithms. for example, the driver yaw and the y-coordinate of the right shoulder features (no. 2 and no. 41) are both significant. according to fig. 8, the 12 most important features (marked as the ten most important features by at least two drivers) are listed in table ii . table ii, the importance predictions given by rf and mic are similar. the most important features are the head rotation angles (yaw, pitch, and roll), eye and nose position, shoulder position, and hand position. the remaining features, such as the wrist, hand tip, and elbow positions, are less likely to influence the behavior detection result. a quantitative analysis of the feature impact on behavior recognition based on an ffnn is proposed in section iii-b."
"potential customers may fear these means of data mining on their sensitive personal data [cit] and decide to not participate in the bonus program, even if the system offers significant rewards and advantages."
". dfdk can achieve lower bias reconstructions than gpl-i, but suffers from increased noise as compared to gpl-b and gpl-bc. however, there is a small range (near the best dfdk mjac performance) where dfdk performs comparably to gpl-b and gpl-bc. figure 4b shows similar trends. for each method the \"best\" reconstruction is defined as the one with the maximum mjac. this maximum mjac value is used to compare the different methods. gpl-bc results in the best reconstruction, followed by gpl-b, dfdk, and gpl-i. the advantage of gpl-bc over gpl-b is more apparent in the mjac plot than the bias/noise plot. figure 5 shows reconstructions of the center line pairs. the bottom half of each image shows the optimal segmentation (i.e., the one resulting in the best mjac). gpl-i results in the worst performance with low contrast line pairs. the line pairs in the dfdk reconstruction are more distinct but both the line pairs and the background exhibit increased noise. finally, the gpl-b and gpl-bc reconstructions have less noise than the dfdk reconstruction without sacrificing line pair visualization. the difference between the gpl-bc and gpl-b reconstructions is subtle, but can be appreciated in the thresholded image, where the gpl-bc method results in thicker and more uniform line pairs. the noise difference between gpl-bc/gpl-b and dfdk is particularly evident in the background of the segmented image, where the dfdk reconstruction contains noisy values above the segmentation threshold, resulting in a \"splotchy\" segmented background image. qualitatively, the visually \"best\" reconstructions correspond to those with the best mjac (indicated by a red outline), confirming the suitability of this metric."
"to define our adversary model, we identify several scenarios where different parties can conduct fraud on other participants. each participant has different interests in the system, and we assume that each might use their capabilities to gain an unfair advantage."
"the remainder of this paper is organized as follows. section ii introduces the experiment setup and data collection and processing methodologies. section iii proposes the method for feature importance prediction based on the integrated method, and an ffnn for task recognition is discussed. then, the task recognition results and driver posture feature evaluation are performed in section iv. section v presents the results discussion and future work. finally, this paper is concluded in section vi."
"the duplication matrices are calculated based on the apparent sourcelet positions from a specific location within the field of view. this approximation is therefore most appropriate near that location (in most cases isocenter), and is generally accurate so long as the apparent focal spot size and shape are relatively constant throughout the field of view."
"this work used a huber penalty for the regularizer (r) [cit] . the huber penalty has an additional parameter, δ, which is the value below which pixel differences will be penalized quadratically. we conducted a parameter sweep over β and δ in order to pick an appropriate value for δ. phantom data were reconstructed using gpl-bc and gpl-i. additionally, two photon fluxes were used: 10 3 photons pixel to approximate the bench study. the high photon flux data utilized the covariance matrix approximation in (10) . both algorithms used 501 iterations, 10 subsets, and momentumbased acceleration. the mjac metric was calculated for each (β, δ) pair."
"the nts as well as other key medullary nuclei subserving the baroreceptor reflex receive inputs from higher centers of the brain, including the hypothalamus and other forebrain regions with important roles in mediating cardiovascular responses to acute stresses. the hypothalamus sends fibers to the dorsal vagal nucleus, to the nucleus ambiguous, to the nts, and to the intermediolateral cell column of the spinal cord [cit] . it receives projections, too, from the amygdala through the stria terminalis and from the septum through the medial forebrain bundle [cit] . besides, the dorsomedial hypothalamic nucleus (dmh) projects directly to the nts and a high proportion of these cells have collateral links to the rvlm [cit] ."
"the reverse customer fraud attack relies on an uninformed merchant that can be framed as an unlawful adversary trying to withhold the transaction. to counter the attack, we add an additional receipt to the point spending process, which proves the customer's intent to exchange points for goods. the additional receipt can then be used by merchants to uncover malicious users. figure 5 shows the necessary changes in the payment protocol."
"in this paper, v2p channel measurements have been presented and a detailed analysis on the large-scale fading in los and nlos scenarios for v2p channel has been introduced. it has been found that the two-ray pathloss model fits well the measured pathloss in the los scenario. in the nlos scenario, a log-distance pathloss model has been proposed. due to the obstruction of the los, the pathloss exponent is equal to 1 implying that the pathloss not only depends on the propagation distance but also on the size of the obstruction. to study the propagation loss due to the obstruction of the los, a 3d tool has been developed. the tool detects the diffraction edges and calculates the fresnel-kirchoff parameter that is used to calculate the knife-edge diffraction loss. the results show a good match between the model and the measured pathloss with a standard deviation of 4 db. finally, the spatial autocorrelation of shadow fading is investigated. in order to study the spatial autocorrelation, shadow fading need to be assumed stationary. therefore, the modified reverse arrangement test is applied to test the stationarity. according to the test, the stationarity hypothesis is verified with 5% significance. three different models have been proposed for the spatial autocorrelation function. the results show that the exponential decaying sinusoid model provides a good match to the empirical spatial autocorrelation function in both scenarios."
"where (2a) is the mean forward model and (2b) is the noise model. (n indicates a normal distribution, in this case with a mean ofȳ and covariance matrix k.) with this notation the exponential function is applied element-wise. throughout, matrix and vector variables are boldfaced, and variables indicating elements of these matrices and vectors are not bold, and have a subscript indicating which element they refer to (e.g., y i is the i th element of y and b ik is the element at the i th row and k th column of b). traditionally, a is the forward projector, b is a diagonal matrix that scales raw transmission values based on the photon fluence associated with each measurement, and k is a diagonal matrix of the measurement variances (i.e., the covariance matrix with an independent noise model). while these are conventional choices in the forward model, (2) is sufficiently general to accommodate more complex physical characteristics including system blurs (if b is a blurring matrix) and correlated noise (if k is a non-diagonal covariance matrix). in this work, we focus on modeling scintillator and focal-spot blur as part of b, and noise correlation due to the scintillator blur in k. specifically, both blurs are modeled as shift-invariant convolutions. while this ignores focal-spot geometric effects (e.g., depth dependence) it is an appropriate approximation in many scenarios (e.g., thin objects, narrow fan angle). see appendix d in the supplementary materials 1 for a derivation of the approximation. (note that this approximation is not imposed by the forward model, which is capable of modeling shift-variance and depth dependence.) equation 2 also assumes the measurements have an underlying gaussian distribution. the pl objective function resulting from (2) is therefore the penalized non-linear least-squares equation"
"a customer's privacy is not threatened in any constellation in our system. the underlying system of electronic cash guarantees privacy when regarding all transactions present in that protocol. at first glance, it may seem that the introduced merchant vouchers reveal to the issuer the merchant that a customer has conducted business with. however, if communication between customer and issuer is anonymous itself (e. g., by using a proxy server), the issuer does not learn anything about the customer's identity. the asymmetric-key-id extension of the original protocol does not leak any more private information than the random ids from the original approach if the assumption of hard reversibility of key generation [cit] holds. like in the original protocol, the asymmetric key pair for the bonus point has to be generated using a truly random source and not contain identifying information like a mac address or other system-specific parameters. when the claim resolution process is activated, information about the inner structure of the private key still does not leak. in this case, only the public key, which is itself derived from the secret key, is leaked. authorization receipts also do not convey any usable information on the customer's identity, as the signing key is different for each bonus point and not linked to a certain customer. the same effect can be observed in case of the challenge-response protocol during fraud resolution. thus, no information is released that could endanger the user's privacy."
"in this section, an artificial neural network is used for driver behavior pattern recognition. specifically, a one-way ffnn is adopted. the ffnn passes the input vectors to the output layer-by-layer without any feedback connections. the ffnn is a powerful tool for solving complex nonlinear mapping problems. by learning the neuron parameters and the connection width, the ffnn model is able to construct a nonlinear mapping between the input and the output. the ffnn can be approximately represented as follows:"
"the kinect sensor is mounted in the middle of the front dashboard. the optical axis of the kinect camera is not perpendicular to the yaw axis of driver's head, which will influence the detected yaw angles. the rotation angle of the kinect sensor in world coordinates is reflected by a constant bias of the detected yaw angle, as shown in fig. 5 . the blue line is the original yaw angle. the yellow line is the shifted yaw angle, which shifts the original signal by a constant offset (30°). the red line shows the ground-truth results of the head tracker. the calibrated kinect signal and the ground truth have similar variations, which means that the head rotation angle detected by the kinect is reliable and can be used for further analysis."
"to evaluate the prediction results of feature importance using the two algorithms, the ten most important features for each subject are extracted and compared. specifically, for each driver, the ten most important features are selected. then, five selected feature vectors are fused into 42 bins, and the count in each bin represents the number of occurrences for each feature of the five subjects. therefore, the highest value, 5, indicates the feature, which is the one of the ten most important features for all five drivers. the statistical results are shown in fig. 8 ."
"the success of mbir methods illustrates the importance of high-fidelity modeling in ct reconstruction. accurate modeling of cbct systems, enabled by the proposed method, improves image quality and permits high-resolution tasks such as microcalcification detection and analysis of trabecular bone morphology. in addition to improving the capabilities of current cbct systems, this method has the potential to alter the trade-offs between hardware/geometry choices and image quality, potentially effecting future cbct system designs, including those that aren't necessarily aiming for high resolution. for example, proper focal-spot modeling may better leverage high-magnification or permit replacing rotating-anode x-ray sources with more economical fixed-anode sources while preserving resolution. future studies will characterize the improvements possible with the proposed gpl-bc approach and their possible impact on cbct system design, in addition to incorporating the different blur models described above and considering systems with different balances of correlating and non-correlating blur. to aid in deriving surrogates matched at the current iterate, θ may be expressed as"
"the second block in table v illustrates driving task classification using only head pose information. specifically, the three head rotation angles: yaw, pitch, and roll are used to construct the feature set. the classification accuracy using head pose is much less than the accuracy in previous cases. for the leftmirror checking and texting tasks, which have significantly different characteristics than other tasks, the detection is accurate. however, for the other tasks, using only head pose information is not sufficient for accurate detection. for example, the driver rear-mirror-checking behavior (t3) is similar to the task of using a video device. moreover, without considering body information, the phone answering behavior cannot be detected accurately, since the driver is usually looking forward to the road, and the head pose is very similar to normal driving. the confusion matrix for driver 5, which has the most accurate results among the five drivers, is shown in fig. 12 . in terms of driver 5, ffnn is not able to accurately distinguish tasks 1, 3, and 7, which are normal driving, rear-mirror checking, and answering the phone. approximately one-third of normal driving cases (224 samples) are classified into the phone answering task. for rear-mirror checking, more samples are falsely detected as the video device using the task. it is obvious from the confusion matrix that, without using body features and using only head pose features, it is difficult to identify the actual driver behavior."
"to calibrate the estimated head rotation results of the kinect, a head-mounted head tracker was used and three-degree rotation data from the head tracker was used as the ground truth. the head tracker is based on an arduino microcontroller board and an intelligent nine-axis absolute orientation sensor (bno055) designed by bosch. the sampling frequency of the orientation sensor is up to 100 hz. the rotation sensor and arduino data-recording sensor are fixed on a headmounted harness belt strap, as shown in fig. 3 . and seven driver behaviors studied in this research are shown in fig. 4 ."
"through natriuresis and diuresis, while the failure of sinoaortic denervation to alter long-term level of arterial pressure was originally used as an argument against a role for the entire nervous system in arterial pressure regulation; such position was later on reaffirmed [cit] . one reason is because over the past few decades the primary focus of many studies has been placed on neural control of the kidneys, considering this vascular bed as playing a major role in long-term control of arterial pressure. increases in the renal autonomic neural system (ans) function results in several responses that potentially and chronically elevate arterial pressure; they include sodium and water retention, increased activity of the renin-angiotensin-aldosterone system and increased renal vascular resistance [cit], put forward, although perhaps not for the first time, the water logging concept in the arterial walls as a mechanism to permanently change their structure and, through it, peripheral resistance and, thus, blood pressure, while guyton and colleagues proposed that the only mechanism by which the sympathetic nervous system can chronically regulate arterial pressure is via alterations in the renal function curve [cit], with their classical and still valid book."
"we used the well-developed technique of non-metric multidimensional scaling (mds) [cit] to place each cone from the mosaic shown in figure 2a at a position in a three-dimensional space such that the inter-cone distances in this space best predicted, up to a free monotonic transformation, the negative log of the correlation matrix. figure 3a shows the result. each point represents a single cone, and the cones are colored red, green, or blue for l, m, and s respectively. it is clear that when the full correlation matrix is used to embed the mosaic cones, their positions separate strongly by class. figure 3b shows an expanded view of the embedding that includes only the l and m cones. in both figure 3a and 3b, the figure 1 . natural image correlations are highly regular in both space and spectrum. (a) an rgb rendering of a hyperspectral image taken from the natural image database described by chakrabarti and zickler [cit] . the code used to render this figure is included in our github repository (https://github.com/davidbrainard/receptorlearning/). (b) image correlations for images from our combined database. correlation is plotted as a function of distance (pixel). each curve represents a different wavelength separation: the black curve represents no wavelength difference while the yellow curve represents a difference of 320 nm (i.e., the 400 nm channel correlated with the 720 nm channel). doi:10.1371/journal.pcbi.1003652.g001"
"based on the above-mentioned evidence, to obtain a better understanding of the tasks that the driver is undergoing, both the head and body features are necessary. from the feature comparison, head pose features are more useful than body features, since the 3-d head pose information leads to better detection results (62.1% average) compared with the 3-d body features (43.9%). fig. 13 shows the model classification results for driver 1 when the model is only trained with body features. the three distraction behaviors are accurately detected using body features, while the four mirror-checking detections are difficult to identify."
"we have presented a generalized reconstruction algorithm (gpl-bc) capable of utilizing a variety of high fidelity cbct system models, which may include focal-spot blur, scintillator blur, and correlated noise. we evaluated this method in a scintillator blur dominated scenario in simulation and on a prototype cbct test bench. these studies show that high fidelity modeling with gpl-bc can improve resolution and produce more accurate reconstructions as compared to more traditional models and fdk approaches. the improved accuracy of the trabecular bone segmentation and tb.th. measurement suggest that gpl-bc can increase the accuracy of quantitative metrics used to study trabecular bone health [cit] . additionally, the improved bias/noise trade-off suggests that gpl-bc produces more accurate attenuation values than dfdk and gpl-i, which is critical for quantitative ct [cit] (however, note that bias includes both attenuation value error and blurring)."
"for all k (i.e., the g matrices are equivalent to within a scale factor). additionally, because each l k is circulant (ignoring boundary conditions), b s is the weighted sum of circulant matrices, and is therefore itself circulant. thus b s represents a convolution matrix."
"second, the appropriate representation of stimulus color depends on the number of cone classes present in the mosaic. in humans [cit] (and also new world primates [cit] ) there is individual variation in this number, primarily in the form of red-green dichromacy. thus it is also of interest to investigate whether learning can detect the number of longer-wavelength-sensitive cone classes present in an individual retinal mosaic."
"as discussed before, the anonymous bonus point system is based on chaum's electronic cash [cit] . we identify three essential actors: many customers, like alice, buy goods and services at several merchants, two of which are bob and charlie. these merchants, in turn, participate like alice in a customer loyalty system operated by the bonus point issuer debbie. in the literature about electronic cash [cit], these actors are often referred to the payers, payees, and the issuer, respectively. figure 1 shows the life cycle of a bonus point. a simple, first idea of how to use chaum's electronic cash for this use case is sketched in the following."
"asymmetric cryptographic keys can be used to implement a proof system where customers prove their knowledge of the bonus point secret to debbie, the central database operator. proving knowledge of the point secret can be used by legitimate point owners to solve disputes where merchants maliciously reject unspent points, thereafter claiming their value themselves. to implement this fraud prevention, we propose to change the point creation process to introduce the point secret. more specifically, we replace the random bonus point id by an asymmetric key pair, for example from the elgamal cryptosystem [cit] or the rsa cryptosystem [cit] . the bonus point id consists of the public key component c pub of an asymmetric key pair, as displayed in figure 3, while alice keeps the associated secret key c secret . alice generates this key pair when she wants to generate a bonus point. the secret key remains secret at all times and is held by the wallet on alice's device at least until the spending transaction is cleared by debbie's database."
"dichromatic mosaics are also correctly detected by our learning algorithm ( figure 5a-5d ), but more easily. this classification relies partly on an arbitrary choice of a confidence threshold (pv0:01) regarding the goodness-of-fit of the flattened z positions of the embedded correlation matrix to a single skew normal distribution, beneath which we reject dichromacy as an explanation of the retinal mosaic. although this threshold is arbitrary, it represents a requirement that we be 99% confident that a distribution is not dichromatic in order to consider it trichromatic. considering our algorithm's good performance on trichromatic mosaics, this implies that the dichromat/trichromat decision is one that the visual system could make reliably."
"reconstructions were assessed with three metrics: bias, noise, and maximum jaccard index (mjac) [cit] . bias and noise were chosen as traditional image quality metrics, while mjac was picked as a metric specific to trabecular bone analysis. these metrics were calculated for the set of line pairs in the middle of figure 1 . the terms are defined based on the truth image t (binned to match voxel size), reconstructions of noiseless dataμ nl (β, δ), reconstructions of noisy dataμ(β, δ), and the number of voxels in the roi (n roi )"
"input images for simulations were taken from three hyperspectral image databases, details of each of which can be found elsewhere [cit] . the images in all three databases sampled scenes along at least 31 evenly-spaced wavelengths. the sampling depended on the database. all contained data between 420 and 700 nm at 10 nm steps; across databases we had data between 400 nm and 720 nm. of the 33 total wavelengths available across all three databases, we extrapolated quadratically to the missing two (either 400 and 410 nm or 710 and 720 nm) when necessary so that the combined database specified the spectrum at each pixel in each image at 10 nm intervals between 400 and 720 nm. although there were fewer than 100 images in the three databases, the simulated mosaics were sufficiently small and the images sufficiently large that we were able to sub-sample the full images to produce many possible image patches. for each simulation, n by n image patches were randomly chosen from the image database. for the value of n~20 used in our main simulations, the database contained *82:5 million distinct image patches with overlap allowed. unless otherwise specified, each simulation used a different set of natural image draws. image patches were drawn randomly with replacement. for our main calculations, we simulated the response of each specified mosaic to 2 million image patches. in supplemental simulations, we explored the effect of varying the number of image patches used ( figure s6 ) as well as the size of the patches ( figure s5) ."
"generally, we observe that bonus points can have many properties of traditional cash. bonus points are emitted by a set of cooperating merchants that want to redeem their customers' loyalty. they can be accumulated, (possibly) transferred to other users, and finally used for buying goods and services at participating merchants. whether these goods and services are restricted to a certain set of items or whether the bonus points can be used as a real means of payment does not affect the technical design of the bonus system. anonymous bonus points also have to be untraceable, but also capable of preventing double spending. these two features are also present in cash money. therefore, a bonus point system based on some form of electronic cash is a manifest option. the following analysis of anonymous payment systems is meant to provide an overview of potential ways to implement such a bonus system. non-anonymous electronic payment systems like, e. g., those found in online games like world of warcraft [cit], will not be discussed, as we aim for a privacy-preserving bonus point system with anonymous participants."
"point generation has no effects on the performance of the protocol. while chaum proposes to use a truly random number, our proposal of applying a public key generation function to produce the key pair with secret c secret also produces a seemingly random numerical public key c pub . that number is treated exactly the same way as the random bonus point id has been used in the original version of the protocol. thus, while message size is slightly increased [cit] bit instead of 128 bit per message), it is still a constant overhead per bonus point."
"where t p is the number of correctly detected distracted cases. p is the total number of distracted cases, which is the total quantity of the three distracted cases. f p is the number of false detections. in this case, it represents the number of normal driving tasks that are classified in the abnormal driving group. finally, n is the total amount of normal driving cases. according to table iv, ffnn binary classification outperforms the other four models, indicating that the ffnn is a powerful model suitable for driver behavior modeling. note that there are no optimization algorithms used in the other four models. these models are used with their default setup in matlab. the rf is constructed with 100 decision trees, and support vector regression uses a radial-based kernel. better results may be obtained with parameter tuning and optimization; however, this is beyond the scope of this paper. the binary classification model is able to distinguish normal driving behavior and distracted behavior. from the perspective of safety, although it may annoy the driver, it is safe to classify normal driving behavior into distracted behavior and warn the driver. on the other hand, if the model classifies distracted behavior into the normal driving group, it is more dangerous than the previous case, and this misclassification should be avoided. in the real world, in terms of nondriving tasks, the time constants are always much longer than normal driving tasks, and texting or answering a phone can last for a few minutes. however, the mirror-checking actions usually last for 1-2 s. these time properties of the different tasks can be adopted to predict the correct states in the future."
the kinect data processing methodologies used in this paper are described in this section. the two data processing steps are head rotation calibration with an orientation sensor and noise removal and smoothing based on a combination of a median filter and an exponential filter.
where p l0 is the pathloss at the reference distance d 0 and n is the pathloss exponent. the first component is the signal attenuation due to the separation between tx and rx. shadow fading is caused by shadowing processes and leads to a change in the local mean of the path loss over relatively large distances. whereas the small-scale fading is the variation due to the superposition of multiple propagation paths. it leads to power variations when moving over relatively short distances in the order of the signal wavelength. the small-scale fading is extracted by applying the aforementioned sliding window.
"we fitted the empirical autocorrelation function to three theoretical autocorrelation function models, the first model is the classical model proposed by gudmundson [cit] which is based on a single negative exponential function,"
"bonus point system issuers like payback [cit] in germany provide a bonus system as a service to merchants with interchangeable bonus points in exchange for aggregated customer data over the entirety of all participating merchants. individual profiles of customers generated through these systems are often used as the foundation for in-depth market analysis and targeted advertising. due to the nature of these systems, there is usually full transparency of the customer for the bonus point issuer, i. e., the issuer can track the users' complete purchase histories. while privacy-preserving data mining approaches are widely known [cit], existing bonus point systems typically use classic ways to conduct their analysis. from the accumulated data, even more sensitive personal information may be derived, e. g., if a customer is on a specific diet or if they are of a specific sexual orientation [cit] . there are also state-level adversaries with potential access to the issuer's database."
"the fact that the first step in the evolution of trichromatic color vision was the expression of a third cone class leads to two important questions. first, how can a post-receptoral visual system detect the presence of a new cone class? second, how can such a system classify individual cones according to their spectral class, so as to take advantage of the information carried by the newly expressed opsin? more generally, the question of what information enables the brain to initially detect and differentiate the presence of novel sensory transducers applies to any sensory modality which provides a multidimensional perceptual representation. here we provide a computational treatment of these fundamental questions in the context of primate color vision and show that the inter-cone correlations induced by natural images may be used to detect when a mosaic contains three rather than two cone classes and to accurately classify l versus m cones."
"hypertension could be looked at as a set point shift due to the influence from higher level pathways, say, increased pressure because of a tumor, or small edematous areas originated in inflammation processes which, in turn, might be linked to autoimmunological reactions. other causes, which might be termed as behavioral, are amenable in this context, as for example, the lateral septal area has been reported to modulate autonomic responses to stress and emotional situations [cit] . since baroreflex parasympathetic component is suppressed during stress [cit], this area could be modulating the baroreflex parasympathetic component during defensive stress situations leading to an increase in blood pressure."
"across individuals, human retinal mosaics vary in two key ways. first, there is substantial variation in the l:m cone ratio, from approximately 1:3 to 16:1 [cit] . second, polymorphisms in the genes that encode the l and m cone opsins lead to variation in the wavelength of peak sensitivity (l max ) of the l and m cones; individuals possessing l or m opsins with atypical spectral sensitivities are referred to as anomalous trichromats [cit] . the existence of these variations motivated us to study how the ability to classify l and m cones depends on the l:m cone ratio and on the separation between the l max values of the l and m cones. figure 4a shows the results. each square in the figure indicates the mean percentage of cones correctly classified for a particular choice of l:m cone ratio and particular separation of l and m l max across three simulations. the results for individual simulations are given in figure s4 . although both l and m cone l max can vary across individuals, we parameterized differences by holding the l cone l max fixed at its typical value of 558.9 nm and varying the m cone l max in 5 nm steps between its typical value of 530 nm and an upper bound of 555 nm. the figure shows that the algorithm is generally robust, but that classification performance falls off at the more extreme cone ratios. approximately 16:1 is the largest reported l:m ratio for individuals whose color vision is normal by standard tests [cit] . algorithm performance also degrades with respect to decreases in the l max separation between the l and m cones."
"the art-based algorithms exhibit a high computational complexity due to the repeated forward and back projections. thus we introduce and evaluate the fdk-jbf method (algorithm 2) as a computationally much faster volume-based denoising strategy: in steps 1-2 an fdk reconstruction with a sharp filter kernel of the baseline and the contrast-enhanced sweeps is applied to compute the mip in steps 3-4 and to create initial volumes with high data consistency. then n v iterations (step 5) are carried out, applying the jbf on all reconstructed volumes (step 6), followed by recomputing m from the filtered volumes (step 7)."
"according to the world health organization, traffic accidents take about 1.35 million lives and cause more than 50 million injuries each year on the roads of the world. [cit], vulnerable road users (vrus), including pedestrians, cyclists and motorcyclists, accounted for almost half of the road victims [cit] . it has been found through experiments and simulations that intelligent transportation system (its) applications have been successful in increasing road safety [cit] . however, vrus received less attention than vehicles. most recent studies which address vru protection focus on driver assistance systems that only rely on on-board perception sensors to detect and locate other surrounding traffic participants [cit] . however, radar sensors, laserscanners and camera-based systems have critical limitations due to the need for line-of-sight (los) and the weather-related performance. to enhance safety and mobility of vrus, vehicle-to-pedestrian (v2p) communication enables a collision avoidance system by establishing direct communication between vehicles and vrus. using v2p communication can improve mutual detection, localization and tracking of both"
"where p t, p r, g tx, and g rx are the transmitted power, received power, tx antenna gain and rx antenna gain, respectively. the path loss consists of two components, namely, the distance-dependent pathloss model and shadow fading. this is expressed in logarithmic scale as:"
"results for simulations without a suppressive surround are given in figure s7 . a comparison with figure 4 shows that the presence of a randomly-wired surround enhances learning performance slightly relative to no surround while smaller retinal mosaics result in slightly worse learning. additional simulations, run using a retinas with the same parameters as those shown in figure 4 but such that the suppressive surround was cone selective (i.e., l cone surrounds were made only of m cones, m cone surrounds were made only of l cones), were also performed ( figure s8 ). interestingly, the presence of cone specific opponent surrounds early in the visual pathways impedes the performance of our algorithm."
"our algorithm correctly detected a tetrachromatic mosaic and did a reasonable but not perfect job of classifying the cones for this mosaic, indicating that there is sufficient information in the correlation of cone responses to natural images to support a degree of tetrachromatic learning. although it is established that certain female carriers of anomalous trichromacy express four cone classes, whether these individuals exhibit tetrachromatic color matching performance has been difficult to determine [cit] . as noted in the results section, our cone classification results were reasonable but not perfect, suggesting that one reason for the lack of routine functional tetrachromacy in female carriers may be that it is difficult for the post-receptoral visual system to reliably classify all of the cones [see also 7] . that is, it may be that rather narrowly delineated conditions must be met for the visual system to learn that its mosaic contains a fourth cone type and classify the cones with sufficient accuracy."
where r is a penalty function which returns a scalar and β is the penalty strength. the weighting matrix w in (3) is the inverse of the covariance matrix k in the forward model (2) .
"so, message size is still in o (1) . when a bonus point id is entered into the issuer's central database it does again not matter if the reported id is a public key derived from the actual random id or the id itself. the issuer has to store these numbers indefinitely in both cases and merchants can check for their validity when accepting bonus point payments, so that database is growing within o(n)."
"the human visual system encodes color by comparing the responses of three different kinds of photoreceptors: the long-(reddish), medium-(greenish), and short-(bluish) wavelength-sensitive cone cells. in order for the visual system to accurately represent the color of stimuli, it must (in effect) know the class of the cone that produced each response. the long-and medium-wavelength-sensitive cones, however, are virtually identical in every known way except that their responses to a given spectrum of light differ. here, we simulate cones in a model human retina and show that by examining the correlation of the responses of cones to natural scenes, it is possible to determine both the number cone classes present in a retinal mosaic and to explicitly determine the class of each cone. these findings shed light on the computational mechanisms that may have enabled the evolution of human color vision, as well as on the more general question of whether and when it is possible for sensory systems to self-organize."
"the embedding shown in figure 3 separates the three cone classes along the z (l max ) dimension of the representational space. the positions of the s cones are highly distinct from those of the l and m cones. the l and m cones also separate well from each other, although in other simulation conditions there is overlap in the z positions of these two cone classes (see, for example, figure s3 ). in most such cases, however, the l and m cones are actually embedded along separate curved surfaces within the full-three dimensional representation, but the curvature of these surfaces produces overlap in the z positions of l and m cones. the curved surfaces may be flattened in a manner that preserves the local planar structure of each (see methods). the result of applying the flattening procedure to the embedding shown in figure 3b is provided in figure 3c, where the separation between the l and m clusters remains apparent. to classify cones, we extracted the z position of each cone in the flattened representation and fit the resulting distribution as the mixture of two skew normal distributions ( figure 3d ). we then used the parameters of the two distributions to determine a threshold along the flattened zaxis, and labeled all cones to the left of the threshold as m and all cones to the right as l. the threshold was taken as the point between the means of the two fit distributions where their probability density functions were equal. note that the labeling of l versus m is not arbitrary, but rather is determined by the position of the well-separated s cones, which are themselves classified algorithmically through a preprocessing step (again see methods). this algorithm classifies 100% of the l and m cones correctly from the correlation matrix, for the mosaic shown in figure 2a ."
"fraction (bv/tv) [cit] . bench data were acquired at 90 kv and 90.7 ma s. the µct data were acquired on a skyscan 1172 ct scanner (bruker microct, kontich, belgium) at 65 kv. to find the \"true\" trabecular bone segmentation with the same voxel size as the reconstructions, the µct image of the trabecular bone was first binned from 0.0076 mm voxel and then registered with an fdk reconstruction of the qcbct bench data. the registration algorithm also reduced the voxel size of the µct image to match that of the fdk reconstruction (and therefore the model-based reconstructions). the resulting image is referred to as µctmv for matched voxel size. the elastix software package [cit] registered the images using the binned µct reconstruction as the moving image, a similarity transformation, and the mutual information metric. a mask was used to limit the evaluation of the registration metric to a sub-volume containing only trabecular bone. the µctmv image was thresholded to generate the \"truth segmentation.\" the threshold value was chosen using a visual histogram inspection. the fdk, gpl-i, gpl-b, and gpl-bc reconstructions were thresholded at 101 equally spaced attenuation values between 0 mm, inclusive, to calculate mjac. the mjac metric was only computed within the trabecular region (using the same mask as the registration). this metric was plotted for each mbir reconstruction method as a function of regularization strength. the most accurate segmented reconstruction from each mbir method was selected as the one with the highest mjac over all regularization strengths, and the most accurate reconstruction was selected as the corresponding pre-thresholded image. the optimal fdk segmentation was defined as the one with the highest mjac over all threshold values. a tb.th. map was calculated from the optimal segmented reconstruction for each reconstruction method and the µctmv image. tb.th. and tb.sp. were calculated with bonej [cit], a plug-in for imagej [cit] . average tb.th., tb.sp, and bv/tv were computed over the area defined by the registration/mjac mask. the tb.th. and bv/tv of the original µct image (before binning and registration) were also calculated using the same mask (transformed to the µct coordinates). (tb.sp. was not calculated for this image due to computation constraints.) slices of the µct scan and µct tb.th. map were transformed using the registration parameters calculated previously, facilitating visual comparison to the other methods. optimal reconstructions, optimal segmentations, and tb.th. maps for fdk, gpl-i, gpl-b, and gpl-bc were compared with corresponding µctmv and original µct images."
". a bias/noise plot and a plot of mjac as a function of β were analyzed for the center set of line pairs in figure 1 and each of the four reconstruction methods. for direct visual comparison we present reconstructions of the line pairs, along with the corresponding optimum segmentations."
"we constructed simulated cone mosaics using parameters typical of human vision. figure 2a illustrates one of our simulated mosaics. the assumed spectral sensitivities are shown in figure 2b . we simulated the responses of each cone in the mosaic to 2 million image patches sampled randomly from our full hyperspectral image dataset and computed the 400 by 400 correlation matrix between the responses of all pairs of cones in the mosaic. because the distal retina forms spatially antagonistic receptive fields [cit], our main analysis used simulated cone responses that incorporated an opponent surround that draws on neighboring cones irrespective of their type (see methods). figure 2c depicts one row of the full correlation matrix in image format, with the gray level of each cone in the panel indicating the correlation between that cone and the single m cone shown in green. two previously observed features of the correlation structure may be seen in this representation [cit] . first, correlation in cone responses decreases as a function of spatial distance. second, once distance is equated, correlations between cones of the same class are higher than correlations between cones of different classes ( figure 2d and 2e). these two features are induced by the spatial-spectral correlation structure of natural hyperspectral images ( figure 1b )."
"to time synchronize the gnss positions with the high rate channel sounder measurement data cubic spline interpolation has been used on the positioning data. the displacement between gnss and the tx antenna of the sounder, as shown in fig. 3, was taken into account when computing the position of the tx antenna, i.e. a correction is performed to shift the gnss position by distance d and angle θ. in a similar way, the calculation of the rx antenna position based on the gnss position is performed."
"primate color vision is initiated when light spectra entering the eye are encoded by three classes of retinal cone photoreceptors: the long-(l), medium-(m), and short-(s) wavelength-sensitive cones. trichromatic encoding alone, however, is not sufficient to support functional trichromatic color vision. in addition, the information encoded by the cones must be preserved by postreceptoral neural circuitry in a manner that enables both color discrimination and the representation of stimulus color."
"first, the driver head and body signals captured with a kinect are very noisy. sometimes the detection is less precise, and the detected joint positions are shifted and unreasonable. this phenomenon is particularly worse for the seated driver inside the vehicle. in this paper, a simple integrated tracking and smoothing technique is used, which consists of a jitter removal filter and an exponential filter. although the integrated filter can recover unreasonable detection and smooth the signals, important information can be lost, and the filter can be further improved by using more advanced filters, such as the kalman filter or a particle filter for joint position tracking. therefore, the quality of kinect signals, as well as the model detection results, can be further improved. moreover, in this paper, only color and depth images are collected; however, kinect also supports audio recording. therefore, in the future, audio information in the cabin can be captured as another important data source to assist in the detection of nondrivingrelated tasks."
"first, we develop an algorithm that uses inter-cone response correlations to explicitly classify cones by their spectral type. although such explicit classification is not a necessary feature of the brain's implementation of visual processing, it allows us to quantify the degree to which unsupervised learning can differentiate cones by type to support color vision. this quantification allows us to explore how classification performance depends on key parameters of the mosaic, for example particular on the number of cone types, the ratio of l to m cones, and the spectral sensitivities of the different cone classes."
"furthermore, central nervous mechanisms modify baroreceptor sensitivity and, thereby, mediate the resetting of baroreceptors [cit] . in all studies in which these receptors were denervated, blood pressure oscillations increased significantly while mean arterial pressure (map) did it only for a few days [cit] . even when both carotid and aortic baroreceptors and chemoreceptors were removed, map increased only for a short period of time [cit] . one possible conclusion was that sensory-neural transduction is not involved in the long-term regulation of blood pressure."
"when paying with a coin in such a system, the user hands over the coin id and the signature to the receiver of the payment. the receiver can check offline whether the signature is correct (given he knows the public key of the central issuer). an online check is required to prevent double spending: the issuer is notified of the secret id the user spends and this id is listed as \"spent\" by the central issuer. if a second transaction using the same id is attempted, the central issuer can tell the receiver to not accept the already spent coin. the downside of that approach is the requirement to have an online connection to the central issuer's system at the time of the payment."
"however, small differences can be noticed in case of authorization receipts. at the time of transaction every bonus point has to be accompanied by an authorization receipt, i. e., one more message exchange per point between merchant and customer for generating the receipt. this leads to a number of exchanged messages in o(n), i. e., a constant message overhead per bonus point. when processing a transaction using authorization receipts, every merchant has to store such a receipt for any point involved, thus holding a database with linear growth."
"the procedure taken to construct a behavior recognition model is described in this section. the driver monitoring system architecture is shown in fig. 1 . the general structure of this paper consists of three parts. first, the driver head and body data are collected and time stamped. then, the signals are smoothed, and the noise is filtered. second, feature importance prediction is proposed using a combination of rfs and mic, and the feature importance given by the two algorithms shows strong consistency. the \"model selection\" block in fig. 1 processes the feature evaluation based on the feature importance provided by the \"feature importance estimation\" block. meanwhile, the influence of depth, head, and body features for the driver status detection will be studied. then, the real-time driver behavior identification will be conducted using an ffnn model withleave-one-out (loo) cross validation. finally, the performance with different features is analyzed, and a behavior classification performance comparison between different algorithms will be proposed."
"we build on previous work, some available only in abstract form, that considers the concrete question of how learning might produce model neural units whose wiring differentiates signals from l and m cones [cit] . importantly, this work shows that there is indeed information in the cone responses to natural images that can differentiate l and m cones to some degree, in the sense that learning enabled above-chance formation of cone-class specific wiring of model units. here, we extend the previous work in two ways."
"it is well known that in the neural control circuit of the circulatory system some cardiovascular variables are fedback by arterial and cardiopulmonary receptors. arterial receptors are of two types, baroreceptors, that is, stretch structures located in the walls of the carotid sinuses and the aortic arch, and chemoreceptors located in the carotid and aortic bodies. today, it is generally accepted that short-term blood pressure (i.e., seconds to minutes) is regulated by a negative feedback loop and that the information from the baroreceptors is very effective to stabilizing such changes (say, for example, during orthostatism). thus, these receptors are the major sense organs which reflexly control systemic arterial blood pressure. historically, the french physiologist marey, in 1859, was the first to recognize the inverse relationship between arterial pressure and heartrate (known as marey's law). around 1861, j. b. a. chauveau developed a procedure (chauveau-marey maneuvre) to manually introduce a sudden blood pressure step to trigger the reflex compensatory heart rate response [cit] ."
"based on the fact that denervation of all the cardiovascular receptors (baro, chemo and cardiopulmonary) provoke sustained hypertension, we conclude that mean long-term blood pressure is regulated by the nervous system. we analyzed the cardiovascular neural circuit, particularly the open loop and the feedback loop closed by cardiovascular receptors. the nts is the only structure that receives information from its rns and from cardiovascular receptors and projects to nuclei that regulate the circulatory variables. there is also a secondary feedback loop closed by the lrn, but without showing salient importance in the long-term regulation, as remarked above in section 4. when the nts is injured, map cannot stabilize. on these bases and from a control theory point of view, we showed that this nucleus has the emergent property of a comparator and its afferents from the rns provide the set point, which determines mean arterial pressure. thus, the baroreflex would stabilize the instantaneous pressure value to the prevailing carotid pressure (map). in this way, the long-term control of arterial pressure occurs independently of arterial baroreceptor input. such result is in agreement with osborn [cit] ."
"other approaches for a distributed bonus system [cit] reward customers for recommending merchants to other customers. the authors propose a mobile system where bonus points can be transferred from one person to another and redeemed at any participating merchant. however, these systems are not anonymous and users can be identified with their central accounts."
"albeit, detecting which merchant is the one cheating is still an unsolved problem: debbie relies on alice's report of where she has unsuccessfully tried to spend the stolen bonus point."
"is a constant, which can be ignored for the purposes of optimization. equation (a.4) is written using summation notation to highlight it's separability. the first surrogate function, q, is q x expressed as a function of the line integrals l:"
"assuming that b and w are chosen such that η is positive, the optimal curvatures (e.g., those producing the widest surrogate function and, thus, the largest step size) are [31, eq. 28]"
"denervation of baroreceptors and chemoreceptors does not open the negative feedback loop, because there is another kind of receptors, the cardiopulmonary ones. these are mechano or stretch receptors located in the heart chambers and in lungs [cit] . cardiopulmonary afferent pathways seem to especially influence those neuron pools supplying the renal resistance vessels, whereas their action on those fibers to skeletal muscle resistance vessels is less pronounced. these receptors cannot sense rapid fluctuations in arterial pressure as arterial baroreceptors do. [cit] . the acute cardiovascular denervation by cold block or acute dissection of these receptors would increase arterial pressure for only a short period of time, as seen after arterial baro and chemo receptor denervation [cit] . only by cardiopulmonary and arterial receptor denervation would the negative feedback be open. after combined denervation, sustained hypertension was found, as well as a large fluctuation in arterial pressure that characterizes arterial receptor denervation [cit] ."
"the voucher then serves as a form of payment for the blind signature to ensure that an eligible transaction grounds the issued blind signature. consequently, when alice asks debbie to sign a bonus point, she has to provide a valid (and yet unused) voucher in exchange. moreover, debbie can bill bob for the exact amount of bonus points originating at his store, simply by billing bob for the vouchers that are issued for him."
"to visualize and exploit the information about cone class carried by the full correlation matrix, consider a three-dimensional representational space where the first two dimensions, call them x and y, provide the spatial position of a cone in the mosaic and where the third dimension, call it z, indicates spectral class. to conceive of z as continuous, we treat it as indicating the wavelength of peak sensitivity, l max, of each cone. because the correlation between two cones decreases both with increasing spatial distance and with increasing l max difference between them, the negative log of the correlation matrix may be thought of as a proxy for the distance between cones in this abstract representational space."
"after alice's proof of point ownership, debbie can reimburse the customer. any other participant in the system cannot provide the signed response to the challenge without breaking the underlying crypto-system. additionally, customers still cannot generate additional bonus point ids, because valid bonus points still carry debbie's signature."
"to limit the effect of undersampling we acquire data in half angle increments (double the sampling of traditional cbct). in summary, we believe the choices of voxel size and angular sampling in this work are appropriate for the system blur studied, and allow a fair comparison of the different mbir system models."
"in this section, we survey existing literature related to our proposal. after introducing the state of the art in electronic bonus point systems we discuss the applicability of existing decentrally and centrally operated electronic payment systems and show where our proposal differs from existing ideas."
"when calculating the loss due to diffraction from the first edge, the second edge is considered as a receiver and then to calculate the loss on the second edge, the first edge is considered as a transmitter and the third edge as a receiver and so forth. hence, modeling the diffraction loss requires computing the geometrical parameters d 1, d 2 and h for the diffraction process at each edge. this can be done by accurately modeling the size of the objects and their distribution in the propagation environment. to this end, we have developed a 3d tool as illustrated in fig. 8, which comprises the following steps: (a) modeling static objects in the environment is done by importing the objects dimensions and locations from the laser scanner measurements and the positions of tx and rx from the gnss measurements. the vehicles are represented by a cuboid and the vehicle front-end is represented by a triangular prism. (b) a direct ray from the tx to the rx is created, then intersections between this ray and the objects in the environment are detected as illustrated in fig. 9 . (c) if the ray intersects with an object, two intersection points will result, entering point and exiting point. the diffraction points are found by projecting the intersection points on the roof of the object. for simplicity, only roof diffraction is considered. (d) the exiting point is then treated as a secondary transmitter and the previous step is repeated to check further intersections with other objects and find the other diffraction points. (f) the diffraction loss is then calculated at each diffraction point and summed up to get the total diffraction loss. (g) the diffraction loss is combined with two-ray pathloss model or a fspl to get the total propagation loss."
"although the sigmoid function will cause the loss of a gradient problem in most scenarios, it is not a serious problem in this shallow network case. in this case, the supervised ffnn is trained with driver head and body signals as the input and an output of the corresponding behavior among the seven actions. unlike some existing research that uses time-series models, the ffnn used in this paper does not consider the previous step status of the driver. the reason for this is that humans can normally distinguish the current driving behavior using one image and do not require video sequences. unlike the inner mental states of the driver, which is a long-term process and depends on previous states, the outer behaviors can be considered a transient state and are not highly dependent on prior information. therefore, the ffnn is applied to detect the driving tasks frame-by-frame based on the collected driver body information."
"the main limitation of the objective function presented is the application of the inverse covariance matrix, which may be computationally expensive if noise correlations are modeled."
"performance for simulations that included a cone-specific surround ( figure s8 ) was lower than for simulations in which the surround drew uniformly on neighboring cones. although the algorithm classified 100% of the cones correctly in the case of a retina with an l:m ratio of 1 and an m cone l max value of 530 nm, and also correctly detected that there were two longerwavelength-sensitive cone classes present in the mosaic, the algorithm struggled with many other simulations with cone selective surrounds. in fact, the embeddings of these simulations were less organized than other simulations. although a full analysis is beyond the scope of this paper, one feature of cone selective surrounds of fixed spatial extent in mosaics with extreme l:m ratios is that the number of cones contributing to the surround will be different for units with l cone centers than for cones with m cone centers. it is possible that this produces variability that impedes learning."
"a few complementary considerations with good back up are pertinent. in adult animals, rostral neural structures to the nts modulate the feedback loop responses [cit] . descending inputs from the hypothalamus and other supramedullary regions are activated as part of the response to an alerting or stressful stimulus; [cit] . the lateral hypothalamus modulates cardiovascular variables in different behavioral situations as well as the responses to electrical stimulation on structures of the feedback loop [cit] . activation of the pvn causes inhibition of the baroreceptor reflex, as occurs in conditions where sympathetic activity is chronically increased, such as heart failure. it is interesting to note that the nts mediates that inhibitory effect on the baroreceptor reflex [cit] . let us recall that responses from the baro and chemoreceptor blood pressure system can be elicited either by mechanical, say aortic or carotid compression, the former being illustrated by the old chauveau-marey maneuver [cit], or electrical at the level of hering's nerve or higher up in the different neural pathways. as examples, the lateral septum also modulates some cardiovascular responses to feedback loop perturbation, such as the bilateral carotid occlusion or the electrical stimulation on the ventrolateral reticular formation [cit] . moreover, the amygdala plays a special role in the regulation of the cardiovascular system during specific behavioral stress [cit] . the cerebellum is another structure that contributes to the neural regulation of blood pressure [56, [cit] . the fastigial nucleus does not affect the cardiovascular variables in resting condition, but it plays a modulation role during exercise [cit] . moreover, secretions from the adrenal medulla have profound cardiovascular influences. with regard to sympathetic neurons, there are descending pathways to the preganglionic neurons of the adrenal medulla, which stem at hypothalamic, midbrain, pontine and medullary cell groups [cit] ."
"where mi c is the mutual information for two continuous vectors, and p(x, y), p(x), and p(y) represent the corresponding probabilistic density functions. as shown in (2), calculating the mutual information of continuous variables is difficult. therefore, the mic technique, which concentrates on the optimal binning method, is applied to assess the mutual information of the continuous case. meanwhile, mic enables the mutual information score to be normalized into the range [cit], which makes assessing the dependence and corelationship between two variables more convenient. in this paper, in addition to the first three continuous head rotation angles, the remaining features are discrete image coordinates and depth values. therefore, the mic can be efficiently used for feature association prediction."
"the issuer however performs its role exclusively in the bonus system and gains income only if the system itself performs well and without complaints by either customers or merchants. thus, the stakes of losing customers' or merchants' trust are high and substantiated reports of fraud have the possibility of killing the issuer's entire business model instantly. the customers' main assets that have to be protected are their privacy and the integrity of their earned points. in contrast, the merchants' goals are usually an uninterrupted operation of the system and the prevention of fraud through customers against them. both parties put significant trust into the issuer while constantly observing its operations in order to check if the system still performs towards their goals. therefore, we assume an honest-but-curious attacker model for the issuer because we are convinced that a malicious issuer would not be operating for long. as established above, merchants and customers can stop participating as soon as they are convinced the operator is no longer acting according to their interests. using this model, we can exclude attacks where the issuer issues bonus points without receiving a valid voucher. the protocol requires that the vouchers are always checked and every blindly signed bonus point can be accounted for. on the other end of the protocol the issuer could deny a valid deposit of a valid bonus point spent by an honest customer at an honest merchant. again, the chosen model prevents this attack as the issuer is assumed to be acting according to the protocol. subsequently, the issuer can gain no advantage in the system without damaging its own source of revenue."
"in order to develop a reliable v2p communication system, accurate channel models are of immense importance. vehicle-to-vehicle (v2v) and vehicle-to-infrastructure (v2i) channels were extensively studied in the last years, and various channel models were proposed. however, to the best of our knowledge, a dedicated v2p channel model in critical scenarios does not exist yet. to derive accurate models, v2p channels have to be thoroughly investigated. these channel models should be able to reproduce reliably and with low complexity the time-variant behavior of the channel characteristics. since the vehicle is a common element in v2v, v2i and v2p, some similarities could arise in the propagation channel. however, also important differences are identified. assuming that a safety system based on v2p communication is incorporated in the pedestrian's smartphone, the following characteristics will have a direct impact on the v2p propagation channel:"
"the emergence of distinct l and m cones in primates is a recent evolutionary event [cit], and the only known difference between these cones is the photopigment opsin contained in their outer segments [cit] . these observations make it unlikely, given current knowledge, that the formation of appropriate postreceptoral circuitry can rely on biochemical markers that distinguish l and m cones and suggest instead that this circuitry must be learned."
"to investigate the performance of the proposed algorithm on physical data, a human iliac-crest bone-biopsy core was scanned on the test bench described in §ii-c. the bone sample comprised both trabecular and cortical bone. b was modeled as described previously (6), with b s and b d representing applications of the models developed in §ii-c. blur matrices were applied functionally as in the simulation study. b d was applied using fourier methods and b s was applied using convolution. the covariance approximation (10) was used. g was a matrix which scaled the values of each pixel by the estimated bare-beam photon flux and each frame by a normalization factor. 4 the projection operator a used the separable footprints algorithm as in the simulation study. the gpl methods used the same readout noise value as the simulation study."
"to calculate the diffraction loss over multiple knife edges, the epstein-peterson method is used [cit], which is illustrated in fig. 7 . this method is found to give the best results for the given geometry. in this method, the total diffraction loss is the sum of the k losses on all edges: d 2, h b ) . similarly, for edge c, the geometrical parameters"
"where new symbols are defined in table iii . this model samples the focal spot into sourcelets and the pixels into subpixels. the vector of attenuation values µ is forward projected by each sourcelet (multiplied by eachã k ), resulting in an individual subpixel line integral for each sourcelet. the negative exponent of each line integral is taken, and the result is scaled by a sourcelet specific gain termg k and summed over sourcelets. a subpixel level blur is applied (b), followed by conversion to pixel sized measurements by summing subpixels (s). while the presented algorithm is capable of incorporating this model directly, in this work we make a number of simplifying approximations. specifically, we don't model subpixels explicitly, and replace the multiple sourcelets model with a convolution operator. we note that these approximations may not be appropriate for all imaging scenarios, especially those with large fields of view."
"the pl reconstruction using this model (2) is formed by finding the volume, µ, that minimizes the objective (4) . note that (4) is non-convex."
"(1) fig. 4 . seven driver behaviors studied in this paper. the first row shows normal driving, right-mirror checking, rear-mirror checking, left-mirror checking, and using video devices. the second row shows mobile phone texting and answering a mobile phone performed by multiple drivers. wherex n is the filtered data value, x n is the raw data, and x pre represents all the nonzero data before step n. the exponential smoothing filter is defined as"
"in this paper, the research scope is narrowed to the range of driving task recognition toward a normal driving and a secondary task monitoring system for lower level and middle level automated vehicles. according to previous studies, driver behavior can be classified into intended and nonintended behaviors [cit] . the intended behavior of the driver is the extension of the driver's mental thought, which can be used to infer the mental state and intent of the driver. in contrast, nonintended behaviors are usually caused by distractions due to outside and inside disturbances. driver behavior has been widely studied in the previous literature. general driver behaviors include the study of driver head pose [cit], eye gaze dynamics [cit], hand motions and gestures [cit], body movement [cit], and foot dynamics [cit] . this behavior information has been successfully used to estimate driver fatigue, driver distraction, driver attention, and so on. in this paper, driver head and upper body information detected using a kinect will be evaluated for normal driving and distraction identification."
"however, the blur matrixb requires an input of subpixel values. an up-sampling matrix may be included to estimate subpixel values from the pixel values. these approximations yield"
"the third block indicates the behavior detection using only body features. there are 30 total features used, containing the x-, y-, and z-coordinates of the hand, wrist, elbow, and shoulder joints. as shown in table v, ignoring the 3-d head pose features and the eyes and nose location information, the detector fails to identify the mirror-checking behaviors. by using only body features, the distraction behavior can be detected with a certain degree of accuracy, while the detection accuracy for the four mirror-checking behaviors is quite low. the worst case is the rear mirror-checking behavior, which only achieved 5.72% accuracy in general."
"we apply the proposed algorithm using a model of focalspot blur and scintillator blur, where the latter adds spatial correlation to the noise. both of these blurs can be represented as factors of the b matrix:"
"3) the spatial correlation of shadow fading is investigated and modeled. the paper is organized as follows: after a review of prior work in section ii, we describe the channel measurement setup and scenarios in section iii. section iv presents the model of the pathloss, the diffraction loss as well as the spatial correlation of shadow fading. finally, conclusions are drawn in section v."
"where d is the distance between tx and rx, and d is the distance between two observed positions. e [·] denotes the expected value of [·] . the estimation of the autocorrelation function of any random process requires having a large set of samples at every observation time t to calculate the statistical average or the so called ensemble average. by assuming ergodicity of a random process i.e., the sample average over time t for one realization (measurement runs) of the random process converges to the ensemble average as the length of the realizations tends to infinity, the autocorrelation function can be calculated by using samples collected from one measurement run."
"at any given voxel, the collection of sourcelet system matrices may be approximated as a single system matrix duplicated by the number of sourcelets, with each duplication left multiplied by l k, which shifts the output in the detector plane. starting from (d.3):"
"our proposed bonus point payment protocol does not remove any of the original functionality found in chaum's electronic cash system. vouchers are used both for accounting purposes in the system and for preventing customers to generate more bonus points than they are entitled to. they bear unique serial numbers, but these are of fixed length and can be managed centrally by the issuer. therefore, only a constant message overhead is necessary to transmit them from merchants to customers when committing a bonuspoint-eligible transaction. the transmission of vouchers from customers to the issuer during bonus point generation adds a constant overhead to every blinded bonus point, too. a typical voucher serial number is 16 bytes long, thus introducing 32 bytes of overall overhead when considering both transmissions from the merchant to the customer and from the customer to the issuer. the number of messages is either unaltered if the vouchers are generated by the merchants themselves or one message more per point than in the original approach if merchants request them on the fly from the issuer. thus, the number of messages with or without vouchers is in o(n) with message size o (1) ."
"while not a focus of this study, we note that incorporating blur into the model decreases the convergence rate. in order to compare nearly converged solutions, many iterations were used. (this is particularly important for regularization sweeps, as different regularization strengths may require different numbers of iterations.) however, we believe that tuning the subset/acceleration schedule can improve the convergence rate in practice. with the current (only partially optimized) implementation, the bench data reconstructions took approximately 10-15 min per iteration. (note the reconstruction volume was much larger than the roi shown.) when the roi is small, as in this work, a multi-resolution reconstruction method may be employed to decrease iteration time [cit] ."
"a particular benefit of our key-pair-based proof protocol is that alice never reveals her secret bonus point id. in particular, the secret is not even revealed when a fraud claim is filed. keeping the point id secret makes our approach resistant against attacks where the merchant bob and the issuer debbie collude to extract alice's point secrets."
combining asymmetric keys as ids and authorization receipts leads to a system where neither merchants nor customers are able to claim being cheated on due to the ability of the other side to prove their intent and knowledge at the time of the transaction.
"coming back to the problem pointed out above, a fraudulent merchant can, however, still illicitly gain a benefit by cheating on alice. when alice pays at charlie's with bonus points, charlie may instantly decline the transaction as invalid, while still gaining knowledge of the secret bonus point id including the corresponding signature of debbie. alice will only receive the response that the bonus point has not been accepted. she cannot check (let alone prove) whether the information received from the merchant is correct. that is, alice cannot check whether the bonus point is indeed not valid, i. e., has previously been used for a payment already, or whether charlie is lying by claiming the, actually unused, point has been used. charlie, in turn, is able to take the role of a customer at another merchant-say, bob-and use the illegally obtained bonus point to purchase goods himself. when alice attempts to spend her bonus point after the fraudulent merchant has done so already, the transaction will (rightfully) be declined as double spending. in this scenario, the bonus points are effectively stolen without alice being able to prove this. the goal of any process handling this problem is to create an atomic transaction that, if aborted, does not result in any useful knowledge gain at either side. a feasible approach, that we will demonstrate in our proposed protocol, is to use the principle of cryptographic knowledge proofs."
"second, in this paper, the feature selection and extraction methods are constructed based on rfs and the mic technique. this integrated method estimates the importance of the driver body features and the ffnn using these features achieved accurate detection results for some drivers. however, detection accuracy decreased significantly for the second driver for a few reasons. to obtain universal accurate task detection results, more drivers must to be studied in the future. increasing the data set volume and data diversity is an efficient way to solve the aforementioned problem. meanwhile, more driver features can be used. in this paper, only the position and depth information for the eyes are used. the driver gaze movement and the gaze tracking technique have been successfully adopted in some research on driver fatigue, inattention, and distraction monitoring. gaze information can be very useful when the drivers prefer not to move their body when performing mirrorchecking tasks."
"should alice's spending be rejected and she suspects fraud committed by bob, alice files a claim at debbie, as displayed in figure 4 . in the next step debbie authenticates herself and sends a random challenge to alice and asks her to sign it using the private key c secret associated with the signed public key c pub that was used in the disputed transaction. alice is supposed to append a random nonce to the challenge before signing it in order to prevent a chosen plaintext attack [cit] . generally, a chosen plaintext attack is an attack where the attacker forces the victim to sign or encrypt a specific message in order to gain knowledge on the structure of the victim's private key. if alice can provide a valid signature to the challenge, it is safe to assume she is in fact in possession of the secret key and has been cheated on by a merchant."
"before we take up the problem of the merchant being able to cheat on the customer, we consider the case of a potentially cheating customer alice. if alice is acting maliciously, she might send blinded bonus point ids to debbie that are not backed by any transaction at one of the merchants. obviously, we need a way for debbie to check that the bonus point creation is backed by a corresponding transaction at one of the merchants. this will go hand in hand with debbie billing the merchants for the issued bonus points-and all that without revealing alice's identity to any other entity."
"model-based iterative reconstruction (mbir) techniques have been shown to improve image quality in multi-detector ct (mdct) [cit] as compared to analytical approaches such as fdk [cit] . much of the advantage of mbir methods derives from the inclusion of a high-fidelity forward model containing both a model of the physical acquisition process and a mathematical formulation of measurement statistics. for example, the noise model informs the reconstruction algorithm about the relative information content of different measurements, allowing weights on the relative importance of these measurements in reconstructing the image."
"our system is based on blind signatures as proposed by david chaum [cit] in the context of e-cash. as we will show, applying chaum's ideas already get us a long way towards the above stated goals. however, there are also shortcomings, and we will point out where naïve applications of the e-cash payment protocol open up potentials for fraud. the usage of an anonymous payment system depends on reliable and instant fraud prevention, protecting both the merchants and the customers. a key contribution of this paper is that we show how these problems can be overcome."
"to evaluate our protocol, we define privacy and security requirements, and we demonstrate how our protocol fulfills these requirements. first, we identify incentives and capabilities for each group of participants in our system, focusing on requirements for user privacy and system integrity. we then discuss how each requirement is fulfilled by the proposed algorithms."
"some female humans possess genes that code for three longerwavelength-sensitive cone photopigments, and there is evidence that all of these photopigments can be expressed in separate cones [cit] . it is uncertain, however, whether these individuals process the output of their genetically tetrachromatic cone mosaics in a fashion that enables functional tetrachromatic color vision [cit] . we simulated a tetrachromatic mosaic and applied our classification algorithm to the resulting correlation matrix. the fourth cone class, which we will call a, was given a l max value of 545 nm, and the l:m:a ratio was 1:1:1. the algorithm correctly detected that this mosaic was tetrachromatic. figure 5e -5h shows the classification results. the l, m, and a cones separate reasonably well but not perfectly in the embedding, and if we fit the flattened z positions with three skew normal distributions ( figure 5h ), the l, m, and a cones were classified with 100%, 100%, and 84% accuracy, respectively. this indicates that there is information in the correlation matrix that can support unsupervised cone classification in human tetrachromats, albeit imperfectly, at least for these specific simulation conditions."
"while mbir methods have been successfully applied to cbct [cit], the system models are often borrowed directly from mdct, and are therefore derived from assumptions that may not be valid for cbct. for example, mdct detectors typically include mechanisms to avoid signal sharing between detector elements (e.g., a pixelated scintillator) whereas flat-panel detectors typically exhibit significant sharing of the light generated by the primary x-ray to secondary light quanta conversion. this effect can be prominent for smaller pixel sizes and leads to increased blur and noise correlation between neighboring measurements as compared to mdct. while previous work [cit] has suggested that focal spot modeling has relatively small advantages in current mdct systems, the x-ray tubes used in many dedicated cbct systems tend to have stationary anodes and larger focal spots than those used in mdct. additionally, cbct detectors have smaller pixels than mdct. specifically, [cit] demonstrated that focal spot modeling lead to improvements when the effective focal spot blur (at the detector) was about 1.3 detector elements wide, which is an uncommon occurrence in mdct but common in cbct (e.g., 0.2 mm pixels with a 0.3 mm focal spot and a system magnification of 2). hence, focal spot blurring effects can be significant in cbct, particularly in systems that leverage higher magnifications. traditional mdct methods do not incorporate such physical effects into their forward models, limiting their ability to resolve fine resolution details when applied to flat-panel cbct data. to get the most of such data (e.g., increasing spatial resolution capabilities), the mbir forward model must adopt high-fidelity models of these physical effects which are conventionally ignored."
"in this section, the impact of the driver's head and body features on driving task classification will be analyzed. the feature evaluation is divided into three parts. first, the depth information of the detected joints and facial landmarks (eyes and nose) is evaluated. then, the task classification using only head signals or only body signals is proposed. the classification results for these three parts are shown in table v ."
"quoting almost verbatim from kunze [cit], his experimental results indicated that after the pressure of an isolated perfused carotid sinus was held at 80 mmhg for 20 min, the threshold pressure necessary to elicit the reflex systemic blood pressure response was about 78 mmhg. when carotid pressure was maintained for 20 min at 120 and 160 mmhg, instead, the threshold rose to 113 and 126 mmhg, respectively. such resetting of the threshold to a stable value upon elevating or reducing carotid sinus pressure was accomplished within 15-20 min. the entire range of operation of the reflex response was shifted to higher carotid pressures as the holding or clamped pressure was elevated while the midrange gain of the response was unchanged at the three holding pressures tested. these findings indicate that the carotid reflex need not operate over a fixed range but that the range may be rapidly adjustable to the prevailing pressure. when arterial pressure is sustained at a level that is elevated or depressed from normal, the carotid baroreceptor reflex acutely resets to operate in the range of the prevailing pressure with a threshold that has moved toward that pressure."
"to study whether there is a neural structure functioning as comparator, the main paths involved in the cardiovascular regulation must be analyzed. the nts receives fibers from baro-and chemoreceptors, mainly through the aortic depressor and the carotid sinus nerves [5, [cit] . as baroreceptor primary afferent fibers, chemoreceptor fibers terminate in the nts [cit] . the cardiopulmonary fibers converge to the same pool of central neurons as the arterial receptors and act in a similar way [cit] . the nts projects to neurons within the caudal and intermediate parts of the ventrolateral medulla (vlm) and it also projects to several brainstem nuclei: the lateral reticular nucleus and the nucleus gigantocellularis, among others [cit] . besides, there are links to a \"depressor area\" in the caudal ventrolateral medulla (cvlm), where inhibition of sympathetic excitatory neurons of a \"pressor area\" in the rostral ventrolateral medulla (rvlm) may take place [cit] . neurophysiological studies indicate that the major source of peripheral chemoreceptor drive to rvlm pre-sympathetic neurons is likely to originate from neurons located in the nts [cit] . moreover, the nucleus sends fibers to the intermediolateral spinal column (iml) [cit] . efferents from the iml pre-sympathetic neurons innervate the myocardium and smooth muscle vessels and the nts projects to the dorsal vagal nucleus and the nucleus ambiguous, which, in turn, send fibers to the heart [cit] ."
"we did not incorporate the blur introduced by the optics of the human eye into our main simulations, in part because we do not know how much blurring was already introduced by the various hyperspectral cameras. in this regard, we note that the addition of optical blur makes the classification problem easier rather than harder for our algorithm ( figure s12 ). this is because blur reduces the variation in cone correlation caused by variation in the spatial structure of natural images [cit] . it is therefore perhaps of interest that in humans, red-green color vision develops over the first three months of infancy [cit], during a period when the optical quality of the image is not yet as good as that in adults [cit] . as an explicit test we explored the effect of introducing gaussian blur (standard deviation of 4 pixels). accuracies for the simulations with blur were high and are shown in figure s12 . the algorithm chose 2 cone classes in 40 of 54 simulations and correctly typed all cones in 49 of 54 simulations performed with blur. in addition to evidence that red-green human color vision develops after birth, monkeys raised in an unusual environment in which the illumination was always one of four monochromatic lights displayed impaired color discrimination [cit] . our algorithm as currently implemented does not capture this effect: we found that when we ran the algorithm with input that simulated the deprivation experiment, it still correctly detected that the mosaic was trichromatic and correctly classified all of the cones. however, the embedding (not shown) produced by this simulation, while leading to good separation by cone class along the z axis, was not typical of other simulation embeddings and did not produce an reasonable representation of the spatial positions of the cones as do our typical embeddings."
"the conclusion is made that for future driving monitoring and behavior understanding, the head and body signals are equally important and necessary. future works will focus on the collection of more real-world data set and recognize more sophisticate driver behaviors. these studies will benefit future adas design and improve driving safety by real-time driver status monitoring."
"to accommodate the nonuniform illumination of the testbench detector, photon flux was modeled with the following equation [cit] : where g i is a random variable representing an offset corrected gain (bare-beam) scan value for pixel i, i 0 is a constant value representing the photon flux at the piercing point, p i is the detector gain for pixel i (detector units per photon), and θ i is the angle between pixel i and the piercing ray. the parameters i 0 and p i were estimated using the mean and noise properties of offset corrected gain scans. measurements were corrected for detector gain (i.e., divided by p i ) to obtain measurements in photon units. to account for focal-spot intensity variations, a normalization factor n v was calculated for each frame (v) using an unobstructed (bare-beam) region of the projection data. g was therefore a matrix which scaled the values of each pixel i by i 0 cos 3 (θ i ) and the values of each frame v by n v ."
"finally, quantity analyses of the impact of the driver's head and body features for driver task recognition are performed, and the predicted important posture features are evaluated separately. since the existing literature seldom considers the driver's body features, this paper quantitatively proved that head and body features are required for driver behavior recognition."
"in this part, the experiment setup and data collection methods are introduced. driver behavior data are collected using the low-cost range camera kinect, which was developed by microsoft. in this paper, the second version of kinect (v2) was adopted. kinect is a consumer camera that supports color images, depth images, audio, and infrared information. it was first designed for indoor human interaction with computers and has been successfully applied in vehicles for driver monitoring [cit] ."
the separable footprints projector [cit] performs the integration over pixel area (i.e. sum over subpixels) inside the ex-ponential. we can therefore represent the separable footprints forward projection for sourcelet k as
"in contrast to traditional bonus point systems, that generally provide no privacy at all, we propose a completely decentralized and anonymous scheme where users actually generate bonus points themselves. this enables users to remain entirely anonymous while participating in a customer loyalty program."
"in this section, we present pathloss models for the los and the nlos scenarios. the diffraction loss due to blockage of the los from the parked vehicles in the nlos scenarios is investigated and modeled. a 3d tool is developed to calculate the diffraction loss and verify the proposed model. finally, shadow fading is extracted from the measurement data and its spatial correlation is modeled in the los and the nlos scenarios."
"guyton [cit], proposed that blood volume regulation takes place through the diuresis/natriuresis functions of the kidneys; the latter obviously coexisting with neural mechanisms controlling blood pressure. dibona [cit] noted that alterations in efferent renal sympathetic nerve activity produce significant changes in renal blood flow, glomerular filtration rate, reabsorption of water, sodium, and other ions, and the release of renin, prostaglandins, and other vasoactive substances. moreover, chronic recordings in freely moving cats showed the presence of continuous background activity in the renal nerves [cit] . this tonic efferent discharge is reduced by elevated blood pressure, increased by exercise, and almost eliminated by ganglionic blockade. also, the loss of neurogenic vasomotor tone can reduce mean pressure from 100 mmhg to 50 mmhg or less, and injection of very small doses of norepinephrine can immediately restore the previous pressure [cit] . finally, renal denervation (under the tonic control of sympathetic premotor neurons in the rvlm has no effect on arterial pressure in normotensive animals [cit] ."
"another possibility is suggested by recent studies of adult dichromatic monkeys and of knock-in mice expressing the human long-wavelength opsin. knock-in female mice heterozygous for the native mouse and human opsins have been shown to be capable of limited trichromatic discrimination [cit] . similarly, the addition of a third opsin using gene therapy can enable trichromatic color discriminations in adult dichromatic monkeys [cit] . although the enhanced discrimination does not necessarily mean that the visual system has used the new cone class to add a dimension to the perceptual representation of color [cit], it does indicate that an immediate adaptive benefit can be bestowed by the expression of a novel photopigment, thus allowing for selective preservation of the mutation and subsequent enhancement through the implementation of a learning algorithm that classifies the cones."
"these metrics were calculated in an roi encompassing the central line pairs. to calculate mjac, a truth segmentation t b was calculated by thresholding the truth image t at 0.040 mm"
where α i (t) and τ i (t) are the complex amplitude and the delay of the i-th mpc at time step t and δ(·) is the dirac function. the instantaneous power delay profile (pdp) is then calculated by:
"in the confusion matrix, the green diagonal shows the number of correct detection cases for that class. the bottom row shows the classification accuracy with respect to the target value, and the far right column shows the classification accuracy with respect to the predicted labels. as shown in fig. 10, the normal driving behavior for driver 2 only achieved 38% detection accuracy, and 289 cases are classified into the phone answering task. this is mainly due to the similar postures between normal driving and phone answering behavior. once hand detection is inaccurate, it is very difficult to classify these two tasks only according to head pose. detailed discussion will be proposed later. in addition, the low detection accuracy means that the trained model using the other four drivers is not sufficient to precisely recognize all the behaviors for driver 2 due to the diversity of the drivers. however, once driver 2 is included in the training data, the model will obtain better detection results for the other four drivers. the most accurate detection occurs for driver 4, and the relative results are shown in fig. 11 . as shown in fig. 11, the classification results for the seven tasks for driver 4 are much better than for driver 2. false detection between different classes decreased significantly. similar results are achieved for the remaining three drivers. in conclusion, although very accurate results were not achieved for driver 2 compared with the other drivers, the general classification accuracy for the seven tasks was 82.4% (the mean value of the average column), which indicates efficient classification results."
"as algorithms enable increased resolution, proper choice of voxel size will be critical [cit] . if one were not attempting resolution recovery, the ideal voxel size would be about the size of the demagnified system blur (0.33 mm for this system)."
"to our knowledge, there exists no bonus point system using electronic cash as a basis for operation. its advantages of guaranteed anonymity, central control, and user-friendliness make it favorable for such a system."
"bonus point spending bonus point spending up an additional attack vector that we address in the following. namely, a fraudulent customer might invoke the disputation process, claiming that the merchant has tried to commit fraud while, in fact, the customer tries to achieve double spending."
"to increase the diversity of each tree in the forest, rf is trained using a bootstrap aggregating (bagging) technique. specifically, the number of trees b in the rf is selected. then, according to this number, b separate training data sets are chosen from the original data set. since bagging is a random sampling technique with replacement, approximately one-third of the data are not used for training each subtree. the remaining data set for each tree is the oob data set. normally, cross validation is not necessary for training rf, since the oob can be used to evaluate the model performance by evaluating the oob errors [cit] . moreover, the oob data set can be used to evaluate the feature importance for model accuracy. to obtain the feature importance, for each variable x i, the variable is randomly permuted. the feature importance is calculated as follows:"
"emission imaging has utilized high-fidelity modeling to recover lost spatial resolution for decades [cit] . the linear forward model in spect and pet imaging permits incorporation of advanced blur models directly into the system matrix. the resulting high-fidelity forward models are linear, simplifying optimization. such approaches have been used to model position-dependent geometric blurs and blurs due to physical detector characteristics (e.g., signal penetration through septa) [cit] . additionally, noise correlations induced by rebinning have been modeled for pet [cit] ."
"the algorithm as described and evaluated above relied on the assumption that the mosaic contains both l and m cones. this assumption entered the calculations at the step where we fit two skew normal distributions to the z position histograms of the embedded cones ( figure 3d ). in addition to variation in l and m cone l max, however, there are also individuals who are dichromats with mosaics containing s cones and only one longer-wavelengthsensitive cone class (i.e., red/green color blindness). for such dichromatic retinal mosaics this algorithm would randomly divide the single longer-wavelength-sensitive cones into two classes. to address the question of whether it is possible to determine the number of longer-wavelength-sensitive cone classes present in a mosaic, we elaborated the algorithm with a step that compared how well a mixture of 1, 2, or 3 skew normal distributions fit the z position histograms and chose the number of longer-wavelengthsensitive cone classes based on this fit. specifically, we applied a kolmogorov-smirnov goodness of fit test [cit] to the discovered mixture distributions and chose the smallest mixture number whose fit to the flattened z positions could not be rejected using a two-tailed critical p-value of 0.01. when the elaborated algorithm is applied to the correlation matrices of a dichromatic retinal mosaic, it correctly detects that there is only a single longerwavelength-sensitive cone class and thus classifies all of the longerwavelength-sensitive cones as belonging together in a single class ( figure 5a-5d )."
"we introduce what we call vouchers into the bonus point generation process. figure 2 shows the accordingly modified bonus point protocol. during the checkout process, alice receives a voucher v for every unit of money spent. the voucher contains a serial number that is unique and linked to the merchant bob. vouchers are generated by or registered at debbie. they can be created either online during the payment process or potentially also proactively in advance. if alice is to receive one bonus point, bob will transfer a voucher to alice."
"when creating a bonus point, alice no longer blinds and transfers the actual secret id c secret to debbie. instead, the id subject to blinding is the public key c pub . the key pair is generated with sufficiently long keys so that an attacker keen on guessing c secret does not succeed in a reasonable amount of time [cit] bit for rsa keys. by that, we also ensure that the probability of a random collision is still low enough, even though not every number is prime and thus can not be the private modulus of an rsa key pair."
the information presented to debbie looks exactly like in the case of actual fraud against evan by the unsuspecting merchant charlie. we call this the reverse customer fraud attack.
"the test vehicle moved around 100 m with an average speed of 11 ms −1 towards the collision point. the pedestrian equipped with the receive antenna walked around 12 m towards the collision point with an average speed of 1.2 ms −1 . to study a realistic v2p signal propagation, we measured the channel in different los and nlos scenarios, with both a static tripod and a moving pedestrian. in this work, we focus on the two following scenarios:"
"while this work utilizes a relatively simple mathematical formulation, we note that gpl-bc is capable of incorporating a wide variety of complicated models. for example, one can extend the model here to incorporate a shift-variant blur and depth-dependence (focal-spot blur) [cit] with proper definition of b or a. the model may also incorporate detector lag (a temporal blur function) with a similar redefinition of b and blur due to gantry motion with modifications to both a and b. the only constraints are that a, b, and w are matrices and η is positive. such modifications are the subject of ongoing and future work."
"the pathloss models included in this work can be applied in simulations for the given scenarios to predict the pathloss and the received power at each tx and rx position. the additional loss due to the obstruction of the los can be calculated by implementing the method of detecting the diffraction edges provided in this paper and then calculating the knife-edge diffraction loss. this provides a more accurate prediction of the received power in the presence of an obstruction from a parked vehicle. additionally, for more realistic simulation, the spatial correlation of shadow fading need to be considered when generating shadow fading map. to generate spatially correlated shadow fading, the uncorrelated shadow fading is generated with a resolution equal to the decorrelation distance. the spatial correlation is then introduced by a spline interpolation. alternatively, to generate more accurate shadow fading map, the provided autocorrelation models can be used to introduce the correlation."
"our approach is computational. that is, we abstract, for the most part, from known features of retinal and cortical circuitry and consider what is possible using information encoded by the distal retina in response to natural images."
"3) the relatively low height of the pedestrian's smartphone antenna, such that the los could be partially or completely obstructed by road side objects, e.g. trees, moving or parked vehicles and surrounding pedestrians. all these aspects need to be accounted for when developing the channel model. therefore, we conducted a wideband v2p channel measurement campaign considering a collision scenario between a vehicle and a pedestrian. the goal of this work is to bring detailed characterizations of shadow fading for the v2p channel in los and non-los (nlos) scenarios. the main contributions of this paper can be summarized as follows:"
"the fresnel-kirchoff parameter v depends on the distance d 1 from the tx to the diffracting edge, the distance d 2 from the diffracting edge to the rx and the effective height h of the diffracting edge:"
"the objective herein is to collect information from the literature at large and from our own experience that, in our view, is close enough to ascertain that there is a neural set point for the long term control of blood pressure. this paper reviews the subject, too."
"in addition, we simulated tetrachromatic retinal mosaics with l, m, a, and s cone l max values of 558.9, 512.8, 466.8, and 420.7 (i.e., with m and a cone l max values evenly spaced between the typical l and s cone l max values) and with l:m:a ratios of 1:1:1, 2:1:1, 2:2:1, 1:2:1, 1:2:2, 1:1:2, and 2:1:2. we found that the algorithm correctly detected 3 longer-wavelength-sensitive cone classes in all cases and correctly classified all but 7 cones correctly between the 7 simulations."
"finally, on-road data collection can be performed in the future for the study of real-time driver behavior detection within normal driving environments. currently, for safety considerations, the drivers were asked to perform the experimental tasks without driving the vehicle, because secondary tasks, such as texting and playing a video device, are extremely dangerous when driving and should be avoided. therefore, the most naturalistic data are difficult to collect. however, in the future, with the help of advanced driver assistance systems (adas) and the midlevel automated vehicle technique, drivers are allowed to remove their hands from the steering wheel. therefore, more distraction behaviors can be collected, and the study for real-time driver distraction detection in a real vehicle can be performed. the real-time driver monitoring study will significantly improve the driving safety for both conventional vehicles and highly automated vehicles."
"first, a driver posture detection method using a kinect, a consumer range camera, inside a vehicle is introduced. the data characteristics of kinect are analyzed, and the data processing technique for in-vehicle application is proposed. in addition, the head rotation signals from the kinect are calibrated with a precise orientation sensor."
"on one hand, the issuer occupies a central role in the bonus point system and might seek opportunities to cheat on customers and on merchants. on the other side, the issuer's business model is based on the successful operation of the anonymous and trusted bonus point system. in contrast, one might argue that merchants use the bonus point system as secondary aid to increase revenue in their primary business and thus might have the objective to unfairly maximize the effectiveness of this tool. if a merchant is caught and subsequently excluded from the system, they might already have gained more advantages than they would have by remaining in the system for a longer term."
"at present, the precise circuitry underlying red-green color vision in primates is not fully understood, and for this reason it is difficult to draw firm conclusions as to the locus where cone-class specific processing occurs. some authors have suggested that circuitry in the adult retina does not distinguish l from m cones [cit] . they note that because midget cell centers in the primate fovea receive input from a single cone [cit], random wiring of cones to ganglion cells would still preserve most of the color information in the fovea. this would then leave the full separation of l and m cone signals to be learned at the level of the cortex. other authors [cit] report varying degrees of l versus m cone-specific wiring for retinal ganglion cells. such retinal specificity could result from activity-dependent synaptic plasticity during development [cit], via the type of learning mechanisms considered here or in earlier work [cit] ."
"based on the results shown in section iv, driver task recognition can be achieved with an ffnn. the ffnn could reasonably detect seven tasks for different drivers and achieved high-precision detection for secondary tasks. the ffnn has advantages for driver task detection over other machine learning methods. classification for different tasks resulted in different detection accuracies. the results indicate that for tasks, such as texting and left-and right-mirror checking, which have obvious distinct features, the detection results are accurate. however, for tasks that have similar postures, the model can be confused. in this paper, normal driving behavior has similar characteristics to rear-mirror-checking behavior and phone-answering tasks; therefore, the detection results for these behaviors are slightly worse than for other behaviors. in addition to the similar characteristics of these behaviors, another reason for less accurate detection results is driving style. although accurate detection results can be achieved for some drivers, the ffnn cannot obtain a universal accuracy for all drivers. for example, task detection for driver 2 is less accurate than for other drivers due to driving style and sensor noise. a driver has a unique driving and mirror-checking style. some drivers prefer to use significant head and body movement during mirror checking while others may try to use less body movement and use eye movement to capture information. therefore, the following aspects are discussed and can be improved to achieve higher task detection accuracy."
"in contrast to emission imaging, transmission imaging forward models are fundamentally nonlinear due to the beerlambert law, preventing blurs from simply being incorporated into the system matrix. applications of advanced forward models in transmission tomography can be coarsely grouped into three categories: sinogram restoration, direct mbir, and preprocessing + mbir. in sinogram restoration approaches, ideal measurements or line integrals are estimated based on a forward model. images are reconstructed from these estimates with either analytical methods (e.g., fdk) or mbir with a simple forward model. sinogram restoration has been used in conjunction with models of blur [cit] and noise correlation [cit] . direct mbir methods incorporate the advanced forward models (e.g., of blur, noise correlation) directly into the mbir objective function. such approaches have been used with an independent noise assumption [cit] . additionally, direct mbir has been used to model blur and noise correlation in tomosynthesis [cit] by assuming uniform quantum noise per view and that features of interest (e.g., microcalcifications) have low-attenuation and are small. preprocessing + mbir is a hybrid approach, with the effects of preprocessing modeled in the subsequent mbir. for example, noise correlations induced by deblurring have been included in an mbir model for cbct [cit] ."
"the remainder of this paper is structured as follows. section ii discusses related work from the areas of cryptographic currencies and customer loyalty systems. following that, section iii leads to our proposal for such a system. fraud prevention techniques are presented in section iv. an evaluation for the proposed strategy is given in section v where we identify the impacts on privacy, security, and the electronic cash protocol. finally, section vi summarizes this paper."
"with respect to representation, it may be that the the analysis of signals to extract an appropriate representational structure for stimuli, which is what mds accomplishes when applied to response correlation matrices, is a canonical biological computation. if so, then the biological extraction of the information about cone classes that we demonstrate is available may occur, in effect, as a result of a more general perceptual learning processes. this notion also provides another possible explanation of why the brain is prepared to learn about and exploit the addition of novel classes of sensory transducers."
"second, the importance of head and body features for task prediction is estimated using an integrated algorithm. the feature importance estimation given by random forest (rf) and maximal information coefficient (mic) is compared and integrated. then, the most important posture features for task recognition are determined. unlike previous studies that use time sequence data for driver behavior recognition [cit], this paper focuses on identifying behavior in a more natural way, only based on the instance samples. the objective is to design a humanlike task detector that can identify driver behaviors according to a single image. therefore, a feedforward neural network (ffnn) model is evaluated and compared with multiple machine learning methods."
"as shown in the far right column of table iii, the average classification result (ave) for each driver is defined as the average of the seven tasks. the mean values shown in the bottom row represent the average classification accuracy for each task of the five drivers. detection results equal to 1 shown in table iii indicate an accuracy of 100%. the ffnn classification model is trained with 60 neurons using the entire feature vector (42 features). the classification results for driver 2 are much lower than the other four drivers, with an average of only 0.630. this is due to the imprecise detection of the driver skeleton during data collection. to have a clear perspective of the detection performance, the confusion matrix for driver 2 is shown in fig. 10 ."
"(the average attenuation value of fat and bone). the reconstructionμ was thresholded by a value t for 101 values of t between the attenuation values of fat and bone, inclusive. the mjac value for a given reconstruction is the maximum jaccard index between the truth segmentation and the segmentedμ over all t:"
"this section presents the results of the prototype test-bench study with human trabecular bone. the mjac for each reconstruction is shown as a function of regularization strength in fig. 6 . the gpl-bc method is able to achieve the highest maximum mjac, followed by gpl-b, gpl-i, and fdk (indicated by the black line). the optimal segmentation thresholds for the most accurate gpl-i, gpl-b, and gpl-bc reconstructions (i.e., those with the maximum mjac over regularization strength, corresponding to the maxima in fig. 6 ) are 0.0322 mm the most accurate reconstructions are shown in fig. 7 and fig. 8, along with the corresponding segmented trabecular bone images (using the optimal thresholds) and tb.th. maps. the fdk reconstruction, the registered µct reconstruction with matched voxel size (µctmv), and the registered µct reconstruction slices with the original µct voxel size (µct) are also included. while the µct reconstruction is the best approximation of the true image volume, the µctmv image is a better approximation of the best achievable reconstruction at the chosen voxel size."
"feature selection is a major research area of feature engineering. by selecting a subset of feature vectors, machine learning models can be trained more efficiently, and better results can be obtained. in this section, to understand how driver features influence the corresponding behavior detection and which features are important for the behavior recognition task, two distinct feature selection methods are applied and compared. first, an rf was used to estimate the driver feature importance with an out-of-bag (oob) data set. second, an mic is used as another indicator for the association between features and the behavior class. the final conclusion of feature importance will be summarized according to the results given by these two distinct algorithms."
"the temporal spikes due to noise can cause more serious problems. the body and head detection results using the kinect can be influenced by lighting conditions or the location and distance to the driver, and human gesture or body pose can influence joint detection, especially inside the vehicle. due to the less precise detection results using the kinect, an integrated signal process scheme combining two different filtering techniques is adopted in this paper."
"1) pathloss models for los and nlos scenarios are proposed. 2) to model the diffraction loss and gain deep insight into the propagation under nlos conditions, we developed a 3d tool to detect the diffraction edges and calculate the diffraction loss."
"other mechanisms for preventing double spending are suitable in an offline scenario. the basic idea is to embed the payer's identity into the coins in such a way that at least two distinct entities need to cooperate in order to reveal a user's identity [cit] . these two entities have to possess certain decryption information, which can only be obtained if the coin has been spent at that entity. identification of the fraudulent payer can only be performed if two parties cooperate and both have received the same coin from the payer. thus, if an honest user does not try to cheat and therefore does not spend coins twice, unwanted identification is not possible. such systems do not need an immediate fraud detection technique, it is sufficient to deanonymize the fraudulent party at a later point and use legal means to punish them and go for compensation. their main advantage is the possibility to be used in scenarios where the victim of the fraud is offline during the transaction and comes online later, after the fraudulent customer has left. all of these systems, however, rely on a central issuer to at least perform the signatures."
"where f is frequency and g is the relative strength of the gaussian term (between 0 and 1). combined with pixel sampling, the mtf model at the detector is"
"in this section, the task recognition results are discussed. specifically, task classification with ffnn is compared with other machine learning methods. in addition, the impact of the head, body, and depth information on the classification results will be evaluated separately in two parts."
"(the large system blur relative to pixel pitch results in most cbct systems binning projection data to increase effective pixel pitch.) in this work voxel size was approximately equal to the demagnified pixel pitch (i.e., much smaller than the limit imposed by system blur). angular sampling also effects voxel size. ct data is almost always angularly undersampled."
"in summary, we have studied a question that is fundamental to the formation of any sensory system that provides multidimensional information. given that the enabling step in the evolution of such systems must be the expression of novel sensory transducers, how is it possible to detect and differentiate their presence? we address these two fundamental questions at the computational level, using primate color vision as a model system. we find that inter-cone correlations provide sufficient information for unsupervised learning to detect the number of cone classes present and to classify individual cones accurately by demonstrating an algorithm that capitalizes on this information. these results fill an important gap in our understanding of how color vision could have evolved, and may also provide insight into the evolution and operation of other sensory systems."
"in the bench data study, we make assumptions to avoid computing this inversion every iteration, but such assumptions will not always be valid (as in the simulation study). in such cases, one may need to make additional approximations to reduce computation time. additionally, patient motion may be a resolution limiting factor on high-resolution systems. however, if patient motion is properly estimated, it may be incorporated into the system matrix to reduce this image degradation without altering the presented algorithm [cit] ."
"(the moore-penrose psuedoinverse of s) to upsample the data, any upsampling matrix may be used for the derivation. so long asb is circular and the upsampling/downsampling operations are shift-invariant, b represents a convolution operator."
"customers are effectively not able to commit double spending, because merchants do not accept a bonus point if the issuer's database indicates prior usage of that point. gaining illicit bonus points without paying in an eligible transaction is excluded by our newly introduced system of vouchers. vouchers ensure that for every blind signature there is a merchant who accounts for its value. counterfeiting a valid voucher without possessing it from an eligible transaction is practically impossible, as the serial number is very long. by choosing a length of at least 128 bit a guessing attacker has to try an infeasible amount of options before generating a valid voucher by chance. the last remaining attack vector for customers is the abuse of our fraud resolution system. the question whether a customer intended to pay at the merchant can be decided by using the proof of point ownership in conjunction with authorization receipts, as discussed in section iv. this combination prevents customers from creating unbacked bonus points and from gaining advantage by wrongly reporting unsuspect merchants as fraudulent. thus, customers are no longer able to gain an unfair advantage."
"in control theory, there are three basic mechanisms of regulation: buffering, feedforward and feedback. in each case, the effect of disturbances on the essential variables is reduced, either by a passive buffer, or by an active regulator in the two latter."
"additionally, we identified a weakness in the concept of the electronic cash system, where a user could cheat. this is fixed by altering the bonus point generation algorithm so that it is possible to prove committed intent of transaction on one side and identity of point generation on the other side. by that we eliminate a specific fraud scenario and thus incentivize bonus system operators to switch to such a system."
"first, the 2-d-only case in table v represents a feature set only consisting of the head rotation and joint coordinates (x-and y-coordinates), and depth information is not used. as shown in table v, the model trained with 2-d information achieves similar accuracy results compared with the model trained with the entire feature set (table iii) . the results indicate that depth information has very limited impact on the model classification task."
"to illustrate the attack, consider a fraudulent customer evan, two honest merchants bob and charlie, and the central issuer debbie. evan purchases a good at bob's store using an already spent bonus point represented by the signed public key component c pub . bob consequently fails to validate the bonus point with debbie and denies the transaction. after that, evan contacts debbie via the fraud resolution system and claims he has been cheated on by a third, uninvolved merchant charlie. evan is able to provide the bonus point's secret key component to debbie and, thereby, confirms ownership of the bonus point."
"when the los path between the tx and the rx is obstructed in the fresnel zone by an obstacle whose dimensions are larger than the wavelength of the radio wave, the measured propagation pathloss is increased. the additional increase in attenuation is due to the blockage of the los by the obstacles and the signal is received by diffraction of the electromagnetic waves. according to huygens principle, the electric field is the sum of the huygens sources located in the plane above the obstruction [cit] . the calculation of the diffraction loss can be done by treating the obstacles as absorbing knifeedges [cit] . applying this simplification, the diffraction loss (in db) becomes a function of only the fresnel-kirchoff parameter v as:"
"additional modifications to this underlying update are also shown in algorithm 1. specifically, the algorithm uses the ordered-subsets approach [cit] to accelerate estimation. the variable m denotes the subset index and subscripts in parentheses indicate that the argument is modified for the corresponding subset (e.g. a (m) µ is a forward projection of µ using only the projection angles in the m th subset). while p indexes the outer loop of iterations using all of the data, the current iterate n is permitted to take on fractional values indicating progress through the inner loop of ordered-subsets updates. algorithm 1 also includes a second column of calculations to optionally apply nesterov acceleration [cit] to further improve the rate of convergence. note that using ordered subsets or acceleration results in an algorithm that might not converge (acceleration is only guaranteed to preserve convergence when 2 available in the supplementary files/multimedia tab the objective function is convex). however, in practice the number of subsets can be chosen such that updates are wellbehaved. additionally, ordered subsets and acceleration can be used to get close to the solution, followed by several iterations without subsets or acceleration. computationally, each iteration requires one forward projection, two back projections, and one application of b t wb."
"equation d.6 can be further approximated by moving the duplication/shift step outside of the exponential. additionally, we assume there is some g and set of ω k s such that"
"in addition to receiving the actual bonus point, the merchant poses a challenge with a random nonce, a current time stamp, and the merchant id. the customer adds an own nonce to the tuple and signs all values using the bonus point's secret key in order to proceed. the merchant's nonce and timestamp ensure freshness and tie the receipt to the current transaction without the customer having to verify individual transaction details. the customer's nonce prevents a chosen-plaintext attack [cit] mounted by the merchant. the merchant can verify the client's signature using the bonus point's public key component. this signed challenge is called an authorization receipt and can be stored and later used by the merchant to prove that the customer had knowledge of the secret key and the intent to pay at the merchant's shop. note that with these receipts it is no longer necessary for the issuer to rely on the customer when identifying cheating merchants."
"in this paper, driving behaviors for different drivers are studied. the driving behaviors are classified into two categories, normal driving tasks and distracting tasks. an ffnn is trained to distinguish the four mirror-checking behaviors from the three secondary driving tasks. both the depth information and the 2-d location of the body joints are collected using kinect. the noisy data are processed with an integrated filtering system. then, the importance of each driver feature for behavior recognition is evaluated using rfs and maximal information efficiency. the feature importance prediction with these two feature evaluation techniques shows consistent results. the most important driver features for driver behavior among all the drivers are determined. the ffnn has been proven to have advantages for behavior detection tasks over other popular machine learning methods. the model achieved an average of greater than 80% accuracy for the five drivers. with the evaluation of feature importance and their influence on the classification task, the head pose feature, hand position, and shoulder positions for the driver are selected as the most important features. in addition, based on the evaluation of the depth, head, and body features, it is found that the depth information for the body joints and facial markers has very limited influence on the behavior recognition. meanwhile, the head and body features should be combined with a comprehensive driver behavior understanding, since only using the head or body features will lead to large false detection rates."
"we begin by considering how to classify l versus m cones in a mosaic that is known to contain both cone classes; below we return to the problem of detecting the number of cone classes present in a mosaic. to proceed, we simulated the responses of retinal cone mosaics to natural images. the images used in the simulations were collected from three hyperspectral image databases [cit] . a hyperspectral image specifies the full light spectrum at each pixel, in contrast to standard color images which contain only red, green, and blue (rgb) values. the use of hyperspectral images allows us to accurately simulate the responses of cones from their specified spectral sensitivities. an rgb rendering of a hyperspectral image from the dataset is shown in figure 1a ."
"where y is the output of the ffnn, f () is the learned model mapping function with model parameter θ, x is the input data vectors, and is the bias between the actual output and the target. for the ffnn, parameter θ represents the set of activation function parameters and the width set between neurons. in this paper, a two-layer ffnn with one hidden layer is used to train the driver behavior recognition model. the sigmoid transfer function in the hidden layer is chosen. the sigmoid activation function for a single neuron is represented as"
"in table iv, the classification results of the ffnn are compared with four other machine learning methods, which are rf, svm, naïve bayes, and k-nearest neighbor (k equals 5 in this case). the accuracy in table iv is defined as the average detection result for the five drivers, i.e., the average of the ave column in table iii . meanwhile, to evaluate the driver distraction detection performance, the seven classification tasks are merged into a binary classification. here, the negative group is defined as the combination of the first four normal driving tasks, and the true distraction group consists of the remaining three distracted driving tasks."
"after all of the above and considering a control theory perspective, the nts in the medulla oblongata would act as a comparator if and only if, a) its lesion suppressed the cardiovascular regulation; b) the negative feedback loop still responded normally to perturbations (such as mechanical or electrical) after cutting the rostral afferent fibers to the nts; c) perturbation of a rns path to the nts modified the set point without changing the pattern, say, the dynamics of the elicited response; and d) cardiovascular responses to perturbations on neural structures within the negative feedback loop compensated much faster than perturbations on the nts rostral neural structures."
"customer loyalty programs are widely used by online and offline merchants. these systems often use some sort of quantifiable and discrete \"currency\", e. g., bonus points or miles. we term one unit of customer reward a \"bonus point\" or \"points\" in the remainder of this article. bonus points are generated for purchases at the issuing merchant. after collecting a certain number of points, customers can redeem rewards and discounts or reach status levels with specific perks. some schemes allow direct payment at the bonus network's merchants using bonus points."
view is of the z-y plane of the representational space. a view of the x-y plane for the l and m cones is provided in figure s1 and shows that the relative spatial positions of these cones are also wellpreserved in the embedding. figure s2 shows that the embedding accounts well for the underlying correlation matrix. figure s3 shows embeddings for additional simulations where the separation of l and m cone spectral sensitivities and l:m cone ratio were varied. these additional simulations are discussed in more detail below.
"the emergence of red-green color vision in primates is relatively recent, initiated by mutations that led to the expression of three rather than two cone opsins. in old world monkeys, this occurred approximately 35 million years ago via duplication of an ancestral m/l opsin gene followed by divergence of the two copies into the modern m and l cone opsin genes [cit] . new world primates have also acquired functional trichromatic color vision, but through a different evolutionary path [cit] . the recent evolution of the l/m split may mean that biochemical differentiation of post-receptoral circuitry has not yet occurred. consistent with this, there are no known molecular markers that differentiate l and m cones aside from their opsins [cit] . earlier work [cit] established that unsupervised learning could produce a degree of cone-specific wiring, based on statistics of mosaic responses to natural images. here we extend our understanding of what can be learned in two fundamental and important ways: we show that it is possible to learn the number of longer-wavelength-sensitive cone classes, and we quantify learning through the performance of an unsupervised learning algorithm that explicitly classifies each cone in the mosaic."
"the mic is designed to efficiently solve the mutual information estimation problem for continuous variables and continuous distributions. the mic provides an equitable measurement for the linear or nonlinear strength association between two variables. the mic introduced a maximal mutual information searching technique by varying the grid that drawn on a scatterplot of two variables [cit] . mutual information usually can be used to evaluate the mutual dependence between different variables and assess the amount of information the two variables share, or more generally, the correlation between the joint distribution of the two variables and the product of the independent distribution of the two variables [cit] . the mutual information for two discrete vectors is defined as where mi d is the mutual information of two discrete vectors and p(x, y) is the joint probabilistic distribution of x and y. p(x) and p(y) are the marginal probability distribution functions of x and y, respectively. for continuous variables, the mutual information format is slightly changed to"
"the system characterization results are shown in fig. 2 . the measured mtfs are plotted in fig. 2a and show that, for the prototype test bench, detector blur is a larger effect than focalspot blur. because detector blur (scintillator blur and pixel aperture blur) is the same at the detector and at isocenter, the difference between the isocenter mtf and the detector mtf is due to focal spot blurring. this difference is relatively small, indicating that this system is dominated by detector blur. the axial and trans-axial detector mtfs are almost equivalent, supporting the radially symmetric assumption used in the model. the mtf models (fig. 2b ) strongly match the measured data."
"many customer loyalty systems use an electronic backend that processes user data in order to generate market reports and targeted advertising. apart from places like small food shops with paper-and-stamp-based loyalty cards, most of these systems either use centralized, account-backed plastic cards or mobile apps for their user management [cit] . payback [cit], the biggest customer loyalty program in germany is used by millions of people and identifies every customer over several stores. each purchase is tracked and targeted advertising, as well as targeted offers, are distributed. while participation in this program does not necessarily require a valid address and the usage of one's real identity, a combination with credit card data or other tracking information impedes or even prevents truly anonymous usage. in contrast to our proposed system, payback's users are not anonymous and their purchases can potentially be tracked in a central system."
"end for end for vector µ, and the b and a terms as elements of matrices b and a, respectively, the notation can be simplified as:"
"where ω i is the weight of the i th input, and normally each neuron has a bias parameter b. an important reason for using the sigmoid activation function is the computation efficiency"
"the gpl-bc reconstruction has improved resolution as compared to the gpl-b, gpl-i, and fdk reconstructions, with sharper trabecular bone boundaries. consequently, gpl-bc results in a more accurate trabecular segmentation. this is particularly evident when comparing to fdk and gpl-i, where the segmentation images contain less detailed trabeculae. this effect is well illustrated in the tb.th. maps. the fdk and gpl-i maps show fewer, thicker trabeculae, while the gpl-bc map is similar to the µctmv and µct maps with thinner and more numerous trabeculae. the gpl-b map fig. 4(b) . the lower half of each image shows the best segmentation for that β/cutoff (i.e., the one resulting in the maximum jaccard index over threshold values). is more similar to the gpl-bc map, but still contains thicker trabeculae. the mean tb.th. calculations (table ii) confirm this observation, with gpl-bc resulting in a tb.th. value closer to those of µctmv and µct than do fdk, gpl-i, and gpl-b. in contrast, gpl-bc shows no advantage with respect to tb.sp. and bv/tv. bv/tv values are similar for all methods, suggesting the loss of fine trabecular structures and the increase in apparent trabecular thickness tend to cancel each other out in terms of bv/tv. the same mechanism is a potential cause for the better accuracy of the fdk and gpl-i mean tb.sp. values: the spacing lost to thicker trabeculae is recovered by the loss of fine trabeculae. in contrast, gpl-bc does a better job in general at recovering small trabeculae, but still reconstructs trabeculae as thicker than they should be, reducing the mean tb.sp. optimizing reconstructions based on one of these metrics instead of mjac may improve metric accuracy, or show that gpl-bc is ill-suited to that metric."
"another subset of retinal parameters for which our algorithm's performance was degraded were the simulations whose m cone l max values were close to those of the l cones, primarily when the m cone l max value was 555 nm, only 3.9 nm from the l cone l max value of 558.9 nm. in most such cases, the algorithm grouped these cones into a single class. considering the small difference in l max, this is perhaps not surprising. our results are thus consistent with experimental studies finding that intermediate color discrimination deficiencies result when the separation of l and m cone l max values is between *5-8 nm but severe color discrimination deficiencies when the l and m cone l max values are separated by ƒ4 nm [cit] . that said, the idea that learning limits performance in anomalous trichromats is speculative, as we have not attempted to quantify the performance losses that would be expected with a failure to correctly classify all of the cones. nor have we compared this to the performance losses expected simply because a reduced separation in l max values reduces the signal-tonoise ratio of the information available to make color discriminations. also of note with regard to the effect of l max separation on performance is the fact that a single nucleotide polymorphism in the longer-wavelength-sensitive opsin gene can produce a shift in l max of *15 nm [cit], so that a single mutational event could have produced a l max shift large enough to enable learning."
"why might the visual system have implemented a learning algorithm based on inter-cone correlations in advance of needing to determine the class of a newly expressed cone opsin? previous authors [cit] have suggested that high acuity spatial vision requires learning of the spatial arrangement of cone positions, and suggested that spontaneous retinal activity as well as changes in interpolated neural images across eye movements provide information that would enable such learning. although not the focus of this paper, our results ( figure s1 ) show that inter-cone correlations contain useful information for learning cone positions. thus it seems possible that a learning algorithm based on this information could already have been in place as part of a system designed for spatial vision, and as such might have been ready to learn cone classes as well."
"for some machine learning tasks, feature vector dimensions can be very high (hundreds or thousands, or even larger). although machine learning methods are particularly suitable for modeling large data sets, they are always viewed as a black box where it is difficult to analyze the intrinsic structure. therefore, it is important to understand how the input features influence or are associated with the model output. in this paper, to understand how the driver signals influence behavior detection, the relationship between body signals and driver behavior is analyzed. such feature evaluation and selection enable subjective understanding of the relationship between driver body signals and behavior."
"the main attack vector in the naïve protocol implementation is for a merchant to steal a bonus point when a customer tries to pay with that point. after gaining knowledge of the secret bonus point id, the merchant denies the fulfillment transaction and uses the point itself. our proposed extension of asymmetric key pairs serves as a method to prove being the original generator of a bonus point without revealing the secret itself. with that, fraud remains impossible as long as the secret key for a specific bonus point is not leaked, because of the challenge posed by the central issuer that has to be freshly signed by the suspected creator of the bonus point. however, the merchant trying to commit said fraud can only be identified if he tries to spend the gained bonus point in a later transaction. otherwise, a fraudulent merchant can only be identified using complaint reports of customers to the system operator."
"mean arterial pressure (map) is regulated by two neural mechanisms: first, a negative feedback loop where the rns to the nts would function as the set point, and second, an open loop where several brainstem nuclei of the closed loop (with fibers to the sympathetic and parasympathetic systems) receive feedforward or open loop (ff, see note at the end of the paragraph) projections from the same rns to the nts. they have at least two functions: to determine the map feedback loop set-point modulating some neural loop structures. since the cardiovascular feedback is too slow, the rns to the nts play a functional role not only under steady-state conditions but it may also vary according to the particular situation (say, different behaviors and pathologies). in hypertension, for example, stress might change some rns inputs to the nts, resulting in a set point modification. in short: in our view, the collected review including results from our own experimental data, gives enough support to the nts as a neural comparator in blood pressure regulation."
"understanding human drivers is necessary both for conventional vehicles and for automated vehicles. in the united states and china, accidents have occurred when a tesla driver trusted or solely relied on the autopilot system while driving. for lower level automated vehicles, especially for level 2 and level 3 automated vehicles (based on the automation definition in society of automotive engineers standard j3016), human drivers need to sit in the driver seat and are responsible for the safety issues. in these vehicles, the driver is allowed to perform secondary tasks for entertainment; however, due to the partially automated limitation, the driver has to take control in emergencies. therefore, the monitoring of human drivers and determining whether they can return to the driving task is more important than in conventional vehicles."
"in this section, driver feature evaluation is proposed to study the relationship between driver features and driver behavior estimation. the most relevant features for driver behavior recognition are detected. then, an ffnn is adopted as the driver behavior classifier to identify the driver actions based on the selected feature vectors."
"the posterior hypothalamus at sites dorsal and medial to the fornix, the hypothalamic defense area (hda) and the dorsal periaqueductal gray (pag) zone are associated with the \"defense reaction\". the dorsolateral portion of the posterior hypothalamus, the hypothalamic vigilance area (hva) and the ventrolateral pag zone are part of the neurocircuit that mediates the \"vigilance reaction\". this circuit underlies affective responses to stressful stimuli and plays a fundamental role in integrating the effects of environmental events on cardiovascular regulation [cit] . the paraventricular nucleus in the hypothalamus (pvn) is sympatho-excitatory and it is tonically activated by inputs that, in turn, are activated by increases in the level of circulating angiotensin ii, chronic stress or anxiety, or peripheral receptors which may be tonically activated under certain conditions [cit] . it is also one of the major direct projections to the nts, as reported by dampney [cit] . it has even been suggested that the medial prefrontal cortex (mpfc) receives a variety of sensory information, including visceral signals, and helps the organism in selecting appropriate behavioral and autonomic responses for those stimuli and the emotional requirements of the situation [cit] . the ventral part of the mpfc projects to the amygdala, among other neuronal structures [cit] . figure 3 summarizes the neural links briefly described above."
"could be used to classify l versus m cones. consider an illustrative algorithm that starts with a single cone (e.g., the m cone shown in figure 2c ) and labels it (arbitrarily) as an m cone. the algorithm would then classify that cone's neighbors as being of the same or different class by setting a threshold on the correlation values. by propagating this procedure across the mosaic, labels would be assigned to all cones. such an algorithm would be effective if the distributions of nearest neighbor within-class correlations had little if any overlap with the distribution of between-class correlations. figure 2f shows the histograms of such correlations for our dataset. although the histograms make clear that within-class correlations are on average higher than between-class correlations and therefore carry information about cone class, the overlap between the two types of correlation means that nearest neighbor correlations alone are not sufficient for good classification. note, however, that the histograms shown in figure 2f only depict the information carried by nearest neighbor correlations, leaving open the possibility that additional information carried by the full correlation matrix could support a robust solution to the cone classification problem."
"(which potentially gives a slight advantage to gpl-i). figure 4 shows the bias/noise trade-off (a) and maximum jaccard index (b) for the center set of line pairs. results are similar but less dramatic for the other two sets of line pairs (not shown). at lower regularization strengths, reconstructions of noiseless data are more accurate (lower bias), but reconstructions of noisy data result in noisy reconstructions. on the other hand, for higher regularization strengths, noise is suppressed at the cost of increased smoothing/blurring of the image, imparting bias. methods with blur modeling (gpl-bc and gpl-b) were able to achieve a lower bias than the method without blur modeling (gpl-i). gpl-bc and gpl-b have a similar bias/noise trade-off, with gpl-bc showing a slight advantage. because it does not include a blur model, gpl-i encounters a bias limit at about 0.013 mm"
"however, the actual primary goal for a modern customer loyalty system is not the generation of highly personal profiles, but an increase in revenue by incentivizing customers to spend more money. this is usually achieved by shipping targeted advertising (and possibly special offers with lower prices) to customers. targeted advertising contains offers that match the customer's needs deduced from their purchasing history. in current systems, this is based on the detailed knowledge of every customer. however, a customer loyalty system where the system learns about the entire market (and not about individuals) and detects trends affecting greater parts of the population can indeed generate advertising offers for selected subgroups of the participants without knowing their individual purchase histories. by shipping these offers to the relevant subgroups without actually knowing the receivers, it becomes possible to create an entirely anonymous bonus point system. with every receiver knowing their own purchasing history, a customer's device can be imagined to filter relevant promotions and adverts from a broadcasted collection of advertisement content."
"given the success of the elaborated algorithm at correctly detecting that the dichromatic retinal mosaic is missing a cone class, we then asked whether it also detected that trichromatic retinal mosaics are in fact trichromatic. figure 4b plots how well the elaborated algorithm detects that trichromatic retinal mosaics contain two classes of longer-wavelength-sensitive cones, as a function of l:m ratio and l max separation. cells colored white indicate cases where the elaborated algorithm correctly detected the presence of both longer-wavelength-sensitive cone classes for all simulations. the pattern of accurate detection performance is generally similar to the patter of accurate classification performance, with detection accuracy falling off as the l:m ratio becomes more extreme and as l max separation decreases. the range of good performance for detection shown, however, is narrower than the range of good classification. an examination of figure s4 indicates that more cases of failure occur when the algorithm detects one (as opposed to three) longer-wavelengthsensitive cone classes than the other way around. thus, we could presumably improve the range of good performance somewhat by adopting a more lenient criterion (lower critical p-value) in our algorithm. we have not systematically explored this parameter space, however, as our view is that the key conclusion to be drawn from this work is that detection is possible for a reasonable range of mosaic parameters. a large number of additional factors (size of image patches, number of images sampled, presence of surround, cone-specificity of surround, nature of images sampled, optical blur, noisiness of cone responses) will all interact to push the exact boundaries of good performance as well as the direction of the failure modes around, and we do not wish to make too much of the particular location of these boundaries but rather focus on their general properties. figures s5-s11 show both detection and classification performance as various of these factors are varied. online supplemental material available at http://color.psych. upenn.edu/supplements/receptorlearning provides animated gif renderings of the 3d embeddings corresponding to each of these supplemental figures (as well as to figures s4 and s9 ). these additional simulations do reveal some cases where the algorithm detects three rather than two longer-wavelength-sensitive cone classes for some of the mosaic parameters. for the same reasons outlined above, we have not pursued the degree to which these over detections could be remedied by adjustment of the critical pvalue or model selection procedure. rather, the broad point we draw from the set of additional simulations is that the algorithm is generally robust; see discussion for additional consideration of these simulations."
"the quantitative results in table 1 and table 2 show that all denoising strategies improve the rmse of reconstructed tacs and the pc of the reconstructed perfusion maps compared to native fdk reconstruction. the best results are achieved using the jbf-based approaches, where the fdk-jbf result is very close to the art-jbf result with a computation time reduced by a factor of ten. comparing the perfusion maps shown in figure 1 and figure 3 reconstructed with the jbf approaches to the fdk approach, the jbf approaches show several advantages: the stroke-affected areas are much better separated from the healthy tissue and their shape is much closer to their shape in the reference maps. also the area of increased perfusion values around the vessels in the cbf and cbv maps (red pixels) due to smoothing is much higher in the fdk than in the jbf maps. the perfusion values in the brain tissue are in a range of 20-60 ml/100g/min for cbf resp. 2-6 ml/100g for cbv in the jbf maps which is the same range as in the reference maps, but the range in the fdk maps is 40-120 ml/100g/min for cbf resp. 4-12 ml/100g for cbv. this also holds for the 3d canine cbf maps from real data shown in figure 2 : the vessels are less blurred and the perfusion value range is closer to physiologically expected values in the fdk-jbf cbf map than in the fdk cbf map. furthermore the fdk-jbf maps are smoother in xz and yz viewing directions. in this work we evaluated the use of different denoising methods for perfusion c-arm ct with a rapid scanning protocol. all evaluated methods showed a perceptible improvement compared to standard fdk-type reconstruction. however, using art-based approaches this comes for the price of a much higher computational effort. thus we introduced a pure volume-based denoising technique with much smaller computational complexity and showed that it performs similarly to the art-based approaches. one problem of jbfbased approaches might be streak artifacts, which cannot be handled well by a bilateral filter. due to the low number of 133 projections they appear in the reconstructed volumes but vanish after subtraction of the baseline volumes. however, in real patient acquisitions they might not be always reproducible and the elimination of such artifacts represents a direction of future research. furthermore hardware improvements involving rotation speed and detector read out rate are also worth of investigation."
"for trichromatic mosaics, our algorithm correctly classified the cones in the retinal mosaic most of the time, with performance at ceiling for mosaic parameters most typical of human vision. the lowest accuracies are found when the l:m ratio is either very large or very small. one reason for this is that our flattening routine performs best when the number of cones of each type is approximately equal (see methods). this observation is equally true of the number of longer-wavelength-sensitive cone classes that our algorithm detects in the trichromatic simulations. the algorithm was most likely to detect only 1 longer-wavelengthsensitive cone class in the case of those simulations with extreme l:m cone ratios. it is possible that a better flattening routine or an embedding algorithm more attuned to the correlation structure of natural images could lead to selective increases in our algorithm's accuracy for the more extreme l:m ratios. on the other hand, it could be that the worsened flattening is a symptom reflecting a greater underlying difficulty in learning for mosaics with more extreme l:m ratios. in this case, the fact that learning performance degrades as cone ratios become asymmetric may indicate fundamental constraints on the mosaic parameters that support trichromacy. the decreased performance with more extreme l:m ratios was consistent across many variations in the details of the simulations (see figures s4-s11) ."
"since time information is not considered in the model construction procedure, the training and testing data set are reordered randomly. for model training, cross validation is used. specifically, the loo method is adopted. for the fivedriver data set, data of four drivers are used for model training and validation and the data of the remaining driver is used to test the classification performance. the general classification accuracy is the average of the five classification results. another hyperparameter for the ffnn is the number of neurons in the hidden layer. to evaluate the influence of neuron quantity on the classification performance, different neuron numbers and cross validation are studied. a boxplot of the classification results is shown in fig. 9 . the neuron numbers vary from 10 to 100 with an interval of 10. the red line represents the mean accuracy of the five drivers with different neurons. as shown in fig. 9, variation in the number of neurons does not significantly influence performance. the most accurate detection occurs at the 100-neuron cases, with an accuracy of approximately 81.2%. in section iv, more detailed statistical results using ffnn with 60 neurons are proposed, and the results are compared with multiple machine learning methods."
"where d c1 is the decorrelation distance. the second model is the double exponential model which is also a well-known and widely used model [cit] . it models the spatial correlation as a sum of two negative independent exponential functions,"
"obviously, an important building block in such an architecture are the bonus points themselves. this is the aspect we focus on in this paper: we show how an anonymous bonus system can be constructed. to this end, we consider the following scenario: alice is customer at several different merchants, including bob's and charlie's shops. bob and charlie both participate in the bonus point system operated by the system operator debbie. alice receives bonus points from debbie for every purchase she conducts at either of the merchants. bonus points are collected locally in a digital wallet on alice's personal mobile device, e. g., on a smartphone. after collecting some of these points, she can use them to pay for a purchase at one of the participating stores. of course, double spending (i. e., alice paying more than once using the same points) is to be avoided, while at the same time we want to achieve that alice does not need to reveal her identity towards any other party during the process. it should also not be possible to trace bonus points, i. e., the transaction or merchant where the used points have been generated should not be revealed."
"oversegmentation. unconstrained motion can be handled by supervoxel based methods [cit] . these methods generate an oversegmentation of the video into spacetime homogeneous, perceptually distinct regions. they are important for early stage video preprocessing, but do not directly solve the problem of video object segmentation as they do not provide any principled approach to flatten the hierarchical decomposition of the video into a binary segmentation [cit] . in section 6 we formulate the problem of reducing overlapping segments into a foreground-background partition by minimizing a novel energy function which we solve optimally by inference on a crf."
"abstract: microrna (mirna) expression profiles hold promise as biomarkers for diagnostics and prognosis of complex diseases. here we present a super-resolution fluorescence imaging-based digital profiling method for specific, sensitive, and multiplexed detection of mirnas. in particular, we applied dna-paint (point accumulation for imaging in nanoscale topography) method to implement a super-resolution geometric barcoding scheme for multiplexed singlemolecule mirna capture and digital counting. using synthetic dna nanostructures as programmable mirna capture \"nano-array\", we demonstrated high-specificity (single nucleotide mismatch discrimination), multiplexed (8-plex, 2 panels) and sensitive measurements on synthetic mirna samples, as well as applied one 8-plex panel to measure endogenous mirnas levels in total rna extract from hela cells."
"with target mirna concentrations ranging from 3 to 300 pm for each mirna target in the context of multiplexed assay ( figure 3) . representative images of dna origami grids at various mirna concentrations are shown in figure s4 . as expected, we observed higher normalized counts for higher concentrations of mirnas. furthermore, the logarithm of target concentration showed linear correlation with the logarithm of the normalized count for all 16 mirnas used in the 2 groups of multiplexed assays (figure 3, s6) . we estimated the theoretical limit of detection (lod) based on the measured normalized counts of the negative control samples (due to occasional origami distortions, poor imaging quality, or occasional non-specific probe binding events), converted to absolute concentration using the linear fits. the lods of all 16 mirnas tested in our experiments ranged from 100 fm to 7 pm without further optimizations (table s8) . we note that, even higher capture and detection sensitivity could be achieved with more stable anchor strands, e.g. using locked nucleic acids (lnas) strands. [cit] to investigate the mirna capture and detection specificity of our method, we tested samples containing different subset combinations of target mirnas arbitrarily chosen from the first group of 8 mirnas, with arbitrarily chosen concentrations (100 pm and 30 pm). the test was performed in the presence of all mirna capture strands and all mirna imager strands. we observed expected levels of signals for the on-target mirnas, with background-level signal for off-target mirna species, showing that our nano-array system can specifically capture and detect target mirnas with negligible crosstalk between different mirna species. (figure 4a, 4b, s7) ."
"in the last term, t i and t j are the corresponding frame numbers of proposals s i and s j, which reduces penalty for assigning different labels to proposals that are distant in time. to efficiently recover y * we use the framework of krähenbühl and koltun [cit], which provides a linear time o(n) algorithm for the inference of n variables on a fullyconnected graph based on a mean field approximation to the crf distribution. the efficiency of the method comes with the limitation that the pairwise potential must be expressed as a linear combination of gaussian kernels having the form:"
"algorithms for computing object proposals are generally designed to have a high recall, proposing at least one region for as many objects in the image as possible. while the set of candidates must remain of limited size, the task of selecting positive samples is left to later stages, and the ratio of regions that truly belong to an object, i.e. precision, is usually not considered a measure of performance."
"we propose an efficient alternative approach which exploits a fully connected spatiotemporal graph built over object proposals. we map our similarity term into a euclidian space, which is computationally efficient to optimize and well suited for modeling long-range connections. the fully connected nature of the graph implies information exchange between both spatially and temporally distant object proposals, which in turn enables our method to be robust to the difficult cases of fast frame-to-frame motion and object occlusions. we additionally propose an energy term that incorporates sparse but confident long range feature tracks, in order to ensure similar temporal labeling of objects. while previous approaches are constrained to the selection of one proposal per frame, our formulation enables the grouping of multiple overlapping proposals in the same frame, yielding robustness to outliers and incorrect proposal boundaries."
"in order to accurately classify elements of s, we must enforce a smoothness prior that says that similar proposals should be similarly classified. conditional random fields provide a natural framework to incorporate all mutual spatiotemporal relationships between proposals as well as our initial proposal confidences."
"in this work, we designed a square lattice pattern with 20 nm spacing on a two-dimensional, rectangular dna origami as our nano-array to implement the multiplexing strategy; the pattern consists of 4 boundary markers and 8 anchor strands, allowing simultaneous identification and quantification of 8 different mirna species (figure 1b, s1 ). higher multiplexing capability can be achieved by using larger nanostructures (e.g., dna tiles and bricks [11a, 11b] ) as nano-arrays or reducing the pattern spacing; alternatively, by using combinatorial designs of boundary markers as barcodes, a much higher multiplexing capacity could be afforded. in the present work, we selected 16 human mirnas that are abnormally expressed in multiple communication cancer types as demonstration targets (table s1), [1, 2c] which are separated into 2 panels of 8-plex assays. to satisfy the technical requirements for dna-paint, [9c] we first evaluated the blinking kinetics for individual mirna targets. in each test, we designed a test dna origami nanostructure with 8 anchor sites that are functionalized with the same anchor strand, all capturing the same mirna target with high efficiency and specificity (figure 2a) . the self-assembled dna origami nano-arrays were attached to glass slide surface via biotinstreptavidin linkage (figure s2), followed by incubation with a high concentration (1 nm) of the corresponding target mirna for 10 min at room temperature; dna-paint was performed in the presence of a cy3b-labeled imager strand complementary to the exposed single-stranded region of the target mirna sequence. we estimated the mirna capture efficiency by counting the number of observed targets on the nano-arrays, and characterized single-molecule blinking on-time by fitting the histogram of blinking event lengths represented as cumulative distribution function of exponential distribution ( figure 2b) . [9c, 12] to ensure suitable blinking kinetics for high-resolution dna-paint imaging (ideal blinking on-time in the range of 0.5 s ~ 2.5 s), we designed and tested various anchor strand and imager strand designs of different lengths and hybridization positions on the target mirna, and measured their individual capture efficiency and blinking kinetics. in particular, we tested the dependence of blinking on-time on the length and gc-content of the duplex formed by mirna and imager strand, as well as the base-stacking between 3′ end of the anchor strand and 5′ end of the imager strand, when bridged by the target mirna molecule. we noticed that the presence of this stacking interaction significantly increased the blinking ontime, giving us an extra degree of design freedom, also suggesting our system is potentially useful for single-molecule studies of nucleic acid base-stacking interactions. for each mirna, we chose a combination of anchor strand and imager strand sequences, that enables both stable capture of mirna target and suitable blinking kinetics for high resolution dna-paint imaging (figure 2c, s3, tables s4 -s6 ). we then designed a new nano-array pattern with 8 distinct anchor strands, each optimized with capture efficiency and dna-paint blinking on-time for a different target mirna, and tested the multiplexing capability of our method. our multiplexed mirna assay was performed in three steps. first, we took a preincubation super-resolution image of the immobilized, empty dna nano-arrays with boundary markers only, by using the marker-specific imager strand. next, we incubated the origami samples with target mirnas (3 -300 pm) for 30 -90 min. finally, we took a post-incubation super-resolution image using a combination of imager strands for both boundary markers and 8 target mirnas, allowing visualization of both the boundary markers and all mirnas captured by anchor strands on the dna nano-arrays (figure 1b) . to measure the number of mirna strands captured on our dna origami platforms, we compared the position and orientation of the super-resolution imaged boundary markers on each dna origami in the pre-incubation and post-incubation images. after image alignment based on the matched marker positions, we measured the number of captured mirna strands for each target by counting the number of visualized points at their specific anchor sites, on all origami nano-arrays. we normalized the counts by the total number of valid dna origamis for quantification (we will refer to it as normalized count hereafter). in a typical experiment, a total of 1000 ~ 1500 dna origami structures passed our quality control criteria, within a ~40 μm x 40 μm field of view. (see supplementary note 1.6 for details)."
"given the trained classifier c, we aim to roughly subdivide the set of object proposals s t extracted at frame t into two spatially disjoint sets s"
"the svm and, as consequence, the effectiveness of the crf is severely limited. while some of the results are comparable with other approaches, these limitations are the reason why our method performs significantly better on the fbms dataset."
"although only 8-plex simultaneous detection and quantification is demonstrated in the current study, a much higher degree of multiplexing could be achieved with our method, for example, by the combination of constructing a larger synthetic nanostructure array [11c] or using tightly packed capture probes (e.g. packing with ~5 nm spacing on the current dna origami platform would potentially allow ~200x different mirna species to be captured [9c] ), and spectrally-multiplexed imaging (e.g. four imaging channels with ~25 mirna-targeting imager probes each would potentially allow ~100x multiplexed imaging simultaneously). alternatively, by designing different boundary barcodes for different panels of mirna targets as barcodes, and using exchange-paint imaging method, a combinatorial multiplexing capacity could be afforded. a higher detection sensitivity could be achieved by using more stable anchor probes, scanning over a larger surface area, using a higher surface density of nanostructures, or more elaborate image analysis to further reduce false positive counts. our method could potentially provide an accurate and sensitive alternative for highly-multiplexed mirna profiling in both basic science and biotechnological applications."
"given as input a set of object proposals and a few annotated foreground proposals, our algorithm consists of three steps. initially, a rough classification and subsampling of the data is performed using a self-trained support vector machine (svm) classifier in order to reduce the size of the proposal space while preserving a large pool of candidate foreground proposals. next, maximum a posteriori (map) inference is performed on a fully connected conditional random field (crf) to determine the final labeling of the candidate proposals. finally, each labeled proposal casts a vote to all pixels that it overlaps. the aggregate result yields the final foreground-background segmentation. we compare our results with an existing benchmark dataset and show that our method outperforms several state-of-theart approaches."
"the featuresĉ,ŝ,p are euclidean vectors of 10, 20 and 50 dimensions respectively. note that the temporal term t is already euclidean, and so it does not require embedding."
unary potentials. the unary term ψ u is directly inferred from the output of the svm and the set of annotated proposalss. we formulate an updated conditional probability
", and scalar weights ω * . in order to distinguish proposals that have similar appearance but belong to different image regions we define the pairwise potential ψ p to be a linear combination of several terms that jointly incorporate color, spatial and temporal information:"
"with single-base mismatches or length heterogeneity have different gene regulatory effects and clinical roles; [1c, 2b] however, they are difficult to be detected specifically with direct hybridization based methods. [2a, 6] to further investigate the specificity of our method in detecting mirnas with highly similar sequences, we went on to performed two groups of tests on synthetic mirna sequences with a singlebase mismatch. (see table s1 for sequence details). in both cases, we observed an expected level of signal from the correct target mirna, while only background-level counts were observed for the single-base-mismatched target (figure 4c, s8), proving high specificity of our assay. these results demonstrated high specificity for mirna detection of our method, against both different mirnas and single-base mismatch mirnas."
"many established video segmentation algorithms are based on background subtraction. these approaches assume that the background is known a priori, and that the camera is stationary [cit] or undergoes a predictable, parametric motion [cit] . while this family of methods cannot handle non-rigid camera movements, they are well suited to specific application scenarios such as surveillance systems [cit] . in contrast our method is designed to handle unconstrained videos with arbitrary camera motion and non-rigid background."
"it is important to note, however, that the resulting set of proposals is likely imbalanced, with potentially many more proposals on background regions than on foreground, depending on object size. furthermore, many proposals will cover both foreground and background. these issues negatively impact segmentation, both in terms of quality and efficiency. to overcome this problem we self-train an svm classifier and resample the pool of proposals."
"we introduce a per-frame pruning step with the goal of rebalancing the set of proposals and selecting only those with higher discriminative power, i.e. those that do not overlap both with foreground and background. the choice of the svm is justified by its proven robustness to skewed vector spaces resulting from class imbalance [cit] and relatively fast performance. we train an svm classifier which operates on elements of s, separating those that overlap with foreground from those that belong to the background (section 5.1), and then resample the set (section 5.2). finally, we use the output of the svm to initialize the unaries of the crf (section 6.1)."
"in a sense, we have developed a nanoscale version of the microarray-based mirna detection method, implemented on an optically super-resolved synthetic nano-array. compared with microarray-based mirna detection methods, our super-resolved, digital counting method has two advantages: (i) sequencespecific mirna detection, especially, discrimination between single-nucleotide mismatch targets, (ii) accurate quantification of mirna concentration by single-molecule digital counting."
"fbms results. the fbms dataset consists of 59 sequences featuring typical challenges of unconstrained videos such as fast motion, motion blur, occlusions, and object appearance changes. the dataset is split into a training and testing set. since none of the methods we compare with requires a training phase, we measure performance on both figure 2 . top to bottom, left to right: qualitative video object segmentation results on six sequences (horses05, farm01, cats01, cars4, marple8 and people5) from the fbms dataset [cit] . our method demonstrates reasonable segmentation quality for challenging cases, e.g., non-rigid motion and considerable appearance changes (horse05, cats01). the rich set of features of the svm and the pairwise potentials of the crf make our method robust to cluttered background (farm1, cars4), while the fully connected graph on which we perform inference provides robustness to partial and full occlusions (marple8). the aggregation of object proposals is also effective for complex, multi-colored objects (people05)."
"in summary, we have developed a super-resolution geometric barcoding method for multiplexed detection and quantitation of mirnas by direct single-molecule detection and super-resolution geometric barcoding, that achieves high detection specificity and sensitivity. our method used programmable dna origami nanostructures with pre-designed nano-array patterns for multiplexed capture, and high-sensitivity dna-paint super-resolution method for amplification-free, single-molecule microscopy readout. in particular, we use the principle of repetitive binding for sensitive discrimination between closely related mirna sequences. by effectively partitioning the space into separate sub-compartments for each mirna species, we demonstrated as a proof of concept multiplexed measurements of 2 panels of 8 mirna species each, using a single fluorescent dye and the same optical path."
"here we present a multiplexed and highly sequence-specific method for mirna profiling, by combining the principle of repetitive probe binding [cit] and dna nanostructure-based superresolution position barcoding."
results in table 3 demonstrates that our method consistently produces a good segmentation yielding roughly a 10% improvement over the current state-of-the-art in terms of average performance. the importance of combining both the svm and crf steps is also apparent.
"the positive impact of our pruning and resampling step on the quality of the video segmentation is shown in section 8. the resulting classification can still be imprecise, but serves the purpose of rebalancing positive and negative instances. the union of the two newly generated sets"
"our method and sea are semi-supervised while the others are unsupervised. for a more informative and fairer comparison, we therefore removed any of the videos from the comparison in table 3, for which at least one of these unsupervised methods did not detect the object.we report detailed sequence evaluation for the test set and the average for the training set. please also refer to the supplementary material for a more detailed evaluation. we separately evaluate the steps of our algorithm: svm only, crf only, and the full approach fcp. corresponding precision, recall, and f-measure plots are shown in fig. 3 . as discussed in the implementation section, in the crf experiment we modified the parameters generating object proposals to produce roughly the same number of proposals that are retained during the pruning step."
"video object segmentation refers to the partitioning of a video into two disjoint sets of pixels representing a foreground object and background regions. the abundance of available literature on video segmentation reflects the importance of the topic, which is an essential building block for numerous applications including video editing and postprocessing, video retrieval, analysis of large video collections, activity recognition, and many more. many existing approaches are based on background subtraction, tracking of feature points and homogeneous regions, spatiotemporal graph-cuts, or hierarchical clustering. recently, methods leveraging advances in object recognition have gained popularity. these methods make use of per-frame object proposals and employ different techniques to select a set of temporally coherent segments, one per frame, typically by figure 1 . example segmentations using our approach on three sequences of the freiburg-berkeley motion segmentation dataset [cit] . top to bottom: we demonstrate robustness to challenging situations typical of unconstrained videos such as fastmotion and motion blur, color ambiguities between fore-and background, and partial occlusions. minimizing an energy function defined over a locally connected spatiotemporal graph. while these methods have achieved state-of-art performance [cit], the sparse graph structure limits their ability to segment videos with fast motion and occlusions."
"we quantitatively evaluate our approach and its components with respect to various state-of-the-art techniques on the freiburg-berkeley motion segmentation dataset (fbms [cit] ), see fig. 2 . please also refer to the supplemental material for details."
"sets. due to running-time and memory constraints of the prior approaches that we compare to, we limit the length of the videos to 75 frames. for the purpose of testing segmentation quality in the presence of fast motion we temporally subsample frames from videos exhibiting slow motion. we report the speed-up factor of each sequence in the supplementary material. in videos that have multiple objects we manually selected the one with dominant motion. similar to previous works [cit] we measure the segmentation quality in terms of intersection-over-union, which is invariant to image resolution and to the size of the foreground object."
"we tested the effect on capture efficiency and normalized counts with different mirna incubation time (30, 60, 90 min), and then chose 90 min as the standard incubation time for the following studies, as it maximizes the capture efficiency for reliable quantification (figure s5) . the observed overall capture efficiency is likely limited by both origami self-assembly defects and mirna sequence-dependent anchor binding stability. we then measured the calibration curves of the normalized counts"
"we presented a novel approach to segment objects in unconstrained videos, which provides state-of-the-art performance on challenging video data. in general we believe that, due to the constant increase in terms of video resolution and quality, even more complex benchmarks than fbms are required to provide real-world application scenarios for evaluating video segmentation algorithms. however, methods based on object proposals appear to be a great candidate for addressing the computational challenges arising from higher resolution video data, since the use of proposals greatly reduces computational complexity, allowing us to employ a fully connected crf over a complete video sequence. a similar, fully connected formulation at the pixel level would be infeasible. there exist several further opportunities for followup work. for example, to improve the final segmentation accuracy, it would be interesting to investigate approaches to combine the prediction of the crf in a more principled manner (e.g., incorporating higher-order potentials), or to employ bilateral filtering techniques to refine the proposal-based segmentation to the pixel level. table 3 . intersection-over-union comparisons on a subset of the fbms dataset. the columns svm and crf correspond to the results obtained using either only our svm-based classification, or only our crf-based labeling, respectively. our full approach (fcp) is generally (close to) the best performing one (highlighted in bold), and achieves the highest average values of all methods. table 4 . intersection-over-union computed on the segtrack dataset. on low resolution video, there are insufficient foreground proposals generated for our method to work well."
"we use lmds to conform the pairwise potential to eq. (4). we express pairwise potentials ψ p in eq. (3) as a linear combination of several terms. for better control of the resulting embedding error, we separately embed each of the components. for each d * term of eq. (3), we empirically determine the dimensionality of the embedding space from the analysis of their dissimilarity matrix eigenvalues. the resulting pairwise potential conforming to eq. (4) is:"
"to enable the use of arbitrary pairwise potentials we seek a new representation of the data in which the l 2 -norm is a good approximation to the distance of the original nonlinear space. in practice, given the original set of features f we seek a new embeddingf into the euclidean space r d s.t.:"
"finally, we tested our method for mirna detection from hela cell extract. we first prepared total rna extract from hela cells (at 400 ng/ul), then diluted the sample into imaging buffer (to 40 ng/ul) for single-molecule capture and quantification. we performed the test on the panel of 8 mirnas shown above. out of the 8 mirnas in our test, 5 showed concentrations above our system's lod. we observed the highest expression level for mir-21 (1.2+/-0.1 amol/ng), followed by mir-16, mir-145, mir-24, and mir-221; the other three mirnas showed concentrations below lod (figure 4d, s8) . these measurements were consistent with previous reports, and further confirmed the specific capture and multiplexed, accurate quantitation ability of our method ( figure s9 ). [cit]"
"adaboost is an iterative algorithm and the core idea is training weak classifiers from a training set, then gather some suitable weak classifiers to a strong classifier. the calculation of sample weight is according to classification is correct or not and the overall classification accuracy of last time. then training the modified new data sets with the next layer. finally grouping the best classifiers of each layer into the final decision classifier. using adaboost classifier can eliminate some unnecessary features and focus on key data training."
"this study is systematized into six sections. section 1 explains the introduction of this study. section 2 deliberates the related works. section 3 discusses the enhanced technique and its essential method. data classification using rbf neural network will be explained in section 4. a discussion of the experiments and outcomes of the study will be demonstrated in section 5. lastly, the study conclusions will be summarized in section 6."
"therefore, this paper proposed the covariance feature (cf) to improve detection speed based on cmf. comparing with cmf, the detection speed was greatly improved and cf is about cmf detection rate. comparing with hlf, the face detection rate was improved and elapsed time was less than hlf. this novel feature is concerned with two regions instead of matrix feature of four regions. it vastly reduces features and simplify the calculation of matrix feature threshold, thereby it improve speed of adaboost face detection ."
"the vastness of a joint framework is not far from being obviously true, in light of the way that a distinct framework has its inadequacy, and an upgraded framework is planned to supplement the deficiency of these distinct canny frameworks. a splendid blend of two-advance batching computation and key backslide procedures is queried, considering the true objective to complement the parameters of each portion of the framework by using the advantages of a distinct framework against its burdens while exciting each weak section distinct from the two frameworks to achieve consistency, trustworthiness and an exact sharp framework extendable for use in gathering. the suggested improved model is made out of selforganization map (som) calculation and strategic relapse utilized, interestingly to form a greater improved scheme with grouping rudiments in light of similar epilepsy disease features."
"the complexity time of the hybrid som-rbf was calculated based on theoretical time complexity. as a result of the epileptic seizure recognition data set structure (vectors of data) that contain of amount of columns (n) and amount of rows (m), the complexity time can be calculated as (n * m) and it is fit to the (n * m) class. where (n) is denotes the epileptic seizure features and (m) represents the epileptic seizure cases. an additional assessment between the suggested scheme and other epilepsy diagnosis methods are shown in table 3 . note that the integrated algorithm between the som and rbf classifier methods acquired optimal diagnosis outcomes based on the epileptic seizure recognition data set."
"(2)cf only need to calculate a covariance matrix to express all the cf value in each test image. however, a covariance matrix is a cmf. four relative sub-regions covariance matrix combine into a cmf."
"therefore, the goal of the present study was to generate a classification model using the dataset obtained from the uci data repository, a publicly available dataset. moreover, using a classification model, we tried to identify epilepsy patients with or without seizures."
"face detection(fd) refers to the process of defining the position, size, posture of face(if it exists) on the input image. as a special case of object detection, face detection problem has attracted extensive attention of researchers in last two decades due to its widely application in various fields. the fields are human-computer interfaces, content retrieval, digital video processing, visual monitoring [cit] and so on. in recent years a lot of face detection methods appeared [cit], and the statistical learning methods has become mainstream in pattern recognition field gradually. [cit], viola proposed adaboost face detection based on haar-like feature(hlf) [cit] . this method can obviously improve the detection rate and speed ."
"if we just use this five feature templates as show, there are 78460 matrix features in 20x20 pixel image. however, we may use more expansion hlf templates [cit] in practical applications. the detector with slightly better performance may contain thousands of hlf. hlf can describe the differences of local gray, most of the time it can achieve a fine detection result. but hlf describe the structure coarsely, it is sensitive to certain graph structure, such as edges, lines,etc. with the above problem, the cmf was proposed by hua et.al. [cit] . cmf itself is a covariance matrix [cit], each value of matrix is a covariance between each sample subregion. the detection process is as follow, firstly, the covariance between each sample are calculated, then combine a portion of covariance value into covariance matrix according to some rule. each covariance matrix is a cmf."
"in this paper, the covariance feature is proposed. it combines the advantages of traditional haar-like feature and covariance matrix feature. this paper also related the combination of cf and adaboost algorithm. the test results on mit+cmu test set showed cf can greatly improve training and test speed while the detection rate is similar to cmf. in recent years, many researches [cit] focus on multi-view face feature and the research results showed these improvements can detect non-front face preferably. in the future, the multi-view cf is a novel direction worthy of studying."
"(1)the representation of cf value is a two-relative region covariance, but the representation of cmf value is euclidean distance between feature matrix and template matrix."
"after the threshold of a test sample is achieved, if the euclidean distance between covariance matrix and template matrix is less than the threshold, the sample is considered to be a face,otherwise it considered a non-face. the weak classifier is as follow:"
"hlf calculate face feature by integral image which can boost computational efficiency. however, hlf is a coarse feature, it is sensitive to edge, line and can only describe specific graph structure. recently, covariance matrix feature(cmf) is proposed [cit], it can reflect the intrinsic relevance of image pixels, and overcome the shortcomings of hlf . nonetheless, the detection time is three times slower than hlf because the threshold calculation is too complex."
"in classical adaboost algorithm the weak classifier used haar-like features which named after similarity to haar wavelet. hlf is a kind of simple rectangle feature and it extract face feature fast. hlf value refers to the difference of gray level sum between the black rectangle and white rectangle on the test image. it reflects the differences of local image gray. figure 1 gives five basic templates of hlf feature. we can matching the feature template in every position of detection and achieve a hlf feature. the matrix in image window should meet the s-t criteria [cit] . the formula of quantity calculation is as the follow: (, )"
"to highlight the significance between this study's epilepsy diagnosis method using the rbf before and after the clustering process using the som technique, an independent sample t-test was applied as described [cit] . the attained results could be significant if the test value was less than 0.05. in table 3, the test value was (0.035) between this study's rbf before and after som clustering. this outcome indicates that the integrated som and rbf attained significant improvement on the accuracy outcomes. therefore, a deduction was drawn that there was significant variance before and after the clustering procedure. table 2 demonstrates the t-test statistical significance outcomes."
"the epileptic seizure recognition data set consists of five unique files, each with 100 documents, with each document representing a single person/subject. each document is a recording of brain activity for 23.6 seconds. the consistent time-series was cased into 4097 data points. each data point was the value of the eeg recording at a different point in time. the inspiration against this creation was to improve access to the data through the production of a .csv form. despite the fact that there were five classes, most authors have done binary classification, namely, class 1 (epileptic seizure) against the rest [cit] ."
"cmf and cf all have to calculate the covariance between each sub-region in common, they all can describe correlative degree of each sub-region. the difference of cmf and cf is as follows:"
"shoeb and guttag [cit] presented a paper in which the machine learning algorithm ''svm'' was applied to a scalp eeg dataset to detect epileptic seizures. this approach achieved 96% accuracy in terms of test data. [cit] performed a preformation comparison of bayesnet, decision table, ibk, j48/c4.5, and vfi prediction methods for eeg-based epileptic seizure data to classify seizures. finally, through ibk, a classifier of 99% accuracy was achieved. by contrast, vfi had the lowest accuracy, and j48 had the most stable accuracy."
"this study focused on the design of a global prediction model of conservative som by integrating the impression of a neural network classification algorithm with som in what we call a nn-som. in this study, an integrated technique that was created by clustering (data reduction) and classification approaches will be suggested. it is comprised of radial basis function neural networks (rbf), involving supervised learning, and a self-organization map algorithm (som), involving unsupervised learning. this integrated technique works on the principle of competitive learning, co-operation, and synaptic."
"the suggested technique has integrated self-organization map (som) clustering algorithm and rbf neural network in order to make it effective and efficient. the combined methods are then utilized through different stages, including preprocessing (organized the eeg-dataset into learning and testing parts) and calculating the similarity between the elements for each attribute. the suggested framework is shown in figure 1 ."
"pre-processing is a significant machine learning stage to cook the corpus before the learning process. in the current study, the corpus was divided into a learning (training) phase and examining (testing) phase. for cooking the corpus, numerous baseline data sets for epilepsy disease prediction and detection roles were used [cit] . one of these data sets is named epileptic seizure recognition data set, which was described by the uci data set. the chief purpose of this data set was to classify and examine epilepsy status patents."
"the rbf network framework was introduced by broomhead and lowe [cit] . a neural network is considered one of the significant learning techniques for data prediction. the rbf network organization has its basis in the traditional estimate hypothesis. it has the ability of widespread estimation. the rbf organization is a well-known, contrasting option to the outstanding multilayer perceptron (mlp), since it has an easier structure and a considerably speedier preparing procedure. the rbf arrangement has its cause in performing precise addition of an arrangement of information, focused in a multidimensional space [cit] . it can be viewed as one sort of practical connection net [cit] . it has a system design similar to the traditional regularization arrangement [cit], in which the premise capacities are the green's elements of the gram's administrator related with the stabilizer. on the off-chance that the stabilizer displays outspread symmetry, an rbf organize is obtained. from the perspective of an estimation hypothesis, the regularization organization has three attractive properties [cit] : it can surmise any multivariate consistent capacity on a smaller space to a subjective exactness, given an adequate number of units; it has the best estimation property since the obscure coefficients are straight; and the arrangement is ideal by limiting a practical containing a regularization term."
"electroencephalogram (eeg) is used to capture brain signals. in eeg, there are five bands (delta, theta, alpha, beta, and gamma) that are generally used for clinical analysis [cit] . hens berger, a german neurologist, was the first to use an eeg to record the electrical action of the human brain. electrodes are placed at several situations on the scalp to record the neural activity of the cerebral cortex. the voltage differences are measured between each pair of scalp electrodes. during the initial stages of eeg scanning of the brain, normal activity is observed; however, very high amplitude and rhythmic activity is soon observed for some time. later, the signal again returns to normal. these rhythmic activities are called spikes during a seizure. these seizures are very short, and a seizure patient may not be aware of it. during seizures, complex spike-and-wave patterns generated by the brain can be recorded on the electroencephalogram (eeg). commonly, diagnosis of epilepsy is made by a neurologist [cit] . we need a trained specialist to perform the interpretation. eeg captures and clarification is time-consuming and exclusive. furthermore, it is difficult to detect early stages of epilepsy. therefore, automatic computer-based detection of seizure activity is a requirement. a machine learning algorithm-based predictive model can be used to differentiate between patients with and without seizures."
this experiment targeted to filter and detect eeg status as an epilepsy disease diagnosis. a technique was performed by looking for epilepsy status within the eeg data. the samples were divided into 10 categories. each category had a specific number of samples (epilepsy cases). nine sets were considered as learning phases while the remaining one was considered for testing dataset in a cross-validation process. the experiments were executed before data collection and after clustering using som. the basis of these experiments was to highlight the robustness of the prediction and classification of rbf neural network when integrated with the som algorithm.
"figures 4 and 5 show both learning and testing results of the rbf and som-rbf algorithms. the 10-fold cross-validations were computed, and the average diagnosis outcomes using rbf without clustering achieved 77.5% and 70.9% for learning and testing tests, respectively. the figures also demonstrate the obtained outcomes of the som-rbf algorithm with 96.05% in the learning and 97.47% in the testing sets. the best diagnosis accuracy with rbf was only obtained in experiment number 2 with 97.9%."
"in the literature, there are many approaches used to expolit the features from eeg data: 1) genetic algorithm (ga); 2) autoregressive (ar); 3) discrete wavelet transform (dwt); 4) particle swarm optimization (pso); and 5) fourier transforms (ft), among others [cit] . among all the aforementioned methods, dwt was the best method for features extraction for eeg data. through dwt, it is easier to capture repeated, irregular and sudden changes in signals. second, feature reduction could be done using a scatter matrix. finally, by applying classification techniques (i.e., k-nearest neighbor (k-nn), artificial neural network (ann), naive bayesian (nb), support vector machine (svm), with different kernel, decision tree, k-means clustering (k-mc) and self-organizing maps (soms)), patients in various phases, namely, (1) pre-seizure, 2) seizure and 3) seizure-free, can be differentiated [cit] ."
"( ) j h x is the judge value of sample x, the threshold offset is j p, the value only can be 1 ±, the threshold is following are the procedure of adaboost classifier learning:"
"in the formula (2), j is the current cmf and a is the current sample, [ ] a j s i is the jth covariance value of sample a, t[i] is the corresponding template value. then calculating the distance mean value of every cmf, the mean value is the final threshold for judgment. variable k is the sum of samples. the threshold formula of feature j is as follow:"
"in order to carry out the experimentation, the epileptic seizure recognition data was used. as it has been mentioned, the 10-fold cross-validation strategy was used in preparing and testing the data set. the examination connected across the dataset using an rbf classifier without clustering and with clustering results to examine the enhancement outcomes of the integrated method. the cross-validations procedure achieved judgment accuracy outcomes as follows: in the experiments, the epileptic seizure recognition data set were used as part of the request to decide the status (epilepsy or not). the integrated method was performed by learning and testing the epileptic seizure recognition data set using the kohonen-rbf technique. using the kohonen method, the eeg data set was then distributed into several clusters with each cluster having several cases. the primary goals of clustering in this study were to produce new structures and patterns by gathering epilepsy cases with related patterns together. therefore, the computational efficiency will be decreased and the diagnosis prediction will be precise. the obtained performances from the learning and testing procedure on the epileptic seizure recognition data set are represented in table 1 and table 2, in which arrangement outcomes were acquired by rbf method and som-rbf method respectively. in the mixture procedure, the yield of the som was used as a new attribute component to label each record in the epileptic seizure recognition data set with a cluster label and rate of belonging to this cluster. this component can increase the coherence among the records by gathering the epileptic seizure recognition data set into various clusters, each with similar records. the rbf technique was used again with the yield of the kohonen algorithm for potential conceivable high diagnosis precision. a 10-fold cross-validation strategy was volume 7, 2019 performed in the learning and testing procedures with and without som. each learning and testing examination used epileptic seizure recognition data set attributes as input layers to the rbf. then, the output layer as a class component was used (from 1 to 5 status). the results of the rbf with kohonen clustering showed enhanced outcomes when the rbf algorithm classified the epileptic seizure recognition data set with the kohonen clustering output. curiously, the som-rbf technique improved the diagnosis precision by 97.47%, as shown in table 1 ."
"epilepsy is a chronic neurological disease affecting approximately 70 million persons universal. an abrupt burst of additional electricity in the brain generates abnormal movements resulting in unexpected seizure attacks [cit] . these attacks are a major sign of epilepsy. around 8% of people will have a seizure during their lifetime, but only 1-2% will be diagnosed with epilepsy [cit] . recurrent epileptic seizures affect the lives of patients and their families. sometimes these seizures lead to death."
"this investigation considered one of the fundamental challenges of the epilepsy disease. seizure detection and prediction give new and independent focus on diagnosis for the analysis, the intervention and the treatment of epilepsy. this framework may take into consideration the determination and recognition of seizures before their clinical onset. the proposed method aimed to examine epilepsy cases based on the som-rbf method to classify epilepsy status. the main contribution of this study was a combined method of the rbf and som clustering techniques to detect and determine epilepsy status. the advantage of applying the som clustering algorithm was to assemble similar epilepsy cases to investigate the shape of epilepsy by concentrating on their patient shapes regarding when epilepsy occurs. the suggested scheme used a uci epileptic seizure recognition data set to construct the diagnosis framework. the suggested diagnosis solution was verified using t-test statistically significant tests to suggest improvement before and after the clustering procedure. we showed that som-rbf can significantly augment diagnosis outcomes and reduce the misdiagnosis of epilepsy."
"in the formula (6), the ith, jth sub-region is interpreted as a two dimension random variable and evaluate the covariance coefficient c(i,j). in virtue of the covariance symmetry and the covariance of sub-region with itself is insignificant, we can extract"
"in recent years, many studies on the prediction and classification of epilepsy and seizure have been done. the detection of seizure in epilepsy is primarily focused on both imaging and eeg data. analysis and interpretation of eeg signals require expertise, and they are time-consuming. in this regard, we are focused on data mining and machine learning-based techniques."
"we chose 356 classifiers of higher detection rate from 4064 weak classifiers after training, the detection sub-regions are distribute mainly beside eyes, eyebrows, nose, mouth. these region can distinguish the face and non-face effectively."
i. each node's weights were initialized. ii. a vector was selected at random from the set of training data and was presented to the network. iii. every node in the network was tested to compute which ones' weights were most similar to the input vector. the winning was usually identified as the best matching unit (bmu). (equation 1 ). the bmu was calculated as:
"the kohonen's self-organizing map algorithm clusters the instances in groups. using som in epilepsy, the dataset produces low-dimensional (typically 1-d, 2-d, and 3-d) or reduced dimensionality of the dataset. it is based on the minimum euclidean distance between an input vector and a weight vector. finally, we generated non-linear decision boundaries using rbf to identify seizures in the dataset. moreover, the predicted target values of an item were the same as other items that had close values to the predictor variables."
"for all of the subsequent measurements, we used the same computer. it has an intel core i7-6700k cpu running at 4.00ghz, 16 gb of ram, a geforce gtx titan x gpu, and it runs a 64-bit ubuntu linux v16.04."
"from an ethical perspective, delivering web-based interventions in the area of mental health could be considered a challenge; thus, security also needed to be addressed during the development process. to prevent information being monitored by a third party, the information sent to and from the user should be encrypted according to ssl (secure sockets layer) security mechanisms. another ethical issue arises from the fact that symptoms of clinical depression and anxiety may overlap with stress. persons with high scores on hospital anxiety and depression scale (hads) may need other support, due to their more vulnerable condition. the screening in msc ensures that the users are experiencing levels of stress that do not present clinical signs of depression or anxiety, for which the programme was not developed. hads has the advantage of being one among few instruments for depression and anxiety which is validated for samples without a psychiatric diagnosis, thus it is reliable to be used for the present purpose."
"the possible end users were recruited from the staff at a university. they received oral and written information about the aim of the study, were given the change to ask questions about the study and were given access to the programme after they signed an informed consent. the possible end users were instructed to proceed through the programme at a more rapid speed than what the programme had originally been designed for. inclusion criteria for the participants in the usability test were a perceived stress score measured with pss-14 [cit], of 17 or higher [cit], being employed, being 18-65 years old, being able to speak and understand the swedish language and consented to take part in the study. exclusion criteria included currently being on sick-leave or scoring 11 or more on either of the subscales on the hads [cit] . the study participants were monitored by the first author through an administration tool in the web application regarding how far in the programme they came. after they finished participation the participants were also asked whether they had reached the final module of the programme."
the films include rationale for the last part of the programme respectively how to seek social support is related to stress management rehearsal. evaluation of satisfaction is graded on an 11-point likert scale. low satisfaction leads to advice where to find more support for stress-related problems.
"during the evaluation, wherever it was possible, we compared the performance of our method with the best state-of-the-art competitors. nevertheless, a direct comparison is hard as different datasets and detection measures are used in state-of-the-art system evaluations. thus, we compared the metrics we found in the relevant publications."
"in this section, we describe two real-world use cases where the presented system can be used. the first one is a live system that will assist medical doctors during endoscopies. currently, we are deploying a proof-of-applicabilty prototype in one of our partner hospitals. the second is a system that will automatically analyze videos captured by vces. several hospitals are involved in this more concrete and applied research, and currently we are setting up the data-sharing agreements and collect the data for a new multi-disease dataset that will be released open-source. the first use case requires fast and reliable processing, and the second requires a system that is able to process a large amount of data in a reliable and scalable way."
"for features extraction in the detection step and for the training procedure, the indexing is performed using the basic eir indexer implementation [cit] . the same set of two global features, namely tamura and jcd, is used. these features were selected by a simple features efficiency estimation by testing different combinations of features on smaller reference datasets to find the best combinations in terms of processing speed and classification accuracy. the selected features can be combined in two different ways. the first is called feature values fusion or early fusion, and it basically combines the feature value vectors of the different features into a single representation before they are used in a decision-making step. the second one is called decision fusion or late fusion where the features are combined after a decision-making step. our multi-class global-feature-based approach implements feature combination using the late fusion."
"regarding the included stress management strategies, the strength of the evidence varies. some strategies' effects have been studied in meta-analyses, for example pleasant activity scheduling [cit], whereas others have a less evidence, for example assertiveness training [cit] . in a recent meta-analysis of employee burnout, it was found that high-quality studies on interventions are scarce and that the interventions need to be more tailored than existing interventions [cit] . one further problem is that there is no consensus in the literature on how to label different stress management strategies [cit] . for example, mindfulness includes cognitive restructuring and relaxation techniques. this makes it difficult to compare stress management interventions in order to decide upon the most effective strategies."
"the results from our experimental evaluation show that, (i) the detection and localization accuracy can reach the same performance or outperform other current state-of-the-art methods, (ii) the processing performance enables frame rates for real-time analysis at high definition resolutions, (iii) the localization-system performance can be improved further using a combination of our basic localization algorithms and neural network approaches, (iv) in our experiments, the global-feature multi-class detection approach slightly outperforms the deep learning neural network approach both in training speed and detection performance, and (v) the system proves to be easily extended by adding new types of abnormalities. thereby, a system based on global features seems to be preferable and gives better performance in multi-class object detection than given existing deep learning network approaches. for the localization, additional research is needed to achieve better performance using a combination of local feature detection and deep learning neural networks."
"the deep learning neural network frameworks tested in this experiment require training before they are able to perform polyp localization. thus, both networks were trained using their default model training parameters. for tensorbox, the neural network model training the green crosses correspond to the true positives, the red correspond to the false positives took 6.5 days, and for darknet-yolo, we needed 5.1 days. after the training, we performed model validation using the corresponding frameworks' routines, and the training dataset as input. the validation confirmed the correctness of the trained models for both tensorbox and darknet-yolo. the deep learning approaches are capable of correctly localizing polyps that were previously detected by the detection subsystem within the training dataset with 98% accuracy for the tensorbox model and 95% accuracy for the darknet-yolo model."
"during the detection step, a term-based query from the hashed feature values of the query image is created for each image, and a comparison with all images in the index is performed, resulting in a ranked list of similar images. the ranked list is sorted by a distance or dissimilarity function associated with the low-level features. this is done by computing the distance between the query image and all images in the index. the distance function for our ranking is the tanimoto distance [cit] . a smaller distance between an image in the index and the query image means a better rank [cit] . the final ranked list is used in the classification step, which implements a simple k-nearest neighbors algorithm [cit] . this algorithm can be used for supervised and unsupervised learning, two or multi-class classification and different types of input data ranging from features extracted from images to videos to meta-data. its main advantages are its simplicity, that it achieves state-of-the-art classification results and that it is very fast in terms of processing time."
"step 3 included the development of tailoring to meet individuals' specific needs for stress management. a tailoring tool, the 32-item symptoms of stress survey (syss), was developed for this purpose. the syss was informed by the calgary symptoms of stress inventory [cit], the karolinska sleep questionnaire (nordin, åkerstedt, & [cit] ), the perceived stress scale [cit] and literature relevant to each stress management technique. the members of the research group worked individually and connected the stress-related symptoms from the syss to the stress management strategies. a consensus within the research group was reached regarding the connecting of stress management strategies to the respective symptoms in the syss."
a possible end user was given access to the first version of the programme and was instructed to click through the programme while the first author was present and note whether anything needed to use the programme was difficult to understand (step 10). the remaining bugs were identified and rectified.
"the aim of the live system is to provide live feedback to the doctors, i.e., a computer-aided diagnosis in real-time. while the endoscopist performs the colonoscopy, the system analyzes the video frames that are captured by the colonoscope. to provide helpful information for the operating doctor, we combine the visual information from the endoscope with our marks. for the detection, we alter the frame borders and show the name of the detected finding in the auxiliary area of the endoscope device monitor. for the implemented localization classes, we put a cross on top of the localized findings (polyps in this system version). at the moment, we have implemented a demo version of the live system [cit] . the live demo supports detection and localization of polyps. it is able to process a fullhd video stream with 30 fps in real-time. an example of the graphical output of the live system is depicted in fig. 16 . bold numbers shows the balanced f-score of each proposed method in addition to supporting the medical expert during the colonoscopy, we are working on an extension of the system, where the system is used to document the examination procedure. we will implement the generation of a document with an overview of the colonoscopic procedure. the doctors will be able to make changes or corrections, and add additional information to that document. the document will be stored or used as an appendix to the written endoscopy report."
"for the content of msc regarding the theoretical framework, stress management strategies, tailoring, and behaviour change techniques coded according to the behaviour change taxonomy [cit] see table 1 ."
"in this paper, a complex automated diagnosis system built for different gi tract disease detection scenarios, colonic polyp localization and big dataset visualization has been presented. we briefly described the whole system from data collection for medical knowledge transfer and system learning, evaluation of the experimental results to visualization of the findings. a detailed evaluation of detection of multiple endoscopic findings, polyp-localization accuracy and system performance has been performed. we introduced two new multi-class classification methods, one based on a deep learning neural network approach and another new multi-class classification algorithm based on global image features. for the localization, we evaluated existing localization approaches based on deep learning neural networks and compared the results to our initial localization method."
"in the final step, 15 possible end users were given access to the final version of the programme so the usability of the platform could be studied. more specifically, what was studied was whether the possible end users understood how to proceed to the programme end."
"stress is a major cause of sick leave in many western countries. work with stress management for those at risk of stress-related ill health is often restricted, due to a lack of healthcare resources. web applications make stress management available for unlimited numbers of users, but there is a gap in the literature regarding development of complex web-based selfmanagement programmes. we suggest a development process for tailored web-based self-management. a web-based programme, including tools identified in previous research as effective, can contribute to individuals' learning about stress and to supporting long-term behavioural change to prevent stress from being harmful and leading to decreased health. the development process ended in the web-based programme my stress control. although it was developed from current evidence in multiple fields, users had trouble reaching the end module. thus, further development is essential to enhance the usability of the programme."
(continued) [cit] the first test of the programme with a single end user resulted in clarifications to the programme and the development of text in the introduction module on how to handle the programme.
"stress management strategies connected to the yes statements in syss will determine the recommendation of strategies. regarding the soc the users will answer questions on how ready they are to start the specific strategy. the programme can steer the user in three directions. they can be led to an ambivalence module, start the module or, if they are in maintenance stage for the current stress management strategy, to continue further in the programme."
"other promising polyp detection approaches utilize quite old, but recently reborn neural networks and their advanced implementation called deep learning neural networks. neural networks are conceptually easy to understand, and large amounts of research has been done in this direction in the last years. results recently reported on, for example, the imagenet dataset, look promising [cit] in the areas of indexing, retrieving, organizing and annotating multimedia data. despite the fact that the neural network model training process is very complicated and time-consuming [cit], their ability to detect and localize various objects can potentially help us to improve our system. however, such an improvement is possible only after careful investigation, to ensure that our system will still run in real-time and be able to deal with the required amount of lesion categories. this is important since we deal with patient health, and the outcome can make the difference between life and death."
"our first version [cit] was developed for detection of polyps, i.e., possible cancer precursors, and it was built on content-based information retrieval methodology using global image features for image content analysis. in this paper, the next generation of our system is presented, where we extend our system using out-of-the-box and improved deep learning neural network approaches and multi-class global-feature classification methods for detection and localization of endoscopic findings. we evaluate our prototype by training new and improved classifiers that are based on various image-recognition approaches. we compare the performance of feature-based analysis and neural network-based analysis in terms of accuracy and real-time processing, and thereby evaluate the different approaches for feasibility of multi-class detection and colonic polyp localization in real use-case scenarios."
"the use of a programme can lead to behaviour change and symptom improvement through different mechanisms of change, and behaviour change can be sustained through treatment maintenance [cit] ). msc supports the user in learning stress management strategies relevant to the person's specific stressrelated problem, which might also produce positive emotions towards the programme. positive emotions towards web-based interventions have been described as being important for adherence [cit] . the fact that the programming was done using a responsiveness design [cit] can also enhance the availability for users, as they can work in the programme on several devices, thus positively affecting adherence. msc is an extensive programme and can be considered overwhelming for users; this may have an adverse effect on stress levels and also on adherence. therefore, it is clarified in the programme that the modules, the films and the web-based assignments are not mandatory for completion of the programme"
"the classifier can be used to classify video frames from an input video into as many classes as the detection subsystem model consists of. the classifier uses indexes generated by the indexer. in contrast to other classifiers that are commonly used, this classifier is not trained in a separate learning step. instead, the classifier searches previously generated indexes, which can be seen as the model, for similar visual features. the output is weighted based on the ranked list of the search results. based on this, a decision is made. the classifier is parallelized and can utilize multiple cpu cores for the extraction of features and the searching in indexes. to increase performance even more, we implemented the most compute intensive parts of the system with gpu computation support."
"based on the evidence found, discussions within the research team led to decisions on the theoretical framework and on which stress management strategies and behaviour change techniques could be feasible in a web-based self-management programme."
"detection of diseases in the gi tract has so far primarily focused on polyps. this is most probably due to the lack of alternative data in the medical field, but also that polyps are precursors of crc. several algorithms, methods and partial systems have, at first glance, achieved promising results [cit] in their respective testing environment. however, none of the related works is able to perform real-time detection or support doctors by computeraided diagnosis in real-time during colonoscopies. furthermore, all of them are limited to a very specific use case, which in most cases is polyp detection for a specific type of camera [cit] . furthermore, in some cases, it is unclear how well the approach would perform as a real system used in hospitals. most of the research conducted in this field uses rather small amounts of training and testing data, making it difficult to generalize the methods beyond the specific cleansed and prepared datasets and test scenarios. therefore, overfitting for the specific datasets can be a problem and can lead to unreliable results."
"the darknet-yolo approach introduces a custom cnn, designed to simultaneously predict multiple bounding boxes and class probabilities for these boxes within each input frame. the main advantage of the algorithm is that the cnn sees the entire image during the training process, so it implicitly encodes contextual information about classes as well as their appearance, resulting in a better generalization of objects' representation. the custom cnn in this approach is also inspired by the googlenet [cit] model."
"next, we performed a main localization run of both frameworks on the test dataset and validation using the corresponding ground truth. both tensorbox and darknet-yolo can be finely tuned by setting confidence threshold values, which limits the number of returned location rectangles to only highly confident ones. in order to investigate how the output of both can be affected by a confidence threshold value, it was set to zero during the first test run, which should give us the full unfiltered localization output. the reason for studying this dependency is that it is the only network tuning parameter in the unseen data process mode, which can help us to maximize their localization accuracy. figure 10 shows a histogram of true polyps' area coverage by location boxes found by tensorbox. we counted only location boxes that cover at least one pixel of a true polyp area. as one can see, the histogram has clearly visible maximum around 16% coverage rate, followed by an exponential decrease to almost constant level. a comparable analysis with the same type of histogram for the darknet-yolo output is depicted in fig. 11 . we observe a similar distribution for coverage rate (higher than 10%). a much higher number of location rectangles with zero coverage rate indicates that tensorbox implements additional localization result filtering. thus, the effect of the confidence threshold level adjustment cannot be as significant as for darknet-yolo, which has the expected output with a high number of location boxes covering small parts of true polyp areas. therefore, darknet-yolo should show a strong response to confidence threshold level. for the following validation and performance evaluation of both frameworks, we used 10% as the threshold value for the minimal required polyp ground truth coverage for true positive events, i.e., 10% must be covered for the event fig. 10 the histogram shows polyps area coverage by location boxes found by the tensorbox localization algorithm with the maximum around 16% coverage rate with following exponential decrease to the almost constant level. the low number of found location rectangles around zero coverage rate is an evidence of some output results pre-filtering to be counted. figures 12 and 13 confirm our assumption about output result filtering in tensorbox. its output contains a relatively small number of found locations with high number of highly-confident locations compared to darknet-yolo, which has a large number of low-confident locations, exactly as expected with the choice of a zero-confidence threshold. fig. 11 the histogram shows polyps area coverage by location boxes found by the darknet-yolo localization algorithm with near to exponential distribution for coverage rate higher than 10%. the higher number of found location rectangles around zero coverage rate gives clear indications that algorithm output unfiltered results the performance results depending on the confidence threshold value are depicted in fig. 14 for tensorbox and fig. 15 for darknet-yolo. as one can see, tensorbox localization performance does not depend on the confidence threshold value in any significant way. the best performance in terms of minimizing the number of false negative events with an acceptable number of false positive events can be achieved by maximizing the algorithm's accuracy metrics. for tensorbox, the maximum accuracy reaches a level of 31.6% for a confidence threshold value of zero with a corresponding polyp miss rate of 66.2%. for tensorbox, this is the best value, and it cannot be improved by adjusting the confidence threshold value. for darknet-yolo, maximum accuracy is reached at a 42.2% confidence threshold. the accuracy is 8% with a corresponding polyp miss rate of 47.9%. darknet-yolo showed more flexibility and a good response to the confidence threshold value. for darknet-yolo, the polyp miss rate can be significantly reduced by decreasing the confidence threshold value, but this gives a significant increase in the number of false positives, making the whole system too noisy. nevertheless, combining darknet-yolo and the basic eir localizer approaches can potentially give better overall system performance and better polyp miss rate. performing a comparison with well-known existing approaches in polyp localization is difficult due to lack of publicly available information (see table 7 ) about other researchers' algorithms' performance and evaluation methods, and due to prevalent non-disclosure restrictions that prevent sharing of datasets in the research community. the available data shows, that our eir basic localization approach has good performance with an f1 score of 41.6%."
"the detection subsystem performs lesion recognition and classification. it is intended for abnormality-presence detection without searching for the precise position of the lesion. the detection is performed using various visual similarity finding techniques. for each lesion that has to be detected, we use a set of reference frames that contains examples of this lesion occurring in different parts of the gi tract. this set can be seen as the model of the specific disease. we also use sets of frames containing examples of all kinds of healthy tissue, normal findings like stool, food, liquids, etc. the final goals of the detection subsystem is to decide if this particular frame analyzed contain any lesion or not, and to detect the exact type of the lesion. the detection system is designed in a modular way and can easily be extended with new diseases. this would, for example, allow not only to detect a polyp, but to distinguish between a polyp with low or high risk for developing crc by using the nice classification. 2"
"the development process (described in a flowchart, figure 1 ) involved three phases. steps 1-3 represent phase i, steps 4-7 represent phase ii and steps 8-10 represent phase iii."
"in our research, we aim for an algorithmic system that detects multiple mucosal pathologies in videos of the gi tract. the idea is to assist endoscopists (physicians, who are highly trained in the procedure) during live examinations. additionally, alternatives to traditional endoscopic examinations have recently emerged with the development of non-invasive vces. the gi tract is visualized using a pill-sized camera (available from vendors such as medtronics/given and olympus) that is swallowed and then records a video of the entire gi tract. the challenge in this context is that medical experts still need to view the fulllength video. our system should provide a scalable tool that can be used in a first-order population screening system where the vce-recorded video is used to determine whether an additional traditional endoscopic examination is needed or not. as a first step, we target the detection and the localization of colorectal polyps, which are known precursors of crc (see for example fig. 1d ). the reason for starting with this scenario is that most colon cancers arise from benign, adenomatous polyps (around 20%) containing dysplastic cells, which may progress to cancer. detection and removal of polyps prevent the development of cancer, and the risk of getting crc in the following 60 months after a colonoscopy depends largely on the endoscopist's ability to detect polyps [cit] . next, we extend our system to support detection of multiple abnormalities and diseases of the gi tract (see fig. 1 ) by training the classifiers using multi-class datasets."
"3. within the modules for the stress management strategies, the user's state whether they are ready to change their behaviour according to soc regarding the specific stress management strategy."
this study was conducted by a multidisciplinary research team. the first and last authors are physiotherapists working within the field of behavioural medicine. caroline eklund is a phd student whose main interest is in supporting individuals performing health-related behaviour change where various stressors hinder starting a new behaviour.
the processing performance of both deep-eir and global-feature-based eir in terms of processing speed meets real-time demands with a good margin for the real-time medical use case. both can process full hd images at a frame rate of 30 frames per second.
"the novelty of the research includes an end-to-end implementation of the whole eir system pipeline, from frame capture, annotation and analysis to user (doctor) feedback, as a combination of many out-of-the-box and modified existing components, as well as several new ones. the experiments showed that the proposed system (i.e., both the global feature-based and the neural network-based implementations) can achieve equal results to state-of-the-art methods in terms of detection performance for state-of-the-art endoscopic data, and a comparable localization performance. further, we showed that the new eir system outperforms state-of-the-art systems in terms of system performance, that it scales in terms of data throughput and that it can be used in a real-time scenario. we concluded, based on our initial experiments, that the global features multi-class detection approach slightly outperforms the tested neural network approaches, and that the localization algorithm can benefit from combining local features and neural network approaches. we also presented automatic analysis of vce videos and live support of colonoscopies as two real-world use cases that can potentially benefit from the proposed system where clinical tests are currently being planned in our partner hospitals. the experimental evaluation of the system as well as dataset creation are performed in collaboration with the cancer registry of norway, and in the near future, the system will be tested in a real-world environment, i.e., it will have a real societal impact."
targeting stress-level is screened with the perceived stress scale 14 (pss-14) [cit] . depression and anxiety are screened with the hospital anxiety and depression scale (hads) [cit] none.
"the subsystems for algorithmic analysis are designed in a modular way, so that they can be extended to different diseases or subcategories of diseases, as well as other tasks like size determination, etc. currently, we have implemented two types of analysis subsystems: the detection subsystem that detects different irregularities in video frames and images, and the localization subsystem that localizes the exact position of the disease (only polyp localization is supported at the moment) in the frame. the detection subsystem is not designed to determine the location of the detected irregularity. the exact lesion position finding is done by the localization subsystem, so that we can use the same localization subsystem for different detection subsystems. the localization subsystem uses the output of the detection system as input and processes only frames marked as containing a localizable disease."
"the performance comparison of different multi-class detection and classification approaches in terms of frame processing speed is depicted in fig. 7 . deep-eir, multi-class global feature-based eir and basic eir perform better in terms of speed than competitors. the single-class basic eir detector can process up to 300 full hd frames per second (for a gpu-accelerated implementation) [cit] . deep-and global feature-based eir classifiers showed 30 frames per second, which fits our medical use case. for further processing speed improvements, we plan to implement additional gpu acceleration for a random-trees not all performance measurements are available for all methods, but including all available information gives an idea about each method's performance"
"in summary, related work primarily targets specialized problems or elements of the more general, holistic medical problem we are attempting to solve. existing systems are either (i) too narrow for a flexible, multi-disease detection system; (ii) have been tested on limited datasets too small to show whether the method would work in a real scenario, or; (iii) provide a processing performance too low for a real-time system or ignore the system performance entirely. last, but not least, we are targeting a holistic end-to-end system where a vce that traverses the entire tract with its video signals is algorithmically analyzed. to solve the fundamental systems problems, we are targeting and developing a close to fully automated, accurate, low false positive, scalable, privacy-preserving and low-cost screening system that will, if we may say so, have significant potential impact on the society."
2. from the second syss different stress management strategies are recommended in relation to the presented symptoms and in relation to the abc-model. the users will be recommended some strategies but will have access to all.
"in contrast to earlier programmes, intervention transparency in the msc is clarified by coding the behaviour change intervention [cit] . additionally, the presentation of the structure of msc (table 1 ) also enhances the transparency of this programme."
"the multi-class dataset from vestre viken hospital does not contain the ground truth for the localization of the findings. therefore, in this experiment, we used the available asumayo clinic polyp database. 12 it consists of training and test sets of images and videos with corresponding ground truth showing the exact polyp location areas. this was the biggest publicly available dataset (until recently, when the owners decided to withdrawn it from the public), consisting of 20 videos from standard colonoscopies with a total of 18,781 frames and different resolutions up to full hd [cit] . for this particular evaluation, we selected only frames containing polyps, which gave us 8,169 frames in total: 3,856 in the training subset and 4,313 in the test subset. the frames with polyps contain various polyp types, fully visible and particularly hidden, clearly visible and blurry, clean and covered by stool. figure 8 depicts variations in polyp sizes (in terms of number of pixels showing polyp bodies within images) across the datasets. as one can see, there are huge variations in polyp sizes in terms of video-frame pixels from very small up to one third of the full video frame size. this reflects real colonoscopy video-capturing scenarios and introduces a big challenge for object localization algorithms. for the localization-performance measurement, we used the common metrics: recall (sensitivity), precision, specificity, accuracy and f1 score. to count the corresponding localization events correctly, we took into account that polyps can have different shapes, they are often not located in compact pixel space areas (in contrast to, e.g., people faces). the shape of the polyps is marked in the ground truth data by binary masks. before computing the localization subsystem performance, we need to figure out how to convert output of different localization algorithms into performance metrics. our initial assumption (from practical experience) was to count each of the neural networks' location rectangles as a true positive localization event if and only if it covers at least 10% of the corresponding ground truth area. otherwise, we count it as a false positive. in our use case, multiple detection of the same polyp does not improve medical outcome. therefore, we count multiple true positives on the same polyp ground truth area as one true positive. polyp misses are counted if, after processing all resulting rectangles for a particular frame, we still have one or more ground truth areas without corresponding true positives. we count such misses as false negatives. thus, there is a possibility of multiple false negatives per one frame, in case we have multiple lesions in the same frame. in this experiment, we process only frames that contain one or more polyps. this means that we do not have true negatives. therefore, specificity of the algorithms can be assumed as 100%. to check our assumptions about minimal coverage areas, we performed an initial performance evaluation and built a graph showing unfiltered output from neural networks. in our eir system, the base localization algorithm outputs points instead of rectangular areas. thus, we count a true positive if a point is located inside of a polyp ground truth area, keeping other rules the same. an example of a polyp localization algorithm output is depicted in fig. 9 . the polyp-location ground truth marked by light green ellipses is computed based on the ground truth binary masks (see fig. 5 ) using the closest elliptical region approximation. due to the limitations of the current version of the localization algorithm, it produces four possible polyp locations per frame without any location ranking. in this evaluation, we consider all four points as equal and always use all of them for calculating the performance metrics. these points are marked by the green and red crosses. the green crosses correspond to the true positive events, and the red crosses show the false positive events."
stress management strategies the programme includes the following stress management strategies: relaxation [cit] time-management (häfner & [cit] ) cognitive restructuring [cit] pleasant activity scheduling [cit] assertiveness training [cit] all the stress management strategies include: short rationale of the module in text readiness for change questions film: a short film/animation with information on how to perform the behaviour and an extended rationale for each stress management technique goal setting and a plan for the assignments specific assignments associated with each strategy: some assignments are time-locked for a predetermined number of days before next assignment opens up.
"phase ii (steps 4-7) was the development process for a web application through which the programme for stress management strategies and behaviour change techniques could be delivered to users in a tailored manner. this phase also included the design of ways to present the content. a programmer, a web designer, an illustrator, and the interdisciplinary research team (experts in behaviour change, stress management, and information design) were all involved in the design process."
"inception v3 achieves good results regarding single-frame classification and has reasonable computational resource consumption. the top one result error is 21.2%, and the top five error is 5.6% with less than 25 million parameters. the training of the inception v3 network is performed from scratch using google tensorflow v1.2rc [cit] . the training takes several weeks on a single modern computer with gpu support. tensorflow is an open source framework that allows all kinds of numerical computations using graphs. nodes within the flow graphs represent mathematical operations, and the edges represent data arrays (called tensors in tensorflow). it is especially built to support scalable machine learning, which includes neural network-based architectures [cit] ."
"second, we evaluated the performance of the state-of-the-art object localization approaches based on deep learning algorithms, and then we compared it with our basic polyp localization algorithm. in this use-case, we compared the ability of different methods to find the location of polyps inside a frame. the main goal of this evaluation is to decide if we can improve the polyp localization performance of our system using a combination of different algorithms."
the taxonomy for behavior change techniques was used to code the content of behaviour change techniques to clarify how and where they were incorporated and to enhance the transparency of the programme for presentation purposes (step 8) [cit] .
"the rest of the paper is organized as follows: first, in section 2, we briefly introduce our medical case study. next, we present related work in the field and compare it to the presented system in section 3. this is followed by a presentation of the complete system in section 4. we present an evaluation of the system in section 5, and in section 6, we discuss two cases where our system will be used in two medical examinations by medical experts. finally, we conclude our results in section 7."
"most modern deep learning frameworks state that they can be used out-of-the-box for different types of input data. this statement sounds promising, but most state-of-the-art neural networks in multimedia research are designed to process images from everyday life, like cats, dogs, bicycles, cars, pedestrians, etc. it needs to be proven that they can be used in medical domains, because it is difficult to evaluate their performance and robustness properly [cit] due to the lack of relevant training and test data. in fact, obtaining such datasets is one of the biggest challenges related to deep learning approaches in connection with the medical field, due to a lack of medical experts needed to annotate data, and legal and ethical issues. some common conditions, like colon polyps, may already have the number of collected images and videos required to perform training of a neural network, while other endoscopic findings, like tattoos from previous endoscopic procedures (black-colored parts of the mucosa), are not that well documented, but still interesting to detect [cit] . recent research [cit] on the topic of transfer learning promises a solution for the problem of insufficient amounts of available training data. transferring the knowledge learned by the deep network on a large dataset, e.g. imagenet, to train a specialized network on a small medically oriented dataset, together with a saliency prediction used to emphasize key image points, can result in better performance of the endoscopic finding detection and localization. thus, in this research, we perform some preliminary experiments to see how neural networks can deal with small training datasets."
"for our experimental evaluation, we use two different use-cases. first, we evaluated the performance of our multi-class classification and detection algorithms in automated colonoscopy video processing. here, we tested our system using six different classes of endoscopic findings that can be found in the colon (shown in fig. 1 ). the classes to be detected are (a) frames with normal colon mucosa (healthy colon wall), (b) frames of the cecum area which is an intraperitoneal pouch that is considered to be the beginning of the colon (an anatomic landmark helping doctors and vce video analysis algorithms to orientate in the colon), (c) frames displaying the z-line which is the gastroesophageal junction that joins the esophagus to the stomach (an anatomic landmark), (d) frames containing one or more polyps, (e) frames with visible tumor areas, and (f) useless blurry frames without any visible and recognizable objects. thus, the developed multi-class classification and detection system should split all the video frames into six classes that can be observed in the human gi tract. the developed method allows us to implement a new generation of endoscopy video processing systems able to efficiently detect various lesions of the gi tract."
"the comparison of these two approaches shows that both approaches have equal excellent overall f1 score of 100% in z-line detection. the global-feature approach with the 100% f1 score outperforms the neural network approach by a small margin in blurry frame detection. the neural network f1 score detection for tumors is 98.2%, which is 1% better than the global-feature approach. detection of other classes is better for the global-feature approach, giving the f1 scores of 79.2% and 74.3% for cecum and normal mucosa. most importantly for our case study, polyp detection performed much better using the global-feature approach, giving the 92.8% f1 score (13.1% better than the neural network approach). the performance evaluation of the cross-validation for both multi-class classification approaches (see table 5 ) confirms the high stability of the models used for the classification."
"the aim to tailor the intervention led to the development of the syss that steers users towards stress management strategies relevant to the symptoms of stress for each user. there is a lack of evidence regarding which types of skills or stress management strategies are most efficacious for different stress-related symptoms. however, it is argued that more physical symptoms should be tackled with muscularly oriented methods, and cognitive and emotional symptoms should be addressed with strategies including components closer to cognitive and behavioural approaches [cit] . because of the limited evidence for tailoring stress management strategies based on stress symptoms, certain techniques will be recommended to the users, but the users will also have the opportunity to choose from all strategies in the programme. a limitation in the tailoring was that the syss's psychometric properties have not been studied. nevertheless, the syss is built on reliable and valid instruments [cit] . the face validity of the syss was investigated through connecting the stress management strategies to the symptoms by the research group. the conformity was high in the decisions about which symptoms were connected to which stress management strategies. users' experiences of the tailoring will be studied when evaluating msc. the use of the ttm and, more specifically, of the soc for further tailoring was influenced by a review [cit] in which self-management programmes were described as ideally including a motivation and intention formation stage, a stage of active goal pursuit, and a stage for maintenance. the condensing of the soc into these stages in msc also made the tailoring more manageable in terms of the potential number of tracks for each user."
"however, four participants did not receive the final module; these four persons did not understand that there were more modules available which could be reached by completing certain assignments. the two persons who quit the programme due to sick leave were considered to have fulfilled the exclusion criteria. the actual dropout rate was than 42% (5 persons of the remaining 12). the results clearly indicate that details in the programme and platform need to be further developed in order to enhance the usability of the programme."
"the decisions made regarding the theoretical framework, the behaviour change techniques and the choices of how to deliver the programme were central to the development process and in agreement with previous research [cit] . for example, studies reporting the use of the tra/tpb show larger effects on behaviour change than those using other theories [cit] ."
"in order to achieve an overview of the algorithms, a conceptual model of the programme structure was developed (step 4). this model also made it easier to decide upon the logical order of the content and how to integrate the tailoring. the conceptual model was also the basis for a paper version of the web application, which showed all the functions of the future web application, such as a skeleton of menus, buttons, and where the assignments, tailoring and films would be found (step 5). the paper version (example in figure 2 ) was tested within the research team. it was also the instruction for ordering the application from a programmer. during the programming process, the programme structure was adjusted, and visual elements were discussed within the team, primarily with the expert in information design (the third author), figure 1 . flowchart of the development process."
"the localization subsystem is intended for finding the exact positioning of a lesion, which is used to show markers on the frame containing the disease. this information is then used by the visualization subsystem. all images that we process during the localization step come from the positive frames list generated by the detection subsystem. processing of the images is implemented as a sequence of intra-frame pre-and main-filters. pre-filtering is needed because we use local image features to find the exact position of objects in the frames. lesion objects or areas can have different shapes, textures, colors and orientations. they can be located anywhere in the frame and also partially be hidden and covered by biological substances, like seeds or stool, and lighted by direct light. moreover, the image itself can be interlaced, noisy, blurry and over-or under-exposed, and it can contain borders and sub-images. apart from that, images can have various resolutions depending on the type of endoscopy equipment used. endoscopic images usually have a lot of flares and flashes caused by a light source located close to the camera. all these nuances affect the local feature-based detection methods negatively and have to be specially treated to reduce localization precision impact. in our case, several sequentially applied filters are used to prepare raw input images for the following analysis. these filters are border and sub-image removal, flare masking and low-pass filtering. after pre-filtering, the images are ready to be used for further analysis."
"the gi tract can potentially be affected by various abnormalities and diseases. some examples of possible findings are shown in fig. 1b -e. crc is a major health issue world-wide, and early detection of crc or polyps as predecessors of crc is crucial for survival. several studies demonstrate that a population-wide screening program improves the prognosis and can even reduce the incidences of crc [cit] . as a consequence, in the current european union guidelines, screening for colorectal cancer is recommended for all people over 50 years old [cit] . colonoscopy, a common medical examination and the gold standard for visualizing the mucosa and the lumen of the entire colon, may be used either as a primary screening tool or in a second step after other positive screening tests [cit] . however, traditional rectal endoscopic procedures are invasive and may lead to great discomfort for patients, and extensive training of physicians and nurses is required to perform the examination. they are performed in real-time, and, therefore, it is challenging to scale the number of examinations to a large population. additionally, the classical endoscopic procedures are expensive. in the us, for example, colonoscopy is the most expensive cancer screening process, with an annual cost of 10 billion dollars (1,100$-6,000$/person) [cit], and a time consumption of about one medical doctor-hour and two nurse-hours per examination."
"our objective is to develop a system that supports doctors in multi-disease detection in the gi tract. the system must (i) be easy to use and less invasive for the patients than existing methods, (ii) support multiple classes of detected gi objects, (iii) be easy to extend to new different diseases and findings, (iv) handle multimedia content in-real time (30 frames per second or more for full hd videos), (v) be usable for real-time computer-aided diagnosis, (vi) achieve high classification performance with minimal false-negative classification results and (vii) have a low computational resource consumption. these properties potentially provide a scalable system with regard to reduced number of specialists required for a larger population, and dramatically increased number of users potentially willing to be screened. therefore, eir consists of three parts: the annotation subsystem [cit], the detection and automatic analysis subsystem and the visualization and computer-aided diagnosis subsystem [cit] ."
"for future work, we plan to further improve the multi-class detection and localization accuracy of the system and support detection and localization of more abnormalities. in this respect, we are currently working with medical experts to collect more training data, annotate them and create new, larger training and testing datasets [cit] . finally, to further improve the performance of the system, we work on a universal system extension that will allow the system to utilize the computing power of one or more gpus on single or multiple nodes. implementing such an extension will allow parallelization of the detection and localization workloads [cit], which is important in our multi-disease analysis system of gi tract [32, 35, [cit] ."
"yvonne eriksson is a professor in information design. her research areas are visual communication, with a focus on perceptual and cognitive processes involved in interpretation of visuals, and gender issues related to visual studies. this study is part of a wider project to develop and evaluate a web-based self-management programme for the handling of work-related stress."
all communication to and from the application is encrypted using secure sockets layer (ssl). ip addresses failing to log in to the system are banned after a certain number of failures.
"our experimental comparison of the deep-eir and the global-feature-based eir of the detection system shows clearly that the global-feature approach outperforms the deep learning neural network approach and gives better accuracy for almost all target detection classes (except several cases of misclassification of tumors) in conjunction with high 92.8% and 97.2% f1 scores for the most important findings: polyps and tumors. moreover, when a sufficiently large training dataset covering all possible detectable lesions of the gi tract is used, the proposed global-feature approach for multi-class detection requires relatively little time for training [cit] compared to days and weeks for the deep learning neural network approach."
"three essential factors have been identified to take into account when developing web-based self-management programmes supporting behaviour change: the theoretical framework of the programme, how evidence-based behaviour change techniques are incorporated into the programme and how the programme is delivered [cit] . studies which report the use of theory when developing web-based programmes to support behaviour change show larger effects than studies which are not theory-based [cit] . regarding programme delivery, fully automated self-management programmes have been shown to have positive effects on perceived stress [cit] . compared to the more common therapist-guided programmes, more is demanded with respect to technical solutions and the pedagogical design in a fully automated programme (hedman, carlbring, ljótsson, & [cit] ) . interventions which are automated and thus available without contact with a therapist can be completed without clinical assessment and therefore reach a large population at a low cost [cit] . the related costs are usually lower for automated programmes than for face-to-face interventions, but the drop-out rate is often higher in programmes involving no contact with therapists [cit] ."
"the occupational health experts had no objections to or suggestions for revisions to the programme content, progress or structure. everything was regarded as necessary and relevant. [cit] structure of msc"
"the existing localization scheme can be extended to support different diseases by implementation of lesion-specific shape, color and texture detection, but such an extension requires experimental studies for each new type of abnormality. in order to reduce the system improvement costs, we performed an evaluation of two universal object localization frameworks, based on deep learning neural network approaches. first is tensorbox 8 [cit], which extends google's reference implementation of the machine-learning framework called tensorflow [cit] . second approach is based on the darknet [cit] open-source deep learning neural network implementation called yolo 9 [cit] . both of these frameworks are designed to provide not only object detection, but also object localization inside frames. they implement gpu-accelerated deep learning algorithms that can work with near to real-time performance and provide the capability of locating various objects out-of-the-box."
"a comparison of deep-eir and global-feature-based eir with existing competitive approaches is shown in table 6 . the basic-, deep-and multi-class global feature-based eir detector versions are depicted in the last table's rows. as one can see, the global featurebased eir approach gives the best performance in terms of precision (90.6%), specificity (98.1%) and accuracy (96.9%), and comparable recall/sensitivity (90.6%). in other words, the results indicate that we can detect different classes of gi tract findings with a precision of almost 91%. if we compare this to the best performing system in table 6, we see that polyp-alert reaches slightly higher detection accuracy on a different dataset. however, our system is faster and can detect colonoscopic findings in real-time, and furthermore, it is not designed and restricted to detect only polyps, it can detect multiple classes of diseases, and eir can further be expanded to any additional diseases if we have the correct training data."
"in recent years, stress as a work life health issue has been highlighted in the majority of industrialised countries. studies show that every fourth worker in europe is exposed to workrelated stress, defined as involving work situations with either high demands and low control, high work load and low reward or organisational injustice [cit] . stress can be defined as involving internal or external demands that are appraised to exceed an individual's real or perceived resources [cit] . stress-evoking demands such as perceived dissatisfaction with work, as well as high psychological and physical strain, are both associated with sick-listing [cit] . sleeping disorders, anxiety and depression are also frequently associated with work-related stress [cit], and work-related stress increases the relative risk for cardiovascular disease [cit] ."
"the performance of the tensorbox approach (see table 7 ) is too low for our real-time use-case. but, as depicted in table 7, darknet-yolo performs well in terms of processing speed and can run at 45 frames per second. our basic approach runs at 120 frames per second, thus a combination of both approaches can give us better localization performance while staying within the required real-time frame rate limits."
"in the multi-class classification experiments, we used cross-validation because of the relatively small number of images in the annotated dataset. for the performance measurement, we used the standard tool from weka for evaluating multi-class classifiers. this tool uses the ground truth to compute a confusion matrix and the common standard metrics: recall (sensitivity), precision, specificity, accuracy and f1 score. we created a new dataset from colonoscopy images that we got from vestre viken hospital, norway. from the whole unannotated dataset, we manually selected 50 different frames of 6 different classes (described in section 2): blurry frames, cecum, normal colon mucosa, polyps, tumor, and z-line. the selected frames were used to create 10 separate datasets, each containing training and test subsets with equal numbers of images. training and test subsets were created by equally splitting random-ordered frame sets for each of the 6 classes. the total number of frames used in this evaluation is 300: 150 in the training subsets and 150 in the test subsets. each training and test subset contains 25 images per class. multi-class classification is then performed on all 10 splits and then combined and averaged. following this strategy, an accurate enough estimation about the performance can be made even with a smaller number of images."
"fifteen persons agreed to test the usability of the programme. one person was excluded due to high scores on hads. regarding the 14 remaining participants, 7 ended their participation before finishing the programme. the reasons for ending participation early included a lack of time and the extensiveness of the programme (four persons), sick leave (two persons) and an unknown reason (one person). the participants went through the programme over approximately 7-9 weeks. of the remaining seven participants, three finished the programme and reached the final module."
"the graphics were designed based on information design principles [cit] . several discussions within the research group and with the programmer and a web designer, led to the selection of the final graphics, colours and font."
"in order to validate the content, progress and structure of the programme, it was presented to four experts in occupational health and rehabilitation (step 9). the experts were asked whether anything was missing or left out and whether the structure of the programme was relevant. each expert answered independently, and their comments were categorised into two categories: relevant or not relevant. the occupational health experts had different expertise: one licensed psychologist, one manager of one of the county council's occupational health unit, one ceo of a private company delivering occupational health services and also responsible for the work environment and health section of the company, and one health strategist at a private company."
"the development of msc resulted in an information-rich, tailored and interactive web-based stress management programme based on evidence from multiple fields. the interactivity in msc includes questionnaires, interactive activities, action planning, self-monitoring, tailored feedback and tasks to complete offline between sessions. the web application reacts to the users' answers and choices in assignments and tailors the programme for each user. the users receive feedback with examples on how to solve some of the assignments related to specific stress management strategies after completing them. msc is not similar to any existing programme in the stress management field, considering the tailoring and multi-tracked opportunities. it is built on a solid theoretical framework. the most central behaviour change techniques, integrated in every module, self-monitoring, goal-setting, re-evaluation of goals, feedback, and to prompt formulation of intention to change have also shown highest evidence in previous research [cit] ). the information is provided in texts, films and audio recordings and is developed with a focus on text design and to not reassert stereotypes."
"first, we performed an evaluation of deep-eir that implements the deep learning neural network multi-class detection approach. table 1 shows the resulting confusion matrix. the detailed performance metrics presented in table 2 and the results can be considered as good, they confirm that deep-eir performs well. all blurry and z-line frames were classified correctly. cecum and normal colon mucosa were often cross-mis-classified, which is a normal behavior, because from a medical point of view, normal colon mucosa is part of the cecum, and under real-world circumstances, this would not be a relevant mistake. interesting polyps bold numbers shows the balanced f-score of each proposed method and tumors were detected correctly in most cases, as well as the z-line landmark, which is important for our medical use case."
"in the context of object detection, localization and tracking in images and videos, a lot of research carried out. particularly, current systems have been developed to detect general objects from the surrounding world, for example human faces, cars and logos. our research targets a totally different domain, which is inside the body of a human being. both the general objects and the gi tract irregularities can have different sizes, shapes, textures, colors and orientations, they can be located anywhere in the frame and also partially be hidden and covered by other objects and obstacle. however, gi tract findings can also have a color, texture and shape properties similar for the different diseases, as well as different for the similar diseases on the various developing stages. the gi findings can be covered by the biological substances, like for example seeds or stool, and lighted by direct and reflected light. moreover, the images coming from the endoscopic equipment itself can be interleaved, noisy, blurry and over-or under-exposed, and it can contain borders, sub-images and a lot of specular reflections (flares) caused by endoscope's light source. therefore, detecting abnormalities and diseases in the gi tract is very different from detecting the objects from the surrounding world listed above. the gi tract can potentially be affected by a wide range of diseases with visible lesions (see fig. 1d -e), but endoscopic findings may also include benign (normal) or man-made lesions. the most common diseases are gastric and colorectal cancer (crc), which are both lethal when detected in a late stage. survival rate of crc ranges from 93% in stage i to 8% in stage iv [cit] . consequently, early detection is crucial. there are several ways of detecting pathology in the gi tract, but systematic population-wide screening is important. however, current methods have limitations regarding sensitivity, specificity, access to qualified medical staff and overall cost. in this scenario, both high precision and recall are important, but so is the frequently ignored system performance in order to provide feedback in real-time. the most recent and most complete related work is the polyp-alert polyp detection system [cit], which can provide near real-time feedback during colonoscopies. however, it is limited to polyp detection, it uses edges, colors and texture in the images, and, at the moment, it is not fast enough for live examinations."
the new multi-class global-feature-based version of eir is based on the initial version of eir with some extensions. the basic search-based classification part of eir is used to create a classifier for each disease that we want to classify. figure 2 gives a detailed overview of the classifier's pipeline for the global-feature-based implementation of the detection. the difference to the basic eir version is that the ranked lists of each search-based classifier are then used in an additional classification step to determine the final class.
goal setting (outcome) for sleep (1.3). action planning (1.4). self-monitoring (2.3). self-monitor outcome of the behaviour (2.4). instruction on how to perform a behaviour (4.1). information about health consequences (5.1). information about social and environmental consequences (5.3). demonstration of behaviour (physical exercise) (6.1). graded tasks (8.7). self-incentive (10.7). self-reward (encouraged) (10.9).
"applying unsupervised clustering on huge unsorted and unannotated datasets significantly reduces the amount of work required from skilled doctors during image labeling and grouping. together with unsupervised clustering, our application provides the users with the ability of tagging and analyzing multiple single images at once and putting them into appropriate groups together. the clustertag application is released as open-source software 11 and might help other research groups in the creation and analysis of new datasets."
"for the final classification, we use the random forest classifier [cit], an ensemble learning method for classification that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes of the individual trees. a decision tree can be seen as a classifier, which basically performs decision-based classification on the given data. to get the final class, the classifier combines decision trees into a final decision implementing a late fusion for the multi-class classification. the advantage of the random forest algorithm is that the training of the classifier is very fast because the classification steps can be parallelized since each tree is processed separately. additionally, it is shown that the random forest is very efficient for large datasets due to the ability to find distinctive classes in the dataset and also to detect the correlation between these classes. the disadvantage is that the training time increases linearly with the number of trees, which means a longer training time when many trees are used at the same time. however, this is not a problem for our use-case since the training is done offline, where time is less critical. our implementation of the random forest classifier uses the version provided by the weka machine learning library 5 [cit], which is a collection of algorithms for machine learning and data mining. we chose the random forest approach, because it is fast and achieves good results [cit] . it is important to point out that for this step, another classification algorithm can also be used."
"msc focuses on the individual and his or her coping strategies and does not focus on the organisational aspects, which has also been in association with risk factors for developing stress [cit] . however, the organisational perspective is discussed in the psychoeducation module, and the users are informed of how stress-related problems can evolve from organisational injustices and poor work-related environments."
"the neural network version of eir called deep-eir is based on a pre-trained convolutional neural network architecture and transfer learning [cit] . we trained a model based on the inception v3 architecture [cit] using the imagenet dataset [cit] and then re-trained and finetuned the last layers. we did not perform complex data augmentation at this point and only relied on transfer learning. we are currently in the process of data collection, and for future work, we will also look into data augmentation and training a network from scratch using the newly collected data, which might lead to better results than transfer learning. figure 3 gives a detailed overview of the complete pipeline for the neural network-based implementation of the detection."
"the tensorbox approach introduces an end-to-end algorithm for detecting objects in images. as input, it accepts images and directly generates a set of object bounding boxes as output. the main advantage of the algorithm is the capability of avoiding multiple detections of the same object by using a recurrent neural network (rnn) with long short-term memory (lstm) units together with fine-tuned image features from the implementation of a convolutional neural network (cnn) for visual objects classification and detection called googlenet [cit] ."
"one focus during the development of msc was to find strategies to prevent dropping out from the programme and to help participants adhere to new behaviours using techniques identified as supporting behaviour change [cit] ). the dropout rate was high, although somewhat lower than reported in earlier studies [cit] where an average of 50% of the participants did not complete the interventions."
"the conceptual model clarified the possible algorithms in the programme and the basis for the paper version (see figure 2 for an example). the model, in combination with discussions with the web-designer, programmer and expert in informational design, resulted in the structure of msc (exemplified by one specific stress management module in figure 3 ). the structure of msc, the content and the behaviour change techniques included in each module are shown in table 1 ."
"to further aid and scale such examinations, we have earlier presented eir 1 [cit], an efficient and scalable automatic analysis and feedback system for medical videos and images. the system is designed to support endoscopists in the detection and interpretation of diseases in the gi tract. eir has initially been tested in video analysis of the lower portions (large bowel) of the gi tract. however, our main objective is to automatically detect abnormalities in the whole gi tract. therefore, we are developing a complete system for detection and in-frame position localization of different endoscopic findings like polyps, tumors, diseases and landmark objects (like the z-line and cecum). the aim is to use next-generation-eir for both (i) a computer assisted diagnosis tool for live endoscopic examinations and (ii) a future fully automated and scalable screening system used together with vces. these goals impose strict requirements on the accuracy of the detection to maximize number of true positives and to avoid false negatives (overlooking a disease), as well as low computational resource consumption to provide massive population screening with vces. the live-assisted system also introduces a real-time processing requirement defined as being able to process at least 30 hd frames per second, i.e., a common frame rate and resolution in modern endoscopic devices."
"regarding the development of the tailoring tool, syss, there was 100% consensus for three of the 32 items within the four-person research team regarding how the members connected the symptoms of stress to the stress management strategies. for the rest of the items of syss, three of the members of the research team agreed regarding the connection of symptoms to a particular stress management strategy. for the details of tailoring in the programme, see table 1 ."
"in four of 14 cases, the possible end users working with the programme had trouble finding their way through it, although the focus for the design of the programme elements was simplicity [cit] . in this case simplicity means that using and understanding the visual elements on the platform should be intuitive and thus that the platform should be easy to understand without instructions in how to use it [cit] . at some point, this was not enough. thus, the programme must be further developed to enable the users to more easily understand the structure of the programme. having an understanding of the programme structure could facilitate the users in adhering to the programme and should therefore be further developed."
"even though work-related stress in many cases is a result of organisational or structural problems, individual stress management has been shown to lower the stress levels of employees [cit] . studies show that stress management intervention programmes are effective in reducing stress at a lower cost than individual face-to-face interventions, which can be expensive and resource demanding [cit] . due to the lack of resources within the health-care sector, especially when it comes to health promotion, digitalisation has become a huge developmental area, and websites and mobile and web applications which target behaviour change with respect to different health behaviours are common."
"in our previous work, we presented our basic eir system [cit] that implements a single-class global-feature-based detector able to recognize the abnormalities in a given video frame. global image features were chosen, because they are easy and fast to calculate, and the exact lesion's position is not needed for detection, i.e., identifying frames that contain a disease. we showed that the global features we chose, tamura feature [cit] and joint composite descriptor (jcd) [cit], which is a combination of fuzzy color and texture histogram (fcth) [cit] and color and edge directivity descriptor (cedd) [cit], can indeed outperform or at least reach the same results as local features. the basic algorithm is based on an improved version of a search-based method for image classification. the overall structure and the data flow in the basic eir system is depicted in fig. 2 . first, we create the index containing the visual features extracted from the training images and videos, which can be seen as a model of the diseases and normal tissue. the index also contains information about the presence and type of the disease in the particular frame. the resulting size of the index is determined by the feature vector sizes and the number of required training samples, which is rather low compared to other methods. thus, the size of the index is relatively small compared to the size of the training data, and it can be easily fit into main memory on a modern computer. next, during the classification stage, a classifier performs a search of the index for the frames that are visually most similar to a given input frame (see section 4.1.3 for a detailed description of the method). the whole basic detector is implemented as two separate tools, an indexer and a classifier. we have released the indexer and the classifier as an open-source project called opensea 3 [cit] ."
"there is a need for advanced web-based self-management programmes founded on current evidence to prevent stress-related health problems. however, way to take such development from an idea to a ready-to-use product, where evidence within the field regarding content, tailoring, interactivity and individual feedback are taken into account, have not been described in the literature. thus, the aim of this study was to describe the systematic development of an evidence-based, tailored, interactive web application for the self-management of work-related stress. the aim was also to test the application's usability with respect to how to proceed through the programme."
"phase i (steps 1-3) was founded on a state-of-the-art literature review [cit] of current evidence in the fields of stress management, behaviour change, and web-based behaviour change in order to decide upon the theoretical framework and content for the programme. in steps 1 and 2, scientific papers and books which were prominent in the fields were identified [cit] . based on these works, further searches were conducted."
"parallel to the development of the web application, all text material, for audio recording, films and stress management assignments was developed. the texts were based on the evidence from phase i and were revised by an expert in informational design and gender studies (the third author) (steps 7-8). one focus of the text design was to avoid stereotyping or reasserting heteronormative stereotypes (eriksson & göthlund, 2012; [cit] ) in order to enhance the acceptability of the programme, but the main focus was on comprehensive text design and the creation of texts easy to read. sixteen audio recordings were prepared, and 14 short films were developed and combined with sound recordings. an expert in informational design produced the four most central films: 10 of the 14 films were developed by the first and third authors. the images included in the films were also chosen with the aim of avoiding heteronormative stereotyping."
"the indexer is implemented as a batch-processing tool. creating the models for the classifier does not influence the real-time capability of the system and can be done off-line, because it is only done once when the training data is first inserted into the system. visual features to calculate and store in the indexes are chosen based on the type of the disease because different sets of features or combinations of features are suitable for different types of diseases. for example, bleeding is easier to detect using color features, whereas polyps require shape and texture information."
earlier research has not clearly described how behaviour change techniques are incorporated into stress management programmes. coding the included behaviour change techniques according to the behavior change technique taxonomy could clarify the content of a programme and thus contribute to the transparency of the intervention [cit] . the taxonomy was developed to standardise definitions of behaviour change techniques used in interventions to more easily identify techniques contributing to effective interventions [cit] .
"msc is a unique programme in the stress management field, but it must be further developed. however, the development process contributes to filling a gap in the literature regarding how to develop evidence-founded, complex web-based interventions for stress management on the web. this study could guide researchers in the area of web-based intervention development regarding how to accomplish such developmental processes. although developed from the evidence in multiple fields, the web-application would benefit of further development to support users to reach the end module."
"as initial models for both approaches, we used database models pre-trained on imagenet [cit] . our custom training and testing data for the algorithms consists of frames and corresponding text files describing ground truth data with defined rectangular areas around objects: a json file for tensorbox and one text file per frame for darknet-yolo. ground truth data was generated using a binary-masked frame set (example shown in fig. 5 ) by the localization validation software used in our experimental studies. both frameworks were trained using the same training dataset, where all frames contained one or more visible polyps. no special filtering or data preprocessing was used, thus the training dataset contained high quality and clearly visible polyp areas as well as blurry, noisy, over-exposed frames and partially visible polyps. the models were trained from scratch using corresponding default-model training settings [cit] . after the training, the test dataset was processed by both neural networks in testing mode. as a result, the frameworks output json (tensorbox) and plain-text (darknet-yolo) files containing sets of rectangles, one set per frame, marking possible polyp locations with corresponding location confidence values. these results have been processed using our localization algorithms."
"rapid development of technologies in areas of sensors, imaging devices and diagnostic methods shifts the paradigm in medical diagnostic from manual analysis by trained doctors to wide usage of automated computer-assisted diagnostic systems. in our research, we are working at the intersection between computer science and pathological medicine, where we target a scalable, real-time, multi-disease detection system for the gastrointestinal (gi) tract. our aim is to develop both a computer-aided, live analysis system of endoscopy videos and a scalable detection system for population-wide screening using a wireless video capsule endoscope (vce). this small capsule with one or more image sensors is swallowed and captures videos while it traverses the entire gi tract."
