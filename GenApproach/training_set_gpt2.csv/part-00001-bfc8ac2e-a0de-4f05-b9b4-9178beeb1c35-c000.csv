text
"to establish the geodesic spatial relationships between the local primitives, a novel mathematical morphology based approach is proposed in this paper. the main contributions are: (1) establishment of connections between local primitives based on the geodesic paths (skeletal line connections between them), (2) adaptation of the mask image after each geodesic dilation iteration to protect the points of contact of the traversed geodesic paths between the local primitives, (3) extraction of adjacency relations (connections) between local primitives based on the image generated by infinite geodesic dilations by the proposed algorithm. the proposed approach is robust in establishing the spatial relationships between all the local primitives found in a line drawing image and is explained in section 3. the detection and classification of local primitives is explained in section 2. the results for the proposed approach are presented and discussed in section 4 and a conclusion is drawn in section 5."
"a novel mathematical morphology based approach employing the grayscale geodesic dilation is proposed to establish the spatial relationships between the local primitives found at the junction and end points in binary line drawing images. the proposed approach is robust and successfully established the geodesic spatial relationships between all the local primitives. as the detection and classification of local primitives found at the junction points and end points is the first step, the establishment of geodesic spatial relationships depends upon the detection of these points. by establishing the geodesic spatial relationships between local primitives, their pairwise co-occurrences in addition to the independent occurrences can be captured in the form of histograms which are useful in the recognition, retrieval and classification of drawing images."
"to evaluate the performance of the proposed approach ten images are selected from a publically available patent image database 1 . the visual results for two out of the ten selected images are shown in figures 3 and 4 . it can be seen that the proposed approach successfully establishes the geodesic spatial relationships between all the local primitives found at the detected junction and end points. having established the geodesic neighborhood relations of the local primitives by the developed method, the pairwise co-occurrences of the local primitives in addition to their independent occurrences can be captured. in an occurrence histogram each bin represents the occurrence frequency of a local primitive, whereas each bin of co-occurrence histogram represents the occurrence frequency of a pair of local primitives. the representation of line drawing images in terms of the occurrence and co-occurrence histograms of local primitives are useful in the recognition, analysis and retrieval of these images. it is noted that the establishment of geodesic spatial relationships is highly dependent on the detection of eps and jps which is a first step in the proposed approach. due to the false eps and jps detections, geodesic spatial relationships can be established between a true and a false local primitive, which can be overcome by eradicating the false detections. the proposed approach takes n/2 iterations to establish the spatial relationships between two primitives which are n pixels apart. so, the computational complexity of the approach depends upon the longest distance (in terms of pixels) of any two primitives in an image and scale of the image as well."
"binary images such as technical drawings, diagrams, flowcharts etc. are found in patents and scientific documents. they are composed of lines intersecting each other in different directions. the local pattern formed by the composition of intersecting or crossing lines at a junction point and end point of lines is called a local primitive. local primitives and their spatial arrangement are useful in the analysis, recognition and retrieval of binary images. to capture the content of binary images found in patents, different geometric shapes and their spatial relationships have been the target information to be explored in the literature."
"for the natural images, the local features lying at an arbitrary distance (euclidean) to a specific local feature inside its circular neighborhood of arbitrary radius are assumed to be spatially related to each other [cit] . in binary line drawing images, the technique of an arbitrary circular neighborhood around a local primitive establishes the spatial relationship between two local primitives which may not be geodesically related to each other. in a line drawing image, two local primitives with a geodesic path (skeletal line connection) between them that does not pass through any other primitives are said to have a geodesic spatial neighborhood relationship."
"to the best of our knowledge, this is the first system which is able to perform real-time object recognition using a stereovision camera. the range data collected by stereovision cameras are relatively noisy and have many dropouts due to depth discontinuity. working with this type of data is extremely challenging for object recognition techniques. this is especially true for local methods such as spin images and point signatures that rely upon an estimate of the normal around each point, which can be very inaccurate. in addition, the objects being recognized were held by hand during the tests, which partially occluded surfaces of the objects and introduced a large number of outliers to the data and made the problem even more difficult. despite these challenges, pwse was able to correctly recognize all of the objects in an efficient and robust manner, which further demonstrated the effectiveness and robustness of the pwse algorithm."
"for all the cases, we implemented the method described in section 3 to find out the best values for the parameters. we use precision and recall to evaluate the performance of the cases above. the results are shown in tableⅱ."
"in figure 6 their encrypted image with sharp edge with the same encryption initial parameters and initial conditions. as we can see, the encrypted image is rough-andtumble, unknowable and 100% obscure of the image figure 6(c) is the decrypted image by use the same encryption key. it can be seen that the decrypted image is clear and correct without any distortion see figure 6 (c) to see no error between original image and decrypted image."
"in the previous sections, the performance of pwse has been evaluated for both object recognition and object class recognition. from these results, it can be concluded that pwse compares favourably to the previous object recognition techniques in terms of efficiency and accuracy. in a previous study [cit], pwse has been compared against the well-known spin images technique. spin images was tested with a 3-d model database of 56 objects using a 2 ghz pc with 2 gb memory. the reported accuracy of the simulated data tests, which were similar to our ideal data tests, was slightly above 90%, and recognition time per query was 43 seconds on 50 objects. their experiments on real data were conduct with 88 real queries against a 90 model database, and their reported accuracy was ∼40%."
"in process of combining five-gram items, if the number of the entries in the last five-gram item is smaller than five, then we use stop words to fill. the n-grams can effectively capture the relationship between words, which have co-occurrence information of keywords."
"the proposed method is based on local optimization techniques, such as the iterative closest point (icp) algorithm, which is used to align 3-d models based on their surface geometry. it is worth noting, however, that the proposed algorithm is not limited by icp: the idea is general and has the potential to work with other optimization techniques."
"the mtl has been also used to investigate how to deal with the sensitivities of collecting and using deeply personal data in real-world situations. in particular, a mtl study investigated the perceived monetary value of mobile information and its association with behavioral characteristics and demographics; the results corroborate the arguments towards giving back to the people (users, citizens, according to the scenario) control on the data they constantly produce [cit] ."
"the experiment contains two steps. in first step, we use the 4480 articles to train. the size of n in n-gram, the values of we come up with sixteen sets of experiments to finalize the parameters in the algorithms above. all the cases are shown in tableⅰ."
"on this matter, new user-centric models for personal data management have been proposed, in order to empower individuals with more control of their own data's life-cycle [cit] . to this end, researchers and companies are developing repositories which implement medium-grained access control to different kinds of personally identifiable information (pii), such as passwords, social security numbers and health data [cit], location [cit] and personal data collected by means of smartphones or connected devices [cit] . a pillar of these approaches is represented by a personal data eco-system, composed by secure vaults of personal data whose owners are granted full control of."
"at the minima of the error surface. each set e i of k minima is called an embedding [cit] of the error surface s i, and is used to compactly and descriptively represent a unique view p i ."
"an experiment using ideal (i.e non-noisy) range data was conducted to prove the effectiveness of the proposed algorithm. the resulting confusion matrix is illustrated in fig. 10 . the experimental result shows that the algorithm was able to correctly recognize over 98% of the objects, and about 97% of the poses were correctly recovered. in addition, pwse is extremely efficient, running at about 122 fps for a database of 60 objects. the worst case is the sport car (no. 41), whose pose determination rate and object recognition rate were 74.5% and 82.5% respectively. after examining the unsuccessful trials on this object, we found that it was mis-recognized as the jeep (no. 56, 25 cases, and no. 57, 4 cases), and the tank (no. 58, 2 cases)."
"how to evaluate the hotness of topic is one of the important problems in hot topic detection. jianjiang li, xuechun zhang, [cit] evaluated the blog hotness based on text opinion analysis. they took the number of reviews, comments and publication time of the sentence the fourth anniversary of the wen chuan earthquake"
"moreover, the automated analysis of anonymized and aggregated largescale human behavioral data offers new possibilities to understand global patterns of human behavior and to help decision makers tackle problems of societal importance [cit], such as monitoring socio-economic deprivation [cit] and crime [cit], mapping the propagation of diseases [cit], or understanding the impact of natural disasters [cit] . thus, researchers, companies, governments, financial institutions, non-governmental organizations and also citizen groups are actively experimenting, innovating and adapting algorithmic decision-making tools, often relying on the analysis of personal information."
"the potential positive impact of big data and machine learning-based approaches to decision-making is huge. however, several researchers and ex-perts [cit] have underlined what we refer to as the dark side of data-driven decision-making, including violations of privacy, information asymmetry, lack of transparency, discrimination and social exclusion. in this section we turn our attention to these elements before outlining three key requirements that would be necessary in order to realize the positive impact, while minimizing the potential negative consequences of data-driven decision-making in the context of social good."
"with the development of web 2.0, blogs have gained massive popularity and have become one of the most influential web social media in our times. anyone with an internet connection can conveniently publish topics. according to a research [cit], there are over 175,000 new blogs are created per day by people all over the world, on a great variety of subjects. the huge growth of blogs provides a wealth of information waiting to be extracted. blogs are becoming an extremely relevant resource for different kinds of studies focused on many useful applications. accordingly, blogs offer a rich opportunity for detecting hot topics that may not be covered in traditional newswire text. unlike news reports, blog article expresses a wide range of topics, opinions, vocabulary and writing style. the change in editorial requirements allows blog authors to comment freely on local, national and international issues, while still expressing their personal sentiment. these forms of self published media might also allow topic detection systems to identify developing topics before official news reports can be written."
"after calculating the keyword list for each blog article, we get the hot keywords by merging keyword lists. the algorithm to merge keyword list is described as follows."
"where ( ) weight j presents the weight of keyword in the jth node in the merged keyword list, and l is the length of the merged keyword list."
"the use of real-time human behavioral data to design and implement policies has been traditionally outside the scope of the way of working in policy making. however, the potential of this type of data will only be realized when policy makers are able to analyze the data, to study human behavior and to test policies in the real world. a possible way is to build living laboratories -communities of volunteers willing to try new ways of doing things in a natural setting-in order to test ideas and hypotheses in a real life setting. an example is the mobile territorial lab (mtl), a living lab launched by fondazione bruno kessler, telecom italia, the mit media lab and telefonica, that has been observing the lives of more than 100 families through multiple channels for more than three years [cit] . data from multiple sources, including smartphones, questionnaires, experience sampling probes, etc. has been collected and used to create a multi-layered view of the lives of the study participants. in particular, social interactions (e.g. call and sms communications), mobility routines and spending patterns, etc. have been captured. one of the mtl goals is to devise new ways of sharing personal data by means of personal data store (pds) technologies, in order to promote greater civic engagement. an example of an application enabled by pds technologies is the sharing of best practices among families with young children. how do other families spend their money? how much do they get out and socialize? once the individual gives permission, mydatastore [cit], the pds system used by mtl participants, allows such personal data to be collected, anonymized, and shared with other young families safely and automatically."
"the main contributions of this paper can be summarized as follows: 1. a new object recognition algorithm is introduced and systematically evaluated. the existence of local minima within the potential well space of the icp algorithm has been known for some time. to the best of our knowledge, this is the first attempt to exploit the existence of local minima, and allows icp, and potentially other local optimization algorithms used for registration, to be extended to solve the pose determination and object recognition problems. 2. the use of a generic model is proposed so that a single 3-d model can be used to compute the feature vectors for different objects during both preprocessing and runtime. the use of a generic model can dramatically simplify the algorithm as well as improve its efficiency. we also propose a practical method to construct an effective generic model, and examine the impact of different generic models on the performance. 3. the algorithm has been implemented and tested on both simulated and real data. in addition, a complete object recognition and tracking system utilizing a commercial stereovision camera was also implemented, that is able to recognize and track 10 freeform objects as shown in figs. 2 and 3 in real-time. 4. we also investigated the performance of applying pwse to the more difficult problem, object class recognition, with a series of experiments with princeton shape benchmark (psb) data set. in the experiments, pwse was able to achieve significantly better nearest neighbour (nn) classification rates than previously reported object retrieval methods on this standardlized data set."
"in addition, we have explored ways of applying pwse to the problem of 3-d object retrieval, and have achieved promising results. these tests were executed on the psb data set, which contains 1,814 3-d objects, divided into training and test sets of 907 objects each. pwse was able to achieved a 90.9% classification rate on the training set, and a 87.8% classification rate on the test set using a single 2.5-d query image. by further applying a multi-view classification technique, which used multiple input 2.5-d images per query, we were able to further improve the classification rate. the classification rate was increased to 96.4% and 94.3% respectively on training and test sets using five input images from different views of the query object. it was further increased to 98.7% and 96.8% using 20 input query images. these recognition rates were a significant improvement over previously reported object retrieval methods on this standardized data set."
"algorithm1 suppose the number of the collected blog articles is n, there are n keyword lists. we use algorithm 1 to merge all the keyword lists. then get the merged keyword list of all the n blog articles. if there is a new blog article, it will be combined with the merged keyword list."
") of the dragon model shown as the first model in fig. 1 . the plots clearly show differences between the two error surfaces, which is the most important property of the error surfaces, that is, the variety of their shapes are related to their input views. in addition, it is important to notice that the result is only based on the translational subspace. for 7-d error surfaces, their shapes become more complex, and they will be even more distinctive."
"an interesting ongoing initiative is the open algorithms (opal) project 14, a multi-partner effort led by orange, the mit media lab, data-pop alliance, imperial college london, and the world economic forum, that aims to open -without exposing-data collected and stored by private companies by \"sending the code to the data\" rather than the other way around. the goal is to enable the design, implementation and monitoring of development policies and programs, accountability of government action, and citizen engagement while leveraging the availability of large scale human behavioral data. opal's core will consist of an open platform allowing open algorithms to run on the servers of partner companies, behind their firewalls, to extract key development indicators and operational data of relevance for a wide range of potential users. requests for approved, certified and pre-determined indicators by third parties -e.g. mobility matrices, poverty maps, population densities-will be sent to them via the platform; certified algorithms will run on the data in a multiple privacy-preserving manner, and results will be made available via an api. the platform will also be used to foster civic engagement of a broad range of social constituents -academic institutions, private sector companies, official institutions, non-governmental and civil society organizations. overall, the opal initiative has three key objectives: (i) engage with data providers, users, and analysts at all the stages of algorithm development; (ii) contribute to building local capacities and help shaping the future technological, ethical and legal frameworks that will govern the collection, control and use of human behavioral data to foster social progress; and (iii) build data literacy among users and partners, conceptualized as \"the ability to constructively engage in society through and about data\". initiatives such as opal have the potential to enable more human-centric accountable and transparent data-driven decision-making and governance."
"a final test was executed to determine the impact of outliers. a total of 1,000 points were first randomly sampled for each image of the ideal data test. spurious data points (i.e., outliers) were then randomly inserted into each test image. the outliers were generated to lie near to surface points, ranging from 10% to 30% of the length of an image's bounding box, with the number of outliers varying between 0% and 200% of the number of points in each image. the results are illustrated in fig. 11 (c), and show a high level of robustness to outliers, as the rate of correct classification declined only from 97% to 90% when the outliers changed from 0% to 100%. the rate of correct classification is still near to 81% when outliers are at the 200% level, where there are twice as many outliers as true data points."
"previously explained. in this chapter, we highlight the need for data-driven machine learning models that are interpretable by humans when such models are going to be used to make decisions that affect individuals or groups of individuals."
"since first introduced by besl and mckay [cit], the icp algorithm has become the most prominent 3-d registration technique, and has been well explored in the literature [cit], 2007) . starting with two 3-d point clouds and an initial estimate of their relative transform, icp solves the 3-d registration problem by determining approximate correspondences between the point clouds, calculating the optimal transformation, applying this transformation, and iterating until a stopping condition is satisfied."
"from a legal perspective, tobler [cit] argued that discrimination derives from \"the application of different rules or practices to comparable situations, or of the same rule or practice to different situations\". in a recent paper, barocas and selbst [cit] elaborate that discrimination may be an artifact of the data collection and analysis process itself; more specifically, even with the best intentions, data-driven algorithmic decision-making can lead to discriminatory practices and outcomes. algorithmic decision procedures can reproduce existing patterns of discrimination, inherit the prejudice of prior decision makers, or simply reflect the widespread biases that persist in society [cit] . it can even have the perverse result of exacerbating existing inequalities by suggesting that historically disadvantaged groups actually deserve less favorable treatment [cit] ."
"the deployment of a machine learning model entails a degree of trust on how satisfactory its performance in the wild will be from the perspectives of both the builders and the users. such trust is assessed at several points during an iterative model building process. nonetheless, many of the state-of-theart machine learning-based models (e.g. neural networks) act as black-boxes once deployed. when such models are used for decision-making, the lack of explanations regarding why and how they have reached their decisions poses several concerns. in order to address this limitation, recent research efforts in the machine learning community have proposed different approaches to make the algorithms more amenable to ex ante and ex post inspection. for example, a number of studies have attempted to tackle the issue of discrimination within algorithms by introducing tools to both identify [cit] and rectify [cit] cases of unwanted bias. recently, [cit] have proposed a model-agnostic method to derive explanations for the predictions of a given model."
"we have also done tween image and its corresponding encrypted image by using the proposed encryption algorithm. the correlation between two vertically adjacent pixels, two horizontally adjacent pixels, and two diagonally adjacent pixels, we have depicted the distributions of two horizontally, vertically and diagonally adjacent pixels in the original and encrypted images respectively see figure 9 and table 1 for lena image and figure 10 and table 2 for sharp edge image, we note the two adjacent pixels in the original image are highly correlated, and the two adjacent pixels in the encrypted image are highly not correlated."
"in the rest of this chapter, we provide the readers with a compendium of the issues arising from current big data approaches, with a particular focus on specific use cases that have been carried out to date, including urban crime prediction [cit], inferring socioeconomic status of countries and individuals [cit], mapping the propagation of diseases [cit] and modeling individuals' mental health [cit] . furthermore, we highlight factors of risk (e.g. privacy violations, lack of transparency and discrimination) that might arise when decisions potentially impacting the daily lives of people are heavily rooted in the outcomes of black-box data-driven predictive models. finally, we turn to the requirements which would make it possible to leverage the predictive power of data-driven human behavior analysis while ensuring transparency, accountability, and civic participation."
"each object is represented as a set of discrete object views. we define a view of an object as a range image acquired from a particular sensor vantage with respect to the object's centroid. when an object is scanned with a conventional range sensor, only the front-facing surfaces of the object are visible from the sensor vantage. the remaining surfaces are selfoccluded, and for this reason the resulting range images are called 2.5-d, with the object's surface bisecting the dimension along the sensor line of sight."
"the translational subspace of is first quantized into discrete grids using quantization vector (d d /15, d d /15, d d /15), and then a total of k hash tables are built for each of the k local minima in a preprocessing step. in each hash table, the indices of the prestored embeddings share the same entry only if their k th local minima are located within the same grid in the translational subspace of . at runtime, the hashcodes for each of the k local minima of the runtime embedding are computed. then, the prestored local minima that are close to each of the k local minima of the runtime embedding can be efficiently retrieved using the precalculated hash tables. each hash table retrieval returns indices of a subset of prestored embeddings, and a total of k retrieved subsets vote on the possible solutions of the closest match of the runtime embedding."
"these methods above could detect hot topic and evaluate hotness of the topic effectively. however, blog has some particular structure features itself. these methods may not be completely suitable for hot topic detection in blogs. in this paper, employing cwst and taking both the several words in the sentence which locate left and right of the keywords into account, we propose a method to detect hot topic in blogs based on ngram model. firstly, the hot keywords are extracted based on the co-occurrence information in n-gram items which provide more information about hot topic. then, hot keywords groups are collected by calculating the similarity of n-gram items containing keywords. finally, the hot topics are detected by computing the similarity between keywords and n-gram items to decide whether the keyword group represents a hot topic. the hotness of the topic is evaluated by the value of keywords' weight."
"an alternative is to use a generic model. in preprocessing, for each object, rather than generating the embedding by convolving the views of the object with a model of itself, they are instead convolved with a single generic model. at runtime, only a single embedding of the error surface of the image is then required to be calculated and compared against the database. for a generic model to be useful, it must generate error surface embeddings that are sufficiently distinctive so as to enable the discrimination of a number of objects. the utility of a given generic model at recognizing a collection of objects would depend upon the relative geometries between the objects in the collection, as well as that of the generic model itself."
"is impressed on the inductor. the converter bridges maintain a unidirectional current flow and as the circuit is inductive the current does not change instantaneously. in this mode of operation, a positive converter voltage produces positive power, which means charging the coil, and a negative converter voltage produces a negative power and discharges the inductor. when the frequency dip in the power system causes a negative voltage to be applied to the inductor, power flows from the inductor into the power system, sharing the sudden load requirement. the reverse process takes place when there is a sudden load rejection in the power system. the frequency increase causes a positive voltage to be impressed on the inductor and the mes unit absorbs the excess power from the power system. the conceptual diagram of active and reactive power modulation under equal-α mode is shown in fig. 4 . in actual practice the inductor current should not be allowed to reach zero to prevent the possibility of discontinuous conduction in the presence of the large disturbances [cit] & [cit] . it is desirable to set the rated inductor current i sm0 such that the maximum allowable energy absorption equals the maximum allowable energy discharge [cit] & [cit] . this makes the smes equally effective in damping swings caused by sudden increase as well as decrease in load. thus, if the lower current limit is chosen at 0.3 i sm0, the upper inductor current limit, based on the equal energy absorption/discharge criterion becomes 1.38 i sm0 [cit] . when the inductor current reaches either of these limits, the dc voltage has to be brought to zero. as the inductor has a finite inductance and hence a finite amount of energy stored in it, the current in the inductor falls as energy is withdrawn from the coil. this deviation in the inductor current is expressed as"
"a set of experiments was performed to study the robustness of the proposed method with respect to data sparseness, noise (i.e. measurement error) and outliers. to measure the influence of data sparseness, a new set of test images was generated by randomly sampling 1,000, 500, 250, 125 and 75 points from each test image from the ideal data tests. the same experiment as in the ideal data tests was then executed on this data set, and the results are shown in fig. 11(a) . it can be seen that the method is robust to data sparseness as the rate of correct recognition varied by only about 0.3% when the number of points per image was reduced from 1,000 to 500. when using only 125 points per image, the algorithm still achieved a 92.6% correctness rate, with an increased efficiency of over 200 fps. the evaluation of robustness vs. measurement error was achieved by first randomly sampling 1,000 points from each test image from the ideal data test, and then adding gaussian noise to each point. a gaussian noise model was used in the tests as is conventional when the exact distribution is unknown. the noise was zero mean and the standard deviation varied between 0% and 20% of the size of an image's bound- fig. 11(b), and show that the proposed algorithm has a good level of robustness to gaussian noise below ∼10%. the rate of correct pose estimation declined fairly slowly when the sensor noise was less than 10%."
"we consider the word whose weight exceeds the predefined hot threshold as a keyword, and put it into a keyword list. we create a keyword list for each blog article. the data structure of the node in the keyword list is defined as follows:"
"as noted in the previous sections, both governments and companies are increasingly using data-driven algorithms for decision support and resource optimization. in the context of social good, accountability in the use of such powerful decision support tools is fundamental in both validating their utility toward the public interest as well as redressing corrupt or unjust harms generated by these algorithms. several scholars have emphasized elements of what we refer to as the dark side of data-driven policies for social good, including violations of individual and group privacy, information asymmetry, lack of transparency, social exclusion and discrimination. arguments against the use of social good algorithms typically call into question the use of machines in decision support and the need to protect the role of human decision-making. however, therein lies a huge potential and imperative for leveraging large scale human behavioral data to design and implement policies that would help improve the lives of millions of people. recent debates have focused on characterizing data-driven policies as either \"good\" or \"bad\" for society. we focus instead on the potential of data-driven policies to lead to positive disruption, such that they reinforce and enable the powerful functions of algorithms as tools generating value while minimizing their dark side."
"model-based techniques are robust to clutter and occlusion because the descriptors are calculated for a limited local support region. however, they also have several drawbacks. first, a large number of descriptors need to be calculated, and their correspondences in the model database need to be solved at runtime. consequently, these techniques in general are time expensive, which makes them difficult to apply to time-critical tasks. second, many of these methods require dense range data which can be relatively slow to acquire. for high-speed range sensors (e.g., stereovision sensors and flash lidar), and scanning lidar in high speed mode (i.e., using lissajous patterns), the acquired data tend to be sparse and contain considerable noise, which poses difficulties for model-based techniques. finally, most model-based techniques involve the step of calculating surface normals in order to establish local coordinate systems, which makes them sensitive to sensor noise [cit] )."
"in pose determination applications, it is assumed that there is a single object under consideration, and a reasonable choice for model m is a representation of that object. alternately, in object recognition there may be multiple objects under consideration, and it is not known a priori which object is being imaged in the scene. the straightforward application of the method to object recognition would be to build the database using multiple models, and then at runtime to calculate embeddings of the image with each of these models and compare these embeddings to the database. this would be expensive, however, as not only would the database grow linearly with the number of models, but so too would the number of embeddings required to be computed at runtime."
"however, researchers from different disciplinary backgrounds have identified a range of social, ethical and legal issues surrounding data-driven decision-making, including privacy and security [cit], transparency and accountability [cit], and bias and discrimination [cit] . for example, barocas and selbst [cit] point out that the use of data-driven decision making processes can result in disproportionate adverse outcomes for disadvantaged groups, in ways that look like discrimination. algorithmic decisions can reproduce patterns of discrimination, due to decision makers' prejudices [cit], or reflect the biases present in the society [cit], the white house released a report, titled \"big data: seizing opportunities, preserving values\" [cit] that highlights the discriminatory potential of big data, including how it could undermine longstanding civil rights protections governing the use of personal information for credit, health, safety, employment, etc. for example, data-driven decisions about applicants for jobs, schools or credit may be affected by hidden biases that tend to flag individuals from particular demographic groups as unfavorable for such opportunities. such outcomes can be self-reinforcing, since systematically reducing individuals' access to credit, employment and educational opportunities may worsen their situation, which can play against them in future applications."
"a simple background subtraction technique was used to separate the objects from background clutter. once the number of foreground points in the subtracted image exceeded a certain predefined threshold, the system deemed that an object was present in the scene and invoked pwse for object recognition. to improve efficiency, a set of points was randomly selected from the subtracted image (e.g., 1,000 points), and this sparse range image served as input to pwse. once the object was recognized by pwse, the output (i.e., object id and initial pose estimate) was fed into fig. 15 class recognition, misclassified models a more efficient tracking algorithm, the bounded hough transform (bht) ). the results were visualized by rendering the 3-d model of the object with the estimated value of the translation and rotation."
"one limitation of the current pwse implementation for object recognition applications is that it is a global method and cannot handle background clutter, and thus requires the foreground objects to be initially segmented from the background scene. without modification, it is difficult to directly apply the proposed algorithm to static scenes, and local techniques may be more suitable for such applications. however, when the object can be separated from background clutter (e.g., through motion segmentation and in 3-d object retrieval), the proposed algorithm is an attractive alternative to local methods due to its efficiency and robustness to data spareness, sensor noise and outliers."
"following a sudden increase in load in the power system, the incremental power expressed by equation (8) is discharged into the power system by the energy storage unit to share with the generator rotor, the extra load demand. figure 5 shows the proposed configuration of smes units in a two-area power system [cit] . two areas are connected by a weak tie-line. when there is sudden rise in power demand in a control area, the stored energy is almost immediately released by the smes through its power conversion system (pcs). as the governor control mechanism starts working to set the power system to the new equilibrium condition, the smes coil stores energy back to its nominal level. similar action happens when there is a sudden decrease in load demand. basically, the operation speed of governor-turbine system is slow compared with that of the excitation system. as a result, fluctuations in terminal voltage can be corrected by the excitation system very quickly, but fluctuations in generated power or frequency are corrected slowly. since load frequency control is primarily concerned with the real power/frequency behavior, the excitation system model will not be required in the approximated analysis [cit] & [cit] . this important simplification paves the way for constructing the simulation model shown in fig. 6 ."
"being able to accurately measure and monitor key sociodemographic and economic indicators is critical to design and implement public policies [cit] . for example, the geographic distribution of poverty and wealth is used by governments to make decisions about how to allocate scarce resources and provides a foundation for the study of the determinants of economic growth [cit] . the quantity and quality of economic data available have significantly improved in recent years. however, the scarcity of reliable key measures in developing countries represents a major challenge to researchers and policy-makers 1, thus hampering efforts to target interventions effectively to areas of greatest need (e.g. african countries) [cit] . recently, several researchers have started to use mobile phone data [cit], social media [cit] and satellite imagery [cit] to infer the poverty and wealth of individual subscribers, as well as to create high-resolution maps of the geographic distribution of wealth and deprivation."
"interestingly, by considering the misclassified cases, we found that pwse is good at capturing the overall geometric characteristics of the models. for instance, the outlier class hot_air_balloons (model 1337-1345) was mis-recognized as helmet (model 1637), snowman (model 1758), ice_cream (model 751, 754, 759 and 760), potted_plant (model 1026) and head (model 347), which are all sphere-shell like models as illustrated in the first row of fig. 15 . similarly misclassified cases include: flying_saucer to round, human_arms_out to fighter_jet, sword to flower_with_stem, fireplace to laptop, stealth_bomber to handgun, book to microchip, chess_set to rectangular and double_doors, desktop to tv, computer_monitor to roman_building, wheel and gear to tire, and so on."
"[] l i pos . so the keyword group represents a hot topic candidate, we need to calculate the similarity between the keyword group and their n-gram items to decide whether the keyword group represents a hot topic. step2 repeat step1 until all the keywords in"
where w and h are the width and height of encrypted image. we obtained npcr for a large number of images by using our encryption scheme and found it to be over 99% showing thereby that the encryption scheme is very
"the results show that when the number of spheres exceeded 60, the differences among the results using different generic models were minor. in fact, the recognition rate only varied within one percent when the number of spheres changed from 60 to 150, which shows pwse to be fairly tolerant to the use of different generic models."
"in a recent book, william easterly evaluates how global economic development and poverty alleviation projects have been governed by a \"tyranny of experts\" -in this case, aid agencies, economists, think tanks and other analysts -who consistently favor top-down, technocratic governance approaches at the expense of the individual rights of citizens [cit] . easterly details how these experts reduce multidimensional social phenomena such as poverty or justice into a set of technical solutions that do not take into account either the political systems in which they operate or the rights of intended beneficiaries. take for example the displacement of farmers in the mubende district of uganda: as a direct result of a world bank project intended to raise the region's income by converting land to higher value uses, farmers in this district were forcibly removed from their homes by government soldiers in order to prepare for a british company to plant trees in the area [cit] . easterly underlines the cyclic nature of this tyranny: technocratic justifications for specific interventions are considered objective; intended beneficiaries are unaware of the opaque, black box decision-making involved in these resource optimization interventions; and experts (and the coercive powers which employ them) act with impunity and without redress."
"in the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n words from a given sequence of text or speech. n-gram models are widely used in statistical natural language processing, and approximate matching. statistical n-gram models capturing patterns of local co-occurrence of contiguous words in sentences have been used in various hybrid implementations of natural language processing and machine translation systems [cit] . in this paper, one contiguous sequence of n words is referred to as an ngram item, and one n-gram item has n entries. we use ngram model to present sentence that contains keyword. words that relate with the keywords can be found in the n-gram items, so the detailed information of topic that keywords depict can be detected. the keywords depicting one topic can be detected through calculating the similarity of n-gram items that contain keywords."
"the results were qualitatively evaluated by examining the 3-d model rendered using the estimated pose. the results confirm that a good recognition rate was obtained for all of the five tested models. five screen shots of selected frames for the different objects being recognized by the system are shown in fig. 16 . each frame contains two views. the left view is used to display the intensity video, whereas the white circle is used to indicate the rough location of the tracked object in the current frame. the right view is used to render the 3-d model of the object using the estimated pose."
"most of researchers agree on the following definition \"chaos is aperiodic long term behavior in a deterministic system that exhibits dependence on initial condition\" [cit] . so to examine theses conditions we note in figure 3 the new system has two attractors, thus satisfying the above definition."
"with little modification, we directly applied pwse algorithm to the more difficult problem of object class recognition, and tested it on the well-known psb data set. the results show that pwse outperformed lfd by 31.1% in terms of nn classification. pwse is slightly less time-efficient and consumes more memory per object than lfd, mainly because the current version of pwse was targeted at object recognition applications, which requires the storage of a large number of embeddings for each view of the objects in order to determine the pose of objects."
"the membership functions, knowledge base and method of defuzzification determine the performance of the fgspi controller in a multi-area power system as shown in (13). mamdani's max-min method is used. the center of gravity method is used for difuzzification to obtain k i . the entire rule base for the fgspi controller is shown in table i . figure 10 outlines the proposed simple control scheme for smes, which is incorporated in each control area to reduce the instantaneous mismatch between the demand and generation, where i sm, v sm and p sm are smes current, smes voltage and smes power respectively. for operating point change due to load changes, gain (k ii ) scheduled supplementary controller is proposed. firstly k ii is determined using the fuzzy controller to obtain frequency deviation, δf, and tie-line power deviation, δptie. finally ace i which is the combination of δptie and δf [as shown in (9)] is used as the input to the smes controller. it is desirable to restore the inductor current to its rated value as quickly as possible after a system disturbance, so that the smes unit can respond properly to any subsequent disturbance. so inductor current deviation is sensed and used as negative feedback signal in the smes control loop to achieve quick restoration of current and smes energy levels."
"this paper is organized as follows: section 2 introduces n-gram model and gives some concepts. section 3 presents our approach for hot topic detection and hotness of the topic evaluation in blogs. section 4 presents the experimental results on chinese corpus. finally, we summarize the future work."
"for each node in hot keyword list, we use its weight value to measure the hotness of its keyword, that is to say, the hotness of a hot keyword is presented with the value of its weight. we rank all the keywords according to their weights' value. the hottest keyword is arranged in the first node of hot keyword list."
"as this value of v sm0 is very small, the firing angle will be nearly 90 0 . at any instant of time the amount of energy stored in the inductor is given by once the rated current in the inductor is reached, the unit is ready to be coupled with the power system application. the frequency deviation δf of the power system is sensed and fed to the mes unit as the error signal. δv sm is then continuously controlled depending on this signal. when there is a sudden increase in load in the power system, the frequency falls and a negative voltage, expressed by equation"
"in this section, we present key human-centric requirements for positive disruption, including a fundamental renegotiation of user-centric data ownership and management, the development of tools and participatory infrastructures towards increased algorithmic transparency and accountability, and the creation of living labs for experimenting and co-creating data-driven policies. we place humans at the center of our discussion as humans are ultimately both the actors and the subjects of the decisions made via algorithmic means. if we are able to ensure that these requirements are met, we should be able to realize the positive potential of data-driven algorithmic decision-making while minimizing the risks and possible negative unintended consequences."
"icp is among the most useful algorithms in range image processing, and many researchers have explored its properties. the existence of local minima in the well space is wellknown, and the general strategy has been to avoid converging to a local minimum by ensuring that the initial pose lies within the global minimum well. in contrast, the proposed method explicitly explores the error surface in an attempt to converge to a set of local minima. to our knowledge, this is the first attempt to exploit the existence of local minima, and allows icp, and potentially other local optimization algorithms used for registration, to be extended to solve the pose determination and object recognition problems."
"most existing object recognition and object class recognition techniques extract features from local neighborhoods of the models or images, and then compress these local features into a compact and invariant representation. for instance, point signatures use curvature features and compress them into 1-d contours that is invariant to rotations; spin images represent a small surface patch around a point by a 2-d histogram that is invariant to rotations; the tensor-based technique represents a triangular mesh by a third order tensor. in extracting and compressing these features, geometric information is lost. also, these features require a dense sampling of the surface around each point, and are sensitive to the effects of data sparseness. as a result, the descriptors resulting from these local features may lack discriminating power, which can lead to a high proportion of incorrect point correspondences between the scene and the model. to compensate for incorrect correspondences, robust techniques such as ransac or the generalized hough transform can be used, which while effective, nevertheless increases the runtime expense of these methods."
"it is worth noting that the psb database is primarily designed for the problem of class recognition, and therefore contains many similar objects. for instance, it has 50 fighter_jet models, 50 human models, 17 head models, etc. in the above experiment, each test image consisted of only 1,000 points, so that the fine details of the models were often lost due to the sampling effect. therefore, there are many cases where pwse mis-recognized the query object as another object within the same class. in addition, the psb also contains many symmetric objects (i.e., swords, tools, hourglass and etc.), which reduces the nominal accuracy of the pose determination rate, as any rotations around the axes of symmetry will be not be accommodated and will incorrectly be reported as a failure."
the detected top four blog topics using the proposed method based on five-gram model is shown in table ⅳ. this list is sorted by weight that represents the hotness of topic in descending order.
"the-fourth-anniversary -of -the; fourth-anniversary -ofthe-wen; anniversary -of -the-wen-chuan; of -the-wen-chuan-earthquake figure. 1 an example of five-gram model blog topic, and the comment opinion into account. lan you, yongping du [cit] evaluated the hotness of the topic through its popularity, quality and message distribution using back-propagation neural network based on classification algorithm. tingting he, guozhong qu [cit] presented a semi-automatic hot event detection approach. they evaluated the activity of events firstly, then filtered and sorted the events according to the activity of events, and finally got hot event."
"although [cit], more difficult objects were also added to our database, such as trees and flowers that contain many discontinuous surfaces. this kind of object would cause particular difficulties for the tensor-based technique, which requires to build a surface mesh and calculate the normals of the points. in contrast, pwse was able to deal with these objects very well because it works directly on 3-d points and doesn't require a mesh or normals. in addition, pwse was also evaluated on the much larger psb data set containing 1,814 objects, and achieved promising results."
"in order to investigate the impact of different generic models on performance, five generic models were generated. these generic models were completely different, with each generated independently by using 30, 60, 90, 120 and 150 spheres. each generic model contained a total of 4,000 points which were randomly sampled from each surface model of the generic models. the same ideal data tests were then reexecuted, using different generic models, and the results are shown in fig. 12 ."
"recent researches of nonlinear dynamical systems have been increasingly based on chaos [cit] . image encryption schemes have been increasingly studied to meet the demand for real-time secure image transmission over private or public networks. conventional image encryption algorithm such as data encryption standard (des), is not suitable for image encryption because of the special storage characteristics of an image [cit] and weakness of low-level efficiency when the image is large [cit] .the chaos-based encryption has suggested a new and efficient way to deal with the intractable problem of fast and highly secure, image encryption."
"generation and distribution of electric energy with good reliability and quality is very important in power system operation and control. this is achieved by automatic generation control (agc). in an interconnected power system, as the load demand varies randomly, the area frequency and tie-line power interchange also vary. the objective of load frequency control (lfc) is to minimize the transient deviations in these variables and to ensure for their steady state values to be zero. the lfc performed by only a governor control imposes a limit on the degree to which the deviations in frequency and tie-line power exchange can be minimized. however, as the lfc is fundamentally for the problem of an instantaneous mismatch between the generation and demand of active power, the incorporation of a fast-acting energy storage device in the power system can improve the performance under such conditions. but fixed gain controllers based on classical control theories are presently used. these are insufficient because of changes in operating points during a daily cycle [cit] & [cit] and are not suitable for all operating conditions. therefore, variable structure controller [cit] & [cit] has been proposed for agc. for designing controllers based on these techniques, the perfect model is required which has to track the state variables and satisfy system constraints. therefore it is difficult to apply these adaptive control techniques to agc in practical implementations. in multi-area power system, if a load variation occurs at any one of the areas in the system, the frequency related with this area is affected first and then that of other areas are also affected from this perturbation through tie lines. when a small load disturbance occurs, power system frequency oscillations continue for a long duration, even in the case with optimized gain of integral controllers [cit] & [cit] . so, to damp out the oscillations in the shortest possible time, automatic generation control including smes unit is proposed. therefore, in the proposed control system, with an addition of the simple smes controller, a supplementary controller with k ii (as shown in fig. 6 ) is designed in order to retain the www.intechopen.com energy storage 88 frequency to the set value after load changes. these controllers must eliminate the frequency transients as soon as possible. using fuzzy logic, the integrator gain (k ii ) of the supplementary controller is so scheduled that it compromise between fast transient recovery and low overshoot in dynamic response of the system. it is seen that with the addition of gain scheduled supplementary controller, a simple controller scheme for smes is sufficient for load frequency control of multi-area power system [cit] ."
"the implementation of icp used an efficient icp variant [cit] ) that uses a euclidean distance map (i.e. a voxel map) to accelerate the nearest neighbour computation. the maximum number of iterations was set to 3, which was used in both preprocessing and runtime. the generic model illustrated in fig. 8 was used for all tests. the generic model was in point cloud format consisting of 4,000 points, which was obtained in preprocessing by randomly sampling a surface model of the object."
"used in pwse are constructed by combining locations of k independent local minima together. to find the closest match of the runtime embedding, it is required only to search among a small portion of prestored embeddings that have the majority of their local minima close to the corresponding local minima of the runtime embedding. in practice, we found that the results are sufficiently good by just using the translational component of each local minimum, which makes the technique even more efficient."
"the remainder of the paper is organized as follows. section 2 presents the main representation as an embedding of the error surfaces of an object's views, and describes the algorithms for preprocessing and online recognition. experimental results on real data, on a simulated database containing 60 objects, and on the psb data set are presented in sect. 3 and demonstrate the effectiveness of the proposed algorithm under various non-ideal conditions. a set of tests, which is designed to evaluate the performance of pwse on object class recognition are also presented in this section. section 4 concludes with a discussion of the results and future work."
"in more recent work, an experiment conducted on the tensor-based technique [cit] ) was close to our ideal data tests. to the author's knowledge, the tensorbased technique achieved the best accuracy reported to date from among the model-based approaches. compared to their work, we used a slightly larger database (60 objects vs. 50 objects), and a much larger number of test images (12,000 images vs. 500 images). pwse was able to achieve a better result than that reported for the tensor-based technique, at 97% vs. 95%. the time efficiency of the tensor-based experiments was not reported, although it was mentioned that their implementation was not optimized for time as it was developed in matlab."
"pixels another important test is measured the number of change rate (npcr) to see the influence of changing a single pixel in the original image on the encrypted image by the proposed algorithm defined by equation (7). the npcr measure the percentage of different pixel numbers between the two images and the uaci (unified average changing intensity) defined by equation (8) . we take two ncrypted images, c1 and c2, whose corresponding e original images have only one-pixel difference. we define a two-dimensional array d, having the same size as the image c1 and c2. if"
"in the following sub-sections, we assess the nature, function and impact of the use of social good algorithms in three key areas: criminal behavior dynamics and predictive policing; socio-economic deprivation and financial inclusion; and public health."
"remark: thd represents en threshold in second step, we use the remaining 3500 articles to test. in addition, we implemented other methods as compare experiments. the methods we used for comparison are multiple keywords combination (mkc) method, single-pass clustering (spc) method."
"discrimination from algorithms can occur for several reasons. first, input data into algorithmic decisions may be poorly weighted, leading to disparate impact; for example, as a form of indirect discrimination, overemphasis of zip code within predictive policing algorithms can lead to the association of low-income african-american neighborhoods with areas of crime and as a result, the application of specific targeting based on group membership [cit] . second, discrimination can occur from the decision to use an algorithm itself."
"where a(x) is the value of grade of membership, 'b' is the width and 'a' is the coordinate of the point at which the grade of membership is 1 and 'x ' is the value of the input variables. the control rules for the proposed strategy are very straightforward and have been developed from the viewpoint of practical system operation and by trial and error methods."
"in our approach, instead of searching for the global minimum, icp is employed as a feature extraction method to deliberately find the local minima in parameter space. these local minima are then used as compact feature vectors to index objects and their poses. the problem of object recognition is thus reduced to comparing these feature vectors to find a match between the runtime data and a preprocessed database. as only the locations of local minima are considered, the performance of our algorithm is not related to the convergence of icp to the global minimum."
"techniques of 3-d object recognition can be roughly divided into two main categories: model-based, and appearance-based. in the model-based approach, a model library is first built in a preprocessing step by extracting descriptors (features) from each 3-d model of the object. each descriptor, as indicated by its name, characterizes the surface shape in a support region surrounding a basis point on the 3-d model. in online recognition, the same descriptor set is extracted from the scene image, and the problem of object recognition is solved by matching the extracted descriptors with those in the library. a variety of such descriptors have been proposed, including: point signature [cit], 1999), surface signature [cit], point fingerprint [cit], regional point descriptor [cit] and recently tensor-based descriptor [cit] ) and variable dimensional local shape descriptor [cit] ."
"at present, there is no public blog corpus, we design a crawler to get blog articles from sina blog, tencent blog, which are two of most popular blogs in china. all the data were published from may 1, 2012 to may 14, 2012. there are up to 7980 articles. all articles have been already preprocessed, such as word segmentation, partof-speech tagging and unknown words recognition."
"the test image set contained 12,000 images, 200 from each of the 60 objects. for each object, simulated range images were generated by applying a random rotation vector (θ, φ, ψ) to the object's canonical pose, where"
"the use of algorithmic data-driven decision processes may also result in individuals mistakenly being denied opportunities based not on their own action but on the actions of others with whom they share some characteristics. for example, some credit card companies have lowered a customer's credit limit, not based on the customer's payment history, but rather based on analysis of other customers with a poor repayment history that had shopped at the same establishments where the customer had shopped [cit] ."
where l and r l are the inductance and the resistance of inductor respectively. once the current reaches its rated value i sm0 it is held constant by reducing the voltage to a value v sm0 enough to overcome the resistive drop. in this case
"although pwse was designed for object recognition, it was interesting to see how well it could deal with the problem of object class recognition. a set of tests were conducted to investigate the performance of pwse on the object class recognition problem using the psb database. the parameter k was set to 60 for all the tests based on the results of the previous section."
"in contrast, the performance of pwse mainly derives from the use of highly descriptive and robust features, which are extracted from the 7-d error surface. each point on the error surface is formed by computing the rms error using all of the image points, which makes it convey much more information than a single point in the image data, and therefore makes it very robust to data sparseness, noise and outlier. like the local methods, pwse also represents the error surface in a compact embedded form, it nevertheless still carries more information than features extracted solely from the image domain. for this reason, pwse features tend to be more discriminating than local features, and don't require additional statistic methods such as ransac to filter out mismatches. in addition, the 7-d error surface has a rich landscape, so that more distinctive and discriminating features can be extracted from it. for this reason, pwse is able to achieve a good classification rate on objects with simple regular geometries, such as a sphere, cone, rectangular, cube, etc."
"chaotic systems have many important properties, such as the sensitive dependence on initial conditions and system parameters, the density of the set of all periodic points and topological transitivity, etc. most properties are related to some requirements such as mixing and diffusion in the sense of cryptography [cit], since then, increasing researches of image encryption technology are based on chaotic systems [4, [cit] . these methods have high-level efficiency but also weakness, such as small key space and weak security and complexity to overcome these drawbacks. this paper proposes a image encryption scheme based on combining two chaotic systems (lorenz and rosslere) and compares the results with the other techniques. the paper is organized as follows. section 2 discusses the proposed scheme. section 3 presents experimental results. section 4 to section 7 discuss and examine the new scheme by applying statistical tests test also the security of the chaotic encryption is discussed. conclusions are drawn in section 8."
"prior to the load disturbance, let the magnitudes of voltage and current are v sm0 and i sm0 (nominal values). thus the initial power flow into the coil can be expressed as"
"there are, of course, different ways that models can be classified, such as structural, functional, etc. the psb recognizes this by offering a variety of different classification schemes, of varying granularity (i.e. coarseness), and the performance of pwse on these different classification granularities was further evaluated. the \"coarse\" classification merges the 92 base classes to form 44 classes. the \"coarser\" classification merges these 44 classes further to from just six classes, and the \"coarsest\" classification merges these six classes to form just two classes: man-made objects and natural objects. the classification rates of pwse with respect to these different classifications is listed in table 3 . as expected, pwse performed better on the coarser classifications. this is particularly true when the results of complete non-overlapping data set are considered, in which case the classification rate increased from 26.9% for the \"base\" granularity, to 80.3% for the \"coarsest\" granularity. in contrast, the improvement for the overlapping classes was less significant. this result indicates that pwse is effective at discriminating between man-made and natural objects, possibly because man-made objects typically comprise primary shapes (e.g., sphere, disk, rectangular, etc.) which pwse is good at recognizing."
the security of lorenz and rossler encryption methods depend on three parameters but in the proposed scheme in this paper the security level was increased to six parameters.
"the ability to recognize and determine the pose of threedimensional (3-d) objects in range image data has long been a goal of computer vision research, and remains a challenging area of investigation. given a 2.5-d point cloud acquired by a range scanner, the goal of a recognition system is to identify objects in the scene by comparing them to a set of known objects in a database, and recovering their pose (i.e., translation and rotation). although many approaches to object recognition from 2 [cit], 2000; [cit], these techniques are sensitive to shadows and illumination due to the limitation of the sensor. with the improvements of range sensors, object recognition from range images has attracted more and more attention, because not only does it not suffer from the above mentioned problems, but it can also provide metric measurement data."
"using a single 2.5-d query image that was generated from each object model, pwse was able to achieve a 90.9% classification rate on the training set, and a 87.8% classification rate on the test set. [cit] . of those twelve methods, the most successful was the light field descriptor (lfd), which achieved a 65.7% classification rate on the psb training data set, 22.1% lower than that achieved by pwse. it is interesting that this higher classification rate was achieved by pwse using only 2.5-d range data as input, such as can be acquired by standard range imaging sensors. in contrast, most of the other 12 [cit] required complete 3-d information about the object, without considering the effects of self-occlusion."
"in general, the error surfaces have very complex shapes and consist of many local minima. as the proposed technique is based on extracting features from the error surfaces, it would be interesting to examine the error surfaces directly. however, it is very time-consuming to compute entire error surfaces, and difficult to visualize such high-dimensional data. to simplify the problem, we produced the error surfaces by convolving over only the translational subspace of which forms a 4-d error surface, and then used curvilinear component analysis (demartines and hérault 1997) (cca) to project the 4-d error surface onto the 3-d space for visualization. although some information is lost using this method, the projections are good enough to illustrate the basic characteristics of error surfaces."
"the use of novel sources of behavioral data and algorithmic decisionmaking processes is also playing a growing role in the area of financial services, for example credit scoring. credit scoring is a widely used tool in the financial sector to compute the risks of lending to potential credit customers. providing information about the ability of customers to pay back their debts or conversely to default, credit scores have become a key variable to build financial models of customers. thus, as lenders have moved from traditional interviewbased decisions to data-driven models to assess credit risk, consumer lending and credit scoring have become increasingly sophisticated. automated credit scoring has become a standard input into the pricing of mortgages, auto loans, and unsecured credit. however, this approach is mainly based on the past financial history of customers (people or businesses) [cit], and thus not adequate to provide credit access to people or businesses when no financial history is available. therefore, researchers and companies are investigating novel sources of data to replace or to improve traditional credit scores, potentially opening credit access to individuals or businesses that traditionally have had poor or no access to mainstream financial services -e.g. people who are unbanked or underbanked, new immigrants, graduating students, etc. researchers have leveraged mobility patterns from credit card transactions [cit] and mobility and communication patterns from mobile phones to automatically build user models of spending behavior [cit] and propensity to credit defaults [cit] . the use of mobile phone, social media, and browsing data for financial risk assessment has also attracted the attention of several entrepreneurial efforts, such as cignifi 2, lenddo 3, inventure 4, and zestfinance 5 ."
the chapter discussed about the simulation studies that have been carried out on a two-area power system to investigate the impact of the proposed intelligently controlled smes on the improvement of power system dynamic performances. the results clearly show that the scheme is very powerful in reducing the frequency and tie-power deviations under a variety of load perturbations. on-line adaptation of supplementary controller gain associated with smes makes the proposed intelligent controllers more effective and are expected to perform optimally under different operating conditions.
"in this chapter we have provided an overview of both the opportunities and the risks of data-driven algorithmic decision-making for the public good. we are witnessing an unprecedented time in our history, where vast amounts of fine grained human behavioral data are available. the analysis of this data has the impact to help inform policies in public health, disaster management, safety, economic development and national statistics among others. in fact, the use of data is at the core of the 17 sustainable development goals (sdgs) defined by united nations, both in order to achieve the goals and to measure progress towards their achievement."
"end for where l i pos contain the context information of keywords in the sentence which includes these keywords, so the n-gram items descript the time and place information of hot topic in detail."
"the goal of the step is to encrypt images by shuffling pixel values and then changing the grey scale values to create an encrypted image. the pixel values are rearranged using the xor and then the grey scale values are changed using a multiple chaotic systems, and therefore after a set number of iterations (which depends on the size of the image), generated elements have been stored within the chaotic matrix of size the same as the original image's size. as with the other algorithms that made use of the xor operation to encrypt, decryption is a simple matter of recreating the matrix of chaotic elements and xor with the decrypted image matrix and therefore after a set number of iterations the original image will return."
"a big question on the table for policy-makers, researchers, and intellectuals is: how do we unlock the value of human behavioral data while preserving the fundamental right to privacy? this question implicitly recognizes the risks, in terms not only of possible abuses but also of a \"missed chance for innovation\", inherent to the current paradigm: the dominant siloed approach to data collection, management, and exploitation, precludes participation to a wide range of actors, most notably to the very producers of personal data (i.e. the users)."
"the experimental results show the proposed method to be both efficient and robust to data sparseness, outliers, and measurement error. only a small number (e.g. 3) of iterations is required for each icp invocation in both preprocessing and runtime. the performance of the proposed algorithm runs at 122 frames per second on standard hardware, and the object recognition and pose determination rates respectively exceed 98% and 97% with a database of 60 objects."
"two or three keywords can generally express a topic, but they can't provide the detailed topic information, such as time, place and related people. a topic in blogs is a cluster that is composed of a number of blog articles sharing theme. each blog article has one or more keywords to depict the theme. for a blog article, in the sentence containing keywords, words that are not far away from the keywords generally provide the topic information. so for the sake of effectiveness, we use ngram model to present sentences, set a distance window of n entries, and take the n-gram items that containing keywords into account."
"more recently, these predictive policing approaches [cit] are moving from the academic realm (universities and research centers) to police departments. in chicago, police officers are paying particular attention to those individuals flagged, through risk analysis techniques, as most likely to be involved in future violence. in santa cruz, california, the police have reported a dramatic reduction in burglaries after adopting algorithms that predict where new burglaries are likely to occur. in charlotte, north carolina, the police department has generated a map of high-risk areas that are likely to be hit by crime. the police departments of los angeles, atlanta and more than 50 other cities in the us are using predpol, an algorithm that generates 500 by 500 square foot predictive boxes on maps, indicating areas where crime is most likely to occur. similar approaches have also been implemented in brasil, the uk and the netherlands. overall, four main predictive policing approaches are currently being used: (i) methods to forecast places and times with an increased risk of crime [cit], (ii) methods to detect offenders and flag individuals at risk of offending in the future [cit], (iii) methods to identify perpetrators [cit], and (iv) methods to identify groups or, in some cases, individuals who are likely to become the victims of crime [cit] ."
"another important area of opportunity within public health is mental health. mental health problems are recognized to be a major public health issue 6 . however, the traditional model of episodic care is suboptimal to prevent mental health outcomes and improve chronic disease outcomes. in order to assess human behavior in the context of mental wellbeing, the standard clinical practice relies on periodic self-reports that suffer from subjectivity and memory biases, and are likely influenced by the current mood state. moreover, individuals with mental conditions typically visit doctors when the crisis has already happened and thus report limited information about precursors useful to prevent the crisis onset. these novel sources of behavioral data yield the possibility of monitoring mental health-related behaviors and symptoms outside of clinical settings and without having to depend on self-reported information [cit] . for example, several studies have shown that behavioral data collected through mobile phones and social media can be exploited to recognize bipolar disorders [cit], mood [cit], personality [cit] and stress [cit] . table 2 summarizes the main points emerging from the literture reviewed in this section. 3 the dark side of data-driven decision-making for social good"
"we present a novel 3-d object recognition algorithm, called potential well space embedding (pwse), which lies midway between the model-based and appearance-based approaches. on the one hand, the proposed algorithm is extremely efficient and accurate. the experimental results show that pwse executes at 122 frames-per-second (fps) on standard hardware with recognition rates exceeding 97% for the database of 60 objects shown in fig. 1 . on the other hand, pwse is also robust to data sparseness, sensor noise and outliers due to the use of novel feature vectors."
"the robustness of the error surfaces to data sparseness, noise and outliers was also quantitatively studied by computing the correlation coefficient between the error surfaces generated by the ideal data and the degraded data. the correlation coefficient between two error surfaces x and y is calculated as:"
"the view that most closely matches the current image is identified by summing the similarities of all corresponding poses in an embedding, and taking the minimum:"
"in this chapter, we focus our attention on social good algorithms, that is algorithms strongly influencing decision-making and resource optimization of public goods, such as public health, safety, access to finance and fair employment. these algorithms are of particular interest given the magnitude of their impact on quality of life and the risks associated with the information asymmetry surrounding their governance."
"both governments and companies use data-driven algorithms for decision making and optimization. thus, accountability in government and corporate use of such decision making tools is fundamental in both validating their utility toward the public interest as well as redressing harms generated by these algorithms."
"a set of experiments was conducted on both real and simulated range image data to assess the effectiveness, robustness and efficiency of the proposed algorithm. all experiments were executed on an x86_64 quad-cores processor with 8gb of memory running windows xp. in order to take advantage of the multi-core capabilities of the cpu, the code was parallelized using the open multi-processing (openmp) api [cit] . other than this, no hardware acceleration or software optimization was applied."
"to further investigate the performance of the method on a large data set, a set of experiments were conducted on the 3-d models in the princeton shape benchmark (psb) [cit] ). the psb contains 1,814 classified 3-d models divided into two sets: the training data set containing 907 models partitioned into 90 classes, and the testing data set containing 907 different models partitioned into 92 classes. the pwse algorithm was first tested on both sets separately. for each data set, a total of 18,140 test images, 20 for each of the 907 objects, were generated using the same method and the same generic model that was used in the simulated data tests."
"a superconducting magnetic energy storage system is a dc current device for storing and instantaneously discharging large quantities of power. the dc current flowing through a superconducting wire in a large magnet creates the magnetic field. the large superconducting coil is contained in a cryostat or dewar consisting of a vacuum vessel and a liquid vessel that cools the coil. a cryogenic system and the power conversion/conditioning system with control and protection functions [cit] are also used to keep the temperature well below the critical temperature of the superconductor. during smes operation, the magnet coils have to remain in the superconducting status. a refrigerator in the cryogenic system maintains the required temperature for proper superconducting operation. a bypass switch is used to reduce energy losses when the coil is on standby. and it also serves other purposes such as bypassing dc coil current if utility tie is lost, removing converter from service, or protecting the coil if cooling is lost [m. h. [cit] . figure 1 shows a basic schematic of an smes system [ http://www.doc.ic.ac.uk/~matti/ise 2grp/energystorage_report/node8.html]. utility system feeds the power to the power conditioning and switching devices that provides energy to charge the coil, thus storing energy. when a voltage sag or momentary power outage occurs, the coil discharges through switching and conditioning devices, feeding conditioned power to the load. the cryogenic (refrigeration) system and helium vessel keep the conductor cold in order to maintain the coil in the superconducting state. [cit] . however, the smes is still an expensive device."
"while this is an exciting time for researchers and practitioners in this new field of computational social sciences, we need to be aware of the risks associated with these new approaches to decision making, including violation of privacy, lack of transparency, information asymmetry, social exclusion and discrimination. we have proposed three human-centric requirements that we consider to be of paramount importance in order to enable positive disruption of data-driven policy-making: user-centric data ownership and management; algorithmic transparency and accountability; and living labs to experiment with data-driven policies in the wild. it will be only when we honor these requirements that we will be able to move from the feared tyranny of data and algorithms to a data-enabled model of democratic governance running against tyrants and autocrats, and for the people."
"we use n-gram model to represent the sentence that contains keywords. in propose of simplification, we do not consider all the n-gram items of the sentence, only the n-gram items that contain keywords are considered. the keyword, keyword's weight and n-gram items containing keyword are stored in the keyword list."
"we use polling method to look over the weight of each keyword in merged keyword list, if there is a keyword whose weight value is greater than the predefined hot threshold, the keyword will be considered as a hot keyword. all the hot keywords are stored in a hot keyword list, which has the same data structure with keyword list."
"however, the ability to accumulate and manipulate behavioral data about customers and citizens on an unprecedented scale may give big companies and intrusive/authoritarian governments powerful means to manipulate segments of the population through targeted marketing efforts and social control strategies. in particular, we might witness an information asymmetry situation where a powerful few have access and use knowledge that the majority do not have access to, thus leading to an -or exacerbating the existing-asymmetry of power between the state or the big companies on one side and the people on the other side [cit] . in addition, the nature and the use of various data-driven algorithms for social good, as well as the lack of computational or data literacy among citizens, makes algorithmic transparency difficult to generalize and accountability difficult to assess [cit] ."
"a sudden application of a load results in an instantaneous mismatch between the demand and supply of electrical power because the generating plants are unable to change the inputs to the prime movers instantaneously. the immediate energy requirement is met by the kinetic energy of the generator rotor and speed falls. so system frequency changes though it becomes normal after a short period due to automatic generation control. again, sudden load rejections give rise to similar problems. the instantaneous surplus generation created by removal of load is absorbed in the kinetic energy of the generator rotors and the frequency changes. the problem of minimizing the deviation of frequency from normal value under such circumstances is known as the load frequency control problem. to be effective in load frequency control application, the energy storage system should be fast acting i.e. the time lag in switching from receiving (charging) mode to delivering (discharging) mode should be very small. for damping the swing caused by small load perturbations the storage units for lfc application need to have only a small quantity of stored energy, though its power rating has to be high, since the stored energy has to be delivered within a short span of time. however, due to high cost of superconductor technology, one can consider the use of non-superconducting of lossy magnetic energy storage (mes) inductors for the same purpose. such systems would be economical maintenance free, long lasting and as reliable as ordinary power transformers. thus a mes system seems to be good to meet the above requirements. the power flow into an energy storage unit can be reversed, by reversing the dc voltage applied to the inductor within a few cycles. a 12-pulse bridge converter with an appropriate control of the firing angles can be adopted for the purpose. thus, these fast acting energy storage devices can be made to share the sudden load requirement with the generator rotors, by continuously controlling the power flow in or out of the inductor depending on the frequency error signals."
"the convergence of icp to the global minimum (i.e. true pose) strongly depends on the initial pose estimate. when the transform between two data sets is large, it is well-known that icp can be easily trapped by a local minimum resulting in an incorrect registration result, which is considered to be a limitation of the icp algorithm. besl suggested in his initial paper [cit] that icp can be extended to pose determination by applying icp from a set of different initial states, and then choosing the result to be that pose with the minimum error. however, it is difficult to decide the optimal set of initial states and a large number of states have to be used, which is computationally expensive. for this reason, icp has been applied only to pose refinement, where it is used to improve the registration accuracy after the rough initial alignment has been resolved."
"to further investigate the applicability of pwse to challenging tasks of object recognition, we have developed a complete real-time object recognition and tracking application using a stereovision sensor, which is able to recognize and track the 10 objects shown in figs. 2 and 3 . the real-time tracking system was implemented in c++, and range data with subpixel disparity interpolation was acquired with a point grey research bumblebee ™ stereovision camera."
"the detected hot topic is represented by the keyword group in combined keyword list, the hotness of the topic is measured by the value of keywords' weight in keyword list. we rank the detected hot topics according to their hotness. suppose the number of keywords in a keyword group is m, the hotness of the topic that the keyword group represents can be defined as"
", the result shows that, under the same experiment condition, the hkc method improves the effectiveness compared with spc, and our approach got the best performance. fig.3 gives the results of comparison of our approach and other two approaches with feature selection on testing data in tri-gram, four-gram, five-gram and sixgram models. in fig.3, we can see that our approach outperforms spc and hkc approaches in most case, and we get the best results when we use five-gram model. that is to say, five-grams can well reflect the relationship of keywords, so the keywords depicting a topic can be effectively clustered together. (d) six-grams figure. 3 select n-grams"
"in future work, we will extend the method to handle significant degrees of scene clutter, which in fact is fairly straightforward by borrowing elements of the model-based technique. rather than calculating the feature vectors for each view of objects, pwse can be employed to compute the feature vectors for a small patch (supporting region) surrounding basis points. as the embeddings are calculated only for a limited local support region, the modified pwse will be able to deal with the clutter scenes. in addition, a method to automatically generate optimal generic models for a given database will be also investigated. not only is the optimal generic model able to improve the accuracy, efficiency and robustness of proposed algorithm, but it also enables pwse to deal with a large number of objects. the problem is difficult and adaptive boosting (adaboost) algorithm [cit] seems promising for tackle the problem. an efficient variation of pwse, which specifically targets for object class recognition, will be also investigated."
"this paper presents a hot topic detection and hotness of the topic evaluation method using n-gram model, which can improve hot topic retrieval in blogs. the main contributions of the paper are as follows. first, we analyze the n-gram model. second we propose an effective way to validate the importance of words in blog article according to the features of blog article. third, we apply n-gram model to design the algorithm to detect hot topic. experiment results on chinese corpus show that the proposed method is promising. however, there are still some shortages. first, the experiment data is trend to be incomplete in real life application. second, the method to optimize the parameters is just through repeated experiments. third, user participation degree, opinion communication degree of blog article should be considered. how to improve the coverage of experiment data, find the optimization algorithm to adjust the threshold and take more features of blog article into account will be conducted in the future. the fourth anniversary of the wen chuan earthquake"
"the unprecedented stream of large-scale, human behavioral data has been described as a \"tidal wave\" of opportunities to both predict and act upon the analysis of the petabytes of digital signals and traces of human actions and interactions. with such massive streams of relevant data to mine and train algorithms with, as well as increased analytical and technical capacities, it is of no surprise that companies and public sector actors are turning to machine learning-based algorithms to tackle complex problems at the limits of human decision-making [cit] . the history of human decision-making -particularly when it comes to questions of power in resource allocation, fairness, justice, and other public goods -is wrought with innumerable examples of extreme bias, leading towards corrupt, inefficient or unjust processes and outcomes [cit] . in short, human decision-making has shown significant limitations and the turn towards data-driven algorithms reflects a search for objectivity, evidence-based decision-making, and a better understanding of our resources and behaviors. diakopoulos [cit] characterizes the function and power of algorithms in four broad categories: 1) classification, the categorization of information into separate \"classes\", based on its features; 2) prioritization, the denotation of emphasis and rank on particular information or results at the expense of others based on a pre-defined set of criteria; 3) association, the determination of correlated relationships between entities; and 4) filtering, the inclusion or exclusion of information based on pre-determined criteria. table 1 provides examples of types of algorithms across these categories. this chapter places emphasis on what we call social good algorithms -algorithms strongly influencing decision-making and resource optimization for public goods. these algorithms are designed to analyze massive amounts of human behavioral data from various sources and then, based on predetermined criteria, select the information most relevant to their intended purpose. while resource allocation and decision optimization over limited resources remain common features of the public sector, the use of social good algorithms brings to a new level the amount of human behavioral data that public sector actors can access, the capacities with which they can analyze this information and deliver results, and the communities of experts and common people who hold these results to be objective. the ability of these algorithms to identify, select and determine information of relevance beyond the scope of human decision-making creates a new kind of decision optimization faciliated by both the design of the algorithms and the data from which they are based. however, as discussed later in the chapter, this new process is often opaque and assumes a level of impartiality that is not always accurate. it also creates information asymmetry and lack of transparency between actors using these algorithms and the intended beneficiaries whose data is being used."
"if we turn to the use, governance and deployment of big data approaches in the public sector, we can draw several parallels towards what we refer to as the \"tyranny of data\", that is the adoption of data-driven decision-making under the technocratic and top-down approaches higlighted by easterly [cit] . we elaborate on the need for social good decision-making algorithms to provide transparency and accountability, to only use personal information -owned and controlled by individuals -with explicit consent, to ensure that privacy is preserved when data is analyzed in aggregated and anonymized form, and to be tested and evaluated in context, that is by means of living lab approaches involving citizens. in our view, these characteristics are crucial for fair datadriven decision-making as well as for citizen engagement and participation."
"categorization -through algorithmic classification, prioritization, association and filtering -can be considered as a form of direct discrimination, whereby algorithms are used for disparate treatment [cit] . third, algorithms can lead to discrimination as a result of the misuse of certain models in different contexts [cit] . fourth, in a form of feedback loop, biased training data can be used both as evidence for the use of algorithms and as proof of their effectiveness [cit] ."
"indeed, we find increasing evidence of detrimental impact already taking place in current non-algorithmic approaches to credit scoring and generally to backgrounds checks. the latter have been widely used in recent years in several contexts: it is common to agree to be subjected to background checks when applying for a job, to lease a new apartment, etc. in fact, hundreds of thousands of people have unknowingly seen themselves adversely affected on existential matters such as job opportunities and housing availability due to simple but common mistakes (for instance, misidentification) in the procedures used by external companies to perform background checks 12 . it is worth noticing that the trivial procedural mistakes causing such adverse outcomes are bound to disappear once fully replaced with data-driven methodologies. alas, this also means that should such methodologies not be transparent in their inner workings, the effects are likely to stay though with different roots. further, the effort required to identify the causes of unfair and discriminative outcomes can be expected to be exponentially larger, as exponentially more complex will be the black-box models employed to assist in the decision-making process. this scenario highlights particularly well the need for machine learning models featuring transparency and accountability: adopting black-box approaches in scenarios where the lives of people would be seriously affected by a machine-driven decision could lead to forms of algorithmic stigma 13, a particularly creepy scenario considering that those stigmatized might never become aware of being so, and the stigmatizer will be an unaccountable machine. recent advances in neural network-based (deep learning) models are yielding unprecedented accuracies in a variety of fields. however, such models tend to be difficult -if not impossible -to interpret, as 12 see, for instance, http://www.chicagotribune.com/business/ ct-background-check-penalties-1030 [cit] 1029-story.html 13 as a social phenomenon, the concept of stigma has received significant attention by sociologists, who under different frames highlighted and categorized the various factors leading individuals or groups to be discriminated by society, the countermoves often adopted by the stigmatized, and analyzed dynamics of reactions and evolution of stigma. we refer the interested reader to the review provided by major and o'brian [cit] ."
"one hot keyword can't depict a hot topic comprehensively, so a hot topic always has several hot keywords, all the keywords are contained in the hot keyword list. in this paper, we combine hot keywords that describe the same hot topic to form a hot keyword group. the algorithm to combine hot keywords describing a hot topic is represented as follows."
"the appearance-based approach is more efficient than the model-based approach when the objects can be segmented from the scene image. an object is first encoded with a set of images collected from different vantages in an offline training phase. in online recognition, the objects and their poses can be retrieved by searching for the best match between the input image and the database of stored images. as it is expensive to operate directly on the image data, these images are first transformed into a lower dimensional space, e.g. by using linear [cit] or non-linear projection methods [cit], so that the comparison can be executed more efficiently within this lower dimensional space. efficiency is an attractive characteristic of the appearancebased approach, which makes it very popular in 2-d object recognition [cit], and some researchers have extended it to handle 3-d range data [cit] . one challenge of the appearance-based approach is that it is sensitive to dropouts, sensor noise and outliers. in addition, it requires that the training and scene images be aligned in a very similar manner."
"the final pose estimate can be then calculated as: a block diagram of the complete algorithm is illustrated in fig. 7 . we have found the method to be both efficient and effective. in previous work, the experimental results show that an object's pose can be recovered with an accuracy of over 98% at over 8 frames per second (fps) on standard hardware by using a common implementation of icp. the runtime efficiency of the process has been improved to over 122 fps on the same hardware by using an efficient icp variant that uses a euclidean distance map (i.e. a voxel map) to accelerate the ratedetermining nearest neighbor computation [cit] . the performance of the method is examined thoroughly in sect. 3."
"fortunately, there is increasing awareness of the importance of reducing or eliminating the opacity of data-driven algorithmic decision-making systems. there are a number of research efforts and initiatives in this direction, including the data transparency lab 8 which is a \"community of technolo-gists, researchers, policymakers and industry representatives working to advance online personal data transparency through research and design\", and the darpa explainable artificial intelligence (xai) project 9 . [cit] acm knowledge and data discovery conference [cit] . researchers from new york university's information law institute, such as helen nissenbaum and solon barocas, and microsoft research, such as kate crawford and tarleton gillespie, have held several workshops and conferences during the past few years on the ethical and legal challenges related to algorithmic governance and decision-making. 10 a nominee for the national book award, cathy o'neil's book, \"weapons of math destruction,\" details several case studies on harms and risks to public accountability associated with big data-driven algorithmic decision-making, particularly in the areas of criminal justice and education [cit] . recently, in partnership with microsoft research and others, the white house office of science and technology policy has co-hosted several public symposiums on the impacts and challenges of algorithms and artificial intelligence, specifically in social inequality, labor, healthcare and ethics."
"a blog article consists of three parts: the title, the content and the reply. for a sentence in blog article, first, we define a stopping-word list to remove the words that are irrelevant to the theme of the blog article. then we do word segmentation and part-of-speech tagging. finally, this sentence is represented by n-gram model. an example of five-gram model is given in fig.1 . every five-gram item has five entries, all of which in an item have some co-occurrence information. every sentence has several five-gram items."
"the execution phase started, when the subjects indicated that they were ready. during this phase they had to perform the trained motion 10 times. they could also decide the start of each of the 10 repetitions by pressing a button on a controller they held in their other hand. they were asked to wait around one or two seconds between each repetition, so that the eeg and emg windows would be clear."
"it is important to note that the training for both nf and av conditions were the same; the motions were presented on a screen as explained before. in the case of the vr condition, the training was performed while using the vr headset (i.e. the motions were presented directly in the vr headset)."
"where, position(t) is the hand's 3d-coordinates at time t, lr is a multiple linear regression (one for each dimension), nn is a neural network, and '+' stands for concatenation."
"when comparing the three blocks, there does not seem to be much difference. based on the cv, the best approach would be vrt, but the difference is not significant."
"regarding the topological distributions of the most important electrodes, they seem to be very similar among the three conditions. the central and centro-parietal electrodes of the contralateral area are among the most important electrodes as is normal in similar studies [cit] . interestingly, the electrodes from the centro-frontal area in the ipsilateral area seem to be also included among the most important. initially, this could be interpreted as an artifact coming from the arm motion itself, but that effect would be then also in the frontal electrodes, or those closer to the arm. it is also important to note that these topographic views do not represent the brain dynamics, but only which parts of the eeg are more useful for the system to reconstruct the position of the hand. these concepts may be related, but are not the same."
"in this study, we showed that there is little difference in terms of cv, when using different feedback methods during the training. even though there are differences in the brain activity, the system seems to be able to adapt itself to each individual and condition."
"we take a further step to observe the energy consumption with respect to content popularity. we assume the average data object size as 5 and plot the energy consumption as a function of content popularity index in figure 9 . as shown in the figure, the energy consumption increases with the decreasing of popularity of data objects. obviously, this is because the unpopular data objects need larger hop distance for transmission, thus leading to larger amount of energy consumption."
"furthermore, in our previous study [cit] we obtained results that suggested that using the previous reconstructed points as input for the predictor could improve the cv. thus, we also calculated the cv for a predictor using two previous reconstructed points. this predictor could be described as:"
"we next plot figure 8 to show the energy consumption as a function of interest request rate. interestingly, we observe that, with the increasing of interest request rate, the energy consumption decreases. this is because with the changes of interest request rate, the transmission energy per bit remains the same. but as observed from (14), the caching energy consumption is an inverse function of interest request rate; the caching energy decreases. thereby, the energy consumption decreases with the increasing of interest request rate. the results in figure 8 indicate that we can adjust interest request rate for energy efficient cooperative design."
"the task during the three conditions was performed while standing up, at approximately 1 m from the screen (to have enough space to move their arm freely). the screen was placed on an approximately 80 cm tall table."
"in neuroscience, motion reconstruction refers to predicting the position of a limb by using biosignals, such as electromyography (emg), or electroencephalography (eeg). it is mainly used for prosthetic control, but can be applied to controlling other robotic devices, or even video games. still, there are several different problems, depending on the target extremity and the used measurement methods, that need to be solved to make the motion reconstruction systems reliable."
"in the data mining phase, we obtained two dimensions of the perceived gastronomic image: space (table 3 ) and time ( figure 3 ). the other dimensions were obtained through quantitative and thematic content analysis of otrs, based on the frequency of keywords (table 4), categorisation, and tripadvisor ratings. table 4 shows keywords that appeared most frequently in restaurant otrs. this table is significant for the affective dimension of the image, given the many adjectives that qualify feelings and moods (e.g., amazing, excellent, delicious). other common words that appear in restaurant reviews contain no meaning (neither positive nor negative), are neutral, and simply configure the cognitive component, such as \"food\", \"restaurant\", \"staff\", or \"place\", among others. in general, according to the words listed above, customers are satisfied. furthermore, based on the most-frequent words, it seems that menus offered include steak, chicken, fish, wine and drinks, and tapas. table 3 shows the spatial distribution of the otrs at a local level, which are mainly concentrated on two islands (tenerife and lanzarote). table 3 and figure 3 [cit] . this trend coincides with that observed in a previous work on another mature destination, attica, greece [cit] . figure 3 also shows a low seasonality. this may be because the archipelago enjoys a temperate climate throughout the year from the action of the trade winds."
"electroencephalography (eeg) provides a noninvasive method for measuring the activity of the brain, related to the internal body image of the subject, even though an actual limb is not present. thus, measuring eeg activity can complement emg by adding information on the motion intensions of the subject. however, predicting even general movements instead of specific positions has shown only average results [cit], and there is ample room for development."
"this study aimed to assess the gastronomic image identity related to the construction of the tdi and improving the strategic positioning of the destination. in this sense, user-generated otrs show not only the image and impressions that tourists perceived in their experience but also the image and identity that will be transmitted to other users. the archipelago offers diverse cuisine, where oriental restaurants, for example, have better ratings than spanish local restaurants. these results indicate new trends and preferences at the cultural gastronomic level, as the greatest popularity and appreciation of international cuisine compared to local cuisine, which may have negative effects in socio-cultural sustainability of local communities. given the analysis of the most frequent affective keywords, one can conclude that tourists of the canary islands conveyed a positive image of the destination and had a positive gastronomic experience. the fact that opinions on restaurants include words with positive emotional content can also relate to the high valuations (more than 83% of the opinions about restaurants have scores of excellent and very good). therefore, the importance of gastronomy as an attraction factor is highlighted."
"in addition to the overall contribution of the eeg, we were interested in studying the impact of eeg information on the reconstruction of each one of the six motions. there are some motions (like motion b) that due to their simplicity, and the muscles involved in their execution, would be easy to reconstruct just by using the emg in the shoulder area. while motions like motion c that have very little emg activity around the shoulder are harder to reconstruct using only the emg in the shoulder area. thus, we explored how the eeg data contributed to the final reconstruction, motion by motion. for this, we used mean square error (mse) instead of the cv, because the number of points for each motion was low (∼40), and in such cases the cv is not stable. still, the mse is directly proportional to the square of the cv."
"mapping. the system model for content centric network with identifier mapping system is shown in figure 1 . the identifier denotes the name object identifier and the router name related identifier. the identifier mapping system is responsible for the mappings between these two identifiers. the reason we consider content centric sensor network with identifier mapping system is because such system can reduce the size of routing table by using router related name for packet forwarding [cit] . this is suitable for sensor networks due to the limited storing space in sensor node. we consider two typical types of topology in this paper: linear topology and tree topology. two kinds of packet exist in the above system, that is, interest packet and data packet. interest packet mainly contains the name of data object and the description of the name, such as the nonce which is generated by each interest for avoiding interest forwarding loop and the type of service of the requested content. the data packet contains the name of the data object and the carried data."
"the first result that we can obtain from table 1 and fig. 5, is that the temporal architecture is significantly better than the normal architecture. still, this difference is statistically different only for the vr condition. this difference can be further seen in the video provided in the supplementary material (produced using the ''final ik'' unity asset from partel lang). the movement is not only more precise, as is suggested by the cvs, but the motion seems also more natural. in the case of the normal architecture, the amplitudes of the motions are closer to the real ones."
"referring to the management implications, these results may lead to the conclusion that the canary islands as a tourist destination \"is doing it well\" in satisfying gastronomic tourists, but \"is not doing it well\" in terms of being a community which offers food that is representative of the local sociocultural identity to visitors. furthermore, results are in line with [cit], who found a notable lack of awareness of regional culinary products among tourists in another spanish region. in this respect, some kind of strategy should be implemented at the cultural level to improve the values of the local cuisine and create signs of cultural identity based on gastronomy. the development of the local cuisine based on local products as a tourist resource helps the sustainability and identity of the territory, increasing the level of local production and distribution and ensuring respect for tradition and heritage. if tourism sustainable development is attached to the enhancement of local culture and identity, the gastronomic image of the destination should certainly be improved in this respect. moreover, otrs generated by users represent a source of valuable information for both restaurants and dmos. however, the positive effect of ewom on gastronomy from otrs shows restaurateurs and managers of tourist destinations that adequate information is provided about restaurants. using social media to further increase the number of ratings and reviews is an incentive for the tourists to get involved, get better positioning, and improve the image of the destination. therefore, it is advised that focus be put on the web to further promote gastronomy and restaurants, featuring the experiential component involved and highlighting the local cuisine as a friendly environmental product or health and proximity food."
"to analyse the influence that the opinions of eating establishment customers have on the image of spanish tourist destinations, the canary islands (the region with the most tourist influx in spain) were chosen as the tourist destination for this case study. the method consisted of selecting the most suitable travel-related websites, downloading the otrs, processing the data, and carrying out the content analysis based on the categorisation from the most frequent keyword terms matrix [cit] ."
"it should also be noted that the conceptual framework and proposed method allowed us to analyse the gastronomic image identity perceived by tourists transmitted through otrs to other users, as well as their cognitive and appraisive aspects, especially highlighting the evaluative and affective character of the dining experience. the proposed metrics allow us to compare two or more destinations or types of food delimited in space and time, which may be of use for local communities to know where they are in terms of local gastronomy projection and direct their strategies. the big data (hundreds of thousands of opinions) neutralise the subjectivity of the perceived image and allow for deducing the gastronomic image as a whole. in terms of statistical analyses, relations between the number of reviews per score, percentage of reviews out of total restaurant reviews per score, and per restaurant specialisation (restaurant country and region classifications) are found and validated."
"each router in the system maintains three data structures: forwarding information base (fib), content store (cs), and pending interest table (pit). fib maintains the outgoing interfaces toward each rid related name and is used for forwarding interest toward the sources of matching data. in each entry in fib, a list of interfaces are maintained, each can be associated with the status information (red/yellow/green), routing cost, and round trip time (rtt) measured by the forwarding plane. based on the stateful information, different forwarding strategies can be used [cit] . in this paper, we consider the best route approach, in which the highest-ranking green interface is used for transmitting interest. if no green interfaces exist, interest is forwarded to the highest-ranking yellow interface. the interface in the fib is ranked by the routing cost for arriving at each rid."
"one of the most important changes that should be done in the field, is the standardization of the fitness function. right now the cv is used in most studies, but this feature provides very little information about how well the system will work in real life applications. in our opinion, after training a motion reconstruction system, the subject should try to use it in either a virtual world or with a prosthetic device, doing a predefined set of motions. these motions should be similar to the ones that are most likely needed in a real-life application, such as reaching motions to different positions. thus, our future studies will focus on this aspect. in addition, amputee patients should be used as well to test the effects of a missing limb, especially when emg is used."
"where contribution is the eeg contribution, emg is the reconstruction of the emg component, and ''second layer'' is the reconstruction of the second layer component."
"as expected, spanish, european and mediterranean gastronomy restaurants are the most numerous (table 7) . however, only two of the host countries and regions are included in the list of the 20 most popular establishments by number of otrs (table 6 ). although spain enjoys a great reputation in the field of gastronomy [cit], because it has great chefs [cit] and high-quality restaurants [cit], the local spanish cuisine does not stand out in the list. it is also unusual to observe that fast-food restaurants, which do not reflect the local community's identity, are significant in the standings, despite the pejorative connotation of such establishments [cit] . behind these numbers, it can be seen that the canary islands as a destination has focused on sun, sand, and nature leaving aside gastronomy. this focus results in tourists looking for any kind of restaurant and food but not local, mainly due to the ignorance of what is traditional in the destination. note: a restaurant can be classified by different concepts."
"in their study, they demonstrated that there is no difference between predicting the position or the velocity in terms of cv. the study obtained a mean cv of 0.7 for five subjects."
"to analyse the spatial dimension of the image at a local community level, the archipelago was divided into eight islands (table 3), to which an abbreviation was assigned: tenerife (ten), gran canaria (grc), lanzarote (lan), fuerteventura (fue), la palma (lap), la gomera (gom), el hierro (hie), and la graciosa (gra). the time dimension uses the date of the review as a reference (figure 3 ) because, given the wide availability of smart mobile devices, reviews are posted online almost instantly or shortly thereafter."
"in prosthetics, the main interface for reconstructing the wanted motion, or detecting the motion intention of the amputee, is usually emg, which is used for acquiring muscle electric signals from the remaining muscles of the amputated limb. foot prostheses have achieved high accuracy [cit] due to their low number of degrees of freedom (dof) as well as for the well-studied dynamics of the lower limbs. although hand prostheses have a much higher number of dof, their motion reconstruction has also achieved high accuracy, even for individual fingers [cit] ."
"the complete results from temporal and spatial errors are presented in fig. 8 . regarding the spatial error (fig. 8 left), at 0.15 noise the cv loss was 0.108 for the nf condition regarding the temporal error (fig. 8 right), the maximum and minimum of the variation were 0.005 at −6 shifts and −0.009 at +8 shifts for the nf condition, 0.001 at −1 shift and −0.012 at +8 shifts for the av condition, and 0.001 at +1 shifts and −0.040 at +8 shifts for the vr condition."
"customers are increasingly using the internet as a source of information on tourism products and services, such as hotel reservations, car rentals, restaurants, flights, and tourist packages [cit] . in fact, according to a survey at the european level [cit], the internet was the most common way to organize a trip (two out of three people use it for this purpose), and the recommendations of websites that collect otrs were the second most important source of information after recommendations from family and friends."
"in relation to the sentiment analysis, among the 11 most-frequent keywords listed in table 4, six indicate a positive polarity (good, great, friendly, lovely, excellent, and nice). in the half-million otrs analysed, more than two million keywords denote positive moods and feelings (table 10), which represent more than 5% of the total words (including stop words), and the more than 150,000 keywords that express positive recommendations. more than 83% of the ratings qualify restaurants with an excellent or very good score. the average score is higher than 81%. the negative polarity of sentiments, recommendations, and ratings only represents about 10% of positive polarity. again, tourists consuming food in canary islands' restaurants are satisfied, even though they ignore, or do not prioritize, local food consumption."
"as subjective analysis, we observed that during the experiment the subjects' motion was most similar to the real motion in the av block, and the most different in the nf block. this was expected since av was the only block in which they could see both their own arm and the virtual representation. in the case of the nf block, we saw that most of the times the subjects reduced the speed considerably comparing to the real motion. however, the system was able to adapt to such delay. after the experiment was done, we asked the subjects which of the three methods they preferred. in every case the vr was the selected option. probably this is due to the novelty of using vr. on the other hand, the worst approach based on the comments was the nf one. the main complaint was that they were not sure if they were doing the motion correctly or not (there was no feedback whatsoever from us during the experiment)."
"to generate the frequency tables, the algorithm described in marine-roig [cit] implemented in java was used. the algorithm needs a list of composite keywords (groups of two or more words that have a different meaning for each word) and another list of non-significant words (stop words) for the case study (determiners, pronouns, prepositions, conjunctions, and adverbs), in both spanish and english languages. in addition, you must define the word separator characters (in this case, considered separator characters are not letters in spanish or english). in case of overlap, the algorithm prioritizes composite keywords; for example, \"not good\" (two words) takes precedence over \"good\" (single keyword) and \"not\" (stop word). when two composite keywords overlap, the first that appears in the list has priority. once a frequency table was generated, the keywords were classified according to the categories described in the previous section."
"the processing of data packet is further illustrated in figure 3 . when a data packet is received, the following steps are needed to process the data packet. new entry created in pit and interest is sent to the destination using rids (i) first of all, the node will check whether a pit exists."
"data mining is the process of extracting information from a large dataset and structures it to facilitate the discovery of patterns. web pages downloaded from tripadvisor in plain text (no multimedia files or other attachments) were between 400 and 500 kb in size. the information useful for the case study was less than 5% of the content of the page [cit] . useful data was extracted using a text search utility that supported expressions (regex) of regular language (search patterns). the paratextual elements [cit] and html (hypertext markup language) metadata of otr web pages [cit] provided the data necessary to measure the perceived image, placing it in space and time (title, language, date, location, type or theme, rating, etc.)."
"before the experiment the subjects were asked about their previous experience with vr. 16 responded ''none'', 5 responded ''little'' (one or two times), 2 responded ''more''. 1 used vr regularly."
"for the post hoc analysis we performed 12 different t-test: av vs. avt, vr vs. vrt, nft vs. avt, nf eeg vs. nf, av eeg vs. av, vr eeg vs. vr, nf emg vs. nf, av emg vs. av, vr emg vs. vr, and the three possible combinations from the three eeg blocks. to the p-value obtained through this method we applied the bonferroni correction (i.e multiplying it by the number of tests performed). we decided not to compare all the possible combinations since we considered it to be irrelevant to compare for example nf vs. avt. table 1 presents the cv results for 24 subjects. the first six columns correspond to the results of the two architectures for each block. the next six columns are the cvs for the eeg and emg components for each block. note that as the results for the components are independent from the used architecture, there is only one result for each block."
"the naming in the above system is hierarchical and unique, which is quite similar with the uniform resource identifier (uri). it consists of a list of characters with different lengths to identify data objects. for example, as shown in figure 1, the original interest has a name \"/status/room1.\" in addition, each node has a unique name for marking the identity, which is shown as \"/rid1\" in figure 1 . the mapping relationships between the name of data objects and the identity of routers are maintained in the identifier mapping system. when an original interest arrives at the network, the interest is encapsulated with rid related names and then transmitted within the network."
"the energy consumption with respect to caching probability is illustrated in figure 7 . the index of popularity is assumed to be 5 in this case. it can be observed that the energy consumption decreases with the increasing of for small value of and then increases for larger value of . this can be explained as follows. when caching probability is small, the caching energy is small, but the data objects are obtained with larger hops, leading to higher value of transmission energy. thereby, within this range, the energy consumption is mainly determined by the transmission energy. on the other hand, if the caching probability is large, the transmission energy becomes small. and the total energy consumption is mainly determined by caching energy. the results in figure 7 indicate that by tuning the caching probability, the total energy consumption can be optimized."
"to further analyze (6), we further discuss the caching strategy in this section. we first consider the basic lru scheme as the caching strategy, in which the node caches every data object and replaces the least used data objects. when taking the energy consumption into consideration, if the data objects are cached at every node passing by, the caching energy may be increased. thereby we propose a revised version of lru scheme, which is named -lru scheme, in which every node caches data objects with a probability of . in this case, when data objects are received by a node, they are processed as in figure 4 ."
"pit is used to keep record of the incoming interfaces of the interest in order to send back the data packets to the request node. usually, pit is a table recording the interest name, the incoming interfaces, and a number of nonce which are associated with a particular interest name. in addition, pit also records the outgoing interfaces which contain the interest forwarding time via the interface which is used for calculating the rtt. an interest is received by the node, the following steps are undertaken."
"the content analysis was based on techniques for mapping symbolic data in a matrix of data suitable for statistical analysis [cit] . a common technique is quantitative analysis for the frequency of words and their categorization. it is assumed that the words most frequently mentioned are those that reflect the greatest interest [cit] . categories are groups of words with meaning and/or a similar connotation. the categories should be mutually exclusive and exhaustive for a rigorous analysis. to analyse the image perceived by customers of dining establishments, cognitive and appraisive components of the overall image (figure 1) were considered."
"many authors argue that the ugc is highly influential and credible [cit] and can have a significant effect on the behaviour, decisions, expectations, and perceptions of tourists. the influence of ugc and social media is growing every day due to the expansion and increasing use of reviews, their acceptance, trustworthiness, capacity to meet tourists' needs and interests, and the many advantages they present for users [cit] . the intentionality is different behind user-generated images"
"in which and are the weights for transmission energy and caching energy. the transmission energy is closely related to the average number of nodes with which the content can be obtained. we use to denote the energy consumption for a node per bit and as the power consumption for a link per bit, and then the average total transmission power required per bit can be derived as"
"in the case of the different components, it is interesting to note that the emg component is the same for the three blocks, while the eeg component is better in the vr block, when compared to the av and nf blocks. regarding the emg, it is coherent with the experimental design. the motion is more or less the same in the three blocks, even if the precision is not the same for all of them. as mentioned before, the emg activity is similar. regarding the eeg, it is interesting that some of the blocks are better than the others. we think that this is because the mental state in each of the three conditions is different. in the case of the nf, we believe that the mental state is focused on the motion, but part of the brain has to focus on remembering the motion to do it as precisely as possible. in the case of the av, the subject is not performing the motion itself, but is actually trying to copy the avatar. so, the subject is focusing his/her attention on copying the avatar more than doing the motion itself. in addition, the subject has to focus the attention into two elements, their own arm and the avatar. finally, in the vr block the subject is completely focused on the motion. different from the av block, there is only one element to pay attention to, the virtual arm. also, in the vr block there is a perfect feedback, because the subject is presented with perfect motion. we think that this may be the reason for the differences in the eeg results. still, further analysis is needed to exclude possible artifacts coming from the mechanical movement of the vr headset over the eeg electrodes. in our preliminary analysis we did not find any clear artifacts (the eeg signal was heavily filtered to the band 0.1-2 hz), but this should be investigated further."
"as for the output, the virtual world gave the 3d coordinates of the avatar's hand at a variable rate of approximately 60 hz. for each input window, we used the mean position as the final position for each dimension. similarly to the emg, the hand's position was calculated in a window of 0.0625 s (1/16 s). this means that every eeg and emg window input had an independent output associated to them."
"international journal of distributed sensor networks in which n is the total number of bits transmitted within and r is the average transmission rate. dividing both sides of the above equation by n, we have the caching power per bit expressed as"
"customers are increasingly using the internet as a source of information on tourism products and services, such as hotel reservations, car rentals, restaurants, flights, and tourist packages [cit] . in fact, according to a survey at the european level [cit], the internet was the most common way to organize a trip (two out of three people use it for this purpose), and the recommendations of websites that collect otrs were the second most important source of information after recommendations from family and friends."
"1) creating a platform that can be used for training transhumeral amputees for their rehabilitation, and 2) discovering whether different feedback methods result in different accuracies of the system. for doing so, we compared the correlation values (cv) obtained with the different modalities. in addition, we calculated the contributions of each component to the final reconstruction. we studied specifically the usefulness of eeg for reconstructing each one of the used training motions. also, we analyzed the results of using a virtual avatar instead of a motion tracking system by comparing our results with other studies. finally, we considered the advantages and disadvantages of using each of the training modalities."
"tripadvisor appears in the first position as the most suitable website for the case study. this selection matches that of most authors who investigate hospitality and tourism through online reviews [cit] . tripadvisor has basically three sections that host many otrs: things to do, hotels, and restaurants. in these three sections, there may be comments or opinions about food and wine, but the vast majority of opinions and ratings related to gastronomy are concentrated in the restaurants section. as seen in table 1, the majority of visitors to the canary islands eat in restaurants outside hotels, and thus, the restaurant section of tripadvisor can be a good data source to analyse the gastronomic image online. moreover, this gastronomic image online has the value that if people search for opinions on the destination's gastronomy, they will encounter these restaurant experiences through search engines [cit] . tripadvisor has more than three million opinions on the canary islands, of which over half a million are written in english and refer to restaurants. using a web copying programme with appropriate filters [cit], we proceeded to download the otrs hosted in the restaurants section of tripadvisor [cit] ."
"in addition, we assume that under steady state, the average received data packets are equal to the number of request interest. and the average transmission rate r can be accordingly approximated as"
"the nf predictor was the best for 0 subjects, the av for 2, the vr for 3, the nft for 6, the avt for 3, and the vrt for 10. it is important to note that the results from the second layer of the predictors (three first columns of the table) were always better than the individual cv from the eeg. also, they were worse than the emg only for subject 18 in the nf condition, subjects 21, 22, and 23 in the av condition, and subjects 6, 8, 22, and 24 in the vr condition."
"future research should be deeper and go beyond statistical analysis to analyse the significant relationships between types of food, islands in the archipelago, ratings, and frequent words. a more thorough comparison between island communities should be made, considering the number of tourists and restaurants. this method should be applied to other tourist destinations as well, and in particular, other spanish regions. part of this analysis could be done to compare tripadvisor with other restaurant review platforms, and compositional analysis could allow, for example, comparisons of (graphically and statistically) type shared by type of food in different destinations. future studies should also be undertaken to understand why the rich spanish and regional gastronomic cultural background is not the most prominent, popular, and best valued among tourists' opinions and comments, and to see how local destinations can advance to ensure gastronomic cultural sustainability."
"(iv) if there is not an existing pit entry, the node will further check if there is a match fib entry with the rid. if there exists an fib entry, the interest will be sending out through the recorded outgoing interfaces using rids and new pit entry is created."
"for calculating the erds, the mean over all subjects and valid movements was taken. due to the exclusion criteria explained in materials and methods, different number of samples was used for each condition and electrode. in table 2, the number of samples for cp2, fcz (used in fig. 9 ), and fpz along with the percentage is presented. due to the small number of repetitions for fpz, it was removed from the analysis presented in fig. 10 . fig. 11 shows the results of the channel optimization process. there were two different analyses. first, the left panel of fig. 11 presents the evolution of the cv when electrodes were removed sequentially. the maximum for the nf condition was with 11 electrodes left, for the av condition it was with 11 electrodes remaining, and for the vr condition it was with 20 electrodes remaining. next, right side of fig. 11 presents the mean rank at which each electrode was removed in a topographic way for each condition."
"while emg and eeg may provide methods for acquiring necessary data for reconstruction, there is also a need for an interface for the patient to train the necessary motions for controlling a prosthesis. for this, virtual reality (vr) may provide several advantages, although its benefits for rehabilitation are not completely clear yet [cit] . for instance, there are studies that show that the performance obtained when vr was used was higher than with a simple 2d feedback display [cit] ."
"in fig. 11 we can see that the cv evolution for the three conditions have a similar shape: a slow increase when electrodes with little motion-related information are removed, followed by a very quick decrease when the electrodes with meaningful information are removed. this process is very easy to implement and relatively quick to process, but provides a significant boost to the cv. also, the fact that the optimal number of the electrodes for the nf and av is lower than for the vr condition, suggests that the information in the two first conditions is more localized on fewer electrodes, while the information of the vr condition is more spread across all the electrodes."
"for each subject, the data from the execution phase of each block (nf, av, and vr) was divided into 10 sets, each containing one trial for each of the movements, i.e. each set had six trials. from those sets, one was selected iteratively for being the validation set (6 trials), leaving other nine for training (54 trials). one extra set from the training group was chosen randomly to be the test set for training the neural network of the emg component. after training the predictor, the cv between the reconstructed and the position of the avatar's hand for the validation data was calculated for three components (eeg, emg, and whole system) for each dimension. this process was performed 10 times, each time using a different validation set. finally, the mean value of the repetitions was calculated for each dimension."
"where x(t), y(t), and z(t) are, respectively, the horizontal, vertical, and depth position of the avatar's hand at time t, n is the number of eeg sensors (16 during the online analysis), k is the number of features of the eeg for each channel (11 during the online analysis), s nt (k) is the kth normalized eeg feature at time t and electrode n, and the a and b variables are weights obtained through multiple linear regression."
"many authors argue that the ugc is highly influential and credible [cit] and can have a significant effect on the behaviour, decisions, expectations, and perceptions of tourists. the influence of ugc and social media is growing every day due to the expansion and increasing use of reviews, their acceptance, trustworthiness, capacity to meet tourists' needs and interests, and the many advantages they present for users [cit] . the intentionality is different behind user-generated images online (e.g., social relationship, self-expression, social recognition, leisure) and the other agents' (among them, restaurants) images that render user-generated images credible in the eyes of other users. the tourist's image construct of projected vs. perceived outline must be rethought and widened to include and emphasize the concept of the image transmitted by tourists (which is, at the same time, a perceived-projected image of tourists) [cit] . these studies have aroused great interest from companies and tourism organizations, as well as researchers [cit] ."
"in this paper, we have developed a comprehensive theoretic framework for analyzing the energy consumption in multimedia sensor network. the energy consumption model has been derived theoretically with close form expression. based on the theoretic model, we propose cooperative energy efficient management design considering the factors of node cache size, caching probability, and interest request rate. we perform extensive numerical results for energy consumption with the impact of various factors. the results have shown that the cache size, caching probability, interest request rate, content popularity, and network size should be designed cooperatively and dynamically since they have different impacts on energy consumption. in future, we aim to solve the cooperative energy efficient design for information centric sensor networks considering different types of sensor network topologies. : the probability of whether the content can be stored : the probability of obtaining content from the th level : cache miss probability : number of content items per class : average size of data object (chunks) : interest request rate y: classes of popularity : zipf 's popularity distribution."
"these advantages may improve the system's performance, and provides a more interesting platform for rehabilitation, since it will be new for most subjects [cit] . finally, for using the proposed method, the only additional device required is a vr headset, which costs less than $1000. as a comparison, using a robotic device as surrogate might cost several times the amount and would require complex calibrations and maintenance."
"we can observe that the energy consumption is determined by multiple variables. and we have listed three major factors in figure 5, which are the cache size c, caching probability, and request rate . by tuning these factors, the energy consumption can be dynamically changed. for example, increasing the caching size will lead to more caching energy but less transmission energy. similarly, increasing the caching probability may result in reducing transmission energy consumption but increasing caching energy. thereby, we need to design energy efficient scheme cooperatively with the consideration of various factors."
"even if the emg around the shoulder may carry information about the motion of the shoulder or the arm, it has very little information about the fingers or the wrist motions. thus, there is a need for complementing the information acquired from emg, to make the reconstruction of the distal parts of the upper limb possible."
"to identify which component of the classifier was more important (eeg, emg, or temporal information), we estimated the contribution of each component to the final cv. the eeg and emg contribution were calculated as each component's cv divided by the sum of both cvs, and then scaled by the cv obtained at the second layer. the temporal component's contribution was calculated by subtracting the cv of the normal architecture from the temporal architecture's cv. this component represented the additional cv added by the temporal information. finally, there was an extra element which was the second layer of the classifier. it was, as described before, a linear regression that predicted the final position by using the output of the eeg's and emg's independent predictors. by subtracting the maximum cv obtained either in the eeg or emg component from the cv of the normal architecture, we obtain the contribution of the second layer. this analysis was performed for all the three blocks."
"the gastronomic tourist (who gives value to gastronomy as a means of socialization and exchange of experiences) is increasingly demanding and has a higher level of spending than the average tourist [cit] . restaurants attracting tourists have become the target of many destinations as they help extend the length of stay, increase tourism spending on local products, promoting the proximity production, the small-scale agriculture, and ultimately create greater sustainability [cit] . this is why many destinations are increasingly positioned as gastronomic [cit] . however, previous studies"
"in today's global, competitive, and dynamic environment, many destinations have similar characteristics and therefore have a growing need to differentiate [cit] . this means that, in many cases, gastronomy has become a differentiator, helping promote the image, the host community's culture, and identity of destinations [cit] . culinary or gastronomic tourism is related to food and eating experiences of travellers. several authors have analysed the relationship between food and tourism from the perspective of food as part of a reflection of the local culture [cit] . culinary tourism includes not only food tasting and the enjoyment of the dining experience but also has an important educational role on the destination's culture and a great potential to contribute to sustainability [cit] . food is culture within the cultural act of tourism [cit] . according to the international culinary tourism association (icta), there is a strong positive correlation between tourists who are interested in cuisine and those who show an interest in cultural attractions and sustainable social network development, generating positive socioeconomic and environmental impacts."
"once all the otrs were written in english (619,149) [cit] (539,124) were selected. subsequently, defective otrs were eliminated (e.g., untitled or titled reviews composed of figures and other non-alphabetic characters). in order to provide readers with an easily visible number, a set of 500,000 otrs was selected by generating for each otr a random number between zero and one, with 15 decimal places. once the dataset was ordered by that numbering, the otrs after position 500,000 were removed. this random sample of 500,000 [cit] ( table 3 and figure 3 ) represents the vast majority of opinions regarding the case study. the english language was selected because it is the most representative of tourists who write reviews about attractions, activities, or services in the canary islands. analysis) and lanzarote (31.33%). data presented in figure 3 go beyond and include temporality. in general, the first and third quarters collect a higher number of reviews."
"the challenge for analyzing (7) is to find the solution of [ ]; that is, the expected number of hops the content has been delivered."
"since multimedia is the major content to be disseminated in multimedia sensor network, a major challenge is the demand for large bandwidth and quality of service (qos) provisioning. to improve content delivery efficiency and qos, information centric networking (icn) has recently been proposed. and several projects are investigating various options for icn architectures, such as data oriented network architecture (dona) [cit], triad [cit], named data networking [cit], and pursuit [cit] . generally in icn, content objects are decoupled from end host locations and contents are accessed by their names [cit] . through network caches, the contents are delivered to end users with much shorter routing hops and less delay [cit] ."
"brain-computer interfaces (bci) provide a way of getting more information on the amputee's intentions, and improving the reconstruction accuracy [cit] . the most precise control can be achieved by using neural implants [cit], which can also be used for restoring sensory function [cit] . however, this requires brain surgery, and thus higher level of safety precautions, due to the possibility of infections and brain damage. consequently, these kinds of approaches have only been used experimentally with animals, or in extreme cases of paralysis. thus, noninvasive technologies should be developed in order to promote the use of shoulder prostheses, by making them available to a wider group of patients."
"(ii) if the interest is not matched, the node will further check whether the received interest is an encapsulated interest with the rid. if not, the node will inquire for the corresponding rid from local cache or the identifier mapping system. if such mapping information does not exist, the processing of interest ends here. if the mapping information is found, the original interest will be encapsulated with the rid."
"we changed the eeg classifier from the neural network that we used in our previous study to the current linear regression, because we found that this approach had significantly better performance. it was also easier to interpret the results of the linear regression and its variables."
"online reviews have become an important source of information for tourism businesses looking to improve their marketing strategies and the satisfaction of customers [cit] . given the large number of otrs, methods for processing big data, which allow extracting useful information from user reviews, are necessary [cit] ."
"consumption. finally, the energy consumption as a function of network topology level is shown in figure 10 . we assume that the popularity index is 5. it is shown that the energy consumption increases almost linearly with the increasing of network topology level. the results in this figure display the trend of energy consumption with respect to network topology and inspire us with the methodology to design network topology with the consideration of energy consumption."
"cs is used to temporally store the data objects collected by sensor nodes and received by other nodes. in this paper, we mainly consider the data objects received by other nodes. the content can be temporarily stored in the cs and updated based on different content replacement strategies. some typical caching algorithms include least recently used (lru), least frequently used (lfu), and random replacement (rr). particularly, lru denotes the case where the least recently used data objects in the cache that are replaced. lfu represents the least frequently used data objects in the cache are replaced. and rr means to randomly replace the data objects in the cache. and we consider the lru approach in this paper."
"in total, we analyzed six different predictors; the three different blocks (nf, av, and vr) for two architectures (normal and temporal). to the normal predictors we will refer to simply as nf, av, and vr. to the temporal ones, we will refer to as nft, avt, and vrt. to compare these predictors, we used a two-way anova. the two categorical variables were the feedback method (nf, av, or vr) and presence or absence of temporal data. in addition, we performed two one-way anovas to compare the results from the eeg component and emg component during the three different blocks."
"as shown in table a1, between 45% and 61% (mean) of total restaurant reviews are ranked as 5*. restaurants in lanzarote and fuerteventura have the highest percentage of reviews ranked 5*, and restaurants located in la palma have the lowest percentage of 5* reviews (44.74%). restaurants in gran canaria have the highest percentage of 1* reviews (6.73%). restaurants located in la palma and gomera have the highest percentage of 4* and 3* reviews. median differences in the percentage of ranked reviews vary along islands (p-values lower than 0.01 and 0.05)."
"based on the skewness and kurtosis statistics presented in table 5, the variable \" [cit] \" is not normally distributed. the kolmogorov-smirnov test was carried out to confirm the non-normality of all numerical variables related to the number of reviews. the analyses were carried out, taking into account the non-normality, so in mean comparison analyses, a robust anova (welch and brown-forsyth tests) was computed. spss 20 (ibm, armonk, ny, usa) was used in all analyses. table 6 shows the top 20 [cit] . regarding the spatial dimension of the image, these 20 restaurants are concentrated on only three islands: tenerife, fuerteventura, and lanzarote. surprisingly, gran canaria, the co-capital of the autonomous community, does not appear. tenerife included nine of the top 20 restaurants and lanzarote, six (the most popular). both the first-and second-ranked establishments have twice as many reviews as the third. thus, the opinions' spatial distribution does not reflect the different territories of the canary islands at a local community level. it might be that hard rock cafe, an emblematic worldwide chain of restaurants, attracts more reviews because tourists want to share their experience. that is, it generates a high expectation regardless of tourists knowing what they will find before eating there, and even knowing that it is not a restaurant offering local food."
"the rest of the paper is listed as follows. in section 2, system model for information centric sensor network is presented. we present the analytical evaluation of energy consumption and cooperative energy efficient design in section 3. extensive evaluation results are shown in section 4. and we conclude the paper in section 5."
to the former predictor we are going to refer during this paper as normal and to the latter as temporal. the number of time steps was chosen based on [cit] .
"naturally, detailed reconstruction of the position and velocity of distal parts of a limb become more difficult with a higher level of amputation due to the lack of muscles related to their motion. still, the motion dynamics of the leg are well known, so it is possible to create reliable above-knee prostheses [cit], even without having direct emg information on the state of the muscles controlling the foot. for the upper limbs, the control problem becomes much more complex in the case of an above elbow, or transhumeral, amputation, where only muscles of the shoulder and proximal part of the arm remain. this is due to the higher level of control required for reproducing the upper limb's detailed motions that are used in everyday life. in theory, the system should be able to reconstruct the motion of the shoulder, elbow, wrist, and fingers in order to control a full dof prosthetic arm."
"in this manuscript we use the word 'feedback' to refer to the different visual training modalities from which the subject perceived the avatar motions for the training. this, strictly speaking, is not a feedback, since the output that the subject is perceiving is independent of their actions. still, to avoid confusion between the training of the classifiers and the training modalities, we will use feedback to refer to the latter."
"with the results obtained in this study, in addition to our previous results [cit], we are confident that the use of a multilayer structure that combines the eeg and emg data, in addition to the temporal data, provides the best results for hand motion reconstruction."
"at an appraisal level, the adjectives used by tourists visiting restaurants show that customers consider a visit to a restaurant as a moment of enjoyment in which relish the experience. in addition, considering that the average valuation of restaurants in the canary archipelago is above eight points and is maintained throughout the period analysed, one can deduce that this is a destination that offers high quality cuisine in relation to tourists' expectations."
"at the end of the preprocessing we obtained a variable number of features depending on the subject. there were 11 from each eeg channel, and 7 from each emg channel (28 in total)."
"tenerife lanzarote gran canaria fuerteventura la palma table 1 shows different data on the canary islands as a tourist destination, such as the number of tourists visiting the islands, the average expenditure, the number of restaurants, and the cost of restaurant activity [cit] ). table 2 shows where tourists in the canary islands eat and buy their meals. some pay full board (3-5%) or half board (18-20%) at hotels, others pay for meals through their travel agency, and about 50-75% pay the restaurants directly. however, for example, tourists in half-board hotels usually eat lunch outside in restaurants, and tourists on all-inclusive trips also eat outside in restaurants during their trip. others have paid meals directly in the destination's restaurants. therefore, about 95% of tourists to the canary islands eat meals in restaurants outside hotels. the development of local cuisine based on local products as a tourist resource helps the sustainability and identity of the territory, increasing the level of local production and distribution and ensuring respect for tradition and heritage. the archipelago is also known as a 'miniature continent' due to its variety of landscapes and rich and diverse gastronomic offerings. the archipelago offers the spanish gastronomic culture, with latin american treats and african influence, with high-quality products of the territory, native fish, pork, rabbit, and goat combined with potatoes of protected designation and origin high-quality wines and cheeses; however, nowadays, only 10% of restaurants of the destination make use of local production."
"even when a subject can replicate the speed of the movement precisely, the position will vary slightly compared to the avatar. to calculate the impact of this variation, we added white noise to the output signal, and then calculated the new cv. we tested 30 different noise levels, with each defined by the size of the noise, ranging from 0.05 to 1 of the variation of the movement in increments of 0.05. each level was tested 10 times and then the mean cv was calculated. repeating was necessary as the noise was selected from a random distribution. we considered that the error caused by the normal shaking of the human is approximately 0.15. this conclusion was obtained after visually analyzing data from a motiontracking system and comparing the real motion against a perfect motion from the avatar."
"previously, we have presented an approach to motion reconstruction using both eeg and emg [cit] . the main problem of this work was that a motion capture system was used for obtaining the position of the hand, which, in the case of a real amputee, would not be possible. still, we demonstrated the advantages of using combined eeg and emg data."
"(i) at the first place the node will check whether a matched data object exists in the cache. if the interest is matched, then data object is returned and the processing of this interest ends at this step."
"regarding the use of vr, we were unable to see whether there is a beneficial or negative impact to the system. still, most of the subjects felt more comfortable with this technology compared to the other conditions. however, table 2 shows that using a vr device increases the noise on the electrodes, rendering some of them unusable. thus, we think that the long-term effects of using vr for this kind of application should be explored. finally, the impact of removing the motion tracking was not definitive due to the high variation of results, even among very similar studies that use motion capture."
"sensor networks have drawn significant attentions in recent years due to their cheaper prices, smaller sizes, and intelligence [cit] . the sensor networks have a wide variety of functionalities, such as monitoring humidity, temperature, movement of an object, and noise [cit] . and this leads to different kinds of sensor networks, such as terrestrial sensor networks, underground sensor networks, and multimedia sensor networks [cit] . particularly, multimedia sensor networks are deployed in order to track or monitor any activity in the form of image, video, and audio. in such networks, each sensor node is installed with a microcamera or microphone for collecting images and voice and they are normally planned to be deployed in a relatively fixed way into the environment [cit] ."
"23740 volume 5, 2017 figure 4. schematic representation of the shifting of the eeg relative to the position used to calculate the time-dependent error. only the steps −1 and +1 are presented. the result for ''shift + 2'' was obtained by applying the ''shift + 1'' process twice, and so on. this is also true for the negative values."
"additionally, two different analyses were performed for the optimization using the data from all the subjects. first, we calculated the evolution of the cv, when electrodes were removed sequentially. next, we calculated the mean rank at which each electrode was removed. because we removed the electrode with the lowest contribution in each iteration, the higher the rank in which the electrode was removed, the better the electrode was."
"the energy consumption mainly consists of two parts: transmission energy ψ tr and caching energy ψ ca . note that we omit the energy consumption for identifier system in the theoretical analysis since the node usually only needs to check the identifier mapping system once before transmitting the packet. in addition, such mapping information may also be stored in the node for further usage. therefore, little energy is consumed by identifier mapping system. and accordingly, the total energy consumption ψ can be presented as"
"the experiment was divided into three blocks based on the used feedback: no feedback, avatar, and vr (see subsection feedback). all test subjects performed all three blocks. each one of the blocks had six motions that the subject had to learn and execute (fig. 1) . the first three motions involved only one dof: shoulder flexion-extension, shoulder abduction-adduction, or elbow flexion-extension. the other three motions were reaching motions towards three different positions in front of the subject. they were more complex and consisted of combinations of the previous three dofs, including also shoulder rotation and forearm pronation-supination. still, as only the hand's position was reconstructed, without considering its rotation, the subjects were instructed to focus mainly on the position. this made the forearm pronation-supination not important for the analysis. in general, the most used motion in other studies is the center out (moving the hand from a central position to specific targets in the periphery of the range of motion). we considered that these kinds of motions would not be easy to reproduce by the subjects just by looking at an avatar. still, the reaching motions were similar to them, but with bigger amplitude."
"the widespread use of social networks and the increased use of online reviews are changing the way restaurants obtain data on the behaviour and preferences of their guests [cit] . in the past, hoteliers and restaurateurs got this valuable information from exhaustive surveys after a guest's stay or through \"mystery shopping\" tools, which assessed the level of service provided, areas for improvement, and customer satisfaction."
"the main limitations of this study include that it was conducted for a specific geographic area, used only tripadvisor, and the otrs were not segmented by tourist nationality, which would have opened the possibility for analysing further cultural aspects in studying the impact of dining reviews on the image identity of the destination. regarding differentiation between locals' reviews and actual tourists' reviews, we have assumed that locals' reviews are written in spanish. segmenting by nationality would also help in differentiating tourists' reviews from locals' reviews. another limitation is that restaurants could be classified by more than one type of food category, and in a few cases they seem incompatible; for example, a popular cafe is classified as both spanish and thai."
"in this paper, we propose an energy efficient management scheme for information centric sensor network. our contributions lie in three aspects. first, we develop a close form analytical framework for analyzing energy efficient management scheme considering both the energy consumption for traffic transmission and traffic caching. second, based on the theoretical results we propose cooperative energy efficient design in information centric sensor network. third, we perform extensive evaluations to analyze the impact of node cache size, caching probability, network sizes, and transmission rate on the energy consumption."
"in addition, showing the subject an avatar of their body in the vr environment would make it possible to present more various feedback to them by just creating the motion using specific software. conversely, in traditional systems that use motion tracking, the task usually consists of moving a real hand to a set of predefined positions. using an avatar has also advantages compared to having an expert as surrogate (who would place the motion tracking sensor in his/her own arm), because the motion performed by the surrogate may vary over repetitions. also, the subject could train with the proposed system as many times as they want, compared to having someone else do the motions for them with a limited schedule."
"altogether, there were 10 repetitions for each of the six motions in each of the three blocks, which sums up to a total of 180 trials. the order of each block was random for each subject in order to remove the possible memorization/training issues from the results."
"one of the problems that can arise, when using an avatar instead of motion tracking to obtain the position of the hand, is that the subject and avatar may not be synchronized. for this reason, we calculated the impact that it may have on the cv. to achieve this, we ''shifted'' the eeg signal to remap the relation between the input and output signals, and then calculated the new cv. a schematic view of this process is given in fig. 4 . we performed this for shift values between −8 and +8 in steps of one, which is the same as shifting the output between −0.5 s and +0.5 s in steps of 0.0625 s. eeg samples from either before or after the movement were used to fill the mapping between eeg and position. conceptually, when shifting negative values, older eeg data are being used; i.e. the system is trying to predict ''the future'' instead of ''the present''. in the case of a positive value, the system tries to predict ''the past''. time-dependent error is more likely to arise in the range ±375 ms [cit] ."
"data mining is the process of extracting information from a large dataset and structures it to facilitate the discovery of patterns. web pages downloaded from tripadvisor in plain text (no multimedia files or other attachments) were between 400 and 500 kb in size. the information useful for the case study was less than 5% of the content of the page [cit] . useful data was extracted using a text search utility that supported expressions (regex) of regular language (search patterns). the paratextual elements [cit] and html (hypertext markup language) metadata of otr web pages [cit] provided the data necessary to measure the perceived image, placing it in space and time (title, language, date, location, type or theme, rating, etc.)."
"regarding the evaluative dimension emanating from restaurant otrs, table 8 shows that, in general, most reviewers have given an excellent (5*) or very good (4*) scores. this is significant for destination marketing or management organisations (dmo). it seems that visitors to the canary islands are satisfied or very satisfied with the destination's restaurants. among the most popular restaurants (counting more than 1300 otrs), the highest rated (greater than 9 score) are asian and are located in tenerife. restaurants specialising in spanish local cuisine have a good score but lower than the aforementioned asian restaurants. the worst rated (6.15) is a british cuisine restaurant in fuerteventura. table 9 shows that an average 82% of total reviews are scored 4* and 5*. as pointed out previously, tourists posting restaurant otrs are, in general satisfied, or highly satisfied. it also shows other statistics of the variables percentage of reviews out of the total restaurant reviews per score (see the appendix a). table a1 shows the percentage of reviews out of the total restaurant reviews per score (5*-1*) and per island (gomera and la graciosa islands were not included in the analyses due to their low number of restaurants). table a2 shows a comparison of scores (1-10), means per region and per country. table 9 . descriptive statistics of % of reviews out of total restaurant reviews per score (5*-1*). table a3 shows the percentage of reviews out of the total restaurant reviews per score (5*-1*) and per region, and table a4 per country. results of these tables are relevant for observing differences between scores among the most-frequently reviewed specialty restaurants, and what proportions of reviews give better and worst scores."
"since we have found the theoretical expression of the total energy consumption model, we propose the following methodology for cooperative energy efficient design. we first find the cache size of each node by assuming that all other parameters are given. in other words, the cache size is precalculated heuristically and offline. once the cache size is fixed, we can adjust the other parameters. we consider two different situations in this case. in the former situation, the network node can adjust the parameter dynamically so that the caching probability can be easily changed. and in the latter situation, the node may not be able to change the parameter and the interest requester can adjust the parameter dynamically, such as the request rate. both situations can be performed online or in real time. the offline method is used before the system parameters are configured so that this can avoid the time and complexity when information is transmitted within the network. the online/real-time design is calculated dynamically by using the real-time information collected. this can achieve the accurate and timely control for energy efficient design. by combining these two methods, we can achieve energy efficient design in both long-term and short-term ways."
"what a gastronomic establishment is and what it offers must be determined in order to classify it into a category. in the tripadvisor restaurants section, cafes, bars, and any other establishment that offers the consumption of some type of food were included. the establishments sponsored by tripadvisor allow multiple classifications, such as region (e.g., american, asian, european), country (e.g., chinese, spanish, indian, italian, japanese, mexican), and type of food (e.g., seafood, steakhouse, vegetarian, vegan), eminently related to the cultural identity of the gastronomic experience. the canary islands are located in the atlantic ocean, close to the african coast, and their cuisine may be influenced by that of the south american countries. however, it is a european region populated mostly by spanish citizens. that is why many restaurant owners, when they register on tripadvisor, classify their establishment as specialising in spanish, european, or mediterranean cuisine. one limitation of the tripadvisor otrs is that the same dining establishment may be classified by different concepts, which makes the analysis more difficult."
"in this study, we performed an experiment in which we combined the use of emg, eeg, and vr with a virtual avatar. we analyzed how presenting different visual training modalities (remembering the motion, seeing it on a screen, and perceiving the motion through vr) to the subjects affected the motion reconstruction and the eeg activity. we did this with two goals in mind:"
"finally, the third component of the predictor took the output of the previous two components in the first layer and computed a multiple linear regression to obtain the final result. the whole predictor could be described as: . schematic view of the system architecture and its components. first, the eeg and the emg were predicted independently. then, the result from those predictions were used by the second layer to predict the final position. additionally, it was tested if using the previous reconstructed points (temporal component), to the second layer increased the cv."
"the results presented in fig. 10 help us to further analyze the differences between the three conditions. the topographic plots confirm that the desynchronization is stronger in the contralateral area compared to the ipsilateral area of the brain. this effect is stronger in the av and vr blocks, but it is possible to observe in the nf block in the mu band at 0.5 s and in the beta band at 0.5 s and 1.0 s. the desynchronization seems to be focused on the centro-parietal (cp) and parietal (p) electrodes. another difference that is possible to appreciate is that the desynchronization is not only stronger, but also starts earlier in the vr block. it is possible to see the beginning of the desynchronization at 0 s, especially in the alpha and beta bands. one possible explanation for this is that subjects start to think about the movement earlier in the vr block."
"regarding the affective dimension, this is about assessing the moods, sensations, or feelings the customers express in the review. among the paratextual elements of the review, the title is paramount because it summarizes and highlights the impressions perceived by its author [cit] . it is also rich in qualifying adjectives, exclamations, and recommendations, which allows classification in expressing positive, negative, or neutral feelings. for example, \"amazing\", \"never disappoints\", or \"wow!\" reflect positive impressions; conversely, \"avoid\", \"never again\", or \"yuck!\" reflect negative impressions."
"we compared the results of our study with similar ones. for this comparison, we selected only the latest available scientific paper of each research group (at the moment of writing this paper) that works on the motion reconstruction problem using eeg and motion tracking devices. we did that to avoid, for example, the multiple results corresponding to different stages of a work in progress. we did this, even in those cases that different available scientific paper belonging to the same research group had different methodologies (the studies are presented in chronological order):"
"(iii) next the node will check whether this is an existing pit entry in the node. if yes, the node will add the incoming interface of the interest and the processing of this interest terminates here."
"for each motion there were two phases: training, and execution. these two phases were preceded by a one minute relaxation time that was used as baseline. during the training phase, the subjects could watch the animation as many times as they wished by using the virtual reality headset in the vr block, and a computer screen (27 [cit] x1080 resolution) in the other two blocks. they were instructed to try to imitate the motion as accurately as possible with regard to position and speed. they could rotate the camera and zoom in and out freely to get a clear view of the motion. also, they were asked to practice the motion, not only to watch. the subjects were allowed to watch/practice the animation until they believed they could replicate the motion."
"other studies focus on the analysis of the territorial specialisation of the image based on ugc in travel blogs and reviews, determining that food and wine are an essential part of the image identity and differentiation between territorial brands [cit] . in a broad sense, the destination image identity includes different values, elements, ideas, experiences, and feelings that are transmitted within an image and represent and define that destination [cit] . in this respect, restaurants are one of the central elements for the development of culinary tourism in destinations and the transmission of destination image and identity through this form of cultural tourism. however, no studies have been found that analyse online travel reviews to unveil the gastronomic image identity of a destination."
"on the other hand, the system seems to be able to adapt without heavy loss to the time shifts. also, we can see that the av and vr conditions do not have a peak with either positive or negative shifts. this would suggest that the subjects were performing the movements at an appropriate speed, while the nf condition shows a peak for negative shifts, suggesting that subjects were performing a slower movement, which is coherent with our subjective observation during the experiment. in any case, the variation of cv was not big enough to provide enough evidence of that."
"massive data processing requires information structured to automate its collection, debugging, arrangement, and analysis. in the field of hospitality and tourism, there is a huge volume of semi-structured ugc on platforms dedicated to the online booking of trips, hotels, restaurants, and other tourism-related activities. applying a weighted formula [cit] based on variables like visibility (quantity and quality of inbound links), popularity (visits received and web traffic in general), and size (number of entries related to the case study), a ranking of websites hosting travel blogs and/or otrs was built."
"one of the most interesting results that we can observe from fig. 6, is that for some motions the contribution of the eeg component is negative. the two motions, in which the eeg contribution is negative, are those that rely heavily on the shoulder. interestingly, motion b is the one with the lowest shoulder motion, but motion f is the one that benefits the most from the eeg component. these results suggest that it would be beneficial for the predictor to include additional layers that may predict the type of motion being performed, and then balance the weight of the eeg component accordingly. the mean contribution by component is also coherent with the rest of the results, the vr condition benefiting the most from eeg, as the cv of the eeg component for the vr condition is the highest one. other than that, there seems to be no relation between the conditions and the eeg contributions. further analysis with more motions should be performed."
"our hypothesis was that the cv while using the vr would be greater than with the other conditions, due to the higher immersion that the subjects may feel. also, we expected that the cv would be lower in every condition compared to other studies that use motion tracking systems instead of virtual avatar. this was due to the intrinsic error that this method adds to the system, as the position used to train the system does not correspond to the user's real hand position, but to the avatar's hand position."
"authentication is an essential security service and a critical method for determining whether an individual is who s/he claims to be. it is usually based on a username and password, with supporting hardware, which can improve a service's security. therefore, authentication is considered to be a significant issue for online service access. as authentication is necessary for individuals to guarantee that their accounts are secure and their information is not exposed to everyone, it is also essential for organizations to have an authentication method in their information systems. in general, there are many reasons why organizations should implement user authentication besides security reasons, including monitoring system activities, filtering incoming and outgoing content to configure role sets and policies and managing time allowances by specifying the total duration of system access for each user."
"a man-in-the-middle attack could occur when a weak authentication scheme is added to the user options by the intruder, which may reveal the client's identifications. therefore, the client should be aware and always select the most reliable scheme from the selections offered [cit] ."
"in this section, the formal verification of security analyses for the proposed solution is discussed. as mentioned previously, we use avispa for simulation and formal verification. to this end, the hlpsl and cas+ models of our proposed solutions are presented, as they are an easy way in which to model a protocol characterized in the alice-bob notation, since this gives a perfect vision of the communication among the parties. the first proposed solution is represented in the alicebob notation, as illustrated in figure 5 . from the figure the steps are easily understandable, only step 3 and 5 need to discuss little-bit for better clarification. in step 3 the key has been computed and in step 5, the received and computed values has been compared."
"2) staggered tesla li and trappe [cit] established staggered timed efficient stream loss-tolerant authentication (tesla) as new a multigrade multicast authentication scheme used when creating the macs for authenticating a packet, by employing staggered and multiple authentication keys. their method is useful to compromise the effects of dos attacks by using multi-grade authentication in multicast scenarios and enhances the filter forged multicast packets by reducing the delay."
"because of the vulnerability of standard passwords, it is critical to manage them appropriately and it is essential to have a high level of certainty when identifying and authenticating users. further control efforts are needed, but, with large systems, this may prove difficult. fortunately, there is a more straightforward solution for adding a second layer of security to user logins and transactions that can be granted using multi-factor authentication. this solution works by involving two or more different factor criteria [cit] ."
"avispa is considered to be one among the most reliable, programmed official secured, verified and analysis tools for secure communication protocols. it has the capability of verifying whether or not the security protocol is secure and is capable of displaying every possible attack and its traces in case it is not entirely secured. avispa, which is freely available, makes it easy to model any security protocol. it has an animated and instinctive language known as high-level protocol specification language (hlpsl) to help the procedure writer and check the specification of the protocol. the basic idea about hlpsl has been derived from the semantic roots of lamport's temporal logic of actions [cit] . it also permits complex stream of designs and data structures and gets them exposed. aslo, it displays the communication method happening between different agents with the help of the ''alice-bob'' notation. the language used to define the protocol is based on various roles and each role is played by an agent. depending on the position assigned, each agent has to perform its task. the primary job assigned for the purpose is usually an event action-dependent transition: the moment an event happens; the agent has to act accordingly by moving itself from one state to another after the completion of a particular action. furthermore, there is always a relation between an event or an action from any agent and an occasion or act from any of the lasting mediators; to be clear, when something is sent or received by the agent, there is continuously extra mediators who acts on it either as receiver or as sender depending on the action happened."
"a service authenticator that can be used by a company, by which the user is validated once for each session, whether by pin or similar. 3. a transaction signature, used by the user to sign data electronically. by using the pad as a user identity manager, authentication can be achieved automatically by every supported service."
"another study [cit] suggested using a smart card to support operator verification, grounded on elgamal's public key cryptosystem. the card's security trusts on the effort of calculating discrete logarithms above finite fields. this is a system of remote user authentication without a password verification table; the user applies to the registration center and is issued with a smart card with a password. after this, when the authorized user wants to log in, s/he inserts the card into the login device along with his/her id and password."
"in this work, dinesha and agrawal [cit] suggested a method of accessing a cloud service that involves generating the password at many levels across the organization, which provides strict authentication and authorization. their technique contains two separate objects: first, to ensure the cloud services, they have a cloud service provider; and second, they have an authenticated client organization for those accessing the cloud service. authentication activities can be applied at the agency, team and user levels."
"acknowledged authentication factors have been placed into three categories, each of which may contain a range of elements used to verify and authenticate the identity of an individual. the classes are as follows: first, knowledge factors (what a user knows), for example, the password; second, ownership factors (what the user has), for example, an id card; and third, inherent factors (who the user is), such as fingerprint data. a more useful approach is to combine two or more authenticator factors to gain benefits in security, convenience or both; for example, an atm requires a bank card and a pin. the bankcard is an example of something a user has and the pin is an example of something a user knows. in this case, to represent this scenario, the preferred term is two-factor authentication [cit] ."
"to prevent replay attacks, the server can pass a dynamic token called a server-specified nonce to the client, which is changed frequently [cit] . the client attaches this nonce token to the volume 6, 2018 password before computing the digest and it can be used as a password salting. mixing the nonce in the password by concatenating it causes the summary to change each time the nonce changes. thus, the replay attack is prevented [cit] ."
"in this section, first, the implementation using a security tool for the formal verification of the proposed protocol is discussed. in the next subsection, an analysis is provided."
this section discusses a mitigation technique for man-in-themiddle and replay attacks and then suggests principles to guide the design of further secure authentication schemes. the complete procedure is displayed in the figure 4 and the details are listed in the steps below:
"the protocol has five interactive handshake communications between the sender and the receiver for mutual authentication with the standard certificate and tcp/ip packets. on top that the protocol needs to compute, two concatenation operations and two hash functions computation."
"to protect trusted platform module objects from unauthorized access, some alternatives have used an object-specific authorization protocol (osap) or session key authorization protocol (skap). by using this situation as a case study, [cit] examined the security analysis value of cpns to demonstrate their applicability as a common device for modeling and analyzing safety procedures. by including error handling and the recovery of many parts of the model, error discovery was improved. this proposed method is done by examining the osap to enhance the skap."
"reference [cit] analyzed a recently proposed handover authentication protocol, pairhand, to discover its vulnerability. they identified a threat to a compromised session key and tried to compromise it through a simple modification of the protocol, without losing any features. pairhand is very efficient in terms of computational complexity and communication, because of its features of shared verification and essential formation; only two handshakes are needed in between mn and ap, with no requirement to transmit any verification certificate as is done in traditional public key cryptosystems [cit] ."
"for this reason, we suggest possible solutions against this attack. first, the client should be aware and always select the most reliable scheme that s/he knows from the selections presented. second, for high-risk and high-security systems that require these guarantees, the user has to turn to secure socket layout (ssl). ssl is a transporting layer security, where the client and server exchange pki certificates before establishing a session; these certificates are issued and verified by a third party, which should be a standard certificate authority [cit] ."
"online service access usually uses a combination of static passwords and hardware devices, which dynamically generate access credentials. this approach requires that the user has many tools for each transaction and more passwords than s/he can memorize. to manage this situation, the personal authentication device (pad) was proposed; the pad can be used for user authentication for every online service, in addition to providing a series of other security services. by using the pad as an identity manager, the user can be authenticated by every supported service automatically. the authentication process can be achieved by passing replay-protected challenge-response communication between the pad and remote servers [cit] ."
"6. if the computation is equal, then access is granted; thus the challenge-response authentication successes. as a conclusion, we present some proposed solutions to mitigate each attack."
"the authors of a ''more efficient and secure dynamic id-based remote user authentication scheme'' declared that their scheme preserves client secrecy, but [cit] suggested that it does not. during authentication, the user cannot choose his/her password, which makes him/her vulnerable to insider attacks. hence, the scheme is not feasible for real-life implementation."
"the hlpsl model for the proposed protocol: using the alice-bob notation in figure 5, the automata format representation is depicted in figures 6 and 7, where the state tran- sitions are shown openly for all the essential roles (offpad, server). hlpsl works in the event action-based model, which is why ''event'' and ''action'' are attached with the transitions. due to the scope limitations of the paper, a few important things are discussed here regarding the hlpsl model for the proposed protocol. the keyword ''rcv'' is used to receive a message from a sender and ''snd'' is used to send a message to a receiver. hlpsl has a default ''start'' state for the initiator to start the protocol. in our model, the offpad takes the initiative for the initial communication by sending a special signal ''snd(start).'' in hlpsl, it is assumed that the roles have some pre-computed shared knowledge to start the protocol. based on this pre-computed shared understanding between the roles, the proposed protocol is executed in the hlpsl model. it is remarked by the avispa state transition that both of the roles are in a safe state. an intruder party cannot have enough knowledge to attack the protocol. volume 6, 2018"
"reference [cit] investigated li-hwang's biometrics-based distant operator verification system by utilizing smart cards and discovered some configuration defects. in the login and confirmation stages, they found that the framework superfluously includes additional correspondence and calculation. second, during the password change phase, there was no verification of the old password in the scheme, even if the user entered his old password incorrectly by mistake; updating the new password would take place mistakenly. finally, they discovered a flaw in the biometric checking hash; when the biometrics information was noisy, the cryptographic hash capacity could not be clearly connected. with the specific goal of overcoming these defects, he proposed changes to the plan."
"antonysamy and patro [cit] highlighted the ''challenges and possibilities of antenna design for the multipurpose wireless authentication device,'' which includes the interface, control of the internal devices, product certification issues and interference with other wireless technologies."
"in this paper, the mitigation techniques for protecting the vulnerabilities of offpad-based authentication solution has been proposed, which is the extended version of their previous work [cit] . the limitation of the proposed technique is that it is not studied the implementation of the solution in a hostile environment."
"using two-factor authentication is an authentication category that combines something a user knows with something a user has, as with a bank account security token. a more advanced device that can be used for multiple systems at the same time is the pad, which can provide security, privacy and multi-service authentication using just one device."
"the target of this paper was to investigate the possible vulnerabilities of offpad-based solutions and provide mitigation techniques for these vulnerabilities. many studies focus on the analysis of the potential to expose a user's privacy while accessing a system through an authentication process. by conducting a detailed classification and rigorous literature review on existing protocols, we showed the existence of different protocols that use extra devices or biometrics. subsequently, we performed a partial investigation of the vulnerabilities in some actual data origin and entity authentication protocols, using an attack tree analysis. we realized that there are many forms of attacks that could threaten the privacy of the user through these protocols. concerning the previous sections, a vulnerability analysis for offpad-based solutions for user authentication was performed. as a consequence of our security analysis, we found that this authentication was vulnerable to attacks, specifically replay and man-inthe-middle attacks; therefore, to mitigate these attacks, new schemes were presented by using ssl and nonces to prevent them. as a future research, the implementation of the proposed solution can be studied in a hostile environment. md sarwar m haque received the m.sc. degree in computer systems and networking from the university of greenwich, london, u.k. he is currently a faculty member with the king fahd university of petroleum and minerals. he has published a number of peer-reviewed publications. he is also involved in several research and development projects. his research interest includes internet of things, network security and privacy, data mining and machine learning techniques, performance analysis and simulation of computer and communication systems, and image processing."
"by exploiting the unrevealed vulnerabilities in existing authentication protocols, an attacker can gain a considerable amount of illegal benefits. therefore, many studies have focused on the analysis of existing protocols to develop new methods that add more layers of security, either based on biometrics or using extra devices. based on this, we conducted a rigorous literature review on the existing protocols with a detailed classification and the taxonomy is provided in fig. 2 ."
"trusted computing (tc) refers to a cluster of ideas, technologies and applications for resolving computer security problems. it ensures that different parts of the system are behaving as expected. this improves the overall trustworthiness, privacy and security of hardware and software [cit] and allows applications to communicate securely with servers and other applications. tc can be achieved through software modifications and hardware enhancements. in pc hardware, encryption keys are built that can be used to verify its identity and integrity. the operating system guarantees the application software's character and integrity by communicating with remote servers securely. to achieve secure operations, hardware-based cryptographic keys are used, which are generated and stored in the hardware manufacturing process. the design of this hardware is so sophisticated that it is not possible to retrieve the key by any method (i.e. reverse engineering). this core is never exposed to any other componenteven to the owner. many applications use the concept of tc, for example, digital rights management, different platform authentication, preventing cheating in multiplayer games, distributed firewalls, third-party computing, improving reputation reckoning and data security and privacy [cit] ."
"reference [cit] discussed a mobile architecture for strong personal authentication (maspa), which uses hash functions, symmetric and asymmetric encryption (a three-pass diffiehellman variant) and cryptographic primitives including digital signatures in its authentication algorithm."
reference [cit] attempted to provide an authentication mechanism (fido) that reduces reliance on passwords. the new verification structure intends to give a favorable split between local-user-to-authenticator authentication and authenticatorto-reliant-party authentication; trusted parties can utilize the new client verification strategies without changing the serverside infrastructure.
"the authentication process can be achieved by passing replay-protected challenge-response messages among the pad and isolated servers; the offpad never exposes the password to the client terminal and remains online for brief periods only. thus, the devices are minimally exposed to the distant server through the operator's computer. therefore, the offpad can be used as:"
"b. b. gupta received the ph.d. degree in information and cyber security from iit roorkee, roorkee, india. he has published over 90 research papers (as well as three books and 14 book chapters) in international journals and conferences of high repute, including the ieee, elsevier, acm, springer, and wiley inderscience. his research interest includes information security, cyber security, mobile security, cloud computing, web security, intrusion detection, computer networks and phishing. [cit] . he has visited several countries, such as canada, japan, china, malaysia, and hong kong, to present his research work. his biography was selected and published in the 30th [cit] . he is also a principal investigator of various research and development projects. he is supervising ten students for their master's and doctoral research work in the area of information and cyber security. he has also served as a technical program committee member of more than 20 international conferences worldwide. he is member of the ieee, the acm, sigcomm, the society of digital information and wireless communications, the internet society, the institute of nanotechnology, a life member of the international association of engineers, and a life member of the international association of computer science and information technology. he is an editor of various international journals and magazines. he is serving as an associate editor for the ieee access, an associate editor for the ijics and inderscience, and an executive editor for the ijitca and inderscience. he is also serving as a reviewer for journals of the ieee, springer, wiley, and taylor & francis, and as guest editor of various reputed journals. volume 6, 2018"
"the security choices in rfc 2617 are discretionary; [cit] mode. in this section, we discuss the attacks that can be caused by using the http daa protocol to authenticate users aligned with the offpad. we discovered two attacks."
reference [cit] tried to address the vulnerabilities of login credentials by analyzing the characteristics of login credential usage and found that the number of subscriber accounts was considerably more significant than previously expected. they also found that many users used the same login credential information as each other.
"if the server only supports the nonce, the intruder can eavesdrop on a message or message components from a previous session and replay it as a new message to establish a new course [cit] . even the user credentials are hashed and sent to the server as a digest. the intruder can use this compendium without knowing the password by capturing it and replaying it to the server to establish a new session."
"our feature extraction takes 0.1s for the first layer and 0.5s for 2 layers (single-threaded, unoptimized matlab code). comparison to other methods. we compare our features learned with and without supervision against the two other pre-engineered features: sift [cit] and hog [cit] . we densely extract both of these features over patches of size 16, and create a codebook of 1000 visual words for each of them over the training data. finally, a histogram of visual words is computed and used as image representation, which we then compare to our representation. note that our codebook size is smaller, consisting of only 400 visual words, which makes a fair comparison. the results are summarized in table 1 . the average classification accuracy based on hand-crafted hog and sift features achieve average accuracies of 50.1% and 62.0%, respectively, while our method without supervision outperforms them and achieves 70.1%. when we introduce label information in the second layer performance further improves to 72%. classification of fine-grained classes benefits particularly from our supervised features, e.g. for blouses leading to a relative improvement of 13.2%. thus learning discrimantive features for apparel classification results in a relative improvement compared to hog and sift by 43.7% and 16.1%, respectively. qualitative results. our cluster means in the first layer capture edges at many different directions as well as background and other simple patterns (cf. fig. 6 ) with strong visual similarity to the features learned by cnns [cit] . while the first layer learns simple shapes, the second layes learns in comparison more complex and composite shapes and textures, cf. fig. 9 . notably, the class-specific clusters capture the characteristical class elements particularly well: e.g., a part of the glasses, a ring or the crotch of pants."
"this paper considers single-user ais transmissions. the information sequence is composed of 168 bits, on which is computed a 16-bit crc. this crc is concatenated to the 168 information bits. the bit stuffing procedure is then applied to the resulting sequence. the frame is encoded using the nrzi coding, and then modulated using a gmsk modulation. some properties of the different parts of the transmitter design are briefly recalled below."
"the world-wide e-commerce market for fashion is growing bigger and bigger. online sales of fashion products are increasing faster than of any other product segment. total e-commerce sales in the us are expected to surpass $50b [cit] at 15% anual growth 1 . the product offering of big retailers such as zappos, nordstrom, amazon and zalando is increasing dramatically. amazon us has currently more than 500k fashion products in stock and on average 12k new products every weekday 2 . in order to provide the customer with a great user experierence all these new items need to be categorized and augmented with metadata like product category (dress, pants, etc.), attributes (material, color, etc.) and sub-product categories (pumps, laceup heels, wedges, etc.). this then enables the customer to conveniently browse the product database in an organized manner and retrieve the relevant information easily. to organize a product database in this way, products can be manually categorized and annotated with meta-data, but such endeavour becomes timeconsuming and impractical as the database grows. as a result of the very high level of product innovation and competition in the apparel market, categories are subdivided in subcategories, new categories emerge, etc. however, since manual re-categorization of products is excessively tedious, better sources of information to cope with such a dynamic setting are required. one such source are the product images, which are often readily available or can be quickly and cheaply ac- quired. instead of manual annotation, the meta-data is extracted automatically from the images."
"where m k denotes the sample of the kth estimated symbol after mf. due to the possible presence of stuffing bits and the use of the crc, the minimization should satisfy the two following constraints"
"the performance of the proposed receiver for an inaccurate modulation index h is shown in fig. 2 . a significant improvement is obtained with respect to the nda algorithm especially when the modulation index is estimated using the ais preamble. the performance with and without estimation of h are 2.5 db and 1 db better than those of the nda algorithm. moreover, they are about 0.5 db and 2 db above those obtained with known modulation index. this figure illustrates the fact that the proposed method outperforms the usual nda approach in the situation (encountered in the ais system) where the actual modulation index deviates from its nominal value."
"the bit stuffing procedure presents two main goals: i) by generating additional transitions in the transmitted signal, it allows the receiver to re-synchronize its clock; ii) it enables specific bit sequences to be avoided, such as begin or end flags. in the ais system, a bit 0 is added after five consecutive bits 1, in order to avoid that a data sequence is considered as the end frame flag byte (composed of two bits 0 on each side of six consecutive bits 1). by definition of the stuffing bits, their presence and their location are random. the receiver must then detect and localize them before recovering the information bits."
"the selected 1-month period is characterized by a significant variability in wind speeds and directions (fig. 3), which allows the evaluation of the new model version under different conditions. during 1-10 august, east winds prevail over the region and increased dust concentrations are found mostly along the central, east and south coastal areas of the arabian peninsula. an anticyclonic circulation is established during 10-15 august over the arabian desert and increased dust concentrations are mostly found over the central desert areas. on 16-26 august the circulation is mainly from northerly directions and thick dust plumes are advected southwards towards the arabian sea. the north winds veer east on 26-31 august, and increased dust loads are found over the gulf on these dates."
"among various approaches to extract the information from images, automatic image classification has shown promising results, and performs well even on large-scale datasets with millions of images and hundreds of classes [cit] . given a set of image categories and an annotated training dataset, a classification model is learned to predict the category of an image unseen during the training of the model. if we need to embed a new fashion product into the existing database ontology, we can predict its precise category by its image. when the product ontology changes, we only need to re-annotate a training set, which is only a small portion of the total data, re-train the classification model and apply it on all the images to re-categorize the products. the accuracy of classification models highly depends on features used to represent the images [cit] . the image is usually described with features at different levels of detail. low-level features are used to describe small patches, while mid-level ones capture larger image regions. manually engineered low-level features based on gradients such as histogram of oriented gradients (hog [cit] ) or scale-invariant feature transform (sift [cit] ) proved to be robust to lighting changes and successfully capture shape cues [cit] . this allows to cluster the patches into a vocabulary of so-called visual words. histograms over different regions are concatenated together to form a pyramid histogram of visual words (phog [cit] ) as a final representation. vocabulary can be built in unsupervised manner by k-means [cit], hierarchical k-means [cit] or by iteratively mining discriminative patches [cit] . however, when a computational overhead is acceptable, vocabularies can also be learned per class in a supervised manner [cit] . these approaches thus employ pre-designed, general low-level features, but learn a specific mid-level representation suited for a particular application."
let (a; α) denote one of the possible states at time k. the squared euclidean distance between the received signal up to time time k and the sequence of k symbols coming to the extended state (a; α) at time k is defined by
the reception of ais signals by satellite is a very active subject [cit] which led us to propose new transmission error correction methods [cit] . this paper is considering a more realistic model for ais signals including an unknown (possibly time-varying) phase. the performance of the resulting joint phase-recovery and demodulation decoding algorithm is shown to be very promising.
"this paper presented a demodulation algorithm coupled with a phase tracking procedure for ais signals received by a satellite (with unknown phase shift and modulation index). this algorithm based on an extended trellis jointly estimates the phase shift, detects and removes the stuffing bits, and demodulates the information bits. simulations showed that the proposed method performs better than the nda phase recovery algorithm. future works will concentrate on the time-delay estimation and multi-user transmissions."
"in fig. 8 we illustrate test images and the classification when our learned features are used. mind that some of the correctly classified cases are hard to categorize even as a human. the misclassified images reveal that it is hard to distinguish between coats, tops and dresses. the confusion matrix in fig. 7 shows that certain categories are particularly hard to distinguish from each other, i.e. blouses and tops as well as jumpers and coats are frequently mixed up by our method. in contrast most of the other classes can be classified very well."
"denote by s(t) the signal generated by the gmsk modulator. in this paper, one considers a frequency-flat channel, whose transmission delay and doppler shift are known by the receiver. these parameters could for instance be estimated by correlation-based techniques using the headers as pilot symbols. however, these estimation issues are beyond the scope of this paper and will be addressed in future works. as mentioned above, variations of the modulation index can be interpreted as a random phase shift φ(t), so that the received signal can be expressed without loss of generality as"
"it is well known that the crc is defined as the remainder of the division (modulo 2) of the polynomial derived from the data and a standardized generator polynomial, whose degree equals the length of the crc plus one. some zeros may be inserted before the remainder in order to obtain a fixed-length crc. a comparison between the crc computed from the received data and the crc contained in the data frame allows the receiver to detect some errors. a key property of the crc is that it can be computed iteratively by initializing the crc to a standard value and by applying operations sequentially to each data bit [cit] . this very important property enables an extended trellis to be defined, as explained in section 3.2. on the other hand, since the crc is included in the information bits on which it is computed, one can define a joint crc on the bit sequence composed of the data and the crc. no error is detected at the receiver if this joint crc is zero, i.e.,"
"the important phase fluctuations induced by the modulation index inaccuracy make the use of an nda algorithm difficult (as it needs a large integration window to limit noise effect). a full complexity psp approach [cit] (i.e., considering all trellis paths) is not appropriate here because of the huge number of states and paths in the trellis considered in this paper. one might think of using reduced complexity psp [cit] or tentative decision (td) algorithms. however, since these algorithms consider a limited number of paths, the selection of these paths (according to their probability) at each symbol period makes these algorithms intractable in our application. the method investigated in this paper overcomes this complexity issue by applying the viterbi algorithm to the extended trellis designed previously, and by including a phase term in the euclidean distances used in the algorithm. the procedure described below explains how the algorithm selects a path reaching a given state (denoted as (b; β)) at time k + 1 from different possible states at time k. note that all the following variables should be indexed by (b; β). however this notation has been omitted for clarity reasons."
"the bit sequence obtained after the bit stuffing procedure is encoded using the non-return-to-zero inverted (nrzi) coding. the resulting sequence is modulated with the gaussian minimum shift-keying (gmsk) modulation. recall that, in the gmsk modulation, the transmitted signal s(t) is a constant-modulus signal defined as"
"since the crc can be computed iteratively, it can be initialized to a particular value given by the crc standard, and then updated at each received bit. a crc state is then defined as a particular intermediate crc value. two consecutive crc states are connected if the second crc state can be obtained from the first one by including a bit 0 or a bit 1."
"v and d are the eigenvectors and diagonalized eigenvalues of σ l, respectively, zca is a small constant fixed to 0.1 and i is the identity matrix. the cluster centroids c"
"the discriminative low-level and mid-level features can also be learned jointly by training convolutional neural network operating on raw pixel values (cnn [cit] ). while cnns achieve extraordinary results [cit], training is computationally intensive, hard to parallelize, slow and needs specialized hardware."
"the paper is organized as follows. section 2 presents the transmitter characteristics and the received signal model. section 3 describes the detection algorithm with a specific attention to the phase tracking technique, which constitutes the main novelty of this paper. some simulation results are given in section 4, which illustrate the performances of the proposed approach on a realistic ais simulator, developed by the cnes of toulouse, france. conclusions are reported in section 5."
"the test simulation period is 1-31 [cit] and the results from both simulations are compared to modis and aeronet aod. a 5-day spin-up model run, prior to the experimental period, is used for establishing the dust background over the domain. after finalizing the experimental model configuration, we perform a complete 1 [cit] and evaluate the results against aeronet stations."
"we evaluation model performance using five metrics: mean bias, root mean square error, correlation coefficient, mean fractional bias, and fractional gross error. concretely, assuming we have n pairs of model values (m i ) and observations (o i ), the mean bias (mb) is defined as"
"where #_of_dust_points is the number of points with ndvi values smaller than 0.1. this approach allows for a dynamic description of dust source areas over the model domain to replace the previously used static database. moreover, the scaling of satellite data over model grid points allows the use of the same algorithm for different model configurations. several mountains in the area (e.g., the sarawat mountains along the red sea coast and the zagros mountains in iraq) could be misclassified as dust sources due to low ndvi values. in order to exclude such unrealistic emissions from non-soil bare areas or snow-covered areas we have applied a limit of zero dust production above 2500 m over the entire domain. this simple approach has been selected in order to keep our straightforward ndvi mapping independent of vegetation and soil information. the threshold value of 2500 m does not suppress the emissions from lowlands and hillsides (e.g., [cit] ) ."
"for the purposes of our study we used the 500 m 16-day averaged ndvi from modis [cit] for the period of interest. the ndvi is a normalized transform of the nearinfrared to red reflectance ratio, designed to provide a standard for vegetation, and takes values between −1 and +1. since it is expressed as a ratio, the ndvi has the advantage of minimizing certain types of band-correlated noise (positively correlated) and influences attributed to variations in irradiance, clouds, atmospheric attenuation, and other parameters [cit] ."
"due to the importance of the market for online shopping of dressed clothing, various methods have been recently proposed to tackle the problem of classification of apparel images [cit] . most of them work with images taken in unconstrained environments and try to classify the apparel [cit], retrieve related images in the database [cit] or infer how much a person displays a certain style [cit] ."
"to compose an extended state, a crc state is associated with a state of the trellis code (a tc state) and both trellis behave in parallel with the same bits. this mechanism is depicted in (5), where the integer k corresponds to the kth received symbol."
"the importance of natural particles, namely desert dust, in the weather and climate has been underlined in a great number of studies. dust is a climatic regulator, as it modifies extensively the radiative balance of the atmospheric column (e.g., [cit] . at the same time dust aerosols modify the atmospheric water content [cit], the way clouds are formed by acting as cloud condensation nuclei (ccn) and ice nuclei (in), and the precipitation process [cit] . in addition, there is a clear connection between dust particles and human health disorders, as the size of the aerosols produced is small enough to cause respiratory and cardiovascular diseases, as well as pathogenic conditions due to the microorganisms that they can potentially carry [cit] ."
"author contributions. ss contributed to the conceptualization, formal analysis, investigation, methodology, project administration, resources, software, validation, visualization, writing of the original draft, review, and editing. aa contributed to the conceptualization, funding acquisition, project administration, supervision, writing, review, and editing. cs contributed to software and data curation, visualization, writing, review, and editing. ib contributed to conceptualization, formal analysis, software, writing, review, and editing. sn contributed to methodology, supervision, writing, review, and editing."
"in the light of the fast growth and high level of innovation in fashion e-commerce, automated apparel classification becomes crucial. in this work we proposed a hierarchical discriminative feature learning framework for apparel classification. we show that learning apparel specific features results in better average accuracy and outperforms hand-engineered features, while supervision further boosts the performance. in future work, we plan to refine the clusters iteratively across the layers to improve discriminativeness of the codebook. we believe that our method could highly benefit from feature encoding, i.e. fisher encoding [cit] ."
"secondly, we evaluate model performance using aeronet aod retrievals at eight photometric stations. aeronet is a network of sun or sky photometers that derive aerosol optical and microphysical properties at a large number of stations around the world [cit] . for this evaluation, we use version 3 aod retrievals that, in comparison with previous versions, improve automatic cloud screening [cit] . level 2 datasets were used for all stations apart from kuwait university, where only level 1.5 data were available. both model and aeronet aods were calculated at 532 nm; this was chosen to facilitate future intercomparing with lidar systems that frequently measure at this wavelength (e.g., [cit] ) . aeronet measurements were converted to this wavelength using the 440-870 ångström exponent and taking into account aod measurements at 440, 675, and 870 nm; in the cases where the 440 nm aod was not available, the 500 nm (mezaira) or 443 nm (kaust campus) measurement was used instead."
"a structural system (a, b) is called structural transittable between two structural states x 0 and x 1 if and only if there exist matricesã,b,x 0 andx 1 which are admissible with respect to a, b, x 0 and x 1, respectively, such that the system (ã,b) is transittable betweenx 0 andx 1 [cit] ."
"there are nine network controllability algorithms implemented in cytoctrlanalyser. the relationships among these algorithms can be represented by fig. s9 . from fig. s9 we can see that the algorithms for mds, mss, mss with preference, output controllability and state transittability are designed to identify a set of nodes which should be actuated by control signals such that different control objectives could be achieved. the algorithms for control capacity, control centrality, node classification and node probability in random mss are designed to evaluate the importance of nodes from different aspects in the controllability of networks. besides algorithms for output controllability and state transittability, all the algorithms are designed based on the completely controllability of networks. the arrows from mds to mss and from mss to mss with preference indicate the progress of the research on network controllability. controlling an mds is a necessary condition for completely controlling a network. to further investigate the controllability, the mss has been proposed, which is a sufficient and necessary condition for completely controlling a network. since msss of a network are not unique, the mss with preference has been studied."
"mss with preference: each node should be assigned a real positive number as the preference for selection of mss. the data type could be either integer, long, float or double."
"cytoctrlanalyser is a cytoscape app that is implemented based on the open service gateway initiative (osgi) framework. osgi is a java framework for developing and deploying modular software programs and libraries. the recent versions of cytoscape platform has adopted osgi technology [cit] . therefore, both core modules and apps in cytoscape 3.x are osgi bundles, which reduces the complexity of developing apps remarkably. in addition, cytoscape is a software developed in java. therefore, cytoscape takes the advantage of java that can be run in different operating systems with the java virtual machine (jvm). the relationships among cytoctrlanalyser, cytoscape and their running environments are shown in fig. s8 ."
"other approaches had mixed goal driven and user driven approach [cit] introduces cadwa approach [cit] that starts eliciting requirements by studying informal needs expressed by decision makers and summarize it into strategic plan. end user will assist in mapping user requirement and extend it to more detail action plan to achieve macro and micro goals. final plan will then be used to build conceptual model of data mart (dm). [cit] classify goals into three levels; strategic, decision and information goals [cit] . strategic goals are subjective business goal from broad view of top management. it will be detailed out by end user into decision goals that are more specific and later into information goals that will distinguish which specific information could lead to business goal."
"fig . s2b is the interface of importing the table file to cytoscape. the first column is selected as the key, which would match the values to corresponding nodes in cytoscape. users need to indicate the data types of different columns: the second column is set as floating point and the third column is set as boolean. fig. s2c is the data imported to cytoscape shown in node table panel: column 2 saves the preference values of nodes and column 3 indicates the node whose states are supposed to be changed in state transition or output control."
"for structural matrix m, if a matrixm can be obtained by fixing the independent entries of m at some specific values, the matrixm is called admissible with respect to m. considering a structural system (a, b), the state vector x is a structural vector, in which entries are independent parameters or fixed zeros."
"control capacity: since the mdss of a given network is not unique, the control capacity measures the likelihood of each node appearing in a random mds [cit] . control capacity values of nodes in a human liver metabolic network have been studied [cit] ."
"has full row rank n [cit] . for structural systems, we say the structural system (a, b) is completely structurally controllable if it is possible to choose the values for the independent entries in matrices a and b such that the kalman's controllability rank condition is satisfied [cit] . the graph-theoretic conditions for structural controllability have been developed in previous studies [cit] . before introducing structural controllability theorem, we introduce two following concepts."
output controllability: each node should be assigned a boolean value which indicates whether the node is corresponding to the output of the network. true means the node corresponds to one output of the network while false means the node is not the output of the network. the first step is to import a network to be studied and open cytoctralanalyser app. fig. s1 is an example of a network with 8 nodes and 10 edges and the cytoctrlanalyser interface in cytoscape.
"cytoctrlanalyser implements an algorithm to classify nodes based on this classification method. to get the classification, firstly the directed human ppi network can be acquired from supplementary materials of reference [cit] . then the network is imported to cytoscape and cytoctrlanalyser is opened. after this, the classification of nodes can be calculated by checking the node classification checkbox and pressing the analyze button. the result is shown in column classification of node table panel, in which 0, 1 and 2 correspond to dispensable, neutral and indispensable, respectively (fig. s7) ."
"for structural system (a, b), by arbitrarily choosing the value of free parameters in a and b, the rank of controllability matrix c can reach a maximum value, which is denoted as generic dimension of controllable subspace gdcs(a, b). then we have the following theorem for structural transittability:"
"the cytoctrlanalyser implements algorithms for studying biomolecular network controllability based on nine recently proposed concepts: minimum driver node set (mds) [cit], minimum steering set (mss) [cit], mss with preference [cit], steering nodes for state transittability [cit] and steering nodes for output controllability [cit], control centrality [cit], control capacity [cit], classification [cit] and probability of each node in a random mss [cit] . the following paragraphs give the descriptions of algorithms that are implemented for investigating network controllability."
"kaldeich & oliveira e sá (2004) incorporates business process within his approach with combination of data driven, user driven and goal driven approach [cit] . the approach starts with analyzing the source data and produces the entity relationship (er) diagrams. this approach investigates the business process and produces business process for current process (as-is model). in this approach end user will be interviewed and will produce a to-be business process model. then the models (as-is and to-be models) will be integrated to form a new integrate-process-diagram (ipd). the ipd is used as comprehensive dw requirements and will be verified through interview with top management to ensure it could lead to achieve business goals."
"in the table, we can also find that 4 steering nodes in the mss are drug targets and each node can interact with 4.17 drugs averagely. by multiplying corresponding values in column approveddrugtarget and column probabilityinmss, we can find that averagely there are only 2.19 steering nodes that are targets of approved drugs in a random mss without preference. similarly, by multiplying corresponding values in column numofpossibledrug and column probabilityinmss, we can find that each steering node in a random mss can interact with only 1.84 drugs averagely. the results demonstrate that steering proteins in the mss with drug binding preference are significantly enriched with drug targets and interactions to drugs, which suggests that con-trolling cac network by actuating states of steering proteins in the mss with drug binding preference is more feasible compared to actuating nodes in a randomly selected mss."
fire emissions database (gfed4) [cit] . this is based on the this methodology requires an initial run of the dvm to produce ba for each year.
"the past few decades had shown explosive of products and services related to data warehousing [cit] .this is as consequences of most large companies worldwide had established data warehouse (dw) as one of their main component in information system environment. dw has become the standard tools and essential component of decision support system [cit] . in addition, it has been successfully deployed in many industries such as manufacturing, retail business, financial services, transportation, telecommunication, utilities and healthcare [cit] to support decision making. dw is built to improve decision making system [cit] by providing consistent, timely, subject oriented information at the required level of detail [cit] .it produces concise analysis to assist decision maker and increases organization corporeal performance."
"data-driven approach, also known as supply-driven approach applies bottom-up technique [cit] with focus on underlying operational data source as a basis to established scope and functionality of dw [cit] . data-driven approach starts with identifying all available data within transactional data source and analyses it in order to generate data mapping. data issues pertaining data overlapping, aligning ambiguous terms, and code mapping will be sorted out to ensure mapped data can be reengineered to build logical data schema. user requirements is determined by selecting portion of data available in data mapping [cit] . since main role in eliciting requirements plays by database administrator, it diminishes business user involvement."
"to simplify the assimilation of the ccl database into a dvm we pooled forests and 12 non-forest fires as identified by ccl-6 together but maintained the distinction 13 between fires occurring in canada and russia. included in a dvm in a way that retains the model structure and fri, but is also 23 consistent with the size distribution of burned area observations (fig. 3) . even though 24 the ccl run adds random spatial variability to the fri, the average magnitude of fri 25 remains largely unaffected over sub-regions of both canada and russia."
"definition 1 (inaccessibility) [cit] a node v i in the digraph g(a,b) is called accessible if and only if there exists a directed paths reaching v i from an input vertex in v u, otherwise it is inaccessible."
"the mdss of a network are not unique, but the cardinality of the mdss are the same. based on the effect of removing a node to the cardinality of the mdss, the paper [cit] classified the nodes as indispensable, neutral or dispensable, which correlate to increasing, no effect, or decreasing the cardinality of the mdss of the network by removing the node and edges that connect to the node."
"requirement analysis is one of the important task to ensure successful data warehouse project [cit] . it collects and restructures base information that establish dw design and development of front-end application [cit] . it also analyzes business requirement to assist estimation of project scope and leads to ensconce project planning. sorting out requirement analysis phase properly and systematically will ensure effective dw implementation [cit] . basically, user requirement analysis approach can fall within four categories: data-driven, user-driven, goal-driven and mixed-driven approaches."
"this heavily skewed distribution assigns high probability to small fires and lower 22 probability to bigger ones. dataset (x, y, t) on which we apply the ccl algorithm."
"in principle, the 6-connected variety of ccl should be sufficient to capture fire available data on fire statistics in order to determine which was more appropriate."
"the statistical tests show that the ccl algorithm produces a histogram of forest fire 5 sizes closely matching that from the clfd, and it also produces a similar probability 6 function for canadian non-forest, especially for the categories containing larger fires. an assumption implicit in the statistical tests performed."
"nine algorithms for network controllability analyses are included in cytoctrlanalyser. to apply controllability algorithms, users simply need to check the algorithms they would like to use. for the state transittability and output controllability, the nodes whose states are going to be changed are required to be indicated. for mss with preference, the preference value of each node should be input. to indicate the customized data, the names of columns containing the corresponding data are required to be indicated in the text boxes on the right of the algorithm check boxes."
"following is a short quick start for the usage of cytoctrlanalyser: three algorithms, which are mss with preference, transittability and output controllability, require customized data of control settings. each of the three algorithms requires the user to indicate a column in the node table panel. the data types are listed:"
"there are few reasons that contribute to its failure; some of them are related to user requirement analysis. [cit] stated that requirement definition phase is paramount and impact almost every decision in dw project [cit] . most of failed dw projects execute poor requirement definition phase and some of them skip this phase [cit] . they tend to focus on technical issues and not paid enough attention to requirement analysis [cit] . groggy method of conducting requirement definition analysis will lead to unstable design process [cit] and poor communication between it people and decision maker [cit] . with lack of higher-level requirements vision [cit], it is difficult to ensure the project will meet business goal. furthermore, incomplete or inconsistence requirement lead developer to incorrect assumption [cit] . in other hand, with tremendous amount of users requirements from different levels and categories cause big challenge to specify important requirement that could lead to business goal [cit] ."
control centrality: control centrality of a node equals to the dimension of the controllable subspace or the size of the controllable subnetwork when a control signal is actuated only on the node [cit] .
"such fires usually last for several days or even weeks, and extend over large areas and nonetheless, the representation of fire in most dvms does not utilize eo information 13 and fails to capture many of the key fire characteristics [cit]"
"we investigated whether this variability in fri is caused by the short spin-up time of 27 the dvm (1000 years) compared to the long fri for the region (100-1000 years),"
"another mixed driven approach that combine all three approaches is triple driven approach [cit] . however, this approach proposes to execute all three basic approaches at same time and integrate all the result at final stage. using goal driven approach as guideline, project manager determines business goals. to ensure successfulness of dw implementation, project manager develops organization corporate strategy and identifies main business field within the organization. top management then identifies key performance indicator (kpi) that could help measure its achievement. at the same time at data driven stage, database administrator should identify data source that relevant with dw project and analyze it to produced subject oriented data schema. on the other hand, eliciting comprehensive requirements by interviewing end user and analyze business reports should be conducted. all documented information within all three stages is then merged to produce logical data schema for dw."
"to investigated the controllability of networks, the studies have formulated the problems in control theory to the graph theoretic problems based on the structural control theorem. therefore, nine network controllability algorithms are implemented based on classical graph-theoretic algorithms. the relationships among the network controllability algorithms and graphtheoretic algorithms are illustrated in fig. s10."
"on the other hand, user driven approach determines information requirement with stress involvement of business users [cit] . similar with data driven approach, it applies bottom up technique that collect information from specific resources, compiles and restructures it before validate with business goals. this approach starts with determining requirements of different business user at tactical level. to elicit information requirements, project manager has to deal with approaches that facilitate user participations [cit] . all user requirements shall be documented properly before integrate for dw requirements. integrated information requirements will be mapped to available data source before designing logical data schema."
"in cytoctrlanalyser, all implemented algorithms are based on networks with linear dynamics. although dynamics of biological systems are nonlinear, the controllability of nonlinear systems is in many aspects structurally similar to that of linear systems. first, investigating controllability of locally linearized system is the first step to ultimately develop control strategies for complex nonlinear networks [cit] . in addition, if a network is structurally controllable, then it is completely controllable for almost all possible parameter realizations [cit] . therefore, the structural controllability of linear system can provide a sufficient condition for controllability for most nonlinear systems [cit] . recently, by applying structural linear controllability theorems to nonlinear c. elegans neuron network, researchers predicted the involvement of each c. elegans neuron in locomotor behaviors and then verified their prediction by experiments [cit], which provided a directly experimental proof of the feasibility of developed structural controllability theorems. in this study, the dynamics of a network with n nodes is represented by the linear time-invariant dynamic model, which is described by the equation:"
"the data should be stored in a to import the customized data, a table file should be created. in fig. s2a, the first column is the names of nodes in cytoscape, which is used as the key to map the nodes and node attributes. the second column in fig. s2a is the preference values of nodes for the mss identification with preference. for cytoctrlanalyser, the preference values could be any positive numbers. the third column contains boolean values for nodes, which is used as the customized data for analysing the network transittability or output controllability. in fig. s2a, node 4, node 5 and node 6 are set as true. for network transittability, the states of node 4, node 5 and node 6 could be changed to any values at the end of control process while the states of other nodes would remain unchanged. for output controllability, the states of node 4, node 5 and node 6 could be changed to any values at the end of control process while the states of other nodes are not considered in the output controllability study."
"it is paramount to choose the right user requirement approach that might be suitable to organization environment. recognizing the strengths and weaknesses of each approach could assist the process of selecting suitable user requirement analysis that might contribute to successful data warehouse project. there are few advantages in applying data driven approach. first and foremost it could simplify etl design [cit] . since the requirements selection are based on data available in data source, it is easy to construct data warehouse schema [cit] and at the same time ensure data availability. data source schema is normally stable and does not change repeatedly. this will establish high stability in multidimensional schema [cit] compare with requirements based on end user needs. however there are few weaknesses of data driven approach. this approach is high risk of wasting resources to handle unneeded information structure [cit] . with abundant of information from data source, it is almost impossible to extract relevant requirements [cit] .this will diminish end user motivation to participate in design process especially when they need to work with large data model [cit] . in addition, it is possible that the result of multidimensional schema did not fit user requirements [cit] due to lack of end user involvement."
"in this paper, we have presented previous works in user requirement analysis in data warehouse design domain. the approaches for analysing user requirement have been surveyed and their strengths and weaknesses have been discussed. as a result, all basic approach has its own strengths and weaknesses. in order to avoid the drawbacks of single approach, few mixed-driven approaches had also been proposed. however, choosing appropriate approach should be done by tailoring it to organization environment and developer's skill. this paper compares mixed-driven approach in user requirement analysis and this will guide data warehouse designer in choosing the right and appropriate approach so that will enhance the success rate of data warehouse implementation."
"the new methodology for simulating fire disturbance in dvms described in this here is model-independent, a dvm with a daily fire step could be used, such as the however, the temporal characteristics of fires obtained by ccl could be assimilated burned area data from gfed4 over the arctic reveals that in a given year fires tend to"
"both the algorithms for transittablilty and output controllability require the users to indicate a column in node table with boolean values. the nodes whose states are intended to be changed by the users are assigned true values. the difference between investigating the structural transittablilty and structural output controllability is that when steering the states of selected nodes, the structural transittablilty does not change the states of other nodes while output controllability does not consider the states of other nodes."
"the user driven approach gives priority to user requirements [cit] and encourages high end user participation during dw design process [cit] . with this approach, dw is tailored exactly to end user's need [cit] and it is highly appreciated by business user [cit] . it minimizes risk of wasting resource [cit] as a result of mapping requirements with certain data available in data source. on the other hand, this approach requires large effort in designing etl [cit] and difficult in mapping user needs onto available data source [cit] . failure in mapping user needs will lead to user disappointments. in addition, it is a risk that end user normally not capable to specify requirements that could assist in achieving business goals because they don't have sufficient knowledge or understanding of whole business process and organizational visions. however, to ensure successfulness of eliciting and integrating user requirements, great effort from project manager with good moderation and leadership skill is required. alternatively, it is suitable to apply goal driven approach that will focus on goals and strategies of the company for an efficient decision making. this will maximize probability of correct identification of relevant indicator. however, it fully depends on the willingness of top management to involve in dw design process. table 1 reports comparison between mixed-driven approaches to organization environment, technical team and analysis output that involve in dw project. organization environment considered in this study are divided into level of involvement among top management and end user to provide user requirements. this is important characteristic to be considered in choosing appropriate approach because it will be the main input to dw project. another consideration is about time frame of project. most of dw projects require long term planning. however, it depends on the organization requirements and failure to deliver project on time could draw millions of dollars. second important consideration is the technical team skill. they should be able to deal with diagram notation and modelling technique complexity. finally is the output of approach that being classify into dw schema quality and ability to fulfil business goal. the main consideration in choosing suitable approach is the characteristic of organization itself. for example if the organization itself gets high commitment from top management but not from end user, it is suitable to choose grand approach that could lead to high quality of data warehouse schema and high probability in fulfilling business goals."
"these values are then fed into the above procedure to define the fires that are accepted the model. in the latter case, and since the process is stochastic, a different set of fires 13 will be produced but the fri will not change."
"dw itself can be discussed in many aspects, however in this paper we will concentrate on user requirement approach in data warehouse design. this paper had been organized as follows. section ii presents the user requirement analysis approach. section iii discusses the advantage and disadvantage of available approach. it also discusses comparison of proposed mixed approach towards organization environment, technical team skills and analysis outcome. finally, section iv present conclusion and future work on user requirement analysis approach for data warehouse."
transittability: each node should be assigned a boolean value which indicates whether the state of the node is going to be changed according to the control objective. nodes with true values could be changed to any states in finite time while states of nodes with false values would remain unchanged at the end of the control process.
"dw is a collection of decision support technologies, aimed at enabling knowledge worker to make better and faster decision [cit] . it is the heart of business intelligence (bi) system and contains all the information integrated from heterogeneous source into multidimensional schema to enhance data access for analysis and decision making. data from various sources go through tremendous extract, transform and loading (etl) process that clean, transform and modify data so that could match multidimensional schema in dw. high volume of data from multiple sources causes high probabilities of errors and anomalies. therefore, data needs to be transformed, scrubbed and audited before they are loaded into dws."
"when running the algorithms, cytoctrlanalyser acquires network information from cytoscape and create a copy of the network, which is stored in the form of adjacent list. therefore, all graph-theoretic algorithms are implemented based on the adjacent list in cytoctrlanalyser, which would enable the control- lability algorithms to run on large networks and have higher efficiency for sparse networks."
"the unique features of dw had exhorted different dw methodologies and design. in dw lifecycle, one of most critical phase that might influence all other phases in dw development is the requirement analysis phase. it is a process of gathering requirement from end user. since dw end users might vary, it is important to identify accurate end users that will represent requirements in different point of views. the predominant objective of this phase is to identify organization goals and elaborate requirements that could measure organization performance. however, dw projects are inherently risky and some of them failed in the implementation. studies shows that more that 80% of dw project fail to fulfill business goals [cit] ."
"fieldwork and earth observation (eo) data show that larger fires contribute most to 27 the total area burned, despite being much rarer. [cit], out of the 1,000-28 14,000 forest fires that occur every year in canadian forests, only 3% exceeded 2 km 2"
"to achieve the functions of cytoctrlanalyser, there are three main functions modules in the cytoctrlanalyser, which are listed below. 3. algorithm module: there are two parts in this module. the first part includes a group of algorithms related to the network control. since the network control problems are formulated as graph-theoretic problems, the second part includes several classical algorithms in graph theorem which are called by the controllability algorithms."
"probability in an mss: similar to the non-uniqueness of mdss of networks, there are different msss of a same network. this algorithm quantifies the probability of each node appearing in a random mss [cit] . nodes with higher probabilities in an mss have been suggested to play more important roles in controlling the network. in addition, by using this algorithm, we have been able to compare preference values of a randomly selected mss and an mss selected with preference."
1) binarizing ('im2bw' function in matlab) theoretical analysis of droplet size with respect to flow rates in a t-junction which is similar to the y-junction was published previously [gfsw06] . that study identified a scaling law for droplet length (l) with respect to flow rate:
an artificial psychological resilience mechanism for supporting investment decision-making processes in the stock market domain was presented. the resilience mechanism required the inclusion of an artificial emotional dimension within a decision algorithm for investment decisions and was tested considering free-access data from the standard & poor's 500 index.
"we have implemented our technique in a tool named bingo and evaluate it using two state-of-the-art analyses: a static datarace analysis [cit] on a suite of 8 java programs each comprising 95ś616 kloc, and a static taint analysis [cit] on a suite of 8 android apps each comprising 40ś 98 kloc. we compare bingo to two baselines: a random ranker, base-r, in which the user inspects each alarm with uniform probability, and a ranker based on a sophisticated alarm classification system [cit], base-c. on average, to discover all true alarms, the user needs to inspect 58.5% fewer alarms with bingo compared to base-r, and 44.2% fewer alarms compared to base-c. we also conduct a user study with 21 java programmers and confirm that small amounts of incorrect user feedback do not significantly affect the quality of the ranking, or overly suppress the remaining true alarms."
"during the last decade, to improve the efficiency and the effectiveness of systems that operate in both public and private ambits, several technical and research works have been proposed, such as those aiming to improve logistics processes and e-commerce [cit], to analyze urban demand-responsive transportation [cit], and to improve learning processes [cit] ."
"we concretize these ideas in the context of analyses expressed in datalog, a logic programming language which is increasingly used to declaratively specify complex program analyses [cit] . inspired by the literature on probabilistic databases [cit], we develop a technique to convert datalog derivation graphs into bayesian networks. computing alarm confidences can then be seen as performing marginal inference to determine the posterior probabilities of individual alarms conditioned on user feedback. by embedding the belief computation directly into the derivation graph, our technique exploits correlations between alarms at a much finer granularity than previous approaches."
"where a, b and c are constant matrices with appropriate dimensions; the model obtained by minimal realization of g(s). v and w are unknown constant vectors that can represent constant model uncertainties or slowly-varying process disturbances [dr12] . by substituting the control input into (s.16) and subtracting the previous time-step equation from (s.16) and (s.17), the model can be rewritten as therefore, closed-loop dynamics to which the proposed controller is applied can be represented by the augmented state model as"
the transient response (not steady-state response) can be tuned by using a single scalable parameter α to alter the original gain matrix g † ss as follows:
"we compare bingo to two baseline algorithms, base-r and base-c. in each iteration, base-r chooses an alarm for inspection uniformly at random from the pool of unlabelled alarms. base-c is based on the alarm classifier eugene [cit] . we describe its operation in appendix c."
"considering the idea that an artificial emotion can be defined as a numeric continuous variable, it is necessary to establish some conditions for activating the resilience mechanism. in this sense, in the present research work, the essential activation condition corresponds to achieving an emotional threshold that is within the range of the negative valence emotional intensity. fig. 2 shows an emotional function achieving the value of −1.0 in period 8. since the resilience activation threshold was defined by −1.0, then the artificial psychological resilience mechanism is activated. the activation of the resilience mechanism supposes a trajectory change in the emotional function. fig. 2 graphically shows that from period 8 there is a resilient trajectory that is a consequence of the resilience mechanism's activation and another nonresilient trajectory, which is assumed to be free to overcome the limits of the resilience activation (below the resilience trajectory). since artificial affectivity is incorporated into a decision algorithm, the change of the emotional intensity valence along time becomes relevant for measuring its effect on the decisions that are made. the use of a resilience mechanism does not ensure that the decisions that are made over time will always be better than the decisions that are made without using some resilience mechanism or emotional containment. in this sense, is interesting for the present research work to explore and analyze the potential benefits of the incorporation and use of such a resilience mechanism for decision-making."
"parameter e xt represents the emotional value for an artificial emotion ''x'' in period ''t''. that is, it corresponds to a negative valence peak. an arctangent function is used since as the negative emotional feeling becomes more intense, the emotional recovery process becomes more difficult. in other words, using an arctangent function allows one to obtain a more accurate recovery value based on the observed emotional state."
"in contrast, other works related to resilience address the psychological approach [cit] and considering a broad spectrum of application domains, such as the following: health professionals [cit], resilience at work [cit], resilience and immunity [cit], resilience and sports [cit], health professional students [cit], resilience in teenagers [cit], resilience in young students [cit], and resilience and cyberterrorism [cit], among others."
"pressureregulator(u c ); 15: end while; measured flow rate is allocated to y m . at line 6, control input is computed as presented in (4). from line 7 to line 12, the control input is evaluated by the saturation function in (6). at line 14, the computed control input is sent to the pressure regulator to insert control input pressure to the microfluidic chip."
"the joy-sadness pair of emotions is defined within a continuous interval [−10,10], where 10 represents joy, −10 represents sadness, and 0 (zero) represents a neutral emotional state. at the beginning of the investment process [cit], a neutral emotional state was considered. in relation to the activation threshold of the resilience mechanism, during all investment periods, the threshold has a value of −1.0. the above means that sadness increases (that is, emotional neutrality is lost by the increasing the negative emotional valence), and the resilience mechanism could be activated if the threshold (−1.0) is reached."
"another line of future work corresponds to the implementation of a resilience mechanism adapted to other domains or decision scenarios. for example, the mechanism can be used in intelligent tutoring systems that teach different disciplines, decision-making assistants in management scenarios (e.g., decisions in project management), or customer relationship management systems (e.g., smart chatbots)."
"the droplet size distribution was obtained using a self-developed image-processing algorithm. droplets generated in the y-junction network have the same vertical length as the channel width (w), but their horizontal length (l) depended on both flow rates. the image processing algorithm to compute the droplet size (horizontal length) proceeds as follows:"
"there are multiple approaches to face simulation scenarios. in the present research work, the arma model approach was considered due to the wide use of such models in the literature for the definition of the stochastic processes that govern the financial price series [cit] . the arma model, in the definition of the stochastic processes, is presented as a standard to contrast the results of the strategies followed by the different artificial investors in a simulated process; however, it is not part of the algorithm for supporting investment decision-making. table 5 shows the observed investment capital at the end of each year. [cit], 10.000 simulations were carried out. in this sense, the values in table 5 for the ri, nri and tfi (for each investment year) correspond to the simple average of 10.000 runs and the obtained experimental results."
"in addition, several factors impact the valuation of a specific emotion. for example, receiving a prize or congratulations can be a reason to increase the level of joy, experiencing uncertainty or volatility may be grounds for decreasing the level of confidence, and being the subject of an act of injustice can be a reason to increase the level of anger. in this sense, when considering an artificial emotion as a numeric continuous variable, it is necessary to define a mechanism that updates the valence of an artificial emotional variable. the present research work defines an update function according to equation (1):"
"mle by expectation maximization (em). in our setting, mle may be performed by a straightforward implementation of the em algorithm [cit] . starting with an arbitrary seed p (0), the algorithm iteratively computes a sequence of estimates"
"ac: adjustment constant. table 1 shows a resilient algorithm for supporting artificial investors in investment processes. first, it is necessary to initialize the emotional profile of an artificial investor. then, the investment parameters are defined (e.g., the amount of investment capital and the investment horizon). subsequently, the data set is obtained, and the investment strategy is defined. an example of an investment strategy defines pre-selected candidate stocks that are to be chosen using the dow jones industrial average [cit] . there are several strategies to face investment processes in stock market [cit] . first, passive strategies suggest tracking investment portfolios configured by other investors. thus, it is possible to track a specific stock index, or also set up an investment portfolio that replicates the behavior of a stock index. meanwhile, there are discretionary trading investment strategies, where an investor takes active investment positions through the configuration of investment portfolios using historical market information. within this type of investment strategies are classic models such as markowitz, capm, apt (extension of capm). other examples of investment strategy are: ''value'', in which the stocks undervalued by the market are identified; ''momentum'', which identifies the stocks with the best performance in a specific period of time; ''size'', in which stocks of relatively small companies are acquired and stocks of relatively large companies are sold; ''multi-factor'', which combines the strategies described above."
"the parameter ufa is an update factor and its value is static during all periods. ufa is represented by a continuous value that is defined between [cit] . when ufa is 1, the upperlimit parameter reaches the maximum possible value. if only the last equation is used, the final value that is obtained by the emotional updating process always will be deterministic since there are no random parameters. since human emotions are essentially non-deterministic, it is necessary to incorporate flexibility in each emotional update process. fig. 3 shows a third emotional behavior titled ''updated emotional value'', which represents a flexible emotional behavior. to generate this flexible emotional behavior, a random component is used to obtain a final updated emotional value. this random component adds a degree of freedom for the emotional reaction when the resilience mechanism is activated. the random component is generated within a range that is composed by lower and upper limits, which are represented by the lowest negative limit (ext) and by the maximum value of emotional recovery (upperlimit), respectively. in other words, the random value needs the maximum emotional amplitude that is available, which is calculated according to equation (3):"
"further out, there is a large body of work on using statistical techniques for mining likely specifications and reporting anomalies as bugs (e.g., [cit] ) and for improving the performance of static analyzers (e.g., [cit] )."
we conducted all our experiments on linux machines with 3.0 ghz processors and 64 gb ram running oracle hotspot jvm 1.6 and libdai version 0.3.2. we set a timeout of 2 hours per alarm proposed for both bingo and the baselines.
"we will now discuss how we may obtain p. with a corpus of fully labelled data, the rule probability is simply the fraction of times the rule produces incorrect conclusions from true hypotheses. with a less extensively labelled dataset, one faces the challenge of latent (or unobserved) variables, which can be solved with the expectation maximization (em) algorithm described below. however, both of these techniques require a large corpus, and a proper experimental evaluation involves partitioning the data into training and test sets. to avoid this problem in our experiments, we uniformly assign each rule to a probability of 0.999."
"diverse forms of program reasoning, including program logics, static analyses, and type systems, all rely on logical modes of deriving facts about programs. however, due to classical reasons such as undecidability and practical considerations such as scalability, program reasoning tools are limited in their ability to accurately deduce properties of the analyzed program. when the user finally examines the tool output to identify true alarms, i.e. properties which the program actually fails to satisfy, her experience is impaired by the large number of false alarms, i.e. properties which the tool is simply unable to prove."
"each table shows the disaggregated estimation parameters for each year of operation. the column indicates the year in which the estimated parameters were obtained using the data that were collected during that year, which is the information that is relevant for the simulation of the investment process of the following year."
"on the other hand, even though the close() method may also be simultaneously invoked by multiple threads on the same requesthandler object, the atomic test-and-set operation on lines l1śl3 ensures that for each object instance, lines l4śl7 are executed at most once. there is therefore no datarace between the pair of accesses to controlsocket on lines l4 and l5, and similarly no datarace between the accesses to request (lines l6 and l7)."
"flow rates of each channel converged simultaneously on the desired flow rates (fig. s.3a ) as a result of rapid adjustment of corresponding pressure inputs (fig. s.3b) . in all channels, the flow rate overshot the target rate, then converged on it by damped oscillations (fig. s.3c -"
"in the industry there is a widely used investment strategy called ''stop loss'', which comes from the technical analysis and considers that the investor defines a lower limit on the price of a financial asset. once that lower limit is reached, the financial asset acquires the status of ''saleable''. on the other hand, the resilience implementation proposed in the present research paper suggests the use of a mechanism for restoring artificial emotional variables within an artificial autonomous system for stock market. in this sense, once a weakened emotional scenario is detected, the resilience mechanism is activated to restore that state. considering all above, both strategies operate on different variables (stop loss operates on the price, and the resilient mechanism operates on emotions), and in addition, they act in different circumstances: stop loss acts based on a direct instruction from a human investor, who sets a lower price limit for a financial asset; on the other hand, the resilience mechanism uses lower limit on the emotional state of an artificial autonomous system. thus, the reaction of the artificial autonomous system becomes more weighted and gradual according to the emotional reactions observed (which are generated by prices fluctuations)."
"we formally describe the workflow of the previous section in algorithm 1. we use an off-the-shelf solver [cit] for the conditional probability queries in step 7(b). in this section and the next, we discuss the main technical ideas in our paper, spanning lines 3ś5."
"in this section, we introduce a more complex microfluidic network ( fig. s.1 ) than the y-junction network presented in the paper. this case study that uses the complex network will clarify 1) construction of the steady-state gain matrix, 2) selection of controllable channels, 3) implementation of the proposed controller, and 4) stability of the closed-loop system."
"nevertheless, regarding the technological solutions that are used in the stock market domain, several commercial software platforms are available for supporting investment processes [cit] . these commercial software platforms operate both online and offline and make different kinds of investment decisions according to the investment parameters that are defined by a real human investor. in this sense, all the investment decisions that are made by these commercial software platforms must strictly comply with the specifications that are defined by a human investor. in other words, commercial software platforms are not able to use their own decision criteria to make investment decisions, which represent a non-autonomous behavior. in addition, these platforms do not consider the affective variables within their own internal decision criteria. thus, there is no stock market domain commercial software platform that considers both autonomous behaviors when performing investment decision-making processes and the inclusion of a psychological approach to resilience within the investment behavior."
"one potential concern with tools such as bingo is that mislabelled alarms will deteriorate their performance. furthermore, concurrency bugs such as dataraces are notoriously hard to diagnose. however, in the user study described in section 5.1, we observed that when a group of professional programmers are made to vote on the ground truth of an alarm, they are able to correctly classify 90% of the alarms. we extrapolated the results of this study and simulated the runs of bingo on ftp where the feedback labels had been corrupted with noise. in table 6, we measure the ranks at which 90% and 100% of the alarms labelled true appear. as is expected of an outlier, the rank of the last true alarm degrades from 103 in the original setting to 203 in the presence of noise, but the rank at which 90% of the true alarms have been inspected increases more gracefully, from 80 originally to 98 in the presence of 10% noise. in all cases, bingo outperforms the original base-c. we conclude that bingo can robustly tolerate reasonable amounts of user error."
"this section includes the following: an explanation of the design of artificial emotions, an explanation of the artificial psychological resilience mechanism's design, and, finally, an explanation for the inclusion of the mentioned resilience mechanism within a decision algorithm for investment decisions."
"the analysis takes the relations n(p 1, p 2 ), u(p 1, p 2 ), and a(p 1, p 2 ) as input, and produces the relations p(p 1, p 2 ) and race(p 1, p 2 ) as output. in all relations, variables p 1 and p 2 range over the domain of program points. each relation may be visualized as the set of tuples indicating some known facts about the program. for example, for the program in figure 1, n(p 1, p 2 ) may contain the tuples n(l1, l2), n(l2, l3), etc. while some input relations, such as n(p 1, p 2 ), may be directly obtained from the text of the program being analyzed, other input relations, such as u(p 1, p 2 ) or a(p 1, p 2 ), may themselves be the result of earlier analyses (in this case, a lockset analysis and a pointer analysis, respectively)."
"another possible future line of work corresponds to extending the scope of the current experimental scenario. this work can include the following: incorporating more artificial investors, adding other capital market indexes within the experimental scenario (e.g., shanghai, nikkei, dax, and ftse), and adding special information for the markets (i.e., signals from the central banks of countries or global regulatory entities)."
"with the maxamp value, it is possible to determinate the final updated emotional value. by considering the upperlimit value as an initial point, the updated emotional value ext' (artificial emotion ''x'' in period ''t'') is calculated according to equation (4): fig. 4 presents four different scenarios for the final updated emotional value. each scenario is based on a specific value of the random component. when the random component is zero or the maxamp, the final updated emotional value corresponds to the upperlimit or to the lowest negative limit (ext), respectively. in other cases, the final updated emotional value is defined within the emotional limits that are mentioned above."
"incomplete analysis rules are the principal cause of false alarms: even though p(l4, l2), n(l2, l3) and u(l4, l3) are all true, it is not the case that p(l4, l3). bingo addresses this problem by relaxing the interpretation of clause nodes, and only treating them probabilistically:"
"the running time of bingo could preclude its integration into an ide. this limitation could be alleviated through solver improvements and modular reasoning techniques for large programs [cit] . secondly, we restrict ourselves to features already present in the analysis, which could potentially limit generalization from user feedback. finally, early stopping criteria, such as those mentioned in section 2.5, affect soundness and raise the possibility of missed bugs."
"to answer q3, we conducted a user study to measure the fraction of alarms mislabelled by professional programmers. we placed an advertisement on upwork.com, an online portal for freelance programmers. we presented respondents with a tutorial on dataraces, and gave them a small 5-question test based on a program similar to that in figure 1 . based on their performance in the test, we chose 21 of the 27 respondents, and assigned each of these developers to one of the benchmarks, hedc, ftp, weblech, and jspider. we gave them 20 alarms for labelling with an 8ś10 hour time limit, such that each alarm was inspected by at least 5 independent programmers. to encourage thoughtful answers, we also asked them to provide simple explanations with their responses. we found that, for 90% of the questions, the majority vote among the responses resulted in the correct label."
"r: it corresponds to the expected profitability vector of each stock that belongs to the portfolio. it is important to mention that to calculate the expected profitability of a stock, the historical profitability is considered."
"it is generally accepted that psychological resilience is important for mental health and well-being [cit] . however, to the best of our knowledge, there are no research studies that examine the psychological approach to resilience for the stock market domain, particularly with respect to the investment decisions of individual investors. the evidence of the above is the extensive literature that is related to the stock market domain that is associated with technical and fundamental analysis [cit] where the studies that are related to explicit sentiments within decision-making processes on stock markets have a secondary minor role."
"the experimental scenario considers that all artificial investors have the same information about the prices and volatilities of stocks and start their investment processes at the same time. in the cases of the ri and nri, the composition of the investment portfolio is determined using the algorithm that is described in table 1 . since the tfi is limited to investing in the s&p500 index, it does not build an investment portfolio."
"four, the joy-sadness values exclusively depend on the observed stock price variations. in real world, joy-sadness is affected by many more dimensions than those considered in this simulation scenario (some examples): at market level, market liquidity, country risk, production and manufacturing indicators; at the international level, policy contexts such as the usa-china trade war or brexit in europe; or on the other hand, variables related to the personal sphere (personality profile, personal affective life). considering the above, the focus of the present research work is to define and test the use of a resilience mechanism to contain and restore the emotional state of an artificial autonomous system for stock market. in this sense, the use of additional information (such as the comments available on social networks) can be incorporated into a subsequent version of the present study, for example: for analyzing different mechanisms devoted to update emotional variables (giving different degrees of relevance to personal variables, related third party recommendations, and open comments available on social networks); to define other approach of an emotional restoring mechanism that considers the formation of an ''external emotional scenario'' derived from the state of financial markets, when the comments available on social networks can have a central role in obtaining a market perception."
"we may use a static analysis to find dataraces in this program. however, due to the undecidable nature of the problem, the analysis may also report alarms on lines l4śl7. in the rest of this section, we illustrate how bingo generalizes from user feedback to guide the analysis away from the false positives and towards the actual datarace. figure 2 shows a simplified version of the analysis in chord, a static datarace detector for java programs [cit] . the analysis is expressed in datalog as a set of logical rules over relations."
"algorithm 1 bingo(d, p, p), where d is the analysis expressed in datalog, p is the program to be analyzed, and p maps each analysis rule r to its firing probability p r ."
"in summary, given an analysis and a program to be analyzed, bingo takes as input the set of tuples and grounded clauses produced by the datalog solver at fixpoint, and constructs the belief network. next, it performs bayesian inference to compute the probability of each alarm, and presents the alarm with highest probability for inspection by the user. the user then indicates its ground truth, and bingo incorporates this feedback as evidence for subsequent iterations. we summarize this process in figure 5 . there are several possible stopping criteria by which the user could cease interaction. she could choose to only inspect alarms with confidence higher than some threshold p 0, and stop once the confidence of the highest ranked alarm drops below p 0 . alternatively, she could choose to only inspect n alarms, and stop after n iterations. of course, in all these situations, we would lose any soundness guarantees provided by the underlying analysis, but given the large number of alarms typically emitted by analysis tools and the time constraints frequently placed on developers, and as evidenced by our experiments in section 5, bingo promises to form a valuable component of the quality control toolchain."
"we can arbitrarily select controllable channels by selectively eliminating rows of g ss . if we want to control q 2, q 3 and q 5, then the 1-st, 4-th, 6-th, 7-th and 8-th rows of g ss should be removed as where the flow rates inside channel 2, 3, 5 can be independently controlled while the others will be determined by their dependency on the controlled flows. any three channels in the network can be simultaneously controlled by this method."
"we present the number of alarms produced and the number of bugs missed by each analyzer in table 5 . for example, by turning on the unsound options, we reduce the number of alarms produced by chord from 522 to 211, but end up missing 39 real dataraces. using bingo, however, a user discovers all true alarms within just 103 iterations, thereby discovering 108% more dataraces while inspecting 51% fewer alarms."
µ: it is a value that is defined by an investor that corresponds to a final profitability goal that is derived from the investor's own investment portfolio.
"considering all of the above, the current research work tries to extend the available knowledge by exploring the effects of the incorporation of an artificial psychological resilience mechanism within an artificial autonomous system that is devoted to making investment decisions in the stock market domain. the theoretical value of the current research work is due to analyzing the psychological resilience of an artificial autonomous system, which is something that, to the best of our knowledge, has not been studied in the literature. meanwhile, the practical relevance of the current research work lies, first, in the evaluation of the effectiveness of decisions that are made in the stock market domain by an artificial autonomous system with psychological resilience, and, second, in the potential applicability of an artificial autonomous system with psychological resilience in other decision environments. in this sense, several domains such as education (through the use of intelligent tutoring systems with psychological resilience to perform teaching-learning processes) or emergency and social crisis management (through the use of software to simulate individuals' resilience and behavior in the face of a crisis) could benefit from the results of the current research work."
"here, we have the following: e xt : value of emotion x in period (t), e xt−1 : value of emotion x in period (t-1), ef : variation of the emotional factors between the previous period (t-1) and the current period (t), and rand[0; 1]: uniform random value between 0 and 1."
"regarding the limitations of the present research work, first, the experiments exclusively considered stocks belonging to the mentioned investment index. in this sense, a possible future research line would be to test the resilience mechanism using more diverse data. these data would extend the evaluation period and also incorporate the globality of companies operating on the nyse composite. second, the experiments only used numerical data from the standard & poor's 500 index and did not consider other ''environmental'' antecedents (e.g., information about country risks and signals coming from ''trade wars'', among others)."
"after choosing an investment strategy, an initial investment portfolio is defined considering all the previous steps. while the last investment period has not been reached, the investment indicators of the portfolio are obtained, and the performance of the investment portfolio is verified. the last step involves updating the emotional state of the artificial investor using equation (1) . if emotional assistance is required, then the resilience mechanism is activated, forcing emotional stabilization (using equation (5))."
"we evaluated bingo on the suite of 16 benchmarks shown in table 3 . the first four datarace benchmarks are commonly used in previous work [cit], while we chose the remaining four from the dacapo suite [cit], and obtained the ground truth by manual inspection. the eight taint analysis benchmarks were chosen from the stamp [cit] repository, and are a combination of apps provided by a major security company, and challenge problems used in past research."
"in general, there are two application domains that can benefit from the results of the present research work. the first is decision scenarios in which people can delegate decision-making to artificial autonomous systems (where these systems incorporate artificial affectivity). the second is scenarios in which it is necessary to understand human behavior, and computational simulation technology is used to achieve this (where this technology incorporates the affective dimension within its internal processes)."
"technological development systematically advances systems to which humans can delegate decision-making. for example, autonomous transportation systems based on agents were being designed about a decade ago [cit], and it is currently possible to observe real advances of autonomous vehicles and systems in urban transportation [cit] . in this sense, the incorporation of the affective dimension into autonomous decision-making systems becomes important to complement rational and objective criteria with other emotional and subjectivity aspects that are natural in humans, which allows one to form and configure a rational-emotional perspective closer to how a human would make a decision. likewise, it is necessary to define mechanisms for the evaluation and control of artificial affectivity in order for both the system behavior and the effectiveness of each decision that is made by the system to comply with the quality metrics and performance criteria. the above reinforces the central objective and the contribution of the present research work. that is, this paper analyzes new mechanisms and criteria for promoting and guiding the development of technology that incorporates artificial affectivity in autonomous decision-making systems. in this way, people can progressively increase their level of confidence in technological systems, and, in particular, in affective artificial systems that face real decision scenarios from a rational-emotional perspective. these systems would then have the competence to manage the high complexity existing at present and have the ability to face and overcome adverse decision scenarios. the above description can be represented by a resilient artificial autonomous system."
"tables 2, 3 and 4 show the statistical parameters of the simulation for the ri, nri and tfi. it is considered that the time series of the returns of the portfolios that are associated with the ri, nri and tfi profiles follow a process that is based on an second-order autoregressive model with a moving average of order 0 (arma(2,0) ). this implies that the profitability data that are observed in period t are determined by the following: a constant, the observed profitability in period t − 1 (l.ar), the observed profitability in t − 2 (l2.ar), and a random number of gaussian characteristics. n corresponds to the number of observations that are used to estimate the simulation parameters for each year."
"consider the java program in figure 1 dataraces are a common and insidious kind of error that plague multi-threaded programs. since getrequest() and close() may be called on the same requesthandler object by different threads in parallel, there exists a datarace between the lines labelled l0 and l7: the first thread may read the request field while the second thread concurrently sets the request field to null."
"progressively, different levels of intelligence and analysis and degrees of autonomy have been incorporated within information systems that are oriented to support decision-making processes [cit] . in particular, the inclusion of affective criteria or variables within decision-making systems represents a promising line of action. in this sense, in the capital market domain, some proposals have been presented, such as those aiming to model the knowledge of the stock market using an affective-oriented ontology [cit], to support decision making by incorporating artificial emotions within an investment decision model [cit], and to control the emotional fluctuation of an artificial investor within an investment scenario [cit] ."
"the resilience mechanism can be activated only when the resilience activation threshold is achieved (always with the negative valence). for example, when considering a stock market decision scenario, if an investment portfolio's value has been decreasing over several investment periods, the valence of an artificial emotional variable can decrease its value. when using trust-fear as an artificial emotion, continuous losses along time can promote the tendency to feel fear. that is, it continues from a positive valence of emotional intensity (trust) to a negative valence of emotional intensity (fear). when the resilience activation threshold is reached, an artificial resilience reaction is generated. fig. 3 shows an example of emotional behavior for an artificial emotion ''x'' when the artificial resilience mechanism is activated. the line that is titled ''negative emotional peak'' represents several possible threshold points for an artificial emotion ''x'', which are points where the resilience mechanism can be activated. in addition, the line that is titled ''max emotional recovery'' represents the maximum recovery value that an artificial emotion can reach, which is associated with its specific negative valence peak. in other words, the maximum emotional recovery has an upperlimit, which is calculated according to equation (2):"
"to build an investment portfolio, markowitz's mean-variance portfolio theory is considered [cit], and an efficient frontier of available portfolios is defined. to calculate each point belonging to this efficient frontier, it is necessary to minimize the standard deviation (risk) of the portfolio, subject to a certain expected return, as follows:"
"to capture this dynamical behavior, we consider the interaction process as represented by the roc curve [cit] . we plot this for the ftp benchmark in figure 8 . the x-and y-axes indicate the false and true alarms present in the benchmark, and each point (x, y) on the curve indicates an instant when the user has inspected x false alarms and y true alarms. at this step, if the system proposes a true alarm, then the next point on the curve is at (x, y + 1), and otherwise, the next point is at (x + 1, y). the solid line is the curve for bingo, while the dotted lines are the ranking runs for each of the runs of base-c, and the diagonal line is the expected behavior of base-r. observe that bingo outperforms base-c not just in the aggregate, but across each of the individual runs."
"in this paper, we fundamentally extend program analyses comprising logical rules with probabilistic modes of reasoning. we do this by quantifying the incompleteness of each deduction rule with a probability, which represents our belief that the rule produces invalid conclusions despite having valid hypotheses. instead of just a set of alarm reports, we now additionally obtain confidence scores which measure our belief that the alarm represents a real bug. furthermore, we are able to consistently update beliefs in response to new information obtained by user feedback. by intelligently selecting reports to present to the user for inspection, and by incorporating user feedback in future iterations, we have the potential to greatly improve the practical utility of program reasoning tools."
step 4 of algorithm 1 is the result of combining the two optimizations we will now describe. we discuss additional engineering details involving the belief propagation algorithm in appendix b.
"recall, from algorithm 2, that we write c д for the conclusion of a grounded clause д, and a д for the set of all its antecedents. informally, a tuple is useful if it is either itself an alarm, or can be used to produce a useful tuple. the pruned set of clauses may finally be defined as follows:"
"from a psychological perspective, resilience has been analyzed as a relevant and necessary element prior to the occurrence of an important event or circumstance (e.g., military operations), as a permanently necessary element for the living conditions of an individual (e.g., stressful work), or as a the associate editor coordinating the review of this manuscript and approving it for publication was vincenzo piuri . necessary element after the occurrence of an important event or circumstance (e.g., natural disasters) [cit] ."
"the psychological approach to resilience considers the human affective dimension as a central component within human behavior and decision-making. in this sense, to design and implement a resilient artificial autonomous system, it is necessary to incorporate artificial emotions as a synthetic representation of real human emotions. since the affective dimension influences human decision-making processes [cit], the observation, recording and analysis of the artificial emotions of an autonomous system becomes relevant since it allows us to explore the possible relationships between the emotional state of the decision-making system and the effectiveness of the decisions that are made by system and understand the effectiveness as the degree of compliance with specific defined goals. for example, in the stock market domain, decision effectiveness can be understood as the increase of investment capital in a specific period."
", where gc is the set of grounded constraints, c is the set of conclusions, and a is the set of alarms produced by the analysis."
"to incorporate artificial emotions within an artificial autonomous system, it is necessary to define the variables that represent specific emotional dimensions (as a synthetic representation of real human emotions), and they can be updated according to the fluctuations and conditions of the decision scenario. thus, it is possible to define an artificial emotion as a numeric continuous variable that, starting from a neutral state (represented by a zero value), can have different values over time depending on the emotional reactions that are observed. for the purposes of the current research work, if the observed value of an artificial emotion variable is more than zero, it will be understood that the emotion has a positive valence. that is, it corresponds to a positive manifestation of an emotion. conversely, if the observed value of an artificial emotion variable is less than zero, it will be understood that the emotion has a negative valence. that is, it corresponds to a negative manifestation of an emotion. to illustrate the above, considering the artificial emotion that is defined as ''trust-fear'', if the value of this variable is greater than zero, it is possible to affirm that the system is ''confident''. conversely, if the value of this variable is less than zero, it is possible to affirm that system is ''afraid''. this functional schema can operate for other artificial emotions (e.g., joy-sadness and tranquility-angry). fig. 1 graphically illustrates the behavior of an artificial emotion defined according to the above explanation. in this sense, it is possible to affirm that major confidence exists at time 3, and major fear exists at time 9. since the present research work defines an artificial emotion as a variable with a ''dual'' nature, it is possible to affirm that if its valence is positive, it implies that some degree of ''confidence'' is observed, and, therefore, there is the absence of ''fear''. conversely, if the valence of the emotional variable is negative, it implies that some degree of ''fear'' is observed, and, therefore, there is the absence of ''confidence''."
"the second problem involves finding an optimal strategy in a markov decision process [cit] with o ((n + 1)!) states. bingo may be viewed as employing a greedy heuristic in this process, and we plan to investigate this significantly more challenging problem in future work."
"the rules are intended to be read from right-to-left, with all variables universally quantified, and the :− operator interpreted as implication. for example, the rule r 1 may be read as saying, łfor all program points p 1, p 2, p 3, if p 1 and p 2 may execute in parallel (p(p 1, p 2 )), and p 3 may be executed immediately after p 2 (n(p 2, p 3 )), and p 1 and p 3 are not guarded by a common lock (u(p 1, p 3 )), then p 1 and p 3 may themselves execute in parallel. ž"
"it is well-known that alarms produced by program reasoning tools are correlated: multiple true alarms often share root causes, and multiple false alarms are often caused by the tool being unable to prove some shared intermediate fact about the analyzed program. this raises the possibility of leveraging user feedback to suppress false alarms and increase the fraction of true alarms presented to the user. indeed, a large body of previous research is aimed at alarm clustering [cit], ranking [cit], and classification [cit] ."
"by eliminating rows that will not be controlled, g ss becomes a full row rank matrix that always has a right inverse matrix. for this case, the right inverse of g ss is y m ← getflowsensors();"
"the closed-loop transient response increases as α increases, and slows as α decreases. this tuning parameter can be used if fast response is required. also, as we mentioned in text s.1.6, the closedloop response can be unstable in the fast discrete implementation. for this case, α can be used to adjust unstable poles to be stable."
"in summary, we conclude that bingo is indeed effective at ranking alarms, and can significantly reduce the number of false alarms that the user needs to triage."
"each artificial investor has us$10,000 of investment capital. [cit] (inclusive). at the end of each investment year, the artificial investors withdraw their profits (or materialize their losses). at the beginning of a new investment year, each investor makes an investment equivalent to us$10,000."
"another possible future line of work corresponds to extending the current resilience mechanism to support decision-making processes, including adding other additional criteria for activating the resilience mechanism, adding other methods or criteria to activate the emotional transition, or incorporating fuzzy logic. as example, the sharpe ratio could be used as a mechanism for updating emotional variables; or also, to use the sharpe ratio as a criterion for activating the resilience mechanism (e.g., that the variation in a specific magnitude on the sharpe ratio can activate the resilience mechanism)."
"here, we define the variables as follows: ef : variation of emotional factors, between the previous period (t-1) and the current period (t). ef is related to the variation of stock prices that belong to investment portfolio."
"we view bingo as the first step in a program to integrate probabilistic techniques more deeply into traditional software engineering tools. our immediate problems of interest include incrementality (how do we carry forward information from one version of the program being analyzed to the next?), and improving accuracy by considering program features which are invisible to the analysis (such as variable names and results of dynamic analyses). in the longer term, we plan to investigate various non-traditional uses of bingo, such as during analysis design, to obtain data-driven insights into otherwise non-statistical analyses."
"another possible future line of work is to use artificial investors with different personality profiles, where each personality profile includes a different resilience profile. in this way, the effects of different levels of resilience on the effectiveness of decision making could be comparatively analyzed."
"in this paper, we presented bingo, a static analysis tool which prioritizes true alarms over false ones by leveraging user feedback and by performing bayesian inference over the derivation graph. we demonstrated significant improvement over two different baselines and across several metrics on two instance analyses and a suite of 16 benchmark programs."
"in an attempt to control the number of alarms produced, users are drawn to precise static and dynamic analysis tools, which promise low false positive rates [cit] . however, as evidenced by missed security vulnerabilities such as heartbleed [cit], precise tools are necessarily unsound, and often miss important bugs. to check whether bingo can help in this situation, we ran two state-of-the-art precise datarace detectors: chord [cit] with unsound flags turned on, and fasttrack [cit], a dynamic datarace detector based on the happens-before relation. we ran fasttrack with the inputs that were supplied with the benchmarks."
"there are several methods to perform marginal inference in bayesian networks. examples include exact methods, such as variable elimination, the junction tree algorithm [cit], and symbolic techniques [cit], approximate methods based on belief propagation [cit], and those based on sampling, such as gibbs sampling or mcmc search. recent advances on the random generation of sat witnesses [cit] also fall in this area. given that exact inference is #p-complete [cit], our main consideration in choosing belief propagation was the desire for deterministic output, and the requirement to scale to large codebases."
"given a program p and an analysis d, the bayesian network of the previous section is parameterized by the vector of rule probabilities, p. to emphasize this dependence, we write pr p for the induced joint probability distribution."
"third, the experiments were based exclusively on defining the affective variation considering the variation of the profitability of an investment portfolio. in this sense, another possible complementary work corresponds to extending the affective dimension by incorporating other emotions in an artificial way (such as the trust-fear pair) and, in turn, relating these additional artificial emotions with other capital market variables (such as volatility, probability of loss, or economic shocks). then, the resilience mechanism can be tested in scenarios with greater affective complexity."
"aggregating across all our benchmarks, there are 379 real dataraces, of which the unsound chord analysis reports only 203 and produces 1,414 alarms. bingo discovers all 379 dataraces within a total of just 1,736 iterations. the user therefore discovers 87% more dataraces by just inspecting 23% more alarms. in all, using bingo allows the user to inspect 100 new bugs which were not reported either by fasttrack or by chord in its unsound setting."
"here, we define the variables as follows: w: it corresponds to the weight vector that describes the stocks distribution within an investment portfolio (i.e., the portfolio composition)."
"on the other hand, the last true alarm discovered may be an outlier and not representative of the entire interaction process. this is evident in the case of sunflow, for which one needs to inspect 838 of the 958 alarms produced to discover all bugs. observe however, in the column rank-90%-t, that the user discovers 90% of the true alarms within just 483 iterations. the more detailed comparison between bingo and base-c presented in figure 7 demonstrates that bingo has a consistently higher yield of true alarms than base-c."
"observe that lines l4 and l2 can indeed execute in parallel, and the original conclusion p(l4, l2), in figure 3, is true. however, the subsequent conclusion p(l4, l3) is spurious, and is caused by the analysis being incomplete: the second thread to enter the synchronized block will necessarily leave the method at line l2. four subsequent false alarmsð race(l4, l5), race(l5, l5), race(l6, l7), and race(l7, l7)ðall result from the analysis incorrectly concluding p(l4, l3). figure 3 . portion of the derivation graph obtained by applying the static datarace analysis to the program in figure 1 . the central question of this paper is the following: if the user identifies race(l4, l5) as a false alarm, then how should this affect our confidence in the remaining conclusions?"
"observe that the analysis is flow-sensitive, i.e. it takes into account the order of program statements, represented by the relation n(p 1, p 2 ), but path-insensitive, i.e. it disregards the satisfiability of path conditions and predicates along branches. this is an example of an approximation to enable the analysis to scale to large programs. a simple static datarace analysis in datalog. we have elided several parts of the analysis, such as the recognition of thread starts and the base case of the p(p 1, p 2 ) relation."
"first, profitability corresponds to the percentage variation of a stock price within a specific investment period. profitability can be positive, negative, or neutral (equivalent to zero). profitability can be calculated using equation (6):"
"by attaching conditional probability distributions (cpds) such as equations 1ś6 to each node of figure 3, bingo views the derivation graph as a bayesian network. specifically, bingo performs marginal inference on the network to associate each alarm with the probability, or belief, that it is a true datarace. this procedure generates a list of alarms ranked by probability, shown in table 1a . for example, it computes the probability of race(l4, l5) as follows:"
"among the works that are related to resilience, one research area addresses to the engineering and sociotechnical system approach [cit] . sociotechnical systems represent different, varied and complex relationships between public and private entities (e.g., transportation systems, healthcare infrastructure, education services, organizations and communities, among others), where to achieve specific goals, these entities interact in terms of technical and social subsystems. thus, resilience is understood as the capacity of a system to recognize, anticipate and absorb disturbances that can affect some function of the system. in addition, the concept is also associated with the capacity to recover any functionality or structural capability that is lost or damaged by the occurrence of a disturbance and adapt the system to future possible new disturbances. some applications of the approach that were mentioned above correspond to transport [cit], financial performance in tourism [cit], and disaster management [cit], among others."
"in our experiments (table s. 1), l increased as oil flow rate increased (fig. s.6a, b) and decreased as water flow rate increased (fig. s.6c) . table. s.1 summarizes all droplet experiments."
"furthermore, the analysis flags determine the number of alarms produced in an unpredictable way: reducing it from 958 alarms to 506 alarms for sunflow, but from 1,870 alarms to 80 alarms for xalan. in contrast, bingo provides the user with much more control over how much effort they would like to spend to find bugs."
"in the field of psychology, resilience is understood as the capacity of people to overcome adverse scenarios [cit] . several studies have been developed related to the psychological approach to resilience, such as those related to internet addiction [cit], women and girls [cit], children [cit], adolescence [cit], and workplace productivity [cit], among others."
"one immediate way to measure the effectiveness of bingo is to determine the rank at which the last true alarm is discovered. we present these statistics in table 4 . for example, the datarace analysis produces 522 alarms on ftp, of which 75 are real dataraces. bingo presents all true alarms for inspection within just 103 rounds of interaction, compared to 368 for base-c, and 520 for base-r. another notable example is luindex from the dacapo suite, on which the analysis produces 940 alarms. of these alarms, only 2 are real dataraces, and bingo reports both bugs within just 14 rounds of interaction, compared to 101 for base-c and 587 for base-r. over all benchmarks, on average, the user needs to inspect 44.2% and 58.5% fewer alarms than base-c and base-r respectively."
"inference techniques. starting with pearl [cit], there is a rich body of work on using graphical models to combine logical and probabilistic reasoning in ai. prominent examples include bayesian networks and markov networks, and we refer the reader to koller and friedman's comprehensive textbook [cit] . a more recent challenge involves extending these models to capture richer logical formalisms such as horn clauses and first-order logic. this has resulted in frameworks such as probabilistic relational models [cit], markov logic networks [cit], bayesian logic programs [cit], and probabilistic languages such as blog [cit], problog [cit], and infer.net [cit] . our work may be viewed as an attempt to apply these ideas to program reasoning."
"the emotional state year after year (i.e., beginning the investment process from a neutral emotional state) in the ri and nri investment profiles was restored to verify the possible effects of having a resilience mechanism under equivalent individual emotional conditions for investing."
"the remainder of this work is organized as follows. section 2 presents the related literature. section 3 shows the artificial psychological resilience mechanism design and its inclusion within a decision algorithm for the stock market domain. section 4 includes the scenario description and experimental results. section 5 presents a discussion of the obtained results. finally, section 6 presents conclusions of the work done and future work."
"the experimental scenario considers official data from the standard & poor's 500 index [cit] . to configure the investment portfolios during experimental volume 7, 2019 the experimental scenario considers three different types of artificial investors: the resilient investor (ri), the nonresilient investor (nri), and the trend-follower investor (tfi). the ri can use the resilience mechanism. meanwhile, the nri and tfi cannot use the resilience mechanism. in turn, the tfi invests according to the standard & poor's 500 index. that is, the tfi is a permanent follower of the trend of the index."
"regarding the emotional factors (ef), their incorporation and degree of influence depend directly on each domain and how this domain is addressed and implemented. for the purposes of the present research work, to simplify the experimental phase, these factors will be associated with the stock price variations in the capital market. that is, emotional variation will be defined with respect to the profitability variation. a random value adds non-deterministic behavior to artificial emotions."
"the experimental scenario demonstrates that the investment decisions that are made from a rational-emotional perspective (the ri and nri investment profiles) are more effective than a purely rational perspective (the tfi investment profile). in addition, the experimental results show that the use of a resilience mechanism makes it possible to adapt decision making in periods with economic shocks. in this way, the ri investment profile has better long-term performance than the nri investment profile. the resilience mechanism prevents investment decision making from being carried out with information that is biased by emotional states of greater sadness, and, therefore, it tries to maintain a certain degree of rationality in decision making (i.e., the decisionmaking does not have an intensely emotional perspective). similarly, the mechanism does not allow for a complete restoration of the emotional state (from a state of sadness to a state full of emotional neutrality), as occurs in people within real life scenarios."
"co-reachability based constraint pruning. while the solution to the datalog program at fixpoint contains all derivable tuples, not all of these tuples are useful in the production of alarms. therefore, our first optimization is to remove unnecessary tuples and clauses by performing a backward pass over gc. we initialize the set of useful tuples u ≔ a, and repeatedly perform the following update until fixpoint:"
"however, to the best of our knowledge, there are no proposals that suggest the inclusion of a psychological approach to resilience within an autonomous decision-making system for the stock market domain. specifically, the incorporation of a psychological approach to resilience allows an autonomous system to face special difficult investment scenarios (e.g., an economic shock) and can prevent the system from permanently turning negative. thus, psychological resilience can enable an artificial autonomous system to adapt their decision-making processes according to uncertain investment environments. thus, the novelties of the present research work are the following: 1) we design an artificial psychological resilience mechanism for the stock market domain, 2) we incorporate the artificial psychological resilience mechanism within a decision algorithm for the stock market domain, 3) we define an experimental scenario based on official data from the standard & poor's 500 index (s&p500) [cit], and 4) we analyze the promising results that are obtained from an experimental scenario."
"in contrast, the present research work suggests the use of a ef (emotional factors) component, which seeks to reflect the variation of factors that influence the valuation of emotional variables. it should be noted that, in this case, emotional influence factors are essentially defined as ''variable'', and not as ''constant''. considering the above, the present research work seeks to extend the mechanisms previously used, suggesting a modification that seeks to bring the decision criteria and emotional update to more real scenarios."
"the present experimental scenario considers that the ri and nri use a single pair of emotions called ''joysadness'', and its variation is defined depending on the observed profitability of an investment portfolio. the variation of the emotional factors ( ef) in equation (10) is determined considering the variation of stock prices ( prices). then, ef is used in equation (1) to update the emotional state."
"meanwhile, risk is associated with the volatility, which is the variation of the stock price along time (a high variation implies high risk), and it is measured using the standard deviation."
"one may notice that mem_ra introduces the long execution time in mm as well. because mm fully utilizes the rf resources and contains quite few memory accesses to trigger the memory-contention aware tfet register allocation. as figure 6 (a) demonstrates, the performance of mm maintains the same under idealmem_ra since the gpu will be equipped with all cmos registers if running such type of benchmarks, and it cannot reduce the energy. on average, idealmem_ra slightly outperforms mem_ra on perfonnance but meanwhile, obtains less energy savings. in summary, mem_ra successfully explores the energy efficient gpgpus and its effectiveness is quite close to that of idealmem_ra ."
978-1-4673-4953-6/13/$31.00 ©2013 ieee 112 of the gpu stream multiprocessor's power [cit] . effectively optilnizing the register files power consumption is critical and the first step towards the energy-efficient gpus.
"modern gpgpu employs the fine-grained multi threading among numerous active threads which leads to the large register files consurning massive dynarnic and leakage power. exploring the optimal power savings in register files become the critical and first step towards the energy-efficient gpgpu. the conventional method to reduce dynarnic power is to scale down the supply voltage wh ich causes substantial leakage in cmos circuits. the tfets are the promising candidates for low voltage operations regarding to both leakage and performance. however, always executing at the low voltage (so that low frequency) will result in significant performance degradation. in this study, we propose the hybrid cmos-tfet based register files. we leverage the unique characteristics of gpus during the off-chip memory accesses, and explore the memory contention-aware tfet register allocation (mem_ra) to make use of tfet registers in alleviating the memory contentions, and meanwhile gaining the attractive energy optirnization. our experiment results show that mem_ra obtains 30% energy (including both dynamic and leakage) reduction in register files compared to the baseline case with power gating technique. especially, it achieves 42% energy savings in memory-intensive benchmarks with only 2.5% performance loss."
"in this paper, we propose the hybrid cmos-tfet based registers in gpus to obtain optimal energy reduction with negligible performance penalty. the contributions of this study are as foliows:"
"as figure 5 shows, register files are partitioned into cmos-based and tfet-based registers, and each physical register vector has a unique identification number. two power supply lines are used to support the high (low) voltage operations on cmos (tfets) registers. the register renaming stage is added into the sm pipeline (shown as the dotted rectangle), during which the register renaming table is accessed. it is indexed by the warp id and register number encoded in the instruction, and each entry holds the corresponding physical register vector number which will be used for register access in the following stage. two fifo buffers are attached to the renaming table to keep the released cmos and tfet register vectors, respectively. the top register in each buffer is consumed for the renaming, while the bottom is filled by the newly released register. in the case that a cmos register is requested while the butler for cmos registers is empty, the butler for tfet registers will provide a free tfet register instead, and vi ce versa. note that there is always at least one free cmositfet register available for renaming since the required resources have already been weil estimated when the block is assigned to the sm."
"data transmission: data transmission in wsns is application specific. it may be continuous or event driven or query-based or hybrid. in case of continuous data transmission, sensor nodes send data to the base station periodically. in event driven and query-based transmission they send data to the base station when some event occurs or a specific query is generated by the base station. hybrid transmission uses a combination of continuous, event driven and query-based transmission, so for architecture and design of wsns data transmission is a very significant issue. scalability: a wsn consists of hundreds to thousands of sensor nodes. routing protocols must be workable with this huge number of nodes i.e., these protocols can be able to handle all of the functionalities of the sensor nodes so that the lifetime of the network can be stable."
"(1) we observe that threads in gpgpu workloads can be seriously delayed while executing in the gpu streaming multiprocessors due to the memory access interference with others. instead of stalling in the pipeline on the occurrence of serious memory contentions, threads can execute at a low speed by using tfet -based registers to postpone their memory requests. it helps to achieve the win-win scenario: preventing the interferences and achieving the attractive power savings."
"the thread throttling mechanism has been proposed recently to alleviate the memory contentions and shrink the pipeline idle time [cit] . it dynamically stalls certain threads to restrict the number of concurrent memory tasks and avoid the interferences among memory requests. as can be seen, appropriately slowing down the threads before their memory accesses can even introduce positive effect on performance."
"note that the warp has already been slowed down to so me degree in previous memory transaction, its following memory request might not interfere with others and it is unnecessary to further delay its progress. this happens in kernels with heavy computation tasks which help to separate the memory transactions and relief the memory contentions. however, the case is different in memory-intensive workloads. even the warp has been delayed before, its following memory access can get involved with memory transactions from other warps due to the frequently issued memory requests, and further postponing its execution progress is desired."
"as described in section 2.1., when launching threads to the sm, a number of registers are statically designated to them according to their resource requirements. the register id encoded in the instruction is used as the index to the physical register being read/written. in other words, the mapping between the register id and the physical register is fixed all the time. however, when applying the same mapping mechanism in the hybrid register, the use of the tfet-based registers cannot be managed at the run time."
"the performance degradation in sla is noticeable under mem_ra and idealmem_ra. because they use the last memory access latency to predict the warp waiting time and enable the tfet register allocation correspondingly, the prediction accuracy is affected when the next memory access pattern differs greatly from the last one. as a result, the tfet registers are excessive utilized which hurts the perfonnance. generally, the last value prediction mechanism achieves pretty high accuracy for most benchmarks and helps mem_ra to minimize the perfonnance penalty. we further split the nonnalized overall energy obtained by mem_ra into the dynmnic and leakage portions and present them in figure 7 . the energy partition under the baseline case and cmos_ra is also included in the figure. as it shows, cmos_ra can barely optimize the dynamic power since the cmos register are frequently accessed. mem_ra exhibits strong capability in reducing not only leakage but also dynamic energy. on average, the dynmnic energy reduction compared to the baseline case is 10%, while the leakage decreases 20%. in addition, the dynmnic (leakage) energy savings in memory-intensive benchmarks is 16% (26%)."
"the title compound, c 22 h 30 n 4 o 2, has a crystallographic inversion center located at the mid-point of the n-n single bond. apart from the four ethyl c atoms, the non-h atoms are nearly coplanar with a mean deviation of 0.0596 (2) å . an intramolecular o-há á án hydrogen bond occurs. in the crystal, weak intermolecular c-há á áo hydrogen bonds link the molecules into layers parallel to (100)."
"the sub-threshold slope of the transistor is the key factor in leakage power consumption, and a steep sub-threshold device achieves low leakage current. traditional cmos devices are li mi ted to 60mv/decade sub-threshold slope which induces high leakage current during the voltage scaling [cit] . while tfets [cit] exhibit sub-60m v /decade sub-threshold slope and achieve very low leakage power consumption at low supply voltage. figure 2 (a) compares the off-state leakage current (ioff) and on current (ion) of the two kinds of devices when vcc is 0.3v. as it shows, tfets are able to obtain much lower leakage current and stronger driven current, therefore, ultra low leakage with high frequency. they are promising for energy-efficient computing. on the other hand, as figure 2 (b) exhibits, although tfets are still able to achieve low 10ff at high supply voltage (e.g. 0.7v), cmos devices have larger driven current and better performance than tfets."
"in order to hide the latency induced by the function unit computation and off-chip memory accesses, gpu employs the fine-grained multi-threading that quickly switches among a large number of simultaneously active threads. as a result, substantial register files are required to keep the register context of each thread. for example, nvidia fermi gpu supports more than 20,000 parallel threads and contains 2mb register files [cit] . accessing such sizeable register files leads to massive power consumption [cit] . it has been reported that the register files consume 15%-20%"
"in other words, one extra cycle is required to finish the tfet register read/write operation. the tfet register allocation is disabled when the predicted stall time is expected to be fully absorbed. note that the register read time lasts 2 cycles as long as there is one tfet-based source register. when a warp diverges at a branch instruction, the extra delay is also modeled for all the sequentially executed threads if they use tfet registers. a warp issues multiple memory transactions when a load instruction is executed and the load requests from threads belonging to that warp fail to get coalesced. those transactions may complete at different time, as a result, the register write back cannot be perfonned concurrently. writing values to tfet registers in a load instruction is likely to induce quite long delay which easily makes the warp over-postponed. the tfet register allocation for load instructions are skipped in mem_ra."
"modem graphics processing unit (gpu) supports tens of thousands of parallel threads and deli vers remarkably high computing throughput. general-purpose computing on gpus (gpgpus) are becoming the attractive platform for general-purpose applications that request high computational performance such as scientific computing, financial applications, medical data processing, and so on. however, gpgpu is facing severe power challenge due to the increasing number of cores placed on a single chip with decreasing feature size [cit] ."
"the sybil attack occurs when a single node claims to be other nodes in the network. karloff and wagner claim that this attack significantly reduces the effectiveness of -fault-tolerant schemes‖ such as distributed storage, multipath routing, and topology maintenance iii."
"the sensor nodes in wsns have limited energy and they use their energy for computation, communication and sensing, so energy consumption is an important issue in wsns. according to some routing protocols nodes take part in data fusion and expend more energy. since the transmission power is proportional to distance squared, multi-hop routing consumes less energy than direct communication, but it has some route management overhead. in this regard, direct communication is efficient. since most of the times sensor nodes are distributed randomly, multi-hop routing is preferable. in some applications nodes sense environment periodically and lose more energy than the nodes used in some applications where they sense environment when some event occurs."
"o for security purposes, the integers p and q should be chosen at random, and should be of similar bit-length. prime integers can be efficiently found using a primality test."
"selective forwarding is a more subtle attack in which some packets are correctly forwarded but others are silently and intentionally dropped. a compromised node could be configured to drop all packets, creating a so-called black hole"
"(3) our evaluation results show that the proposed register allocation technique in the hybrid register design exhibits the strong capability of reducing the register energy consumption (including both dynamic and static energy) by 30% compared to the case with naive power optimization technique (i.e. power gating the unused registers [cit] ). especially, it achieves 42% energy reduction (16% dynamic saving and 26% leakage saving) in memory-intensive benchmarks with only 2.5% performance degradation."
"in this simulation i have implemented security using rsa algorithm. all the results are shown in matlab. in the first scenario source to destination communication is performed using shortest path algorithm. the root is shown in green with arrows. in this scenario energy of nodes is shown after the attack and the energy level is almost same, so it does not loss any data during aggregation. v."
"rsa is an algorithm for public-key cryptography that is based on the presumed difficulty of factoring large integers, the factoring problem. [cit] . a user of rsa creates and then publishes the product of two large prime numbers, along with an auxiliary value, as their public key. the prime factors must be kept secret. anyone can use the public key to encrypt a message, but with currently published methods, if the public key is large enough, only someone with knowledge of the prime factors can feasibly decode the message. ["
"recall that sm supports the spmd execution model, threads from a warp exhibit the same progress and stall for the same amount of time, therefore, the stall time at warp level is the finest granularity can be considered. the warp stall time due to the off-chip memory access implies the severity of the memory contentions. a long waiting time means the occurrence of serious contentions, and if the memory request from the warp had been postponed by using the tfet registers, such contentions may be removed successfully. unfortunately, the waiting time is not available until the request has already been serviced and the contentions already take place. we use the last value prediction mechanism to predict the warp stall time in its next global memory transaction based on the previous memory access latency, and utilize tfet registers to absorb that predicted stall time before the warp sends out its memory request."
"as discussed in section 3.3., ideally, the size of cmos registers would exactly match their usage. we further investigate the effectiveness of mem_ra when the cmos registers size is set ideally, called idealmem_ra. (we name the mem_ra using 12.5k cmos and 3.5k tfet registers as mem_ra for short.) since benchmarks exhibit different memory access patterns, their requirements on cmos registers vary greatly. although designers rarely fabricate a gpu with certain number of cmos(tfet) registers to specifically satisfy a single benchmark's requirement, the results of idealmem_ra provide a more accurate evaluation on the capability of mem_ra on power optimizations while maintaining the performance."
"the title compound, as shown in fig. 1, all bond lengths and angles are in the normal ranges. except for four carbon atoms, all the other non-hydrogen atoms nearly lie on the same plane. the intramolecular o-h···n and intermolecular c-h···o hydrogen bonds (table 1) link the molecules into layers prallel to (100)."
the title compound was prepared according to the literature [cit] . single crystals suitable for x-ray diffraction were prepared by slow evaporation a mixture of dichloromethane and petroleum (60-90 °c) at room temperature.
"we implement our mem_ra technique on the cycle accurate, open-source, and publicly available simulator gpgpu-sim [cit] to obtain the gpgpu perfonnance statistics. we build our power model based on the energy analysis tool cacti [cit] . we set the high supply voltage as 0.7v and low supply voltage as 0.3v. the read/write times to cmos-and tfet-based registers and the total execution time are collected from the modified gpgpu-sim to evaluate both rf dynamic and leakage energy consumption. our energy estimation is consistent with previous studies [cit] ."
"(2) we propose to build the hybrid tfet-based and cmos-based registers, and perfonn the memory contention14th int'l symposium on quality electronic design aware register allocation. based on the access latency of previous memory transaction, we predict the thread stall time during its following memory access, and allocate tfet-based registers to that thread to postpone its execution progress to the maximum degree without performance loss. by doing this, we maximize the utilization of the tfet based registers, thus, optimize the energy consumption while maintaining the performance."
rsa involves a public key and a private key. [cit] the public key can be known to everyone and is used for encrypting messages. messages encrypted with the public key can only be decrypted using the private key. the keys for the rsa algorithm are generated the following way:
"the main design goal of wireless sensor networks is to transmit data by increasing the lifetime of the network and by employing energy efficient routing protocols. depending on the applications used, different architectures and designs have been applied in sensor networks. again, the performance of a routing protocol depends on the architecture and design of the network, so the architecture and design of the network is very important features in wsns. the design of the wireless sensor network is affected by many challenging factors which must be overcome before an efficient network can be achieved in wsns."
"in the gpgpu applications, all threads in a kernel execute the same code [cit], and exhibit similar execution progress in the fine-grained multi-threading environment. when one thread encounters an off-chip memory access, other threads are likely to issue the requests at approximately the same time, leading to severe memory contentions which extend the memory access time. and the pipeline in the gpu stream multiprocessor stalls when all threads stall due to the long-latency memory accesses. it has been found that the performance of numerous gpgpu workloads are still bounded by the memory resources even modern gpus provide very high memory bandwidth [cit] . in order to alleviate the memory contentions and efficiently utilize the memory bandwidth, threads can run at different paces which effectively avoid the interferences among memory requests. it enables the implementation of the tfet-based registers in gpgpus for a number of threads so that they can run at a lower frequency without any performance degradation, and meanwhile, both the dynarnic and leakage power of the registers reduces substantially. on the other hand, applying tfets for all registers in the gpgpu will cause significant performance penalty since many threads still need to execute at high frequency to achieve the high computational throughput."
"supply voltage scaling is the fundamental technique to reduce the dynarnic power consumption, but it is limited by the leakage constraints in cmos digital circuits. recently, inter-bank tunneling field effect transistors (tfets) have shown to be the attractive candidates to operate at low supply voltages (e.g. 0.3v) with ultra low leakage and higher frequency than cmos [cit] . however, at higher supply voltage, cmos devices are able to achieve much better performance than tfets. the unique characteristics of cmos and tfets at different voltage levels provide great opportunity in gpu power savings without hurting the performance."
"the major hardware added into the sm is the register renaming pipeline stage including the register renaming table, two buffers for the released cmos and tfet register vectors, and some simple combinational logics. in order to keep the renaming infonnation for all physical registers, the number of entries in the renaming table is equal to the amount of register vectors which is 512 in our default gpu configuration. similarly, the total size of the two buffers is 512 as weil. each entry in those three structures contains 9 bits. the hardware in the renaming stage causes around 2% area overhead to the register files in the sm. in addition, to predict the warp stall time, thirty-two li-bit counters [cit] cycles), and the unit perfonning simple integer arithmetic and logic operations are added in the sm. the overall hardware overhead to the sm register files is 3%. we develop the power model (including both dynmnic and leakage power) for the added hardware, and find that it induces around 2.9% power overhead to the register files by running a large set of gpgpu benchmarks."
"salicylaldehyde azine belongs to the photochromic aromatic schiff base molecules with two intramolecular hydrogen bonds [cit] . the photochromism of the molecules, owing to enol-keto intramolecular tautomerism, attracts much interest because of possible applications, for example, in molecular memories and switches [cit] . herein, we report the crystal structure of the title compound."
"processing units (gpgpus) architecture a typical gpu consists of a scalable number of in-order streaming multiprocessors (sm) that can access to multiple on-chip memory controllers via an on-chip interconnection network [cit] . in gpu programming models, highly-parallel kernel functions are launched to the gpu for execution. the kernel is composed of a grid of light-weighted threads; a grid is divided into a set of blocks; each block is composed of hundreds of threads. threads in the kernel are assigned to the sms at the granularity of blocks. figure 1 illustrates the sm microarchitecture. threads in the sm execute on the single-program multiple-data (spmd) model. a number of individual threads (e.g. 32 threads) from the same block are grouped together, called warp. in the pipeline, threads within a warp execute the same instruction but with different data values. as figure 1 shows, each warp has a dedicated slot in the warp scheduler. at every cycle, a ready warp is selected by the scheduler to feed the pipeline. the instruction is then fetched from the instruction cache based on the pc of the issued warp, and further decoded. in the sm, a number of registers are statically allocated to each warp when the block is distributed. all threads in the warp access a number of registers (i.e. the register vector) simultaneously based on the warp id and the register number, the register values are processed in parallel across the streaming processors (sp)."
"the critical challenge in the hybrid register design becomes the runtime cmositfet physical register allocation to the destination register id in the warp. aggressively utilizing the tfet registers degrades the performance significantly; on the other hand, too conservatively using the tfet registers fails to achieve the goal of maxirnizing the registers power savings. moreover, the tfet utilization among warps needs to be different to well control the warp execution progress and avoid the interferences. as can be seen, it is crucial that the tfet based register allocation adapts to the memory access pattern of the workloads. for example, randomly or periodically renaming the destination registers to tfet registers can easily hurt the performance as they are blind to the memory accesses. it is highly possible that the tfet registers are improperly used when there are few memory transactions and the high throughput is expected during that period of the workload execution. we propose the memory contention-aware tfet register allocation (named as mem_ra as abbreviation) to achieve the optimal power savings with little perfonnance penalty."
"in gpus, the off-chip memory requests from sms need to go through the on-chip network routing to certain memory controller and wait there to be served. when numerous requests are issued at sünilar time by multiple sms, both on-chip network and memory controllers will be severely congested which significantly increases the memory access time. unfortunately, such congestion issue occurs frequently in gpus due to the unique characteristic of the gpgpu applications: all threads in the kernel across sms execute the same instructions and proceed at sirnilar rate in the fine-grained multithreading environment. although there are up to thousands of active threads running in each sm, they are unlikely to fully hide the extremely long-iatency memory transaction caused by the memory contentions. as a result, the sm suffers long-time pipeline stall. the gpu memory bandwidth is already considered as one of the resource constraints for many gpgpu workloads even modern gpus provide pretty high memory bandwidth [cit] . ) shows an example of the memory resource contentions among sms. several sms encounter the global memory access instructions and send out memory requests simultaneously. the buffers in network-on-chip (noc) and memory controllers are quickly filled up by those requests and they have to be served sequentially to access the dram buffers ( figure 3 (a) takes a snapshot on the noc and memory controllers). therefore, the memory transactions spend longer time to finish, and the pipeline in sms quickly turns to be idle (highlighted as red circles in figure 3 (a)) since other active threads in the sm will stall at the memory instructions in the near future as weil."
"in the sinkhole attack, a node spuriously advertises a very good route to a sink node in order to lure all nearby traffic to itself. thus all traffic within some sphere of influence is drawn into the sinkhole centered at the compromised node. this attack enables the selective forwarding attack along with other attacks. [cit]"
"once the stall time is caiculated by using the analytical model above, the warp starts to allocate tfet-based registers to the destination register ids in its following execution. generally, the read/write time to tfet-based sram operating at low supply voltage is as twice as that of the cmos-based sram at nonnal voltage [cit] . the access time to tfet registers is modeled as 2 cycles in our study."
"jamming, a well-known attack on wireless communication is simply interference with the radio frequencies used by a device\"s transceiver. it represents an attack on the availability of a network. jamming is only different from normal radio propagation in that it is unwanted and disruptive, thus creating a denial-of-service condition"
"allocating the tfet-based registers to those threads and managing them to execute at low frequency during the register read/write operations provides the perfect approach to control the thread progress. figure 3 (b) demonstrates the example of intelligently leveraging the low frequency operations on tfets to absorb the pipeline stall time (shown in green rectangles) and meanwhile, separate the memory requests from sms. as it shows, both noc and memory controllers have few queued requests, and the off chip memory access time reduces significantly. more importantly, the benefit of tfets on reducing both dynarnic and leakage energy is effectively explored. obviously, cmos-based registers are essential during the normal execution. in this work, we propose the hybrid cmos tfet based registers, and use tfet-based registers to delay threads execution speed to the maximal degree so that achieve the goal of maximizing the energy savings without hurting the perfonnance."
"hello messages are used in many protocols by nodes that want to announce their presence and proximity to their neighbors. most of these protocols rely on the assumption that a node a is within the radio transmission range of another node b if a is able to receive messages from b. in a hello flood attack, a malicious node may try to transmit a message with an abnormally high power so as to make all nodes believe that it is their neighbor . [cit] wormhole attacks a wormhole attack: where an attacker receives packets at one location in the network, tunnels and then replays them at another remote location in the network. the route request can be tunneled to the target area by the attacker through wormholes. thus, the sensor nodes in the target area build the route through the attacker. later, the attacker can tamper the data, messages, or selectively forward data messages to disrupt the functions of the network"
"node distribution [cit] in wsns is either deterministic or selforganizing and application dependant. the uniformity of the node distribution directly affects the performance of the routing protocol used for this network. in the case of deterministic node distribution, the sensor nodes are mutually placed and gathered data is transmitted through pre-determined paths. in the other case, the sensor nodes are spread over the area of interest randomly thus creating an infrastructure in an ad hoc manner. network dynamicity: since the nodes in wsns may be static or dynamic, dynamicity of the network is a challenging issue. most of the routing protocols assume that the sensor nodes and the base stations are fixed i.e., they are static, but in the case of dynamic bs or nodes routes from one node to another must be reported periodically within the network so that all nodes can transmit data via the reported route. again depending on the application, the sensed event can be dynamic or static. for example, in target detection/tracking applications, the event is dynamic, whereas forest monitoring for early fire prevention is an example of a static event. monitoring static events works in reactive mode. on the other hand, dynamic events work in proactive mode."
"he first turns m into an integer m, such that by using an agreed-upon reversible protocol known as a padding scheme. he then computes the ciphertext corresponding to . this can be done quickly using the method of exponentiation by squaring. bob then transmits to alice."
"the rest of the paper is organized as foliows: section 2 provides the background of gpgpu and tfets. section 3 proposes the hybrid cmos-tfet based registers and the memory contention-aware tfet-based register allocation. section 4 describes our experimental methodologies and evaluates the proposed mechanism. we discuss the related work in section 5, and conclude with section 6."
"note that at least nine values of m could yield a ciphertext c equal to m, [cit] but this is very unlikely to occur in practice."
"in order to justify the etlectiveness of mem_ra, we compare it with several power reduction techniques. the baseline case studied in this paper is employing only cmos-based registers and power gating the unused registers during the kernel execution. another naive mechanism for power saving is simply applying tfets to all sm registers, it is named as all_tfet. in previous work, the drowsy cache has been proposed to reduce the cache leakage power [cit] . similarly, registers belonging to a warp can be put into the sleep mode when the warp stalls in the pipeline, but it takes couple of cycles to wake them up for further accesses. we also investigate the effect of drowsy register from the performance and power perspectives. in the hybrid register design, the long access time to tfet registers may largely degrade the performance when they are randomly used. a straightforward technique to maintain performance is to avoid the allocation of tfet registers if possible. in other words, the cmos register is selected for renaming as long as there is any one free. we name this technique as cmos_ra, it is applied on the hybrid 12.5k cmos registers and 3.5k tfet registers. note that the power gating technique is integrated into drowsy register and cmos_ra, respectively, for the fair comparison. since tfet has extremely low leakage power, the power gating is not triggered in alltfet mechanism."
"figures 6 describes (a) the execution time and (b) the overall energy when running the investigated benchmarks under the impact of several power reduction techniques described above. the results are normalized to the baseline case. note that the performance and energy overhead caused by each technique is also included in the results. as figure 6 (a) shows, alltfet hurts the gpu performance significantly, the execution time is alm ost doubled in several benchmarks (e.g. bn, cp, mm, and ra y). on average, alltfet degrades the perfonnance by 56%. although it reduces the energy consumption significantly (total energy decreases to 16% as shown in figure 6 (b», it is not worth to scarify such large portion of throughput to achieve the low energy consumption. interestingly, the kernel execution time under drowsy register mechanism remains the same although there is time overhead to wake up registers staying in the sleep mode, because the wake up time is trivial with regard to the hundred-cycle long memory access. moreover, the energy reduction achieved by drowsy register is smali, only around 7%."
"gpu is usually equipped with its own off-chip external memory (e.g. global memory) connected to the on-chip memory controllers. the off-chip memory access can last hundreds of cycles, and a long latency memory transaction from one thread would stall all threads within a warp. in other words, the warp cannot proceed until all the memory accesses from its threads complete."
the results produced in data aggregation in wireless sensor networks are much better than any other algorithm. the energy efficiency and timing constraints are also better. it provides graphical clear results which are easy to understand.
"security related issues in wsn have become an important part of research in present scenario. detecting abnormal/malicious function of nodes and offering efficient counter measures is a difficult task. in the proposed paper a method that\"s node authentication method. in the proposed method there is no need for specific hardware and neither is the need for clock synchronization due to use of the cryptographic concepts like digital signature. in that each node is authenticated using digital signature(rsa). the received node at the destination node is verified and if the digital signature is false the information about that is sent to the sender node using data_ack..in the future there is a plan to simulate node authentication method with other wireless sensor protocols."
"the ca1/pfc coupling network model is illustrated in the schematic shown in to mimic the ca3 input that drives the ca1 network, we give sequential noisy inputs (see section \"materials and methods\") to both the py neurons and bs neurons in the ca1 network. in this occasion, the py cells in the ca1 network get depolarized and fire sequentially, as shown in figures 2a,b . as a consequence, the local field potential (lfp) recordings in the soma layer show ripple transients overlapped on the sharp-waves (figures 2c,d) . figure 2e is a spectrogram of the lfp recordings in the ca1 network, showing ripple range oscillations around 200 hz. also, we examined the timing of the firing activity of py cells and bs cells in the ca1 network with respect to the phase of the recorded lfp ripples. as shown in supplementary figure s1, the result shows obvious phase locking for both py cells and bs cells, consistent with the previous experimental and modeling literature [cit] . note that, besides the compressed forward sequence replay, by adjusting the input to the ca1 network, our model can also achieve compressed reverse replay and sequential activations in behavioral time scales (supplementary figure s2) ."
"in this section we explore the computational tractability of our optimal ilp-based partitioning scheme. further, we evaluate the performance of the greedy slacker heuristic presented in this work and present a comparison with other resource-aware and generic bin-packing heuristics."
"in future work, it would be interesting to simplify the proposed ilp formulation to achieve higher scalability. further, we aim to apply the greedy slacker heuristic to other locking protocols and scenarios such as uniform and heterogeneous multiprocessors."
"1) classic bin-packing heuristics: commonly used heuristics include the first-fit, next-fit, best-fit and worst-fit heuristics [cit], which we describe in brief. all heuristics take a sequence of objects as input and successively assign them to bins. the first-fit heuristic iterates over all bins in the order they were allocated, and assigns the current object to the first bin with sufficient remaining capacity. if no such bin exists, it allocates a new bin and assigns the current object to it. the next-fit is simpler in that it only checks the last allocated bin and allocates a new bin if the last allocated bin does not have sufficient capacity to fit the current object. both the first-fit and nextfit heuristics report a failure if the maximum number of bins is exceeded. the best-fit and worst-fit heuristics allocate the maximum number of bins upfront and then assign each object to a bin such that the remaining capacity in that bin is minimized or maximized, respectively. if no bin with sufficient capacity exists, they report a failure. the any-fit heuristic, which we denote as af in the following, subsumes all previously described bin-packing heuristics in that it tries all of them (in the order worst-fit, bestfit, first-fit, next-fit) and returns the first successfully computed result. for all heuristics, we consider the input being processed in order of decreasing size, which typically results in a lower number of required bins [cit] ."
"partitioned fixed-priority (p-fp) scheduling, under which tasks are statically distributed across all processors and each processor is individually scheduled by a local fixed-priority scheduler, is the de facto standard in embedded multiprocessor real-time systems today. for instance, p-fp scheduling is mandated by the widely adopted autosar standard for automotive systems [cit], and also supported by virtually all posixcompliant real-time oss such as vxworks, qnx, lynxos, etc."
"for global resources, i.e., resources accessed from different processors, the msrp cannot use the uniprocessor srp, which relies on per-processor ceilings and does not generalize to multiprocessor systems. instead, the msrp uses non-preemptive fifo spin locks to coordinate access to global resources: to gain access to a global resource l q, a job becomes non-preemptive and starts spinning until it gains access to l q . concurrent requests by jobs on other processors to the same resource are served in fifo order. once a job finishes its critical section, it becomes preemptive again and normal scheduling resumes."
"in our ca1 network model, 400 py cells are randomly allocated into five different groups, each of which has 80 py cells and represents one place cell assembly, as observed in experimental studies [cit] . to mimic the synaptic input from ca3, each place cell assembly receives sequential poisson noisy input. the parameters of the input is summarized in supplementary table s3 . the setting of this ca3 input pattern is based on recent experimental studies, which show that during awake swr associated replay, the decoded trajectory from the firing activities of ca1 place cells exhibit discrete \"jump\" behaviors, which is believed to result from the attractor state changes in the upstream ca3 [cit] . since the reported mean time for the decoded location change during one ripple event falls within the 25-50 hz slow gamma range, we choose to set the duration of each noisy input to be consistent within this range."
"iv. the case for optimal partitioning exact partitioning approaches for task sets with shared resources can be computationally expensive due to the hardness of the underlying bin-packing problem. this complexity raises the question whether exact approaches can offer substantial benefit over resource-aware heuristics. to answer this question, we conducted an experiment to evaluate whether there exists a potential that is left unused by heuristics but could be exploited by an exact approach. to this end, we generated task sets for which a valid partitioning was known to exist by construction, and hence an exact partitioning approach would have found a valid partitioning. then we let resource-oblivious and resourceaware heuristics partition the same task sets and checked schedulability of the computed partitionings. priorities were assigned in a rate-monotonic fashion [cit], and before assigning a task to a processor (i.e., to determine whether a task \"fits\") a response-time schedulability test was applied to rule out choices that render the task set unschedulable. fig.1 shows the fraction of schedulable task sets under each partitioning heuristic depending on the number of tasks in the system. the straight line at the top of the graph marks the fraction of task sets that can be successfully partitioned by an exact approach, that is, all task sets as only partitionable task sets were considered in this experiment. as is apparent from fig.1, af is able to produce valid partitionings for all task sets consisting of up to 20 tasks. for larger task sets, af is unable to produce valid partitionings for a large fraction of the generated task sets although valid partitionings exist and hence, an optimal partitioning scheme would have found them. both the mpcp heuristic and bpa show surprisingly low schedulability, an effect we revisit in sec.vii-b. while fig. 1 shows results for specific parameter choices, similar results can be obtained for many other configurations: if blocking due to resource sharing constitutes a \"bottleneck\" with respect to schedulability, then an ill-chosen task assignment can render a partitioning invalid. clearly, for task sets in which blocking durations are not significant, resource-oblivious heuristics may yield results comparable to resource-aware heuristics. however, as demonstrated in fig.1, if blocking is not negligible, then there exists a significant potential to be exploited by an exact approach. next, we present such an approach based on a novel ilp encoding of the partitioning problem."
"for cases where the cost of the ilp-based partitioning approach cannot be afforded, we presented greedy slacker, a novel resource-aware partitioning heuristic, which we have demonstrated to perform well on average. greedy slacker is generic as it is neither tailored to a specific locking protocol nor dependent on task-set-specific parameter tuning, and, due to its simplicity, it is resilient in the sense that it is able to exploit locality when existent without unreasonably degrading in performance if faced with an unanticipated, ill-structured task set composition, unlike the mpcp heuristic and bpa."
"p-fp scheduling is attractive because it is well understood and trivial to implement with low runtime overheads; however, it also requires a valid partitioning to be provided, that is, a static mapping of tasks to processors under which all tasks are schedulable (i.e., guaranteed to satisfy all timing requirements). computing such a partitioning can be challenging. in fact, even if tasks are independent, that is, if they do not share any resources besides the processor, finding a valid assignment requires solving a bin-packing-like problem [cit], which is known to be np-hard, and for which heuristics are typically used (e.g., see [cit] )."
"the connections between the cells in the pfc network is shown in supplementary table s5 . it has been reported that one py cell in rat visual cortex makes unidirectional and bidirectional connections to other py cells with a probability of 0.13 and 0.06 [cit] . since the pfc area has abundant recurrent py-py connections, which are more than double the rate than in visual cortex [cit], we set the probability of unidirectional connection to 0.25 and the probability of bidirectional connection to 0.12. for the in cells, each of them is connected to randomly chosen 10 in cells based on previous anatomical studies [cit] ."
"allocation problems similar to bin-packing [cit] arise in a vast range of settings; due to space constraints, we restrict our focus to prior works most relevant to the partitioning of real-time workloads onto multiprocessor platforms."
"it is well known that the hippocampus closely interacts with a large number of cortex areas, including the pfc, in both monosynaptic and multisynaptic pathways [cit] . the ventral hippocampus ca1 region and the proximal subiculum make monosynaptic projections directly to both the excitatory and inhibitory neurons in pfc [cit] . based on these experimental findings, the pyramidal cells in our ca1 model project to both the pyramidal cells and interneurons in pfc via ampa synapses. the specifics of the connection are shown in supplementary table s6 . in the pfc model, 50 pyramidal cells are randomly chosen and divided into five different groups. each pfc py cell in one group receives ampa projections from randomly selected 30 out of 80 py cells from a specific place cell assembly in ca1. similarly, the selected ca1 py cells also form ampa synapses to all the in cells in pfc for feedforward inhibition."
"in this paper, we focus on spin locks (where blocked tasks busy-wait) rather than on semaphores (where blocked tasks suspend) due to the considerable practical relevance of spin locks in the context of autosar [cit], and because the partitioning problem in the presence of spin locks, despite their widespread use, has not been studied in prior work, which we review next."
"generic bin-packing heuristics are commonly used to map tasks to processors. bin-packing heuristics distribute a set of different objects (tasks) of a given size (processor utilization) to bins (processors), such that each object is assigned to exactly one bin and the total size of all objects assigned to a bin does not exceed the bin's capacity (all tasks are schedulable)."
"in our second experiment, we evaluated the impact of task set size on solving time. an increase in task set size leads to a larger ilp size, and hence potentially to longer solving times. to study this effect, we fixed the total task set utilization to 2.0, 2.5 and 3.0, respectively, and varied the number of tasks in the task set from 4 to 20. task periods and resource accesses where chosen as in the first experiment. the results are shown in fig. 2b and exhibit a clear increase in run time as the task set is growing. since the total utilization was kept constant, we ruled out the effect studied in the first experiment where growing utilization makes the partitioning problem harder to solve, which is reflected in higher run times. rather, we attributed the observed effect to the growth in ilp size (in terms of the number of both constraints and variables) and resource contention, both of which increase with each additional task."
"the results imply that the increase of total utilization and task set size each independently cause a significant increase in runtime of the ilp-based approach presented in sec.v. however, the results also demonstrate that, with today's hardware, our exact ilp-based partitioning approach is applicable to small and moderate application instances (note that the runtimes reported in figs. 2a and 2b are in the range of a couple of seconds on average). even though run times may grow quickly for larger applications, our ilp-based partitioning technique may still be the preferred approach as it is only a one-time effort that may well be worth the cost in the context of commercial development cycles that can stretch many months or even years."
"viii. conclusion in this work, we have considered the problem of partitioning a set of sporadic real-time tasks that share resources protected by spin locks onto a set of identical processors. our work is motivated by the common need to minimize swap requirements and component costs to the extent possible. to this end, we presented an ilp-based approach for task set partitioning and priority assignment for shared-memory multiprocessor systems with shared resources. in contrast to commonly used partitioning heuristics, this approach yields optimal results (with regard to the underlying schedulability analysis) and thereby avoids overprovisioning, but is subject to high computational costs."
"in this work, we approach the problem of partitioning a set of tasks that share resources protected by spin locks in two ways, reflecting two interpretations of \"efficient.\" first, in sec.v, we present an exact approach that uses integer linear programming (ilp) to assign each task a processor and a unique priority such that all tasks are schedulable under the msrp [cit], a real-time locking protocol for shared-memory systems based on spin locks (reviewed in sec.iii-b). our ilp-based approach is optimal in that it yields a valid partitioning and priority assignment if one exists (with regard to the underlying schedulability analysis of the msrp [cit] ). to the best of our knowledge, this is the first partitioning method that is optimal in the presence of shared resources protected by spin locks, which is made possible by the novel approach of encoding the msrp analysis [cit] as ilp constraints. our ilp-based approach is efficient in the sense that it prevents over-provisioning, but due to the np-hardness of the assignment problem, it also faces inherent scalability limits, which we empirically evaluate in sec.vii-a."
"surprisingly, both the mpcp heuristic and bpa led to significantly lower schedulability than the af. this effect was unexpected since both the mpcp heuristic and bpa were particularly designed for scenarios with resource sharing, while af is resource-oblivious. we found that the reason for this effect lies in the way bpa and the mpcp heuristic partition task sets: both of them compute a connected component consisting of tasks that share resources (possibly transitively). for the configuration considered, this connected component is likely to include a large fraction of the task set. in this case, the mpcp heuristic and bpa attempt to break up the connected component into smaller chunks that can be fitted on a single processor such that the extent of resource sharing between these chunks is small. however, in the task sets we generated, requests to all resources are uniformly distributed over all tasks, without exhibiting a particular structure or locality among tasks and resources that could be exploited by these heuristics. the bpa and mpcp heuristics thus frequently failed to find an appropriate partitioning."
"1) precedence constraints: task precedence constraints specify a partial temporal order among jobs that can be used to express an output-input dependency among tasks (e.g., in a \"pipeline\" processing flow, where jobs of one task produce an output consumed by a job of second task, in which case the second job cannot start executing before the first job completed)."
"in this work, we build a biophysical model that includes both the hippocampus ca1 network and the pfc network to study the memory transfer and reactivation. under sequential input, the pyramidal cells in the ca1 network exhibit both ordered replay activities and ripples in the lfp recordings, consistent with the experimental findings (diba and buzsáki, 2007) . also, as shown in supplementary figure s1, the pyramidal cells and the basket cells fire at preferred ripple phases, consistent with previous experimental study [cit] . besides producing the electrophysiological signals that are consistent with the existing literature, our model also makes a few assumptions and predictions that can be tested in future experimental studies. first, our model suggests the existence of cortical ripples in pfc network coupled to the hippocampal ripples. this is already confirmed with the recent study suggesting mpfc-hippocampal lfp coupling during nrem sleep. however, it is still unknown if this cortical-hippocampal ripple coupling happens during awake state too. second, our model indicates that the sequence reactivation in pfc network can be achieved through both cellspecific stimulation and spontaneous background synaptic noise."
"a job of task t i can incur arrival blocking when, upon its release, a lower-priority job running on the same processor is either executing non-preemptively or holding a local resource with a priority ceiling of at least t i 's priority. similarly, the use of non-preemptive fifo spin locks for global resources can cause a job of t i to incur arrival blocking when a lower-priority job issues a request to a global resource. in this case, the lowerpriority job non-preemptively spins until gaining access and then executes the request without giving t i 's job a chance to execute."
"in contrast, exact partitioning approaches are optimal in that they fail to produce a valid partitioning only if no such partitioning exists. exact ilp-based approaches for the partitioning of independent tasks under edf and fp scheduling were presented by baruah [cit] and baruah and bini [cit] . targeting boolean satisfiability (sat) instead of linear programming as the underlying formalism, metzner and herde proposed rtsat [cit], which first transforms an ilp-formulation (similar to ours) to a sat instance, and then employs a specialized sat solver. for task sets with precedence constraints, [cit] presented an ilp formulation that explicitly considers interference due to communication on a shared bus. most closely related to our work is an ilp formulation by zeng and di natale [cit], who recently incorporated blocking due to local resource sharing (i.e., due to resources accessed only on one processor)."
"many modeling studies have been conducted to model the ca1 and ca3 networks in the hippocampus. among these works, most of them are focusing on the mechanism of ripple generation and the sequence storage and reactivation. under external excitatory inputs from ca3 or entorhinal cortex, the ca1 ripples have been proposed to be generated either by pyramidal to pyramidal coupling through gap junctions [cit], or by the pacing effect of feedback inhibition to the pyramidal cells from interneurons [cit] . our model falls within the second category in that the pyramidal cell firing are paced by the synchronous firing of interneurons and no gap junctions exist between pyramidal cells. in terms of the generation mechanism of sequence replay in ca1 network, since the ca1 region lacks recurrent pyramidal-to-pyramidal connectivity, most modeling works suggest that the replay results from the sequential input from the ca3 network [cit] . recently, it has been suggested that by modifying the synaptic strength from ca3 to ca1 and the feedback inhibition between interneurons and pyramidal neurons in ca1, the network can exhibit sequence replay [cit] . the sequence replay generation mechanism in our modeling work is similar to the first group. instead of explicitly modeling the ca3 network, we give sequential excitatory input to both the pyramidal cells and the basket cells in the ca1 network to induce sequence replay. compared to the above modeling works, our model focuses more on the memory trace transfer from ca1 to pfc network and the different mechanisms and conditions affecting the reactivation of the transferred memory traces."
"3) partial specifications: generalizing the locality constraints described previously, system designers might want to enforce a certain priority assignment (e.g., because the most critical task should run at highest priority) or processor assignment (e.g., because some tasks rely on a functionality only available on certain processors) for a subset of tasks. another use case for enforcing such partial specifications is the extension of an existing application where new tasks and/or processors are algorithm 1 greedy slacker partitioning heuristic 1: for all tasks tx order of decreasing density do 2:"
"in our model, the background noise of each neuron is introduced through the noisy ampa synapses of poissonian characteristics. in order to quantify the intensity of the background noise, we compute the second moment of the noise current, which is the same as previous studies [cit] ."
"in a real system, tasks are subject to overheads such as context switch costs or the loss of cache affinity when preempted. we assume that all non-negligible overheads have already been factored into the relevant task parameters (i.e., mainly e i and each l i,q ) using standard accounting techniques (see [10, chs. 3 and 7] for a detailed discussion). next, we review the real-time locking protocol that mediates resource access."
"to study the performance of the mpcp heuristic and bpa when the task set exhibits some structure in terms of requests to shared resources, we generated task sets in which tasks are combined into task groups. a task group can be considered as a functional unit in a system composed of multiple tasks that share resources among them. notably, no resources are shared across group boundaries, which results in multiple smaller connected components (one for each task group) that can be assigned to partitions without breaking them up into smaller chunks. within each task group, tasks share the same number of resources as in the previous experiment. these resources are private to each task group, that is, different task groups share disjoint sets of resources. fig.3b depicts the schedulability results for task sets with the same configuration as above, but with tasks assigned to 8 disjoint task groups. the results indicate that both the mpcp heuristic and bpa can efficiently exploit this structure and yield significantly higher schedulability results than before. further, greedy slacker and af heuristics also exhibit higher schedulability in fig. 3b than in fig. 3a, which indicates that blocking is less of a bottleneck in this scenario."
"after the successful storage of the ca1 sequence, we investigate the retrieval of the memory in the pfc network by cell-specific stimulations to part of the py cells in pfc network. the cellspecific input induced replay represents the scenarios where the memory is recalled by the sensory stimulation or natural cues, which is commonly observed in studies on contextual fear conditioning [cit] ."
"in case l q is a remote resource and requests for l q can cause t i to incur arrival blocking, b i,q,k has to be set to at least the longest critical section length of any request for l q from processor k:"
"next, we investigate the possibility of the memory transfer through ca1-pfc communication and stdp. the ca1 network generates 5 ripples per second and the simulation time is set to 3 s. the raster plot and lfp recordings in pfc from one representative simulation are shown in figure 3 . when the ca1 network generates ripples, parts of the py cells in pfc fire sequentially, while all the interneurons (basket cells) in pfc increase their firing rate compared with no-ripple time intervals due to both feed-forward inputs from ca1 py cells and excitatory inputs from pfc py cells. due to the stdp rule, the py-py connections and in-py connections are updated according to the spike timing of the cells (figures 4a,c) . as shown in figure 4b, by the end of one representative simulation (3000 ms training time), the feed-forward ampa connections among the py cells that receive direct ca1 inputs are strengthened, whereas the feedbackward ampa connections are weakened. since the inhibitory stdp rule is symmetric and ltp-only, all the in-py gabaa connections for the py cells that receive direct ca1 inputs are stronger by the end of the simulation (figure 4d) . however, for the py cells that do not directly receive ca1 input, the strengths of their gabaa synapses are almost unchanged because of their sparse firing activities. therefore, under the stdp rule for both excitatory and inhibitory synapses in the pfc network, the sequence initiated in ca1 is successfully transferred to the pfc network and stored in the recurrent synaptic connections of the cortical cell populations."
"the hippocampus (hpc) plays important roles in memory consolidation and sharp-wave ripples (swr) are believed to transfer the compressed temporary information stored in the hippocampus to the distributed cortical networks (buzsáki, 1989; [cit] ) through the abundant connections between the hippocampus and the cortex. systems consolidation theory hypothesizes that memory consolidation process redistribute the hippocampal-dependent memories to support integration of the newly acquired memories with the related existing ones by reorganizing the cortical networks [cit] . in support of this idea, it has been shown that during sleep, the neurons in prefrontal cortex (pfc) display learning-dependent reactivations when swr are generated [cit] ). furthermore, the firing of pfc neurons falls within the plasticity time window after hpc swr occurs [cit] ). it has also been reported that enhancing the oscillation coupling between hpc and pfc boosts the memory task performance [cit] . importantly, during sleep the neurons in pfc can also exhibit fast sequential reactivations with a compression factor of 6-7 compared to that during behavioral states [cit] . these experimental findings support the view that hpc swr indeed play important roles in memory consolidation during sleep."
"note that specifying lower bounds on s i (rather than using constraints to determine the exact values of s i ) is sufficient for our goal of finding a valid partitioning and priority assignment because any partitioning that is deemed schedulable assuming \"too much\" blocking is will still be schedulable if blocking is reduced. next, we consider arrival blocking, which tasks can incur if lower-priority, co-located tasks access shared resources."
"second, in sec. vi, we present a novel resource-aware partitioning heuristic, called greedy slacker, that often produces valid partitionings and priority assignments even in cases where prior heuristics fail. greedy slacker greedily assigns tasks such that the least slack (i.e., the difference between maximum response time and deadline) among all tasks is maximized. this approach is much simpler than prior resource-aware heuristics (see sec. ii), yet our evaluation in sec. vii-b shows that it delivers equal-or-better schedulability in a wide range of scenarios. while greedy slacker is not optimal, it is efficient in the sense that it scales to large problem instances."
"in the context of multicore platforms, a common limitation of all of the above-cited ilp formulations is that they either do not consider shared resources at all, or only local shared resources. in contrast, our objective is the efficient partitioning of task sets with global shared resources (i.e., resources accessed on multiple processors) that are protected by spin locks."
"the maximum interference i i of t i is the maximum total duration that a job of t i cannot execute due to higher-priority jobs executing on the same processor, not counting any time that higher-priority jobs spend spinning. to constraint i i, we first define the integer variable h i,x to denote the maximum number of jobs of t x that can preempt a single job of t i . this allows us to express the interference i i as the sum of interference a job of t i may incur from each other task:"
"). this ensures that the ilp solution is never \"optimistic\" (i.e., unschedulable under the msrp analysis), while also ensuring that a schedulable task set implies a valid ilp solution. next, we outline straight-forward extensions of our ilp formulation."
"to investigate the sequence learning capability of the cortex model under ca1 input, we implement stdp for both the py-py ampa synapses and in-py gabaa synapses to make sure that the excitation-inhibition balance is maintained throughout the learning process [cit] . the excitatory stdp for py-py ampa synapses has a classic asymmetric shape [cit], while the inhibitory stdp for gabaa synapses has a symmetric shape, which has been reported recently in layer v cortical network [cit] . the formula for the stdp of py-py ampa synapses is shown in eq. 1, where w ampa is the current ampa synaptic strength; w th 1 is the target ltp synaptic strength for ampa synapses; p 1 is the potentiation factor; d is the depression factor; τ p1 is the ltp time constant; τ d is the ltd time constant. the formula for the stdp of in-py gabaa synapses is shown in eq. 2, where w gaba is the current gabaa synaptic strength; w th 2 is the target ltp synaptic strength for gabaa synapses; p 2 is the potentiation factor; τ p2 is the ltp time constant. to prevent divergence, the synaptic weights for ampa and gabaa synapses are restricted in a defined range shown in eqs 1 and 2. the values of the parameters in the stdp rule is summarized in supplementary table s7 . note that our stdp rule assumes a linear integration of the stdp potentiation and depression effect when multiple spikes happen in a short interval. in experimental studies, it has been found that non-linear integration of stdp exists [cit] ."
"return s to all tasks assigned to a given processor, starting with the lowest-possible priority. for each priority level, tryassign checks whether the tasks to which no priority was assigned yet would remain schedulable under the current priority level (alg. 2, line 5). if so, it is further checked whether this priority assignment would cause tasks assigned to other partitions to become unschedulable (alg. 2, line 8). among all possible assignments, the current priority level is assigned to the task with the longest period (alg. 2, line 12). the algorithm continues until priorities are assigned to all tasks on the given processor, or no candidate task can be found for a priority level. in the latter case, t i cannot be assigned to the given processor and the function returns a value indicating failure (alg. 2, line 10). the function returns the minimal slack of all tasks assigned to the current processor if a priority assignment could be determined that ensures that all tasks are schedulable (alg. 2, line 16)."
"the hippocampus ca1 model is based on the previous work, which has demonstrated the generation of sharp-wave-ripples under noisy inputs [cit] . our adapted model consists of 400 pyramidal cells (py cells) and 100 basket cells (bs cells), which are chosen to comply with the ca1 neuroanatomy [cit] and other ca1 modeling work [cit] . note that even if the ratio may not be exact compared to the real biological ca1 network, the net inhibition or excitation can always be compensated by adjusting the relative synaptic strength between pyramidal and interneurons to accurately model spw-rs. similar to the previous publications [cit], the pyramidal cell model has five compartments, namely the soma, the basal dendrite, and three apical dendrites. since the axon has small surface area and does not contribute much to the lfp recordings, we do not explicitly model it. the basket cell is modeled as a three-compartment soma, without dendrite and axons. the geometry of each compartment is listed in supplementary table s1 ."
"the spin times s i,k can be further split into the delays due to different resources. that is, we can express s i,k as the sum of spin times s i,k,q that a job of t i is delayed (directly or transitively) due to requests originating on processor k for l q :"
"in this section, we present our ilp formulation of the task set partitioning and priority assignment problem in the presence of msrp-arbitrated shared resources. this approach is optimal with regard to the msrp analysis, that is, if a solution under the msrp analysis exists, a valid partitioning and priority assignment will be found. initially, the ilp formulation does not specify an objective function. that is, we accept any solution that satisfies all constraints of the ilp, which allows the objective function to be used to optimize other criteria (such as the required number of processors, see sec.v-d below)."
"note that the presented heuristic does not include terms specific to any locking protocol, nor does it rely on parameters that need to be tuned for specific task sets. in fact, our heuristic is oblivious to the choice of locking protocol and uses an intriguingly simple greedy approach. this is possible because our heuristic aims to maximize the minimal slack among all tasks, which implicitly considers the impact of blocking due to resource sharing. next, we evaluate runtime characteristics of our ilp-based partitioning scheme and the performance of our heuristic in comparison with prior approaches."
"recent studies on hpc-pfc interactions have revealed various neural activity modulations in both awake and sleep state [cit] . during sleep, it has been shown that the localized ripple oscillations detected in the lfp recordings at pfc are strongly coupled to the ripple events in the hippocampus [cit] . also, the coupling strength gets stronger after the animal performs a spatial learning task. our lfp simulation results are consistent with these observations, suggesting that the ripple-ripple cross-frequency coupling might serve as a communication link between the cortex and hippocampus for memory trace transfer."
"further, h i,x has to be set to at least (r i + j x )/p x for local higher-priority tasks. for lower-priority and remote tasks, h i,x should be allowed to take the value of 0 as they do not interfere with t i . this is achieved with the following constraint:"
"for the pfc network, when the ca1 py cells fire and send excitatory inputs to both the py cells and in cells in pfc through monosynaptic connections, the py cells in pfc fire sequentially (figures 3a,b) . the in cells also increase their firing frequencies. in the same time, as shown in figures 3c,d, the lfp recordings in the pfc network exhibit transient oscillations that have components in high gamma (60-100 hz) and ripple range (100-250 hz). this can also be seen from the spectrogram of the lfp recordings in pfc network (figure 3e) ."
"non-preemptive spin locks are in widespread use. for instance, the autosar 4.0 specification provides the suspendallinterrupts() and getspinlock() apis to initiate non-preemptive execution and to acquire spin locks, respectively. the msrp can thus be realized in today's systems."
"note that these constraints make use of the variables we already defined in our ilp formulation. more complex partial specifications or requirements can be implemented by introducing additional variables to model application-specific properties. such application-specific extensions to our ilp formulation do not require fundamental changes to our approach, but rather can be realized by specifying additional ilp constraints. we thus believe this to be a flexible technique well-suited to the realities of embedded systems development and optimization in practice."
"we compared schedulability under the greedy slacker heuristic, the mpcp partitioning heuristic [cit], bpa [cit], and the resource-oblivious any-fit heuristic (which tries the first-, best-, next-, and worst-fit strategies, and returns the result of the first to succeed). for any-fit, we considered the following variants:"
"to constrain these per-resource, per-processor arrival blocking times for t i, we first define a decision variable z i,q that is set to 1 if critical sections of other tasks accessing resource l q can cause a job of t i to incur arrival blocking. to consider arrival blocking due to a local resource l q, we enforce that z i,q is set to 1 if t i can incur blocking due to a local lower-priority task t x accessing l q and t i 's priority does not exceed l q 's ceiling:"
"2) resource-aware partitioning heuristics: resource sharing causes dependencies among tasks (i.e., β ) that are not considered by generic bin-packing heuristics. resource-aware partitioning heuristics account for these effects and take the resource access patterns into account when mapping tasks to processors. we outline the mpcp partitioning heuristic [cit] and the blocking-aware partitioning algorithm (bpa) [cit] ."
"here, by using one-sided rank sum test, we are only treating the forward replay (mi close to + 1) as significant events. these results show that given a proper noise level, the memory replay is indeed possible under non-specific background noise in the trained pfc network."
"the resulting ampa recurrent connection matrix and the sequence replay are shown in figure 7 . it can be seen that for the shorter training time (2000 ms), the feed-forward connections in the pfc network are already established through ltp (figure 7a) . however, under the stimulation to the first 10 py cells, the sequence cannot be retrieved because of the weak ampa connections that are not sufficient to depolarize the membrane potential (figure 7b) . for the longer training time, the ampa connections among the py cells in pfc are further potentiated (figure 7c ) and the sequence can be replayed by the same stimulation on the first 10 py cells (figure 7d) . these results support the idea that the \"silent engrams\" can get mature with longer training and eventually turn into \"active engrams\" for successful memory retrieval."
"the use of non-preemptive fifo spin locks can cause blocking that contributes to a task's response time. this spin time is determined by the mapping of tasks to processors and the task parameters that characterize its resource access patterns, that is, l i,q and n i,q . the spin time s i models the total amount of direct and transitive delay that a job of j i incurs due to busywaiting carried out either by itself or any higher-priority job (by which it was preempted). the total spin time s i can be broken down by the remote processors on which the critical section is executed that causes the spinning to occur. we let s i,k denote the worst-case cumulative delay incurred by any job of t i due to critical sections on processor k. then:"
"note that, although we specify all of these terms in our ilp formulation through constraints, we often do not use tight constraints on these terms, but rather upper or lower bounds that are sufficient for our goal of finding a valid partitioning. for instance, we impose only lower bounds on the spin time s i of a task. as a consequence, if a solution to the ilp formulation, and hence a valid partitioning of a task set, can be found, this means that the task set under the partitioning implied by the set of a i,k and π i,p variables is schedulable; however, no other conclusions can be derived from other ilp variables (e.g., about arrival blocking b i or spin delay s i ) as these variables are not constrained to be accurate. rather, they are merely constrained to be \"sufficiently large\" to rule out unschedulable partitionings. this exploits the observation that the ilp solver has an \"incentive\" to minimize each b i, s i, and i i to satisfy constraints c6 and c7; it is therefore not necessary to specify upper bounds for variables contributing to b i, s i, or i i . as an analogy, in object-oriented terminology, the set of a i,k and π i,p variables represent the \"public\" interface to our ilp-based partitioning approach, whereas all other variables should be considered \"private\" and for ilp-internal use only. next, we specify constraints to model the interference i i, which reflects delays due to preemptions by higher-priority jobs (modulo any spinning of such jobs, which is included in s i )."
"such inadvertently wasteful task allocation can prevent multicore platforms from being utilized efficiently, in the sense that additional or faster processors may appear necessary, even though, given a better partitioning, a simpler and cheaper platform could have sufficed (we provide empirical evidence supporting this observation in sec. iv). in many embedded application domains such as avionics, automotive systems, or mobile systems, such a superfluous increase in space, weight, and power (swap) requirements-and ultimately cost-can render a product technologically or economically unviable. to realize the full potential of embedded multicore platforms, resource-aware partitioning approaches are needed."
"in our model, we allow jobs to incur release jitter to model precedence constraints for tasks [cit] . that is, after arrival of t i 's v th job at time a i,v, it may take up to j i time units before the job is released and becomes available for execution."
"it has been reported that during the early stage of pfc engram cell formation, the natural cues cannot induce the memory recall. however, the engram cells can be activated by optogenetic stimulations, which will lead to successful memory retrieval [cit] . also, the spine density of pfc engram cells in the later days of learning is significantly higher than the early days [cit] . these findings lead to the postulation of \"silent engrams\" and \"active engrams, \" which refer to the neurons that encode memory, having weak or strong synaptic connections between them, respectively. to explore this idea from a computational perspective, we simulate the network for two different numbers of swrs during training (2000 ms, 10 swrs and 3800 ms, 19 swrs) and test the sequence replay using strong current injection to the first 10 py cells."
"we consider the jitter j i, the deadline d i, the cost e i, the period p i, the maximum request length l i,q, and the maximum number of requests n i,q of each task t i to be given task properties that are constants (from an ilp point of view). similarly, the number of processors m and the number of tasks in the task set n are considered constant. all other terms used in the ilp constraints are variables unless specified otherwise. at the core of the ilp formulation are four helper variables, which we define first."
"for settings where the computational complexity of the ilpbased approach is prohibitive, we proposed the resource-aware greedy slacker partitioning heuristics, which we evaluated with schedulability experiments, as we discuss next."
"toward this goal, we build a unified biophysical computational model that couples a hippocampal ca1 network [cit] with a pfc network to study the memory transfer from the hpc to the pfc and memory trace reactivations. under the sequential inputs from a virtual ca3 network, the ca1 network generates ripple range oscillations in the local field potentials along with simultaneous sequential pyramidal cell replays. the firing activity in ca1 potentiates the pyramidal cells in pfc through the monosynaptic connections to induce coordinated sequence reactivations. we demonstrate that the sequence can be transferred and stored in the recurrent connections of the pfc network through spike-timingdependent plasticity (stdp) between pfc neurons. later, we find that the stored memory traces can be reactivated through two different mechanisms, namely the cell-specific local stimulation and non-specific spontaneous background noise. interestingly, in both conditions, the memory trace reactivation is robust to the connections loss between the pyramidal cells in the pfc network. finally, the reactivation quality in the pfc network can be improved by either by increased number of swrs or adjusting the background noise level to optimal value. our work provides a mechanistic model for the hippocampus dependent memory formation through swr based cortex-hippocampus interactions in awake state and examines neuronal and network level parameters affecting spontaneous and stimulus-induced memory retrieval."
"it should be noted that our model mainly focuses on the memory trace transfer from ca1 to pfc via monosynaptic connections. this model structure is a simplified version that covers part of the hpc-pfc networks. as mentioned before in the section \"materials and methods, \" anatomical studies show that besides the direct monosynaptic connection from ventral ca1 to pfc, there are also abundant indirect multisynaptic pathways from ca1 to pfc via thalamus and connections from ca3 to pfc via lateral entorhinal cortex. a more comprehensive model including intermediate brain networks can be built to further investigate the memory transfer between hippocampus and pfc in multiple time scales ranging from seconds to hours and days. also, it should be noted that our model does not intend to reproduce all the physiological phenomena during memory transfer and consolidation observed in animal experiments. for example, after the forward sequence transfer, our model does not generate bi-directional sequence replays in the pfc network. also, in this study, we do not model the signature lfps (cortical slow oscillations and spindles) that are important for memory consolidation during slow-wave sleep. finally, in the current paper, the memory trace transfer is demonstrated as an equivalent transfer of sequential activity from hippocampus to the pfc. we want to clarify that the general memory consolidation and transfer is much more complicated and involves the integration of new memory with related existing hippocampal-dependent memories in the distributed cortical networks."
"we first split the total arrival blocking b i into the blocking times b i,q due requests from other tasks for each resource l q :"
"in practice, tasks often share limited resources such as i/o devices, co-processors, or message buffers, which requires the use of locks to serialize conflicting accesses. for example, the autosar specification [cit] mandates the availability of spin locks for this purpose. importantly, when using locks, tasks are no longer fully independent as they may be subject to blocking, which must be bounded and accounted for during schedulability analysis. crucially, as we review in sec.iii-b, the extent to which tasks are subject to blocking depends on how resource-sharing tasks are assigned to processors. classic bin-packing heuristics, however, do not consider blocking and thus are liable to expose tasks to excessive blocking."
"the performance of an optimal partitioning scheme in terms of schedulability is given by its definition: for each task set that can be partitioned such that all tasks are schedulable, an optimal partitioning scheme will find such a partitioning. optimal partitioning approaches, however, are inherently complex which raises the question of computational tractability. we evaluated the proposed optimal ilp-based partitioning scheme in terms of average runtime depending on two key task set characteristics: total utilization and task set size. for solving the generated ilps, we used the cplex 12.4 [cit] optimizer running on a server-class machine equipped with 24 intel xeon x5650 cores with a speed of 2.66 ghz and 48 gb main memory."
"real applications are likely to exhibit some structure. however, tasks also often interact via resources shared across group boundaries (e.g., autosar has the concept of a virtual functional bus, which is shared by all tasks [cit] ). to study the effects of cross-group resource sharing, we considered the same task-group scenario as before with the difference that a single resource is shared by all task groups. the results are shown in fig.3c . introducing cross-group resource sharing again results in a few, large connected components that the mpcp heuristic and bpa fail to partition effectively. notably, the greedy slacker heuristic yields high schedulability results independently of the structure that a task set may (or may not) exhibit, and does not depend on any protocol-specific heuristics or parameters (besides appropriate response-time analysis) the reported trends can be observed over the full range of considered configurations, which shows greedy slacker to be an attractive choice in a variety of scenarios, especially if it cannot be guaranteed that task sets will always exhibit a convenient structure."
"the initial parameters of gaussian mixture, w, play an important role in the efficiency and robustness of the new method. a good guess of important regions can dramatically increase the speed of convergence and decrease the required number of samples while a poor guess can either increase the computational cost or cause convergence to only some of the important regions instead of all of them. [cit] method has two major deficiencies in the selection of the initial w [cit] .to correct these deficiencies, it is recommended herein that the latin hypercube sampling method be used to generate μ j in eq. 10. as an efficient stratified sampling method, latin hypercube sampling is capable of generating representative values of the whole sample space. correlation between random variables is controlled with simulated annealing (vořechovský, 2004) . the target correlation matrix can be defined according to the configuration of the considered structural system. for a series system, a negative-correlation matrix can be used as the target correlation matrix. [cit] algorithm is employed to find the nearest correlation matrix if the proposed correlation matrix is not positive definitive. this algorithm is also able to deal with systems with elements of different degrees of importance."
"it is the fraction of generated packets by received packets. that is, the ratios of packets received at the destination to those of the packets generated by the source. as of relative amount, the usual calculation of this system of measurement is in percentage (%) form. higher the percentage, more privileged is the routing protocol."
"proof. the minimum and maximum values for each of the n pages containing a records per page need to be found to create the value intervals (line 3), hence the cost is n a. we run the code between lines 2 and 12 n t times because during each iteration of the while loop we process t of the n pages either into a naturally occurring page run or normal sorted run. the shortest path algorithm used in line 5 incurs the dominant run time cost and it has a run time complexity of o(m + e) [cit] . therefore, the run time complexity of"
"in previous experiments, we used the default real data column 7 of the us census data. in this section, we report the results for columns 6 to 20 [cit] data. this allows us to show that the use of column 7 in the previous experiments was representative of the different columns of the us census data set. we actually tested the first 100 columns of the real data but due to space constraints, we only show the results for columns 6 to 20. the performance difference between the algorithms did not vary much across all 100 columns tested. the columns below 6 were not tested since they contained categorical data instead of numerical data."
"future additions, such as the random walk with reset implementation, can be added to this service without any additional effort on the part of the user beyond an update of the application. we will also be adding support for real values, rather than binary inputs for the input vector, which will allow users to experiment with diffusing effect sizes on networks. this might be useful in, for instance, network-based genome wide association studies as a way to implicate weak associations that occur together in network neighborhoods. we will also be adding support for weighted edges. edge weights may be a good way to encode interaction confidence. different edge weights might also be an appropriate way of encoding different edge types into the same network diffusion process."
1) safety-critical these are used in the case of hazardous situations (e.g. collisions) [cit] . it includes the situations where the danger is high or danger is imminent [cit] . safety-critical applications involve communication between vehicles (v2v) or between vehicles and infrastructure/infrastructure and vehicles (v2i/i2v).
"2) safety-related these include safety applications where the danger is either low (curve speed warning) or elevated (work zone warning), but still foreseeable [cit] . in safety-related applications, the latency requirements are not as stringent as in the case of safety-critical ones. safety-related applications can be v2v or v2i/i2v."
"flash memory is becoming more popular due to its rapidly decreasing price and increasing performance. the compact form of flash memory is used for small mobile devices while larger ones called solid state drives (ssd) are used to replace hard disk drives. recent advances in flash memory technology has meant ssds with the capacity of hundreds of gb are becoming ever more affordable. the study by lee et. al. [cit] shows that the use of flash memory can greatly increase the performance of database systems. the performance increase comes from flash memory's ability to perform fast random reads. however, writes are comparably slow due to flash memory's inability to write in-place. therefore, any effective algorithm that uses flash memory needs to take advantage of fast random reads while performing writes sparingly."
19. filter out the first neighbors of the vemurafenib query that did not make the lox--imvi filter cut by making a filter to select nodes where diffusion_input is 0 (i.e. not in the query) and diffusion_output_rank_1 is not less that 262 (fig 2c) .
"in the case the number of already discovered nodes in a certain sector crosses the expected number of neighbors, the node may stop staying in that sector. similarly, if the node finds no new neighbor information repeatedly (say, 3 times) in a particular sector, it will decide that there is no undiscovered neighbor there and it will not switch to that direction for neighbor discovery from the next iteration."
"this paper develops a collaborative neighbor discovery (cond) mechanism for directional sensor networks (dsns), where nodes exchanges polling messages in different sectors to find their neighbors. the collaboration among the nodes, i.e., indirect discovery helps the cond to reduce discovery latency significantly. the performance evaluation results, carried out in ns-3, show the effectiveness of collaboration in reducing neighbor discovery delay and operation overheads."
"which indicates that only one single sample is needed to estimate the probability of failure. though the direct use of eq.7 is impossible due to the unknown p f, a close approximation of the optimal sampling function can significantly enhance sampling efficiency. to achieve this objective, an iterative process can be used to update a sampling kernel progressively to approach a near-optimal sampling function. this category of importance sampling techniques is referred to as adaptive importance sampling. [cit] utilized this type of methods to look for the location (i.e. the mean vector μ vopt )of the optimal sampling function iteratively. [cit] updated adaptively both the best location and the best shape (i.e. the covariance matrix σ vopt ) of the optimal sampling function."
"at the beginning, all nanosensors -except nccs -remain in harvesting mode. after the time allocation, nc sends a wake-up preamble to activate the layer with the highest priority, in turn, activating all the adtns of that layer for transmission."
"generally, the probability of failure is extremely low in structural reliability problems, resulting in the low efficiency of crude monte carlo simulation. in order to improve efficiency, importance sampling has often been used by switching the sampling effort to the more important region(s)with h v (x;v), the pdf of a new sampling function with parameters v:"
"contemporary research on moocs agrees to a great extent that learners at scale represent rather loosely coupled groups than longlasting communities of learners [cit] . this conclusion comes as no surprise, given that moocs usually bring together learners from all around the world, with different backgrounds, intents, and motivations to engage with a course [cit] . however, one of the reasons that interactions between students never evolve into communities in learning at scale might be simply related to the length of a course. usually delivered in a short period of time (e.g. between four and eight weeks [cit] ), moocs perhaps do not allow for more intensive collaboration to occur in the first place (or at least do not allow as part of course design) [cit] . nevertheless, the importance of studying emerging groups of learners in studying at scale has been well-evidenced [cit] ."
"sector (a) number of rounds to di scover neighbor nodes the neighbor discovery is improved considerably for higher number of sectors. this is happened due to the fact that the number of contending nodes is decreased in each sector with the reduction of antenna beam width. it further reduces the number of collisions and conflicts in neighbor discovery process. we also observe that the number of rounds and control byte overhead required for neighbor discovery in cond(d) and cond(d-i) are much less than those of sand, as shown in fig. 4 (a) and (b). however, the excessive increase of sector numbers decreases the probability that the two neighbor nodes face to each other and they require additional rounds to discover each other, as depicted in the fig. 4 ."
"eq. 2means that the performance function should not be very complex. for time-dependent reliability analysis, the performance function can usually be simplified to either eq. 1 or the following equation:"
in modern vehicles battery power and storage are unlimited. thus it has enough computing power which is unavailable in manet. it is helpful for effective communication and making routing decisions.
"these are applications that provide traffic information and enhance driving comfort. non-safety applications mostly involve a v2i or i2v communication [cit] . these services access the channels in the communication system, except the control channel. they access the channel in a low priority mode compared to safety applications."
"the inter-cluster-inter-layer period starts (see algorithm 4). the nccs aggregate data and forward them to nccs of an adjacent lower layer. after forwarding data, nccs call for new ncc election using round-robin scheduling. new nccs then multicast their new status to other ncms. previous nccs give their information list to new nccs and go to harvesting mode. all nodes except the new ncc start harvesting and the data transmission mode is paused until another wake-up message arrives. fig. 4 shows three possible transmission scenarios. in the first scenario (a and d) where the priority order of the 3 layers are set as layer 3, layer 2, and layer 1. to initiate the transmission, the nc first sends a wake-up preamble to layer 3. the adtns belonging to layer 3 are activated in response to the preamble and set to data transmission mode while the remaining adtns stay in harvesting mode. in addition to the adtns of layer 3, the nccs of all layers are activated during the transmission process. once the data transmission is completed, new nccs are elected and all other adtns along with previous nccs return to harvesting mode. in the second scenario (b and e) the transmission layer order is changed to layer 2, layer 1, and layer 3. here, time slots are allocated for layer 2 first, and the wake-up preamble is sent to layer 2 to complete the transmission which is then followed by layer 1 and finally by layer 3. in the third scenario (c and f), only two layers transmit data in the order layer 1 and layer 3. as layer 2 does not transmit, no time slot has been allocated for it. in the first time slots upon arrival of the wake-up preamble layer 1 transmits and then layer 3 transmits in the second time slot. the generic flow chart of the data transmission process is shown in fig. 5 ."
"we do not control when the re-sort is needed, but instead assume some mechanism exists to tell us when re-sorting is necessary. in this paper, we extend the traditional external merge sort algorithm to take advantage of partially sorted data and the unique characteristics of flash memory. our approach uses the partially sorted nature of the data to reduce the number of writes at the expense of increased random reads. this is particularly suitable for use on flash memory since flash memory is fast at random reads but slow at writing."
"we have included advanced settings to adjust the time parameter. however, we have found that the order of nodes that is returned by a query is quite robust to the choice of t. in order to demonstrate this, created a scale-free random graph of increasing size and diffused a random query of 10 percent of the nodes. s1 we have implemented this calculation using the scipy python library [cit] on our servers. however, the installation of scipy is not necessary for the use of the service. alternatively, this same queries can be run programmatically through the api. for an example, see the jupyter notebook at: https://github.com/idekerlab/heat-diffusion/blob/master/demo.ipynb."
"as expected, when the amount of updates increases, naturalmsort loses its performance advantage against the competing algorithms. however, it is encouraging to see that naturalmsort can tolerate around 50% update in both u pdatep ercentage and m axu pdaterange before it starts to be slightly worse than the competing algorithms. these results show naturalmsort can tolerate a large amount of updates to partially sorted data when finding naturally occurring page runs."
"experiments performed on the speech retrieval collections containing conversational speech [cit] suggest that classic information retrieval methods alone are not sufficient enough for successful speech retrieval, especially when the collections contain speech data in other languages than english. the biggest issue here is that the query words are often not found in the documents from the collection. one cause of this problem is the high word error rate of the automatic speech recognition (asr) causing the query words to be misrecognized. this problem can be dealt with through the use of asr lattices for ir. the second cause is that the query words was actually not spoken in the recordings and thus are not contained in the documents. to deal with this issue the query expansion techniques are often used."
"2) average end-to-end delay (e2e delay) it is the calculation of typical time taken by packet (in average packets) to cover its journey from the source end to the destination end. in other words, it covers all of the potential delays such as route discovery, buffering processes, various in-between queuing stays, etc, during the entire trip of transmission of the packet. the classical unit of this metric is millisecond (ms). for this metric, lower the time taken, more privileged the routing protocol is considered. in this paper the analysis of ad hoc routing protocol is done in realistic scenario of vanet. after doing the simulation based analysis of aodv, dsr, olsr and dsdv in realistic scenario of vanet we can see that the performance of aodv in terms of pdr is very good approximate 98% and dsdv is approximate 97%. olsr has average performance as the pdr. the average end to end delay of aodv is very high. the dsr performs well in both of the scenario in terms of avg. end to end delay. olsr is also having low end to end delay. packet delivery ratio of aodv is better than other three protocols so we can say this protocol is applicable to carry sensitive information in vanet but it fails for the scenario where transmission time should be very less as it has highest end to end delay. for quick transmission dsr performs well but not suitable to carry information as packet loss is very high. the performance of olsr is average"
"it should be noted that re-sorting partially sorted data has been studied extensively in the existing literature in the form of adaptive sorting [cit] . however, almost all existing work in this area is focused on internal sorting (where all data fit in ram). the adaptive internal sorting algorithms are not designed to minimize the number of accesses to secondary storage by using a ram buffer and therefore are not suitable for re-sorting data residing in secondary storage (the focus of this paper)."
outage (p out ) is given by eq. (6) with q[·] error function when γ j falls below a threshold (γ th ) [cit] .
"where y is the number of multipaths between the source and the fusion node, γ y is the yth link's sinr from the source node to the fusion node, and n is the number of layers. the outage capacity probability, c(p out ), determines the constant data rate in non-outage fading states using eq. (9) the communication mechanism consists of inter-cluster and intra-cluster communication. after a layer is activated, data transmission begins. in intra-cluster communication, all adtns send their data packet to the ncc within their associated cluster. ncc fuses data and conducts inter-cluster communication towards the nc. once a transmission is completed by an ncc, it calls for new ncc election within the same cluster for conducting succeeding transmission. after a new ncc is elected, previous nccs with retired adtns harvest until another wake-up call arrives from the nc."
"the traditional external merge sort algorithm has two phases. in the first phase, blocks of data are loaded into ram and sorted to create sorted runs. in the next phase, the sorted runs are repeatedly merged until a single sorted run containing all the data is created. our key insight is that the sorted run generation phase produces a lot of writes which can be minimized by finding disk pages that form a \"naturally\" occurring page run. we define a naturally occurring page run as a sequence of pages whose record value intervals do not overlap each other. therefore all values in the second page of the naturally occurring page run are larger than all values in the first page and so on. p2 p1 p3 p4 p5 p1 p2 p3 p4 p5 p1, p3, p5"
"where σ kij areelements of the covariance matrix of the k-th gaussian component, and σ pre,kij are the elements of the covariance matrix of the k-th gaussian component right after the preliminary sampling (i.e. the sampling to arrive at the near-optimal sampling function).based on the results of a trial-and-error process, it is recommended herein that k opt be reduced from 2.00 to 1.00at an interval of 0.05 as long as the simulated failure probability from n k samples does not drop dramatically [cit] .this control process can mitigate the observed oscillation while preserving the computational efficiency."
"networks operating in the terahertz (thz) band (i.e., 0.06 − 10 thz) offer promising solutions for many applications, including health monitoring, biochemical weapon monitoring, and plant monitoring, which otherwise are not possible [cit] . recent advances in thz communication allow development of a new generation of nano-em communication schemes. because many molecules resonate within the thz band, thz radiation may exploit this to penetrate opaque materials with high selectivity [cit], making it suitable for biomedical and healthcare applications [cit] ."
"route maintenance is the mechanism by which node s is able to detect, while using a source route to d, if the network topology has changed such that it can no longer use its route to d because a link along the route no longer works. when route maintenance indicates a source route is broken, s can attempt to use any other route it happens to know to d, or it can invoke route discovery again to find a new route for subsequent packets to d. route maintenance for this route is used only when s is actually sending packets to d."
"where rgrc, rgw c, m rg, m w c refer to the following costs respectively: sorted run generation read io cost; sorted run generation write io cost; merge phase read cost; and merge phase write cost. avoiding some rgrc would mean we can create the sorted runs without ever loading some of the pages into ram. this seems impossible unless we already have some kind of index on the data. however, we assume no such index exists. although some existing techniques try to avoid some rgw c and m rc by keeping the last generated sorted run in ram and use it directly during merging, this approach does not result in much savings if the data set size n is much larger than the memory size m . finally, it seems impossible to avoid m w c since the aim is to generate a sorted sequence onto flash memory. in contrast to the previous cost analysis, it is possible to avoid close to all rgw c by finding naturally occurring sorted runs using our technique described in section 5 and only write indexes to them rather than the actual runs themselves. in addition, avoiding write io is much more profitable than read io for flash memory due to the asymmetrical read versus write costs of flash memory."
"13. repeat steps 11 and 12 with the a375_skin column to create a new diffusion result. 14. create a new filter, selecting nodes that are in the top 10 percent of each of the original vemurafinib query and lox-imvi query, but not in the top 10 percent of the a375 query (fig 2b) ."
"where r(t) and s(t) are the time-dependent resistance and load effect, respectively. in order to assess structural safety during the entire service life, various methods have been proposed by researchers [cit], e.g. the point-in-time reliability method, the time-integrated method, and the stochastic-process-based method. among these methods,the stochastic-process-based method is the most accurate and assumes that the arrival of live load events follows a discrete stochastic process [cit] . using this method, the time-dependent failure probability can be computed using the conditional probability theory as follows:"
"the network is organized in layers to help the nanosensors discover the layers they belong to. assuming the sensing field to be circular, at first the whole network is divided into several virtual layers with respect to the nc. the radius of each layer is computed as n r 2 where n is the number of layers and r is the transmission range of each nanosensor. layers are divided in a manner that ensures the transmission range of each nanosensor of a particular layer spans its adjacent layers. upon receiving a broadcasted message from nc, all the nanosensors calculate their distances from nc, compare their distances with the layer radius, and register themselves to appropriate layers. the mechanism of nanosensor distribution over layers (ndl) is presented in algorithm 1."
"in this section we have described about the tools and methodology used in our paper for analysis of adhoc routing protocol performance i.e. about simulation tool, simulation setup(traffic scenario, mobility model) performance metrics used and finally the performance of protocols is represented by using excel graph."
(a)shape of the optimal sampling function (b)near-optimal sampling function after adaptation figure 1 shape of optimal sampling function and its approximation using the proposed method (a) mori and ellingwood's method (b) bucher's method figure 2 approximation of the optimal sampling function using the existing methods
"in the future research, we aim at further exploring student behavior in those different groups, identified as close submitters. specifically, the population of accounts detected as close submitters can be performing different behaviors such as genuine learning groups, unethical collaborations, and even cheating by using methods like cameo. while previous research on cameo [cit] targeted only identifying cameo behavior, the approach in this study is broader and capable of detecting more situations; additionally, previous work on cameo used the ip of the submissions for the detection, whereas for the algorithm introduced in this study that was not a requirement. as the certificate accomplishment ratio of close submitters to be lower in this study, this finding could be associated with cameo where students would create fake accounts to use exhaustive search to harvest correct solutions, and those accounts would not earn a certificate [cit] . some of these behaviors might be more hurtful for the learning process than others, and maybe actions should be carried out if accurately detected; however more work is still required in the direction of how learning is affected by these different behaviors."
"this study has been focused on providing an algorithm and a method to detect accounts that submit their assignments close in time in online learning environments. we have discussed design details and applied the algorithm to two moocs delivered by the university of edinburgh on the coursera platform. we have shown how the population of detected accounts labeled as close submitters had features statistically different than the rest of accounts in the course. we hypothesized that this population of close submitters were students who were collaborating or were engaged in some academically dishonest behavior, and that is why they were able to achieve certificates with much less activity with the course contents than their peers."
"four adhoc routing protocols are used, aodv, dsr, olsr and dsdv. aodv and dsr is reactive (on demand) where as olsr and dsdv is proactive (table driven) routing protocol. table i shows the comparison between proactive and reactive routing protocols."
"the data used in the study were collected from two moocs offered by the university of edinburgh: introduction to philosophy (phil) and music theory (music). both courses utilized graded course quizzes and lasted seven and five weeks, respectively. both courses had one graded quiz per week, with 6-12 and 10-14 questions per quiz, respectively. students did not receive any specific instruction to encourage collaboration. we used coursera trace data in json, which includes in a raw format of events, most of the actions and clicks performed by the student while interacting with the mooc. overall, we collected data about 2,359 and 5,159 students from the phil and music courses, respectively. for each student in both courses we extracted:"
is the expected value of ln(h v1 (x)/h v2 (x)) with x being drawn following the pdf h v1 (x); and w is the group of parameters of sampling function h v (x;w).
"where, e residual k is the residual energy of nanosensor k, and e max is the maximum energy. if a given nanosensor does not find another nanosensor with larger ζ k, it declares itself as ncc. after nccs have been elected, cluster formation begins. the new nccs multicast short range advertisement messages to all nonclustered nanonodes within the same layer. each of the non-member nanosensors having a higher received signal strength indicator (rssi) sends a join request to its respective ncc and registers itself as an ncm. this process continues until clusters are formed and all nodes are assigned to their respective nccs. to reduce clustering overhead, clustering is done only once for the whole network. only the ncc will be changed after each transmission using a round robin process. the ncca is presented in algorithm 2."
"external sorting is a fundamental operation in relational database systems, with applications such as: sorted merge join; facilitating fast range query lookups; result sorting; duplicate removal; uniqueness verifications etc."
"a cross-entropy-based adaptive importance sampling method has been proposed for the efficient computation of time-dependent reliability of structural systems. the method uses gaussian mixture to accommodate multiple important regions that may occur in structural systems. from the results and discussions presented in the paper, the following conclusions can be drawn:"
the results for this experiment are reported in table 9 . the results show the performance of naturalmsort did not vary much across the different columns tested. we analyzed the data distribution of the different columns and found that they all conform to a very skewed zipfian distribution.
the second measure we used was the mean squared deviation (msd) distance metric which calculated the distance between two vectors as the average of squared differences of vector elements:
"the cytoscape implementation of diffusion provides a simple and convenient user interface that allows a user to visually select a query node set, invokes the diffusion service, and then visualizes the diffusion results. it is delivered as a core feature as of cytoscape v3.6, and it is available as an app (http://apps.cytoscape.org/apps/diffusion) for previous cytoscape versions."
"this study aims at contributing to this line of research by providing automated methods for detecting students who tend to work together. the study did not start with the intention to target any specific behavior but it rather aimed at providing a general approach that will detect different types of associations between user accounts that can further be investigated later on. in doing so, we rely on both trace, discussion, and assessment data obtained from the coursera mooc. our primary goal is to provide an algorithm that would allow for identifying various types of collaboration in online learning settings in general, and moocs in particular."
"after the distances between all course participants had been calculated, we selected the list of close submitters by examining the distribution of distances between students. we did this by first plotting the distribution of account pair distances and then selecting the initial \"common-sense\" threshold for mad metric. in our case, we selected 30 min as the initial mad threshold for both courses and then calculated the corresponding quantile values (i.e., the percentage of distances smaller than 30 minutes) for mad distance metric in both courses. as the two courses had slightly different distributions of student distances, the same mad threshold value of 30 minutes resulted in two separate mad quantile threshold values."
"each of these packets consists of information about the adtn (ncm id) and nc (nc id), adtns distance from nc (d k ), adtn's layer information (l#), amount of data to be sent (d amnt ), and the residual energy (e residual ). in turn, the nc will prioritize data to be received, estimate total time considering propagation delay for a complete transmission and specify packet transmission order for the adtns. transmission timeline scheduling is done in the following three phases: phase 1: the nc allocates a variable-length transmission time slot for each layer. the slot length depends on the transmission parameters, i.e., total amount of transmittable data of the layer, total time required for the data to be transmitted, and priority of the data. the nc calculates the layer ordering weight -α -employing eq. (2) and determines the layer order of data transmission."
"in this paper, we proposed a novel approach for speeding up the sorted run generation phase of an external merge sort. the approach is designed to sort partially sorted data. the idea is to replace writing sorted runs to disk with writing indexes to naturally occurring page runs which can potentially save up to a factor of 1024 on write io costs. the approach is particularly suitable for the characteristics of flash memory, where write costs are higher than read costs. in this paper, we proposed a formula for determining the optimal size of naturally occurring page runs. we also map the problem of finding naturally occurring runs into the shortest path problem on a dag. accordingly, we use a shortest path algorithm to solve the problem. however, the algorithm was found to have high run time complexity and experimentally found to be impractical to use for even moderately sized data. hence, we proposed a fast heuristic solution which executes much faster but still finds a high percentage of naturally occurring page runs."
"future work on network propagation will work to bridge the gap to clinical applications; for example, considering propagated mutation profiles of patients, rather than cell lines as we have demonstrated here, will allow for the extraction of clinically relevant subnetworks that may inform patient care. in addition, many important questions remain about the use of network propagation: for instance, what are the best networks to use in different applications? how should heterogeneous edge types affect how propagation should be performed? a consistent algorithm such as the one provided here will be essential in answering these questions."
"in this paper, we have proposed a collaborative neighbor discovery (cond) mechanism for dsns. the sensor nodes discover their neighbors collaboratively by sharing the discovered neighborhood information with the surrounding nodes. the key idea behind the cond mechanism is that, each directional node performs neighbor discovery in each sector using a contention based polling mechanism. the goal of neighbor discovery mechanism is to increase the number of discovered nodes with minimum delay using a two-way collaborative polling mechanism. the proposed cond is fully distributed and it explores both direct and indirect neighbor discovery processes to reduce the latency, as depicted in performance evaluation results carried out in network simulator version -"
"the performance of the scheme was evaluated for energy saving probability, outage probability, and outage capacity. simulation results demonstrated that the proposed scheme outperforms the existing ones in energy saving and capacity. additionally, multisensor fusion at the fusion node ensures an enhanced link quality by decreasing the outage probability which is very important in case of smart healthcare applications."
"in the standard approach to the blind relevance feedback the number of documents and terms is defined experimentally in advance the same for all queries. in our experiments we would like to find the number of relevant documents for each query automatically by selecting the \"true\" relevant documents for each query to dynamically define the number of top retrieved documents to be used in brf."
"remove processed pages from ram buffer 13: end while 14: if index page is not empty, flush it to flash memory theorem 5.1 (run time complexity of algorithm 1)."
"the results in figures 8(b) and 8(c) show that naturalmsort outperforms tradmsort and replacemsort at larger ram sizes because of its ability to reduce write io costs rather than read io costs. this matches our aim explained in section 4. the reduced write io costs is due to the high percentage of naturally occurring runs found, as shown in table 5 ."
"the need for energy-efficient, low cost, simple and scalable healthcare solutions is at its peak and can be served with miniaturized devices (md) [cit] . however, existing md systems have difficulty performing multiple tasks simultaneously, i.e., functions offered by consumer electronic devices, and functions related to healthcare (e.g., longterm health monitoring). the wireless body sensor network (wbsn) can achieve this objective by providing inexpensive and continuous health monitoring by implanting tiny biosensors inside the human body, which do not impair normal activities [cit] . these devices then communicate among themselves, as required, and transmit the required information to a receiving end. to make this communication smooth, different strategies have been investigated [cit] . in particular, due to the short communication range of the nanodevices, a multihop em communication paradigm has been extensively explored with appropriate em power [cit] . some late methods addressing communication at the nanolevel include graphene-enabled em communication, molecular diffusion, foster resonance energy transfer [cit], etc. however, direct applicability at the nanoscale is hampered by device size, complexity, and energy consumption [cit] . additionally, the large bandwidth of the thz band (i.e., terabits per second, tbps) is susceptible to shadowing and noise [cit] ."
"in a directional wireless sensor network (dsn), sensor devices with directional transmission and reception capabilities deliver sensed data packets to a sink or base station in multi hop fashion. nowadays, many applications including infrastructure monitoring, health-care monitoring, target tracking, disaster response, etc. are implemented using dsns [cit] . the first step of designing a data communication protocol (media access, routing, etc.) is to identify neighbor nodes. hence, for directional nodes, it is mandatory that two nodes within the same transmission range must beam form their antennas to each other so as they can exchange neighbor discovery messages with each other [cit] . this requirement not only increases the discovery latency but also it decreases the accuracy at which the neighbors are discovered."
"given the success of this wide variety of applications, it is essential for network propagation to be readily available to any biologist, including those without dedicated bioinformatics support. cytoscape is a well-established platform for many network applications [cit], so it is a natural choice to host a network propagation algorithm. there have been a few cytoscape apps (genemania [cit], propagate, and tiedie) that have previously implemented network propagation algorithms; however, in these cases the basic network propagation operation is embedded within a more complex and extensive workflow or does not scale well to large biological networks. nonetheless, the presence of these apps demonstrate the community's need for a stand-alone, robust network propagation algorithm."
we only report the results for naturaldagmsort for experiment 1 since naturaldagmsort runs too slowly for all other experiments due to the larger data size.
"in this study, we have established conservative thresholds to detect close submitters which warrant further consideration of the number of accounts detected as a lower bound. for instance, we saw that certain communities became bigger when higher thresholds are used. however, as the increase in the recall would likely decrease precision, more work is required to set up the detection threshold and to improve the algorithm to control for false positives. for researchers seeking to replicate and use this algorithm, we would recommend to take our reported thresholds as a guide and encourage them to empirically seek the ones adequate for their case study and context. for example, the average difficulty and number of questions per quiz can also have an effect on the mean of the distribution. hence, a special care should be taken when trying to establish a threshold for detecting close submitters in any particular context. with this in mind, we suggest the use of averaged dissimilarity metrics that help for comparison among courses with different number of submissions and also squared metrics that can penalize large distances better."
"the speed and choice of path defines the dynamic topology of vanet. if we assume two vehicles moving away from each other with a speed of 60 mph ( 25m/sec) and if the transmission range is about 250m, then the link between these two vehicles will last for only 5 seconds ( 250m/ 50ms-1). this defines its highly dynamic topology."
"in the future, we plan to propose even faster and more effective heuristics for finding naturally occurring page runs. we will investigate how our approach can be integrated into the sorted merge join operation."
"for practical reasons, only specific paradigms can be applied to specific scenarios. for example, due to a very high absorption coefficient of acoustic waves in bones and muscle fiber, using acoustic paradigm based communication is impractical [cit] . nanomechanical communication requires physical contact between transmitting and receiving nanomachines, which is not always possible. the molecular and em communication paradigms have been very promising [cit] with molecules as the encoder, transmitter and receiver of information in molecular communication [cit] and em waves as the communication means among nanomachines in em communication [cit] ."
the optimization problem has analytical solutions for distributions in the exponential family [cit] . [cit] applied eq. 9 to time-independent reliability problems with multiple important regions by using gaussian mixture given below as the importance sampling kernel:
"the nanomachines with limited capabilities perform tasks by forming an interconnected nanonetwork. the traditional communication protocols are not applicable to enable the nodes to communicate among themselves at the nanoscale, these conventional systems need to undergo extensive revisions. based on the preferred transmission medium, different communication paradigms have been proposed over time, which include acoustic, nanomechanical, molecular, and electromagnetic (em) [cit] ."
"in this study, we approach this issue from a more general perspective where we can detect more diverse types of behaviors. in particular, we aim to detect user accounts of students 1 in online courses that always submit their assignments very close in time. by observing activities of identified groups, we aim at further understanding students' intention to work in groups -e.g., watching videos and submitting assignments around the same time [cit] or perhaps even focusing on more unethical collaborations between students e.g. cameo as reported in moocs [cit] . we want to address this issue by providing a systematic method and algorithm that could be easily applied to any online environment where students have to perform certain learning activities. this approach can potentially detect interesting associations such as unethical or genuine collaborations between students, but can also detect some students who engage in cameo behavior. more specifically, the objectives that we have for this study are as follow:"
"for both real and synthetic data, we did the following to convert them into partially sorted data. the data was first sorted. next, updates were introduced by randomly picking a certain percentage of values to be updated (we call this u pdatep ercentage). for each picked value x, a random new value was assigned in the range x − x δ 100 and x + x δ 100 using uniform random distribution, where δ is a parameter we call m axu pdaterange. note m axu pdaterange is a percentage value. table 2 shows the default parameter settings used for the real and synthetic data sets."
we used the selected triplets of close submitters to plot an undirected graph that represent the communities found by the algorithm. each of the disconnected components of the overall graph was a set of student accounts which were identified as close submitters.
"during the service life of a structure, its resistance is likely to deteriorate due to factors such as material degradations (e.g. steel corrosion) and damage from overloading or natural disasters. in addition, the load that a structure has to resist may change over time. therefore, the performance function of a structure is timedependent:"
"the remainder of the paper is organized as follows. related work on existing schemes is presented in section 2. the system model is presented in 3, which includes the channel behavior model and nanosensor energy model. section 4 is devoted to performance assessment of the proposed scheme from simulation results. in addition, final concluding remarks are provided in section 5."
the remainder of the paper is organized as follows: section 2 describes the characteristics of flash memory; section 3 describes the traditional external merge sort algorithm; section 4 describes why it is a good idea to minimize write io costs during the run generation phase; section 5 describes our approach of minimizing write io costs during run generation using naturally occurring runs; section 6 describes the experimental setup used to test our solution; section 7 shows the experimental results and analysis for our empirical study on the effectiveness of our solutions; section 8 describes the related work and finally in section 9 we conclude the paper and outline directions for future work.
"a. safety safety applications have the ability to reduce traffic accidents and to improve general safety. these can be further categorized as safety-critical and safety-related applications. in the design of security, it should be made sure safety messages which are not forged."
"we can map the value intervals of definition 5.1 to a directed acyclic graph (dag) as follows. each vertex represents a page p i and there is a directed edge from vertex p i to p j if and only if p i and p j are non-overlapping and p i 's range is less than p j 's range. there can not be any cycles in this graph because of the transitivity of less than. then finding a naturally occurring run of size t is equivalent to finding a path of length t . this is a simple variant of the longest path problem in a dag, where instead of finding the longest path, we stop as soon as we find a path of length t . the longest path problem is np for a general graph, however, for a dag, it can be computed in polynomial time by mapping it into the shortest path problem where the edge weights are negated."
"vanet is a special case of the general manet to provide communications among nearby vehicles and between vehicles and nearby fixed roadside equipments. vanet networks, nodes are characterized by high dynamic and mobility, in addition to the high rate of topology changes and density variability [cit] . vanets are a subset of manets (mobile ad-hoc networks) in which communication nodes are mainly vehicles. as such, this kind of network should deal with a great number of highly mobile nodes, eventually dispersed in different roads. in vanets, vehicles can communicate each other (v2v, vehicle-to-vehicle communications). they can connect to an infrastructure (v2i, vehicle-to-infrastructure) or infrastructure to vehicle (i2v) to get some service. this infrastructure is assumed to be located along the roads."
the reason the fast algorithm is generally the worst performer is that it tries to create longer sorted runs by taking too many read passes through the data. this can be seen from figure 8 (b).
"1. the proposed sampling method is more efficient than the existing methods, especially for series systems with multiple important regions. 2. with gaussian mixture as the sampling kernel, a multimodal near-optimal sampling function can be obtained after only a few steps of adaptation. 3. a number of numerical measures were proposed and shown to improve the efficiency and robustness of the proposed sampling method; these include the use of latin hypercube sampling, simulated annealing, appropriate design of the target correlation matrix, and updating of this matrix using higham's algorithm. 4. it is important to control correlation adaptation during preliminary sampling in order to eliminate possible oscillations of the estimated failure probability."
"ftl is a software layer between the file system and flash memory hardware. ftl provides the file system with an interface to the flash memory that is identical to that of the common hdd. as a result, common file systems such as fat, ata or sata can be used with flash memory. ftl performs updates out-of-place, garbage collection, wear leveling and error detection transparent to the file system. this is done by mapping virtual addresses to physical addresses. although ftl addresses the need for wear leveling on flash memory, it does not address the issue of the skewed read/write speed ratio."
"while less restrictive thresholds result in a higher algorithm recall and the number of correctly identified student pairs, it also reduces its precision and increases the false positive rate. given the recall vs. precision tradeoff involved in selecting the optimal threshold value, we tended to favor the algorithm precision over recall due to two reasons. first, as the use of a higher threshold exponentially increases the number of identified account pairs, it, in turn, produces larger account/student communities. however, those communities are far less likely given the distributed nature of learning in moocs and can be a conglomeration of several smaller communities through a certain number of false positive pairs. secondly, we were interested in examining the statistical differences in several measures between the close submitters and regular accounts which are significantly different in size. as a result, the effect of false positives on the score distribution of close submitters was far bigger than the effect of false negatives on the score distribution of the regular student accounts. due to these two reasons, in the rest of the analysis, we used the 6e−6 quantile threshold which identified 78 close submitter pairs in the music course and 17 close submitter account pairs in the phil course."
"in this section, we first formally define the problem of finding naturally occurring page runs and then show how it can be mapped into the well known problem of finding the shortest path in a directed acyclic graph."
"the jth hop's sinr (γ j ) can be computed by fusing equations (3) and (4) and considering additive white gaussian noise power at destination j (n j ) using eq. (5) [cit] . as per the data transmission mechanism, nc calculates the α value (see eq. (2)) to prioritize different layers' data dispatching and assigns them the necessary timeslots to relay. case 1 (a,d): layers are prioritized as layer 3, layer 2, and layer 1 to transmit during t 1, t 2, and t 3 timeslots. case 2 (b,e): the priority of the layers are layer 2, layer 1, and layer 3 and they transmit during t 1, t 2, and t 3 timeslots, respectively. case 3 (c, f): only two layers transmit in the order layer 1 and layer 3 during t 1 and t 2 timeslots."
this section shows the results for our first experiment with score normalization methods. for the standard blind relevance feedback we have chosen the settings used for brf in the paper dealing with the experiments on this collection [cit] -take first 20 documents as relevant and extract 5 terms with the best score for the query enhancement. on the training topic set the experiments with the selection of another number of top retrieved documents to be chosen as relevant for standard brf was also performed.
"we compared the performance of our approach against three rival external sorting algorithms. they are described below, along with any parameter settings we have used for them."
"the relevant documents selection can be described in the same way: first, we need to retrieve the documents which have the best likelihood scores for the query and second, we have to choose only the relevant documents which really generated the query. the only difference is that we try to find more than one relevant document. the normalization methods from osti-si can be used in the same way, but have to be applied to all documents likelihoods."
"a major problem in wnsn communication is the high probability of link failure due to low transmission power and high transmission losses (e.g., path loss and shadowing). this problem can be addressed successfully using cooperative communication among nccs (i.e., amplify-and forward, and decode-and-forward [cit] and information fusion at the fusion node [cit] . the role of the fusion node is executed by either the destination ncc or the nc where the information fusion occurs."
"the results show that naturalmsort outperforms the other external sorting algorithms for a wide range of update amounts. when an update amount is low, naturalmsort can outperform tradmsort by more than a factor of 2. this can be explained by two facts. first, naturalmsort can potentially save half of the write io costs and it also uses less cpu time. second, as we saw in the previous experiments, write io costs are the dominant costs on total execution time. due to the design of naturalmsort, it can virtually avoid any write io during run generation and thereby effectively halve the write io costs. in addition, we found tradmsort can spend a factor of 13.7 more cpu time for run generation compared to naturalmsort. this is because tradmsort needs to sort large runs in ram which takes a lot more time. in contrast, naturalmsort does not need to sort naturally occurring runs at all during run generation. although naturalmsort incurs slightly more cpu overhead during the merge phase (internal sorting of pages belonging to naturally occurring page runs) than naturalmsort, this overhead is much lower than the cpu overhead savings made during run generation."
"in this section, we will define the optimal size of naturally occurring page runs. this is non-trivial since larger naturally occurring page runs are hard to find, but smaller ones may cause more than one merge pass. the reason larger naturally occurring runs are hard to find is that they require more nonoverlapping pages. smaller naturally occurring page runs may increase the number of merge passes because it might exceed the maximum number of runs that can be merged in ram in one pass. therefore, we define the optimal naturally occurring run size as follows:"
"to understand the differences between close submitters and regular accounts, we examined the differences in each of the extracted measures (section 3.1). in addition to examining the distribution plots for each of the extracted measures, we also used a one-way multivariate analysis of variance (manova) to understand the differences between the two groups of accounts and a series of followup univariate t-tests for each of the dependent measures."
"one of the possible query expansion methods often used in the ir field is the relevance feedback method. the idea is to take the information from the relevant documents retrieved in the first run of the search and use it to enhance the query with some new terms for the second run of the retrieval. the selection of the relevant documents can be done either by the user of the system or automatically without the human interaction -the method is then usually called the blind relevance feedback. the automatic selection is usually handled only by selecting the first n retrieved documents, which are considered to be relevant."
the unknown model d i can be approximated by the collection model m c which was created as a language model from all documents in the retrieval collection. this technique was inspired by the world model normalization [cit] . the normalization score of a model d i is defined as:
"here we introduce diffusion, a network propagation algorithm delivered as both a new cytoscape feature and as an internet service available to a broad array of web, desktop and server-based applications such as jupyter notebooks. the service is callable via a rest-based application programming interface [cit] (api, documented at https://github.com/ idekerlab/heat-diffusion)."
"where, the value of 15 is chosen in such a way that the discovery latency is minimized. if a node discovers most of its neighbors in a sector, it is wise for the node to stay for less period of time there and vice-versa; otherwise, discovery delay will unnecessarily be increased. thus, the value of 15 helps us to dynamically control the staying period of a sensor node in a certain sector for neighbor discovery. let nd represents the percentage of nodes in a sector are already discovered, then the value of 15 is determined as follows, nd ~ 30%, nd ~ 50%, nd ;:: 80% ."
"in order to specify our algorithm, we first start with the basic notation used in the rest of the paper. let n denote the total number of students in a course and m the total number of graded assignments in the course. let us also define n vectors for each of the student so that"
"this work proposes a lightweight communication scheme suitable for wbsn operating at the thz band with four main contributions: i) first, an energy-efficient forwarding scheme based on nano cluster composition algorithm (ncca) is proposed for thz band communication in wbsn. ii) second, a channel behavior model is investigated on the basis of the aggregated impact of molecular absorption, spreading losses, and shadowing. iii) third, the joint impact of energy harvesting and energy consumption processes is considered for developing a nanosensor energy model, which includes the stochastic nature of energy arrival. iv) finally, closed-form outage probability expressions are deduced for both single and multilinks. to enhance information detection precision and energy-efficiency, in the case of multilink, a cooperative fusion approach is used where data from disparate sources are fused at a predefined fusion node. given the outage probability, the approach has been extended to determine outage capacity of the considered channel."
"the merge phase of our approach is the same as the external merge sort with two exceptions. first, pages belonging to naturally occurring runs are first internally sorted when loaded into ram. second, we reserve a small portion of the ram buffer to buffer index pages to naturally occurring page runs. we use the least recently used buffer replacement policy for the index page buffer. the size of the index page buffer can be very small, just 20 pages in our experiments. this is because each index page can contain indexes to 1024 pages of naturally occurring runs."
route discovery is the mechanism by which a node s wishing to send a packet to a destination node d obtains a source route to d. route discovery is used only when s attempts to send a packet to d and does not already know a route to d.
"the proposed approach (see fig. 2 ) uses a multi-layered topology based on the distance of deployed nanosensors from nc. to increase network stability and achieve high energy efficiency, each layer is divided into multiple nanoclusters and re-clustering is avoided. nano cluster controllers (nccs) from each layer are elected based on the residual energy of nanosensors. nccs collect data from nano-cluster members (ncms) and forward aggregated data towards nc. clusters of one layer are capable of communicating with clusters of the immediate upper or lower layers. it is assumed that nanosensors are capable of controlling power dynamically. ncc conducts intra-cluster communication using low power transmission range and inter-cluster communication using high power transmission range."
"in this section, we explain why determining the best cluster size (io unit) for an external merge sort is particularly important for performance. we highlight the difference between the best cluster size for hdd and flash memory. figure 1(a) shows that when the cluster size is large, only a very few runs can be merged in each step, resulting in more merge passes. during each merge pass, all data pages need to be loaded from the disk and written back to the disk in a sorted order. therefore, less merge passes result in less io. however, using smaller cluster sizes as shown in figure 1 (b) results in more random seeks (which is very costly on the hdd) because the same amount of data is loaded into ram in smaller clusters. the positive effect of smaller clusters is less merge passes. therefore, the best cluster size is non-trivial to determine for the hdd. this has been demonstrated experimentally in the paper by lee et. al. [cit] . the io cost of the merge phase (m cost) can be described using the equation below (assuming the output buffer size is one cluster): where n, m and c are the data size in the pages, the ram buffer size in the pages and the cluster size in the pages, respectively. hdd seek is hdd seek cost, hdd rc the hdd transfer cost for reading one page and hdd w c the hdd transfer cost for writing one page."
"molecular absorption causes attenuation or absorption loss depending on the type of molecular composition constituting the medium. the human body is the transmission medium when placed nanosensors constitute the bsn. in the human body, water molecules outnumber other molecules, thus, the value of the molecular absorption coefficient is given by the weighted average of different absorption coefficients of water molecules present in different body parts [cit] ."
"the experiments were conducted using a 256gb super talent ultra drive gx ftm56gx25h ssd. as mentioned in the introduction, ssds are high capacity flash devices, designed to replace hard disk drives. the read and write characteristics of the flash drive compared to common hdd are shown in table 1 . from the table, we can see the random read performance of the ssd is around 2.5 times faster than random write. however, the hdd random read and write performances are much more similar. the random read performance of the ssd is much higher than the hdd."
"2) reactive/ad hoc based routing reactive routing opens the route only when it is necessary for a node to communicate with each other. it maintains only the routes that are currently in usage. as a result it reduces the burden in the network. reactive routing consists of route discovery phase in which the query packets are flooded into the network for the path search and this phase completes when route is found. the various types of reactive routing protocols are aodv, pgb, dsr and tora."
"the overall idea behind the traditional external merge sort algorithm is to use the limited ram buffer to sort small portions of the input data at a time and store them as \"sorted runs\". then, the sorted runs are repeatedly merged into larger sorted runs until a single sorted run is produced. accordingly the algorithm is divided into two phases, the run generation phase and the merge phase. assume a ram buffer of m pages and an input data of n pages. the run generation phase loads up m input data pages into the ram buffer, sorts them and writes them back to disk as sorted runs. this is repeated until ⌈n/m ⌉ sorted runs are generated. figure 2 (a) shows this diagrammatically. figure 2 (b) shows an example which uses a three-page buffer to generate a three-page sorted run. figure 3 (a) shows a diagram describing the overview of the merge phase. in the merge phase, k sorted runs are merged at a time, where k is a user-determined parameter. each of the k sorted runs are allocated an input buffer which stores a cluster (unit of io) of the sorted run's data. a single output buffer is allocated to store the current portion of the merged sorted run. figure 3(b) shows an example of how sorted runs are merged. in the example, the data need to be sorted in ascending order. the first cluster (in this case just one page) of each sorted run is loaded into their respective input buffers. next, the output buffer is filled with the three smallest values of all the input buffers (crossed out values). next, the output buffer is flushed to disk and emptied. the same process is used repeatedly to fill the output buffer. whenever an input buffer becomes empty, the next cluster of the corresponding sorted run is loaded into the input buffer. this entire procedure is repeated until the k sorted runs are merged into a single sorted run. then, each of the merged sorted runs are combined repeatedly until there is only one resulting sorted run for the entire data set. section 8 reviews recent research into improving the traditional external merge sort algorithm and other approaches to achieving fast external merge sort."
"given that there are several potential operationalizations of the general problem description from the previous subsection, in this paper, we operationalized the problem using the following set of criteria:"
"as different courses utilize graded assignments in slightly different ways, the time required for their completion can be substantially different. the difference in course duration can have an adverse effect on the identification of close submitters. given that we also want to compare close submitters between the two courses, we used the initial mad quantile threshold values to establish the list of common quantile thresholds which were then applied to both courses and for both distance metrics. the use of the same quantile thresholds in both courses enabled us a) to compare the number of close submitters between the two courses, and b) to have a procedure for close submitter identification which did not depend on the particular course context and design. we opted to use more than one threshold value in order to examine how a particular threshold affected the number of close submitters identified."
"we also examined the differences between the close submitters and the rest of the course populations in terms of the five extracted measures described in section 3.1. to keep the comparison between two groups sensible, we compared separately students who obtained a certificate and those who did not (figure 3) . we see that both close submitters and regular accounts had a similar distribution of their final grades, regardless of whether they obtained a certificate or not. this was somewhat expected due to the ceiling effect of the certificate grade threshold. for the rest of the indicators, we can see a clear difference between the two groups, with close submitters having considerably lower values than the rest of accounts population for the certificate and non-certificate earners of both courses."
"where the time-independent dead load effect s d is isolated from the time-dependent load effect. the integration in eq. 2usually needs to be carried out with an efficient simulation method such as the adaptive importance sampling method [cit] . although the conventional adaptive importance sampling method [cit] can effectively reduce the computational burdenfor simple problems, the stochastic-process-based method still faces computational difficulties, especially for those problems involving multiple random variables and multiple important regions, e.g. in system reliability problems. this is mainly because the method [cit] employs a unimodal sampling function that cannot generate samples efficiently when the actual regions of importance are multimodal. to facilitate the application of the stochastic-process-based method in complex problems, a cross-entropy-based adaptive sampling method using gaussian mixture is proposed in this paper. [cit] method for time-independent reliability problems to time-dependent domains. [cit] . two numerical examples are given to illustrate the efficiency of the new method."
"an em wave in thz band experiences several attenuations (in particular absorption and spreading losses) while propagating through human tissues. both link distance d i,j (for i − j link) and signaling frequency f influence these attenuation."
"the score normalization methods from the open-set text-independent speaker identification (osti-si) problem were successfully used in the task of the multi-label classification to select the relevant topics for each newspaper article [cit] in the output of a generative classifier. this is the same problem as in the information retrieval task, where as the result only the ranked list of documents with their likelihoods is returned. usually the idea is, that the user of the retrieval system will look though the top n documents and therefore the specific selection of which document is relevant and which not is not needed. on the contrary when the blind relevance feedback is used, the selection of the true relevant documents can be very useful."
"the above feature necessitates that in about every 5 seconds or so, the nodes needed another link with nearby vehicle to maintain seamless connectivity. but in case of such failure, particularly in case of low vehicle density zone, frequent disruption of network connectivity will occur. such problems are at times addressed by road-side deployment of relay nodes."
"network propagation has become a critical analysis technique in computational systems biology. most commonly, the basic network propagation process starts with some query nodes, and a network. the query nodes are given some initial value, then a smoothing process is applied to the initial values on those query nodes, passing some of the value to neighboring nodes. by examining the final distribution of values, we can, for instance, find a subnetwork where the nodes are closely related by the network to the original query. more abstractly, network propagation supplies a robust estimate of network closeness, which can be used for many different applications. in computational biology, the most common meaning for the nodes is genes, and the edges usually represent various types of functional relationships between genes, for instance protein binding interaction, transcriptional regulation or signaling by phosphorylation."
"furthermore, we have varied the number of sectors from 2 rv 10 and the performance results are shown in fig. 4 . our in depth look into the simulation trace files reveals that .e -8 6"
"where and as well as and are the estimated probabilities of failure and their corresponding variances from the preliminary and the main sampling cycles, respectively."
"in vanet, the routing protocols are classified into five categories: topology based routing protocol, position based routing protocol, cluster based routing protocol, geo cast routing protocol and broadcast routing protocol. these protocols are characterized on the basis of area / application where they are most suitable. fig. 1 . shows the different routing protocols in vanet."
"we conducted six experiments. the first four experiments used the real data set and the last two used the synthetic data set. in the first experiment, we varied the data size of the real data. in the second experiment, we varied ram size. in the third experiment, we varied the amount of update. [cit] data. in the fifth experiment, we tested the scalability of the different algorithms by varying the size of the synthetic data set. finally, in the sixth experiment, we varied the record size of the synthetic data set."
"a detailed experimental study was conducted into the effectiveness of our proposed algorithms against three likely competitors, using both real and synthetic data sets. the results show our fast heuristic algorithm can find a high percentage of naturally occurring runs, even when there was a high percentage of updates. further, finding naturally occurring runs had a significant impact on reducing total execution time. the results show our approach prefers a larger ram to total data size ratio, since in these conditions, the required size of the naturally occurring runs are smaller. however, a ram to total data size ratio of 0.067 is all that is needed for our approach to outperform its competitors. our approach can halve the sorting time compared to its competitors under favorable situations but is only slightly worse under unfavorable situations."
although [cit] data. the synthetic data used in subsequent sections use uniform distribution. this means we have tested our algorithms across both a very skewed zipfian distributed data set and an uniform distributed data set.
"residual energy. this election process involves, at the initial round, the ncc candidates competing for the ability to upgrade to ncc by multicasting their weight, denoted by ζ k and calculated using eq. (1), to neighboring candidates."
"having specified a way of computing the optimal naturally occurring page run size and proven it is correct, we describe our algorithm for finding naturally occurring runs of the optimal size. in the remainder of this paper, whenever we mention naturally occurring page runs, we mean page runs of optimal size as specified in this section. our heuristic approach to finding naturally occurring page runs of optimal size starts by loading as many pages as possible into the ram buffer and then looking for nonoverlapping pages among them. to maximize our chances of finding naturally occurring page runs, we use the following three techniques:"
"we chose to implement diffusion as a service to leverage the myriad benefits of service oriented architectures (soas) [cit] . in an soa design, computations are packaged as components, called services, running on servers that are remotely accessible across the internet. consequently, client applications (regardless of operating system or environment) can call the service without needing to install it on local hardware or provision substantial memory or processor resources. additionally, the soa enables diffusion to produce consistent results for all clients while allowing transparent and seamless service improvements and optimizations."
"given the energy required for multi and singlehop transmission, the energy saving probability for multihop transmission, p es, is defined by eq. (19) ."
"although the naturalmsort algorithm executes much faster than naturaldagmsort, it still manages to find a high percentage of naturally occurring page runs as shown in table 3 . the table shows naturaldagmsort is able to find a higher percentage of naturally occurring page runs than naturalmsort, but this comes at a very high cpu cost."
"geo cast routing is basically a location based multicast routing. its objective is to deliver the packet from source node to all other nodes within a specified geographical region (zone of relevance zor). in geo cast routing vehicles outside the zor are not alerted to avoid unnecessary hasty reaction. geo cast is considered as a multicast service within a specific geographic region. it normally defines a forwarding zone where it directs the flooding of packets in order to reduce message overhead and network congestion caused by simply flooding packets everywhere. in the destination zone, unicast routing can be used to forward the packet. one pitfall of geo cast is network partitioning and also unfavorable neighbors, which may hinder the proper forwarding of messages. the various geo cast routing protocols are ivg, dg-castor and drg."
"each node in the network switches its active sector sequentially for discovering neighbor nodes in that sector. all nodes in the network calculate the approximate number of neighbors around them after the deployment of the nodes in the network. the expected number of neighbors of each node can be calculated as,"
"the retrieval results are for each query the best with different number of documents used (because the number of truly relevant documents is different for each query). in the standard brf the number of relevant documents is set the same for all the queries, therefore the mean results for the set of queries can not be the best which can be achieved. the use of score normalization methods for the automatic dynamic selection of relevant documents for each query independently solves this problem."
"some motivations of the promising vanet technology include, increase traveler safety, enhance traveler mobility, decrease travelling time, conserve energy and protect the environment, magnify transportation system efficiency, boost on-board luxury but it is not enough many other services can be served by using this technology. the creation of vehicular ad hoc networks (vanet) has spawn much interest all over the world, in german there is the fleetnet [cit] project and in japan the its(intelligent transportation system) project. vehicular ad hoc networks are also known under a number of different terms such as inter vehicle communication (ivc), dedicated short range communication (dsrc) or wireless access in vehicular environments (wave) [cit] . the goal of most of these projects is to create new network algorithms or modify the existing for use in a vehicular environment. in the future vehicular ad hoc networks will assist the drivers of vehicles and help to create safer roads by reducing the number of automobile accidents. vehicles equipped with wireless communication technologies and acting like computer nodes will be on the road soon and this will revolutionize the concept of travelling. vanets bring lots of possibilities for new range of applications which will not only make the travel safer but fun as well."
"dissimilarity measure: in the study, we used two dissimilarity measures which are also metric distances. the first measure we used was the mean absolute deviation (mad) distance which calculated the distance between two vectors as the average of absolute differences between vector elements:"
"using the method for the detection of close submitters described in subsection 3.2, we extracted dissimilarity matrices ds and the set of distances d among all user accounts using the two distance metrics (i.e., mad and msd) from the music and phil courses. figure 1 shows the density distribution of the extracted student distances in both courses. we can see that in both courses, the distribution of mad distances follows the skewed normal distribution. the skewness of the distribution was likely the result of each assignment having a due date and that many students submitted their assignments relatively close to the due dates. figure 1 also shows that the distributions of the msd distances followed a decreasing exponential distribution, which was likely due to the fact that time difference is squared when using msd metric. figure 1 also shows that the variance of the distances in the phil course was higher than in music, which was probably the result of seven graded quizzes instead of five graded quizzes as it was the case in the music course. this difference in the number of assignments could likely increase the variance of the distance distribution."
"after all nanosensors are assigned to their respective layers, the initial nccs are automatically elected based on their, where n is the number of layers and r is the transmission range of each nanosensor. the singlehop transmission range for a nano cluster controller, marked with *, is indicated by the orange dashed-circle. the nanocontroller is assumed to be placed at the center of all layers. nanosensors form clusters under each layer follow the proposed algorithm 2."
"for a power source of β cycles with cycle length τ, v nps is computed using the expressions shown in eq. (12) . where v g is the generator voltage, r g is the resistance of the nano power source, and ∆ω is the harvested charge per cycle (cy.). the maximum energy stored in the nano power source, e nps−max, is estimated using eq. (13) ."
"from the calculated distance matrix, we extract a set d that represents distances between any two accounts in the course using the lower triangular portion of the matrix ds. each entry in a set di,j is a triplet in a form (i, j, dsi,j):"
"after we selected the particular quantile threshold t, we built a set of close submitter account pairs c by keeping the members of the set d that have the distance smaller than t:"
"nano electromagnetic communication for wbsn is in its infancy and there are quite a few challenges to be addressed before it can be applied efficiently in the biomedical field. due to the size of the nanonodes, their resources are limited. low processing capability, limited battery power, and small memory unit pose major bottlenecks in designing communication protocols for nanonetworks. to overcome the bottlenecks, different protocol stacks have been proposed; however, the network layer has been less studied and explored in nanocommunication. this work proposed a new energy-efficient forwarding scheme for electromagnetic wireless nanonetworks to improve the performance of nanocommunication over the thz band for a wbsn. the channel behavior of the proposed scheme was investigated considering the combined effect of molecular absorption, spreading loss, and shadowing. to allow continuous operation, the nanonodes of the system require energy harvesting. the proposed scheme facilitates energy efficiency by selecting a multiple layered cluster based multihop communication which is supported by a modified nanosensor energy model. to ensure the quality of service of the wbsn communication, which is hampered by factors such as -network lifetime, data detection accuracy, and data aggregation delays, we adopted a cooperative fusion approach."
"broadcast routing is frequently used in vanet for sharing, traffic, weather and emergency, road conditions among vehicles and delivering advertisements and announcements. the various broadcast routing protocols are broadcomm, umb, v-trade, and dv-cast."
"having established that the optimal cluster size is one page for flash memory, we show in table a .1 the number of merge passes (⌈log ⌊m ⌋−1 (⌈n/m ⌉)⌉) for various m and n sizes. we assume the page size of 4 kb. from the table, we can conclude for most typical memory and data sizes, the optimal number of merge passes is 1. the only case where more than 1 merge pass is needed is when 50 mb of memory is used to sort 1 tb of data."
"we map the problem of finding naturally occurring page runs into the problem of finding the shortest path in a directed acyclic graph (dag). instead of finding the shortest path, we find the path whose length exactly equals the size of the page run. we adapt the well known dag-shortestpaths algorithm [cit] to solve our problem. although the algorithm is guaranteed to find a page run of the required length if it exists, it is unfortunately too slow with a run time complexity of o(n a + n t (m + e)), where n is the total number of pages to be sorted, t is the size of the sorted runs, m is the number of pages that can fit in ram, a is the number of records per page and e is the number of edges in the dag. in the worst case, e can equal (m 2 + m )/2. accordingly, we propose a heuristic solution which comes close to the dag-shortest-paths algorithm in terms of finding a high percentage of naturally occurring page runs but incurs much lower overhead. the run time complexity of our heuristic solution is o(n (a + log 2 m )). experimental results show our heuristic algorithm can halve the sorting time compared to three likely competitors. in addition, our heuristic algorithm finds a high percentage of naturally occurring runs even when there is a significant percentage of random updates to previously sorted data."
"this problem is quite similar to the osti-si problem. similarly as in the speaker identification, the relevant documents selection in the retrieval results can be described as a twofold problem: first, the speaker model best matching the utterance has to be found and secondly it has to be decided, if the utterance has really been produced by this best-matching model or by some other speaker outside the set. the difficulty in this task is that the speakers are not obliged to provide the same utterance that was the system trained on."
"write index entries for pages of naturally occurring run found from cl into index page 20: flush index page if it becomes full 21: remove pages in cl from ram buffer 22: end if 23: end while 24: if index page is not empty flush it to flash memory the benefits of finding naturally occurring runs would be outweighed by its computational costs if algorithm 2 is too slow. therefore, we use the theorem below to show the computation costs of algorithm 2 is modest."
"the safety aspect (such as accidents, brake event) of vanet application warrants on time delivery of message to relevant nodes. it simply cannot compromise with any hard data delay in this regard. therefore high data rates are not as important an issue for vanet as overcoming the issues of hard delay constraints."
"where, α is the layer ordering weight; l, ι, ν are the ids of layer, cluster, and node respectively; t, t are the transfer time per data unit for a layer and transmission cycle completion time; γ is the propagation delay; m is the number of clusters in a layer; κ is the number of adtns in a cluster; p is the priority of data to be sent; and m is the amount of data to be transmitted. phase 2: the nc then divides the total allocated time for a particular layer among its clusters based on the amount of data the cluster needs to transmit. this will prevent tdma intra-cluster transmission from colliding with other cluster transmissions from the same layer. phase 3: based on the assigned time slots for each cluster, each adtn under it is allocated a new sub time slot using the mechanism shown in algorithm 3. fig. 3 shows an exemplary time scheduling procedure with a 3 layer network which requires t time to complete a transmission cycle. using α value (calculated by eq. (2)) the nc prioritized the transmitting layers as layer 2, layer 1, and layer 3 during t"
"in this section, we compare the performance of the algorithms when the total data size is varied for the synthetic data set. the remaining parameters are set to the default vary synthetic data size values described in section 2. we show both ssd and hdd total execution time results because we want to verify our claim that naturalmsort is customized for the unique characteristics of the ssd and therefore not suitable for the hdd. we will first discuss the results for the ssd. figure 9 (a) shows the total execution time results for the different algorithms when using the ssd. table 10 presents the same results in terms of the ratio of total execution time of each of the sorting algorithms against naturalmsort. therefore, a value of two means naturalmsort outperforms the competing algorithm by a factor of 2. the results show naturalmsort significantly outperforms the competing algorithms when the total data size is smaller than 300, 000 pages. this is because naturalmsort finds a high percentage of naturally occurring page runs at this lower total data size range, as can be seen in table 11 naturalmsort loses its advantage over the competing algorithms when the total data size increases. this is because, in this experiment, the ram size does not change and therefore, as the data size increases, the ram to data size ratio drops rapidly. when the total data size increases beyond 300, 000 pages, the corresponding ram size / data size ratio drops below 0.0067. lower ram size / data size ratio results in increases in the size of naturally occurring runs which make them harder to find (see table 11 )."
"most research in the area of external sorting have focused on speeding up the run generation or merge phases of the external merge sort, hence we start our discussion on these two sub-areas of research."
"a normal sorted run is created by sorting the values inside its set of pages and writing into the disk sorted like the traditional external merge sort. when creating a normal sorted run, we try to pick the remaining pages that have the longest interval length. this is because the intervals with the longest interval length are more likely to overlap with other pages. the length tree can also be implemented by any binary search tree which allows duplicate keys. figure 6 (b) shows an example where the page p 9 was the first picked, using the min tree since it is the one with the smallest minimum value. however, after that, no other ram buffer resident pages can be picked since they all have minimum values larger than the maximum value of p 9 . therefore, the length tree is used to pick two pages (p 2 and p 6 ) with the longest remaining interval length to be sorted to create a normal sorted run. fill ram buffer with next set of unprocessed pages from p using separation distance loading."
"the run time complexity of algorithm 1 can be very high since in the worst case e equals (m 2 + m )/2. our experiments show the execution time of algorithm 1 increases very fast with increasing data size, making it unusable even for moderate sized data. accordingly, we developed a fast heuristic algorithm to solve the problem of finding naturally occurring page runs without the need to create a graph. this algorithm is presented in section 5.4."
"diffusion results can be combined in informative ways. in steps 13-15 we combine our previous two query results with a third query from the a375 mutations to create a final network. the goal of this third criteria is to select against mutations that are similar to observed mutations in the vemurafenib sensitive a375 cell line, which may be unlikely to be conferring resistance in lox-imvi."
"the comparative performance of the studied neighbor discovery algorithms is evaluated in terms of number of rounds required by them to discover 90% or above neighbor nodes and control byte overhead, which is the ratio of total number of control bytes exchanged during the simulation period to the total number of neighbors discovered in an algorithm. a round of discovery is completed when a node finishes the operation in all sectors (either clockwise or anti-clockwise). we consider discovery of at least 90% neighbors of a node is a reasonable performance metric since, in the worst case, finding out a single undiscovered node might take significantly large amount of time. we study the performances for increasing number of sensor nodes deployed in the network (keeping the number of sectors of each node is fixed at 4) and the number of sectors each sensor has (keeping the number of nodes fixed at 500). figure 3(a) shows that all the number of rounds required to discover 90% or above neighbor nodes in all the studied algorithms is increased with the number of deployed sensor nodes in the network, which is also expected theoretically. the graphs also depict that the number of rounds is increased linearly in sand with growing number of nodes as a central node manages the neighbor discovery and sends/receives a token from each node. indirect discovery of cond mechanism decreases the neighbor discovery delay as each node shares it's own neighbor table with other nodes. hence, the number of rounds required for neighbor discovery is much less in the neighbor discovery with direct and indirect discovery, (cond(d-i) ), than the one with only direct discovery, (cond(d)). similarly, the control byte overhead per discovered neighbors also increases with the growing number of nodes. the cond requires less overhead than that of sand since the later needs to send more control packets for neighbor discovery due to collisions and losses of token as depicted in fig. 3(b) ."
"the cpu we used had the following specifications: intel core 2 duo e8500; 3.2 ghz; and 6 mb l2 cache. the machine had 3 gb of total available ram. however, we further restricted the amount of ram available to the tested algorithms. the experiments were conducted on the linux operating system. linux automatically caches all io requests. this would invalidate our experiment results since it would mean pages loaded during run generation will be available for reuse in the merge phase without the need to reload from the ssd. therefore, we disabled the operating system's caching functionality."
"research in traditional face-to-face and online learning, on the other hand, recognizes the importance of learning inside (small) groups [cit] . besides being more successful completing course assignments, work in groups allows students to develop and improve their collaborative and communication skills, which are some of the core 21st-century learning skills. likewise, mooc research also confirmed student tendency to study in groups [cit], whereas registering for a course with friends seems to be positively associated with the course completion and achievement [cit] . however, the question remains to what extent we can identify emerging groups in moocs and perhaps provide an opportunity for those potentially isolated groups to become part of a larger learning community."
"and γ i,j is referred to as the \"responsibility\" of the component distribution n(x;μ j,σ j )with respect to the observation x i, as defined in the following equation:"
"as we can see from equation a.2, the number of merge passes ( ⌈log ⌊m/c⌋−1 (⌈n/m ⌉)⌉) decreases with decreasing cluster size c, but the number of hdd seeks per pass (⌈n/c⌉) increases with decreasing c. hdd seek is typically much higher than hdd rc and hdd w c . appendix a.2. the best merge cluster size for flash memory"
"but for city mobility model, street structure, variable node density, presence of buildings and trees that behave as obstacles to even small distance communication make the model application that very complex and difficult."
"driven by the recent technological advances, a socio-material paradigm shift in online educational settings provided learners with a greater opportunity to take control of their learning [cit] . instead of focusing on delivering knowledge, learning in online set- tings has been conceptualized around meeting students' needs and allowing for greater flexibility in achieving educational goals [cit] . thus, learning in today's complex and digitally connected educational settings assumes that students are able to learn at their own pace, to choose whom they want to learn with, and which information are relevant for attaining their personal objectives [cit] . nevertheless, the emergence of massive open online courses (moocs), as a format of delivering online education at scale, brought an abundance of possibilities and made even more difficult for students to engage within learning communities [cit] ."
"the diffusion codebase and api documentation is available at https://github.com/idekerlab/ heat-diffusion. however, this code is not required to use the service, as the api can be accessed by any script at v3.heat-diffusion.cytoscape.io. the cytoscape plugin can be accessed through the cytoscape store at http://apps.cytoscape.org/apps/diffusion, or it can be installed through the cytoscape application manager. in cytoscape version 3.6, installation of the diffusion application is not required, as it is included in the core functionality of cytoscape."
"in eqs 8 and 9, d(h v1 (x),h v2 (x)) is the kullback-leibler cross-entropy between the pdf h v1 (x) and pdf h v2 (x);"
"we use wifisimpleadhocgrid model for sensor nodes and yanswifiphy model to define the channel properties such as propagation and loss characteristics. we deploy 100 rv 1000 sensors with uniform random distribution in an area of 500 x 500m 2 . each node has directional antenna and the number of sectors vary from 2 rv 10. each node has the information about network density and it calculates the expected number of neighbor nodes in each sector based on that information. the transmission and sensing ranges of each node are 100m and 50m, respectively. for each data points in the graph, we take average results from 20 simulation runs with different seed values."
"we applied the proposed algorithm to two introductory moocs, one on philosophy and another one on music. we expect that we would able to find students collaborating or cheating in most of online environments and contexts. however, subject area, university profile, type of assignments, and other factors can have an effect on the number of students engaging in such behaviors in each case study. for example, if we check in moocs about topics that might have a higher industry value such as computer science or data science, or from one of the top schools such as harvard or mit, should we expect the number of students collaborating or cheating to be higher? additionally, in other contexts, such as online, on-campus, for-credit courses, we could also expect to see an impact on this issue as well."
"position based routing consists of class of routing algorithm. they share the property of using geographic positioning information in order to select the next forwarding hops. the packet is send without any map knowledge to the one hop neighbor which is closest to destination. position based routing is beneficial since no global route from source node to destination node need to be created and maintained. position based routing is broadly divided in two types: position based greedy v2v protocols, delay tolerant protocols."
"the rest of the paper is organized as follows . we describe the network model and assumptions and the proposed cond system in section ii. the simulation study is given in section ii and finally, we conclude the paper in section iv."
"in this section, we describe the rationale for focusing on optimizing the write io costs of run generation instead of the other io costs associated with an external merge sort. the merge cluster size is an important determinant of the performance of the external merge sort. according to the analysis in appendix a.2, the best cluster size for flash memory is one page. as shown in"
"without going into the further investigation of potentially distinct behaviors of close completers, our approach allows for detecting distinct groups of students who tend to work together to successfully complete an online course. here, we specifically focused on learning at scale (i.e., moocs), as a particularly challenging environment from various aspects. for example, given the number of learners commonly enrolled in moocs, it is rather chal- [cit] . the proposed method, therefore, allows for automated identification of students who tend to study together and potentially reveal different types of collaborationse.g., those who genuinely collaborate to learn and obtain a certificate and those who perhaps show certain behaviors that could be characterized as academic dishonesty. such information could be potentially relevant for both -students who are looking for potential collaborators, as well as for teachers, in the form of insight into various behaviors emerging from student interactions. our findings are in line with brooks and colleagues [cit] and li and colleagues [cit] who also observed students' tendency to study in small groups within mooc educational settings. it also seems that such an approach to learning is associated with higher course performance and persistence in a course. however, our study further revealed that this form of collaboration also involves close assignment submission. moreover, our study indicated the existence of additional forms of student collaborations. it is also striking that students tend to organize in small, rather disconnected groups, based on their interaction outside the \"classroom\" settings [cit] . it seems that further support is needed to provide a more comprehensive guidance that would allow those small groups to emerge into sustainable communities [cit] . on the other hand, this particular behavior could be a determining characteristic of students who are primarily performance-oriented and focused on obtaining a certificate. for example, while [cit] observed that those students who enroll together in a course tend to participate in the discussion forums as well, our results suggest that this group of students was less engaged in forum interactions than other students who did not submit assignments together. this further suggests that our algorithm identified groups of students that were particularly focused on working in small groups towards achieving a common goal, without the tendency to actively engage with other peers."
"the concept of network propagation is commonly implemented by either of two related algorithms. the first algorithm goes by several names: google pagerank, the random surfer model, or random walk with restart. the second algorithm is called heat diffusion. the difference between these two approaches is a modeling choice, and each has proven competitive in different applications. in fact, random walk with restart is equivalent to heat diffusion with the following assumptions; no restart, undirected edges on the graph, time steps that approach zero in length, and not running the random walk to equilibrium, but rather stopping after some short amount of time. in the random walk case, the amount of spread from the original distribution is parameterized by the restart probability, while in heat diffusion the spread is controlled by the time parameter t below. however, due to algorithmic advantages in memory use [cit] the heat diffusion model is currently much faster to compute, and so we have selected it here. the calculation is given by:"
"we now define the equation we use to compute the optimal page run size given the following: a data size of n pages; a ram size of m pages; one-page used to accumulate index entries during run generation; a one page output buffer during the merge; and an index buffer of e pages used during the merge. we need a small index buffer of e pages during the merge to prevent repeated disk loads for naturally occurring page run index lookups. since the index pages are densely packed, we found in our experiments we only need a very small index buffer of 20 pages to prevent almost any repeated index loads. the optimal naturally occurring page run size (onrs) can be computed by the equation below: proof. for equation 2 to be correct, its denominator must specify the maximum number of runs subject to the constraint of definition 5.2. this is because ⌈ n numberof runs ⌉ equals the run size. so, we turn our attention to proving the denominator of 2 equals the maximum number of runs subject to the constraint in definition 5.2. the ⌈log m −e−1 ⌈ n m −1 ⌉⌉ expression, which we will call m inp asses, specifies the minimum number of merge passes. this is because the ⌈ n m −1 ⌉ expression inside the m inp asses expression specifies the maximum run size possible, given a ram of m pages with one reserved for accumulating indexes to naturally occurring runs. the minimum number of merge passes occurs when the maximum number of runs (m − e − 1) are merged in one pass. this is because of the m pages available ram, e pages are reserved for the index buffer and another one page is reserved for the output buffer. hence m inp asses is the minimum number of merge passes for any given run size."
"the detailed deduction of eqs 11 to 15 can be found in the aforementioned studies [cit] .eqs11 to 15 provide a clear and explicit adaptation procedure to approach the nearoptimal sampling function. following eqs11 to 15, parameters w will adapt to the optimal parameters v in eq. 9, leading to an efficient sampling function that can dramatically reduce the computational cost. in particular, in each adaptation step (i.e. step i), can be determined using eqs11 to 15 with .usually, the adaptation process converges to the optimal values after only a few steps."
"the best merge cluster size for flash memory is much more trivial to find since flash memory does not have rotational latency and therefore, its random read and sequential read performance is much more similar. hence, we can approximate the merge io costs for flash memory (m costf lash) as follows (assuming the output buffer size is one cluster): equation a.2 shows smaller cluster size c will result in a lower cost of merge on the flash memory since it results in less merge passes with no corresponding increase in io cost during each pass. this is confirmed by lee et. al. [cit] whose experiments show the optimal cluster size for flash memory is in the range of 2 to 4 kb. most modern flash memory drives have a page size of 4 kb. therefore, the optimal cluster size for flash memory is just one page in size."
"after we had identified close submitter accounts, we examined the differences between them and the rest of the student population. we first analyzed the difference in terms of earning the certificate between close submitters and the rest of the accounts ( table 2) . as close submitter accounts submitted all graded quizzes, to make the comparison sensible, we compared them with only the accounts who also submitted all graded quizzes. table 2 shows that for regular accounts the certificate accomplishment ratio was 84.3% and 95.5% for phil and music respectively; in the case of close submitter accounts the certificate accomplishment ratio was 78.8% and 76.9% for phil and music. therefore, the ratio of certificate accomplishment was lower for close submitters, which was somewhat an unexpected finding."
"the above features for connectivity therefore needed the knowledge of node positions and their movements which as such is very difficult to predict keeping in view the nature and pattern of movement of each vehicle. nonetheless, a mobility model and node prediction based on study of predefined roadways model and vehicle speed is of paramount importance for effective network design."
"in this section, we answer the question of how robust naturalmsort is to varying amounts of updates in the partially sorted data. we answer this question by injecting varying amount of updates to the sorted real data by varying the values of u pdatep ercentage (as described in section 6) and m axu pdaterange from 5% to 100%. we left the other parameters to the default settings specified in table 2 . table 6, 7, and 8 show the performance of naturalmsort against tradmsort, replacemsort and fast, respectively. the tables show the ratio of total execution time of the competing external sort algorithm against the total execution time of naturalmsort. therefore, a value of two means naturalmsort outperforms the competing algorithm by a factor of 2."
"we use the well known dag-shortest-paths algorithm [cit] as the basis for our solution. since we only have a limited amount of ram, we can only search among the pages that can fit into the memory. algorithm 1 details the algorithm we use. the algorithm works by first filling the ram buffer with unprocessed pages (line 2) from the flash memory. we load the pages in a particular order to maximize the chances of finding page runs (please see separation distance loading in section 5.4 for more details). next, we find the minimum and maximum values from the loaded pages to construct the value intervals. we then insert these value intervals into the dag. in line 5, we use a simple variant of the shortest path algorithm to find a path of length t . instead of finding the shortest path, we stop the search as soon as we have found a path of length t . if no path is found, we create a normal sorted run by sorting t pages with the longest intervals (line 7) and writing them into the flash memory. removing the longest intervals is desirable since these intervals generally are less connected with other pages because of their tendency to overlap with other pages (e.g. page p 5 in figure 4 ). if a naturally occurring page run is found we do not write the pages out but instead write index entries to the locations of the pages into an index buffer page."
"gaussian mixture is capable of accommodating multiple peaks of the optimal sampling function and is thus more efficient in system reliability problems. in this example, the time-dependent reliability analysis of a series structural system was conducted to demonstrate the high efficiency of the new method. the system is the superstructure of an rc girder bridge that consists of five girders [cit] . the system is modelled as a series system, i.e. the failure of any girder indicates the failure of the whole superstructure. eq. 18 is used in this example. the initial resistance r 0 follows a lognormal distribution with a mean equal to 1790 knm and a cov of 0.16; the dead load is a deterministic variable equal to 231.2 knm; and the live load s l at each arrival is a normal random variable with a mean of 301.4 knm and a cov of 0.40. the arrival rate of live load is 1000 times/year. in the analysis, it was assumed that all the bridge girders have identical flexural resistance."
"in this experiment, we compare the performance of the different algorithms when the ram size is varied from 200 pages (around 1% of total data size) to 5000 pages (around 26% of total data size) [cit] data. we left the other parameters to the default settings specified in table 2 . we do not include the results for naturaldagmsort in this or any subsequent experiments because it too slow for any data size larger than 3000 pages, as shown in section 7.1. figure 8 (a) shows the total execution time result when the ram size is varied. table 4 shows the ratio of total execution time of the competing external sort algorithm against the total execution time of naturalmsort. therefore, a ratio of 2 means naturalmsort outperforms the competing algorithm by a factor of 2. the results show that when the ram size is 800 pages (4% of total data size) or larger, naturalmsort significantly outperforms the other algorithms. this can be explained by looking at table 5, which shows that when the ram size gets to 800 pages, 72% of the runs used by naturalmsort are naturally occurring. furthermore, the table also shows at 800 pages, runs need to be 25 pages long. this shows that even in this difficult situation, our heuristic algorithm can find naturally occurring page runs a large percentage of the time. figure 8 (a) also shows naturalmsort does not perform as well as the other algorithms when ram size is very small (below 800 pages or 4% of total data size). this is because when the ram size is that small, the size of the naturally occurring page runs is large (see table 5 ), which makes it hard to find many non-overlapping pages. at this point, the small benefit from finding the small percentage of naturally occurring page runs is outweighed by the high cost of looking for them."
"the record size is an important parameter for determining the success of our approach of finding naturally occurring run, the reason being that more records in one page makes it harder to find naturally occurring runs, since it increases the chances of having more extreme maximum and minimum values in each page. accordingly, in this section, we compare the performance of the algorithms when the record size is varied for the synthetic data set. the remaining parameters are set to the default values described in section 2. figure 10 (a) and table 12 show naturalmsort consis- tently outperforms the other algorithms for the entire range of record sizes tested. furthermore, it shows naturalmsort can outperform competing algorithms by more than 30%, even when the record size is just 50 bytes (translates to 81 records in one page). this is hard to achieve since finding naturally occurring page runs becomes difficult when so many records in the same page can be updated. table 13 shows even when records are only 50 bytes, naturalmsort can find 63.58% of naturally occurring page runs. these results show the robustness of naturalmsort to changing record sizes."
"the proactive routing means that the routing information, like next forwarding hop is maintained in the background irrespective of communication requests. the advantage of proactive routing protocol is that there is no route discovery since the destination route is stored in the background, but the disadvantage of this protocol is that it provides low latency for real time application. a table is constructed and maintained within a node. so that, each entry in the table indicates the next hop node towards a certain destination. it also leads to the maintenance of unused data paths, which causes the reduction in the available bandwidth. the various types of proactive routing protocols are: lsr, fsr."
"communication is expected to be from inside to outside of the body through nanosensor nodes. nanocontrollers (nc) are deployed for in-body communication, which aggregate information coming from in-body nanosensors. the micro gateway, a wireless interface connecting the subnetwork of nanosensors to the internet, collects data and forwards them to the relevant entity (e.g., healthcare provider, medical staff, etc.) using the internet and/or intranet."
"as we calculated the distances between students using the two distance metrics (i..e, mad, and msd) we used superscripts to denote a particular distance metric. similarly, we used subscripts to denote a particular course (i.e., mus and phi for the music and phil courses, respectively). using our notation, ds m sd mus would, for example, denote a distance matrix in the music course using the msd distance metric."
"the main limitation of this study is that we showed that the two populations were statistically different given the selected indicators. however, we still have to deepen into these findings to infer the various associations between accounts and to delve into the behavior of students. therefore, we are still unable to fully explain the actions that these students were performing and this is part of the future work."
"because of the existence of an optimal sampling function, one can restate the problem as an optimization problem. in this case, the kullback-leibler cross-entropy, an indication of the difference between two probability densities defined as follows"
cluster based routing is preferred in clusters. a group of nodes identifies themselves to be a part of cluster and a node is designated as cluster head will broadcast the packet to cluster. good scalability can be provided for large networks but network delays and overhead are incurred when forming clusters in highly mobile vanet. in cluster based routing virtual network infrastructure must be created through the clustering of nodes in order to provide scalability. the various clusters based routing protocols are coin and lora_cbf.
"in direct neighbor discovery, each node gets the neighborhood information by direct transmission of hello-reply messages. after getting the information about any neighbor, each node updates its own neighbor table. the table contains information about the node id, sector number, and position of a neighbor."
"a hierarchical network architecture integrating body area nano-network (bannet) and an external macroscale healthcare monitoring system was proposed [cit] . in this work, two different energy-harvesting aware protocol stacks using optimal routing protocol and a greedy routing approach were conceived for handling the communication of nanosensors moving uniformly in an environment mimicking human blood vessels. an energyaware mac protocol was used in both strategies to identify the available nanonodes through a handshake mechanism. these two schemes performed well compared with the simple flooding scheme. however, high computational capacity was required for the optimal scheme."
"in modeling the network, it is assumed that a network of nanosensor nodes, constrained by energy, is placed into a human body to construct a body sensor network (bsn). a possible system architecture is shown in fig. 1 the network is composed of nanonodes, nanocontrollers, and a micro gateway. the in-body and on-body nanosensors are placed to accumulate data from inside and outside of the human body, respectively. nanocontrollers collect data from the nanosensors and forward them to the micro gateway, which is responsible for storing the data in a remote database using the internet, and these data can be accessed by the relevant entities."
"the results show replacemsort and fast consistently outperform tradmsort. this is because both replacemsort and fast create longer sorted runs than tradmsort. longer run size means less runs are merged together which, in turn, reduces cpu usage. figure 9 (d) shows the total execution time results for the hdd. as expected, the results show that the naturalmsort is consistently the worst performer. this is due to the extra seeks incurred by naturalmsort, outweighing the benefits gained from a reduced number of write io. naturalmsort incurs extra seeks in two places: when loading pages far apart from each other during run generation; and loading the scattered pages of naturally occurring page runs during the merge phase. these extra seeks do not incur much overhead when naturalmsort is operating on the ssd but on the hdd, it dominates the execution time."
"query expansion techniques based on the blind relevance feedback (brf) has been shown to improve the results of the information retrieval. the idea behind the blind relevance feedback is that amongst the top retrieved documents most of them are relevant to the query and the information contained in them can be used to enhance the query for acquiring better retrieval results. first, the initial retrieval run is performed, documents are ranked according to the query likelihood computed by (3) . then the top n documents are selected as relevant and the top k terms (according to some importance weight l t, for example tf-idf ) from them is extracted and used to enhance the query. the second retrieval run is then performed with the expanded query."
"algorithm 1 findnaturalruns(input p :sequence of pages to be sorted, t : optimal naturally occurring run size; m : ram buffer size; n input data size )"
"in this experiment, we compare the performance of the different algorithms when the data size is varied from 1000 pages to 5000 [cit] data. we set the ram size to 500 pages. we left the other parameters to the default settings specified in table 2 . figure 7 shows the results for this experiment. the results show our heuristic naturalmsort algorithm is the best performer for total execution time (figure 7 (b) ). this can be explained by the similarity between the shape of total execution time and the number of write io curves ( figure 7 (b) ). this suggests that the write io costs dominate all other costs and that naturalmsort produces the second least number of write io. although naturaldagmsort produces a lower number of write io than naturalmsort, it is not practical to use it when the data size is above 3000 pages (it takes more than 2 hours for 4000 pages) because of its high run time complexity."
"following the method described in subsection 3.3.1, we established an initial mad threshold of 30 minutes that corresponded to the 4.81e−6 quantile in the music course and 5.75e−6 quantile in the phil course. based on this preliminary mad threshold, we selected three quantile values (i.e., 6e−6, 1e−5 and 5e−5) and examined the corresponding mad and msd threshold values for each course (table 1) . table 1 shows that the use of the most restrictive threshold quantile (6e−6) results in the identification of 78 and 17 close submitter pairs in the music and phil courses, respectively. the table also shows that this threshold corresponds to very similar values for the mad distance metric (i.e., 0.61h and 0.57h for music and phil courses, respectively) and the same threshold value for the msd distance metric in both courses (i.e., 0.51h 2 ). on the other hand, the use of other two less restrictive threshold values resulted in a significantly higher number of account pairs being preserved and also higher variability in both metrics between the two courses (table 1)."
"16. with the original complete nci-pid network selected, run the original query again to select braf, pdgfrb, nras, hgf, map2k1, and mapk1. right click one of the selected nodes and click select first neighbors."
"the mobility model highly varies from highways to that of city environment. the node prediction design and routing algorithm also therefore need to adapt for these changes. highway mobility model, which is essentially a one-dimensional model, is rather simple and easy to predict."
"diffusion is hosted on servers operated by the national resource for network biology (nrnb.org), though the code, which is open source, can be hosted elsewhere, too. additionally, it is part of the cytoscape cyberinfrastructure (ci), a collection of services that deliver functionality on networks encoded by the common transfer format called cx. biological applications that use diffusion are also well positioned to compose multiple ci services to create novel and complex workflows quickly."
"the sensed data are aggregated and forwarded to nc in multihop fashion using a novel tdma based dynamic scheduling scheme. in our scenario, to collect information from the whole bsn, the nc periodically seeks data transmission request packets from all the nanosensors. in response, each active data transmission node (adtn, i.e., an ncm with data to transmit) sends transmission request packets whose structure is shown in table 1."
"time is assumed to be divided into contiguous slots and all nodes independently discover their neighbors within a bounded time. the total time frame is logically separated into transmit and receive time slots. each time slot is divided into multiple mini-slots. a node either transmit a hello message or listen to the medium in an individual time slot. after the deployment, each cond node in the network repeatedly tries to discover neighbor nodes information in all sectors. each node stays in a sector for a dynamic period of time depending on the number of undiscovered nodes in that sector and performs neighbor discovery directly and indirectly using polling mechanism. the mechanism is condensed into following three steps: initialization of neighbor discovery with a suitable value of delay tuning parameter (k), contention based polling mechanism and collaborative update of neighbor table."
"section 4 explained the rationale behind reducing write io costs during the run generation phase of an external merge sort instead of other io costs. in this section, we present our approach for achieving this, using naturally occurring runs. the idea is to create naturally occurring page runs by finding pages whose value ranges do not overlap each other. we call these naturally occurring page runs. an example is shown in figure 1 . in the example, the naturally occurring page run consists of 3 pages, p1, p3 and p5. notice the value interval created from their minimum and maximum values do not overlap. we then densely pack these indexes into pages and write them out instead of naturally occurring runs themselves. assuming an index entry occupies 4 bytes and a page occupies 4 kb, this translates to a 1024 factor saving in write costs during run generation. the remainder of this section is presented as follows. first, we formally define the problem of finding naturally occurring runs in section 5.1. second, we show the problem of finding naturally occurring page runs can be mapped to finding the shortest path problem in a directed acyclic graph in section 5.2. third, as it is not clear we should look for large page runs or small ones, since large ones will result in less merge passes but are harder to find, in section 5.3 we describe the analysis used to arrive at the optimal size of naturally occurring page runs. finally in section 5.4, we describe a fast heuristic algorithm for finding naturally occurring runs."
extract value intervals from loaded pages 4: insert newly extracted intervals into dag 5: find the first naturally occurring page run of size t in dag using dag-shortest-paths algorithm [cit] 6:
this work is supported under the australian research council's discovery funding scheme (project number dp0985451). we would like to thank the anonymous reviewers of this paper for their insightful comments and suggestions.
"if p t (i) is the transmitted power at the source node i, then the total received power (p total (i, j)) at the destination j considering the molecular absorption, spreading, and shadowing effect is given by eq. (3) [cit] ."
2) indirect discovery of neighbor nodes: the neighbor table records of a node may be shared with its surrounding nodes to help them discover more nodes quickly by using the node ids and their locations. a node shares its neighbor table with the information about all the discovered nodes while sending hellolreply message. the neighbor that receives the message updates its own table with that new neighbors information which decreases the neighbor discovery delay significantly.
"where c i arethe distribution factors of live load [e.g. girder distribution factors (gdfs) in bridge superstructures];and m is the number of elements in the series system. it should be noted thatthe uncertainty of dead load has little influence on the time-dependent reliability of bridge superstructures [cit] . therefore, the cdfs of s l havebeen substituted by those of the total load effect s in eq. 18.in the time-dependent reliability analysis, eqs17 and18 are evaluated by numerical integration, while the total probability of failure is computed with a simulation method. for system reliability problems, multiple important regionsmay occur [cit], which will compromise the efficiency of existingmethods. the above cross-entropy-based sampling method using gaussian mixture provides anefficientalternative in such situations."
"2. apply the algorithm to mooc data, and tag the accounts detected as submitting their solutions close in time as \"close submitters\" while the rest will be referred to as \"regular accounts\". report the results in the following directions:"
"although not strictly necessary, it can be useful to add in the original query to better understand why nodes were selected. in the following sequence, we add in the original vemurafenib nodes."
"after we had identified pairs of sufficiently similar student accounts based on the assignment submission times, we used them to construct a graph of close student submitters (figure 2 ). using the close submitter pairs of accounts, we plotted the different accounts as graph nodes connected with a undirected edge between each one of the pairs. the number and size of the groups of learners that we find were as follows:"
"the probabilistic interpretation for this calculation is that if the input values are a probability distribution of starting positions for particles diffusing across the edges of the graph, the final value is the position distribution after t time units. as t goes to infinity, the probability distribution approaches a uniform distribution over all nodes."
"where e nps is the energy stored in the nano power source [cit], c nps is the total capacitance of the nano power source, and v nps is the voltage of the charging power source. when the value of β approaches ∞, the voltage of the charging power source (v nps ) becomes equal to the voltage of the generator (v g ). in this case, the maximum energy (e nps−max ) is given by eq. (14) ."
"in aodv [cit] routing, upon receipt of a broadcast query (rreq), nodes record the address of the node sending the query in their routing table. this procedure of recording its previous hop is called backward learning. upon arriving at the destination, a reply packet (rrep) is then sent through the complete path obtained from backward learning to the source. the aodv algorithm enables dynamic, self-starting, multihop routing between participating mobile nodes wishing to establish and maintain an ad hoc network. aodv allows mobile nodes to obtain routes quickly for new destinations, and does not require nodes to maintain routes to destinations that are not in active communication. aodv allows mobile nodes to respond to link breakages and changes in network topology in a timely manner. the operation of aodv is loop-free, and by avoiding the bellman-ford \"counting to infinity\" problem offers quick convergence when the adhoc network topology changes (typically, when a node moves in the network). when links break, aodv causes the affected set of nodes to be notified so that they are able to invalidate the routes using the lost link. route requests (rreqs), route replies (rreps) and route errors (rerrs) are message types defined by aodv [cit] ."
"where e b−− is the average bit energy required for transmission; sr is the transmission from source (adtn/ ncc) to a relay node (ncc), rd is the transmission from the relay node to destination (nc), sd is the direct transmission from source to destination; and θ is the rate constant, which depends on the density of the deployed nodes. finally, the rate of change of p es is quantified using eq. (20) ."
"besides the preceding covariance control, the covariance matrix should also be expanded during the first few steps of adaptation in the preliminary sampling so that there are enough points falling into the important region(s). [cit] according to the preceding discussion, the cross-entropy-based adaptive importance sampling method needs two cycles of sampling: preliminary sampling and main sampling. during the preliminary sampling cycle, adaptation is conducted to obtain the near-optimal sampling function. during the main sampling cycle, only a relatively small number of samples are needed to predict failure probability. the results from both cycles of sampling are then combined to obtain an unbiased estimation of failure probability as follows [cit] :"
this article has shown the first experiments with the use of score normalization method for selection of the relevant documents for the blind relevance feedback in speech information retrieval. the result are showing that with the score normalization better retrieval results can be achieved than with the standard blind relevance feedback with the number of relevant documents set beforehand. we have also confirmed that the blind relevance feedback in any form is very useful in the speech information retrieval.
"although this work appears similar to ours, there are a number of key differences. first, we create naturally occurring page runs by combining non-overlapping pages of tuples to speed up the run generation. in contrast, their work is focused on virtually concatenating non-overlapping sorted runs to speed up the merge phase. section appendix a.2 explains our analysis which concludes optimal external merge sort for flash memory typically has only one merge pass. however, their approach is only useful for multi-pass merge. in addition, our problem is more challenging since it has a much larger search space. this is because we search for non-overlapping pages instead of sorted runs. second, we develop a method to determine the optimal size for naturally occurring runs based on the characteristics of flash memory (see section 5.3), whereas, no such work has been done in virtual run generation. third, we show the problem of finding naturally occurring page runs can be mapped to the shortest path algorithm for a dag. no such work exists for finding virtual runs. fourth, we propose a heuristic solution to the problem of finding naturally occurring runs which we empirically show is computationally efficient and can find a high percentage of naturally occurring runs. no existing empirical or theoretical work has been conducted on the effectiveness of using virtual runs to speed up external merge sort."
"we now describe the existing work that is closest to ours. haerder [cit] first proposed the idea of combining multiple non-overlapping runs into one longer run during the merge phase of external sorting. the non-overlapping runs can be virtually concatenated by declaring they belong to one run instead of physically joining them in the disk. graefe [cit] called these \"virtual\" runs and stated that this approach is particularly suitable for almost sorted input. graefe goes a step further to suggest virtual runs can be created by concatenating \"partial\" virtual runs. for example, if a large part of the one sorted run does not overlap with a large portion of another sorted run, then these can be joined to form one longer virtual sorted run. however, no experimental work was done to verify the effectiveness of the approach."
"graded problems: we used answers to automated graded quizzes as input to matrix sp . given that students are often allowed unlimited attempts for each quiz, we used only the time of last submissions to each quiz."
"the field of information retrieval (ir) has received a significant attention in the past years, mainly because of the development of world wide web and the rapidly increasing number of documents available in electronic form. recently the internet has been looked upon as an universal information media, more than the text information source it becomes the multimedia information source. especially since large audiovisual databases are available on-line, the research in the field of information retrieval extends to the retrieval of speech content."
"1) simulation tool in this paper the simulation tool used for analysis is ns-2 [cit] which is highly preferred by research communities. ns is a discrete event simulator targeted at networking research. ns provides substantial support for simulation of tcp, routing, and multicast protocols over wired and wireless (local and satellite) networks. ns2 is an object oriented simulator, written in c++, with an otcl interpreter as a frontend. this means that most of the simulation scripts are created in tcl (tool command language). if the components have to be developed for ns2, then both tcl and c++ have to be used."
"values within each page do not need to be internally sorted. we can then store indexes to these pages instead of creating a normal sorted run. assuming an index entry occupies 4 bytes and a page occupies 4096 bytes, this results in a 1024-fold saving in write costs for sorted run generation. figure 1 shows an example of a naturally occurring page run in partially sorted data. note there is no overlap between the minimum and maximum value ranges of the pages of the naturally occurring page run. from the above description, it can be seen that our technique aims to reduce the write io costs during sorted run generation. in section 4, we explain why it is a particularly good idea to optimize write io costs in an external merge sort on flash memory."
"when any node receives a hellolreply message from its neighbor, it enters the new information in the neighbor table. the neighbor table is updated with direct and indirect neighbor discovery methods."
"during the adaptation process, the covariance of samples may shrink or expand to mimic the optimal sampling function. this process does give a good approximation of the optimal sampling function. [cit] and shown in numerical example 1 given later, the estimated failure probability may oscillate around the real value when the covariance of the sampling function becomes too small. it is also inefficient to use a larger standard deviation (std) to overcome the problem as this increases the chance of obtaining sample points of low importance. moreover, for gaussian mixture adaptations, a larger std of sampling function may result in the clustering of gaussian mixture components."
"r ecent advances in nanotechnology expanded its potential applications in diverse fields. in the biomedical field nanotechnology has been successfully applied in healthcare, health monitoring, support immune systems, creating novel drug delivery systems, detecting the presence of infectious agents, and identifying specific types of cancer [cit] . it has been used in waste-water management, controlling biodiversity, and assisting biodegradation to improve the environment [cit] . nanomaterials play important roles in industrial manufacturing processes, quality control procedures, and self-cleaning anti-microbial textiles [cit] . 'nanomachines' have also been deployed in nuclear, biological and chemical defense systems for surveillance for military and defense purposes [cit] . a nanomachine is composed of a power supply, memory, antenna and cpu module, and it behaves as an autonomous node capable of performing tasks such as computing, storing, sensing and/or actuating at the nanolevel. the nanomachines are fabricated using three approaches -top-down, bottom-up, and bio-hybrid approach [cit] . existing electronic devices are scaled down to the nanolevel using the top-down approach, but in the case of bottom-up, molecular components are used to create the nanomachines. the bio-hybrid approach uses biological elements as patterns to be replicated with synthesized components [cit] ."
"where, t sw itch is the switching delay for a node to switch into a new sector from another, m is the set of sectors, and k is the delay tuning parameter, changed after each iteration depending on the number of discovered neighbor nodes in a particular sector as follows,"
"as can be seen from the table 1, for the training topic set the results for the brf with the score normalization method used are better than with the standard brf with the predefined number of documents. for the evaluation topic set the results with the standard brf are even slightly worse than without brf, but with the score normalization used, the results are better than without brf. it can be also seen that the results for the standard brf are almost the same for different number of documents. this is caused by the fact that for each query different number of documents is relevant. for one query the result for brf with 10 documents was better than with 30 documents, for another one the other way around. the dynamic number of relevant documents chosen by the score normalization method deals with this problem."
the following two factors aid us in finding a high percentage of naturally occurring page runs: 1) the input data is partially sorted; and 2) we generate the smallest possible run size which does not increase the number of merge passes. partially sorted data typically means the values in each page span a smaller range because the data is still mostly sorted. smaller run sizes are easier to find since less pages with non-overlapping value ranges are required.
"11. create a new selection filter based on the imported data by clicking the select tab on the control panel, then clicking the down arrow next to the filter name. select create new filter and name the filter. click the plus sign to create a new selection criterion. choose column filter to specify that the information for the filter is in a node column. from the choose column selector menu, select one of the node columns corresponding to the data that you imported from the table, in this case loximvi_skin. slide the selector so that only nodes with a 1 in the relevant node column are selected."
the markov chain of the model without self transitions is shown in fig. 7 and is defined by its transition rate matrix ω(t) calculated by eq. (11). each element of the matrix ω susv refers to the transition rate from state s u to state s v and is defined in eq. (10) .
"a small optimization that can improve the performance of the index page buffer is to rearrange index page entries so that the same i th page of page runs are grouped into the same page. for example, an index page would contain only the 2 nd pages of page runs. this improves the temporal and spatial locality of references to index pages during the merge. the reason is the merge phase tends to progress through the pages of the page runs close to parallel. for example, when the 2nd page of one page run is being processed, it is common to be processing the 2nd page of the other page runs at the same time."
"(a) the number of close submitter accounts detected, the size and the shape of communities of accounts and distribution of distances between submissions. (b) the comparison between groups (close submitters vs. regular accounts) regarding different student features such as the grade, the average number of submissions, and the number of videos. the comparison intends to show that we can indeed detect two distinct populations from a statistical point of view."
"structures experience deterioration and varying load effects during their service life, so the reliability of a structure during its lifetime is a time-independent problem. for this reason, time-dependent reliability analysis has been used by many researchers to evaluate the structural safety of structural systems throughout their service life. different researchers have adopted different methods in their analysis, and these methods can, in some cases, lead to dramatically different results [cit] . the theoretically rigorous method(i.e. the stochastic-process-based method) for time-dependent reliability analysis usually involves tedious computations, which hinders their application to complex reliability problems with multiple important regions (e.g. timedependent reliability of structural systems). the objective of this paper is to propose a new and efficient sampling method to facilitate the wide application of the stochastic-process-based method in complex timedependent reliability problems."
"to demonstrate the diffusion service with a simple example, we created a 10-by-10 grid network and diffused a query from the upper left corner of the grid. the resultant output (fig 1a) is a good proxy for distance to the upper left corner."
"almost all external sorting algorithms are some variant of the traditional external merge sort. therefore, we first provide a detailed description of the tradition external merge sort."
"subsequent queries after the first diffusion will create new input and output columns, with each new entry denoted with an incremented counter. one can also use an imported table of data to drive a query. we have provided the mutations of the two cell lines in question (lox--imvi and a375) in a table that can be imported, downloaded from the cancer cell line encyclopedia [cit] . steps 9-12 provide an example of performing diffusion based on an imported set of features from a table:"
"in this paper we will present the first experiments aimed at the better automatic selection of the relevant documents for the blind relevance feedback method. our idea is to apply the score normalization techniques used in the speaker identification/verification task [cit], which was successfully used in the multi-label classification task for finding the threshold between the \"correct\" and \"incorrect\" topics of a newspaper article in the output of a generative classifier [cit], to dynamically select the relevant documents for each query depending on the content of the retrieved documents instead of just experimentally defining the number of the relevant documents to be used for the blind relevance feedback in advance."
"initially, each sensor node s e s explores the neighbors in all of its sectors m e m by changing its direction clockwise after a certain interval of t seconds, determined as follows,"
"after the initial retrieval run, we have the ranked list of documents with their likelihoods computed by (3) . we have to find the threshold for the selection of the relevant documents. a score normalization methods have been used to tackle the problem of the compensation for the distortions in the utterances in the second phase of the openset text-independent speaker identification problem [cit] . in the ir task, the likelihood score of a document is dependent on the content of the query, therefore the beforehand set number of relevant documents is not suitable."
"in this paper, we focus on re-sorting data that was initially sorted but becomes progressively unsorted due to updates. we call this \"partially\" sorted data. one situation which prefers sorted data, but will still work (albeit sub-optimally) when the data is partially sorted is answering range queries using an unclustered index. in this situation, the sought range would span less disk pages when the tuples are closer to being fully sorted. another situation is a mostly read only database being updated in batch. in this case re-sorting can occur right after a batched update."
"in the new method proposed in this paper, n k samples will be generated after the adaptation process in order to determine k opt which is defined as follows:"
"as a biological example, we chose to investigate a vemurafenib resistant melanoma cell line, lox-imvi, as compared to a sensitive cell line a375 [cit] with the goal of implicating mutations that may be affecting lox-imvi's response to the drug. as an input query, we chose six genes (braf, pdgfrb, nras, hgf, map2k1, mapk1) with known relationships to the drug [cit] . the goal of this analysis is to find a subnetwork that is near the mutations of lox-imvi (which may be conferring resistance) and the known drug associated genes, but not near mutations that occur in the sensitive a375 line. for the underlying network, we used the nci pathway interaction database [cit], an amalgamation of expert-curated cancer pathways. the version of this database that we used is provided in the supporting information as s1 file. as described in the following protocol, using a cutoff of the top 10 percent of nodes relevant to both vemurafenib and lox-imvi mutations, but not a375 mutations according to diffusion, we produced a subnetwork of 53 nodes and 448 edges (fig 1b) . we demonstrate how this network-based hypothesis was reached using our cytoscape application in the following example:"
"in the thz band bsn, the wave propagation range through human tissues is constrained by a large spreading loss [cit] and shadowing [cit] . shadowing, caused by body part movement, increases the blocking probability because of broken line-of-sight links between the transmitter and receiver [cit] ."
"so what patterns will emerge as the patient, here conceptualized as an open, self-organizing, chaotic system, advances from less-integrated to more-integrated, from less-coherent to more-coherent? i hypothesize that, over time and by way of input that either challenges or supports, healing cycles of disruption and repair will be induced, recursive cycles of disorganization and reorganization, defensive collapse and adaptive reconstitution at ever-higher levels of psychological complexity as the patient responds -either defensively (prompting collapse) or adaptively (prompting reconstitution) -to the ongoing stressful input."
"in other words, i believe that all living systems will evolve, given enough time, from stability through increasing complexity to chaos -by virtue of the fact that they are open, complex adaptive, nonlinear dynamical, selforganizing, and chaotic. why will living systems evolve in this manner? for the very same unfathomable reason that a sandpile, in response to ongoing input from the outside, will advance through its cycles..."
"1. too much challenge, too much anxiety, too much stress will be too overwhelming for the patient to process and integrate, triggering instead defensive collapse and temporary derailment of the therapeutic process. 2. too little challenge, too little anxiety, too little stress will provide too little impetus for transformation and growth because there will be nothing that needs to be mastered; too little challenge will serve simply to reinforce the status quo. 3. but just the right amount of challenge, just the right amount of anxiety, just the right amount of stress -to which the father of stress, hans selye, referred as \"eustress\" -will offer just the right combination of challenge and support needed ultimately to prompt, perhaps after an initial (defensive) derailment, subsequent (adaptive) reconstitution at a higher level of order, complexity, and integration."
"rather, the dosage of the stressor, the underlying adaptability of the system, and the intimate edge [cit] ) between stressor and system will determine whether the system, in response to the environmental input, defends and devolves to ever-greater disorganization or adapts and evolves, by way of a series of healing cycles, to ever-more complex levels of organization and dynamic balance."
"based on the therapist's moment-by-moment assessment of what the patient can tolerate, the therapist will therefore either challenge (by way of anxiety-provoking interpretive statements that call into question the defenses to which the patient has long clung in order to preserve her psychological equilibrium) or support (by way of anxiety-assuaging empathic statements that honor those self-protective defenses -a therapeutic stance often referred to as \"going with the resistance\")."
"as happens with any open system, the patient's advancement from less-complex to more-complex is never simple, straightforward, or linear. rather, evolution from illness to wellness is generally a much more protracted and unpredictable process involving multiple stops and starts, downs and ups, backward and forward movements, regressions and progressions, disruptions and repairs."
"in essence, adaptations involve higher-level processing and therefore a higher level of complexity, whereas defenses involve lower-level processing and therefore a lower level of complexity. a system that \"can\" will adapt, whereas a system that \"can't\" will defend. psychoanalysts speak of the capacity to adapt and the need to defend [cit] ."
"psychotherapists are ever busy formulating interventions that will either challenge or support -that is, challenge the patient by directing her attention to where she isn't (but where the therapist would like the patient to go) or support the patient by resonating with she is (and where the patient would seem to need to be)."
"i am suggesting, more generally, that when the interface between stressor and system (that is, between the \"dose\" and the \"response\") is such that the system is able to process, integrate, and ultimately adapt to the cumulative impact of the stressful input, then the system will be able to progress to ever-higher levels of complexity -in other words, to everhigher levels of organization and interrelatedness. but when the interface between stressor and system is such that the system is not able to process, integrate, and ultimately adapt -and, instead, simply defendsthen the system will regress to a lower level of complexity."
"in any event, my proposal is that we use the evolution of a sandpile to conceptualize the complex -and paradoxical -responsiveness of patients to the myriad of environmental stressors to which they are being continuously exposed. and my hope is that, eventually, the hormetic effect will come to be represented as not just a simple biphasic dose-response curve marked by one transition point but a complex series of nonmonotonic curves marked by multiple transition points (a complicated topological structure) -until the point of toxic overload, at which juncture the system will collapse entirely, only to resume its ongoing evolution from an entirely new baseline."
"the controller h k in section iii is such that, if h p satisfies the hybrid basic conditions, then the resulting closedloop system also satisfies the hybrid basic conditions. this fact already implies that the closed-loop system has structural robustness properties to small perturbations. in particular, [4, proposition 6 .14] and [4, proposition 6 .34] assure that, over finite time horizons, small perturbations on the initial conditions and on the data of the closed-loop system change the behavior of the solutions only slightly. the latter kind of perturbations allows for uncertainty in the model of the hybrid plant and of the controller, such as unmodeled plant dynamics and uncertainty in the values of t s and t u, as well as the presence of small external disturbances, such as small noise in the samples of z and in the computation of the feedback laws; see [4, definition 6 .27] for more details."
"so the nonlinear evolution of a chaotic system proceeds from stability through increasing complexity to chaos. initially, the system, in the face of minimal load, maintains itself, by way of ongoing homeostatic adjustments, at a baseline level of complexity. then, in the face of optimal load, the system will evolve to ever-newer levels of homeostatic balance and ever-higher levels of complexity. eventually, however, the system, in the face of overload and having exhausted its adaptation reserves, will collapse entirely, thereby devolving to a much lower level of complexityand chaos."
"in other words, for patients to progress from illness to wellness, there must be both environmental input (which constitutes the dose) and capacity of the system to manage that input (which constitutes the response). this paper will address the paradoxical impact of stress on complex adaptive systems and will develop the idea that an optimal dose of stressful input, by triggering the body's innate ability to heal itself, will provoke \"modest overcompensation\" and a strengthening at the broken places."
"shifting now from patient to sandpile: long intriguing to chaos theorists is the sandpile model [cit], which offers a dramatic depiction of the cumulative impact, over time, of environmental perturbations on open systems. the evolution of the sandpile is governed by some complex mathematical formulas and is well-known in many scientific circles but has rarely been applied to living systems and has never been used to demonstrate the paradoxical impact of stress on the living system. i believe, however, that this simulation model provides an elegant visual metaphor for how the mindbodymatrix (a term that i have coined to highlight the complex interdependence of mind and body) is continuously refashioning itself at ever-higher levels of complexity and integration -not just in spite of stressful input from the outside but by way of that input! amazingly enough, the grains of sand being steadily added to the gradually evolving sandpile are the occasion for both its disruption and its repair. not only do the grains of sand being added precipitate partial collapse of the sandpile but also they become the means by which the sandpile is able to build itself back up -each time at a new level of homeostasis. the system will therefore have been able not only to manage the impact of the stressful input but also to benefit from that impact."
"in other words, recursive cycles of disruption and repair will continue indefinitely, until some indeterminate point in time when a critical threshold will have been reached, a tipping point [cit] ), a saturation point, a point of toxic accumulation [cit] that will trigger a devastating, cataclysmic breakdown of the system -and the whole process will then begin anew, but this time from an entirely different baseline of complexity."
"in essence, psychotherapy affords the patient an opportunity, often long after the fact, to \"manage\" experience that had once been overwhelming -and therefore defended against -but that can now, with enough support from the outside, be processed, integrated, and adapted to. psychotherapy is therefore a story about the belated processing of unmastered experience and, in the face of optimal challenge, adaptive reconstitution at ever-higher levels of awareness, acceptance, and accountability [cit] ."
"i therefore propose that, in order to optimize the potential for transformation and growth, the therapist must offer, in an ongoing fashion, an optimal balance of challenge and support -alternately challenging the sandpile model: optimal stress and hormesis (when possible) and supporting (when necessary) -such that an optimal level of anxiety will be provoked in the patient, anxiety that will then provide the impetus for the patient to evolve to a higher level of awareness, acceptance, and accountability -a higher level of complex orderedness and integrated coherence."
"briefly, in the language of complexity theory [cit], an open system is chaotic (which speaks to the system's underlying orderedness despite its apparent randomness -an orderedness that will emerge as the system evolves), complex (which speaks to the interdependence of the system's constituent components), adaptive (which speaks to the system's capacity to benefit from experience), nonlinear (which speaks to the totally unpredictable but deeply patterned evolution of the system over time and in response to input from the outside), dynamical (which speaks to the emergence of novel structural configurations involving both repetition and innovation), and self-organizing (which speaks to the emergence of global patterns arising solely from local interactions between the system's constituent components)."
"in the remainder of this paper, we will assume that the feedback pair (ρ c, ρ d ) in (8)- (9) is continuous and renders a given compact set a ⊂ r n p globally pre-asymptotically stable."
"1. \"minimal load\" (the initial stage during which the system's homeostatic mechanisms will allow it to preserve both its status quo and its level of complexity); 2. \"optimal load\" (a compensatory stage during which the system's underlying resilience will enable it to evolve to ever-higher levels of complexity as it advances, over time, through iterative cycles of defensive collapse -a \"minor avalanche\" in chaos theory -and adaptive reconstitution); and"
"in other words, if the interface between stressor and system is such that the stressor is able to provoke recovery within the system, then what would have been poison becomes medication, what would have constituted toxic input becomes therapeutic input, what would have been deemed traumatic stress becomes optimal stress, and what would have overwhelmed becomes transformative."
"but if modest overcompensation (to an earlier disruption) is posited as one of the primary causes of low-dose stimulation (particularly with respect to \"toxins\"), then don't toxicology and pharmacology experiments need to be designed to factor in the element of time? in other words, if we want to be able, convincingly, to demonstrate the hormetic effect, then don't we need to conduct studies that track not just how different subjects will respond to different \"doses of stress\" but also how individual subjects will respond, over time, to the cumulative impact of stressful input?"
"as a psychoanalyst and holistic psychiatrist, i have long been interested in understanding how exactly it is that patients get better -in other words, what exactly it is that allows them to advance from illness to wellness. over the course of the years, i have come increasingly to appreciate something that is probably quite obvious, namely, that it will be input from the outside and the patient's capacity to process, integrate, and adapt to that input that will enable the patient to get better."
"indeed, complexity theory has it that the internal structure of a selforganizing (or chaotic) system is intrinsically such that, in response to regulatory input from the environment, order will ultimately emerge m. stark from chaos [cit] . this process of self-organization demonstrates nonlinearity, with erratic, often dramatic, and sometimes catastrophic transitions from one state of complexity to another whenever some critical threshold is reached (the timing for which is never knowable in advance)."
"with respect to the dose-response curve, i am therefore proposing that we consider the x-axis (the dose) to reflect the element of time and the y-axis (the response) to reflect the level of complexity in the systema lower level of complexity going hand-in-hand with defensive reactions and a higher level of complexity going hand-in-hand with adaptive responses."
"the hybrid system h p under the effect of the hybrid controller h k leads to a closed-loop system given by a hybrid system, which we denote h cl . solutions to a hybrid systems h are given in terms of hybrid arcs and hybrid inputs on hybrid time domains; see [cit] ."
"we will require the hybrid plant and the resulting closedloop system h cl to satisfy the following mild properties. we state these conditions for a general hybrid system h with data (c, f, d, g), which reduce to conditions for the closed-loop system when the inputs are removed."
"so whether the primary target is mind or body and the clinical manifestation therefore psychiatric or medical, the critical issue will be the ability of the mindbodymatrix to handle stress through adaptation. again, too much stress will overwhelm and prompt defense; too little stress will offer too little opportunity for transformation and growth; but just the right amount of stress -optimal stress -will provide just the right amount of therapeutic leverage to induce, after initial disruption, adap-"
"3. \"overload\" (the terminal stage of decompensation during which the overburdened system -the load having exceeded the system's adaptive capacity -will sustain catastrophic collapse -a \"major avalanche\" in chaos theory -and devolve to a much lower level of complexity)."
"and if the therapist's interventions make the patient too anxious, the patient may \"get defensive\" and then be unable to take in -or benefit from -the therapist's input (because the patient will have become too overwhelmed to process and integrate it). but if the anxiety elicited by the therapist's interventions is more manageable, the patient may then be able to process and integrate the therapist's input and ultimately adapt to it by reconstituting at a higher level of self-awareness and complex understanding."
"as a psychoanalyst and holistic psychiatrist, the tools of my trade include psychological interventions, psychotropic medications, and an assortment of alternative therapies. the focus will first be on psychological interventions."
"stressful stuff happens all the time. but it will be how well the living system is able to process and integrate its impact -psychologically, physiologically, and energetically -that will make of it either a growth-disrupting (sandpile-destabilizing) event or a growth-promoting (sandpile-restabilizing) opportunity. in other words, it will be how well the mindbodymatrix is able to manage the cumulative impact, over time, of environmental stressors that will either hasten a compromised system's deterioration or support a more resilient system's evolution toward increasing complexity."
"in closing: as we know, the hormetic effect speaks to the almost universal biphasic dose-response curve -a curve characterized by low-dose stimulation (whether secondary to modest overcompensation to disruption in homeostasis or to direct stimulation) and higher-dose inhibition [cit] ."
"this closed-loop system model enforces flows when both flows of the plant and controller are possible, and allows jumps according to the three possible conditions triggering a jump: jumps due to being in the jump set of the plant but not of the controller (first difference equation), jumps due to being in the jump set of the controller but not of the plant (next difference inclusion), and jumps due to being in both the jump set of the plant and of the controller (last difference inclusion)."
"then, combining the bounds above on the change of w and using the lower bound on v, there exists a positive definite function α 4 such that at jumps"
"ecs-1150306 and cns-1544396, and by afosr grant and fa9550-16-1-0015. 1 by computationally tractability of an algorithm, we mean that a computer can calculate the outcome of the algorithm in a reasonable amount of time; see [2, chapter 8] ."
"since the flow map and the jump map of the hybrid plant h p are single valued, f p and g p satisfy (a2) and (a3), respectively, when they are continuous."
"based upon study of the sandpile model, i postulate that -whatever the biological system, whatever the agent (\"poison\" or \"medication\"), whatever the endpoint (\"health-promoting\" or \"disease-promoting\"), and biochemical individuality notwithstanding [cit] -three distinct stages (fig. 1) will inevitably emerge along the dose-response curve (here intended to represent the response, over time, of a single system to ongoing environmental input):"
"in celebration of hormesis, i have written a poem entitled \"optimal stress and hormesis\" excess stress will cause mental and physical distress, and, as time passes, dyshomeostasis and chronic illness. but my hypothesis is that less stress, if well enough processed, will provide the impetus for healing and wellness, and a strengthening at the broken places -'cause of hormesis. if a bone is fractured and then heals, the area of the break will be stronger than the surrounding bone and will not again easily break. are we too not stronger at our broken places? and is there not a certain beauty in brokenness, a beauty never achieved by things unbroken? do we not acquire a quiet strength from surviving adversity and hardship and mastering the experience of disappointment and heartbreak? and then, when we finally rise above it, do we not rise up in quiet triumph, even if only we notice?"
"coalescence: coalescence is a technique for optimizing memory accesses. memory accesses from different threads can be merged into a single access to the device memory if the required conditions are fulfilled [cit] . this fusion process is known as coalescence and it is defined as a mean to gather several simultaneous memory accesses in parallel. it is promoting during the global memory accesses and it consists in a mechanism that fuses into an unique operation all the read/write accesses from the running threads in the current active block. gpus have specific hardware that detects and makes this fusion, hiding the high latency of threads accessing to local or global memory when cache is not available."
"real-time image processing systems are specially relevant in computer vision. any advanced image processing application requires a previous extraction of significant features. these features could be used in recognition or tracking systems for several applications. our proposal is oriented to improve drastically the performance of image segmentation systems. concretely, we focus on feature extraction and object classification based on those features, not only over pre-recorded video sequences but also from live video streaming."
a comparison between implementations on pc3 over the intel c?? [cpu3] and over the cuda [gpu3] applying all the optimizations is shown in table 5 .
"for this optimization process, performance analysis tools, such as intel vtune performance analyzer [cit] were applied to identify the possible hotspots. this tool aimed at increasing performance, as well as the location of hotspots, allowing us to perform a deep analysis of them. thus, vtune lets us detect, re-code and optimize our implementation, improving the performance substantially."
"in order to evaluate and achieve high performance over gpu, several tools have been used to refine code: cuda visual profiler, cuda occupancy calculator, and decompiler. this last one is a tool for disassemble code generated by a cuda project. it provides the exact register mapping of the gpu, so bottlenecks in terms of number of registers used by the kernel can be checked. we used a specific decompiler named decuda that is available at http://www.wiki.github.com/laanwj/decuda/ [cit]"
"in the light of previous results, we can conclude that blob labeling is not efficient for parallel computing and, in case of necessity for posterior stages such as tracking or distracter removal (football field lines), must be relegated to the cpu. taking this decision as a new starting point, the next step consists in the optimization of all the stages."
"the simplest technique to model the appearance coefficients consists in assuming the target as a monochrome region and modeling it as a gaussian using only two parameters: mean l and covariance r 2 . although this assumption limits the generality of the methodology, it can be easily extended by dividing the target into a predefined set of monochrome regions [cit] ."
"-classification: in this step, every pixel is classified into one of the different groups. for this, the distance between the pixel candidate and the different model of every group is computed (eqs. 6, 7) and a final decision based on minimum distance are taken (eq. 8). in addition, the membership degree to every group is computed inside a probabilistic framework giving, as result, probability images [cit] that can be used to improve the tracking quality based on stochastic approaches."
"the evaluation of our implementation has been made over a set of different low cost gpus with 16, 32 and 64 cores to study the scalability of the implementation. these tests have also been run under different cpus, to clarify as much as possible the real contribution of our implementation."
"-blob labeling: this algorithm searches for connected zones in the image. the nature of the connectivity search produces a strong dependency among neighbors. there is not a simple parallel solution and a new algorithm should be developed to take advantage of the available features. we have tried many different algorithms and implementations. the more parallel code is, the more synchronization between cuda blocks is needed, so more performance lost. evaluation: not suitable. computational cost: 93.34 ms (&54.88%). -color classification: it is also a good candidate to be implemented on gpu as computation does not have dependencies with the neighbors and it implies a substantial part of the total time in the cpu implementation. it can be decomposed into three substages: resulting image calculation by consulting the corresponding lut entry, lut update for the next frame and noise filtering by morphological operators. the coalescence ratio for reading is low because lut accesses are not regular. divergence is minimal or none. again, since this is the final stage, it supports the driver overhead of returning data results to the cpu. evaluation: suitable. computational cost: 44.37 ms (&26.09%)."
"-since transfer time is a non-negligible limitation, a detailed study for minimizing the number of data transfer operations and kernels invocations has to be done -bayertorgb performance accounts for the driver overhead and its time is worse than the cpu implementation. -motion detection has a good behavior since processing is pixelwise. high speed-up has been obtained, being 5.27 times faster."
"in fig. 11, results are compared at the application level between the gpu-cpu configurations, and the same tendency can be appreciated. a very low-cost laptop equipped with gpu1 is able to obtain enough processing ratio in fps to connect a camera to the tracking stage (8 fps or more). nevertheless a highly optimized single threaded implementation over a medium pc as cpu4 is not able to do that. a comparison gpu-cpu in pc1 shows that achieved improvement is around 2.119, 2.329 in pc2, and 3.419 in pc3. a considerable speedup has been obtained (10.679) with gpu4, a geforce gtx 260, processing 63.38 frames per second versus the 5.94 from cpu4, and 7.329 if we compare it with gpu1."
"-high-capability computing devices, such as current gpus, have an enormous potential for video processing applications. as proof, segmenting football players in real time have been possible by making an efficient use of these platforms. -the usage of gpus has meant a significant success for our application. we are able to improve all the processing stages, with the exception of labeling, with speed-ups up to 409 and using medium-cost hardware. -an hybrid segmentation implementation, where classifications is done for the whole image in the gpu and labeling is later done by the cpu without any penalty, gives us better performance. -the global performance improvement is 10.679 over a single thread implementation, making possible a processing rate of 63.38 fps over a single gpu. -over a 4-core processor we are able to process almost 25 fps in a multithreaded implementation."
"empirical experiments allow us to conclude: a successful tracking can be obtained with a processing frame rate between 8 and 15 per each camera, i.e., a processing time per image per camera around 66-125 ms, and to cover the whole football field, at least eight cameras are needed to obtain enough overlapping. as conclusion, this requirement allows us to define the concept of real time and the scalability of the processing kernel for our particular needs. fig. 4 . the procedure to generate three channels from a bayer sequence rggb needs a particular calculation for every channel. rgb values which match up in the rggb sequence are mapped directly, while other channels are calculated as an arithmetic mean of all neighbors corresponding to the same channel. for example, rgb value for a red position can be reconstructed as: -r value is copied as the same value."
"1. off-line computing -sample selection: supervised sample selection for every group (team 1, team 2 and background). -parameter tuning: a crucial issue is the adequate number of gaussians used to model every group. given the special characteristics of several collaborative sports, like a football match, where colors are well-defined, but where the video compression can generated halos around the players, a deep study was made in order to optimize it as much as possible (eqs. 4, 5). -training: expectation maximization (em) algorithm using fuzzy c-means as initialization [cit] provides final model."
"below, a brief introduction to the main techniques in cuda optimizations are described attending gpu characteristics and simt paradigm. next, a preliminary study of our application is needed taking in mind these techniques as well as different criteria such as computational cost or massively parallel computing redesign. finally, the optimized results for cpu and gpu implementations are shown and discussed."
"in the last few years, the amount of scientific application tested over gp-gpu has increased [cit] . although generally those researches [cit] are focused on specific calculations, they provide an initial idea about the intrinsic potential of this new platform [cit] . particularly, in our field of interest, several studies probe this capability in modern gpus [cit] . traditional methodologies have been implemented, such as pattern recognition algorithms based on textures [cit], gaussian mixture models [cit] or image feature extraction techniques [cit] . all these examples give an idea of the increase of efficiency that can be achieved thanks to these devices."
"a remark about the architectures and characteristics of the different equipments under test can also be extracted. despite the fact that the pair cpu-gpu are contemporary, the evolution of both architectures are not equal over time. cpu power increase in the last 2 years is negligible in comparison with gpus in the same period. this can be explained due to the maturity of both technologies and the improvement margin. cpu1 is able to process 4.11 fps while cpu3 only goes up to 6,59 fps and cpu4 only achieves 5,94 fps even slower than the previous generation."
"previous options have been tested and results are shown in fig. 9 over pc3 . by minimizing the computational cost t total 1, t total 2 and t total 3, the optimum decision can be taken. as fig. 9 shows, option 3 provides the optimum solution (64.78 ms) in comparison with the other alternatives whose costs are 144.91 and 71.25 ms. option 1 is even more expensive than [icc] implementation whose processing time is about 100.52 ms. because the extra data transfers and the kernel context switching, option 2 is worse than option 3 although the whole image is classified in this last one."
"-to study the evolution of processing rate according to image resolution. -feature modeling has been assumed as known. we propose to study the scalability according to variation in the target model (number of gaussians, nonparametric models, etc.). -to study how the classification metric (euclidean distance, mahalanobis, etc.) or even the classification methodology (neural networks, som, etc.) can affect to the final results. -to extend the application field to other compatible disciplines such as facial recognition or human tracking, to name a few."
"although gmm is a successful and broadly used method for feature extraction, its computational cost is a strong handicap for real time applications. the spectacular evolution that cpus experimented in the past has provided a tool for mitigating the problem. nevertheless, the progressive slowdown during the last years has stopped this progression, whereas it has promoted parallel architectures, such as multi-core, as a solution for increasing the computational power."
"this novel style of multi-core design and programming acquires even a more relevant position thanks to the last technological developments. return of these advances are the newest graphics processing units or gpu containing up to hundred of simple processor cores. the gpu architecture is optimized for massively parallel processing with peaks up to hundreds of gflops. but their most interesting features of these devices is that they can also be harnessed for general computing in a modality known as general-propose gpu (gp-gpu) [cit] . recently, in order to take advantage of these high performance computing devices, some extensions to well-known programming languages have been generated, such as cuda c [cit] . this language is a set of parallel extensions of the c/c?? programming languages and it is able to interact with a special hardware interface built into all current nvidia gpus [cit] ."
"the outline of the paper is as follows. in sect. 2, the hardware infrastructure is described. section 3 introduces the stages that compose our methodology and discusses their computational cost. section 4 compares the computational cost between a version in c?? using microsoft visual studio compiler and a version highly optimized using intel c?? compiler, running in a conventional multicore cpu. in sect. 5, the parallelization methodology is introduced and a cuda implementation is detailed. section 6 presents a comparison between cpu and gpu results and its scalability. finally, conclusions and future work are presented in sect. 7."
"-g value is, as shown in fig. 4c ), the average of the four-neighbor pixels: left, right, up and low pixels. -b value is, as shown in fig. 4c ), the average of the fourneighbor pixels in diagonal: up-left, up-right low-left, low-right pixels. -color space conversion rgb to hsv: under variable illumination conditions, better classification results can be obtained by applying a transformation in the color space [cit] . instead of rgb, hsv (huge, saturation, value) has shown a better accuracy (eqs. 1-3). -motion detection: it consists in a thresholded subtraction between the current image ( fig. 5a ) of every camera and a pre-generated image of the scenario, called background (fig. 5b) . process is shown in fig. 5c . motion detection image contains the dynamic areas, which will be used for posterior processing like distracter removal."
"our first implementation of the algorithm was made in c?? language running under windows. once the accuracy of the results were validated with a matlab prototype, a set of optimizations was included to obtain an improved c?? version."
all the cameras are linked by ethernet optical fiber and shielded twisted pair with a computer set which has to process the received data and combine results. a distribution schema and its connection with the computing system can be seen in fig. 2 and a overlapped zones schema can be seen in fig. 1 .
"analyzing at the stage level (fig. 10), it is important to note that improvement increase with gpu power, almost always proportional to the number of cores. the only exceptions are the conversion bayertorgb and classification stages, where driver overhead, input data dependence, and memory bandwidth produces a slightly lower rate (see fig. 10 )."
"motion detection: since it is basically a pixelwise subtraction, there is not dependency. as in the previous stage, motion detection has a high coalescence degree and there is no divergence. evaluation: suitable. computational cost: 7.2 ms (&4.23%)."
"our method to extract those features consists in an image segmentation according to color information. segmentation systems are usually a first stage inside an image processing framework. thus, for instance, results generated by segmentation techniques can be used as input for a tracking algorithm. in the literature, it exists a broad variety of methods for a reliable segmentation of objects in an image, being the most interesting ones, those capable of dealing with objects composed of various colors [cit] . one of the most popular approaches consists in a gaussian mixture model (gmm) in which every object can be represented by one or more gaussians. this is because most objects are composed not only of an unique color but also of a mixture of different tones associated with an unique color or even of several different colors."
"as a summary, main characteristics of every stage are shown in table 4 . the cuda implementation was tested over pc3, obtaining the results shown in fig. 8 . we can conclude:"
"in collaborative sport applications, feature extraction and classification, although a difficult task, have an important advantage in comparison with more general approaches like video surveillance. it is known a priori that both teams, as well as background, are defined by clear and distinctive color patterns in their clothing. these color patterns can be easily modeled by parametric methods. gmm is a method that allows a reliable object modeling and image classification even in presence of complex targets, which can be composed of multimodal appearance distributions. since it is a parametric technique, it needs an off-line training phase to calculate those parameters. training results are used afterwards in classification (on-line stage)."
gpus allow several memory access operations to run simultaneously. -gpu use single instruction multiple thread (simt) paradigm. this specific execution allows and needs many independent and simultaneous active threads that execute the same instructions over different data. all of them running as an unique kernel.
"in table 2 it is shown that confronting [mvcc] and [icc] implementations a big difference in performance exists just by compiling the code. the results prove that, as expected, using an optimizing compiler increases performance considerably."
"our goal consists in the processing and classification of football players in video sequences provided from one or multiple cameras installed in a real football stadium. the minimum number of cameras required for covering a football field depends on several factors, such as camera resolution, angle of vision and height of installation. in our infrastructure, we propose a system composed of eight static high definition digital cameras (1,388 9 1,036) positioned on the roof around the stadium. thus, we obtain a detailed coverage of the two goalkeeper areas (2 cameras for each one) as well as the rest of the field which is covered by other four cameras. it is important to remark the importance of a multi-camera representation, since overlapping cameras are crucial to solve occlusions, specially in conflictive areas. on the other hand, the more cameras you have, the more increase of computational cost. for this reason the number of eight cameras has been chosen, since we consider it is the minimum number to make viable the processing of the match: it ensures the coverage of a player by at least two cameras at any point of the pitch and with an acceptable resolution level."
"in order to check the performance improvement that our implementation achieves, we have tested the algorithm over different types of processors and gpus. thus, four different types of pcs are available: core 2 duo 2.2 ghz 3gb ram, core 2 duo 2.4 ghz 3.5 gb ram, core 2 quad 2.83 ghz 3 gb ram, and core i7 quad 2.66 ghz 4gb ram. these equipments are close to the average current processors, giving us a significant sampling of the market. on the other hand, four different gpus have been tested too: geforce 8600m gs, quadro fx 1600m and quadro fx 1800. all of them can be considered low-cost gpus containing 16, 32 and 64 cores, respectively. the fourth gpu, gtx260 with 216 cores, has been chosen to confirm the tendency."
"intensive use of simd and code modifications have been also done to allow the compiler to automatically apply simd instructions. special care has been taken in the alignment of data in memory, and vector and simd pragmas has been used. classical code optimizations [cit] as loopinvariant code motion, strength reduction, and arithmetic pointers have been used to clear loops. compiler generated code has been analyzed following the compiler high level loop optimizations (hlo) and vectorization reports (/qopt-report), vtune, and in some cases studying the generated assembler code and comparing performance with a not-vectorized version."
"-most stages are performed per pixel, so there is plenty of parallelism. consecutive stages could be grouped and executed invoking a single kernel, reducing driver and synchronization overheads. -motion detection and conversion rgbtohsv stages prove a good behavior when they are implemented over cuda. when comparing the cpu and the gpu implementation, times goes from 9.83 to 7.20 ms and from 35.61 to 5.68 ms, respectively. -in spite of pixelwise calculation, conversion bayertorgb stage presents several dependencies in its data and divergence in the operations. cuda implementation has to be carefully studied because time is higher in the cuda implementation (19.47 vs. 15.7 ms in the cpu). -labeling is not parallelizable and our designed algorithm for gpu has a deficient behavior. its computation time has increased almost 209. -cuda implementation of the classification stage presents a significant improvement in performance, representing around 58% of the total time (if we do not account labeling)."
"given the good performance achieved which confirms the initial promising idea, we consider this paper as a first step in a future research line. for that, we propose several ideas that, due to lack of time, resources or for being out of the scope of the paper have not been studied properly. future lines of research can use this increase not only for increasing the processing rate but also for an intrinsic improvement of the processing stage."
"a comparative studio between the default microsoft visual studio compiler and intel c?? was made for our application, showing that the usage of this last one was always beneficial with a general speedup of almost 4x. full optimization and specific architecture compilation flags are both used in this implementation. these specific flags perform aggressive loop and memory-access optimizations, such as scalar replacement, loop unrolling, loop blocking to allow more efficient use of cache and additional data prefetching."
"the preliminary study of the gpu execution concludes that on-line processing are composed of four stages (bayertorgb conversion, rgbtohsv conversion, motion detection, and classification), all of them are done per pixel. in addition, our implementation over gpu consists of an unique kernel, avoiding thus the extra time introduced by context changes or driver overload. this kernel receives frame data and runs the four pixelwise processes, and ends transferring the resulting data from the classification to the cpu."
"in this section, the adequacy of each stage to be implemented as a gpu kernel has been analyzed. stages are independently implemented in different kernels in order to check their behavior using gpu paradigm. this test has been performed over pc3 and the results are presented below."
"in the next section the implementation on c?? and cpu optimizations are described. section 5 does the same for the implementation on gpu and in the last section, a scalability test is performed."
"for every possible combination of both platforms (cpus and gpus), a scalability study was made. a scalability study aims to assess the performance of our algorithm as a function of the number of images, the number of cameras or the computational power. to this end, we have processed the algorithms on several computers as it is shown in table 1 ."
"another metric usually employed to evaluate the computing capability of a real-time oriented system is the processing rate or rate. rate measures how many frames are processed per second. rate equation can be described as follows: in a multicore processor we could have more than one core doing image processing. as image processing is composed of many pipelined stages, we could assign each stage to a different thread or we could have many cores working on the same frame. due to load balancing problems between threads and the added synchronization and communication, we found that it was much better to have each core working on a different frame (from the same camera or from another camera)."
"occupancy: occupancy is defined as the number of threads assigned to each processor. maintaining a high occupancy in the gpu is important in order to mask the high latency of memory accesses. it can be achieved by means of three different ways: taking care with dataindependent instructions, maintaining the number of registers per thread as low as possible and/or obtaining the best compromise occupancy-shared memory size per thread. therefore, it is important to fully exploit the parallelism available in the application."
"our system is composed of multiple and identical high definition cameras with a resolution 1,388 9 1,036. as requirement, this application must perform the capture of eight images per second, the processing of all frames (including extraction and classification processes), visualization tasks, communication and, finally, tracking."
"data in table 3 show that, while in the single thread implementation we are able to process around 6.58 fps, running one instance per core we reach about 25 fps, so it follows that there is a minimal overhead for each stage at around 2.5 % for this particular execution. conversion bayertorgb is the stage that more variability supports with a 4.84 % penalty due mainly to the increased l3 cache miss ratio."
"image processing is clearly cpu bound, but as the different cores share the last level cache and the memory bandwidth, we expect a certain performance penalty. we have run multiple instances over different frames to observe the effect on each of the stages. the results are presented in table 3 on a given run of four threads over the four cores of pc3."
"several techniques are at our disposal for an optimum use of gpu capacities according to recommended methodologies [cit] . across all the stages these techniques have been evaluated. a gpu is a device designed for highly parallel computation having a very high number of functional units and a high memory bandwidth. therefore, the main techniques for increasing performance are based on keeping up the occupation of functional units (known as occupancy), maximizing the use of effective bandwidth to memory (using techniques like coalescence) and minimizing branch divergency."
"-a gpu quadro fx 1800 (gpu3) processes 22.45 fps, so we need at least 3 low-cost gpus. -in a hybrid implementation using a pc3 and a gpu3 it was possible to process^45 fps. -a geforce gtx 260, while its price is around 150 dollars, shows a processing ratio of around 64 fps."
"the performance of a gpu system is mainly determined by the number of cores and the memory bandwidth. to verify this, we have selected different systems with different resources (shown in table 1 ) to test the performance. the aim is to study the cost evolution per stage and globally. the first 3 gpus have been chosen with a consistent growing criterion in the number of gpu cores (16, 32 and 64) . memory bandwidth almost doubles from gpu1 to gpu2, and gpu3 has almost six times more than gpu1. the fourth gpu, with 216 cores and 112 gb/s, is chosen to confirm the tendency showed in the previous tests. two comparative analyses have been done. the first one, at the stage level, evaluating the time cost for every stage for each gpu (fig. 10) . the second one, comparing the global performance of the application using the four different cpus against the gpus measured in frames per second fps (fig. 11 )."
"as a result, we obtain the differences between an optimized single threaded implementation in c?? using microsoft visual studio compiler [mvcc] versus the same code compiled with intel c?? compiler [icc] . results are depicted in table 1 (obtained using pc3 described in sect. 3.3). as can be seen, there are stages with low speed-up (like rgbtohsv), while conversion bayertorgb get a boosts in performance of 4.569. the main gain comes from segmentation that goes from 297.5 down to 90.69 ms."
"conversion rgbtohsv: in the same way as the previous stage, processing is pixelwise but there is no data dependency regarding the neighbor pixels. there is no divergence and as rgb data is kept in memory in planar form accesses are fully coalesced. evaluation: suitable. computational cost: 5.68 ms (&3.31%)."
"the hardware architecture of a system with a gpu can be seen in fig. 7 . a gpu is a hardware device connected to the main system through a fast bus, second-generation pci express currently. it has some very specific processing features regarding the current cpus. specifically, the features that make gpus specially powerful in massively parallel computing are: -hardware composed of several computing functional units and several multicores. -in single precision floating point, a gpu can reach up to 500 gflops owed to the 30-50 gflops of conventional cpus. -high bandwidth for the internal memory up to one order of magnitude higher than the bandwidth of a cpu and system memory (up 111.9 gb/s in gpu4)."
"-blob labeling: it is the algorithm that seeks connected areas, called blobs, in the resulting image of the previous step. by grouping pixels into blobs and assigning a common label we simplify the posterior tracking stage. -color segmentation: this procedure tackles the problem of identifying different areas of the image. gmm (gaussian mixture model) has been chosen as paradigm, which implies a preliminar training by extracting color features from regions of interest. thanks to this technique, a distinction into three groups is obtained: player of team 1, player of team 2 and noise from the background."
"all image processing operations described in this section have been implemented in the corresponding cpu and gpu versions. for validating them, results were compared with a prototype modeled in matlab, confirming that insignificant differences are only due to typical rounding errors."
"professional sport is an extremely competitive world. mass media coverage has contributed to the popularity of many sports, increasing its importance in our current society due to the money and fame that it generates. in this environment, in which any assistance is welcome, video-based applications have proliferated. video-based approaches have shown themselves to be an important tool for analysis of athletic performance, especially in collaborative sports, where many hours of manual work are required to analyze tactics and collaborative strategies. computer-vision-based methods can provide help in automating many of those tasks."
"these measurements have been obtained using the evaluation metric shown in eq. 9, where sp fi is the speed-up for stage i, t fi,mvcc is the execution time of stage i optimized using visual studio and t fi,icc the execution time of stage i optimized using intel c??."
"in our research, we have developed an application which is able to detect football players in a video sequence. once they are extracted from background, each player is classified into any of the teams. for classification purposes, a color-based method is employed based on expectation maximization for gaussian mixture models [cit] . since one of our main objectives is to process multi-highresolution cameras, detection and classification processes must be applied on real time in an extremely efficient manner. in order to achieve that, we have adapted and implemented those tasks over gpu platform taking advantage of its high parallel computational capability (sect. 5)."
"divergency: in the simt paradigm implementation of cuda gpus, high performance is obtained when all the thread in the same active block are executing identical instruction. in conditional execution code (i.e. conditional branches) several threads could take different paths. the result could be the serialized executions of diverging threads within a block, and therefore, increasing the cost for every divergent thread."
"conversion bayertorgb: this stage requires, for every pixel, access to the neighbor pixels in order to fig. 7 hardware architecture of a system with gpu calculate the resulting rgb. the processing is made per pixel independently, although the final result also depends on the adjacent input values such as fig. 4 shown. therefore, there is no coalescence in reading or writing, it has a high grade of divergency (each pixel is computed in a different way) and because it is the first stage, it supports the driver overhead (data has to be send to the gpu). resulting rgb data are saved in memory as planar form to take advantage of coalescence in the following stages. evaluation: suitable. computational cost: 19.47 ms (&11.45%)."
"the proposed classification algorithm can be decomposed into a set of steps. most of them should be done per frame and per camera. the steps and input data that they require are described at following sect. 3.1 and in fig. 3 the processing flow per camera is detailed. output generated from previous stages can be used as input for a tracking algorithm in order to ensure the temporal coherence. several different options can be found in the literature [cit] . although it is out of the scope of this paper, a multi-camera uncensted kalman filter (mcukf) [cit] has been used to demonstrate the global feasibility."
semantic similarity and relatedness measures quantify the degree to which two concepts are similar (e.g. liver-organ) or related (e.g. headache-aspirin). these metrics are critical to improving many natural language processing tasks involving retrieval and clustering of biomedical and clinical documents and developing biomedical terminologies and ontologies. numerous ways exist to quantify these measures between distributional context vectors but to date there has not been a direct comparison between these metrics nor an exploration of representing multi-word context vectors. we explore several multi-word aggregation methods of distributional context vectors for the task of semantic similarity and relatedness in the biomedical domain.
"we found that vector dimensionality of 200 is best for skip-gram and continuous bag of words, and a dimensionality of 1000 is best for svd. sg and cbow created better vector representations than explicit and svd, but their is no significant increase in correlation using sg versus cbow. in regards to multi-word term aggregation methods including the summation and averaging of component word vectors, creating multi-word term vectors using the compoundify tool, and creating concept vectors using the metamap tool. concept vectors achieved the highest sum of correlations across all four datasets, but only marginally. no statistical significance was found between any multi-word term aggregation method across all dimensionality reduction techniques, and dimensions tested."
(ii) no overlap of a label and another peak location. (iii) each label is placed among the four possible labeling rectangular spaces of the peak locations. (iv) at most five labels for a peak location can be assigned.
"after obtaining the global position of the human by this fusion process, the controller translates the skeleton to this position and updates the relative positions and orientations of each bone of the skeleton with the rotational measurements from the inertial sensors of the motion capture system. the relative transformation of each bone is also applied to the corresponding bounding volume which covers it. thereby, the system controller is able to update on real-time the position and orientation of all the bounding volumes which composes the body of the human operator. finally, the system controller computes the minimum distance between the bounding volumes which cover the body of the human operator and the bounding volumes which cover both robots. the bounding volumes of the robotic manipulator are updated by applying forward kinematics to its joint values obtained from the robot controller. the bounding volumes of the mini-robot are updated by applying inverse kinematics to its end's pose obtained from the virtual visual servoing described in section 2.3. thereby, the system controller computes the minimum human-robot distance by calculating the distance between each bounding volume of the human and each bounding volume of one robot and choosing the minimum value between all the pairwise distances. fig. 17 shows the minimum distance between the human operator and the mini-robot (red line) and the pa10 manipulator (blue line), which are computed by the system controller during the execution of the developed task. initially, the human operator is out of the robots workspaces (5 m away from the pa10 manipulator and 2 m away from the mini-robot). then, the human operator begins to approach the robotic system by following the linear trajectory which is identified by fig. 16 . therefore, the distances between the human operator and the robots begin to decrease. meanwhile, both robots continue their normal operation and they track the pre-established paths shown in figs. 10-12. nevertheless, when one of the human-robot distances is below the safety threshold (1m in this case), both robots stop their tracking process. in this experiment, the distance between the human and the mini-robot arrives at the safety threshold (1m) and this fact makes both robots stop at the positions of the trajectories marked by two arrows in fig. 13 ."
"this first experiment is presented in order to describe the correct behaviour of the proposed direct visual servoing approach. to do this, the image tracking of the desired trajectory presented in fig. 5a is shown. fig. 5 represents the desired trajectory and the one obtained by using the presented controller (in the image and in the 3d space). a correct behavior is obtained, and a correct tracking of the desired trajectory is performed. fig. 6 depicts the joint torques sent to the first and second joints (very low values for the third joint are obtained). fig. 7 shows the robot velocity during the tracking."
"label computation. the term distribution in the set of documents in the vicinity of a peak is compared with a reference distribution. a chi-square test of significance with yates' correction determines over-represented terms. the term co-occurrence analysis, based on pattern matching algorithm, along with trigger phrases based on regular expressions, is used to identify the frequently appearing text fragments within the same sentences and within the documents [cit] . the redundancy of nouns' singular and plural forms and synonyms in the resultant list of labels are removed by using a combination of regular expression queries and wordnet library lookup."
"the proposed direct visual servoing system is applied for the guidance of a 3 dof mini-robot in a multi-robotic system. the camera is located at the end-effector of the mini-robot. the images acquired from this camera enable the direct visual servo of this mini-robot. this mini-robot is located at the end-effector of a 7 dof pa-10 robotic manipulator. this robotic manipulator is also guided with the images obtained from the camera positioned at the mini-robot, by performing an eye-to-hand visual servoing system. the contributions of this paper focus on the method used for guiding the mini-robot, while the system employed to guide the robotic manipulator is just a classical eye-to-hand visual servoing widely described in the literature [cit] . the camera is positioned in the scene by the mini-robot and thus, the manipulation task visibility is increased by using direct visual servoing, whereas the robotic manipulator accomplishes the manipulation task. this application is presented in this paper in conjunction with a safety behavior which stops the robot motion when human operators are near the mini-robot. this paper is organized as follows: section 2 presents the visual controllers employed. in section 3, the experimental setup and the safety behaviour is described. section 4 shows the obtained experimental results. finally, some conclusions are discussed in section 5."
"the image-based approach described in section 2.1 is used to guide the robotic manipulator. however, in this case, the camera is at the end of a moving robot (i.e. the mini-robot). as it was previously indicated, the guidance of the robotic manipulator is not the objective of this paper. finally, in order to perform the mini-robot guidance, the direct visual servoing system described in section 2.2 is used. the desired image trajectory employed to guide the mini-robot is obtained by using a teaching by showing approach. although singularities can appear during the task, this problem is not addressed in this article. in order to test the proposed controllers, singularity-free trajectories have been considered."
"at time t is composed by the global coordinates of the skeleton of the human operator. this filter is composed by two main steps: prediction and correction. the measurements from the inertial motion capture system ( ) inertial t z are applied in the prediction step of the filter, as shown in the following expressions:"
"with document layout positions at hand, we compute an elevation matrix (8) that represents an information landscape model. we then utilize this matrix to identify peak locations, heights and a list of documents related to the peak (9). the peak detection employs a kernel window convolution over the landscape model. the peak label assignment module (10) determines the peak's labels by using the list of documents under the peak for querying and comparing with the semantically tagged reference corpora (1e), which is continuously refined by the weblyzard platform. finally, the assigned labels are positioned on the information landscape surface images (12), computed based on the coloring scheme (8) and the heuristic labeling algorithms of the landscape image rendering module (11)."
the use of visual servoing systems allows relative positioning with respect to an object in a dynamic workspace by using visual information in the system's feedback. uncertainties in the initial position of the object or in the robot kinematic model do not affect control performances or the system behavior. visual servoing systems permit to carry out point-to-point motion of a robot using visual information. the most known classification for this kind of systems divides them into position-based and image-based visual servoing systems [cit] . the first ones use 3d information of the real workspace to guide the robot and the second ones employ 2d information provided by the camera images. this last approximation is used in this paper.
"unfortunately, we observed that landscapes generated with this positioning method reflected geometrical edges; i.e., documents were positioned in straight lines. to maintain the viewing experience of a realistic landscape without artifacts, and to achieve a linear running time, we introduce an improved approach for fast document positioning which is essentially based on a simple spring forces-based model (cf. [cit] ). in this model we assumed that the document, in two dimensions, is attached to each centroid, in two dimensions, by a spring having a spring constant proportional to the similarity between the document and the centroid in the n-dimensional space."
"both steps of this kalman filter are only executed when the corresponding measurements are received. thereby, the prediction step of the filter is performed with new measurements from the inertial system while the correction step is only executed with uwb measurements. in addition, since all the measurements should be represented in the same coordinate system, a transformation matrix uwb inertial t is used in order to represent the inertial measurements in the coordinate system of the uwb system. this transformation matrix is recalculated each time a new measurement from the uwb system is received. thereby, the error of previous inertial measurements is not accumulated in the following inertial measurements because this matrix is computed from the new uwb measurement, which has better position accuracy."
"( ) uwb t z from the uwb localization system are applied in the correction step of the filter, as shown in the following expressions:"
"image-based visual servoing only uses the visual data obtained in an image to control the robot movement. this system is adequate to position a robot from an initial point to a desired location, but it cannot control intermediate 3d positions of the end-effector. the behaviour of these systems has been proved to be robust in local conditions (i.e., in conditions in which the initial position of the robot is very close to its final location) [cit] . however, in large displacements, the errors in the computation of the intrinsic parameters of the camera [cit] or in the estimation of the distance to the object [cit] can drive the system to a local minimum. when the trajectories performed by the robotic manipulator are not long, an image-based visual servoing system is adequate. however, in order to avoid this problem during the minirobot guidance, the approach described in the next section is proposed."
"landscape modeling. information landscapes with specific resolutions are modeled as elevation matrices of the same resolution. a document is thought of as a small gaussian peak at the corresponding position on the underlying matrix cells. the influence of a document on a matrix cell location is reflected by the value of gaussian density at that location. thus the height and the asymptotic radius of the gaussian peak reflect the document's influence in the landscape. we further assume a document has a fixed influence on its own location on the matrix cell. the densities of all documents at particular location are superimposed, adding to the elevation values of the underlying matrix cells."
"peak detection. a kernel window-based peak detection algorithm is used to detect the significant peaks of the landscape (cf. [cit] . the average of the convolution of the window with the elevation matrix is compared with the center value of the matrix cell. a peak is assumed if the center value is higher than the average convolving value. after detecting the significant peaks, documents are assigned to their nearest peak by using the minimum euclidean distance criterion in the 2d layout."
"this paper presents an incremental, scalable algorithmic approach for computing dynamic topography information landscapes capable of visualizing dynamically changing text repositories. our incremental processing pipeline is introduced in section 3 and includes implementation details of text preprocessing, projection (dimensionality reduction), labeling and rendering stages where the projection part combines document clustering, cluster force-directed placement and, an improved approach to fast document positioning. we conclude this section by visualizing a temporal sequence of eight incrementally computed information landscapes, which reflect weekly changes in the underlying document set. in section 4, we experimentally verify our approach's runtime behavior which we discuss only in theory in section 3. in addition, we evaluate our incremental computation framework by comparing stress values between incrementally and non-incrementally computed layouts. documents for these experiments are taken from the environmental blog sample of the media watch on climate change [cit], a web content aggregator on climate change and related environmental issues. our experimental results show that the incremental computation approach yields not only comparable, but even slightly better stress values and thereby indicate our framework's validity."
"in incremental mode the k-means algorithm module is initialized by the previously computed clustering result (1c). the centroid positioning algorithm (6) uses results (5a) and (5b). the algorithm can be initialized with previous centroid positions (1d) for the incremental case, or by assigning random positions for the non-incremental computation. the centroid positions (6) and the document to centroid similarity matrix (5c) are then used for computing the document positions (7). document clustering. we apply the spherical k-means algorithm [cit] to partition the documents into topical clusters. the k-means algorithm is known to be highly sensitive to the initial guess of the cluster partitions and the number of partitions. to overcome this sensitivity, we use the k-means++ seeding method [cit] . in addition, we split and merge the clusters [cit] for deducting the number of clusters within the limit of specified minimum and maximum bounds. as human cognition puts certain limits to conceiving visualizations, we limited the number of clusters to account for usability. we observe that setting the minimum and maximum number of cluster bounds to be 30 and 40, respectively, result in meaningful and aesthetically pleasing information landscapes. therefore, in subsequent iterations we perform the splitting of large clusters to obtain higher cluster cohesion as well as the merging of small, similar clusters, according to improvements using bayesian information criterion [cit] ."
"the image-based visual servoing approach entails extracting visual data from an image acquired from a camera and comparing it with the visual data obtained at the desired position of the robot. by minimizing the error between the two images, it is possible to drive the robot to the desired position. a visual servoing task can be described by an error function, e, which must be regulated to 0:"
"prior to the beginning of the computation, the raw textual data to be analyzed is gathered via a web crawler, then converted and annotated into the content repositories based on previous research [cit] . we utilize our experiences with weblyzard, an established and scalable media monitoring and web intelligence platform (www.weblyzard.com), to generate the document keyword relevance table (1a) and the document word frequency table (1b). using the information from 1a and 1b, we create the document keyword matrix (2a) as well as the document word matrix (2b). both matrices are then linearly combined into one augmented document term matrix (3) with unique terms ids."
the stress values (y-axis) for incrementally computed documents layout and for non-incrementally computed documents layout over a period of 10 weeks; right: run times in seconds (y-axis) for landscape computation framework with different document sets (x-axis)
"incremental dimensionality reduction. dimensionality reduction techniques transform high-dimensional data into low-dimensional data seeking to lose as little information as possible. this transformation has turned out to be particularly useful in the field of visualization for projecting the high-dimension data into the low-dimensional visualization space. to face the growing amount of data, incremental variants have been developed usually on top of batch methods. incremental unsupervised techniques include multi-dimensional scaling (cf. [cit] ), singular value decomposition (cf. [cit] ), principal component analysis (cf. [cit] ), random indexing (cf. [cit] ) or locally linear embeddings (cf. [cit] ). unsupervised methods are effective in finding compact representations, but ignore valuable class label information of the training data. incremental supervised techniques are thus better suited for pattern classification tasks. representatives of incremental supervised dimensionality reduction techniques include linear discriminant analysis (cf. [cit] ) or subspace learning (cf. [cit] )."
"for the sake of clarity, a new notation is presented in order to define the virtual visual servoing approach employed in this paper. the observed visual features in the image are indicated by pd. furthermore, p are the current positions of the image features. this last set of features is obtained by projecting in the image the set of 3d points, o p, by using the current camera intrinsic parameters  (principal point and focal which are determined by using [cit] ) and the extrinsic parameters c mo (pose of the object frame with respect to the camera frame):"
"where s is a k x 1 vector containing k visual features corresponding to the current state, while sd denotes the visual features values in the desired state. the variations of these visual features in the image space can be related with the variations of the velocity of the camera by the interaction matrix ls [cit] :"
"this experiment represents a task in which both robots perform a positioning task. in this experiment, the minirobot tracks a straight line between the initial and final locations represented in fig. 10 and fig. 11, respectively. furthermore, the initial and desired locations for the robotic manipulator are indicated in fig. 12 . by using the proposed visual servoing approach, the trajectories presented in fig. 13 are obtained. a correct behavior is also obtained. furthermore, the image trajectory is represented in fig. 14. this figure represents the evolution of the image features extracted by the minirobot during the task. it can be observed that the final visual features converge towards the desired ones and the desired trajectory (straight line) is correctly followed."
"future work will focus on improving layout quality by utilizing semantic information in the process of calculating similarities between documents. these semantics will help us to better handle linguistic concepts such as synonymy and thus to capture more implicit, meaningful associations amongst textual resources."
"in order to test the safety behavior of the proposed system, the same desired path of the previous experiment is tracked by the system. nevertheless, in this test, a human operator enters the robot's workspace whereas the robot is performing the path tracking. when the human-robot distance computed by the safety system described in section 3.2 exceeds the safety threshold, a safety behavior is activated and the normal control of the robots is deactivated. this safety behavior involves stopping the movements of both robots. the instant when this safety threshold is exceeded is shown in fig. 13 . the 3d trajectories shown in this figure have been obtained by using virtual visual servoing and inverse kinematics in the case of the mini-robot and by using forward kinematics in the case of the robotic manipulator. this approach allows the accurate determination of their endeffector locations."
"we have introduced and evaluated an incremental approach to generating dynamic topography information landscapes, and applied this approach to visualize the content dynamics of environmental blogs. our method combines well-known algorithmic approaches, such as k-means clustering and force-directed placement, and introduces an improved method for fast document positioning which relies on previously computed cluster centroid positions. in experiments, we have compared the quality of incrementally and non-incrementally computed layouts where the incremental version achieves not only comparable, but even slightly superior stress values. by capturing changes in textual data repositories such as news and social media archives, and by revealing the emergence and decay of major topics in such repositories, an incremental version for computing information landscapes extends the repertoire of existing web intelligence and social media analytics applications such as the media watch on climate change (www.ecoresearch.net/climate)."
"a great number of image-based control systems implement the control by using the indirect approach. these systems employ two nested loops running at different frequencies. the external control loop computes the kinematic control to guide the robot to the desired location. since the controller has to send torques to the actuators, the role of the internal control loop is to regulate the robot velocity. the external control loop works at camera velocity whereas the internal control loop works at a higher frequency."
"in an earlier version of the algorithm [cit], we used an algorithm based on delaunay triangulation of centroid positions in the 2d space. the most similar triangle was chosen based on the similarities between the document and the most similar centroids, and the document position was assigned using barycentric coordinates in o(m) time, m being the number of centroid vertices."
"to compute dynamic topography information landscapes in an incremental and thus timely efficient manner, we integrate and combine incremental aspects into the generation process. (i) for clustering, we apply a simple, spherical k-means [cit] and use previously computed partitions of the document set as initial state for incremental computations. (ii) we introduce an improved approach for document positioning which is essentially based on a simple spring forces-based model (cf. [cit] ) since we observed that landscapes generated with standard positioning method displayed geometrical edges. (iii) we use a force-directed placement (fdp) algorithm [cit] for projecting these high-dimensional cluster centroids into a 2d visualization space. the parameters of fdp-based methods provide significant control over the layout, which allows them to deliver more pleasing layouts than traditional methods. the fdp algorithm is intrinsically incremental when applied on a previously computed stable layout. re-applying fdp on a previous layout of centroids with modified similarities will produce a new layout closely resembling the previous one."
"hence, a model of the observed object is necessary in order to apply the virtual visual servoing approach. this model represents the 3d position of some interest points with respect to the object coordinate frame (i.e. o p)."
"we propose a visual control system for the guidance of two coupled robots (mini-robot and manipulator). however, the main contribution is the definition of a dynamic visual control system for the guidance of the mini-robot. the proposed system performs correctly the tracking of desired trajectories in cooperation with the robotic manipulator. furthermore a virtual visual servoing approach is defined in order to maintain the safety in human-robot cooperation. currently, we are improving various aspects of the mini-robot dynamics as well as implementing new control schemes to implement the visual servoing on real-time."
"this second experiment shows the tracking of a different reference path. in this case, this path is a complex path with a direction change in the image. the desired evolution of the features in the image is shown in blue in fig. 8a . the image trajectory obtained by the mini-robot when the proposed direct visual servoing tracker is used for the path tracking is also shown in fig. 8a . the system behaviour is correct not only in the image space, but also in the 3d cartesian space (see fig. 8b )."
three different experiments are presented in this section. the first one represents the tracking of a trajectory by only using the mini-robot. the second one shows the correct tracking behaviour with a more complex path. the third one combines the two robots. the extracted features are the corners of a square pattern of side 25 cm. this pattern is located at the workspace so that the camera is able to extract the visual features during the task.
"3.1 experimental setup fig. 3 represents the experimental setup, which is composed by two cooperating robots. the robotic manipulator is a mitsubishi pa-10 robot with 7 dof. this robot has a barrett robotic hand at its end-effector. at the end of this robot, a mini-robot with 3 dof is also attached. this mini-robot has a gigabit ethernet tm6740gev camera at its end-effector. the visual information obtained from this camera is employed to guide both robots."
"the previous approach positions a robot with respect to the observed object. however, if the robot has to track a given trajectory, some modifications need to be performed in the previous approach. to do so, firstly"
"the resulting stress values for both computation types are summarized in fig. 3 (left) . [cit] documents was taken from september 30 th, 2011. every week this document selection changes, i.e. new documents arrive whereas the same number of documents, the oldest ones, are removed resulting in a set of constant size. stress values for both computation types are decreasing while values for the incremental computation appear to be slightly lower than for the non-incremental computation. in the non-incremental case, the curve exhibits more fluctuations, e.g. the peak on november 4 th . in our opinion this behavior is due to k-means' and fdp's sensitivity to initial conditions. we hypothesize that stress values for the incremental computations are lower because these weekly incremental changes have the potential to shake the fdp process out of local minima so that the performance can improve. the experimental results corroborate that our algorithmic approach is capable of accurately generating dynamic information landscapes in an incremental manner."
"is considered as a sampling of the desired trajectory to be tracked by the robot, where n is the number of samples of the path. the error function e applied in (5) is modified in order to allow the tracking:"
a fusion algorithm based on a kalman filter [cit] has been applied in order to combine the translation measurements from both trackers. the state of this filter
"to examine the algorithm's execution times for different data set sizes, we experimentally verified the runtime estimates for individual processing steps given in sections 3.1 to 3.3. fig. 3 (right) [cit] to 16000. processing steps include document-term matrix preparation (a), clustering (b), document positioning (c) (including cluster positioning with fdp which is in constant time for fixed number of clusters) and peak detection, label positioning and image construction (d). graph (t) reflects the total runtime for generating dynamic topography information landscapes for different data set sizes. according to fig. 3 (right), the clustering step (b) appears to be the algorithm's runtime bottleneck."
"to determine the pose of the end of the mini-robot, the virtual visual servoing approach described in section 2.3 is applied. this system determines the camera extrinsic parameters. furthermore, the relation between the camera and the mini-robot end-effector is constant. therefore, the pose of the mini-robot can be easily obtained. inverse kinematics is applied to this pose in order to determine the current configuration of the minirobot and update the positions of its bounding volumes accordingly. similarly, forward kinematics is used to update the positions of the bounding volumes of the robotic manipulator. next, these updated bounding volumes of the robots and the bounding volumes of the human are used to obtain the minimum human-robot distance by executing pairwise distance tests between them. when the human-robot distance is smaller than the safety threshold, the robots will stop their normal behaviour and thus, collisions between the human and the robots are avoided and human safety is ensured."
where h a 3x3 identity matrix and r is a 3x3 diagonal matrix whose terms correspond to the components of the mean error of the measurements from the uwb system.
"where a is a 3x3 identity matrix which directly applies the inertial measurement ( ) inertial t z as prior estimate ˆ t x of the global position of the human operator.  t p is the prior error covariance matrix which represents the accuracy of the state estimate. this matrix is computed from the matrix q, which is a 3x3 diagonal matrix whose terms represent the components of the mean error of the measurements from the inertial system."
"(ii) in a second step (cyan), we cluster and position the documents. we use the k-means clustering algorithm to partition the documents into topically related clusters. we then employ force directed placement to project clusters centroid positions into 2d visualization space, and apply a fast method for positioning documents in 2d based on cluster positions. (iii) in the last step (magenta), we use the documents' layout position to model a topical landscape which is essentially an elevation matrix on a 2d grid. a coloring scheme is used to construct landscape surface images. a peak detection algorithm then finds major peaks (hills) and collects underlying documents to compute text descriptors for labeling the peaks. note that previous computation results are used as initial state for incremental processing (in orange). details on these three main components will be provided in the following subsections 3.1 to 3.3, followed by a separate description of the architecture's incremental aspects in section 3.4."
"the global position estimates obtained from this kalman filter and the rotational measurements of each inertial sensor of the motion capture system are applied on a skeleton which represents the kinematic structure of the human operator. the bones of this skeleton are segments which do not consider the dimensions of the surfaces of the human's body. therefore, this skeleton has been covered by a group of bounding volumes (see fig. 4 ) which approximate the surface of each limb of the human operator's body [cit] . in particular, each bone has been covered with a swept-sphere line (ssl) since this type of bounding volume has a good relation between the tightening of the cylindrical shape of the human limbs and the computational efficiency of the pairwise distance tests. the structures of the robots are also covered by these bounding volumes in order to compute efficiently the minimum distance between them. the result of the fusion algorithm is a set of translational measurements which determine the global position of the human operator in the workplace. these measurements are applied to the relative measurements of the motion capture skeleton in order to obtain the global position of each limb of the human operator's body. then, this updated position is applied over the corresponding bounding volume so that the positions and orientations of all the bounding volumes which cover the skeleton are updated on real-time."
"an inertial motion capture system is used to avoid possible collisions between the human operator who collaborates in the task and the robots (see fig. 4 ). this system is able to track all the movements of the full body of the human operator and it represents them on a 3d hierarchical skeleton. thereby, this system not only estimates the global position of the operator in the environment but it also determines the location of all the limbs of his/her body. although this system registers very precisely the relative positions of the different parts of the skeleton, it accumulates an important error in the global displacement of the skeleton in the workplace. to solve this problem, a uwb localization system is used to correct the global translational error of the motion capture system. the fusion of the global translation measurements from both tracking systems will combine their advantages: the motion capture system will keep a high sampling rate (30hz) while the uwb system will correct the accumulated translation error."
"in order to determine the camera parameters, it is necessary to minimize iteratively the error between the observed data, pd, and the position of the same features p computed by using (8) . therefore the error is defined as:"
"these days we are confronted not only with constantly growing, but also with continuously and often rapidly changing \"big data\" repositories. information landscapes represent a powerful visualization technique for conveying topical relatedness in large document repositories [cit] . yet, the concept of information landscapes does only allow for visualizing static conditions. in previous research, we have introduced dynamic topography information landscapes [cit] to address both (i) topical relatedness and (ii) visualization of data changes. as such, dynamic landscapes have proved valuable in enterprise scenarios involving visual knowledge discovery in large, dynamic text repositories, where they have been applied for tracking of topical relationships and trends in media and patent databases [cit] . dynamic topography information landscapes are visual representations based on a geographic map metaphor where topical relatedness is conveyed through spatial proximity in the visualization space with hills representing agglomerations (clusters) of topically similar documents. hills are labeled with dominant terms from the underlying documents to facilitate the users' orientation. when a document repository changes over time, e.g. new documents are added or old documents are removed, the overall topical structure changes as well. dynamic information landscapes convey these changes as tectonic processes which modify the landscape topography accordingly. in the process of generating information landscapes, high-dimensional data is projected into a lower-dimensional space. yet, existing dimensionality reduction approaches lack several aspects including (i) support for incremental computation, (ii) scalability with respect to data set size and high-dimensionality (iii) and generation of aesthetically pleasing layouts which are necessary for visual applications."
"the incremental clustering algorithm (4) takes the document term matrix (3) as input and outputs (i) a centroid-to-centroid similarity matrix (5a), (ii) a document-centroid relationship graph (5b), and (iii) a documents-to-centroids similarity matrix (5c)."
"in this section we introduce and describe our approach to generating dynamic information landscapes. fig. 1 depicts the overall workflow, which can be grouped into three main components: (i) first (shown in green), we prepare an augmented document-term matrix by combining information from keyword relevance and word frequency tables."
"the implemented routines can be used in studies on molecular (co-)evolution by working on provided fasta-files. the mi computation can be run in batchmode to allow for compute cluster usage. the output of computed mutual information values and their weighting by z-scores can be opened by the interactive matrix visualization software provided. there, as described above the user can navigate the vast amount of data, applying filters in sequence space and in value space, and using zscores (or other externally provided weights) to estimate and visualize the statistical significance of the mutual information values."
"recently, visualization has been widely recognized as a promising approach to help analysts and researchers to better understand such large amounts of complex data. the approach suggests to have visual-interactive displays appropriately encode information using visual mappings; and let the user interactively manipulate these displays to navigate, drill-down, and explore [cit] ."
"although in the post-genomic era [cit] we have access to huge databases of sequences, in a typical setting the number of sequences available is still only of the order 10 2 . we have previously shown that this limitation might lead to substantial finite-size effects in the computed mi values [cit] . these effects can be compensated by normalizing the obtained mi values to a null model of evolution [cit] ."
"in the mimatrixviz package we provide routines to compute mutual information of evolutionary dynamics in molecules. the package is capable of normalizing those values and therefore accounts for finite-sized data sets. the visualization part is separated from this to allow batch-usage on servers and clusters for sufficient statistics. the visual approach allows to interactively explore the data, and investigate patterns, structures, and particular interesting spots within the mutual information matrices."
"the user can explore a corpus consisting of 39 presidential debates held in the us annotated with argumentative components, i.e., premises and claims. the user can select the debate she is interested in, and after the selection, she is provided with the list of premises and claims proposed in the debate together with the candidate who put forward the argument component and the date in which the debate held. in addition, the user can visualize the whole debate transcript, where the premises and the claims can be highlighted with different colors, depending on the goal of the user. analyze your own debate: if the user is not interested in the debates of the corpus, she is presented with the opportunity to paste in a free text window the text of the debate she aims at analyzing, from the argumentative point of view. the result of the analysis is the pasted text where the claims are highlighted with the green color, and the premises with the blue color. explore the us presidential debates: the user can also explore the corpus based on the named entities we identified in the debates using the stanford named entity recognizer. 1 the user can filter the data based on the type of named entity (i.e., person, location, nationality, organization and religion), on the year in which the debate held [cit] are listed), and on the speaker (all the candidates of the debates are listed)."
"in this paper, we presented disputool, a tool to support humanities scholars in addressing an argumentative analysis of political debates. the tool automatically identifies premises and claims put forward by the candidates during political debates, and highlights the main named entities discussed in these debates. to the best of our knowledge, this is the first tool tackling this challenging goal in a fully automated way by employing natural language processing and machine learning methods. alas, several future work directions still have to be considered. first of all, we are annotating the us election corpus with relations between the argumentative components (i.e., support and attack), and we will train a classifier for this task as well, so that disputool can be empowered with this further feature. second, we will provide a graph-based visualization of the argumentative structure of each candidate viewpoint and the one combining the opinions of the different candidates about a specific topic. third, we will include in disputool the possibility to highlight contradictions that can possibly hold between the viewpoints of the same candidate or candidates from the same political party on a particular topic discussed in the debates."
"important problems to address in designing effective matrix visualization systems involve choosing an appropriate color scale [cit], data preprocessing steps, and applying suitable sorting on the matrices to be visualized. the latter is specifically important, as is allows to make assessments on the overall structure of the relationships. matrix sorting usually arranges rows and columns of the matrix by similarity, with an appropriate similarity function defined on the row and column vectors of the matrix."
"the stand-alone program micato (mutual information calculation tool) reads a sequence file in fasta format and calculates the mi of the sequence contained therein. by separation from the visualization tool micato can be run on e.g., clusters using job-scheduling systems. this is useful in particular for sampling large instances of null models for normalization."
"disputool relies on argumentation mining methods to automatically identify argumentative components (i.e., premises and claims) from the textual transcripts of political debates in english. it allows to highlight the premises and claims proposed in the debate by the different candidates and to explore these debates, showing the main named entities mentioned in the debates by the candidates based on the year of the debate. two tasks are crucial in argumentation mining: (1) argument component detection: the identification of arguments within the input natural language text. this step may be further split in two different stages such as the detection of argument components (e.g., claim, premises) and the further identification of their textual boundaries. approaches addressing this task adopt different methods like support vector machines (svm), naïve bayes classifiers, logistic regression, and neural networks; (2) relation prediction: the prediction of the relations holding between the argumentative components identified in the first stage. the predicted standard relations between the argument components are attacks and supports. different methods have been employed for this task, from standard svms to textual entailment. in dis-putool, we focus on the argument component detection task, while we leave as future work the relation prediction one."
"the implemented application generically supports two data matrices: one data and one weight matrix (see section normalization & weighting) which can be inspected individually or jointly. the general approach is to map the normalized matrix values to an appropriate color map and display it as a grid. while using color is typically not the first choice for representing absolute values, it allows comparative analysis of value ranges and provides a highly compact view of the overall data distribution. specifically in the case of large data matrices and in conjunction with an appropriate matrix sorting mechanism it allows the assessment of the features of the matrix. the application supports detailed visualization of either one of the two matrices. detailed information is available by interactively zooming into parts of the matrix and restricting the displayed data to specific value ranges. thereby, the approach follows shneiderman's information visualization mantra (\"overview first -zoom, filter, refinedetails on demand\" [cit] ). sorting the matrix by arranging rows and columns by similarity reduces its complexity and allows identification of systematic (similar) relationships between entities of the experiment by homogeneous colors [cit] ."
"the mutual information (mi) is widely used to detect such correlated evolutionary dynamics. the computation of the mi (see section information-theoretical measure for details) itself is straightforward. however, proper normalization needs to be carefully taken into account [cit] for typical, finite-sized data sets. this issue is discussed in more detail below. the problem of analyzing the obtained mutual information values was, however, not tackled until now: as the mi is a quantification between any two sites within a protein, of e.g., n amino acids, for such a molecule we need to compute and to analyze mi values. even for modest sized proteins with n ~ 100 this means to visualize 5050 real values. typically one cannot"
"the understanding of molecular evolution requires a detailed understanding of the dynamics among the constituents of a molecule during its evolution [cit] . computational biologists seek evolutionary signals in data sets of sequences of biomolecules and signatures of a correlated dynamic of the evolutionary processes shaping the characteristics of molecules under investigation. such co-evolution occurs mostly, when either an amino acid or a nucleotide within a biomolecule evolves in concert with another \"site\" within the same or a partnering molecule."
"our sorting algorithm works by finding a so-called seed row according to the maximum of the sum of contained values. this row is made the top row. then, the sorting algorithm among the remaining rows finds the one that is most similar to the seed row, where the degree of (dis)similarity is measured by the l 1 norm between the respective row vectors. the algorithm places the found row just below the seed row, makes the found row the new seed row, and iteratively continues until all rows have been processed. the same approach is then applied on the columns of the matrix. this sorting algorithm is rather simple, yet provides a useful starting point for the visual analysis. the algorithm has quadratic runtime complexity. for an overview of the application design, please see the system illustration and description provided in section results and discussion."
"other matrix-oriented data, as e.g., obtained by dna microarray experiments, can be visually analyzed with the tool, too. external knowledge can be incorporated by the weight matrix to augment the insight one gains from the expression levels detected at the feature sites. potential scenarios include phylogenetic likelihoods for particular hits on reporters, gauging bias to cope with potential shortcomings in the production and/or binding processes."
"mi ij (ache) (held s, hoffgaard f, hamacher k: biophysical annotation of molecular coevolution of acetylcholinesterase, submitted). figure 2 illustrates this example application of the system and the steps undertaken to identify an important subset of highly co-evolving residues. figure 2(a) shows the input mutual information matrix. a sorted version of the matrix is shown in figure 2(b) . the user can detect an interesting cluster of residues in an area that is extracted by filtering for high z scores and high mi values as illustrated in figure 2(c) . we show how the software can be used to restrict the display to the supposedly important ranges of mi and z scores. in part (d) of the same figure we show the residues marked in the molecular structure. these are (for the sequence of t. californica) y70, v236, n280, f284, f288, g335, s345, v360, q374. most of these residues belong to the peripheral anionic site (pas) site of ache, which is important for establishing contact with the substrate [cit] . also, various ache inhibitors bind to this site, suggesting a partial explanation why evolutionary signals occur at these spots. the involved residues are subject to a co-evolutionary pressure, the origin of which one can now start to investigate based on the insight we gained from the application of mimatrixviz to the ache sequence data."
"the application allows the user to export matrix images in the lossless png file format, and to export selected data subsets as plain ascii files."
"despite the plethora of existing approaches for argumentation mining, only few of them tackle the issue of mining argumentative structures from political debates [cit] . however, to the best of our knowledge, none of them take on the design of a tool like disputool where the identification of premises and claims is addressed together with named entity recognition (ner), supporting intelligent data exploration on political debate textual transcripts."
"matrix visualization as a technique is well-known and to date has found its way into software systems such as r [cit] or matlab [cit] . however, many implementations are focused on producing static images, offering only limited support for interactive parameter change and navigation in the matrix display by the user. we therefore developed a fully interactive matrix visualization system in java. it allows the user to change important parameters and navigate the data on the fly by means of a twostage zooming mechanism. furthermore, we support the joint visualization of two matrices, supporting our specific analysis problem."
"in this section, we first highlight the main facilities of dis-putool, and second, we describe the dataset we annotated to train our argumentative component classifiers, the experimental setting we set up for training the system, and the results we obtained."
"digital humanities (dh) is the research field concerned with the application of computational methods to traditional humanities disciplines such as literature, history, and philosophy. this field is receiving a growing attention from the artificial intelligence (ai) community, as many recent advances in ai can be thought to ease the work of humanities scholars."
"a natural example is represented by political speeches and debates, where huge amount of textual data has to be analyzed to set up or verify the hypothesis of historians and social scientists. political debates, in particular, are public interviews where the candidates of political elections are requested to confront each other about topics like unemployment, taxes, and foreign policy. they are particularly important during the presidential elections in the us, where it is customary for the two candidates of the two largest parties, i.e., the democratic party and the republican party, to engage in a debate around the most controversial issues of the time. even though these debates are not constitutionally mandated, they are considered as a de facto election process. * shohreh haddadan hereby acknowledges that this research is supported by the luxembourg national research fund (fnr) given the huge amount of data available in each election campaign, the work of historians and social scientists on this data requires a big effort. natural language processing (nlp) methods and, in particular, argument(ation) mining methods [cit] b; [cit] can be successfully employed to automatically process this data and assist humanities scholars in data exploration and argumentative analysis. in this paper, we present dis-putool, a tool conceived to support humanities scholars in the exploration and evaluation of textual political debates."
"typically, the visual analytics approach does not guarantee to reveal all relevant features of empirical data. however, in a generic biological application one does not know beforehand what signals to look for. this renders automatic processing ineffective and one has to resort to visual and interactive inspection. future work includes extending the functionality of the visualization software with additional functionality. first, additional matrix sorting algorithms with user-settable sorting criteria should be included, allowing the user to take suitable views on the data set. the matrix display should be extended by side views showing the similarity of rows and columns as well as the reordering (confusion) index in case the matrix has been sorted. in the long run, the sys-tem should be integrated with additional relevant meta data, and linked with additional viewing components such as 3d molecular viewers. our software is provided as java bytecode. the sourcecode can be made available upon request. we are also open for collaboration aiming at improving the functionality of the software and applying it to new use cases."
"to this end micato calculates in a first step the sequence entropies of each column of the sequence alignment and stores it. then the joint entropy of each pairing of two columns is calculated and by equation 1 the mi is calculated and stored in a matrix mi ij for a pair of positions (i, j). then micato runs a user defined number of independent column shuffles to generate a statistically significant number of instances of the null model (see section normalization & weighting for details). the mi matrix is exported as a csv file, as well as the z-scores of those mi values with respect to the statistics of the null model. the csv format can be read by the mimatrix-viz program without further conversion."
"the user can generate graphics and filtered data sets with the package in publication ready quality. to this end, figure 2 matrix sorting & filtering for ache. a workflow using the software package decomposed into sequential steps a)-d). the sorting and filtering algorithms are the important steps to extract evolutionary signals. (a) shows the mutual information matrix in its generic order; (b) shows the matrix sorted with our quadratic sorting algorithm. rows and columns are reordered, so that similar ones are next to each other; (c) shows a screenshot of the filtered and sorted data, revealing an interesting portion of residues in the molecule (small box), which are known to form the peripheral anionic site. filtering was done by setting ranges to be displayed to appropriate values (in the two histograms on the left); (d) the residues found in step (c) to be significantly and highly co-evolving."
"most current approaches extract features across the entire face and concatenate them for au detection. within local regions, however, many of these features are correlated. we define local regions as patches centered around facial landmarks. by modeling features within local patches informed by facs, it is possible to give greater weights to informative regions of interest and to reduce a large number of correlated features to achieve efficient learning. [cit] effectively applied patch learning to detect prototypic expressions (e.g., happy or sad). we apply patch learning to the more demanding problem of au detection."
"the developed simulation framework treats phev fleet charging as a series of stochastic events that occur in a chronological sequence according to a first-arrived, first-served policy. it treats the power grid as a collection of outlets. if the power demand approaches grid constraints, arriving phevs will be placed in a virtual queue, as shown in fig. 1 . the vehicles in the queue will either receive service if the overall load demand drops below grid constraints or eventually give up the attempt to receive a charge and leave the queue if the wait time is too long. the simulation framework yields the mean wait time, percentages of charged and uncharged phevs, and resulting load profiles. the concepts, inputs, workflows, and outputs of the system are described below."
"multi-label learning: existing research suggest the existence of strong au correlations [cit] . for instance, aus 6 and 12 are known co-occur in expressions of enjoyment and embarrassment. we can use such au correlations to improve au detection (e.g., [cit] ). to this end, bayesian networks (bn) [cit] and dynamic bn [cit] have been used to exploit au correlations. other approaches exist, as well. using generic domain knowledge, au correlations can be modeled as a directional graph without training data [cit] . in addition, a sparse multi-task model can be employed, assuming tasks are similar [cit] . without further research, it is unclear how these methods can best identify a discriminative subset of patches to improve au detection. we propose a joint patch and multilabel learning (jpml) framework that simultaneously addresses patch-and multi-label learning for au detection. these tasks prove mutually beneficial."
"positive correlation (1,2), (6,7), (6,10), (7, 10), (6, 12), (7, 12), (10, 12), (17,24) negative competition (1, 6), (1, 7), (2, 6), (2, 7), (10, 17), (10, 23), (10, 24), (12, 15), (12, 17), (12, 23), (12, 24), (15, 23), (15, 24), (23, 24) separately and thus do not compete against each other. in addition, one can observe that the absolute values of lower matrix are much larger than the upper ones, providing another evidence that out of thousands of au combinations, most rarely co-occur, coinciding with [cit] . incorporate au relations into jpml: denote the set of au pairs with positive correlations and with negative competitions as p and n, respectively. for instance, (1,2) and (6, 12) are in p; (15, 23), (15, 24), and (23, 24) are in n . to incorporate the au relations discovered above, we introduce the relational regularizer as:"
"similarly, just as features within patches have constraints, or correlation, aus have constraints as well. au 1 (inner-brow raise) increases the likelihood of au 2 (outerbrow raise) and decreases the likelihood of au 6 (cheek raiser). multi-label learning builds upon this knowledge. learning related aus simultaneously improves learning in part by implicitly increasing the sample size for each au. recent efforts have explored au relationships using bayesian networks (bn) [cit] and dynamic bayesian networks (dbn) [cit] . some developed generic domain knowledge to learn au models without training data [cit] ."
"comparative methods: to investigate the benefits of jpml, we compared it with methods that omit patch-and multi-label learning and with approaches that use patch-or multi-label learning but not an integration of both. for baseline without pl or ml, we trained linear svms (lsvm) [cit] on individual au. as a baseline for feature learning, we used l1-regularized logistic regression (ll1) [cit] . all use features without considering patches."
"the proportion of the total studied vehicles that are charged at the end of the simulation is calculated as (20) 2) percentage of uncharged phevs: the proportion of phevs that give up due to the shortage of available power is calculated as (21) 3) mean wait time: the mean wait time is the time that phevs wait in the queue before either receiving a charge or giving up, which is calculated as in (22) . a shorter wait time indicates a more promising response of the power system to the charging demand. (22) 4) charging load profile: at each time point, the amount of power delivered to the phevs depends on the number of busy outlets and the charging level. therefore (23) and the charging demand can be calculated as (24) 5) total load profile: the total load profile is calculated as (25) 6) idle capacity: the capacity of the power grid that is not used, in kilowatt hours, is calculated as (26) 7) unmet energy: the amount of energy that the power grid could not provide to charge those phevs that gave up is calculated as (27)"
"the amount of time that it would take for a phev to receive a full charge depends on: 1) its battery capacity; 2) the soc of its battery upon arrival; and 3) the charging level. assuming that the battery capacity of the vehicles is designed based on their all-electric range, one can write (5) fig. 4 [cit] nhts data. the most probable number of miles driven daily appears to be 30 [cit] . assuming that all vehicles have a 30-mile all-electric range and their battery capacity ( ) is as noted in table ii, one can obtain the probability distribution for the energy demand (see fig. 5 ). [cit] nhts data [cit] . the energy required to fully charge vehicle, and the ercp and charging duration time (in minutes) of vehicle are expressed as (6) the time duration of the charging process is (7)"
"the four inputs of the simulation framework are defined as follows: 1) the number of available outlets, which represents the idle capacity of the power grid; 2) the vehicle arrival time; 3) the charging duration time, which is the amount of time required to fully charge a vehicle; and 4) tolerance duration, which indicates how long a vehicle would wait before giving up. fig. 2 depicts the block diagram in which the four inputs are determined."
"(2) gft [cit] consists of 720 participants recorded during group-formation tasks. previously unacquainted participants sat together in groups of 3 at a round table for 30 minutes while getting to know each other. we used 2 minutes of video from 50 participants. for each participant, we randomly sampled 100 positive frames and 200 negative frames for training purposes."
"the study described here develops a discrete-event simulation framework for examining the power system's potential to meet the load demand of phevs. real-world data available from the national household transportation survey (nhts) [cit] are used to develop probability density functions (pdfs) for a vehicle's arrival time and required charge. nhts includes a large amount of statistical transportation data collected from all over the u.s. [cit] . the present work can be distinguished from the above-mentioned studies in three ways. first, statistical transportation data are used to determine not only a vehicle's plugging-in time but also the energy required to reach a full charge. second, more details for calculating the state of charge (soc) of the vehicles by considering the vehicle type and miles driven are provided. third, this simulation framework first sets the power system constraints and then allows phevs to be charged as long as the constraints are not violated. this approach yields different metrics. for example, previous studies have determined how much the operating temperature of the distribution transformer would rise [cit] . however, this study determines what percentage of vehicles could be served, as well as the wait time, given the existing capacity of the power system."
"we address patch and multi-label learning with one stone. by taking both pl and ml into account, we model dependencies among both features and aus. we explore two types of au relations, termed positive correlation and negative competition, by statistically analyzing more than 350,000 samples from three varied datasets that include both posed and spontaneous facial behavior. the latter in-cludes two-and three-person social contexts and a range of emotion inductions. given such au relations, we develop joint patch and multi-label learning (jpml) to simultaneously select a discriminative subset of patches and learn multi-au classifiers. jpml leverages the structure in the classification matrix and au labels, and naturally blends two tasks into one. fig. 1 illustrates the main idea. (a) shows a classification matrix in which columns correspond to patch indices and rows to individual au classifiers; (b) shows likely and unlikely co-occurring aus; (c) shows patch indices. (d) illustrates the patches selected by jpml, illustrating that jpml is able to finding a discriminative subset of patches to identify a target au, in this case au12 (oblique lip corner puller). in experiments, we will show that the joint processes of jpml are mutually-beneficial due to the complementary characteristics in the classification matrix."
"tables 3∼5 show the results on ck+, gft, and bp4d, respectively. aus without relationships are underlined. we excluded these aus for ml and jpml and denoted theirs results as \"−\". for ck+, because each video starts from a neutral face to a particular peak expression, we evaluated with only f1-norm. for gft and bp4d consisting of spontaneous videos, we used both f1-norm and f1-event to capture the imbalance nature of au detection and the abil- one explanation is that our apl uses patches around facial landmarks, and thus better adapts to appearance changes on spontaneous expressions. in particular, as can be seen in tables 3∼5, apl performs more effectively when applied to lower face aus, which typically involves larger motions on mouth regions. in summary, we justify that apl is more reasonable than standard feature learning and patch learning with fixed patches."
"this paper proposes a joint patch and multi-label learning (jpml) for facial au detection. active patches for each au are selected more specificity by group sparsity learning. jointly with patch learning, positive correlations and negative competitions among aus are introduced to model a discriminative multi-label classifier. compared with patch learning based and multi-label learning based algorithms separately, jpml obtained the best predictions across three datasets. according to the conclusion of results in experiments, imbalance data learning and video-based learning algorithm should be studied in the future work."
"the simulation runs for 27.7 million vehicles (the region's total number of conventional vehicles [cit] ), and the grid capacity limitation is assumed to be 90 000 mw. figs. 9 and 10 show the phev charging load profiles with 1.4-and 7.68-kw charging levels, respectively. the dotted curves represent the case when there is no limitation, that is, all phevs are plugged in (begin charging) upon arrival regardless of the grid's capacity. the solid lines represent the case when the limitations of the grid are applied. these figures indicate the extent of load profile deformation due to grid limitations at 100% penetration. according to the figures, a higher charging level causes larger deformation. figs. 11 and 12 show the resulting total load profiles when the simulation runs at 33%, 66%, and 100% phev penetration rates with 1.4-and 7.68-kw charging levels, respectively. in these two figures, the phev charging load profile has been added to the domestic load profile to obtain the total load profile. in case of 33% penetration, the total load profile is not fig. 9 . phev charging load profiles at a charging level of 1.4 kw and 100% penetration for ecar. fig. 10 . phev charging load profiles at a charging level of 7.68 kw at 100% penetration for ecar. cut off. this indicates that all vehicles can receive charge. however, in case of 66% penetration, there is a cut off but not as wide as the 100% penetration case. similar to deformations, the cutoffs at the 7.68-kw charging level are wider than those at the 1.4-kw level. table iii compares the quantitative values for 1.4-and 7.68-kw charging levels, indicating that the existing power system can meet the charging demand of at least 76% of phevs. however, phevs must wait an average of 30-45 min before receiving any charge. this table also shows that the results at a 1.4-kw charging level are better than those of a 7.68-kw charging level. at the lower charging level, a smaller percentage of vehicles will be denied service; also, the wait time is shorter. in addition, the unmet energy is half of that of the higher charging level. according to table iii and (26), after adding the phev charging load to the domestic load, still the grid has a large idle capacity compared with the unmet energy in both charging levels. therefore, applying a charging policy (controlled charging) in this region will be very effective in minimizing or even eliminating the unmet energy."
"there are several challenges associated with phev fleet studies. information about the number of vehicles, the size of their energy storage devices [cit], their arrival time, and their daily distance driven must be available. also, regarding the grid, one needs to know the domestic load profile, as well as the available number of charging stations and their charging level. considering the scale of the system, finding or collecting reliable data often is difficult. uncertainty arising from the availability and reliability of real-world data has led researchers to evaluate the impact of phevs using probabilistic approaches [cit] ."
"t he transportation sector is one of the major contributors to greenhouse gas emissions, urban air pollution, high energy prices, and the rapid depletion of fossil fuel resources. concerns over the adverse impacts of conventional vehicles necessitate a cleaner and more efficient vehicle technology. plug-in hybrid electric vehicles (phevs) are the most promising approach to sustainable transportation [cit] ."
"upon arriving at the simulation framework, each phev is plugged in (begins charging) if there is an idle outlet; otherwise, it must wait in the queue. once an outlet becomes available, the first vehicle in the queue starts charging. if the wait time for a vehicle exceeds its tolerance duration, it gives up and leaves the queue. therefore, when charging is complete or the tolerance duration is exhausted, the phev leaves the system. consequently, three events are possible in this simulation framework: the arrival event (ar), the charging-complete event (cc), and the giving-up event (gu)."
"these patch learning approaches have been proved effective on posed expressions. however, the patch locations are pre-defined on a normalized template, and hence could fail to precisely capture the specificity of patches due to nonrigidity of human faces. besides, it is unclear how aus relations can be incorporated in these studies."
") is the logistic loss, ω(w) is the patch regularizer that enforces sparse rows of w as groups, and ψ(w, x) is a relational regularizer that constrains predictions on x with au relations. tuning parameters are α for ω(·) and (β 1, β 2 ) included in ψ(·, ·). problem (1) involves two tasks: identify a discriminative subset of patches for each au (patch learning), and incorporate au relations into model learning (multi-label learning). below we detail each task in turn."
several studies have evaluated the impact of phevs on the power grid from different angles. most examine the capacity of the power grid to handle an extra load by comparing the relative change in the utility load profile with and without phev loads [cit] . studies using the valley-filling approach have claimed that 50%-73% of the american light duty fleet could be electrified using idle generation capacity. the potential of the power grid to charge phevs has been defined as the difference between the highest demand during the off-peak period and the system's maximum generation capacity [cit] . other studies have proposed optimization models for minimizing system costs and maximizing profit [cit] .
"2) vehicle arriving time: vehicles arrive independently. therefore, the number of vehicles arriving at time is considered to follow a poisson distribution. consequently, the time between each pair of consecutive vehicle arrivals, i.e., inter-arrival time, has an exponential distribution with parameter [cit] . given a random value drawn from the uniform distribution on the unit interval (0, 1), one can generate the inter-arrival time as (3) where is the arrival rate. [cit] nhts data are used to obtain the arrival rate. fig. 3 suggests that this rate strongly depends on the time of day [cit] . in this study, the 24-h cycle is divided into 96 equal intervals. the arrival rate during each 15-min time interval is considered to be constant. it is obtained from fig. 3 scaled by the total number of vehicles in the region. the number of phevs in a specific region can be estimated based on the number of conventional cars in that region multiplied by the phev penetration rate. according to (3), the inter-arrival time between the arrival of vehicles and is (4) where represents the value of the arrival rate at the arriving time of vehicle ."
"the simulation framework can be applied to any region, provided that information is available regarding the power grid's capacity, the domestic load profile, and the number of conventional vehicles. here, we apply the proposed simulation framework to two north american electric reliability corporation (nerc) regions. the summer load is higher than the winter load in both regions; the average summer domestic load profile serves as the basis of the study. the simulation framework runs for three days with an accuracy of 1 min, and the results are shown for the second day. the simulation assumes that most drivers can tolerate, on average, a 1-h delay; hence, the tolerance duration follows a normal distribution with a mean of 1 h and a standard deviation of 15 min or . the charging levels used in this study are 1.4 and 7.68 kw. also, the grid capacities are assumed to be 5% over the maximum domestic loads."
"because ω(w) and ψ(w, x) constrain on w differently, problem (1) cannot be solved directly. we rewrite problem (1) by introducing auxiliary variables w 1, w 2 .,"
"the first task addresses patch learning. according to facs [cit], aus are defined according to appearance changes at particular facial regions. unlike standard feature learning methods that treats features separately [cit], patch learning constrains local dependencies in facial patches and gains better interpretation. existing work select patches on uniformly distributed grid [cit], while this paper exploits landmark patches that are centered at facial landmarks (as depicted in fig. 1(c) ). these landmark patches adapt better in real-world facial expression recognition scenario because of the non-rigidity of faces. in particular, we describe each patch using a 128-d sift descriptor. each facial image is then represented as a 6272-d feature vector by concatenating sift descriptors of all landmarks."
"discover au relations: we seek au relations by statistically analyzing three datasets, ck+ [cit], gft [cit] and bp4d [cit], which contains 214 subjects and more than 350,000 valid frames with au labels. the most frequently occurring aus are used throughout this paper. here, our goal is to discover likely and rarely co-occurring aus. fig. 4 shows the relation matrix studied on the datasets. the (i, j)-th entry of the upper right matrix was computed as the coefficient correlation between the i-th and the j-th au using ground truth labels; an entry of the lower left matrix was computed on the labels containing at least either the i-th or the j-th au. one could interpret the upper matrix in fig. 4 as a mutual relation of concurring au pairs, and the lower matrix as an exclusive relation that one au competes against another. after investigating this matrix with the facs [cit] and related studies [cit], we derive two types of au relations, positive correlation and negative competition, as summarized in table 1 ."
"although the penetration of phevs is not yet significant, their impact on the power grid already has been the subject of many research studies. the random nature of the charging behavior of a large number of phevs would introduce uncertainty into a power system's operation, particularly in cases of high penetration [cit] . given the opportunity, most phev owners will likely plug in their vehicles during afternoon hours [cit] . consequently, the electricity demand during those hours will increase, putting a significant load on the power grid. in an effort to quantify this scenario, it is necessary to examine the power grid from the following two perspectives: the degree to which the peak charge demand of the phev fleet overlaps the peak time of the domestic load profile and the degree to which the peak domestic load profile will exacerbate."
"(3) bp4d [cit] contains 2d/3d videos of spontaneous facial expressions in young adults during various emotion inductions while interacting with an experimenter. we used 328 2d videos from 41 participants. for each video, we randomly sampled 50 positive frames and 100 negative frames for training purpose."
"results for the two nerc regions studied by the simulation framework indicate that the power system's ability to supply the charging demand is different for each region. this ability strongly depends on the region's population and grid capacity. hence, the power grid of the first region meets 76% of the charging demand, while that of the second region supplies only 28% of the phevs' demand. therefore, in some areas, the existing power system is better prepared to meet the charging demand even in the case of high penetration levels. however, in other areas, it is necessary to extend the existing power system or to apply strict policies even in the case of low penetration levels."
"patch importance: to validate the ability of maintaining the specificity of patches, we compare standard feature learning 2 (treat each feature independently) and our patch 2 ℓ 1 -regularized linear svm [cit] was used as feature learning. learning (treat features as groups), using the defined patch importance w p ℓ 2 . as shown in fig. 2, patch learning offers a better interpretation of important patches corresponding to three au examples. for instance, patches around inner eyebrow contain higher importance for au1; for au24, patches around mouth (especially upper lips) are shown more important. moreover, compared to previous work that manually defines a fixed region for au12 (e.g., [cit] ), our patch learning for au12 automatically emphasizes not only upper lips (not lower lips), but also the patches around lower nose and slightly minor importance on the lower eyelid (corresponding to au6). it can be seen that patch learning facilitates the specificity of relevant facial patches. similar results could be obtained on other aus and basic emotions."
"multi-label learning: this paragraph discusses the benefits of considering relations between au labels using multi-label learning. closest to our work is mt-mkl that assumes classifiers within the same au group behave similarly. on the contrary, our ml (sec. 3.3) considers positive correlation as well as negative competition on labels (instead of classifiers), and thus more naturally fits the problem in hand. in table 3, averaging f1-norm over the 6 aus we implemented for mt-mkl, ml outperforms against mt-mkl by 8.8%. in tables 4 and 5, we have seen that ml consistently outperforms standard binary classifiers (ll1, lsvm, spsd and sp-svm), showing that relations between au labels are essential to assist au detection."
"examples include the number of phevs in the system (either under charge or in the queue), the number of idle outlets, and a phev's status."
"is the domestic load at time, and is the charging level (all three in kw). also, one can write (2) where and are the numbers of idle outlets and busy outlets, respectively, at time ."
"the number of vehicles giving up is one of the power grid's performance evaluation parameters. in the most pessimistic case, the tolerance duration is zero, and utility customers expect the charging service to be available upon request. in the most optimistic case, the tolerance duration is unlimited, and users are flexible enough to utilize the charging service even a few hours later. however, this simulation framework considers a normal distribution with a mean of and a variance of as (8)"
"3) giving-up event: when the fel determines that a giving-up event will occur at time but the phev is not plugged in by that time, the queue length will decrease by one. in other words, the vehicle will leave the queue and give up. the state variables of vehicle at time are the waiting, plugged-in, charging-complete, or giving-up states. they vary by each event occurrence, therefore otherwise (15) otherwise (16) otherwise (17) otherwise (18) each vehicle at each moment must have only one state, so (19)"
"a simulation framework has been proposed for examining the power system's ability to meet the charging demand of phevs. it sets the power system's limitation so that the total load profile never surpasses the power grid's limits during the simulation. it simulates the customers' charging request behaviors and evaluates the response of the power system to the charging demands through the output load profiles and values. output parameters, which consist of both quantitative and qualitative values, indicate how readily and efficiently the power system supplies the extra demands due to the phev charging load from the customers' perspectives."
"as shown in fig. 6, when vehicle arrives at time, if at least one outlet is available, its charging-start time and charging-complete time are scheduled in the fel as (9) (10) otherwise, the queue length increases by one, and the giving-up event is scheduled in the fel as (11) also, the next vehicle's arriving time is programmed as (12) when (13) and the number of phevs having arrived before time is updated as (14) 2) charging-complete event: when a charging-complete event occurs, the charging-start time and charging-complete time of any vehicles waiting in the queue are scheduled in the fel based on (9) and (10) . otherwise, the number of idle outlets increases by 1, as shown in fig. 7 ."
automatic facial au detection has been a vital research domain for objectively describing facial action related to emotion. see [cit] for comprehensive reviews. our work closely follows recent efforts in patch learning and multi-label learning. below we review each in turn.
"datasets: we evaluated the effectiveness of jpml in three datasets that include both posed and spontaneous facial behavior in varied contexts. each database had been facs coded by well-experienced coders. inter-observer agreement in each was quantified using coefficient kappa, which control for chance agreement between coders, and it was maintained at a kappa of 0.80 or higher, which represents high inter-observer agreement."
"to address the regional appearance changes on aus, we define a group-wise sparsity on the classification matrix w. group sparsity learning aims to split variables into groups and then to select groups in sparsity. it has been shown to effectively recover joint sparsity across input dimensions, and successfully applied to computer vision (e.g., [cit] ). given the structural nature of our problem, within each column of w, we split every 128 values into non-overlapping groups, where each group corresponds to the sift features extracted from a particular patch. this grouping encourages a sparse selection of patches by jointly setting a group of rows to zero. in particular, problem (1) reduces to:"
"as shown in figs. 6-8, during the simulation and at the end of each event, related statistics are collected, and the following outputs are generated. the results are for 24 h, and the time step for the following equations is 1 min."
"jpml: apl and ml alone have shown good performance over three datasets. this paragraph focuses on the discussion of jpml that jointly considers patch selection and au relations. in all, jpml achieves the best or second best for 22/27 aus in f1-norm and for 12/18 aus for f1-event. in table 3, jpml performs the best for aus (1, 2, 12, 15), and improves about 1.3% and 5.0% than apl and ml respectively for f1-norm. it improves more than 7.3% and 7.8% for f1-norm, and 13% and 67% for f1-event than apl and ml respectively. in tables 4 and 5, as more spontaneous expression are involved, the improvement becomes more obvious. since the ratio of training and test samples in bp4d is a little small in this paper and samples in bp4d is much more complex than gft, the results in table 5 is smaller than ones in table 4 in average. in all, jpml method achieved the highest overall scores in five comparisons on three datasets. in bp4d, apl is slightly higher than jpml. in no cases, the other approaches match or exceed apl and jpml. this suggests that our patchbased approach is more powerful, and further boost the performance with additional ml. in addition, there are some interesting observations in our results. jpml yields better improvement in aus with larger skew (e.g., au1 and au2 in gft and bp4d), as shown in table 2 . to summarize, jpml validates the effectiveness of jointly learning the patches and au relations, showing that iterating the ml and the apl process is beneficial."
"the remainder of this paper is organized as follows. section ii introduces the simulation framework of phev charging events and presents its concepts, inputs, workflows, and outputs. it also extracts the pdfs, which are the main inputs to the simulation framework. then, section iii applies this simulation framework to two regions in the u.s. as case studies and summarizes the results. finally, section iv offers concluding remarks."
"the next task is to exploit label relations for au detection. learning multiple related labels effectively increases the sample size for each class, and improves the prediction performance (e.g., [cit] ). despite the au relations derived from prior knowledge [cit], this section explores statistically the au co-occurrence among more than 350,000 frames. below we describe how we discover these relations, and how they can be incorporated into jpml."
"in order to simulate a more realistic customer behavior and include the customers' constraints in the simulation framework, the tolerance duration is considered as one of the inputs to the simulation framework. this input expresses how long the driver is willing to wait before the charging process begins. the tolerance duration also reflects the constraints of the power grid, as a phev is allowed to receive a charge as long as the total load does not violate the maximum electricity generation."
"distribution of privacy leakage. we use the formula described in section 4.1 to quantify the privacy leakage as shown in fig. 8 . these results demonstrate one of the major advantages of divert: it reduces the privacy leakage by up to 92 percent for both debksp and dar ã . this is due to the fact that privacy-aware reporting avoids submitting location reports from highly sensitive low density roads and submits reports with low per-vehicle frequency in high density roads. therefore, drivers are less prone to be identified by the untrusted server and their location reports are difficult to be linked to each other; thus, driver location privacy is protected. on average, dar ã has less privacy leakage than debksp because it has lower travel time: in dar ã, each vehicle finishes the journey faster, and thus there are fewer location reports."
"the real driver in the dd can access the hardware but interrupts are handled by the hypervisor. the communication between the dd and the hypervisor, and between the dd and a domu are based on event channels (ec1 and ec2 in fig. 1 ). an i/o request issued by a guest os (domu) creates an event on ec1. the data are transmitted to the be in the dd through a ring buffer (based on memory pages shared between the domu and the dd). the be is then responsible for invoking the real driver."
"let us notice that only debksp can take advantage of larger k values, while the centralized version cannot. a larger k allows for better traffic balancing but introduces higher computational complexity since the centralized server needs to compute k paths for all the selected vehicles. this is not a problem for debksp which distributes the path computation to individual vehicles. therefore, debksp can result in higher performance than ebksp when higher k values are used."
"case 1: for the road segments without any traffic reports, the server estimates the travel time to be the free flow travel time. note that this travel time is an approximate value for some roads, as vehicles only report when the vehicle density is above the threshold density."
"the contributions described in this paper have been implemented in the xen system (the most popular open source virtualization system, used by amazon ec2). this section presents an overview of virtualization technologies in xen. this presentation only covers virtualization aspects which are relevant to our study: i/o virtualization, and cpu allocation and accounting."
"we have presented a method for guiding a phrasebased decoder with translation tables partitioned on the basis of k-best list reranking making use of complex features. our results showed consistent improvements in bleu score over a strong rerank baseline using the same features. we experimented with a simple criterion for iteratively partitioning the original phrase table of the system, and found that focusing on providing the next iteration decoder with the bi-phrases that were prefered at first rank by rerank (in) performed best. 6 we now intend to study how to better take advantage of the expected characteristics of our in and out tables, possibly by adding more features to our iteration-specific tables, or by exploiting information on bi-phrases computed on the full reranked lists. for our future work, we also plan to study approaches that can enhance the diversity in the k-best lists [cit] between each iteration of the multi-pass decoding to train a better rerank after each decoding pass. another area for improvement lies in the addition of yet more complex features, for instance to allow a better dis-course coherence modelling over iterations [cit] . going further, we could study the effect of using other hypotheses instead of the rerank one-best to perform the comparison with the moses one-best hypothesis. for instance, we can reasonably expect that making this comparison with the output of a rewriting system, such as the one proposed in our previous work [cit], could extract more misused and useful bi-phrases on which to base our translation table partitioning since this rewriting system's output is usually better than the rerank one-best and not in the k-best list of the decoder."
"the fact that, in today's schedulers, the cpu time consumed by system components is not charged to vms can be problematic for cloud clients (performance predictability) and cloud provider (resource and money waste). if the dd's computing capacity is limited, the performance isolation is compromized as vm performance can be influenced by other vms which shared the dd resources. otherwise, if the dd's computing capacity is unlimited, the resource management (and especially vm consolidation) is affected. these two cases are considered in the following subsections. performance unpredictability: we consider here that the dd's computing capacity is limited. in a cloud, when a client books an amount of resource for his vm, he expects a predictible and reproductible performance level for his applications. as we have seen, the aggregated amount of resources effectively used by a vm depends on some shared components (the dd and the hypervisor) of the virtualization system. knowing that the amount of resource available for the dd and the hypervisor is shared among tenant vms, the client application performance is unpredictable. resource and money waste: giving an infinite resource to the dd without accounting and charging its processing time to client vms is like giving a blank cheque to clients. this is a shortfall in terms of money for the provider. in addition, without accounting t 1 + t 2 + t 3, the scheduler accelerates the vm de-consolidation (moving a vm from an overloaded pm to an underloaded pm) rate, which results in the use of more resources than needed."
"these requirements suggest a distributed system architecture. however, a fully decentralized architecture is not suitable for a proactive re-routing system. for example, by creating vehicular ad hoc networks (vanets), the vehicles can exchange information using multi-hop communication, and thus can detect signs of congestion in small regions while preserving their privacy. however, vanets do not permit vehicles to get an accurate global traffic view of the road network, resulting in wrong or at least sub-optimal rerouting decisions. in addition, in a fully distributed architecture, due to the lack of a coordinator, the vehicles cannot take synchronized actions at the same time, which makes it infeasible to make collaborative decisions in real-time."
"the above problem can be solved by dynamic traffic assignment (dta) which assigns each driver either systemoptimal or user-optimal routes [cit] . however, dta algorithms may not be able to compute the equilibrium fast enough to inform the vehicles about their new routes in time to avoid congestion. divert, on the other hand, is designed to be effective and fast, although not optimal, in deciding which vehicles should be re-routed when signs of congestion occur as well as computing alternative routes for these vehicles."
"packet size: a disk request size is upper bounded by the memory page size (e.g. 4kb). a request which arrives at the backend is fragmented into several bio data structures, the processing unit in the linux kernel. any bio is of the same size, configured at compilation time. thus, our calibration is performed at the bio granularity. therefore, we evaluate on the one hand the time used to build a bio (noted t bio compute ) from a disk request. on the other hand, we evaluate the time needed to submit a bio (noted t bio handling ). in order to avoid disk speed effects (shown in section iii-a), our calibration takes place when the bio is placed within the real driver buffer. fig. 4 shows the calibration results for our experimental environment."
"the main objective of our simulation-based evaluation is to study the performance of the distributed re-routing strategies in divert. specifically, the evaluation has four goals:"
"computation cost. in divert, the bottleneck at the server of computing all the alternative paths is removed. if the computation time is high in the centralized system, the drivers would be informed too late about alternative paths, thus defeating the purpose of the system. in divert, each car performs its own path computation using information received from neighboring vehicles. therefore, this system is expected to have higher scalability. to prove this hypothesis, experiments are performed on a smart phone (the expected computing platform in the vehicles) and the obtained results are plugged into analytical formulas for the two distributed strategies."
"re-routings frequency per hour. from the system point of view, having a low number of re-routings means decreasing user distraction. fig. 9 shows the cdf of re-routing frequency per hour in the distributed strategies compared to their centralized counterparts. ebksp and ar ã have a relatively low number of re-routings due to the global knowledge of all vehicles' paths. nevertheless, the distributed strategies only result in 4.5 and 0.4 percent more reroutings, respectively. thus, divert proves to be practical from this perspective as well."
"the i/o request type: disk operations are read and write. unlike the network, they are always initiated by the vm. according to the linux and the backend source code, their processing follows the same path and they require the same processing time. therefore, our implementation does not distinguish the disk operation type."
"a potential problem of the above algorithm is that it does not consider the case in which the vehicle transmission range is shorter than the road segment length. first of all, this is a very rare case since the typical transmission range is 500 meters, which can cover most urban road segments (e.g., the average road segment length is less than 300 meters for the two real road networks used in our experiments). second, if indeed a segment is longer than the transmission range, the reported density is inaccurate especially when the traffic density along the segment is extremely skewed (e.g., congestion starts to form at one end of the segment, while the other end still has light traffic). in this case, our algorithm overestimates the density and may trigger re-routing. however, this is not necessarily an issue because if nothing is done, the congestion is expected to increase anyway when the incoming traffic on the segment is heavy."
"as its description suggests, our solution introduces a negligible overhead (near zero). it could have been otherwise if for example we had introduced a new hypercall for informing the hypervisor level (about the 2 the cost of not calibrated packet sizes is obtained by interpolation."
"we learn three lessons from the results: (1) each vehicle only needs to know the trip information of its neighboring vehicles to make re-routing effective. global information about all the vehicle routes brings minimal benefits. (2) the re-routing with privacy-aware reporting is less effective because the server may misestimate the travel time on certain road segments. however, the benefits of providing higher privacy overcome this generally acceptable performance loss. (3) dar ã achieves better performance than debksp. this is due to the tiny packet size of dar ã, which leads to less contention and thus fewer packet losses. however, debksp is more robust than dar ã to privacy-aware reporting and packet losses."
"we ran experiments on two translation tasks for different domains: the wmt'14 medical translation task (medical) and the wmt'11 news translation task (news) for the language pair fren on both directions. for both tasks we trained two strong baseline systems using data provided by wmt 4 . statistics about the training, development and testing data are presented in table 1 ."
"case 2: for the road segments with a non-zero traffic density but for which the last report time is older than a predefined time interval, the server does an expiration operation. specifically, if the difference between the last update time and the current time is greater than t times the free flow travel time of the road, then the road density is reset to zero. the value for t is also based on empirical results and is chosen to be equal to four in this system."
"the total time consumed for dar ã is the sum of the communication time among vehicles to collect information and the actual local computation time at a vehicle. since the number of received trip data is a function of t max (the communication time), the total time consumed for dar ã is"
"in this paper, we propose a solution which overcomes the problem identified in the previous section. this solution, implemented at the scheduler level (in the virtualization system), aims at satisfying the following equation, instead of equation 1:"
"second, divert should be designed to respect the privacy of the users from its conception, i.e., a privacy-bydesign system, which can be essential for the wide acceptance of the system. implicitly, by offloading the path computation to the vehicles, the drivers' exposure is reduced significantly since very sensitive location information (i.e., the od pairs) is not sent to the server anymore. nevertheless, protecting only the od of a vehicle is not sufficient. divert needs a mechanism to protect the identity of vehicles while reporting location data."
"as a token can appear more than once in an input text and in a sentence, and because complex features are computed locally, the source tokens are located: an identifier is concatenated to each token to make them unique in the source text to translate. tokens of source phrases in the translation table are also located, meaning that each bi-phrases is duplicated to cover all located tokens. this procedure allows our approach to differentiate changes between moses and rerank one-best hypotheses at the token level by taking context into ac-a means of transport safer is the subway . le@0 moyen@1 de@2 transport@3 le@4 plus@5 sûr@6 c'@7 est@8 le@9 métro@10 .@11 the safest means of transport is the subway . count. an example of in and out translation tables extraction with located tokens is presented in figure 1 ."
"for our experiments we used the moses phrasebased smt toolkit [cit] with default settings and features, including the five features from the translation table, and kb-mira tuning [cit] . rerank is trained using kb-mira on the 1,000-best list generated by moses on the development set with the 4 http://www.statmt.org/wmt14 distinct-nbest parameter to have no duplicates. testing is also performed on distinct 1,000-best lists. rerank uses all the decoder features along with the following complex features:"
"tdd 800 (the maximum cpu load of a pm is 800%). fig. 9 (b) and (d) respectively present the resource saving in univ1 (an average of about 2 pm/hour) and univ2 (an average of about 6 pm/hour). still on the basis of t dd, we can evaluate the benefits in terms of money. without our solution, t dd can be seen as what we have called the \"blank cheque\" given to clients. if we convert t dd into a number of m3.medium vm instances, the \"blank cheque\" can be evaluated by the following formula: tdd 100 (a m3.medium vm is allocated a unique processor). knowing that a m3.medium vm instance is $0.070 hourly in amazon at the time of writing, the operating loss for our simulation period is amounted to about $114 for univ1 and $249 for univ2. the extrapolation of these results in a full scale cloud (thousands of machines hosting millions of vms) would show very significant benefit. without a precise accounting of system time, the provider has to integrate these costs in the global operating budget of the data center."
"this paper has proposed a complementary solution to two relevant problems in the cloud: performance unpredictability and resource waste. we have addressed them using a new light solution. roughly, instead of investigating microarchitecture components (for performance unpredictability) or proposing yet-another consolidation algorithm, we have proposed an orthogonal solution based on an efficient charging of cpu time used by the system components to vm. the article describes our solution, including a prototype. the latter has been evaluated with various workloads (including real data center traces). it has demonstrated its ability to accurately overcome the initial problems without an overhead."
"the rest of the article is structured as follows. section ii introduces the necessary background for the paper. section iii presents the motivations for this work. section iv overviews our contributions. the implementation is depicted in section iv-c. an evaluation is reported in section v. the latter is followed by a review of related work in section vi, we present our conclusions and perspectives in section vii."
"however, despite achieving a substantial decrease in the travel time experienced by drivers, centralized solutions such as ours suffer from two intrinsic problems. first, the central server has to perform intensive computation (to re-assign vehicles to new paths) and communication with the vehicles (to send the paths and to receive location updates) in realtime. this can make centralized solutions infeasible for large regions with many vehicles. second, in a centralized architecture, the server requires the real-time locations as well as the origins and destinations of the vehicles to estimate the traffic conditions and provide effective individual re-routing guidance. this leads to major privacy concerns for the drivers and may prevent the adoption of such solutions due to \"big brother\" fears. as long as vehicles' traces are fully disclosed, user's identity can easily be inferred even if pseudonyms are used [cit] . this is due to the fact that location can contain identity information [cit] . moreover, a sequence of location samples will eventually reveal the vehicle's identity [cit] . therefore, it is important to make the system work without disclosing the users' origin-destination (od) pairs and with the least number of location updates along a user trip."
"case 3: for the road segments that do not fit in cases 1 and 2, the server uses the greenshield model [cit] to estimate the travel time according to the speed-density-volume relation. this model is used extensively by transportation researchers and was shown empirically to describe well the speed-density relation for relatively low densities. the model considers a linear relationship between the estimated road speed v i and the traffic density k i (vehicles per meter) on road segment r i :"
"recently, we proposed a rewriting system that explores in a greedy fashion the neighborhood of the one-best hypothesis found by the reranking pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis [cit] . nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings."
"the server uses the vehicle traffic reports to build an accurate and global view of the road network traffic. the network is represented as a directed graph where each edge corresponds to a road segment. in addition, each edge has associated a dynamic weight representing the real-time traffic density on the edge. a road segment is considered to exhibit signs of congestion when the traffic density is greater than a threshold value. each time new road segments exhibit congestion signs, the server sends a partial weighted graph (i.e., only the edges having a travel time different from the free flow travel time) to the cars that reported recently and are close to the congestion segments (see section 4)."
"given the described design principles, a hybrid architecture is proposed to implement divert as shown in fig. 1 . the architecture is composed of a central server and a software stack running on an on-board device (e.g., a smart phone) in each participating vehicle. divert uses two types of communication. the vehicles communicate with the server over a cellular network to report local traffic density data and to receive the global traffic density in the road network (i.e., the green arrows in fig. 1 ). the vehicles report data according to a privacy-aware algorithm that is detailed in section 4. also, the vehicles that are closely located communicate with each other over vanets to determine the local traffic density, to disseminate the traffic data received from the server, and to implement a distributed re-routing strategy (i.e., the red lines in fig. 1 ) as detailed in section 5."
number of i/o requests handled by the dd). we avoided this approach by using an existing hypercall. we run a witness vm (noted v witness ) hosting an application (ycruncher [cit] ) which is both cpu and memory bound. v witness is configured with a single vcpu pinned to the same processor as the dd (having also a single vcpu). both the dd and v witness have access to the entire processor capacity. the pm also hosts a set of client vms (called injectors) whose number varies during the experiments (to increase the traffics within the dd). each client vm runs the same web application based on wordpress. experiments were carried out in two contexts. the first context is based on native systems. it is the baseline. the second context uses our solution in which the mechanism of charging debts to vms is disabled (in order to keep injectors with the same behaviours as in the baseline context). fig. 5 shows that our solution does not introduce overhead since the v witness performance is the same in the two contexts.
"divert also reduces the network load on the server, which could become a major bottleneck as the number of vehicles increases. since the privacy enhancement protocol only allows vehicles to send traffic reports when the privacy metric meets the probabilistic criterion, the number of messages is decreased by 95 percent, as indicated in fig. 13 ."
"similarly, in the case of dar ã, the notified vehicle chooses a random rank but it does not compute the k shortest paths. instead, the notified vehicles only broadcast their od pairs. in the event of a broadcast timeout, for each received od pair in the buffer, the vehicle applies the original ar ã algorithm to compute a virtual path. the current vehicle assumes that the vehicle with that od pair will take that virtual path. by the end of the process, the current vehicle computes the best shortest path for itself based on other vehicles' paths. algorithm 4 describes how debksp and dar* are executed when the information dissemination phase ends. debksp and dar ã have opposite behaviors with regards to the two main phases of the re-routing process. debksp incurs a higher overhead in the communication phase than dar ã . the packets in debksp are significantly larger than those in dar ã : debksp packets contain the k shortest paths, while dar ã packets contain only the vehicle od pair. on the other hand, the computation phase is more efficient in debksp than in dar ã . since the paths are individually computed and disseminated in debksp, a vehicle only has to choose between the k paths for all the vehicles from which it has received re-routing data, which has a very low computation cost. in dar ã, a vehicle has to compute the shortest paths for all the vehicles, ranked higher than itself, from which it has received rerouting data. this computation difference is explained in detail in section 7."
"the article demonstrates that a practical, cost-effective, and efficient traffic re-routing system can be implemented and deployed in real-life settings. this system, divert, offloads a large part of the re-routing computation at the vehicles, and thus, the re-routing process becomes scalable in real-time. to make collaborative re-routing decisions, the vehicles exchange messages over vanets. we have optimized vanet data dissemination to allow for efficient distributed re-routing computation. in addition, the system balances user privacy with the re-routing effectiveness. the simulation results demonstrate that, compared with a centralized system, divert increases the user privacy substantially, while the re-routing effectiveness is minimally impacted."
from the above description we can see that the processing time used by the dd to operate i/o requests on behalf of a vm is not charged to the vm by the scheduler. this is the source of several problems in the cloud.
"in the xen system, the credit scheduler is the best fit for cloud platforms where a client books for an amount of computing capacity which should be guaranteed by the provider without wasting resources. therefore, our contributions only consider this scheduler. the latter works as follows. a vm v is configured at the start time with a credit c which should be ensured by the scheduler. the scheduler defines remaincredit, a scheduling variable, initialized with c. each time a v's virtual processor (vcpu) is scheduled on a processor, (1) the scheduler translates into a credit value (lets say burntcredit) the time spent by v on that processor. (2) subsequently, the scheduler computes a new value for remaincredit by subtracting burntcredit from it."
"impact of vanet optimizations. during our implementation and testing phase, we noticed that the prioritized broadcast and the distance-based timer approach are essential in our system because without them very little re-routing information is exchanged among the vehicles due to contention in congested regions. among the remaining two optimizations, i.e., k path compression and xor coding, the path compression brings the most benefits for the debksp strategy. fig. 10 shows the benefits of compression on this strategy."
"to evaluate the benefits that our solution brings in terms of resource saving, we performed an experiment using real traces of two data centers named respectively univ1 and univ2 [cit] . these traces contain network traffic statistics of the data centers during a period of time. a machine is composed of several network devices. thus, the same pm can be involved in several networking operations at the same time. we used these traces to simulate network traffics of two cloud platforms hosting several vms as follows. we consider that a pm has the same characteristics as a p 1 in our private cluster. each ip address found within the traces is seen as a vm of type m3.medium from amazon ec2 nomenclature. thus, we have up to 6035 vms for univ1 and 5461 for univ2 in the entire traces, each pm providing a hosting capacity of up to 8 vms (a pm has 8 processors). from these assumptions and using our calibration results, we have simulated the amount of resources saved (respectively wasted) by our solution (respectively by the native implementation) in the simulated clouds. fig. 9 presents the results of these simulations. fig. 9 (a) and (c) respectively present the intensity of network traffics in univ1 and univ2. the latter contains the most important traffic. from this traffic information, we computed (using calibration results) the total amount of cpu load they induced in the dd. lets say t dd represents this number at a given time. therefore, the amount of resources (in terms of pm) saved by our solution is given by the following formula:"
"to tackle all these problems, this article proposes divert, a distributed vehicular re-routing system for congestion avoidance, which leverages both cellular internet and vanet communication. divert is a hybrid system because it still uses a server, reachable over the internet, to determine an accurate global view of the traffic. the centralized server acts as a coordinator that collects location reports, detects traffic congestion and distributes re-routing notifications (i.e., updated travel times in the road network) to the vehicles. however, the system offloads a large part of the re-routing computation at the vehicles and thus the rerouting process becomes practical in real-time. to take collaborative re-routing decisions, the vehicles situated in the same region exchange messages over vanets. also, divert implements a privacy enhancement protocol to protect the users' privacy, where each vehicle detects the road density locally using vanet and anonymously reports data with a certain probability only from high traffic density roads. when signs of congestion are detected, the server sends the traffic map only to the vehicles that sent the latest updates. subsequently, these vehicles disseminate the traffic data received from the server in their region. user privacy is greatly improved since this protocol reduces dramatically the number of vehicle location updates to the server and, thus, the driver exposure and identification risks. moreover, in this hybrid architecture, the server does not know the od pairs of the users."
"the main motivation in this paper is performance unpredictability and resource waste in the cloud. several works have investigated i/o virtualization issue [cit] . nonetheless, several studies [cit] have highlighted performance unpredictability in the cloud because of the vms competition on shared resources. works in this field can be classified in two categories. the first category consists of studies that proposed solutions at a micro architectural level (e.g. cache contention effects). this approach consists in placing vms intelligently on machines in order to avoid compete workloads atop the same machine [cit] . studies [cit] of the second category have addressed the problem at the software level. in this category, they advocate for bandwidth allocation to vms. each vm is allocated a minimum bandwidth, which is guaranteed. [cit] goes in the same vein by limiting a vm bandwidth proportionally to its booked cpu capacity. as we said in our previous work [cit], all these studies do not accurately guarantee performance predictability, even less cpu time charged on clients vms as we do in this paper. concerning the second category, the approach could be efficient if a given bandwidth always leads to the same cpu time in the dd. as we have shown, this is not true since several factors intervene. for instance, fig. 10 presents the dd cpu load when a vm uses its entire network bandwidth (48mb/s) for sending/receiving packets of different sizes. we can see a significant difference, up to twice the load for smaller packets. our solution tackles all these issues at a scheduler level."
"divert's goal is to protect driver's location privacy against attackers at the server side because the server could link traffic reports (which include locations) to driver identities. the traffic reports need to be frequent to compute a global traffic view and detect congestion accurately. yet, the location reports, when sent frequently, can create severe privacy leakage [cit] . even if pseudonyms are used for location reports and are changed frequently, attackers at the server side can use background information to discover the user identity for certain location points (home, work, etc.) and then use prediction algorithms to identify the whole location trace [cit] . therefore, divert strives to minimize the driver's privacy leakage by reducing the amount of location reports uploaded at the server, while maintaining good traffic accuracy."
"cloud computing enables remote on-demand access to a set of computing resources through infrastructures, so-called infrastructure as a service (iaas). the latter is the most popular cloud model because it offers a high flexibility to cloud users. in order to provide isolation, iaas clouds are often virtualized such that resource acquiring is performed through virtual machines (vms). a vm is launched with a fixed allocated computing capacity that should be strictly provided by the hosting system scheduler. the respect of the allocated capacity has two main motivations: (1) for the customer, performance isolation and predictability [cit], i.e. a vm performance should not be influenced by other vms running on the same physical machine. (2) for the provider, resource management and cost reduction, i.e. a vm should not be allowed to consume resources that are not charged to the customer. unfortunately, the allocated capacities to vms are not always respected [cit], due to the activities of some hypervisor system components (mainly network and disk drivers) [cit] . surprisingly, the cpu time consumed by the system components is not charged to vms. for instance, we observe that a significant amount of cpu is consumed by the underlying system components. this consumed cpu time is not only difficult to monitor, but also is not charged to vm capacities. therefore, we have vms using more computing capacities than the allocated values. such a situation can lead to performance unpredictability for cloud clients, and resources waste for the cloud provider. we have highlighted this issue in a previous work [cit] . in this paper, we complete our previous analysis [cit] by proposing a system extension which determines the cpu time consumed by the system components on behalf of the individual vm in order to charge this time to each vm. therefore, we significantly reduce performance disturbance coming from the competition on the hosting system components. note that most researches [cit] in this domain have investigated performance disturbance at a micro-architectural level (e.g. processor caches). our extension is complementary to them since it addresses another level of contention. a prototype has been implemented in the xen system and extensive evaluations have been made with various workloads (including real data center traces). the evaluations demonstrate that:"
"if the server detects signs of congestion in the road network, it will alert the vehicles by sending the updated map information, i.e., the \"updatedmap\" parameter in algorithm 2 containing tuples (road id, new computed travel time) for all the roads that have a current travel time different from the free flow travel time. the server sends messages only to the vehicles that reported most recently and that are located near a congestion spot, i.e., no further than three road segments from the congested segment. the server notification triggers the re-routing process that consists of a dissemination phase and a route computation phase. in addition, the dissemination phase has two sub-phases as presented in algorithm 2. when a vehicle receives such a notification message either directly from the server or from the surrounding vehicles, it executes the procedure described in algorithm 2. the first part of the procedure (lines 2-4 in algorithm 2) consists in disseminating to other vehicles the updated travel times in the network. in the second part of the dissemination phase, the vehicles that received the notification broadcast personal route information to the other vehicles. the route information depends on the re-routing strategy employed by divert, i.e., either the k-shortest paths or the od pair of the vehicle (lines 6-10 and 11-15 in algorithm 2). on receiving a route information message, the vehicles store the received data as indicated in algorithm 3. the received data will be used in the route computation phase to compute a new best path for the current vehicle. note that all the notified vehicles participate in the updated map data dissemination, but only the vehicles whose current paths traverse a congestion spot execute the computation phase (line 5 in algorithm 2). we only re-route vehicles that are directly impacted by congestion since this is sufficient to alleviate congestion and improve the travel times for all vehicles. moreover, this approach reduces the re-routing frequency for a driver and thus the computation and communication overhead [cit] . in the remainder of this section, we present an overview of our two centralized re-routing strategies that have proven to be the most effective in alleviating congestion, and then describe their distributed counterparts which allow the vehicles to compute alternative paths in a collaborative manner when congestion happens."
"packet size: as shown in the four rightmost curves of fig. 3, the cost of handling a network packet in the dd varies with its size (even if it is within the mtu). this variation comes from the real driver which includes data copies. by considering packet size, our calibration is also effective with other network technologies such as jumbo."
"this section presents the evaluation results of our solution. we evaluate the following aspects: (1) the overhead of the solution, (2) its efficiency regarding performance predictability, (3) its efficiency regarding resource waste minimization, and (4) its application to other hypervisors."
"divert is built around two design principles corresponding to the two major requirements described in section 1. first, the re-routing path computation should be offloaded from the central server to the vehicles to reduce the computation and communication burden on the server and achieve better scalability. therefore, the alternative routes should be computed by vehicles when there are signs of congestion on the roads. at the same time, the re-routing computation should be collaborative in order to achieve a better re-routing effectiveness. to this end, the vehicles could exchange messages over vanets and implement a distributed re-routing process."
"definition -weighted footprint counter. a weighted footprint counter, fc i, of a road segment r i is defined as follows:, where len avg is the average road segment length in the network, vf avg is the average free flow speed of the network, len i is the length of r i, vf i is the free flow speed of r i, and lane i is number of lanes of r i ."
"in divert, a distance-based timer approach is used to reduce excessive broadcasting when multiple vehicles are within communication range. after receiving a broadcast message, the vehicle waits for a certain time period until rebroadcasting the message. the waiting time period is inversely proportional to the distance between the receiving vehicle and the source vehicle. therefore, a vehicle that is farther from the message source should re-broadcast the message earlier. during the waiting period, if the current vehicle receives copies of the message, it means that another vehicle has already forwarded the message. thus, the current vehicle drops the message."
"our privacy-aware reporting is based on the observation that in dense areas, vehicles naturally experience a higher degree of anonymity similar to a person walking through an inner-city crowd. therefore, a density-based traffic reporting mechanism is proposed wherein vehicles report to the server only if the road density is higher than a predefined threshold. the server computes the smoothed average of the traffic density on each road segment as it receives new traffic reports. computing the smoothed average of the traffic density at each vehicle (using a moving time window) is of little use in our case because the vehicles do not report often due to our privacy-aware reporting protocol (e.g., a vehicle rarely reports twice from the same road segment). this mechanism is beneficial for both the re-routing effectiveness and the vehicle privacy, since the server can still accurately detect the congestion signs at the cost of lower user privacy exposure."
"hence, the main contribution of this article is the distributed system for re-routing. this system, divert, has four main features: (1) a scalable system architecture for distributed re-routing, (2) distributed re-routing algorithms that use vanets to cooperatively compute an individual alternative path for each vehicle that takes into account the surrounding vehicles' future paths. (3) privacy-aware rerouting that significantly decreases sensitive location data exposure of the vehicles, and (4) optimizations to reduce the vanet overhead and thus improve vehicle-to-vehicle communication latency."
"ã greatly improve the vehicles' travel time. however, these strategies are designed for a centralized architecture in which all the re-routing computation is done at the server side. our objective is twofold. first, we want to provide re-routing strategies that are based on the same ideas as the effective centralized strategies, but that can run in divert. this is challenging, since the whole computation can only be done by the vehicles in order to comply with the system design principles (see section 3.1). second, the distributed re-routing should ideally have similar effectiveness as the centralized re-routing. in the following, we present debksp (distributed ebksp) and dar ã (distributed ar ã ), two distributed re-routing strategies that achieve these objectives."
"we measured the effectiveness and efficiency of divert through extensive simulations over two real medium-size urban road networks. the experimental results show that, in comparison with the centralized system, divert can decrease the privacy exposure by 92 percent in addition to not revealing the od pairs of the user trips. in terms of average travel time, divert's performance is slightly less than that of the centralized system, but it still achieves substantial gains compared to the no re-routing case. divert is more scalable since it offloads most of the computation burden to the vehicles and reduces the network load on the server by 95 percent."
"this section presents the evaluation results of the accuracy of our solution for both network and disk workloads using micro-benchmark. we experimented two benchmarks: (1) a web application based on wordpress for network evaluation, and (2) linux dd command for writing data to a portion of a vm disk. the vm credit is set to 30 when running the network benchmark and 15 for the disk benchmark. the experiment is realized in two contexts: with our solution and with the native xen system. we show the ability of our approach to ensure that the aggregated cpu consumed by a client vm remains within its booked credit, which is not the case with the native system. this also allows to guarantee performance predictability. the leftmost curve in fig. 6 presents the results of these experiments. we can see that using our solution, the aggregated cpu load of the client vm is about the value of its credit (30 or 15). the margin of error is negligible. the three rightmost curves in fig. 6 focus on the network case. they present results for performance predictability. the second curve highlights the issue of performance unpredictability in the xen system when two vms share the dd (the throughput of the indicator vm goes from 1200req/sec when the vm is alone to 800req/sec when it is colocated). the third curve shows the results of the same experiment when our solution is used. we can see that the vm has the same performance, about 800req/sec. the latter represents the throughput the vm should provide regarding its booked credit. indeed, our implementation avoids the saturation of the dd since its allocated credit was enough for handling vms traffics when their aggregated cpu load stay within their booked credit. the rightmost curve presents the evaluation of our solution when several vms performing network requests are colocated. we can observe that the indicator vm performance is always the same regardless the number of colocated vms."
"the estimated density is computed locally by each vehicle, which obtains information about its neighbor vehicles by periodic exchange of beacons. each vehicle counts the received beacons in a short time window (i.e., 5 seconds in our implementation) and each vehicle emits beacons with the same frequency (such that each vehicle is counted exactly once in each period). as depicted in algorithm 1, each vehicle periodically checks the number of vehicles n i on the current road r i . to obtain accurate traffic information, each vehicle encapsulates in the beacon the current road identifier (i.e., r i ) and direction of traffic (i.e., side). when a vehicle estimates the number of cars (line 2), it only counts the beacons with the same r i and side as itself."
"the graph computation in divert has two parts. one is to update the travel time in the road network, equivalent to updating the weights of the graph's edges. the other is to compute the shortest paths in the road network. updating the graph has an oðeþ complexity, while the path computation has an oðnlog ðn þ eþþ complexity. we performed an experiment to compare the two parts for our specific settings. table 5 shows that the graph weight updating time is approximately 0.001 percent of the path computation for the centralized versions of ebksp and ar*. since divert offloads the path computation to the vehicles and the server is only responsible for the graph weight updating, these results demonstrate a substantial cpu load reduction at the server."
"the effectiveness of the distributed re-routing strategies mainly depends on the amplitude of the re-routing information dissemination among vehicles. this dissemination has two dimensions that are related. the first is represented by the total number of vehicles that receive rerouting information in a congested region. the second regards the average volume of information received by the vehicles. clearly, the higher the number of receiving vehicles and the higher the amount of information are, the more effective the re-routing process is. ideally, each vehicle affected by congestion should receive re-routing information about all the vehicles in their region. in this case, the re-routing process can have a similar effectiveness with centralized re-routing. however, achieving this level of dissemination in vanets is challenging for two main reasons. first, the data dissemination has to be done in realtime and therefore the dissemination time interval is short. typically, the data dissemination phase is limited to 0.2 seconds in the system. second, regular data dissemination in vanets exhibits poor performance in congested areas because of wireless contention [cit] . in this section, we present four optimization techniques implemented in divert which are applied together to improve the data dissemination efficiency in vanets. these techniques are: i) prioritized data dissemination, ii) k-shortest paths compression, iii) xor coding for packet loss recovery, and iv) distance-based timer for efficient broadcast."
"specifically, the first vehicle is assigned the current best path without considering others. then, the footprints counters are updated based on the new path. when assigning the second vehicle, the popularity scores of its k-shortest paths are calculated as defined below, and the least popular path will be chosen. the process is then repeated for the rest of the re-routed vehicles."
"according to the split-driver model, we have seen that i/o requests are handled by both the hypervisor and the dd on behalf of vms. therefore, they use their own processing time (hereafter referred to as \"system time\") to perform operations on behalf of vms. current schedulers (including credit) do not consider this system time when accounting a vm's processing time. this processing time is distributed as follows:"
"a number of mechanisms provide solutions for highly accurate real-time location updates, while achieving good privacy protection [cit] . however, these mechanisms require a trusted centralized entity such as a proxy server for location reporting. our privacy aware mechanism works in a distributed and probabilistic fashion without any help from trusted entities. thus, the risk of location tracking is distributed over vanets, and we argue that this is qualitatively better than trusting a single central entity."
"the experimental environment is the same as in section iii-a. the dd's computing capacity is configured to 30% of the processor (credit 30). vms are configured with a single vcpu (pinned to a dedicated processor, different from the one used by the dd)."
"because all bi-phrases initially belong to the same translation table, they share their feature weights after tuning. our main idea is to partition the set of bi-phrases by putting aside, in new translation tables, possibly misused bi-phrases according to the reranking with complex features of the decoder k-best list (rerank). this partitioning gives to subsequent tunings the opportunity to assign more adapted weights to the features of these specific groups of bi-phrases. intuitively, if the rerank one-best hypothesis is different from that of the initial decoder, the bi-phrases that account for the differences should have received different weights to encourage the decoder to either choose them or instead avoid them."
"more formally, on a pm p, if v has n vcpus (noted vcp u 1 to vcp u n ), burntcredit(vcp u i ) and pmp rocesst ime(p, v) provide respectively the aggregated processing time used by vcp u i and the time elapsed since the start-up of v, then the credit scheduler goal it to satisfy the following equation at each scheduling time:"
"to achieve the partitioning of the translation table we compare the rerank one-best hypothesis to the decoder one-best and compute their differences. on the one hand, there are n-grams from the decoder one-best hypothesis that are not found any more in the rerank one-best; on the other hand, there are n-grams that only exist in the rerank one-best hypothesis. since the decoder produces word alignments between the source sentence to translate and its hypotheses, we can extract all the bi-phrases from the translation table that are compatible with these n-grams and their alignments. each set of bi-phrases extracted from n-grams 1 either appearing (in) or disappearing (out) in the rerank one-best hypothesis compared to the decoder's, is moved to a specific translation table. then a new tuning is performed for each relevant partitioning configuration."
"our goal is to minimize the number of vehicle reports, i.e., only a fraction of the vehicles situated on a road segment will send traffic reports. specifically, density reports sent to the server conform to the following rules: (1) cars submit reports only when they perceive that the density on the road segments is above a threshold that would signal a chance of congestion, (2) cars decide probabilistically when to submit data as function of the density-i.e., the more cars there are, the fewer reports each car submits as the reports are distributed among the cars on the segment, (3) cars send their messages through anonymizers (e.g., tor [cit] ) to protect their identities."
the rest of this article is organized as follows. section 2 summarizes the related work. section 3 explains the design principles of divert. section 4 introduces the privacy enhancement mechanism and the privacy metrics. section 5 introduces the two distributed re-routing strategies. the four vanet optimization techniques are presented in sections 6. the results and associated analysis are discussed in section 7. we conclude in section 8.
"both debksp and dar ã require the re-routed vehicles to be ranked. in the centralized version, the rank of each vehicle is assigned by the server. here, each vehicle picks a random rank value between 0 and rank max, which is the estimated total number of re-routed vehicles; rank max is calculated by each re-routed vehicle based on the traffic density of the incoming road segments no further than l segments from the congestion point (e.g., l is 3 in our experiments). a vehicle of a certain rank computes a new route by considering the higher ranked vehicle paths. in debksp, each vehicle affected by congestion calculates the k loop-less shortest paths based on its current od pair and the updated travel times on the roads. then, the vehicles disseminate their rank and k shortest paths in their region for a predefined time interval (see section 6) . at the end of the route dissemination phase, each vehicle receives the k shortest paths of the vehicles in the region. however, given the nature of the dissemination process, the information gathered by a vehicle can be incomplete and different from one vehicle to another. in the final route computation phase, each vehicle iterates through the local sorted list of vehicles for which it has received information. it selects the (potentially) best path based on the original ebksp algorithm for each vehicle with a higher rank and eventually selects the best path for itself."
"however, the higher the traffic density of a road is, the more accurately the traffic density will be estimated. this is important since the re-routing effectiveness mainly depends on the traffic accuracy of the dense traffic roads. in section 7, we show that the loss of accuracy in the traffic view has only a marginal effect on the re-routing effectiveness, but greatly improves the privacy protection of the users."
"the decrease in performance observed when comparing the distributed strategies without privacy-aware reporting with the centralized strategies is due to lost packets in vanet during the path assignment phase of the strategies. the number of lost packets is small because our vanet dissemination optimizations reduce significantly the effect of network contention. therefore, the decrease in performance due to missing global information is very small (at most 8 percent) when compared to the increase in performance obtained by these strategies w.r.t. the no re-routing case (200-300 percent)."
"divert uses a prioritized dissemination to avoid that all the notified vehicles in a region start broadcasting at the same time, and thus reduce the network contention. when receiving a congestion notification, vehicle v i waits t i seconds before broadcasting its od pair or its k-shortest paths. the waiting time is determined based on the rank of the vehicle defined in section 5. the rationale is that the higher rank vehicle information is more important since each vehicle computes its own path based on the higher ranked vehicle paths. therefore, the waiting time function in equation (4) gives the higher ranked vehicles more time to disseminate their path data:"
"another option to optimize the information dissemination between vehicles is data compression. on the one hand, a large packet size increases the communication overhead and decreases the dissemination effectiveness. on the other hand, the mac layer protocol limits the payload of a packet that can be sent on the communication channel. the dar ã re-routing strategy is very efficient from the communication point of view since vehicles only disseminate their od pair. however, this is not the case in debksp algorithm that requires vehicles to transmit their k-shortest paths. hence, depending on the k value and the distance between the origin and destination, the size of the messages can be large. since the k-shortest paths generally present a high degree of overlapping, a compression algorithm is proposed to exploit this feature. fig. 3a shows a simple example of the potential space saving that could be obtained for three paths between the roads a and j. if all the three paths are naively broadcasted, 15 spaces are needed in total. however, the three paths only cover nine distinct roads and therefore optimally need nine spaces to be transmitted. to obtain a compact representation of the k paths without any loss of information, we represent only the differences between the k paths. specifically, the problem comes down to finding the sequence of the k paths resulting in the best compression. however, this problem is reducible to the hamiltonian path problem and therefore it is np complete. hence, a \"greedy\" algorithm is described to iteratively compress the k paths, based on the number of overlapping edges as shown in algorithm 5. the function compressðp; p k þ compresses path p with respect to path p k . the function produces a bitmap of size equal to the number of segments of p, in which bit i corresponds to the segment i in path p . bit i is set to 0 if the segment i belongs to path p k and to 1 otherwise. for each bit equal to 1 in the bitmap, the corresponding node id is also generated by the compression function in order to be able to re-compute path p based on p k and the compressed value of p ."
"other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one [cit] or by making small changes to the scoring function to extract k-best lists from other parts of the search space [cit] . some useful diversity can be obtained as these hypotheses can be combined using smt system combination or help to better train reranking systems. but in spite of the introduction of more diversity, these methods do not guarantee that eventually lists containing hypotheses that are more relevant to complex features will be obtained."
"the described translation table partioning procedure can be performed iteratively as each new decoding can be followed by rerank on the new k-best list generated. the differences between rerank and the decoder one-bests are extracted anew and put in new translation tables at each iteration. 2 iterations are performed until no more improvements of the bleu score are obtained by rerank on a development set. the decoder is retuned and rerank is re-trained after each iteration 3 to obtain more specific and updated weights for each old or new translation table. finally, at test time, the learned weights corresponding to the current iteration are applied."
"virtualization configuration: xen provides 3 possible network configurations for the dd. these are bridging, routing and nating. bridging is the most used configuration. the path (between the real driver and the backend) taken by frames through the linux network stack differs for each mode. routing and nating use a very similar path while it is different for bridging. as illustrated in fig. 3 (leftmost), the routing requires more cpu time (e.g. for frame header analysis, packets fragmentation/defragmentation) than bridging (which does not go beyond the second level of the iso model). bridging is used in the rest of the article, unless otherwise specified."
"when remaincredit reaches a lower threshold, the vm is no longer allowed to access a processor. periodically, the scheduler increases the value of remaincredit for each vm according to its initial credit c to give it a chance to become schedulable."
"to this end, we introduce first a privacy metric in section 4.1. then, we propose in section 4.2 a privacy-aware traffic reporting mechanism based on the road traffic density to reduce the privacy leakage for the reporting vehicles. our system considers that the vehicles are trusted. indeed, to avoid congestion, vehicles make collaborative decisions which require a vehicle to share shortest path or od pair information with other nearby vehicles. while privacy enforcement at the vehicle side is out of the scope of this paper, it is worth mentioning that several recent works consider the problem of shared data protection by leveraging the trusted execution environment of (personal) secure devices. be it the secure trustzone cpu [cit] of the arm cortex-a series equipping most of mobile devices today, a tamper-resistant hardware security module securing the on-board computer of a vehicle [cit], a secure portable token [cit] communicating with the user smart phone or plugged inside it (e.g., google vault), all such secure devices offer tangible, hardware-based security guarantees. such technologies can be leveraged for secure data processing in a distributed architecture as divert to protect the shared information and prevent malicious vehicles obtaining access to unauthorized data [cit] . instead of hardwarebased security, other techniques such as secure multiparty computing or protocols based on homomorphic encryption could also be considered, but they may impact the real-time constraints of divert. finally, in section 4.3, we present the algorithm used by the central server to compute the travel time in the road network."
"the i/o request type: packet transmission (when a vm sends a packet) and packet reception (when a vm receives a packet) do not follow the same path in the backend [cit] . therefore, the processing time needed to handle a network packet within the dd depends on its direction. as illustrated in fig. 3, transmission is less expensive than reception. this difference is located in the backend which performs notably one more hypercall when a packet is received. a worrying observation is that even if a vm does not host a network application, it could be concerned by a load generated within the dd. in fact, since the dd is not aware of the applications within a vm, it always handles incoming requests regardless if the vms are expecting i/o activities or not. this can be used by malicious users to saturate the dd [cit] ."
"divert differs from the above research in three aspects. first, we take full advantage of both cellular and vanet communication to perform scalable re-routing. thus, each vehicle can get accurate global knowledge of the travel time and, at the same time, is able to exchange route planning decisions with surrounding vehicles more efficiently. second, the route computation is performed in a distributed way over vanets. therefore, it is more scalable since it reduces the computation burden of the central server. third, we designed and evaluated a privacy enhancement mechanism, where each vehicle only uploads its location report when located in low sensitivity areas."
. the value of a is experimentally chosen to be 0.05. the server then estimates the travel time of each road segment by considering the following three cases.
"an i/o request received by the hardware (as an interrupt) generates an event on ec2. from this event, the dd generates a virtual interrupt which is handled by the real driver. the kernel in the dd is configured so that the be is the forwarding destination for any i/o request."
assess the effectiveness and efficiency of divert compared to the centralized system. investigate the performance difference between divert with and without privacy-aware traffic reporting. quantify the strength of the privacy protection mechanism. understand which vanet optimizations provide the most benefits. compare the cpu and network load at the server between divert and the centralized system.
we implemented in c a sender/receiver application based on udp to calibrate the cost of handling a network operation in the dd. the sender always sends the same frame to the receiver. both the sender and the receiver are within the same lan.
"the notified vehicles disseminate the information (i.e., traffic graph and vehicle route) in their regions with a limited number of hops to avoid excessive flooding. the dissemination also has a timeout, which is a constant parameter in the proposed system. when the time is up, based on the traffic graph and route information shared by other vehicles, each vehicle, whose current path traverses the congestion spot, locally computes a new route to its destination. this re-routing process is presented in section 5."
"average travel time. fig. 6 shows the average travel time for divert compared to the centralized system. divert achieves similar travel time as the centralized system for both debksp and dar ã, and both distributed strategies improve the travel time by more than 200 and 300 percent compared to the no re-routing case in brooklyn and newark, respectively. specifically, the travel time for debksp without privacyaware traffic reporting is only 1.4 percent less in brooklyn and 8 percent less in newark than the time for ebksp. when privacy-aware reporting is used, the performance decrease is 6.6 percent in brooklyn and 25 percent in newark. similar results are obtained for dar ã, with performance decrease of 0.9 and 13 percent without privacy and 10.2 and 24 percent with privacy, in brooklyn and newark respectively."
"the xor coding optimization also plays an important role in improving the data dissemination process so that each vehicle is able to receive more routing information from the nearby vehicles. the results in fig. 11 show that xor coding increases the average number of received re-routing data items by 41 percent for debksp and by 57 percent for dar ã . the benefit is lower for debksp because debksp generates larger packets than dar ã, and appending the xor coding field increases even more the fig. 11 . the average number of received re-routing data items. packet size. however, in spite of this significant improvement in the amount of disseminated data, the gain in the average travel time is only marginal as illustrated in fig. 12 . the explanation is that knowing the nearby vehicles' path information is sufficient for debksp and dar ã to provide similar re-routing effectiveness with their centralized counterparts, as already indicated in our first observation under the \"average travel time\" results. scalability. divert improves the system scalability by reducing the cpu utilization at the server side and reducing the number of messages exchanged between the server and the vehicles."
"in the xen para-virtualized system, the real driver of each i/o device resides within a particular vm named \"driver domain\" (dd). the dd conducts i/o operations on behalf of vms which run a fake driver called frontend (fe), as illustrated in fig. 1 . the frontend communicates with the real driver via a backend (be) module (within the dd) which allows multiplexing the real device. this i/o virtualization architecture is used by the majority of virtualization systems and is known as the \"split-driver model\". it enhances the reliability of the physical machine (pm) by isolating faults which occur within a driver. it functions as follows."
"the first centralized re-routing strategy is the entropy based k shortest paths (ebksp). the server first ranks the vehicles to be re-routed as a function of their remaining travel time to destinations (i.e., shortest time first). then, it computes k alternative shortest paths for each vehicle. the server sequentially goes through the ranked list and assigns to each vehicle the best path out of the k computed paths. this is the least popular path among the k alternatives in order to avoid potential future congestion. to compute the least popular path, a weighted footprint counter as defined below is attached to each road segment."
"this section presents the implementation of our solution in both the dd's kernel (version 3.13.11.7) and the xen hypervisor (version 4.2.0). this implementation is not intrusive for client vms since it only requires the modification of the backend and the hypervisor. regarding the former, new data structures have been introduced for storing information on the number of handled i/o requests. benefiting from the existence of the gnttab batch copy hypercall performed at the end of each i/o request, the backend sends the content of its data structures to the hypervisor. this is done periodically after a configured number of handled requests. regarding the hypervisor, we have modified the vm data structure (struct domain) so that it contains the cpu time used by the dd on behalf of a vm: net debt and disk debt . these variables give the debts that the vm must repay. we have also added a linked list of vm debts which buffers incoming information from the dd (see below). the new when the scheduler wakes-up, it parses the linked list and for each vm, it distributes its debts (by subtraction) to all its vcpus. this is done before invoking the scheduling function which chooses the eligible vcpu. note that distributing a vm's debts could require many scheduling round since xen imposes a lower threshold for vcpu credit. we have also provided a new version of procps [cit] so that cloud clients can know within their vm the amount of load used by the dd on behalf of their vms."
"a large body of works consider the problem of preserving the user's privacy in the context of location based services (lbss). for instance, the middle layer of dsrc defines the security services for application and message management [cit] . authentication schemes are designed to preserve the driver privacy in dsrc-based vanets [cit] . to prevent malicious tracking, a vehicle could change its anonymous key within an interval of a few minutes [cit] . divert has a different goal from all these works: it focuses on protecting the driver's location privacy from the central server, not from the other drivers in vanet. for driver-to-driver privacy, divert can leverage the existing solutions. scms [cit] provides privacy protection from both outsiders (i.e., other vehicles or eavesdroppers) and insiders (i.e., administrators of the servers). divert is complementary to scms, as its goal is to minimize the amount of privacy sensitive information uploaded to the server (i.e., location and od pairs), not to protect the information privacy once it has been uploaded. furthermore, scms relies on the organizational separation assumption to protect against insider attacks (i.e., different server component are distributed between different organizations which do not collude with each other). divert, on the other hand, achieves a good level of location privacy protection even if this assumption does not hold."
"the dd regularly informs the hypervisor level after a (configurable) number of i/o requests. the choice of this number is important for interactivity. if this number is too high, repaying debts on vcpus burnt credits will require several scheduling round since xen credit scheduler imposes a low level threshold for burnt credits. it can impact the interactivity of vms with latency sensitive applications (i/o intensive workloads), alternating between long phases of activity and starving (debts charging) as illustrated in fig. 8 . we evaluated three arbitrary rates (1, 35000, and 70000) and compared the fluctuation of the cpu when using a native systems (unmodified xen). we can see that a small value such as 1 is ideal, especially as it does not incur any overhead. note that regardless this phenomenon, the vm does receive its entire credit."
"state-of-the-art phrase-based statistical machine translation (pbsmt) systems can use a large number of feature functions decomposable into local scores to efficiently evaluate the partial hypotheses built during decoding. however, some feature functions are difficult to integrate into the decoder mainly because they are not easily decomposable, very costly to compute and/or only available after complete hypotheses have been posited. usually such complex features are used through the rescoring and reranking of the k-best translation hypotheses produced by the decoder [cit] . although this reranking pass is performed over the best part of the decoder search space, it is limited by the actual diversity expressed in the k-best list. additionally, reranking being performed on a list generated by a simpler set of features, it may not have access to hypotheses that can best exploit the potential of the complex features used. we describe a translation table partitioning approach that exploits the result of such a reranking to iteratively guide the decoder to produce new hypotheses that are more relevant to the complex features used. to this end, we focus in this work on the simple exploitation of the disagreement between hypotheses ranked best according to the decoder and to our featurerich decoder. in particular, we seek to provide the next-pass decoder with separate translation tables that either contain bi-phrases that are unique to the decoder's one-best or to the reranker's onebest, in the hope that it will tend, in a soft manner, to exploit the preferences expressed by the complex features, and to otherwise explore alternative translation choices. such a comparison is then iteratively repeated, until convergence on a development set between the new pass of the decoder and a reranker trained on the full set of hypotheses generated thus far. on the test data, this procedure thus produces after each iteration a new decoder n-best, as well as an iteration-specific new reranker best hypothesis. we report consistent improvements of translation quality over a strong reranking baseline using the same features on 2 different domains and 2 translation directions."
"virtualization configuration: xen provides different ways to configure how a vm disk is managed in the dd. these are tap, qdisk, and phy. the latter is the most used configuration. the configuration mode influences the processing time used by the dd. tap as well as qdisk requires much more processing time than phy. we use phy in the rest of the article."
"the server receives reports from vehicles indicating the number of vehicles on a road segment and computes the traffic density on the roads. every time the server receives a report concerning a road r i, it will smooth the computed density value k i using the following formula:"
"t max is the total dissemination time introduced in section 5, i.e., the time after which everyone stops disseminating and starts computing the new route. rank i is the rank of vehicle v i and rank max is the maximum rank of all vehicles (e.g., the total number of selected vehicles). rank max is estimated by each vehicle from the road network density data received at the beginning of the re-routing process. g is a predefined system parameter that is set to be 1.5 in our implementation. the waiting function has the following properties: i) the waiting time t i for each vehicle is a value in the interval [0, t max ]; ii) the higher ranked vehicles wait less time than the lower ranked vehicles. specifically, the vehicle with maximum rank transmits without waiting, while the vehicle with lowest rank waits t max time. fig. 2 illustrates the ranking function when t max is 0.2 s and rank max is 300. as shown, if a vehicle has high rank, it waits little time before broadcasting. conversely, if the vehicle's rank is low, it waits a long time before broadcasting."
"for the vehicle dynamics we use a standard model, see for instance [cit] . a description of the constants of the dynamical model is given in table i ."
"as specified in the following corollary of theorem 1, the asymptotic exponential decay rate of the maximum minimal type ii probability of error can be characterized by the divergence rate expression θ m (s) when the emu uses the optimal privacy-preserving hypothesis-unaware policy with memory."
"10 volunteers participated to data acquisition protocol (4 female and 6 male) aged between 23 and 38 years; 5 subjects had already experience with the bci systems. scalp eeg data were acquired from each subject during bci sessions using the g.mobilab device from g.tec (austria, 256hz). the eeg was recorded from 8 ag-agcl electrodes : fz, cz, pz, oz, p3, p4, po7 and po8. this channel set represents the union of the classical channels used to extract the p300 response (fz, cz and pz) with channels in the posterior regions that have strong correlation with desired matrix target [cit] ."
"when designing driver-assist systems such as ldas, a crucial question is how the system should interact with the driver. studies show that the acceptance of systems that provide assistance in lane keeping is higher for systems that interfere less with the driver's steering [cit] . the present work focuses on the design of a ldas that is guaranteed to keep the vehicle in the lane while overriding the driver only when necessary to do so. the difference with the above cited approaches is that here we apply control input only when a lane departure is imminent, while the other approaches continuously apply steering to keep the vehicle near the center of the lane."
control and view: the subject during the nocontrol trials was watching a video on the other half of the screen. control and answer: during the nocontrol trials the subject had to do computation as quickly as possible looking at the fixation cross at the center of the interface. following this approach the data set contains nocontrol trials representing real situations in which the user turns his attention elsewhere or speaks with another person.
"as depicted in figure 6, if the parameter verification fails one has to revise the parameter choice. by propositions 3.2 and 3.3, the verification fails if either v 1 or v 2 is too small. tightening the active constraints of the corresponding optimization problems (5) and (7) will usually increase the value and thus eventually lead to valid parameters."
"while solving the optimization problem in (16) leads to the asymptotic privacy-preserving guarantee, the energy supply alphabet y can be arbitrarily large which means a highly complex optimization problem. moreover, the energy demand alphabet x is determined by a number of operation modes of the appliances and is typically finite. we show in the next theorem that the alphabet y can be limited to the alphabet x . this result can greatly simplify the numerical evaluation of the asymptotic privacy-preserving guarantee."
"approximation of reachable set r first one has to compute an over approximation r of r(ṽ,r). this is a well studied topic, see for instance [cit], therefore we do not go into details here. important considerations concerning the implementation are provided in section iv-b."
"we have modeled the smart meter privacy problem as a neyman-pearson test on the consumer behavior, and characterized different privacy-preserving energy management policies in the asymptotic regime by divergence rate expressions. in the worst case scenario where the type i probability of error upper bound is close to one, we obtained a singleletter divergence expression for the asymptotic exponential decay rate of the maximum minimal type ii probability of error if the privacy-preserving memoryless hypothesis-aware policy is used; and we showed that the privacy-preserving memoryless hypothesis-aware policy cannot outperform the privacy-preserving hypothesis-unaware policy with memory. furthermore, we have proved that the energy supply alphabet can be constrained to the energy demand alphabet without loss of optimality for the evaluation of the single-letter-divergence privacy-preserving guarantee, which can simplify the problem and the numerical simulation. more importantly, we have shown that the proposed two-phase hypothesis-unaware energy management policy with memory, where the emu first learns the consumer behavior, can achieve the same asymptotic performance as the privacy-preserving memoryless hypothesisaware policy."
"in this work we show a simple and fast selfpaced system based on an heuristic method; we allow the system to recognize user's status, introducing thresholds in the classifier. moreover the choice of target stimulus become dynamic and this allows system to improve its selection speed depending on the user's ability and attention level. for this reason particular attention will be also put on system performances in terms of information transfer rate (itr) and accuracy. another important thing to emphasize is that we conducted our experiments in an environment completely operated by the bci system [cit] ) and we based user's task on real life situations. this is because we want to demonstrate the feasibility of using these systems in everyday life."
the last component is the actual safety supervisor. it checks whether the driver's input will lead to an unavoidable lane departure and overrides the driver when this is the case. the next section iii-b is devoted to a detailed discussion of this supervisor and contains the statement of the formal safety guarantees in theorem 3.1.
"in practice, the emu might have a limited processing capability, and at time slot i, applies a random memoryless hypothesis-aware energy management policy π i to determine the energy supply y i based on the current demand x i and the hypothesis information h as"
"the major challenge in designing such a system is that it requires a procedure that allows to identify the system states from which there exists a steering input that can keep the vehicle in the lane. the set of these states is called controlled invariant safe set. determining whether a state is in this set (a safety verification task) often requires computationally intensive algorithmic procedures. therefore the application of controlled invariance to real-world engineering problems has been limited to systems with special structures or small dimensions, see [cit] and references therein. here we show that under certain conditions, that we specify, the controlled invariant safe set has a simple characterization that can be computed efficiently online. we therefore propose a system architecture that performs an initial safety check at the time the driver requests activation of the ldas. if activation is safe (based on the above mentioned conditions), the system is enabled and continuously monitors the driver's steering inputs. the driver's inputs are overridden only if necessary to keep the vehicle in the lane."
"then one uses r to check whether conditions 1-3 hold true. for condition 1 this is straightforward. to check condition 2-3 we introduce two auxiliary optimization problems, the value of which will then allow to formulate sufficient conditions for these assumptions."
the following corollary of theorem 1 specifies the asymptotic exponential decay rate of the maximum minimal type ii probability of error by the divergence rate expression θ l (s) when the emu uses the optimal privacy-preserving memoryless hypothesis-aware policy.
"matrix elements were 16 simple b&w icons (in order to minimize the variability and possible veps due to a high variety of colors and load information of each icon) representing the actions that the user could perform on the environment (e.g. light control, dvd player, webcam for remote monitoring, mobile phone and opening the door). stimulation consisted in a random intensifications of each stimulus class, with a duration of 125ms each one. inter stimulus interval (isi) was set to 125ms, so 250ms lag between two stimuli."
"alternatively, the inequality (19) follows since φ(s, s) is the asymptotic exponential decay rate of the minimal type ii probability of error achieved by a memoryless hypothesisaware policy by using the single-slot policy"
"in corollaries 1-3, we have characterized the asymptotic exponential decay rate of the maximum minimal type ii probability of error in the worst privacy leakage scenario by a divergence rate expression. however, the numerical evaluation of θ(s) or θ m (s) is difficult. on the other hand, φ(s, s) provides an upper bound on the optimal asymptotic exponential decay rate. hence, we use the single-letter divergence expression φ(s, s) as an asymptotic privacy-preserving guarantee in this work."
"as discussed in detail in section iii-c, the parameter verification requires to find an over approximation of r(ṽ,r) and the solution of the optimization problems (5) and (7) . there exist already many tools for these tasks. we would however like to stress that in order to have safety guarantees, one needs to use tools with guaranteed accuracy. to be precise, for the reachable set we need a guaranteed over approximation. similarly, for the optimization problems we need to find a guaranteed lower bound of the values v 1 and v 2 . such bounds can be provided by global optimization algorithms, see for instance [cit] ."
the remainder of the paper starts with a discussion of the model and the formal statement of the problem. section iii contains the description of the proposed driver-assist system. the implementation is discussed in section iv which is followed by some concluding remarks.
we implemented algorithm 1 in matlab and used it to prevent lane departures of a simulated driver with a tendency to drift to the right. the result of the simulation is shown in figure 7 . in the simulation we represent the center of gravity of the car by a red circle and its heading by a black arrow. the safety supervisor keeps the center of gravity between the solid blue lines. at 3.15s the driver is overridden to avoid a lane departure. when the danger is averted control is given back to the driver.
"the first component is an initialization check, performed when the driver first tries to switch on the ldas. the role of this check is to guarantee that the safety supervisor is not enabled while the driver is performing an extreme maneuver. mathematically this corresponds to the conditions outlined in figure 4, where u min, u max,ṽ,r andψ are design parameters of the system. the appropriate selection of these parameters is discussed in section iii-c. only if the initialization check is successful the system is enabled."
"assumption 3: for every given longitudinal speed u, there exists a (feedback) torque input τ s w that allows to keep the longitudinal speed constant at u ."
"conditions 1-3 all impose some requirements on the design parameters u min, u max,ṽ,r,ψ andδ f . consequently, before the supervisor introduced in the previous section can be used to guarantee safety online, one has to verify offline that for the given choice of design parameters these conditions are satisfied. this verification process is depicted in figure 6 and has two parts."
"3) finite look-ahead horizon: the sets w + and w − are based on a prediction on the time horizon t + and t − . as this time horizon is finite but we want to keep the vehicle in the lane on the infinite horizon r +, we need to ensure that at the end of the look-ahead horizons t + and t − there still exists a steering input that can keep the vehicle in the lane. as we show in theorem 3.1 below, this is the case if the following conditions hold true:"
is enabled? fig. 4 . block diagram of the ldas. the sets w + and w − are defined in (3) . the system has a number of design parameters that have to be validated offline before the safety supervisor is guaranteed to keep the vehicle safe.
"as discussed in section ii-a, when overriding the driver the safety supervisor is restricted to keep the longitudinal speed constant and to use the following steering strategies: 1. steer left to avoid right lane boundary; 2. steer right to avoid left lane boundary. since the focus in this paper is on steering control, to satisfy the requirement on longitudinal speed we assume:"
"since the hypothesis test strategy in (7) is not necessarily optimal for the ad, the definition of the maximum minimal type ii probability of error implies that"
"the development of autonomous vehicles has made rapid progress over the past 30 years, yet there remain important challenges, among which the ability to verify that systems can operate safely and robustly. nevertheless, already today it is possible to benefit from this progress via semi-autonomous safety systems. a good example are lane departure assist systems (ldas) which already on their own have a large potential benefit for improving traffic safety. according to the nhtsa fars database [cit], 12,703 people died in the united states after a lane departure, i.e. 42% of all traffic fatalities were preceded by a lane departure."
"where a n denotes the decision region for h 0 of the ad. the privacy-preserving objective of the emu is to maximize the probability of error of the ad. more specifically, for a given res energy generation rate s, the emu uses the optimal energy management policy to achieve the maximum minimal type ii probability of error subject to a type i probability of error constraint"
"the ldas described in section iii has two main parts. before the system is put into operation, we have to find a set of parameters that is safe according to the test outlined in figure 6 . using a verified set of parameters, the safety supervisor depicted in figure 4 runs online to keep the vehicle safe. in the following we provide details on the implementation of these two parts."
"the vehicle dynamics take place in the continuous state space x ⊂ r 6 . in the following we will use a body-fixed reference frame that is fixed to the ego-vehicle's center of gravity, see figure 1 . all dynamical interactions will be described within this reference frame. the major simplifying assumption that we make is that the road lane is straight."
"front and rear tire slip angles, which shall be denoted by α f and α r, are functions of the steering angle and the front and rear sideslip angles,"
"we distinguished two different states in which the user can be: the control state, during which he/she was attending to the stimulation because he intended to exercise control over the surroundings through the interface; and the nocontrol state, during which the user was engaged in another task and then he wanted the system refrained to make decisions. the eeg signal was reorganized in overlapping epochs representing the 800 ms time intervals immediately following the onset of each stimulus; we can distinguish the epochs acquired during control trials in target epochs and notarget epochs. target epochs relate to the onset of the row or the column stimulus containing the icon that the user intends to select, while notarget are related to no relevant stimuli. then epochs were grouped into sequences; a sequence denotes a single presentation of each stimulus class on the control interface; in this case a sequence consisted of 8 epochs, one for each stimulus class of the interface. a single sequence lasted 2 seconds. with the term trial we refer to a set of sequences at the end of which a selection is made. between 2 trials we took 4 seconds during which the system presented to the users the target icon. all the icons on the interface were proposed as a target to the subject with the same frequency and this because we wanted that each stimulus was equally likely for subsequent analysis. finally, a run consists of a series of trials at the end of which data acquisition is stopped."
"compared with the privacy-preserving memoryless hypothesis-aware policy, the privacy-preserving hypothesisunaware policy with memory has all past demands and supplies while it does not know the correct hypothesis. we next compare the asymptotic privacy-preserving performances of the two policies."
"the second component is a status update, which continuously monitors whether the driver is keeping the speed and front wheel steering angle within the allowed range of operation, [u min, u max ] and [−δ f,δ f ] respectively. hereδ f is a design parameter. the necessity of this check comes from the fact that the system does not actively constrain the driver's wheel torque requests or front wheel steering angle. instead, if the check fails the ldas is disabled."
"the most restrictive assumption that we made is that the lane is straight. in future work this assumption should be relaxed. in addition, it was assumed that the system dynamics could be modeled accurately. as in practice these dynamics are subject to uncertainties and disturbances, the design should be extended to accommodate bounded uncertainties. finally, it would be desirable to have an algorithm that allows to revise the parameter choice automatically when the parameter verification fails."
this section describes the results obtained through the off-line analysis performed on data acquired during the 2 recording sessions. then we compare our self-paced system with a classic p300 based bci in terms of information transfer rate considering only the control trials.
"self-paced control is based on the introduction of some thresholds in the classifier; the classification was performed as explained before but the system will refrain from making a selection until a row and a column scores exceeds the threshold value. the threshold values were chosen through a procedure that relies on the use of roc curves. in particular we calculated the scores value on the target, notarget and nocontrol epochs. a normal distribution well fit the scores distributions and to confirm this we ran a t-test on the 3 different score distributions for each subject. t-test results show that the hypothesis of normal distribution is true with 95% confidence level. next step was investigating if the 3 score's distributions can be considered different; for this reason we ran the kolmogorov-smirnov test on each pair of samples. table 1 report the results of this test; the hypothesis of different distribution was confirmed with the 95% confidence level for all subjects except one (subject 1: no target vs nocontrol). for this reason it is necessary to take account of nocontrol trials for thresholds extraction. the threshold values were chosen according to the number of stimulation sequences accumulated in the trial. in fact the scores for the general stimulus i at the sequence s will be defined as:"
we start this section with a description of the lane departure assist system (ldas) that we want to design. in the second part of the section we introduce the model and provide a formal problem statement.
"we now consider the case when the emu does not know the correct hypothesis but has a large memory storage and a powerful processing capability. at time slot i, the emu follows a random hypothesis-unaware energy management policy with memory ρ i to determine the energy supply y i from the ep based on the demands x i and the past supplies y i−1"
remark 2. the optimal privacy-preserving memoryless hypothesis-aware policy cannot outperform the optimal privacy-preserving hypothesis-unaware policy with memory. that is because the emu having no direct access to the hypothesis information can learn the hypothesis with an arbitrarily small probability of error after observing a sufficiently long energy demand process.
"recall that for a provably safe design that respects the above mentioned restrictions it is important to assure that the steering required to prevent the crossing of one lane boundary will not cause an unavoidable crossing of the other, see figure 2 . finally, as discussed in section ii-b, the dynamical model is only valid when assumption 2 holds true. to enforce these constraints in a semi-autonomous manner, we propose an architecture that has three main components, see figure 4 ."
"correct classification: the target is correctly recognized; wrong classification: there's a target misclassification; missed classification: the thresholds are never exceeded, for this reason the system abstains from take a decision. during nocontrol trials possible classification outcome can be: abstention: the system properly refrain from taking decisions; missed abstention: the thresholds are exceeded and the system wrongly makes a choice. figure 2 also shows the averaged number of sequences needed to make a selection when the user was in the control state. from the graph it can be see how the system was proved to be robust during nocontrol trials in fact abstentions reached an average of 98%. on the other hand there is a high percentage of abstentions during the control trials (average 8,4%), this may seem an inconvenience but it really represents the system's ability to avoid misclassification, because the percentage of wrong classification did not exceed an average of less than 3%. we used the first 2 runs of the first 2 sessions, in which the subject was always in a control state, to assess his ability with a classic p300-based bci. we found the 'optimal' number of stimulation sequences just doing a cross validation offline: particularly we used 2 runs to train swlda and to extract significant features and the other 2 to test these parameters, then we averaged the results of classification for each possible combination of training and testing data set. the figure 3 shows the trend of the percentages of correct classification based on the number of stimulation sequences accumulated for each subject. the black line represents an accuracy of 95%, which corresponds to a false positive rate of 5% that is the maximum of false positive allowed in the self-paced system through roc curves. we used these results to estimate the information transfer rate."
in this section we design the feedback strategy π s that provides a solution to problem 1. the design requires three conditions that are introduced and discussed in the following three paragraphs. these conditions can be checked offline as described in section iii-c. in the fourth paragraph we formally define π s and elaborate on the safety guarantees that it provides.
"the introduction of a threshold based classification system in the p300-based bcis allows the user to divert his attention from control interface at any time and without the use of external inputs, and it also brings positive effects on the bit rate that is incremented when the user is in the best control conditions. a further advantage consists in increasing the accuracy of the system by preventing errors through abstentions; in this way the bci system acquires more dynamicity and flexibility by reducing its gap with traditional input interfaces. future applications could consider the use of dynamic thresholds fitting the user's current action or the environment state: the system would be able to automatically identify the user's most likely action and facilitate its selection by reducing the threshold values for that item. this work represents a step towards the use of the bci systems as an aid for functional communication and environmental control for people with severe motor disabilities or as alternative means when the usual channels of communication and interaction are temporarily compromised."
"next, it is shown that the asymptotic exponential decay rate of the maximum minimal type ii probability of error subject to a type i probability of error constraint can be characterized by θ(s)."
"each subject completed 2 recording sessions, we used collected data for subsequent off-line analysis and during recording session subject had not any feedback about classification results. the first recording session included a total of 8 runs, the first 2 compounded of 8 trials each during which the subject was asked to always exercise control on the interface; the number of stimulation sequences per trial was fixed a priori to 10. over the last 6 runs control trials and nocontrol trials were alternated for 10 trials in total. during the nocontrol trials the subject was asked to focus the eyes on a fixation cross in the center of the stimulation interface. this is because we wanted that the subject was not completely immune to the stimulation even when he was in the nocontrol state. it is assumed that a bci user can't move his head or eyes to ignore the stimulation, so it is essential that the nocontrol state is robust to random stimuli which the user does not actually paying attention to."
"the second session provided 2 runs in control state to verify the subject's attitude to control the interface using the p300 potential, and 6 runs in which control trials and nocontrol trials were alternated, however we considered different nocontrol tasks; in fact, the last 6 runs can be divided into 2 groups of 3 runs each:"
"we consider a vehicle equipped with sensors, for instance a camera, that are able to determine the vehicle's distance from the left and right lane boundary, denoted by d l and d r respectively. in addition we assume that measurements on the vehicle's dynamics are available. in particular we assume the vehicle's sensors provide measurements of the longitudinal and lateral velocity u and v, yaw rate r and heading angle ψ, see figure 1 . notice that estimates of yaw rate and lateral velocity could be obtained from measurements of lateral acceleration, see for instance [cit] . the ldas then uses this information to override the driver when a lane departure is imminent. it is therefore a semi-autonomous vehicle function that should act only when otherwise safety cannot be guaranteed. we call such a vehicle function a safety supervisor. here we consider the case of a straight lane, see assumption 1 below. moreover, ldas is restricted to use either right steering to avoid crossing the left lane boundary or left steering to avoid the right lane boundary and it cannot change the vehicle's speed, i.e. the longitudinal velocity u must remain constant during interventions of ldas. in this setting the requirements for the safety supervisor are:"
"we used a formal approach to design a semi-autonomous lane departure warning system. the core of this system is a safety supervisor that uses the controlled invariant safe set to determine when to act. this guarantees both safety and that the driver is overridden only when necessary. moreover, the supervisor is computationally efficient and easy to implement. the design of the supervisor is based on a number of parameters that have to be verified before safety can be guaranteed. it was shown that this verification can be done algorithmically."
"1) range of operation of the system: throughout this sectionτ w andδ f are positive constants as in section ii-b. similarly, in accordance with the previous section iii-a, u min, u max,ṽ,r andψ are positive constants. the choice of these constants is the subject of the next section. define the reachable set of the lateral dynamics as follows:"
"this express the bit rate or bit/trial for each selection. the information transfer rate (bits/minute) is equal to b wolpaw multiplied by speed of selection s (selection per minute). in turn the speed selection for p300-based system depends on the number of sequences of stimulation used and when we calculated it we have taken account of the 4 seconds between a trial and the other which are used to present the results of the classification. the table 2 shows the values of itr for each subject calculated using the average number of sequences and the percentage of accuracy obtained by off-line analysis both for self-paced bci and for the synchronous one. in particular, for the synchronous system we imposed the number of stimulation sequences that allowed subject to achieve 95% of accuracy, if this did not happen we have used the minimum number of sequences that produced the highest accuracy."
"most major car manufacturers are currently offering some type of lane departure assistance. the spectrum ranges from pure warning systems to systems that proactively keep the vehicle in the center of the lane. these are vision based systems that use the vehicle position in the lane, its heading and a limited look-ahead horizon (based on the sensor capacity) to determine whether a warning or steering input is necessary, see for instance [cit] . the drawback of such a design is that it does not provide formal safety guarantees."
"where i denote all features related to single stimulus j. it is assumed that a p300 is elicited for one of the four row/column intensifications during control trials, and that the p300 response is invariant to row/column stimuli, the resultant classification is taken as the maximum of the scored feature vectors for the respective rows, as well as for the columns:"
in practice such a torque input could be obtained from cruise control. drivers can change the longitudinal speed whenever they are not overridden by the safety supervisor.
"in the following safety will always mean that the system state x belongs to the safe set s. since we investigate a semiautonomous ldas, we seek a feedback control strategy that keeps the state x in s for all time and overrides the driver steering only when necessary to do so. such a feedback strategy will be called a safety supervisor. the principle is illustrated in figure 3 ."
"diagram shows the interaction between safety supervisor and driver. the system state is x, driver's and supervisor's input are denoted by (τ d w, δ d f ) and (τ s w, δ s f ) respectively. the supervisor's feedback control strategy π s equals the driver's input whenever this input is safe."
"an off-line cross validation was performed on the data collected during recording sessions. in particular, we divided the data into a training data set consisting of 3 runs from the first session and 4 runs from the second session. in this way we included in the training data set nocontrol trials related to all 3 different nocontrol tasks. the remainder of the data set was used as a test data set. specifically, the train data set was used for features extraction and to select the threshold values. the figure 2 shows the results obtained in cross validation."
"assumption 2 (i) assures the stability requirement and is satisfied by cars and trucks, see [13, sec. 6] . the second assumption guarantees small sideslip and tire slip angles respectively. for a more detailed discussion on the validity of this model see for instance [cit] ."
"2) separation of steering tasks: in general it is not possible to consider the objectives: 1) stay left of the right lane boundary; 2) stay right of the left lane boundary; as independent control tasks, see fig. 2 . in the following we provide a condition under which a decoupling of these tasks is possible. hence this condition allows to design a safety supervisor that respects the restrictions on the steering actions that were imposed in section ii-a."
"states that no other face in ۶ can be closer to ‫ܥ‬ ାଵ ሬሬሬሬሬሬሬሬሬԧ than ‫ݔ‬ ԧ, the face that is closest to ‫ܥ‬ ାଵ ሬሬሬሬሬሬሬሬሬԧ must be a member of ۱ ାଵ . in other words, the face that is closest to the average of a cluster must be a member of that cluster when clusters are formed in the way described in theorem 2, i.e. selecting the face that is to closest the current average and adding it to the cluster. this is exactly the way how k-same-furthestሺሻ forms its clusters (lines 11-12 and 20-21). however, k-same-furthestሺሻ forms two clusters simultaneously, meaning ۱ in theorem 2 is the union of ۱ and ۱ in k-same-furthestሺሻ. a member in ۱ can only be the closest face to the average of ۱ when there is overlapping between ۱ and ۱, and vice versa. since k-same-furthestሺሻ (lines 14 and 23 in fig. 2 ) ensures that no overlapping is allowed between ۱ and ۱, among the remaining original faces the face that is closest to the average of ۱ must be a member of ۱ according to theorem 2, and vice versa. as k-same-furthestሺሻ never de-identifies the members of a cluster as the average of that cluster, the deidentified faces (average of this cluster) can never be matched with their corresponding original faces (faces rather than members of this cluster), as long as the matching process uses the same distance measure as k-same-furthestሺሻ. theorem 3 is proved. theorem 3 means that as long as the recognition software uses the same distance measure as itself, k-same-furthestሺሻ guarantees to thwarts face recognition software for every face in its ۶ ௗ and therefore the best k-same solution in terms of privacy protection."
"snt will expose customisation controls to the user, including traffic management options, specifying the characteristics of traffic emerging from each snt port. snt assumes that the commodity switch is properly validated, and some of its properties can (and were) validated using osnt. the switch can also be used as an interface converter, e.g., traffic returning from 100ge port on the dut can connect to the switch, and sent back to osnt using a 10ge port."
"where w i is the precoding vector for the i-th user. (6) our precoder design criterion is to minimize the total transmitted power subject to minimum per-user sinr requirements and perantenna power constraints (papcs). formally, the problem can be stated as"
"the problem becomes even more profound when one considers the variety of network interconnects and port's form factors supported by high end platforms: ranging from 10ge sfp+, through 100ge using qsfp28 to emerging 400ge solutions. researchers and developers rarely have access to the entire range of interconnects."
"moreover, consensus could also be reached on weights representing the relative importance of criteria. additionally, we have planned an experiment with ontology m ontology engineering group (oeg), http://www.oeg-upm.net. experts to contrast the results of our approach with the results of experts selecting the set of ontologies themselves."
"we have proposed a maut approach to deal with the selection of domain ontologies for reuse in the development a new ontology within the neon methodology framework. as part of this approach, we have suggested the use of the gmaa decision support system to support the selection."
"in this work, we propose scalable network tester (snt), a network tester midway between low-cost, limited functionality network testers and very high-cost, full-functionality network testers. snt achieves this trade-off by combining a commodity switch with an open source network tester to provide high throughput, capable of saturating 100+gbe links, while remaining affordable for academic research groups."
"the neon methodology is a scenario-based methodology that provides guidance for all main processes and activities in ontology engineering projects. the onto-com model c provides information about how many people should be involved in a particular ontology network development. this is a cost estimation model, whose goal is to predict the costs (expressed in person months) of typical ontology engineering processes and activities."
". high, when the candidate ontology has been reused between 4 and 7 times and . very high, when the candidate ontology has been reused 8 or more times."
"di®erent approaches have been proposed to support the ontology reuse process: fernandez-lopez and colleagues5 and uschold and colleagues6 propose di®erent activities and tasks for reusing domain ontologies. pinto and martins 7 propose some activities for reusing ontologies as part of the integration process. finally, paslaru and mochol 8 propose an incremental reuse process."
"the goal of reusing domain ontologies is to ¯nd and select one or more domain ontologies that can be reused to develop a new ontology. in accordance with the methodological guidelines proposed in the neon methodology, this reuse process is composed of four activities, 3, 9 which are explained in detail as follows. activity 1. domain ontology search. the objective of this activity is to search libraries, repositories, and registries for candidate domain ontologies that could satisfy the needs of the ontology network under development. the terms speci¯ed in the pre-glossary included in the ontology requirements speci¯cation document (orsd) 15 are the input for this activity."
"for the benefit of discussions in this paper, the k-same solutions that de-identify an original face based on the faces that are closest to it are referred to as \"k-same-closest algorithms\". examples of k-same-closest algorithms include ksame-pixels [cit], k-same-eigen [cit], k-same-m [cit] and k-sameselect [cit] . in fact, k-same-closest algorithms are the worst ksame solutions in terms of privacy protection. when no overlapping exists between any two clusters, the original face that is closest to the average/center of a cluster must lie within the cluster (see proof of theorem 2 in the next section). the ksame-closest algorithms uses the center of a cluster to deidentify the k faces in that cluster, meaning the algorithms will always lead to a recognition rate equal to the theoretical maximum of 1/݇ . when overlapping exists between two clusters the center of a cluster can be closest to an original face from the overlapping cluster (such as that demonstrated in fig. 1 where the inner cluster of ሼ3,4,5ሽ is formed earlier than the outer cluster ሼ1,2,6ሽ and the outer cluster has an average equal to a member of the inner cluster), giving k-same-closest algorithms lucky escapes. however, when de-identification is performed to an original face instead of a face set the k-sameclosest algorithms will never escape from the theoretical maximum and always generate the worst privacy protection within the k-anonymity framework. even with a face set, experimental results (fig. 6) show that the recognition rate of faces de-identified by the k-same-closest algorithms always stays synchronized with the theoretical maximum of 1/݇ and force the k-same-closest algorithms to use large k's to achieve decent privacy protection. large values of k will not only lead to the requirement of large image gallery for the deidentification process but also to the discrimination issue with"
"after hydrolysis of complex substrate into simpler organic compounds such as glucose, short chain fatty organic matter takes place, acidogens degrade glucose into acetic, propionic and butyric acids. hydrogen is considered as an inhibitor for acidogens according to embden-myerhoof pathway since the correlation of nad + /nadh in the cells of biomass depend on the concentration of hydrogen."
"we have identi¯ed new attributes for measuring the performances of the candidate ontologies for the di®erent criteria under consideration. most are objective rather than subjective attributes and are easier for experts to measure. for instance, ontologies are evaluated using oops! to detect pitfalls. this is an advantage over earlier research on ontology reuse that tended to use subjective attributes."
"these candidate sports ontologies were analyzed with respect to the set of criteria and measurement attributes de¯ned in sec. 4.1 as part of the domain ontology selection activity. table 2 shows the sports ontology performances for the 10 criteria under consideration. note that dms were allowed to use performance intervals (continuous attribute) or several discrete performances (discrete attribute) to enter imprecise performances. table 2 does not include reuse cost because the nine candidate ontologies under consideration were available for free. thus, this attribute was omitted from the analysis."
"evaluate the candidate ontologies in order to determine whether candidate ontologies contain errors or anomalies. oops! is a web application that helps ontology developers to detect some of the most common pitfalls in ontologies. 49 oops! detects 21 common pitfalls and reports the number of times each pitfall appears in the analyzed ontology. examples of pitfalls are: (a) creating synonyms as classes (when several classes whose identi¯ers are synonyms are created and de¯ned as equivalent); (b) creating unconnected ontology elements (when some ontology elements are created that bear no relation to the rest of the ontology); (c) de¯ning wrong inverse relationships (when two relationships are de¯ned as inverse relations and are not necessarily so); (d) missing disjointness 50 (when axioms between classes or between properties that should be de¯ned as disjoint are omitted from the ontology); and (e) missing domain or range in properties (when the ontology includes relationships and/or attributes without domain or range (or either)). this criterion should be measured using a discrete attribute based on the number of pitfalls present in the candidate ontology. thus, the attribute range would be [cit] ."
". low, when the candidate ontology was developed for internal and/or academic use; . medium, when the candidate ontology was developed as a part of a national research project and . high, when the candidate ontology was developed as a part of a european or international research project."
"network testing equipment has always been a trade off between cost and performance -specialised hardware based equipment offers the best throughput, number of ports and precise configuration capabilities. however, in addition to being prohibitively expensive this commercial testing equipment is also closed and proprietary."
"fig . 6 shows the rank-1 recognition rates for the cropped original faces and the faces de-identified using either k-same-m or the proposed k-same-furthest. as shown in fig. 6, when the same distance measure is used, k-same-furthest always produces a recognition rate of zero. when pca representation of face images is used the face recognition software, the recognition rates of the k-same-furthest de-identified faces are slightly above zero whilst stay far lower than k-same-m faces. when the face recognition software represents faces as aam parameters, the recognition rates of the faces de-identified by k-same-m remain around 2-3% below the theoretical maximum of 1/݇. when pca representation is used by the face recognition software, the recognition rates of the k-same-m faces is lower than those calculated by the aam-based recognition but still stay much higher than the recognition rate achieved by k-same-furthest. regardless of which feature space the recognition software uses to represent face images, the recognition rate of the k-same-m faces always stay synchronized with the theoretical maximum of 1/݇ . this forces k-same-m to use large k's in order to achieve decent privacy protection. with k-same-furthest, the recognition rate stay at zero or slightly above zero, allowing decent privacy protection to be achievable with small k's. this means that for a required level of privacy protection, k-same-furthest requires a smaller k than k-same-closest and in turn delivers a better discrimination among faces in the de-identified face set. after all, there are k copies of each face in the set. fig. 7 is the first face to be de-identified, the same face will always be selected as λ ଵ (line 7 in fig. 2 ) regardless of k and the selected λ ଵ is the furthest face to it over the entire face gallery. the cluster of faces that is used to de-identify it is the cluster of k faces that are closest to face λ ଵ . since λ ଵ is the same regardless of k, nearly identical de-identified faces are generated by k-same-furthest for each original face in fig. 7 over various values of k. when the original face is not the first to de-identify the face that is furthest away from it over the entire face gallery might have already been taken by the previously formed clusters. as a result, a different λ ଵ and hence a different de-identified face will be generated for different k values. fig. 8 displays the visual results of the proposed k-samefurthest algorithm where no preservation of data utility is implemented and hence the entire image gallery containing faces with various expressions and head poses is used to form the face clusters. the de-identified faces generated with various values of k display various expressions and head poses but a nearly identical identity. when k is small the de-identified faces tend to display an expression and a head pose of an extreme from the gallery. as k increases, the cluster on which the de-identified face is based becomes more and more diverse. as a result, the de-identified face converges to the average of the half gallery that is further to the original."
"shared reflects the notion that an ontology captures consensual knowledge, that is, it is not private of some individual, but accepted by a group\" .1 a great many of ontologies have been developed to date by di®erent groups, taking di®erent approaches and using di®erent methodologies, methods and techniques. a series of methods and methodologies for developing ontologies from scratch have been reported in ref. 2 . as opposed to custom-building new ontologies from scratch, a new approach is now being followed in the ontology engineering ¯eld,3 emphasizing: (1) knowledge resource (ontologies, nonontological resources, and ontology design patterns) reuse and subsequent reengineering, (2) collaborative and argumentative ontology development, and (3) construction of ontology networks. if applied e±ciently, this approach is able to reduce the time and costs associated with ontology development, because it avoids the re-implementation of existing ontologies. additionally, this approach has the potential to (a) spread good practices and (b) increase the overall quality of ontological models."
"a very promising approach is the use of full frequency reuse among beams jointly with precoding in order to reduce the multibeam interference [cit] . as a result, the multibeam satellite system can switch from the current frequency reuse factor 4 to full frequency reuse, leading to a substantial increase of the user bandwidth. as long as the precoding is able to keep a sufficiently large signal-tonoise ratio (sinr) for each user, the multibeam system capacity will be increased."
"this paper refers to k-same-pixel/-eigen and all their current extensions as the k-same-closest algorithms and has pointed out that they share the fundamental drawback of deidentifying faces based on faces closest to them. this means that the k-same-closest algorithms are actually trying to minimize identity loss instead of maximising it and restricts the k-same-closest algorithms to achieve decent privacy protection at the cost of large values of k, which in turn lead to the demand for a large image gallery or otherwise lack of discrimination among the de-identified faces (number of distinctive faces in the de-identified face set is equal to or less than size of the gallery divided by k)."
"additionally, taking advantage of the fact that the oeg research group m is involved in many eu projects on ontology development and collaborates with numerous ontology engineering experts around the world, interviews and questionnaires could be administered to build a consensus on such attributes, i.e., to come up with a collection of criteria created with ontology community agreement."
"additionally, several researchers have already suggested series of criteria for ranking and selecting ontologies. pinto and martins, 10 for example, focus on a set of criteria related to what knowledge is missing, what knowledge is super°uous; what knowledge should be relocated, and which knowledge sources, documentation, terminology, de¯nitions, and practices should be changed. lozano-tello and gomezperez11 de¯ne a detailed set of 117 criteria, organized in a three-level framework, for selecting the best ontology. alani and brewster12 propose aktiverank, a system for ranking ontologies that aggregates several measures related to structural features of ontology concepts. the measures used in aktiverank are ontology coverage for given terms, concept centrality in a hierarchy and in the ontology, structural density, and semantic similarity between concepts. park and colleagues 13 present ontologyrank, an approach for selecting and ranking ontologies based on semantic matching as well as lexical matching. the total score is calculated by aggregating the values from three-criteria measures (semantic similarity, topic coverage, and richness). each measure has a weight that depends on the domain and context, thus they use the regression model to ¯t such weights. finally, martínez-romero and colleagues 14 propose an approach for automatic ontology recommendation. this approach is based on measuring the adequacy of an ontology to a given context according to three independent criteria: (a) the coverage of the ontology in the requested context, (b) the semantic richness of the ontology in the context, and (c) the popularity of the ontology."
"where i kn is a kn-dimensional identity matrix, j j is a k x k zero matrix whose j-th diagonal entry is equal to 1. in addition, ej is a n-dimensional vector with all entries equal to 0 apart from the j-th entry which is set to one. finally, m n is a n x n zero matrix whose n-th diagonal entry is equal to 1 and 0 denotes the kronecker product."
"ontology evaluation (o evaluation) refers to whether the ontology has been properly evaluated. if there is no information on this issue, oops! f should be used to f http://www.oeg-upm.net/oops."
". low, when the transformation between the language of the candidate ontology and the language of the ontology to be developed is not easy; . medium, when the transformation between the language of the candidate ontology and the language of the ontology to be developed is easy; and . high, when the candidate ontology and the ontology to be developed are in the same language."
regarding pattern conformance we considered that logical and content patterns are equally important and both more important than reasoning patterns. table 4 shows the utilities for the discrete attribute values.
"multibeam satellite precoding can be categorized as the wellknown multiuser multiple-input-single-output (miso) multi group multicast problem [cit] . in other words, in a given time instant more than one user is served at each beam in a multicast fashion [cit] . the user signals are precoded at the earth station and they are transmitted to the satellite through the feeder link. the satellite routes the received precoded signals through an array fed reflector able to radiate the multi user information over a very large coverage area."
"in this slowly growing and ph sensitive acteogens oxidize propionic and butyric acids to acetate. at high partial pressure of hydrogen, it acts as an inhibitor in acetogenesis phase. also acetate inhibition of the propionic and butyric acid degradation step has been considered in numerous studies. so this can be represented by non-competitive type inhibition model."
"understandability e®ort (understand.) is an estimate of the workload involved in understanding the candidate ontology. it has been split into two subcriteria, documentation availability and quality and code clarity. documentation availability and quality (a/q document) refers to whether there is any communicable material used to describe or explain di®erent aspects of the candidate ontology, as well as the enacted development process. the documentation should explain the statements contained in the ontology for a novice target audience. 10 a discrete attribute with three possible values has been established to measure this criterion:"
"experiments in this work were conducted with the imm dataset [cit], which contains images of 40 subjects. only images with a near-frontal pose were used. these include a neutral, a happy and an arbitrary expression face images per subject. there is variation in head pose among the neutral as well as the happy faces. there is variation in both pose and lighting among the arbitrary expression faces. fig. 5 shows some example face images from the imm dataset."
"but the stochastic approach regards the time evolution similar to a random walk process which is governed by a single differential equation which is called the master equation. in the deterministic approach, given the initial concentration of various species in a free homogeneous macroscopic environment, the concentration at all future time intervals is determined by averaging random fluctuations to produce a predictable deterministic behaviour [cit] . all the elementary reactions (first, second order) follow the reaction rate law where the rate of the reaction is always proportional to the concentration of each reactant involved in the reaction. the ode solvers propagate the system's state using finite time steps. for non-linear reactions, extremely small time steps need to be adopted to keep the numerical exactness. in that case adaptive step size or implicit method is recommendable. the ode approach is empirically accurate for reaction systems where large concentrations occur and may not be adequate for systems with small concentrations. in the deterministic approach, the species population is described by a continuous state although a chemical reaction involves random collisions between individual species. also a predictable system is assumed for the reaction rate or velocity and time evolution. these two assumptions are not appropriate for a system with low concentration of species where high relative fluctuations due to stochastic effect occur. stochasticity in the state change occurs as intrinsic and extrinsic stochasticity. the intrinsic stochasticity is inherent to the system and mainly arises due to low concentration of species and extrinsic stochasticity arises due to the random variation of environmental factors."
"the generic multi-attribute analysis system 39,40 (gmaa) is a user-friendly pcbased decision support system that is intended to allay the operational di±culties involved in the da methodology. the gmaa system d has been used by the authors to support di®erent complex decision-making problems. [cit] it accounts for uncertainty 44, 45 about alternative performances and for incomplete information about dm preferences, which leads to classes of component utility functions and weight intervals. moreover, it checks inconsistencies in dm responses when assessing their preferences. the alternatives are evaluated by means of an additive multi-attribute utility function, which is used to assess, average overall utilities, on which the ranking of alternatives is based, and also, minimum and maximum overall utilities, which give further insight into the robustness of this ranking. as in most cases, the information obtained is not meaningful enough so as to de¯nitively recommend an alternative (very overlapped utility intervals are output), the system provides several tools for sensitivity analysis that take advantage of the imprecise input to reach further insight into the robustness of the ranking of alternatives."
"snt combines osnt with the commodity switch, harnessing sophisticated traffic management and qos functions from both the switch and osnt. snt can efficiently saturate high capacity links with scripted scheduling and rate-limiting capabilities. snt allows full rate packet blasting combined with fine-grained traffic shaping, applicable to many testing scenarios. feedback is provided from the dut to the source, allowing reproducible latency measurements under loads of highly selective traffic flows of varying characteristics."
"in all the above models, many species are involved in more than one reaction. the reaction kinetics is solved by formulated ordinary differential equations (ode). to determine the rate of concentration of a particular species in time, all the reactions in which the species is involved are to be included during the formulation of odes. the complexity increases with increase in number of reactions and species. to avoid this, stochastic algorithm has been applied where the state of the system is updated based on the current state of the system and the transition probability."
"reuse cost refers to the ¯nancial cost of accessing and using the candidate ontology. a continuous attribute representing the possible cost (in euros or dollars) of the di®erent ontology licenses is associated with this criterion. if the candidate ontology has a free license, then the value should be zero. otherwise the value for this criterion should be the cost of purchasing or using the license."
"in this paper, we cast the multi beam satellite precoder design optimization problem in the form of multigroup multicasting with papcs. the c-admm algorithm was used to obtain high quality, sub-optimal solutions for this non-convex and np-hard optimization problem. c-admm directly tackles the problem by first reformulating it in consensus optimization form, which results in qcqp sub-problems with a single constraint. these problems can always be solved to global optimality in an efficient manner, even when they lack convexity. we proposed a modification to c-admm which reduces overall complexity in our context while preserving the simplicity of the updates. numerical simulations reveal that the proposed scheme exhibits lower computational complexity and distributed implementation capabilities when compared to the state-of-the-art, while providing very similar performance results."
"the use of multibeam satellite systems is becoming attractive for mobile operators as a cost-effective solution for providing broad band connectivity everywhere. this is the case of viasat 1, echostar 17 and kasat satellites which employ dozens of beams for providing intern et access over a continental coverage area. due to the forthcoming ubiquitous high data rate demands, both academia and industry are investigating novel schemes in order to increase the overall multibeam satellite capacity."
"the approach has been illustrated in a use case from the buscamedia project concerning the development of an ontology in the sports domain. the gmaa has proved to be a very useful tool. it helps dms to think about the problem in more depth, and accounts for imprecision in the inputs. this makes the process less stressful for experts and suitable for group decision making. moreover, sensitivity analyses have proved to be especially useful for exploiting imprecise information about the input parameters to select a set of sports ontologies for reuse."
"taking into account the trade-o® between number of selected ontologies and the percentage of covered requirements explained in sec. 4, the set of selected ontologies are baseball, ontosem and tsinaraki. this set of ontologies covers eight out of the 25 originally stated cqs. it is important to note that the next ontology in the ranking (unspsc) increases the number of cqs covered by only one. for this reason we decided not to include it in the ¯nal set of selected ontologies."
"privacy protection ability of the proposed k-same-furthest algorithm is measured through recognition experiments using eigenface technique and the eigenface equivalent in the aam space (i.e. that is used in k-same-furthest). cropped face images showing only the region inside the outline of the aamfitted shape are used in the experiments. 70% of the subjects are used for training the eigenface or the aam space with cropped original images. in testing, all cropped original images with various expressions are used as the gallery and the deidentified images as the probes. all results reported are based on randomly selecting ten different training and gallery/probe sets and computing the average recognition rate over all configurations."
"* precision 18 is de¯ned as the proportion of the retrieved material that is actually relevant. to adapt this measure to the ontological context, it is necessary to de¯ne candidatedomainontologyterminology as the set of terms included in the candidate domain ontology and orsdterminology as the set of identi¯ed terms included in the orsd. thus, within the ontological context, precision is de¯ned as the ratio of candidate domain ontology terms listed in the terms identi¯ed in the orsd to candidate domain ontology terms. * coverage is based on the recall measure used in information retrieval. 18 recall is de¯ned as the proportion of relevant material actually retrieved to answer a search request. to adapt this measure to the ontological context, the above mentioned de¯nitions of candidatedomainontologyterminology and orsdterminology are used. thus, in this context, coverage is the ratio of the terms identi¯ed in the orsd that are listed in the candidate domain ontology terms to terms identi¯ed in the orsd."
"the demand for high end networked-systems has been steadily rising, but equipment to test such devices has remained high cost and out of reach of most academic environments. high end networked-systems today achieve over 10tbps [cit] at the top of rack (tor) switch, and scale up to 922 tbps [cit] . this is in stark contrast to the equipment available to academic networking researchers, often using software-based traffic generators or are lucky to have access to a few 10gbps ports."
"the adequacy of knowledge extraction (know extract) refers to whether it is easy to identify parts of the candidate ontology to be reused. this is basically concerned with whether a modular approach has been used to develop the candidate ontology, i.e., whether the ontology is composed of di®erent modules. a binary attribute, indicating whether or not the ontology is modular (0-no, 1-yes), was used to measure this criterion. note that there are tools for extracting modules if the ontology is not modularized."
"decision analysis (da) is aimed at structuring and simplifying the task of making hard decisions as well and as easily as the type of decision permits. 19, [cit] da is developed on the assumption that the alternatives will appeal to experts depending on how likely the possible performances of each alternative are and what preferences experts have for the possible performances. what sets da apart is how these factors are quanti¯ed and formally incorporated into the problem analysis. existing information, collected data, models and professional judgments are used to quantify the likelihoods of a range of performances, and utility theory is used to quantify preferences. da can be divided into four steps: (a) structure the problem; (b) identify feasible strategies, their impact and uncertainty (if necessary); (c) quantify preferences; and (d) evaluate alternatives and analyze sensitivity."
"documentation availability and quality is the most important attribute, followed by adequacy of knowledge extraction and adequacy of implementation language and ontology evaluation, respectively. this situation conforms to previous works 3, 46 where the most important criteria are related to the understandability e®ort, adequacy of knowledge extraction and ontology evaluation."
"carlo simulation performs risk analysis by building models of possible results by substituting a range of values -a probability distribution -for any factor that is inherent uncertain. in the gmaa system, attribute weights are randomly assigned values taking into account the elicited weight intervals, see fig. 4 . while the simulation is running, the system computes several statistics about the rankings of each sports ontology (mode, minimum, maximum, mean, standard deviation and the 25th, 50th and 75th percentiles) and outputs a multiple boxplot for the alternatives. figure 6 shows the resulting multiple boxplot (the respective statistics are shown in fig. 7) . looking at the box-plot corresponding to athlete (fig. 6) we ¯nd that the best rank for this ontology throughout the simulation is fourth, whereas its 25th percentile is sixth, its 75th percentile is eighth and its worst percentile is eighth. mode is seventh and the mean ranking is 6.9 (fig. 4) ."
"thus, the selection of the best ontologies for reuse in the development of a new ontology is a complex decision-making problem where di®erent con°icting objectives have to be taken into account simultaneously. several multicriteria decision analysis methodologies could be used to tackle this selection problem. we propose using the gmaa decision support system to facilitate this selection task. gmaa is based on multi-attribute utility theory (maut)."
"refers to an abstract model of some phenomenon in the world by having identified the relevant concepts of that phenomenon. explicit means that the type of concepts used, and the constraints on their use are explicitly defined. formal refers to the fact that the ontology should be machine-readable."
"we use the delta agc7648a [cit] (fig 1), an open compute project (ocp) tor switch with deep buffers offering 100ge uplink ports. the total i/o bandwidth is 800g and the oversubscription ratio is up to 1.35:1. the switch supports traffic management and quality of service (qos) mechanisms, including classification, marking, queuing, policing, and scheduling. hierarchical quality of service (qos) allows for traffic shaping distribution based on bandwidth available/consumed from other flows. ingress/egress policing is made possible through standard 2-rate, 3-colour policing and marking."
"where d kqn is the distance between the q-th user terminal in the k-th beam and the satellite, a is the carrier wavelength, k b is the boltzmann constant, bw is the carrier bandwidth, c~ the user terminal receive antenna gain, and tr the receiver noise temperature. the term akqn refers to the gain from the n-th feed to the q-th user in the k-th beam. it is important to mention that the matrix g has been normalized to the receiver noise term."
we illustrate how the proposed approach and the gmaa system are used to select the best sports ontologies for reuse in the development of a new ontology in the sports domain.
"only three sports ontologies ranked best across all 10,000 simulations: baseball, ontosem and tsinaraki. these matches up with the results of the average overall utilities since the three top-ranked ontologies are the same as shown in fig. 5 . their worst ranking is fourth, third and fourth, respectively. also, the best ranking for unspsc is second but the worst is seventh, whereas the best ranking for the other ontologies is fourth. consequently, we can conclude that the ranking is robust."
"most of the above methods recommend high-level steps but do not provide detailed guidelines explaining how to perform each step. however, the neon methodology 3, 9 sets out some prescriptive methodological guidelines for reusing general and domain ontologies. these guidelines cover the following activities: (1) search repositories and registries for candidate ontologies that could satisfy the needs of the ontology network under development; (2) assess whether the set of candidate ontologies are useful for building the ontology network; (3) select the best candidate ontologies for developing the ontology network on the basis of a set of criteria; and, (4) integrate the selected ontologies into the ontology network under construction. note that the neon methodology is one of the main outcomes of the neon project (life cycle support for networked ontologies), which was funded by the european commission's sixth framework programme under grant number fp6-027595."
"finally, popularity refers to whether there are any projects, applications or ontologies reusing the candidate ontology. 51 this criterion can be measured using the following discrete values:"
"we also accounted for missing performances for ontology evaluation. the performances of two ontologies, ontosem and unspsc, for this attribute were unknown. consequently, the attribute range [cit] was considered as a missing performance, as explained in ref. 52."
"ontologies are formalized vocabularies of terms that cover a speci¯c domain of interest and are shared by a community of users. one of the better known de¯nitions follows: \\an ontology is a formal, explicit specification of a shared conceptualization."
"an initial collection of criteria to be taken into account for domain ontology reuse was explained in ref. 3 . the collection included 14 criteria organized in four dimensions (reuse cost, understandability e®ort, integration e®ort, and reliability). this initial set of criteria was modi¯ed in refs. 46 and 47. in this paper, we have manually analyzed both set of criteria and decided (a) to delete some of the criteria due to the di±culty of measuring them in an objective way (e.g., development team reputation) and (b) to merge other criteria because they were slightly overlapped (e.g., availability of tests and former evaluation). as a result of this analysis, we have obtained a set of 11 criteria organized according to four main objectives, as shown in fig. 1 . in addition to the update in the quantity, we have improved the collection of criteria by means of de¯ning associated attributes to measure criteria in an objective fashion."
"note that in the ontology engineering area most ontologies are currently available free of charge (that is, have a free license). however, the ontology engineering community is discussing licensing issues, and this criterion will probably have to be taken into account in the future when there are di®erent types of ontology licenses."
"it is possible to trade-off and achieve a comparatively costeffective and efficient solution as evidenced by projects such as osnt [cit] and flower [cit] . in these situations, we often see somewhat specialised hardware, or even commodity server nics, merged with open source software effortsfor example, moongen [cit] . however, these projects have throughput limitations that do not match the benchmarking needs of next generation high throughput platforms and applications. software based packet generators, providing more flexibility, rely on commodity hardware, making the speed, precision and number of ports quite restrictive."
"note that according to the methodological guidelines on ontology reuse speci¯ed in neon methodology, we have to select a subset of the top-ranked sports ontologies that simultaneously account for many of the cqs identi¯ed for the ontology under development."
"nowadays, the sports domain is receiving growing attention from broadcasters and producers, sponsors and viewers. people are continuously consuming multimedia contents in di®erent formats and from di®erent sources using google, flickr, picasa, youtube, and so on. in most cases, people want to quickly locate multimedia contents giving a natural language description of what they want (e.g., [cit] fifa world cup featuring iker casillas and andres iniesta). to do this, multimedia contents need to be semantically described for interpretation by both human agents (users) and machine agents (computers). in particular, multimedia contents about sports can be described semantically using sport ontologies."
"in this respect, an ontology network called m3g is being developed as part of buscamedia project. this ontology network covers three perspectives: it should be a multimedia, multilingual and multidomain ontology network. within the multidomain perspective, we have developed a sports ontology network following the neon methodology. in this paper, we describe the reuse process performed during the development of such a sports ontology."
"step iii -acetoclastic methanogenic stage in this step, ph -sensitive and slowly growing acetocalstic methanogens reduce acetate to methane. here, free ammonia is the inhibitor for the growth of methanogens. the stoichiometric equation and specific growth kinetic reactions are given below:"
"in the deterministic approach for reaction simulation, the time evolution of the system is considered as continuous which is governed by a set of coupled odes."
"the adequacy of naming conventions (naming conv) refers to whether the two ontologies (the candidate ontology and the ontology under development) follow the same rules for naming the di®erent ontology components (e.g., one possibility is that concept names start with capital letters and relation names start with noncapital letters). this criterion can be measured using a binary attribute with value 1 when the candidate ontology and the ontology under development follow the same naming conventions and value 0 when they do not."
"activity 2. domain ontology assessment. the objective of this activity is to ¯nd out if the set of candidate domain ontologies are useful for developing the ontology network. the input for this activity is the set of domain ontologies output in activity 1. in this activity, the ontology development team should remove the domain ontologies that are unsuitable for reuse from the set. to carry out this activity, the methodological guidelines recommend analyzing the domain coverage in order to decide whether a particular domain ontology is useful. this means studying whether the domain ontology totally or partially covers the requirements identi¯ed in the orsd of the ontology network that is being developed. to do this, the methodological guidelines propose the following actions:"
"various models have been developed based on five major categories: models considering: (a) non-ionised volatile fatty acids and total vfa inhibition; (b) h 2 as regulator for vfa production; (c) ammonia inhibition. the growth of methanogenic population is greatly affected by un-dissociated acetic acid, un-ionised vfa and total vfa which cause a drop in ph. several models have been developed taking the substrate (un-dissociated acetate and vfa) as inhibition factor [cit] . the factor which regulates the amount of fatty acids generation is the liquid phase redox potential which is expressed as a function of h 2 partial pressure. due to sudden increase in organic loading, the accumulation of vfa takes place, since acetogens grow at a slower rate than the acidogens. this will increase ph which in turn the h 2 partial pressure is increased. this will cause further accumulation of acids and thus methane generation is reduced. few models have been developed based on the h 2 partial pressure as inhibition factor [cit] and manure as substrate and generated ammonia as inhibition factor [cit] . ammonia inhibits the methanogenesis process, thus acetic acid is accumulated. this in turn inhibits acetogenesis process and thus the total vfa accumulates. the reduction in ph causes decrease in ammonia concentration and the inhibition of methanogenesis process. thus the ammonia inhibition is a self-regulatory."
"quantifying preferences involves assessing single attribute utilities that represent dm preferences concerning the possible ontology performances. we accounted for imprecise information in the assessment process. this led to classes of k http://www.eccma.org/. ontosem and unspsc are not implemented using the standard rdf understood by jena, which is the java framework for building semantic web applications that supports oops!. for this reason, oops! was not able to evaluate such ontologies. experts provided imprecise values to assess the component utility function for code documentation, see fig. 2, whereas a decreasingly precise linear utility function was assigned to ontology evaluation. table 3 shows the imprecise component utilities identi¯ed for attributes on a discrete scale, excluding pattern conformance."
"note that the accuracy of weight elicitation methods decreases with the increase of the number of criteria. however, this is not the case for the problem under consideration since only 10 criteria were considered and weights were hierarchically"
"the key aspect of this update is that it admits a closed form solution given by the euclidean projection of ekq +l onto the constraint set of (23), which can be succinctly expressed as (24) where itl' (.) denotes the euclidean projection onto the convex set"
"as already mentioned in sec. 2, the aim of the domain ontology selection activity is to ¯nd out the most appropriate domain ontologies for reusing in the development of a new ontology network. to identify the most suitable domain ontologies, several con°icting objectives have to be taken into account simultaneously. that is, the candidate ontologies should be analyzed with respect to a set of criteria and attributes. after such an analysis, a table with the ontology performances is obtained. the values in this table are then introduced in the gmaa system, which should be con¯gured with the particular preferences depending on the situation (which attributes are most important and which nature have them). gmaa obtains a ranking of the candidate ontologies. the approach we propose is to select those domain ontologies that (a) have better score in the ranking and (b) cover the largest possible number of requirements, having the set of selected ontologies as minimal as possible. thus, this approach involves a trade-o® between the number of selected ontologies and the percentage of requirements covered by such ontologies."
"in snt, traffic generation and monitoring is conducted by the low cost, open source osnt [cit] platform, while traffic management and amplification is done in a commodity switch [cit] ."
"master equation describes the transition of a system from one state to another state using probabilistic formulations. the state of system is determined by incoming and outgoing transitions. since the state change of a chemical reaction system occurs in a discrete number, the probability to find the current state of the system can be described according to the master equation approach. in the master equation for reactions, all states are represented by a discrete number of molecules and transition intensities are given by reactions per second. various stochastic simulation algorithms (ssa) such as gillespie direct method, tau leap methods, slow reaction method and adaptive tau leap method to generate the trajectories by evolving the reaction type and the time of occurrence of the reaction through the probability distribution [cit] . it was developed based on the assumption that the reaction system is well mixed and homogeneous. in this work, an explicit tau leap method is adopted for reaction simulation. in gillespie's tau leap method, leaping occurs over sub time intervals τ. the number of reactions occurring figure 2 flowchart of gillespie's tau leap method [cit] ."
"where a e ir kqxkq is diagonal matrix whose diagonal entries are the atmospheric fading terms corresponding to the q-th user in the k-th beam. matrix g e ir kqx n takes into account the rest of the gain and loss factors. its (kq, n)-th entry can be described as follows"
purpose of use (purpose use) refers to the (subjective) idea that the candidate ontology's end use can a®ect the perception of the ontology's reliability. a discrete attribute with three possible values was established to measure this criterion:
"the remainder of the paper is structured as follows. section 2 reviews the k-same framework and points out the drawback shared by all existing k-same solutions. section 3 describes a new k-same solution and proves that this new solution can always achieve zero recognition rate. section 4 evaluates the proposed algorithm's ability to protect privacy and compares it to that of the existing k-same solutions. finally, the findings of this work are summarized and further discussed in section 5."
"to avoid members of a cluster become the closest original to the center of the other (i.e. the situation illustrated in fig. 1 ), overlapping must be avoided between ۱ and ۱ . whenever a new member is added to a cluster, k-same-furthest checks to see whether overlapping is caused by this new member (lines 14 and 23). if so, this new member is removed from the cluster and the clustering loop for both ۱ and ۱ is stopped, as this new member is the closest to the cluster and therefore adding any other remaining face to ۱ or ۱ would cause even more overlapping between the two clusters. if clustering stopped before ۱ and ۱ has been assigned k faces each, faces closest to the center of each cluster are selected and added to the cluster to fill up the gaps. however, the centers of ۱ and ۱ are calculated using only those members that are added to them by the clustering process (i.e. before overlapping appears)."
"speci¯cally, the gmaa system computes which of the nondominated alternatives are potentially optimal, 58^60 that is, which are the top-ranked alternatives for at least one combination of imprecise parameters, i.e., weights and component utility functions. as soccer is the only dominated ontology, it is the only alternative that can be discarded on the basis of this sa, and further analysis is required to make a ¯nal selection."
"in this work, four anaerobic microbial groups are considered for degradation: (i) glucose fermenting acidogens; (ii) propionic acid degrading acetogens; (iii) butyric acid degrading actetogens, acetoclatic methanogens and (iv) hydrogenotrohic methanogens."
"recent advances in both camera technology and computing hardware have highly facilitated the effectiveness and efficiency of image and video acquisition. this capability is now widely used in a variety of scenarios to capture images of people in target environments, either for immediate inspection or for storage and subsequent analysis/sharing [cit] . these improved recording capabilities, however, have ignited concerns about the privacy of people identifiable in the scenes. [cit] formally declared privacy protection as a human right. [cit] data protection directive of the european union (directive 95/46/ec), which demands the deployment of appropriate technical and organizational measures to protect private information in the course of transferring or processing such data. this legal requirement along with ethical responsibilities has restricted data sharing and utilization while various organizations may require the use of such data for research, business, academic, security and many other purposes. to comply with the regulations, de-identification has become the focus of attention by many organizations with the ultimate goal of removing all personal identifying information while protecting the utility of the data."
"where x^j is the performance of ontology o; for attribute xj; uj(xij) is the utility associated with value x^j, and wj are the weights of each attribute. as we have accounted for imprecise information about the dm preferences and uncertainty about the ontology performances, the additive model is then used to assess average overall utilities and minimum and maximum overall utilities, see fig. 5 . average overall utilities (vertical line) are obtained by taking into account the mid-points of the performance intervals in the respective attributes (or the precise performance if applicable), their respective average component utilities and the average normalized attribute weights. to assess the minimum overall utilities, the system takes the lower end-points of the imprecise attribute weights, the lower endpoint of the performance intervals if the respective component utility function is increasing, or the upper end-point if it is decreasing (or the precise performance if applicable), and the lower utilities in the imprecise utilities corresponding to the above performances. the ranking of alternatives is based on the average overall utilities, and the minimum and maximum overall utilities provide further insight into the robustness of this ranking."
"as a future research line, we propose to undertake a further analysis of the criteria and attributes used to measure the candidate ontology performances. for instance, the 21 common pitfalls detected using oops! may not be equally important, and the number of times each pitfall appears in the ontology is another factor to be taken into account. thus, it might be worthwhile considering a weighted aggregation of the number of the di®erent types of pitfalls, where it would be necessary to elicit the relative importance of the pitfalls. a simpler approach would be to build pitfall subsets that account for equally important pitfalls, eliciting the relative importance of each subset and counting the number of times pitfalls from each subset appear."
"in this section, we propose a maut approach for selecting domain ontologies for reuse in the development of a new ontology network. in this general approach, we use the gmaa system to support the selection."
"apart from the well-known challenges in terrestrial multiantenna systems [cit], multi beam satellite systems suffer from additional ones [cit] . among them, the high number of transmit antenna elements and beams limit the precoding designs due to optimization complexity. for instance, current echostar 17 deploys 60 beams, requiring optimization of a complex vector of 3600 components."
"in this model, the kinetic parameters (µ max, k s ) which describe the microbial processes are able to predict the conditions of maximum growth and when the activity will cease. the main disadvantage of this model is that since the kinetic parameters (µ max, k s ) vary with substrate, one set of parameters cannot describe biological process at short and longer retention time. to overcome this limitation, first -order models are used [cit] ."
"biochemical reactions and their kinetics in the anaerobic digestion system were assumed to follow first order reactions (hydrolysis), monod type kinetic reactions and inhibition reaction. the complex particulate waste from industries or household is first disintegrated into carbohydrate, protein and lip (both particulate and soluble inert material). during hydrolysis by extra cellular enzymes (hydrolases), monossaccharides, amino acids and long chain fatty acids (lcfa) are formed. all these bio-chemical extracellular steps were assumed as first order [cit] . the first order kinetic model is an empirical relation, which assumes that the hydrolysis rate is a linear function of the available biodegradable substrate at a certain ph and temperature. the acidogenic bacteria turn the products of hydrolysis into simple organic compounds such as short chain volatile acids (va), e.g. propionic, formic, lactic, and butyric and alcohols such as ethanol, methanol, glycerol and acetone. then two types of acetogenic mechanism can occur [cit] (a) acetogenic hydrogenations and (b) acetogenic dehydrogenations. in acetogenic hydrogenations, the organic acids formed are subsequently converted by acetogenic bacteria to acetate as the main product. acetogenic dehydrogenations include the anaerobic oxidation of volatile long and short chain fatty acids. in this reaction, acetate is formed from the separated carbon atoms. during this process, due to high hydrogen partial pressure, oxidation process can be inhibited. so the hydrogen produced by these organisms is consumed by a hydrogen-utilizing methanogenic group and acetate by an aceticlastic methanogenic group. almost the 64-70% of methane production is from acetate. methanogenic bacteria are very sensitive to ph, temperature, loading rate and other compounds. all the substrate uptake reactions are intracellular reactions and they are modeled using monod kinetics reaction (single monod, double monod and also with competitive and non competitive reactions) methane is considered to be water insoluble, whereas the carbondioxide is partially soluble and partly escapes into gas phase. when temporary accumulation of volatile fatty acids occurs, the ph of the digester is reduced. this will increase the concentration of unionized vfa in the system. this will inhibit methanogenic activity. inhibition function includes ph, hydrogen and free ammonia. hydrogen and free ammonia inhibition can be represented by non-competitive reaction whereas ph inhibition can be represented by empirical equations. the inorganic nitrogen uptake is represented by competitive secondary monod-kinetics, where the prevention of growth due to limitation of nitrogen and competition for uptake of butyrate and valerate occur."
resents the electronic commerce code management association unspsc code de¯nition. this ontology includes an equipment-based taxonomy of sports. it is available at http://www.cs.vu.nl/^mcaklein/unspsc/unspsc84-title.rdfs and implemented in rdf(s).
"finally, we are also going to analyze the possibility of providing rdf-based annotations (as an omv extension or similar approach) for the ontologies analyzed following the approach presented in this paper."
step iv -hydrogenotrohic methanogenesis there may be growth limitations due to deficiency of co 2 in the reaction system due to digestion of propionic and butyric acids by acetogens. so dual substrate form of monod equation can be applied to represent the specific growth rate of hydrogenotrohic methanogens.
". check whether the purpose established in the orsd and the purpose of the candidate domain ontology are similar. the results of this informal checking can be `yes' if both purposes are similar (e.g., when the purpose of both ontologies is to provide a consensual knowledge model of the employment domain that can be used by public e-employment services), `no' if both purposes are not comparable (e.g., when the candidate ontology was conceived for use for job searching purposes and the new ontology is going to be used for gathering employment statistics), or `unknown' if information about the purpose of the candidate ontology is unavailable."
"consequently, the number of consensus variables is reduced from kq + n to kq + 1. we note that while our proposed modification decreases overall complexity, it does not come at the expense of the simplicity of the updates. the overall algorithm is depicted in algorithm 1."
the adequacy of the implementation language (imp language) refers to whether the languages (of the candidate ontology and the ontology under development) are the same or are at least able to represent similar knowledge with the same granularity. a discrete attribute with three possible values was established to measure this criterion:
"finally, reliability refers to an analysis of whether ontology developers can trust the candidate ontology for reuse. reliability is rated by ontology evaluation, purpose of use and popularity."
". -partially, by analyzing if the essential terms for the new ontology development appear in the candidate domain ontology to be reused. 16, 17 -again partially, by calculating the precision and coverage of the terminology of the candidate domain ontologies with respect to the terminology included in cqs."
"the ontology development team analyzes the set of candidate domain ontologies according to the above-mentioned criteria. as an output of this analysis, the team should ¯ll in an assessment table modeled on the template shown in table 1 ."
"based on stoichiometric values, each species is apportioned with theoretical numbers of particles based on the assumed mass. the fluctuations which occur due to stochastic nature of simulation can become less pronounced either by repeating the simulation with less number of particles and computing the mean or by assuming more number of particles. according to the theoretical statistical physics, the fluctuations in the system are inversely proportional to the square root of the number of particles involved in the simulation. so in this work, more number of particles is assigned for a particular species in which the substrate glucose is assigned with theoretical number of particles proportional to the concentration in the ratio of 1:5. figure 3 shows flowchart of processes involved with species degraders."
"quantifying preferences also involves eliciting weights that represent the relative importance of criteria. weights were elicited along the branches of the hierarchy using a method based on trade-o®s, 19 accounting for imprecise responses to the probability questions put to dms. this leads to intervals rather than precise weights. figure 3 shows the average normalized weights across the objective hierarchy. then, the attribute weights used in the multi-attribute additive utility model are assessed by multiplying the elicited weights in the path from the overall objective to the respective attributes. figure 4 numerically and graphically shows the average normalized weight and normalized weight intervals for the 10 attributes established in the lowest-level objectives for the selection of sport ontologies."
"when more number of reactions are involved in a biochemical processes, the rate of change of concentration of species is determined by evolving a rate equation for each species. as the number of species increases, the number of rate of reactions also increases to evolve the change in the concentration of species. but it is a complex process. the difficulty in formulation of ordinary differential equation (ode) for reaction simulation can be overcome using gillespie's algorithms where the reactions can be represented in the form of matrix which involves the stoichiometric constants and the reaction rate constants. stochastic algorithm (sa) has been identified as a better solution where multiscale concentration species are involved in reactions such as sorption, precipitation, degradation, sequential and parallel decay reactions etc. in this paper, the feasibility of applying sa (gillespie tauleap method) in simulation of various bio chemical processes occurring during anaerobic degradation of complex organic matter has been studied."
"the complex organic matter which is called substrate is converted into simpler form through various steps by living cells called biomass. these cells grow at suitable environmental conditions of ph, temperature etc. they interact with the environment and substrates in a complicated way. the reaction kinetics of growth and decay of biomass and conversion of substrates from one form to another are detailed in the following sections."
"as mentioned in sec. 1, the selection of the most suitable domain ontologies for reuse in the development of a new ontology is a complex decision-making problem where di®erent con°icting objectives, like understandability e®ort, integration e®ort, and reliability, have to be taken into account simultaneously. there is no agreement about which is the best methodology for all decisionmaking situations. as it is hard to determine which aspects should be considered to select an appropriate approach, see ref. 30, the decision usually depends on the problem in question, the model that decision makers are most comfortable and familiar with or like most. according to linkovet al., 31 selecting an approach from the available methods may be itself an expression of subjective values or a purely pragmatic choice (such as familiarity or perceived ease of implementation). the ¯ndings concerning experiences gathered about the application of mcda methods over the years to various domains and cases follow:"
"anaerobic digestion (ad) is the process by which the complex form of organic matter such as carbohydrates, fats and proteins are converted into simpler form by the cells of microorganisms in the absence of oxygen. energy production, high organic loading and low sludge production are major advantages of ad process. the energy produced can replace fossil fuel use, and also has positive effect on reduction of global warming. modeling is a powerful tool which can be applied to simulate various processes occurring in the digester. models are applied for parameter estimation also. using the simulation results it is easy to predict and avoid digester failure. the modeling results give useful guidelines for the design of the digester also. the reaction system in an anaerobic digester is complex with many sequential and parallel steps. the reactions can be biochemical or physicochemical in nature which involves species of higher and lower concentrations. a stochastic approach can be applied to simulate these reactions in exact manner. the complex organic matter which is called substrate is converted into simpler form through various steps by living cells called biomass. these cells grow at suitable environmental conditions of ph, temperature etc. they interact with the environment and substrates in a complicated way. generally the biochemical processes include acidogenesis, acteogenesis, and anaerobic oxidation of volatile fatty acids, methanogenesis and extracellular hydrolysis step. the reaction kinetics of growth and decay of biomass and conversion of substrates from one form to another are detailed in the following sections."
"code clarity refers to whether the code is easy to understand and modify, that is, if the knowledge entities follow uni¯ed patterns and are intuitive.10 it is advantageous to use the same pattern for sibling de¯nitions, thus improving ontology understanding and making it easier to include new de¯nitions. 48 clarity also refers to whether the code is well documented, that is, if it includes clear and coherent de¯nitions and comments for the knowledge entities represented in the candidate ontology. the di®erence between this criterion and the documentation availability and quality is that code clarity refers to the element de¯nitions and comments inside the ontology code, whereas the documentation availability and quality of the documentation refers to external documentation (papers, manuals, etc.)."
"in contrast to k-same-closest algorithms, the proposed ksame-furthest algorithm de-identifies faces based on the faces that are furthest away from them and hence maximizes identity loss, achieving the perfect privacy protection regardless of the value of k."
"although tsinaraki is the top-ranked sports ontology, its maximum utility is lower than for ontosem and very similar to the value for baseball and unspsc. additionally, there is a big overlap of overall utility intervals of these ontologies. on other hand, the other sports ontologies appear to be inferior to the four topranked ontologies since their maximum overall utilities are clearly lower. sensitivity analysis (sa) could provide further insight into the selection of the subset of sports ontologies."
"note that the approach presented in this paper can be easily adapted for the selection of nonontological resources (e.g., thesauri, glossaries) and ontology design patterns. preliminary adaptations have been performed as part of the buscamedia project."
"section 2 summarizes the methodological guidelines proposed in the neon methodology for reusing domain ontologies in the development of a new ontology. in sec. 3, multicriteria decision analysis approaches for reusing ontologies are overviewed. in sec. 4, we propose a maut approach for selecting domain ontologies for reuse in the development of a new ontology network. we illustrate the proposed approach for selecting the best sports ontologies for reuse in the development of a new ontology in the sports domain. finally, some conclusions and future research lines are provided in sec. 5."
"viewing the education as a service makes the students the core costumers, which is a natural consequence of taking marketing in higher education? these main marketing activities are in support of recruiting and retention efforts [cit] . university follows the practice of enterprise to highlight customer needs and market competition."
"in the earlier sections, we assume that wireless mesh networks use multiple transmission channels together with careful channel planning such that interferences among active transmission links are minimized. however, when only one wireless channel is used, wireless interference becomes crucial. here, we investigate the performance of our proposed scheme when there is only one wireless channel for transmission. we use the protocol model [cit], which is a simplified version of wireless interference model, to define the conditions for a successful wireless transmission. in this model, each node n i is equipped with a radio module with a transmission range r i and a potentially larger interference range r i . a transmission on link (n i, n j ) with the physical distance of d ij will be successful if two conditions are satisfied as follows."
"different unicast flows can be combined to reduce the use of network resources when there are bottlenecks in the network. our goal is to apply cnc as much as possible while guaranteeing the qos of unicast flows of different data sublayers. however, if combining unicast flows for cnc leads to a violation of the qos requirement, cnc will not be performed and unicast flows will be separately transmitted."
"step 8: for each x structure in step 7, use (26) and (27) to compute the reliabilities of two unicast flows. if the unicast flows travel through the previous a-b structure, the reliabilities of (26) or (27) will be modified by multiplying the successful transmission probability of the link l c n from the a-b structure. if the unicast flows travel through the previous y structure, the reliabilities of (26) or (27) will be modified by multiplying (1 − p e bc ) or (1 − p e ac )(1 − p e ad ) in the y structure, depending on the unicast flows. if the unicast flow travels through the a-b as well as the y structures, both modifications are adopted."
"this property ultimately arises from the central limit theorem. consider a chain of n neurons where every neuron is modeled by the same synaptic kernel k(t). that is, the activity of every neuron g i (t) is the convolution of its synaptic kernel with the activity of the previous neuron."
"this last is essential due to help during the decisionmaking process with insightful information that cooperates improving the decision effectiveness even in the different educational field such as resource planning, teacher's management, curriculum design, and related factors. although the visualization nodes provided by knime are efficient to represent the results for this study, a future goal for this research includes the development of a computational tool for deans and university administrators. specific technician knowledge is required to understand the reports and graphics on knime, and a more straightforward interpretation of the academic data and predictions using human computer interaction techniques would significantly support the decisions making process."
"there are other neural circuit models that produce sequentially activated neurons, but to our knowledge, the present model is the first one that has the additional feature of scale-invariant neuronal activity. however, functionally identical models with different biological realizations of the same equations might also be possible. for example, rather than implementing long functional time constants via intrinsic currents, one could construct an analog of equation (2) using recurrent connections."
"depict insightful information through an intentional graphic, improve not just the data' understanding through the recognition of trends and hidden patterns, but enhance the actions taken from the data processed. for instance, figure 7, allows heis administrators to observe how the average grade and the number of lost subjects impact the fact that students get graduated or not, to mention some of the features that are analyzed from the data set. although some graphics provide by knime are useful, we propose in the next section significant improvements to succeed in the information launch and discuss potential efficient metrics that could be added."
"the search for cnc structures is executed by the central controller. the optimal paths obtained from section 4 are investigated over all links to find a-b, y, and x structures. the central controller detects each cnc structure by examining whether a group of links match with the considered cnc structure. if a group of links match the underlying cnc structure, these links must convey two unicast flows having the same transmission rate."
"as illustrated in figure 2 ., although the same classification algorithms as related works are used, they differ from our research. (i) our data comes from a face-to-face educational model. (ii) due to more features are included, algorithms architecture differ. (iii) the stakeholders are directors and deans from heis which have particular visualization results needs, and have not been adressed before. (iv) strategic decisions are supported when the right information is given to the high chain management as would be exposed in section 3."
"specifically, we can explain each constraint as follows. constraint (14b) is the flow conservation constraint. constraint (14c) is the reliability constraint. constraints (14d) and (14e) are the wireless link scheduling constraints. constraint (14f) is the layered data constraint. a transmission path of a higher sublayer will be chosen only if a transmission path of a lower sublayer has been selected. constraints (14g), (14h), and (14i) are the feasible values of f"
the computed reliabilities at the end of step 9 yield the final reliability of sublayer l have the qos guarantees since their end-to-end successful transmission probability are equal to or greater than their qos requirements.
"each link has a normalized positive integral capacity or transmission rate denoted by c l . a normalized unit capacity can be translated into bits per second. the probability of a packet loss of link l is denoted by p l, where 0"
"1. packet ρ j belongs to a flow that has traveled through n i, where n i keeps the packet in its memory for a period of time for the purpose of cnc. this situation, known as the nonopportunistic listening cnc operation, is applicable to the a-b structure 2. node n i receives packet ρ j by overhearing the packet from a transmission of its adjacent node. for the network coding operation, node n i keeps the packet for later decoding of an encoded packet. the operation, called the opportunistic listening cnc operation, is used by the x structure."
"step 4: in r step 5: for each y structure in step 4, use (22) and (23) to compute the reliabilities of two unicast flows. if the unicast flows transverse through the previous a-b structure, the reliabilities of (22) or (23) will be modified by multiplying the successful transmission probability of the link l c n from the a-b structure. this modification is needed since the reliability of the current cnc in the y structure relies on the reliability of the cnc in the a-b structure."
the cnc establishment (cnce) protocol is presented in this section and is used to decide whether or not cnc will be performed on different unicast pairs at intermediate nodes. the decision criterion is derived based on the qos requirement of transmitted layered data.
"a routing scheme that provides qos guarantee for heterogeneous layered unicast transmissions in multirate lossy wireless networks with and without cnc was investigated and compared to its alternatives in this research. the path of each layered unicast flow is obtained by solving a constrained linear optimization problem subject to the qos requirement of each flow. the associated cnce algorithm decides whether or not cnc will be performed at an intermediate node by considering the a-b, y, and x structures in the network. it was demonstrated by computer simulations that the proposed qos-aware routing scheme yields better throughput and higher channel use efficiency with the qos guarantee on heterogeneous unicast flows."
"the values of τ * in e f are controlled by the values of s in f. it remains to specify the distribution of values of s and thus τ * . in order to preserve scale-invariance, equate the information content of adjacent nodes [cit] and enable e f to implement weber-fechner scaling [cit], we choose the values of τ * to be logarithmically spaced as shown in figure 3c . this is equivalent to choosing the number density of s to go down like s −1 ."
"while recurrent connections with appropriate eigenvalues would implement this property perfectly well, here we follow previous work that uses known single-cell properties to build long time constants. [cit] developed a computational model of single neurons that uses a calcium-activated nonspecific (can) cationic current to achieve decay time constants up to tens of seconds under a realistic choice of parameters. here we utilize that same model as the neural realization of the f nodes."
"since a shorter path can result in a smaller number of transmissions used for each flow, leading to more efficient channel utilization and shorter delay in wireless networks. based on the above discussion, we first select the following objective function:"
each tree in the forest gives a classification and ''votes'' for that class. the forest chooses the classification having the most votes (over all the trees in the forest). rf regression predictor has the form:
"vicente gacía-díaz received the ph.d. [cit], where he is currently a software engineer and an associate professor with the department of computer science. he has supervised over 60 academic projects and published over 70 research papers in journals, conferences, and books. his research interests include machine learning, natural language processing, model-driven engineering, and domain-specific languages. he is also part of the editorial and advisory board of several journals and has been an editor of several special issues in books and journals. claudio camilo gonzález received the m.sc. degree in urban development from san buenaventura university, colombia. he is currently pursuing the ph.d. degree in educational technology with lleida university. he is also a system engineer and a member of the byte and desing research group, unad university, colombia. his research interests include education, e-learning, and decision support systems."
"there are 9 groups of layer i neurons in total, each neuron within a group has the same time constant. within each group there are 12 layer i neurons with the same parameters. for every three of them, their psps are summed up and sent as input to one layer ii neuron, as shown in equation (22)"
"note that the proposed coding rules may not be optimal in terms of the number of channel uses in some network topologies. other cope structures, which consist of more than two information flows and accordingly establish more complex encoding/decoding structures than ours, have different coding rules and potentially provide more reduction in the number of channel uses. however, these complex cope structures are rarely seen in practical networks since they require overlapping transmission ranges of more nodes to form their structures compared to the basic local structures in our work."
"a biological constraint on the neural circuit that can cause deviation from scale-invariance is the input-output function (f-i curve) of the layer ii neuron. only when the layer ii neurons are in their linear regimes can they faithfully relay the temporal information from the presynatic neurons to the output layer neurons. since we modeled the layer ii neurons as leaky integrate and fire neurons, their f-i curves are discontinuous near the threshold input value. thus some background firing is required for the layer ii neurons to be in their linear regimes. also some steady background firing for the layer i neurons would not change the scaleinvariance property, since a constant shift in f(s,t) would not affect the derivative that contributes to the inverse laplace transform."
the experiment was conducted with real data from a public university in colombia. the data source contains information from 6100 engineer students. [cit] because engineering careers take ten semesters to graduate and therefore will not address the supervised algorithms needs. more than 55200 records were available to analyze.
"in this section, the effects of performing cnc in lossy wireless networks on the reliability of layered data transmissions are investigated. a terminal node in each cnc structure can reproduce its desired packet if it has the coded packet and all the other involved noncoded packets. in addition, to encode a packet successfully for a unicast flow that passes node a and then node b, an intermediate node needs all required noncoded packets from other we examine the participating links of the basic cnc structures in the following."
"the purpose of the tactic level is to identify and execute the detailed plans made at the strategic level. generally, deans work together with the head of departments or programs directors to achieve the above planning. intermediate directors coordinate resources usage efficiently, providing management and planning at specific times. once the strategic planning is accepted, the tactic level is in charge of its implementation and control. thus, quality assurance is an essential task at this level. this middle management performs decisions such as the number of students per teacher or curriculum changes. algorithms such as naïve bayes and artificial neural network have been used to ensure teaching, assessment and timetabling quality."
"we observed that each of the levels in the pyramid have a decision-making process that, from top to bottom, affect a more significant portion of the community. although some software applications can support decision-making processes, higher levels generally work with the information provided by the operational stage and data is not analyzed and visualized easily to support decisions at high stages. norman and ahmed identified the central software misfit in higher education planning software. some of the cases they stated are poor consultant effectiveness, poor-reliance on heavy customization, reduced it infrastructure, poor project management effectiveness, poor management support, too tight project schedule and poor knowledge transfer [cit] ."
"to evaluate the performance of the compared machine learning algorithms, we use the area under the curve (auc) as the evaluation criteria. auc is a popular measure for ranking class performance of the learned classifiers [cit] auc is calculated as follows:"
"the uppermost level defines the policies and strategies for the organization integrating the primary goals and actions into a cohesive whole. the higher level of the institutions is the more ambitious in their strategic planning [cit] . the managing positions are frequently represented by the governing board, rector and deans [cit] ."
"this classification algorithm could be confusing by its name. it is used to estimate discrete values (e.g., binary values) based on a given set of independent variables."
"to perform the pre-processing of data and all the machine learning algorithms we used knime 3.4.0 (konstanz information miner) analytic platform [cit] . knime is open-source software, developed in java which allows etl processes (extraction, transformation, and loading) in addition to various modular components for machine learning and data mining."
the activity of the f nodes is transformed by a second layer of e f nodes. the e f nodes are in one-to-one correspondence with the f nodes. at each moment the activity of the f node labeled by s is transformed in the following way:
"as our case study is a public university, data policies are strict. although our research was restricted by their dataprotection policies, and we lack information about students' gender or age among other socio-demographic data, for the most part we use students' academic records to held the graduation rates prediction. we believe the inclusion of sociodemographic and socio-economic data would be worth to analyze in the future. however, in this study the academic information obtained is efficient to analyze the insightful outcomes."
"therefore, the usage of efficient computational algorithms is vital to enhance this process. through the latest years, machine learning has shown its outstanding capacity for pattern recognition and predicting outcomes for diverse datasets despite the field. most of the work done in machine learning has focused on supervised algorithms. their main strength is that they produce models that we can incorporate in the decision-making process [cit] . in order to choose the most suitable learning algorithm, a clear objective is required, and an analysis of previous data must be performing. thus, the feasibility of using a supervised algorithm volume 7, 2019 this work is licensed under a creative commons attribution 3.0 license. for more information, see http://creativecommons.org/licenses/by/3.0/ over a not-supervised algorithm can be determined. afterwards, depending on the viability of each, a choice between classification or regression algorithms needs to be made. even though many studies have used ml algorithms to identify students; our research differs from the existing ones."
"figure 1 sequentially activated neurons in the brain. each row on each heatplot displays the normalized firing rate for one time cell. red corresponds to high firing rate, while blue corresponds to low firing rate. the cells are sorted with respect to the median of the spike time in the delay interval. two features related to temporal accuracy can be seen from examination of the heatmaps. first, time fields later in the delay are more broad than time fields earlier in the delay. this can be seen as the widening of the central ridge as the peak moves to the right. in addition the peak times of the time cells were not evenly distributed across the delay, with later time periods represented by fewer cells than early time periods. this can be seen in the curvature of the central ridge; a uniform distribution of time fields would manifest as a straight line. set of activations is not scale-invariant, as they are not of the same functional form. figure 2 shows the actual and scaled neuronal activity in the chain. the rescaled neuronal activity becomes more concentrated for neurons that are activated later."
we discuss the objective function as well as the set of constraints in the following subsections. and normalized transmission rate t. one of our objectives is to maximize the total throughput while taking the reliability into account. the information value of the sublayer l
", and a j, respectively. searching for a set of paths that maximize the throughput of each layered unicast session requires high computational complexity because all feasible links must be considered. to reduce the complexity of the problem, the objective function in (14a) is modified as"
"although in real world governance exhibits variations a mixture between these three categories, it is quite usual universities to prefer and implement the academic and bureaucracy styles [cit] . among heis are specific differences in the mission and management strategies, for instance, private universities are more market-oriented and action-oriented while public universities focus on the roles of students and alumni in the society. however, all of them work on behalf of students' success, and heis make their decisions to ensure it. therefore, in order to classify the kind of decisions at heis, and considering their primary goal between the types of heis, we follow the hierarchic structure to divide work vertically according to decisions' responsibilities, which resemble an organizational pyramid structure [cit] ."
"step 9: if the computed reliabilities from step 8 of the x structure satisfy qos requirements of these two unicast http://jwcn.eurasipjournals.com/content/2014/1/81 flows, perform cnc on the two unicast flows. otherwise, they will be separately transmitted."
"the factors in the parentheses account for the fact that the accuracy of the slope further away from the point s 0 is a less accurate estimate of the derivative. 1 by modulating α(t) with velocity, the model can produce place cells [cit] . 2 note that this definition of τ * differs from the notation in some previous papers where τ * was defined to be negative. we adopt this convention for convenience here. f(s, t) ."
"to keep the firing rates of the layer ii neurons in a reasonable regime, the postsynaptic potentials (psps) coming from three layer i neurons from the same group are used as the input to one layer ii neuron. layer ii neurons are modeled as leaky integrate and fire neurons. the time evolution of the layer ii neurons is given by equation (22), where e int is the resting potential for the layer ii neurons, τ int is the membrane time constant, psp pre,i is the psp coming from layer i neuron i and a noise term u (0,1) represents a random number drawn uniformly from (0,1). we added a background current to achieve a 0.42 mv voltage offset so that the layer ii neurons are in their linear regime, an essential point to ensure scale-invariance."
(i) the data comes from a face-to-face educational model. (ii) more diverse and numerous features from data collection are included on the algorithms architecture leading to achieve a higher overall accuracy that we analyze. (iii) neither the stakeholders nor the objective goals have been evaluated before. (iv) we investigate deans and directors concerns when making academic decisions [cit] as our work driver. this article seeks to classify the decision's structure at heis and the influence of the institutional governance among them. this section is developed with the aim of depicting the impact and responsibility of strategic decisions not just in the academic context but the complete environment where the higher educational institution is located.
"the comparison of roc, accuracy, precision rates and recall rates were conducted. we observed that the overall accuracy is prevailing in random forest although the area under the curve (auc) is slightly superior in logistic regression. however, as effectiveness metric, accuracy is more significance than roc curve due to roc being insensitive to data sets with unbalanced proportion classes [cit] . regarding precision rate and recall rate, the three algorithms are similar."
"the link conveying data from node a to node b is denoted by (a, b) . in general, a wireless link connecting any pair of nodes is bidirectional. however, we can represent a bidirectional link using two directed links having opposite flow directions. for example, a bidirectional link between node a and node b can be split to two links,"
"first subsection presents data characterization, cleaning and preparation. subsequently, subsection b contextualize the three machine learning algorithms used in this work. we present their basics, method, architecture, and configuration used. the tools and metrics used are respectively indicated in subsection c and d."
"scale-invariance is also observed in the associative learning rate in animal conditioning experiments. for instance, it has been shown that the number of trials needed for animals to develop a conditioned response increases when the reinforcement latency is increased and decreases when the intertrial interval is increased [cit] . moreover, as long as the ratio between the intertrial interval and the reinforcement latency is fixed, the number of trials needed to develop a conditioned response is fixed, again indicating scaleinvariance in the animal's timing behavior."
"continuing with the research done in previous work [cit] we compare different machine learning algorithms in this paper as well as we analyze decision's structure at heis and how they are managed according to the institutional governance. having these students recognized early can allow heis governance strategic planning abilities are respecting students' exclusion policies, students' dropout rates, retention rates, strengthen programs, and a whole host of others."
"where, f t− τ * þ ð is the value of the stimulus function a time τ * prior to the present. the approximation becomes exact when k ! ∞ [cit] . because there are many units in e f τ *, tþ ð, that set of units trace out the past values of the input function such that an approximation of the entire function is available at time t. thus we can see that the set of activations e f τ *, tþ ð constitutes a faithful representation of the original stimulus function delayed by τ * . this is true regardless of the form of the function f(t)."
"a source-destination (s-d) pair transmits one base layer and two enhancement layers. we set t equal to one normalized unit which is 512 kbps. the transmission rates of the base layer, the first enhancement layer, and the second enhancement layer are equal to 2, 1, and 1 units, respectively. we set the qos requirements, which are successful transmission probabilities, to 0.90, 0.80, and 0.70 for the base layer, the first enhancement layer, and the second enhancement layer, respectively. therefore, each s-d pair transmits four sublayers, l routing scheme is obtained from the python programming language [cit] together with the pulp package [cit] and the coinmp solver [cit] ."
"psps generated by the layer ii neurons with five different time constants of the intermediate neurons are summed up and provide input to the output layer neurons, as in figure 4 . the weight matrix w l is implemented by different psp amplitudes of individual layer ii neurons, which can be computed from equations (11)- (15). we also rescaled the individual psps so that the inputs to the different output layer neurons have the same maximum. this ensures that the output layer neurons are all in their linear regimes. a biophysical mechanism to achieve such regime could be due to homeostatic synaptic scaling in which the activity of neurons regulates the magnitude of synaptic inputs [cit] . later we will elaborate that as the time constants of nearby neurons become closer (i.e., s i + 1 − s i ! 0), the receptive field will closely resemble an off-surround, on-center one."
"based on the above discussion, we can formulate the optimal path selection problem as a linear optimization using the objective function in (5) with constraints of the flow conservation in (8), the reliability in (9), and the wireless link scheduling in (12) and (13) . the overall problem is summarized in the following: maximize"
"an extensive literature review looks for the classification of the uses of information and communication technologies (ict) at heis and ongoing applications that used ml in the education field. the primary goal focuses on the comparison of three supervised machine learning (ml) algorithms that, used as predictors, would enhance decision at the strategic level. specifically, we applied decision trees, random forests and logistic regression to predict graduation rates using real data from a face-to-face model education university in south america. analysis of the roc curve and accuracy are executed as measures of effectiveness to compare and evaluate the three algorithms."
"notoriously a ''disengagement'' occurs regarding higher educational institutions and education policymakers, students, managers and their subordinates [cit] . many barriers including technological conditions, rigid governance structure or vulnerability to continuous changes in government rules, may impede the support needed by managers and university's directors when making a decision."
"behavioral scale-invariance requires that the neural system supporting behavior is also scale-invariant. recent neurophysiological recordings in behaving animals show spiking activity at specific temporal intervals by individual neurons, referred to as time cells. these experimental data provide a possible neural substrate for timing behavior and various forms of memory [cit] ."
"the managing positions are frequently represented by the governing board, rector and deans. they discuss the critical factors in strategic planning and provide guidelines for its execution. decision-making at this level impacts the entire university. for instance, one strategic decision at this level is the number of freshmen accepted each semester. the decision affects the university's resource allocation (e.g., budget, teachers, and facilities), as well as the society as more people, might access professional programs. at this level, according to relative works analysis, machine learning algorithms have not been used to aid this corporate stage."
"in regard to technological conditions, face-to-face model education still addresses significant obstacles. for example, their administrative, as well as their academic information, is stored in various silos making formats employed vary significantly [cit] . additionally, as we observe in our case study university, some of the transactional processes such as the record of students' attendance or registered graduate documents, to mention a few examples, are still done manually in paper notebooks."
"the from source node s to destination node d is equal to sublayer rate t. thus, the constraint on information flow conservation can be expressed mathematically as"
"information and communication technologies (ict) have transformed the academic field, not just what we teach (curriculum) or how we teach it (pedagogy), but how the institutions respond and manage these changes [cit] . researchers set out investigating the ict impact on education in the past, in particular in the e-learning arena."
"this mathematical framework produces a set of functions that resembles the firing rates of time cells. moreover this mechanism gives rise to time cells that are scale-invariant, which would be a desirable property for the brain to possess. however, it is not clear whether it is possible for neural circuits in the brain to actually implement this hypothesized mechanism. we will demonstrate that this is indeed neurally realistic by constructing a biologically detailed neural circuit that utilizes a biophysical model of exponentially decaying persistent firing neuron [cit] to perform the computation of this mathematical model, thereby generating a set of scale-invariant time cells."
"where γ is a tuning parameter between zero and one. the objective function in (15) gives a suboptimal solution with respect to the original objective function in (14a). the objective function in (15) may not satisfy the second property of the original objective function since it does not take into account the successful transmission probability of each link. however, the third property of the original objective function still holds, i.e., the objective function in (15) provides shortest paths in terms of hop distances satisfying both the transmission rate and qos requirement of each sublayer, which can be proven by using the similar approach to the third property of theorem 1."
"for the extended a-b structure that has two intermediate nodes, i.e., nodes c 1 and c 2, as shown in figure 5, we can generalize (17) to"
"the parallel coordinates plots just 50 records were taken randomly to this plot. black color represents not graduated students, while gray color graduated students. first vertical parallel line from left to right, represents the objective sought (graduated or not graduated), subsequently is approved subjects, followed by lost subjects and average grade. we observe that graduated students (gray lines) reach a better average grade and more approved subjects. by contrast, not graduated students (black lines) approve fewer subjects and reach lower average grades. these patterns are easily recognized thanks to machine learning algorithm."
"for example, gavornik and shouval showed that in a spiking recurrent neural network trained to encode specific time intervals, units exhibit persistent spiking activity [cit] . other neural circuits for computing the inverse laplace transform are also possible. the computation of the inverse laplace transform amounts to a suitable linear combination of inputs from cells with exponentially decaying firing rates. [cit] showed, in a detailed"
"where e ac is the incoming link to node c on r (a,b) i, e bc is the incoming link to node c on r"
"higher educational institutions are a particular type of organization of the tertiary sector. they currently hold primary responsibility for the governance and management of their finances, activities, and personnel by retaining the autonomy to decide their organizational operations. in higher education how decisions are made about institutional priorities, strategies, goals, and resource allocations, and who is held accountable for these decisions, are all functions of institutional governance [cit] ."
"even though at educational field machine learning is still emerging, its effectiveness to analyze information is notorious. through the analysis, predictions, and visualizations of information, heis' directors obtain a greater understanding of the different variables involved when making a decision. machine learning supports this process providing various algorithms suitable to the different kinds of data and the different kinds of predictions required."
"by equation (5), the connection weights should depend on the discretized kth derivative with respect to s. to write derivatives in matrix form, imagine there are three successive f nodes, with labels s −1, s 0, and s 1 . note that the first derivative of a function with respect to s 0 can be approximated as a weighted average of the slope of the line connecting the successive points on the curve."
which can be solved by linear optimization. the equivalence between these two objective functions is stated and proved as theorem 1. the objective function in (3) can be rewritten as
"and (27) where e ac and e ec are the incoming links to node c on r, and e eb is the incoming link to node b from any node upstream of e on r (e,d) i ."
"when rf receives and (x) input vector, made up of the values of different evidential features analyzed for a given training area, rf builds a number k of regression trees and averages the results. after k such trees t(x) k 1 are grown. about the architecture used, the split attribute chosen was gain radio since it did not represent a significant difference among the other options provided by knime (information gain, information gain radio y gini index). moreover, the number of models used was one hundred mainly because of the dataset size and testing results. the number of decision trees to learn of number of models set is 100."
"the x-axis indicates the false positive rate, while the y-axis indicates the true positive rate. it is clear that the auc for logistic regression (i.e., 0.9028) represented in fig. 5 is slightly higher than random forest's auc (i.e., 0.8994) and decision trees (i.e., 0.8830). one of the goals of this study is to identify the potential graduated students. comparatively, rf was the most effective and more precise in predicting student graduates. table 4 shows the results of the methods mentioned in section 4.3. in terms of the precision rate and the recall rate of the graduated class, there was little difference in the performance of the three prediction models: random forest has the highest recall rate (91.93%), followed by decision tree (91.38%) and logistic regression with the lowest recall rate (90.935 %). however, regarding the precision rates of the same class rf is the lowest (86.44%), followed by dt (86.61%) and lr (87.04%) with the highest precision rate."
"applications of machine learning (ml) in the academic field focused mainly on using supervised algorithms to predict students' behaviors with the possibility of early intervention. some authors have covered different educational problems using ml such as course planning, institutions' and teachers' quality, intervention and prediction, and learning product selection. although we focus on higher educational institutions applications, other works using and comparing machine learning algorithms in public and private schools have been developed [cit] ."
"i. using the scaling method, we transformed data by giving them values from a range [cit] 0 as a minimum and a maximum of 1."
"it is known as logit regression because it predicts the probability of an occurrence, of any event, by fitting data into a logit function [cit] . logistic regression gives linear class boundaries. due to the fact it uses an 's'-shaped curve instead of a straight line it is a natural fit for dividing data into groups. figure 4 shows an example took from azure of logistic regression to two-class data with just one feature. the class boundary is the point at which the logistic curve is just as close to both classes [cit] ."
"among the three prediction models, rf had the highest overall accuracy (84.11%), followed by lr (84.02%), while the dt had the lowest accuracy rate (83.92%). the three models had a relatively high overall accuracy rate that exceeded 83%. with the aim of comparing the impact of the first features include in each training algorithm, we test them using 6, 15 and 19 features as shown in table 5 . we observe that the overall accuracy increases as the number of features tested increase. revealing more features leads to obtaining greater accuracy. for instance, random forest accomplishes an accuracy enhancement of 4.4%. considering 10000 students and increase from 79.71% to 81.11% would represent more than 400 students correctly classified. algorithm's improvement also relies on data distribution and data set."
"therefore, a more in-depth analysis of the number of features tested, or data normalization, will be presented in a forthcoming paper. future research will also include the analysis of other effectiveness metrics such as f-measure or specificity as well as the comparison with other classification algorithms, that would be worth to analyze in other higher educational institutions. moreover, would be worth to include socio-demographic and socio-economical information about the students when analyzing the variables volume 7, 2019 that might affect the graduation rates at higher educational institutions, performing this recognition has managerial implications not just for reducing processing time but increasing reliability on its prediction."
"next, figure 7 evaluates the performances of routing schemes in terms of the number of channel uses. the numbers of channel uses of qossp-r and qos-r are significantly less than that of sp-r at all link qualities. the number of channel uses from sp-r is the highest at all link qualities although its achievable throughput increases as a function of the successful transmission probability. in other words, sp-r has the lowest efficiency of channel utilization, especially at low link qualities. qos-r has a lower number of channel uses than qossp-r both with and without cnce algorithm. qossp-r selects paths with the highest transmission reliability regardless of the number of links used to transmit bitstreams whereas qos-r chooses the shortest paths that satisfy the qos requirements."
"iii. using a stratified sampling technique, split the dataset into two subsets 70% for training and 30% for prediction, to keep the data distribution. the sampling method alleviate the effect of class imbalance problem as one of the most employed method [cit] ."
"layer i consists of 108 integrate-and-fire neurons driven by the can current described above, modeling the f nodes from the mathematical model. the set of model neurons spans 9 different values of s with 12 neurons for each value. to model the experimental finding of a logarithmically compressed timeline, the time constants 1/s were chosen to be logarithmically spaced between 2 and 50 seconds by adjusting the maximum can current conductance g can and initial calcium concentration in the can current driven persistent firing neuron model above according to"
"from a recent literature review [cit], we observed not just their efficiency but also their acceptance on the research field when contributing to decision processes."
"the model works because cells contain a slowly deactivating current that depends on calcium concentration. because this current causes spikes, and because spikes cause an influx of calcium, this mechanism can result in very long functional time constants. because the functional time constant depends on spiking, mechanisms that alter the amount of current needed to cause a spike also alter the functional time constant."
vi. finally the algorithms are executed to train the whole training set using the best values obtained for the hyperparameters in each algorithm. hyperparameters set in each algorithm as well as the architecture and contextualization of each algorithm is exposed in the next subsection.
"in definition 2. an optimal solution to the problem can be obtained by various mathematical programming tools. we select coinmp [cit], which is a c-api library that supports most of the functionality of coin linear programming (clp), coin branch-and-cut (cbc), and cut generation library (cgl) projects, to be the solver for linear programming. this is the first step of the proposed qos-aware routing scheme. these obtained optimal paths are inputs to the second step of the proposed qos-aware routing scheme as described in the next section."
"v. we execute the algorithms with the initial settings. in each k-fold we save the accuracy obtained as well as the values entered in each hyper parameter, to adjust them in each run and encounter the most suitable values for them until the accuracy reached the expectations."
", and e ad is the incoming link to node d from any node upstream of node c on r (a,b) i . for the x structure that has six links as shown in figure 3c, the reliabilities of unicast flows traveling from node a to node b and from node e to node d with cnc at node c are expressed as (24) and"
"the y structure conducts both nonopportunistic and opportunistic listening cnc operations. in what follows, we adopt these conditions as the coding rules and opportunities for the cnc establishment."
"yuri nieto received the m.sc. degree in computer science and communications from francisco jose de caldas district university. she is currently pursuing the ph.d. degree with the computer science program, university of oviedo. [cit], she was an industrial engineer with the axon and giira investigation groups, where she is currently a member. her research interests include machine learning, decision support systems, learning analytics, distributed systems, and virtualization."
the dotted lines are exponential functions; the degree to which the firing rates align with these theoretical functions confirm that the firing rates indeed decay exponentially. this is in accordance with the activity of the f nodes in the mathematical model.
"once the classification objective was set (i.e., graduated and not graduated students) and data was acquired, we conducted the following steps to build every algorithm model:"
"alternatively, any type-i model neuron with a linear f-i curve would satisfy the biological constraint imposed by scale-invariance, with or without background firing. by appropriately modeling an adaptation current, a log-type f-i curve could be transformed into a linear one [cit] ."
"while more accurate path loss models can be derived from complex analytical models or from measurements where system specifications such as the locations of access points must be known, a simplified path loss model is used because it can sufficiently capture the essence of signal propagation for the purpose of data delivering as well as interference consideration. note that the proposed cnce algorithm can also be applied when other path loss models are assumed. we perform numerical experiments by adjusting one of the following three parameters:"
"here g can is the maximal value of the ion conductance measured in mho cm 2, e can is the reversal potential of the can current ion channels and m is a dimensionless quantity between 0 and 1 that is associated with the activation of the can current ion channels."
", is based on its priority and type of application and is obtained from experiences of end users. for example, in voice over ip (voip) traffic, the packet loss rate should not exceed 5% to not affect the quality significantly. when link qualities of a wireless mesh network are in hostile conditions, the original qos requirements may not be feasible because the proposed optimization framework cannot find feasible transmitting paths guaranteeing the original qos requirements of those data layers. this infeasibility is, however, common to communication networks. the problem can typically be handled through the process of call admission control (cac), which we assume to exist but whose details are beyond the scope of our investigation. the ilp problem can be used for network resource allocation in conjunction with cac. http://jwcn.eurasipjournals.com/content/2014/1/81"
"is e bc since node c needs a noncoded packet from node b to generate the coded packet, which is obtained by performing the xor operation of a packet from node a and a packet from node b. similarly, we can derive the participating link to the flow on path r are e ac and e ad since both nodes c and d need packets from node a to generate and decode the coded packet, respectively. note that node d can receive a packet from node a through opportunistic listening."
"for the x structure, there are two unicast flows: (1) from node a to node b and (2) from node e to node d. network coding is operated at node c. the coded data packet is then broadcast to nodes d and b. transmission delays and energy consumptions of these unicast flows are reduced since the number of channel uses is reduced due to cnc. however, the reliability in transmitted data deteriorates due to the dependency on required packets in data decoding at destination nodes."
"quality in education has been discussed in one of the following themes: measure the impact of knowledge [cit], teaching quality [cit], assessment quality [cit] and timetabling quality [cit] . higher educational institutions must guarantee quality for all stakeholders: students, teachers, directors, government and society. in computer assistedlearning, the possibilities of rendering information are more numerous [cit], which improves the outcomes and allows that the data insight the educational field supports quality measurement. besides it leads us the third ict usage at higher education institutions [cit] ."
"we model a wireless mesh network as a directed graph g(n, e), where n and e are the sets of nodes and bidirectional links in the network, respectively. there are several unicast sessions in the network. each session is defined by a unique source-destination pair. let s and d denote source and destination nodes of an arbitrary unicast session, respectively. table 1 summarizes the notations used in this paper."
"due to the broadcast nature of wireless communications, a transmission of a particular node can affect transmissions of other nodes in its coverage range. since wireless channels are shared among multiple nodes, a node placed in the transmission and coverage ranges of other nodes may be interfered by simultaneous communications. in this work, we assume that the wireless interference can be managed by an appropriate channel planning [cit] . a receiver node cannot simultaneously receive more than one packet whereas a transmitter node can send no more than one packet at a given time. therefore, a wireless link scheduling technique is needed to coordinate wireless broadcasting."
"furthermore, it allows the weighting of different attributes and misclassification types and separates the data automatically to help reduce noise [cit] . when constructing decision trees, it is essential to find the best splitting point measurement (i.e., information gain, gain ratio, gini index, and entropy measure). the selection of the split attribute should directly decide the learning trend [cit] . gini index is used as a split measure for choosing the most appropriate splitting attribute for each node. the split function has this form:"
"for the y structure, there are two unicast flows: (1) from node a to node b and (2) from node b to node d. cnc is conducted at node c. in particular, node d receives a packet transmitted by node a via opportunistic listening. node c encodes each pair of packets received from node a and node b, and then broadcasts the network coded packet to nodes b and d simultaneously."
"the first subsection seeks to isolate the different uses of ict at heis with the aim of highlight the small research held for the academic decision-making process. hence, the focus was on classifying the ict used in the educational field into the following categories; e-learning, academic research, quality measurement and decision-making process."
"2. in an optimal set of paths for each unicast session, for any two paths in this set, a path having a higher reliability transmits packets of either the same or higher information value. 3. given a lossy network, the objective function yields a set of paths that has the minimum number of channel uses in the case of equal link loss probabilities."
"step 6: if the computed reliabilities from step 5 of the y structure satisfy the qos requirements of these two unicast flows, perform cnc on two unicast flows. otherwise, these two unicast flows will be transmitted separately."
we use the logarithmic throughput in (2) since a sum of logarithmic utility functions ensures proportional fairness. to avoid nonlinear optimization which demands http://jwcn.eurasipjournals.com/content/2014/1/81 figure 1 an example of defining the information value of each sublayer based on its original layer. to show the concept used for defining the same priority of the sublayers belonging to the same layer.
"academic research has been transformed by information technology due to the rapid, widespread diffusion of electronic papers, digitalization of libraries and journals, web access to information and repositories among other facilities [cit] . worldwide, researchers and enterprise leaders collaborate from different perspectives in diverse projects thanks to the capability for remote exchange and communication [cit] . the innovative advances on use of data (technical use) and elaboration and presentation of projects (academic use) enhance teachers' curriculum and university's visibility [cit] . furthermore, information technologies serve as a control mechanism for academic misconduct like plagiarism, self-plagiarism, coercive citations, and questionable reviewing [cit] ."
"the impact of information and communication technologies depends on its infrastructure, accessibility, and the intensity of use. although the computational advancement in processing speed and algorithms designed, has shown significant progress, more efficient and user-friendlier applications are needed when it comes to the decision-making process. opportunely, machine learning arises with different algorithms that learn from data to support various task in this field."
"time cells exhibit phenomena that are suggestive of time-scaleinvariance. the firing fields of time cells that fire later in the delay period are wider than the firing fields of time cells that fire earlier in the delay period (figure 1) . moreover, the number density of time cells goes down with delay. although there is not yet quantitative evidence that time cells are scale-invariant, these findings imply that the representation of the past is compressed [cit] because it provides the temporal basis for an animal to use the same set of mechanisms to integrate information and make decisions over different time scales. because the natural world's choice of scale is not known a priori, treating all scales equally is adaptive. it can be shown that logarithmically spaced one-dimensional receptors optimally represent a function when the statistics of the stimulus function is unknown [cit] . just as in the visual system acuity decreases further away from the fovea and facilitates saccades, a scale-invariant representation of time where the temporal acuity decreases as we recede into the past would potentially facilitate retrieval of episodic memory [cit] ."
"generally speaking, the existing studies and application of ict at higher educational institutions have primarily focused on four major streams: e-learning, academic research, a quality measurement, and decision-making process."
"two global statistics are frequently cited as measures of student success: the cohort graduation rate and the freshmanto-sophomore retention rate. thus, faculty decisions should focus on their enhancement. students' persistence to complete their educational goals are a key gauge of student success, and therefore institutional success [cit] . hence, in the next section, we propose the usage of machine learning algorithms to predict graduation rates and collaborate with academic decisions on behalf of student's success. moreover, we set a baseline to support decision making at the strategic level at heis according to directors' needs analyzed in our previous work."
"the importance of cnce algorithm is also scrutinized with the considered routing schemes. first, there is not much difference in terms of the number of channel uses between sp-r and sp-r w/ cnce. when sp-r is a routing scheme, the selected transmission paths of sp-r generally have low reliabilities. applying cnce algorithm will further deteriorate transmission reliabilities and qos guarantees. therefore, cnc structures are rarely formed to enhance channel utilization in this environment. however, the gain from using cnce algorithm can be seen in both qossp-r and qos-r. the number of channel uses of both routing schemes decreases due to cnce algorithm. in addition, the cnce algorithm can decrease the number of channel uses for qossp-r more than for qos-r. this comes from the fact that qossp-r selects the optimal paths with higher reliabilities than qos-r. therefore, the cnce algorithm has a better chance to establish more cnc structures without breaking qos requirements. figure 8 shows the throughput per channel use of all routing schemes. qos-r w/ cnce achieves the best throughput per channel use among all routing schemes. both qossp-r w/ cnce and qos-r w/ cnce significantly achieve a better throughput per channel use than sp-r with and without cnce algorithm in all network environments. figures 9, 10, and 11 exhibit the throughput, number of channel uses, and throughput per channel use of all routing schemes, when the number of nodes in the simulated network is equal to 20. from the results, the performances of all routing schemes show the same properties as those for the case of 15 nodes."
"stored data is not enough when directors and managers are deciding. educational data, whether it is systematically or manually stored, should be analyzed to provide a proper presentation of valuable information to support these complex processes."
"thus, the new advances in e-learning have created lifelong learning/teaching, the transfer of knowledge [cit] and introduced new concepts like mobile learning (m-learning) with the increase of technology [cit] . moreover, the raising of public awareness about environmental problems demands new competencies in sustainability. hence, a few years ago e-learning had been used to promote and improve the quality of life-long education through the acquirement of knowledge, skills, and values for sustainable development [cit] ."
"effective transmissions in wireless mesh networks can be achieved by exploiting wireless broadcast and network coding [cit] . when there is more than one unicast flow in the network, cooperative network coding (cnc) [cit] can be used to improve the total network throughput and channel utilization. in general, cnc is applied to unicast flows at intermediate nodes in a network. however, cnc may affect the reliability of data in each unicast flow, especially in lossy wireless mesh networks, since the success of a unicast session now depends on another unicast session involved in cnc. essentially, all cnc packets must be delivered correctly at the encoding and decoding nodes. it is challenging to achieve high network throughput, data quality guarantee, and efficient channel utilization under unreliable wireless mesh environments. for multimedia transmissions over heterogeneous wireless networks, data are often separated into multiple data layers. depending on the end-to-end transmission capacity between a source and a destination, the number of data layers received determines the quality perceived by the destination. in short, cnc requires that the involved unicast sessions have the same data rate. with separated data layers, cnc can be applied in some data layers even though the overall data rates (or equivalently the transmission capacities) of the involved unicast sessions are not equal. hence, layered coding can increase the applicability of cnc in heterogeneous wireless networks."
"where node c n is the node that performs cnc and l c n is the incoming link of node c n in the direction opposite to the outgoing link of node c n in r (a,b) i . for the y structure that has five transmission links as shown in figure 3b, the reliabilities of unicast flows traveling from node a to node b and from node b to node d with cnc at node c can be expressed as"
"once the 55200 records were divided into the training set (i.e., 70%) and test set (i.e., 30%) three machine learning methods were used to process the test set (i.e., decision trees, random forests and logistic regression). first, we used the receiver operating characteristic curve (roc) as a standard metric for the binary outcome expected (graduated or not graduated.) auc helps to reduce the roc curve to a single value, representing the expected performance of the classifier."
"for example, in interval timing experiments, the variability of the reproduced interval is proportional to the duration of the interval [cit] . the distributions of the response to different intervals are scale-invariant in that they overlap when rescaled by the duration of the interval, a phenomenon termed the scalar property [cit] ."
"to better understand the properties of e f τ *, tþ ð, consider the form it has if f(t) is a delta function at time zero. then, each node in f(s) decays exponentially as e -st and each e f node is given by:"
"one of the main goals of this study is to compare the effectiveness of existing machine learning algorithms in predicting graduate rates that will support decision making at the strategic level. thus, we will be classifying student's academy performance to predict the number of graduated and not graduated students, being this our objective variable."
"since the functions k(t) and g i (t) are bounded, we can treat them as probability distributions up to a scale factor. then the activity of the ith neuron g i (t) is proportional to the probability distribution of the sum of the random variables described by k(t) and g i−1 (t [cit], much like neurophysiologically observed time cells [cit] ."
"in order to improve the decision-making process, the information first needed to be automatized, even small transactional operations such as attendance lists. decisions made at heis have an administrative and academic nature. thus, universities have computational systems to aid mostly administrative operations. most of these are separated by departments, such as the accounting information system or the academic information system, where data can be extracted from the different silos to support the decision-making process. ict at universities serves to help management (supporter) but also serves to improve (enabler) the decisionmaking process. in the organizational context some examples using information and communication technologies are applied in accounting systems [cit] and enterprise resource planning [cit], as well as the academic management [cit] ."
"afterward, we provide an overview of selected works that uses machine learning at higher educational institutions for solving academic problems. we set their stakeholder, goals, and algorithms used. moreover, some of the ongoing applications are highlighted to establish the reliability of using these algorithms in a face to face educational model with the dataset obtained."
"the rest of the paper is laid out as follows: in section 2, a review of the current literature is discussed. in section 3, we introduce the classification of decisions at higher educational institutions. in section 4, details of the method to compare decision tree, random forest and logistic regression in a real case study are illustrated. results are presented in section 5. ultimately conclusions and discussions are reported in section 6."
"is the ith sublayer with the same common transmission rate t, so that network coding can be applied across heterogeneous unicast sessions. let the set of sublayer indices for all (s, d) be i m, where, can be expressed as"
these local structures are potentially embedded in general random topologies. we provide numerical results in terms of the transmission reliability for general random topologies that perform cnc using these three local structures in section 5.3.
"the lower obtained throughputs can be explained as follows. by using the protocol model, there is one more condition added to the definition of an independent set. in particular, the receiver node of a link in an independent set must not be in the interference ranges of the transmitter nodes of the other links in the same independent set. consequently, the size of an independent set is in general reduced, whereas the size of a family of independent sets whose union can cover all links of the network is in general increased. given the same amount of traffic, the wireless link scheduling constraint, and the same link capacities, the number of channel uses available from independent sets to support traffic demands per unit time becomes smaller. therefore, the capacity of wireless mesh networks decreases when there is a single wireless channel available."
"nevertheless, in distance and blended educational models, the arena offers more resources. the amount of systematized information is natural because computers are highly suitable and practical for this work [cit] ."
"therefore, in the next sections, we focus on decisions' classification at heis, and we use machine learning algorithms in a real case study to set a baseline to support directors at higher educational institutions during the decision-making process regarding issues of graduation rates."
"rubén gonzález crespo received the ph.d. degree in computer science engineering. he is currently the dean of the higher school of engineering, unir, and the director of the aenor (spanish association for standardization and certification) chair in certification, quality and technology standards. he is also a member of different committees at the iso organization. he is also an advisory board member of the ministry of education at colombia and an evaluator of the national agency for quality evaluation and accreditation of spain (aneca). volume 7, 2019"
"in the current dynamic environmental economy sectors, in particular, the tertiary sector (service sector) has to keep track with ict and align these technologies to satisfy stakeholder's needs and expectations. in these contexts, universities have to adapt the services they provide to develop, improve, and enhance the quality of the provided services [cit] ."
"we can draw a conclusion from our experiments that qos-r should be used in transmissions with qos guarantees. qos-r gives almost the same throughput as qoss-r, whereas it provides better channel utilizations in all network environments. sp-r is not suitable to be used in wireless networks with poor link qualities since it cannot provide both qos guarantees and high channel utilizations."
the rest of this paper is organized as follows. related work is discussed in section 2. the network model and assumptions made in this research are described in section 3. the optimization formulation used to compute the optimal routing of layered data transmissions is derived in section 4. a set of equations pertaining the reliability of cnc encoded flows is derived and the qos-aware cnc decision is presented in section 5. the performance of the proposed qos-aware routing scheme is evaluated in section 6 using numerical experiments under random network topologies and different traffic conditions. concluding remarks are given in section 7.
"in a public university in brazil [cit] they used four prediction techniques: support vector machine, decision tree, neural network and naive bayes to predict students' failure rates in introductory programming courses. to this aim, data was extracted from distance education. after applying data preprocessing and algorithm fine-tuning, the effectiveness of these algorithms was improved. first, they reduce the number of attributes and balance the information by applying the synthetic minority over-sampling technique. then they finetune the data according to the parameters in each algorithm. like them, we compare machine learning algorithms using the effectiveness metric to predict student's failure rates. besides, our research is allocated on predictive models from educational data. however, the focus in our research is on graduation rates, involving the whole curriculum rather than a particular subject. moreover, stakeholders and educational model are also different. preprocessing data considering the number attributes is held to this aim."
"the neural circuit presented in this article is built upon a mathematical framework that has been proposed to construct a representation of the recent past in a distributed, scale-invariant way [cit] ). this mathematical model has two layers of nodes. nodes in the first layer integrate the input stimuli with an exponential kernel, equivalent to performing a laplace transform on the stimuli. the activity of the nodes in the second layer is obtained by inverting the laplace transform using the post approximation [cit] . after presenting a delta function as input to the first layer, the activity of units in the second layer resembles the firing rates of scale-invariant time cells. the model can be implemented as a two layer feedfoward neural network where the weights can be explicitly computed as a function of the time constants of the nodes in the first layer. here we give a brief overview of the mathematical model and emphasize the connection to our neural circuit that will be introduced later."
"transforming the conventional face-to-face education model through technological platforms have set up blended-learning and distance learning models [cit] . having virtual classrooms has changed communication and interaction between teachers and students, education resources, and others. although e-learning has developed new educational models, researchers have become aware of the need to sustain the development of abilities and competencies to promote intellectual capital [cit] ."
"results from memory experiments also point to a scale-invariant representation of time. the classic power-law of forgetting [cit] indicates that a single mechanism may underlie both short and long term forgetting. in free recall, subjects are given a list of words and are asked to recall them in any order. the recency effect refers to the phenomenon that words from the end of a list are more easily recalled. this effect has been observed over a wide range of timescales, from fractions of seconds [cit] to several minutes [cit], indicating that a single memory mechanism with a scaleinvariant representation of time may serve under different timescales."
"range of behavioral tasks and in many brain regions. time cells were observed when an animal is performing delayed match to sample [cit], delayed match to category [cit], spatial alternation [cit], or temporal discrimination tasks [cit], prefrontal cortex (pfc) [cit] and striatum [cit] . a recent study suggests that neurons in the amygdala are sequentially activated during the intertrial interval of a conditioning task [cit] ."
"instead of receiving and forwarding incoming packets, each intermediate node encodes several incoming packets using the xor (⊕) operation and then forwards each coded packet to the next-hop node. the next-hop node can decode each coded packet if all the other involved coded packets have been received, possibly through wireless broadcasting. over all, cnc can help improve both throughput and energy efficiency in wireless networks."
"specific tasks and transactional activities are performed to support the operations of the institution. this level holds the majority of information technology requires by heis. it governance in this stage works as an instrument to control and manage the it resources such as infrastructure technology and people [cit] . collaborators as teachers, advisors, tutors, programs assistants, and secretaries, among others, execute their task according to the guidelines provided by the strategic and tactic level. although decisionmaking at this level affects a smaller population within the university, it might impact students' success (i.e., schedule and timetabling evaluation) and operational mechanisms (i.e., subject registration process). at the state-of-the-art examination, we found that most of the works done using machine learning on the educational field are a focus on the operational stage. some of the prominent algorithms used are artificial neural networks and support vector machine."
"note that the probability of a successful packet transmission along path r we use the a-b structure as an example. when the involved transmission links are lossy, the successful transmission probability of sublayer l"
"additionally, the use of static random seed is required y knime to start the prediction, and the one automatically generated by the software was used (i.e., 1508210392822)."
"for the a-b structure, cnc is employed at node c, which combines each pair of packets received from node a and node b, and then broadcasts the combined packet back to those nodes. transmission delay and energy consumption in a shared network can be reduced at the cost of lower reliability of transmitted data. this is because to receive the transmitted data correctly at nodes a and b, all data packets involved in the network coding operation must be successfully received by node c, while the network coded packets at node c must be successfully received by nodes a and b."
"in this work, we investigate a novel quality-of-service (qos)-aware routing scheme in lossy wireless mesh networks. the proposed scheme supports heterogeneous layered unicast transmissions with qos guarantee and improve channel utilization by applying cnc based on the local structure of the network. the proposed routing scheme consists of two steps. in the first step, the scheme uses a linear optimization formulation to compute routes http://jwcn.eurasipjournals.com/content/2014/1/81 of all layered unicast flows. the constraints of this optimization problem, such as the transmission rate of each data layer and tolerable error rates in wireless transmissions, are derived to achieve qos guarantee. in the second step, the proposed scheme decides whether or not cnc will be applied to different unicast flows at intermediate nodes to improve channel utilization. the decision criteria are determined by the local network structure and the corresponding qos guarantee."
"it is a highly used classifiers due to its simplicity for understanding and interpretation. it requires little data preparation, handles numerical and categorical data, and performs very well with large data set in a short time [cit] . additionally, the hierarchical tree structure resembles a human way of decision-making, providing extending information about the sequence to classify and individually into a class, discovering rules in a more comprehensible manner [cit] . in our case study, the classification falls either into ''graduated'' or ''not graduated.'' dt is a type of supervised learning algorithm that is mostly used for classification problems. surprisingly, it works for both categorical and continuous dependent variables. although there are many specific decision tree algorithms (e.g., id3, c4.4, c5.0, cart, and chaid), we worked with the most popular developed by quinlan. c5.0 is significantly faster and more efficient than its predecessors c4.5 and id3. c5.0 supports boosting, which gives the trees more accuracy."
formulation proved computationally difficult. the purpose of this work is to 93 restrict the dynamic model to linear form while capturing benefits of the inte-94 gration of scheduling and control. there is a large installed base of advanced 95 controls that utilize linear models [cit] . a unique aspect of this work is a time-
"to preserve the qp structure, the complementarity constraints are included while other wheels or sub-wheels may be a more complex cycles that include 284 less frequently produced products."
"where v is the volume of the reactor, c a is the concentration of reactant a, step tests in the jacket cooling and linear model regression of the step response."
"many different nfv service platforms implementations have emerged in the last few years using different network service programming models. a network service programming model defines the concepts and abstractions that developers build and use in order to define network services descriptors (nsd) and vnf descriptors (vnfd). for example, etsi nfv isg has proposed the tosca framework extended to support nfv services, the openstack heat has also been extended for such purpose, and many research projects and the osm project have proposed proprietary network service descriptors based on yaml, json, xml, or even dedicated programming languages. this implies that a conversion and transformation process is needed in order to cope with the current diversity in network service description formats. in the same way, the vnf packages (vnfd and vnf image) required to allow the exchange of such services between different entities and platforms use their package specifications making the onboarding process to different catalogues or platforms very complicated. to address this, etsi recently started to define and specify a common vnf package format, based on the tosca csar standard [cit] ."
"this paper has presented a hierarchical and recursive network service orchestration architecture to compose endto-end network services by aggregating per-domain network services and vnfs. it is combined with the v&v platform developed in the 5gtango project to test the per-domain network services on each nfvi domain with vendor-specific or open-source nfv service platform implementations such as sonata, osm, or onap."
"while this method is capable of producing cyclic schedules, the optimizer 319 should begin from current conditions rather than steady-state product condi- to a different set of constraints -production amounts and due dates. these 324 constraints give more freedom to the optimizer so the economic objective will 325 improve or be equal to the solution with periodic constraints."
"much like the tapi for the transport sdn controllers, the definition of a common api as northbound and southbound interface for the nfv service platform is key to enable hierarchy and recursiveness in the orchestration of end-to-end network services. etsi nfv isg is defining the interface and information model specification for the os-ma-nfvo reference point used for exchanges between the nfvo and the oss [cit] . this interface may be used as the common api for recursive and hierarchical orchestration. the os-ma-nfvo reference point supports the following interfaces:"
there is demand for three products with quantities that must be met in the 416 schedule over a 48-hour horizon. the product descriptions and quantities are 417 shown in table 2 .
"one drawback to the prior examples is that all spare production capacity is 336 typically placed on the highest value product. over-production of any product 337 can have the effect of lowering the selling price because of supply and demand 338 market forces. in scheduling, there is often a range of production quantity that 339 is acceptable instead of just a single hard limit. to accommodate this, the 340 scheduling and control algorithm can use an 1 -norm objective function to give 341 a target region for the production quantity, rather than one specific hard limit."
"thus, the verification and validation of developed the per-domain network services and vnfs in a multi-nfvi domain environment with different nfv service platforms implementations are extremely challenging due to the large diversity in descriptor formats and packages. 5gtango project is designing a verification and validation (v&v) platform [cit] . the v&v provides a verification and validation service that tests submitted vnfs or network services to ensure they pass a range of tests. the v&v can run the network service or vnf in a perdomain nfv service platform, stressing the network service and vnf and collecting the results. the v&v can test network services or vnfs which have been prepared for any nfv service platform implementation (both open source or proprietary). thus, this ensures that when a developer, network operator, or other user obtains a vnf or network service from the v&v they can be sure that its v&v status is valid and up to date, and that the network service or vnf has passed the v&v process."
"where y is the output, u is the input, k is the discrete time step, and t is the the time scaling approach adjusts either the controller cycle time or the 151 discrete model time step based on the change in unit throughput q relative to 152 the nominal throughputq.τ p is the nominal time constant associated withq."
"the broad adoption of network function virtualization (nfv) in the fifth generation of mobile technology (5g) requires distributed computing resources seamless integrated with the optical access and transport networks from the edge to the core of the network as shown in fig. 1 . in general, an nfv infrastructure (nfvi) is composed of multiple nfvi points of presences (nfvi-pops) at the edge and core of the network. an nfvi-pop is a set of computing, storage and network resources to deploy virtualised network functions (vnfs) through the virtualisation layer (e.g. virtual machine -vm-or container-ct-based technologies). it is deployed in microdcs in the edge nodes (e.g. cell sites, street cabinets, lamppost), small-dcs (e.g., in the central offices -cos-) for low/moderate-computation capacity and low response time, and core-dcs in the core network for highcomputational capacity and moderate response time. on the network side, the nfvi-pop are interconnected with heterogeneous optical access and transport networks that go beyond a commodity providing fixed bandwidth pipes for bulk data. latest advances in programmable and elastic optical systems and subsystems enable to provide dynamic and adaptive connectivity between distributed nvfi-pops. the adoption of a reference architecture such as the etsi nfv management and orchestration (mano) framework [cit] can provide an efficient network service (ns) management and resource orchestration from an end-to-end perspective. the etsi nfv mano architectural framework identifies three functional blocks; the virtualized infrastructure manager (vim), the vnf managers (vnfms), and the nfv orchestrator (nfvo). in general, the vim is responsible for managing the nfvi compute, storage and network resources. however, the etsi nfv mano framework has also defined the wan infrastructure manager (wim), as a particular vim. in this scenario, the vim is responsible for controlling and managing the nfvi-pop's resources, whilst the wim is used to establish connectivity between nfvi-pop's. the vim is commonly implemented using a cloud controller (e.g., openstack), and the wim can be performed by a dedicated transport sdn controller (e.g. opendaylight, onos, ryu) in charge of managing the optical and packet network resources. on the other hand, the vnfm is responsible for the lifecycle management (i.e., instantiation, scaling, updating and termination) of vnf instances running on top of vms/cts managed by the vim. the nfvo has two main responsibilities; the orchestration of nfv infrastructure resources across multiple vims and wims (resource orchestration), and the lifecycle management of nfv network services (ns orchestration). the ns orchestration is responsible for managing and interconnecting groups of vnf instances that jointly realise a more complex function (e.g. service function chaining) by interfacing with the vnfms and vims/wims. the nfvo and vnfms are typically implemented together in open source service platforms such as open network automation platform nfv (onap), open source mano (osm), and sonata. a single and monolithic nfv service platform covering the network operator's nfvi from end-to-end is not a feasible solution. network operators will fragment their transport networks and dcs into multiple nfvi domains to cope with administrative and regional organisations. each nfvi domain will be provided by different vendors relying on proprietary nfv service platforms or leveraging open source solutions. in this paper, we explore the hierarchy and recursiveness of the nfv service platform in order to enable end-to-end network services across multiple nfvi domains. we propose a hierarchical and recursive network service orchestration architecture to compose end-to-end network services by aggregating network services provided by per-domain nfv service platforms together with network functions (vnfs or pnfs). we also propose to make use of the verification and validation (v&v) platform developed in 5gtango project to properly test perdomain network services on each nfvi domain with different nfv service platform implementations."
"although p 3 has the lowest price, it also has the highest required quantity. spare 420 capacity in the production facility favors product p 2 . a potential drawback to 421 always switching to p 2 at the end of a campaign is that there is lost material the slack variables and complementarity conditions combine to create dis-466 crete steps with a function that has continuous first and second derivatives."
"the scheduling objective determines the order and quantity of production at 514 each grade even with half-rate reduction during peak electricity demand. the 515 formulation is sufficiently fast enough, and includes enough process dynamics, 516 to be utilized in on-line control. this presents a fully unified optimization that 517 fulfills the roles of, and can replace, both control and scheduling for a compa-"
"on top of the three nfvi domains that are composed by two domain service platform and one nfvi, we deploy a top nfv service orchestrator that manages end-to-end network services composed of vnfs and the per-domain network services. it enables faster and easier reuse of existing network services by just reusing and referencing the corresponding per-domain nsds. 5gtango has already started to introduce the notion of recursive nsds to describe recursive network services that reference other network services [cit] . similarly to vnfs, per-domain network services can also be represented by connection points that are mapped to the network service termination points. then, virtual links can connect connection points of vnfs and network services. the vnf forwarding graph (vnffg) of an end-to-end network service can be defined by using the constituting perdomain network services, vnfs, and virtual links to construct the vnf forwarding paths (vnffp). figure 1b shows an example of two per-domain network services and an end-to-end aggregated network service composed by the two per-domain network services and one vnf. more specifically, nfvi domain a has a network service (ns1) that is composed of vnf-1 and three virtual links (vl1, vl2 and vl3) connecting the three vnf-1's connection points with the ns1 endpoints. ns1 also defines a vnffg composed of two nfp as shown in fig. 1b . the second per-domain network service (ns2) is offered by nfvi domain b. it is composed of vnf-2 that is connected with two virtual links (vlb and vlc) to the two ns2 end-points. additionally, there is a virtual link (vla) crossing the whole domain connecting other two ns2 end-points. ns2 also has defined a vnffg composed of two paths, one going through vnf-2 and another through the vla that crosses the nfvi domain b. finally, the top nfv service platform offers an aggregated end-to-end network service that is composed of ns1, ns2 and vnf-3. the connection points of the nss and the vnf are connected with virtual links, and a vnffg is defined with two vnffps as shown in fig. 1b ."
"the contribution of this work is the discrete time, extended controller and 286 scheduler that is also able to produce a cyclic schedule. however, this cyclic transition back to product c, the scheduler puts excess production of product"
"in this formulation, φ is the objective function, x is the production quantity this range formulation is not used in this work but is presented to demon-"
step function slack 1 slack 2 intuitive because the optimizer produces excess product p 2 (highest volumetric 476 price) but meets only the minimum production quantities for the lower value 477 products (p 1 and p 3 ).
"in the early development phase, vision algorithms, as well as other data processing routines, are prototyped using the available tool set: different alternatives can be implemented and evaluated in an interactive manner using tools like jupyter and supported by opencv and a pool of scientific python libraries (numpy, pandas, scikit-image, scikit-learn). as the result of prototyping, a collection of well-tested functions is developed. at this stage, the developer can specify computational graphs from the pool of these functions. the process of computational graph engineering involves a great deal of prototyping itself. despite the fact that compgraph constitutes a highly-structured entity, the flexibility of its definition brings a number of advantages over coding up the algorithm as a single function. most importantly, the flat structure of the computational graph, along with graphviz-based visualization capabilities, gives a transparent view on the data flow in the developed algorithm. it also allows for incorporating several alternative branches as a part of the same graph. the uniquely-named tokens provide an isolated namespace, which is specifically useful when prototyping in a jupyter notebook. the mechanism of hyperparameter tokens allows for systematic management of the set of thresholds and other configuration values while being on a single hierarchical level (without a cascade of function calls). the well-defined structure of a computational graph facilitates automated manipulation of it, for example, extending the original graph with additional functions, union of two or more graphs, and union with renaming of functions and tokens."
"as shown in fig. 12, the case system is comprised of three nodes: (1) the robot control node, (2) the image acquisition service, and (3) the epypes-based image processing node. the robot control node coordinates the robot's work cycle and realizes communication with external systems. the system performing stereo acquisition from two cameras is designed as a streaming service, built using the fxis framework [cit] . for each associated camera, a stream of images is captured in its own thread of execution, and a number of recent frames are retained at each moment. external systems can request images from the service that closely correspond to the request timestamp. the nodes run in the distributed environment and communicate through zeromq publish/subscribe sockets and in-process blocking queues. for publishing and subscribing, epypes provides two thread-based abstractions, namely zmqpublisher and zmqsubscriber. the former encapsulates a zeromq pub socket and acts as a consumer of an in-process queue: as a new data is available on the queue, it gets published. an example in fig. 12 is the pub2/q out pair. zmqsubscriber encapsulates a zeromq sub socket, which is polled with the poller object. on arrival of a new message, the latter is put on the connected in-process queue. an example in fig. 12 is the sub1/q in pair"
"computational graph overhead o cg is measured internally by the pipeline (p.compute_overhead()), and constitutes the difference between total processing time of the pipeline and the sum of processing times of all the enclosed nodes:"
"the idea of explicit utilization graph-based representation of data processing algorithms has been on the surface for many years. the availability of engineering tools, data science frameworks, and modeling formalisms, described in the background section, shows the efficacy of the pipeline thinking when designing systems with streaming logic. the distinctive approach of epypes lies in its tight integration with the python ecosystem, support for algorithm prototyping, and abstractions for integration of the developed computational graphs into distributed systems. [cit] . this earlier work attempted to define a data flow formalism with distinct notion of event as the one used in publish/subscribe systems. however, the presented formalism didn't include a reference implementation at the time. epypes has, in turn, refined the notion of reactive pipelines and made it usable in real scenarios."
"in recent years, the increased availability of computational resources, coupled with the advances in machine learning methods and ability to gather large amounts of data, opened new possibilities of developing more advanced data-driven systems. visual data, acquired by various types of imaging equipment, constitutes one of the main inputs to advanced data analysis algorithms."
"the computational graph shown in fig. 13 forms a basis for an instance of a fullpipeline. its event dispatcher f in handles tuples with pairs of images put onto q images . the output preparation function f out is responsible for packaging the output data as a justbytes protobuf message, with its content being the pickle-serialized value of the first 20 rows of the keypoints_paired token (numpy.ndarray), and the attributes filled by timestamps and durations captured with the image acquisition service and the epypes pipeline."
"a wide range of computational systems, particularly those with streaming behavior, can be represented as directed graphs, in which data is routed through processing nodes. not only is this representation accessible to human understanding (particularly for engineers), but it also has been used in various settings to realize improvement of the function of the systems."
"in order to illustrate practical application of the epypes framework and show its suitability for building data processing components in distributed environments, this section presents a run time use case scenario with the associated experiment. the presented scenario demonstrates how epypes can be deployed as a part of a real distributed system (with the communication based on zeromq and protobuf) and what timing properties may be expected in this case. in particular, a major concern is how much overhead is introduced by the additional abstractions in the epypes architecture. furthermore, it is of interest how repeatable this overhead is, as well as what role it plays comparing to communication latency and the application-specific processing time."
"to introduce additional functionality to algorithms expressed as computational graphs and transform them into runtime reactive components, a hierarchy of pipeline classes is defined. as shown in fig. 5, the basic building block of epypes pipelines is a node, which is a runtime counterpart to a function. an instance of node based on function f can be invoked as a callable object, with parameter values corresponding to the positional input arguments of f. a network of node instances corresponding to the graph g form a nodebasedcompgraph. the latter constitutes the main component of a pipeline, as well as its subclasses (sourcepipeline, sinkpipeline, and fullpipeline)."
"in the contemporary robotics research, the robot operating system (ros) is widely used as the underlying platform for the distributed robotic applications relying on data from sensors and cameras. the general architecture in this case is based on a collection of nodes that react to arrival of data through publish/subscribe topics, which makes the overall logic graph-based. the related concept of nodelet (and component in ros2) allows to realize a processing graph structure as a part of a single operating system process. examples of this approach is often demonstrated on the applications of point cloud processing [cit], as to minimize latency due to inter-process or remote communication. ros-based processing graphs, especially in the single-process case, are somewhat similar to epypes pipelines. they, however, target applications with already developed algorithms, as opposed to epypes, which supports early-stage prototyping using the graph-based abstractions."
"the limitation of the proposed implementation lies in its non-deterministic overhead due to the use of the interpreted garbage-collected programming language. hence, applications requiring high rate of operation and more deterministic running time are more likely to be developed in c++ with custom udp-based communication protocols or real-time middleware such as dds. it is of interest therefore to validate the principles of epypes using c++ codebase, as well as to devise a strategy of transforming epypes-based computational graphs to high-performance computing components, for example, via code generation."
"the robot control node announces a series of vision requests and extracts attributes from the response protobuf messages. in addition, it records the timestamps of when the vision request get announced (t vreq ) and when the corresponding response is obtained (t vresp ). the difference between these timestamps accounts for the trip duration of the current request:"
"for execution of both the image acquisition service and the vision pipeline, two timestamps are added to the properties set: t react, when the component reacted to the incoming event, and t pub, right before publishing the outgoing event. their difference t r!p provides the measurement of the component's processing time, including processing of incoming and outgoing events:"
"because pipeline is defined as a subclass of node, its instances constitute callable objects, and are functionally equivalent to instances of node. the whole pipeline is orchestrated by an instance of compgraphrunner (fig. 5 ). the internal structure of a pipeline is visualized in fig. 6 . additional capabilities of a pipeline, as compared with a raw compgraphrunner, include time measurement of nodes' durations, computation of computational graph overhead, storage of additional attributes, and other functionality added by subclassing pipeline."
"to be executable, a computational graph has to be supplied to the constructor of compgraphrunner. the latter is used to store the hyperparameter tokens and schedule execution of the graph with the topological sort. internally compgraphrunner delegates storage and retrieval of token data to an instance of tokenmanager (fig. 3 )."
"to allow for reactive behavior of pipelines, they are combined with event queues, which can be used for subscription to triggering events and publishing the results of data processing. to realize this, aside from pipeline, which is not reactive, three other types of pipelines, coupled with event queues, are defined. reactive pipelines operate in context of thread-based concurrency with blocking queues as the synchronization mechanism. in the python standard library, a queue.queue object can be used to communicate between two threads: the producer thread puts an object on the queue, and the consumer thread request the object and blocks until the latter becomes available. the principle of such interaction is shown in a sequence diagram in fig. 7 ."
"a sourcepipeline, see fig. 8, is a subclass of pipeline whose final output is put to the output queue q out . a sourcepipeline is in addition parametrized by f out, an output preparation function, responsible for packaging the chosen data from the pipeline tokens into a single message that gets published on q out ."
"in the following example, we specify the gaussian blur kernel, and low/high threshold of the canny algorithm in dictionary params. the latter, together with the original computational graph cg is used to construct a compgraphrunner:"
"the defining components of a particular middleware solution are the communication protocol (transport-level tcp and udp, wire-level amqp, zeromq/zmtp, mqtt), the communication styles (request/reply, publish/subscribe), and the data serialization method (typically realized with an interface definition language like protobuf or apache thrift). many middleware solutions are based on a central broker, for example, activemq and rabbitmq. the additional hop through the broker adds a constant value to the communication latency [cit] . zeromq is an example of broker-less middleware, in which the message queuing logic runs locally within each communicating component [cit] ."
"to run a compgraphrunner, its run method is invoked with keyword arguments corresponding to names and values of free source tokens. in the provided example the only free source token is image. therefore, the running syntax is the following:"
"2. images most closely associated with the request are acquired and, as a tuple of numpy. ndarray, communicated to the processing component via the common in-process queue q images ."
"this paper has presented epypes, an architecture and python-based software framework for building event-driven data processing pipelines. because most of vision algorithms and many data processing routines are naturally modeled as pipelines, epypes offers a capability of implementing data processing systems as dags. apart from the functional components comprising the prototype implementation of epypes, this paper has presented a system development framework that supports evolution of computational graphs from an early prototyping phase to their deployment as reactive pipelines."
"data has been collected from five experiments, each with 500 vision requests. for each experiment, a maximum likelihood estimation of log-normal probability density function is performed for distributions of o cg and o epypes . the same estimation is performed for all data combined. figures 15 and 16 show visualization of these pdfs. a pdf for each individual experiment is visualized as a shaded area under the curve. the pdf for all data is shown as a thick curve. the thin vertical line specify the modal value of the pdf for the combined dataset, and the enclosing thick vertical lines delimit the overall range of measurements for the combined dataset."
"a distinction between a static computational graph and its runtime counterparts is realized in order to facilitate smooth system evolution from an early ad hoc development phase to a more integral whole with well-functioning reactive behavior. as shown in fig. 11, the development starts with components having less structure, and proceeds by extension of these components with functionality and behavior that are facilitated by the proposed tools."
"in computer science, graph-based representation of systems has been used for a range of different purposes: data flow models, task graphs (for parallel processing scheduling), symbolic representation of computational expressions (for machine learning and automatic computation of gradients), representation of concurrent process networks (e.g., communicating sequential processes), workflow languages, etc. in the software engineering community, the pipes and filters architecture applies the same ideas to data processing systems design and development. the well-known pipes mechanism of unix-like operating systems has proved to be particularly powerful when it comes to composition of multiple tools to solve a complex task."
"other highly related work within the formal methods domain is stream algebra [cit], with its go-based implementation [cit] . this approach models an image processing algorithm as a set of data streams that get altered by a set of operators. in the algebra implementation, a stream corresponds to a go channel, and the set of defined operators allow to define usable workflow patterns such as pipeline graphs, fork-join graphs, and pipeline graphs with feedback. the latter option is naturally supported due to the concurrency features of go. this approach, similarly to epypes, allows to construct high level algorithm from finer functions, including those from the opencv library. the distinctive feature is the support for feedback, which is disallowed in epypes due to the acyclicity requirement. the feedback with epypes, however, can be realized on a higher systemic level, by incorporating additional distributed components."
control engineering and signal processings has a long tradition of graphically modeling systems in a form of block diagrams. matlab simulink and labview are widely used in this context as engineering tools with formally defined abstractions. the field of cyberphysical systems (cps) makes great use of graph-based system models together with the associated models of computations [cit] . a notable cps modeling environment is ptolemy ii.
"middleware allows to decouple the communicating components by introducing message queuing, built-in address resolution (e.g., via handling logical addresses such as topic names), and usage of a common data serialization format [cit] ."
"epypes is a python-based software framework that combines pipes and filters and publish-subscribe architectures. it allows to develop data processing pipelines, the behavior of which is defined by their response to events. epypes defines a computational graph, which is a static data structure modeling a data processing algorithm, abstractions used for execution of computational graphs, and a hierarchy of pipelines, which extend the algorithm logic defined with computational graphs to be a part of a publish-subscribe system."
"data science has seen a surge of tools based on explicit handling of data processing systems in a form of dag. many of them are intended to be run on a computing cluster, and the dag architecture in this case facilitates scheduling of parallel execution of data processing tasks. apache storm is a cluster-based stream processing engine. apache airflow is workflow management platform for batch processing on a cluster. dask is a python parallelization library that utilizes dag modeling for scaling algorithms written with numpy and pandas primitives to be used with massive datasets."
"after importing the opencv python module (cv2), two helper functions are defined for grayscaling and blurring (the function for edge detection is used as-is). the structure of the computational graph is specified as two dictionaries. the func_dict dictionary defines mapping from unique function identifiers (in this case, strings \"grayscale\", \"blur\", \"canny\") to the respective callable objects. the func_io dictionary defines input/output relationships between the functions in a form of tokens. each function identifier is mapped to a tuple describing input and output tokens that can be one of the following forms, depending on the respective functions' signatures:"
"3. image processing node extracts the desired features from the images, which are communicated back to the robot via the pub2/sub2 asynchronous socket pair. the target vision algorithm performs orb feature detection, description, and matching [cit] . figure 13 shows the corresponding computational graph. after image features are identified in each image, collections of feature descriptors are matched against each other using opencv's bfmatcher object, with the matches returned in sorted order by match distance. the final gather_keypoints function produces an array of the matched keypoints' coordinates."
"an important part of further work should be connected with development of software abstractions on the highest level of the system development continuum shown in fig. 11 . this will enable fine-tuning and enhancing of reactive pipelines, for example, with adapters to different messaging systems (e.g., mqtt, rabbitmq, dds), parallelizable nodes, and specialized pipeline management logic. an important task in this case is implementation of systematic error handling. a failure inside the pipeline (e.g., in the case of a vision system, due to changed lighting conditions) can be handled by issuing the corresponding event that will be processed by a remote component. in addition to queues providing asynchronous messaging, other communication modalities can be used. an rpc api (such as rest or grpc) can be established to allow external systems getting meta-information about the running pipeline and changing values of hyperparameters. last, but not least, functionality for interaction with databases should be integrated."
"consider a simple computational graph that defines a processing chain in which a color image is first converted to grayscale, then blurred with a gaussian kernel, with the blurred image further used to perform edge detection with the canny algorithm."
"in computer vision, the opencv library has become a de-facto standard providing a pool of community-contributed image processing and computer vision algorithms. similarly, the point cloud library (pcl) provides open-source routines for point clouds processing. a multitude of tools from the python ecosystem are widely used for data science and scientific computing. they are built upon the numpy array library, and include pandas, scikit-learn, scikit-image, and many others. the abovementioned opencv and pcl, as well as many other low-level tools, expose python bindings, which makes it possible to perform rapid system developed with preserved high performance of the applied algorithms."
"it can be seen from fig. 15 that overhead from performing data processing based on a computational graph o cg is characterized by matching log-normal distributions for every experiment, with most of the probability density located around 0.3 ms. the epypes overhead o epypes, as shown in fig. 16, has much tighter range of possible values, distributed log-normally with matching distributions for every experiment, and with most of the probability density around 0.02 ms. overall, for a vision algorithm that naturally requires tens of milliseconds to perform the processing, the overheads introduces by epypes can be considered negligible."
"the open source movement has gained a big popularity within the fields of data science, computer vision, and robotics in recent years. even though the established proprietary engineering tools are pervasive in the industrial context, they often lack flexibility and hinder a deeper understanding of how a system functions. conversely, open source tools provide community-contributed implementation of common functionality, which is flexible to use and allows for building more scalable and reproducible solutions."
"at the core of epypes lies compgraph, a data structure that models a data processing algorithm as a computational graph, that is, as a network of functions and data tokens. formally, a compgraph can be described as a bipartite dag g:"
"when a computational graph is developed, it can be used to construct pipelines. the non-reactive pipeline provides additional capabilities to the computational graph: it is runnable, includes time measurement functionality, and can be flexibly subclassed, as done in reactive pipelines (sinkpipeline, sourcepipeline, and fullpipeline). the latter are used to expose the developed algorithm in online mode."
"after each request is finished, the robot control node records all the obtained properties. the latter are further aggregated in a pandas data frame, with a row of properties' values per each request. from the available data, the following overhead metrics can be computed:"
"as the presented software framework is implemented in python, it naturally gears toward system prototyping use cases. the static abstractions are useful for algorithm prototyping, while the transition to the reactive components allow for rapid deployment of the computational graphs to distributed environments. this allows for harnessing the available python data science tools and integrating them into industrial automation workflow."
"an event-driven system is characterized by a discrete state space, where state transition happen on occurrence of events at sporadic time instants [cit] . in distributed systems, events are often embodied as messages sent over a network in a publish-subscribe communication system. such messages can signalize a change of a system state (change event) or a notification from an observation (status event), expressed as a tuple with a timestamp and an application-specific descriptive parameters [cit] ). message-based middleware provides a unified set of communication and input/output capabilities in such sense-respond systems."
"token t 1 is the only free source token, and its value is required to perform a computation. 2 the term hyperparameters is borrowed from machine learning, where it refers to parameters that characterize a particular algorithm, as opposed to model parameters. semantics of hyperparameter tokens in this paper is similar, although the considered computational graphs can be used to model a wide variety of algorithms."
"once a computational graph g is constructed, and it conforms to the requirements of acyclicity, its execution can be scheduled. topological sort of g results in an order of vertices (functions and tokens) so that all the directed edges point from a vertex earlier in the order to a vertex later in the order. with invoking functions in this topological order, all the precedence constraints will be satisfied."
"in manufacturing automation, vision systems has a long history of use in combination with dedicated automated equipment and industrial robots, serving a role of contact-less sensing for, amongst others, quality inspection and robot guidance. what differentiates industrial vision solutions from general-purpose computer vision systems, is their coupling with the associated mechatronic components possessing an actuation function. this entails that most industrial vision systems operate in online mode, with their operation being synchronized with external systems by various forms of remote communication. the practical applicability of the proposed framework is validated in a distributed experimental setup comprised of a robot, an image acquisition service, and an image processing component, communicating in a publish-subscribe manner using zeromq middleware. it is shown that the epypes architecture facilitates seamless transition between various deployment configurations in a distributed computing environment. this paper is structured as follows. first, the background areas are introduced, including overview of computational systems based on dags, the python data science/computer vision ecosystem, and event-based middleware. further, the epypes abstractions are presented with code examples and architectural relationships. finally, a distributed system experiment based on epypes provides a more detailed view into the runtime properties of the framework."
"a compgraphrunner can be used as a namespace for accessing any token value by the token key. the interface for this operation is the same as for a python dictionary. for example, to visualize the blurred image from the computational graph in fig. 4 using matplotlib, the following syntax is applied:"
"the robot control node runs on an arm-based raspberry pi 3 single-board computer with the raspbian operating system, while the vision-related components are deployed to an ubuntu-based x86-64 machine. the latter has an ethernet connection to a stereo camera pair (gige vision-based prosilica gc1350), which are used by the image acquisition node."
"attributelist represents a collection of key/value attributes, where an attribute can be either a string, a double, or an int32. event, sent over pub1/sub1, is comprised of an id (string), a type (string), and attributes (attributelist); justbytes, sent over pub2/sub2, is comprised of an id (string), content (bytes), and attributes (attributelist);"
"an instance of sourcepipeline is constructed as follows: as an example of the output preparation function, consider a pipeline, whose computational graph contains a token with the key pose, corresponding to a 3d pose estimated from images. to take the data corresponding to this token and package it as a python pickle, the following function can be defined: another subclass of pipeline is sinkpipeline, shown in fig. 9 . it is meant not to be called manually, but to be triggered as an event e is announced in q in . because e can be an arbitrary object, it is necessary to map its contents to a dictionary d e that describes what data should correspond to the pipeline's free source tokens. such mapping is defined by event dispatcher function f in ."
"other academic examples of similar robot/vision architectures include the one based on the supervisory control theory of discrete-event systems (košecka, [cit] ) and service-oriented dataflow-like components, auto-tuned by higher-level supervisors [cit] ."
"epypes overhead is computed as an excess time in the vision pipeline in addition to the processing in the computational graph and in the functions f in and f out : figure 14 demonstrates the timeline of 100 vision requests and the associated durations of the image acquisition service, the vision pipeline, and the overhead from network communication."
"the principle of the epypes abstraction is demonstrated on the example of constructing a computational graph for edge detection and discussing the inner structure of the hierarchy of pipelines. further, a real scenario of deployment of an epypes pipeline for features detection and matching to a distributed system is experimentally studied. it was shown that the ability to adapt reactive behavior to various publish/subscribe middleware solutions allows to combine epypes pipelines with already available systems. the measured timing properties of the image processing component based on epypes show that the latter introduces negligible overhead comparing to the application-inherent processing time."
"some simple mathematical operations applied on these specific 1d numerical maps, e.g. cumulative sum along the numerical vector, can reveal a specific trend in the sequence [cit], but its general utilization is limited. thus, finding a linkage between the dna sequence and its translation to protein in their numerical representations is not straightforward."
"in addition, an average value of correlation coefficient exceeded 0.9, which is sufficient for most of the common analyses, e.g. motif searching or comparative analysis. for more precise results, an additional optimization based on features of analyzed sequences such as codon bias is necessary. many of the genetic codes are newly discovered and thus public repositories lack a sufficient number of sequences for reliable optimization. frequently, only a single sequence is available. for these insufficiently represented genetic codes, it is convenient to use a numerical map based on a simple, clear and general optimization criterion. another possibility is to use a globally optimal numerical map independent of the genetic code."
"an example of phylogenetic analysis based on comparison of signals was conducted to demonstrate the effect of usage of different variants of the numerical maps. three phylogenetic trees were constructed from the coding sequences of mammalian hbb genes. only the tree based on the proposed optimal numerical map had comparable topology with the reference taxonomy. as the analysis demonstrates, even such a simple task is highly dependent on the utilized numerical map, while poor results are obtained for non-optimal maps."
"for these reasons, the proposed optimization criterion was not sufficient and the results strongly depended on features of the particular analyzed sequences. for example, fig. 2 shows signals for vertebrate mitochondrial sequence for which the optimal numerical map was also the best variant."
"the results are summarized in table 4 . similarly, a mean percentage deviation was evaluated, as table 5 shows. in both tables, the first column corresponds to the genetic code used, the second column summarizes the results for our optimal numerical maps, the third column is for previously published optimal map, and the fourth column shows the results for our globally optimal map in cases where it is not identical to the proposed optimal map for a specific genetic code. the last two columns show the best and the worst results from all other variants of numerical map. the best results are shown only in cases where they differ from the results of the proposed optimal map. our optimal numerical maps are not always the best possible maps for real sequences (e.g. genetic code 2) because these maps were theoretically derived with the assumption of uniform codons distribution in sequences. cases where this theoretical assumption is not satisfied are discussed below."
"the amino acids reach values from 1 to 20. value 0 is reserved for termination codons regardless of the order of the corresponding codon numerical representative. this assignment to termination codons prevents discontinuity in assignments to amino acids. the termination codon is the last codon of a gene sequence and in most sequence analyses it is not used. after the assignment of decimal values to all amino acids, the transformational function can be visualized as depicted in fig. 1 ."
"in addition to the verification of proposed optimal numerical maps for particular genetic codes, the globally optimal map was verified. such a map can be applied for general use in analyses without a specified genetic code or not permitting a change of settings. the globally optimal map was the best possible solution for 15 of the 24 genetic codes based on our optimization criterion and for eight of the 13 sets of real sequences. moreover, our globally optimal map was, for 10 of 13 real datasets, better than the original optimal map."
"the aim of this paper is to contribute to the standardization of basic operations in genomic signal processing, which is a rapidly developing new branch of bioinformatics. the proposed optimization sets new rules for the first step of genomic signal processing, which is the transformation of symbolic sequences to numerical representation. in comparison with other authors, we are not proposing a new type of sophisticated numerical transformations, which are frequently suitable only for one type of analysis, but we optimize the known conversion of nucleotides to integers 0, 1, 2, and 3. this numerical mapping is simple, versatile and currently widely used. many users of bioinformatics software are using it unknowingly. computational functions prefer processing of numbers rather than symbols. this simple numerical map and its variations, based on different assignments of values to nucleotides, can be optimized for the purposes of complex analyses of dna sequences and proteins, e.g. genome mapping or comparative genomics. for this purpose, it is necessary to minimalize the loss of genetic information caused by translation in the numerical form."
"in this paper, we report the construction of the optimal numerical maps for genetic codes for which translational tables are available on the afore mentioned ncbi website. basic translational tables were used; no special cases were incorporated. the theoretically derived numerical maps were verified using real sequences. a verification dataset was created for selected genetic codes. each dataset was comprised of dna sequences for 50 genes from several organisms. the genetic codes for which only few sequences, mostly from one species, are available in databases were excluded from our study as their optimal numerical maps cannot be reliably verified. only records of sequences containing a note of used translational tables were added to the datasets. there was also a condition that the records annotations must be verified (not only automatically annotated or predicted) and must include cds location, because identification of mrna segments is not enough. in total, datasets covering 13 different genetic codes were used to verify the proposed versatile numerical map. a summary of the used sequences in datasets for each of the 13 genetic codes is shown in table 2 ."
"to conclude, the proposed globally optimal map provides better results than the original optimal map in 10 out of 13 cases. in the remaining three cases, the results are slightly worse. there was even a single case in which the original optimal map was significantly worse than our maps. it was the case of genetic code nr. 25 for which the average percentage deviation was above 10% and the correlation coefficient was under 0.8. the globally optimal map was identical to the proposed optimal maps for seven out of 13 tested genetic codes. additionally, for six of these seven codes, this map gave the best results, with a single exception in genetic code nr. 10, where a map with slightly better percentage deviation could be found. table 5 evaluation of genomic signal distortion based on percentage deviation d."
"the trees of all numerical maps were compared to reference tree by calculating robinson-foulds distance (rfdist) [cit] for rooted trees using r software (packages phytools and phangorn). the robustness of the phylogenetic trees was evaluated by the bootstrapping statistical test [cit] . the implementation of this standard statistical test for symbolic sequence based phylogenetic trees is practically identical for genomic signal based trees. however, it must be taken into account that each mutation in numerical representation have different influence to result tree. the variability of bootstrap replications is therefore much higher than for symbolic sequences. while symbolic sequence based methods need at least 100 bootstrap replications for reliable statistical verification, the genomic signal based implementation requires 1000."
"we evaluated signal distortion for the optimal numerical maps of 13 genetic codes. each genetic code was represented by 50 real dna sequences. as the results differ for each sequence and genetic code, a mean corrcoef and its standard deviation (std) were calculated."
"the goal of the proposed optimization was to achieve minimal divergence from a linear trend of the transformation function, which leads to the minimal difference between genomic and proteomic signal. the difference is caused by degeneration of the genetic code. the optimal numerical map can minimalize distortion of the numerical representation of translated protein. we evaluated the influence of the map to signal distortion. two frequently used parameters for evaluation of the differences between two signals were used: pearson correlation coefficient (corrcoef) and percentage deviation (d). because the corrcoef parameter is not affected by mean value of the signals it can be used as a quality criterion in its basic definition. on the contrary, the percentage deviation needs adjustment of the signal ranges. both signals have to be normalized by their maximal possible value, which is 63 for codon signal and 20 for amino acid signal. the percentage deviation d for the normalized signals can be computed as bold highlighted text indicates where is the global optimal map correspond with local map for particular genetic code."
"although the numerical map was already optimized [cit], the optimization criterion was set simply as a number of amino acids degenerations and the resulting numerical map is not robust enough for the processing of real data. we proposed optimization according to a new optimization criterion that is focused on minimizing information loss between genomic and proteomic signals. the optimal numerical map ensures maximal similarity of the numerical representation of nucleotides and amino acids despite the degeneration of the genetic code. the basis of optimization criterion lies in minimizing the divergence of numerical values of codons representing multiple degenerated amino acids, e.g. leucine with six codons. this optimization takes into account not only a number of degenerated amino acids but also the weight of introduced errors. another disadvantage of the original optimal numerical map comes from its exclusive definition only for the standard genetic code. therefore, the selected variant of value assignment is not optimal for alternative genetic codes and its general utilization is limited."
"in addition to the analysis of signal distortion caused by translation, an influence of signal distortion on the topology of phylogenetic tree was tested. a dataset covering eight protein coding sequences of hbb (beta globin) genes of mammals from genbank was used; see table 6 . all sequences have the same length of 444 nucleotides, 148 codons/ amino acids, respectively. therefore, no signal alignment was needed. the close phylogenetic relationship of some species allowed us to examine the resolution of phylogenetic classification using the signals. a reference taxonomic tree was constructed according to taxonomy published at ncbi [cit] and is shown in fig. 4 ."
"the worst result from all variants of numerical maps suffered from a percentage deviation of over 20%, which is more than two times worse than the worst result of the proposed optimal maps. in such cases, it is not possible to differentiate between the deviation caused by translation and real mutations in sequences. conclusively, usage of the optimal numerical map is important and the random assignment of numbers to nucleotides is not reliable."
"we applied the new optimization criterion to all known genetic codes to derive particular optimal maps. moreover, we were able to propose the globally optimal map based on complex analysis of the proposed optimal maps for specific genetic codes as well as the globally optimal map were verified using 650 gene sequences of different organisms and different types of dna, e.g. nuclear, mitochondrial etc. results of verification were compared to results for the original optimal map and to the worst and the best cases of all numerical maps. two parameters, correlation coefficient and percentage deviation, were used to evaluate dissimilarity between codon and amino acid signal. while the first of them quantifies dependence between signals before and after translation, the latter evaluates dissimilarity of these signals. for most of the genetic codes, the best results were obtained using our newly proposed optimal maps. three cases of slightly better results of percentage deviation and only two cases of correlation coefficient were recorded when using other maps. in these cases, the results depended heavily on sequences used for verification due to their codon bias, which manifests differently for various organisms. an additional optimization is needed for more precise analysis using sequences with a high level of codon bias. unfortunately, current databases do not contain a sufficient number of sequences for many of the genetic codes."
"although our proposed optimal maps did not provide the best results for all scenarios, the correlation coefficient always exceeded 0.9 and the maximal percentage deviation was kept under 8%. the value of percentage deviation may seem quite high, but the translation itself from codons to amino acids causes loss of signal resolution as the value range of amino acid signal is one-third of the codon signal and the range reduction is not linear because of the genetic code degeneration."
"practically, different crs may need different content items, and have potential links with different chs by reusing the spectral resources of different cues according to their own qos requirements. this can be represented by a graph as shown in the left top part of fig. 4 . for notational simplicity, we can denote by u 1 the set of crs, denote by u 2 the set of chs and denote by u 3 the set of cues, where"
"the os decides whether virtual caching may be used at the granularity of memory regions. these are an internal os abstraction for contiguous virtual-address ranges with shared properties, such as for program code, the stack, the heap, or a memory-mapped file. when allocating virtual addresses for a memory region the os virtual address range allocator predicts whether the region could have read-write synonyms (unsafe) or frequent permission/mapping changes (inefficient), and if so, uses addresses that allows physical caching and otherwise uses virtual caching."
"notice that a link is deemed eligible only when its outage probability is no larger than q out max . denote by q out i, j,c,r the outage probability of cr u j obtaining a coded fragment of content item r from ch u i by reusing cue u c 's spectrum. recall that we can only get statistical csi of all the channel links. thus, taking the outage probability constraints, sinr requirements, and power constraints for users into account, we can have"
"since correctness must be absolute, we instead propose a best-of-both-worlds approach with opportunistic virtual caching (ovc) that exposes virtual caching as a dynamic optimization rather than a hardware design point. ovc hardware can cache a block with either a virtual or physical address. rather than provide complex support for synonyms in hardware [cit] or enforce limits on which virtual addresses can be synonyms [cit], ovc requires that the os (with optional hints from applications) declare which addresses are not subject to read-write synonyms and can use virtual caching; all others use physical addresses and a normal tlb. this flexibility provides 100% compatibility with existing software by defaulting to physical caching. the os can then save energy by enabling virtual caching when it is safe (i.e., no read-write synonyms) and efficient (i.e., few permission changes). ovc provides a graceful software adoption strategy, where ovc can initially be disabled, then used only in simple cases (e.g., readonly and private pages) and later extended to more complex uses (e.g., os page caches)."
"let r j and d j,r, respectively, denote the number of chs assigned for cr u j that desires content item r, and the cost of cr u j obtaining content item r via d2d links. based on the notation defined in section ii-b, we have"
"handling homonyms and page permission changes: ovc implementation uses conventional addressspace identifiers (asids) to distinguish between different mappings of the same virtual address and avoids cache flushes on context switches. both the asid and the tag need to match for a cache hit to occur. ovc uses an all-zero asid for blocks cached under the physical address (which results in an asid match for any physical cache access). to handle the kernel address space, which is shared by all processes, we copy the global bit of the x86 pte (which is set for globally shared kernel memory) to each cache block. privileged mode access for blocks with this bit set do not need an asid match. asid overflow can be handled by modifying linux's existing asid management code to trigger a cache flush before reusing an asid."
"almost all commercial processors today cache data and instructions using physical addresses and consult a tlb on every load, store, and instruction fetch. thus, a tlb access must be performed for each cache access. however, processor designs are increasingly constrained by power, and physically addressed caches lead to energy dissipation inefficiencies. tlb lookup must be fast, rarely miss-often necessitating an energy-hungry highly associative structure. industrial sources report that 3-13% of core power (including caches) is due to tlb [cit], and an early study finds that tlb power can be as high as 15-17% of the chip power [cit] . our own analysis shows that a tlb lookup can consume 20-38% of the energy of an l1 cache lookup."
"to optimize the problem (23), one must determine the optimum selection of three related three-dimensional arrays x, s, and m and vector k. this full-scale optimization is a fourdimensional matching among content items, chs, crs, and cues and is np-hard. generally, the chs are assumed to have high physical and social centrality, such that they can serve more requesters with satisfactory wireless links. therefore, it is reasonable and preferable to solve content distribution in advance based on physical and social centrality rather than distribute content items based on certain content requests. to simplify, we can determine the selection of chs for content caching in advance, thereby reducing our optimization into a three-dimensional matching problem."
"the empirical analyses in the previous section suggest that while virtual cache challenges are real, they occur rarely in practice. all the applications studied provide ample dynamic opportunities for safe (i.e., no read-write synonyms) and efficient (i.e., no page permission/protection changes) use of virtual caches. unfortunately, correctness and backward compatibility must be absolute and not -almost always‖."
"where p max h and p max c, respectively, represent transmit power limits of chs and cues, η c c and η d j, respectively, denote the achievable signal-to-interference-and-noise ratio (sinr) of cue u c and requester u j, η c min and η d min, respectively, correspond to the minimum required sinrs for cues and d2d links. constraint (3b) indicates that each ch stores at most one fragment of an item and each fragment is stored in only one node. constraint (3c) ensures a one-to-one matching between d2d links and cellular spectrums. constraint (3d) limits that, one ch can simultaneously serve multiple crs requesting different content items, but two or more crs requesting a given content item r cannot be simultaneously served by the same ch. in addition, a cr must connect to at least k r chs holding the desired content item r to ensure successful d2d content retrieval. constraint (3e) and constraint (3f) demonstrate the sinr and power constraints for both cues and d2d users. according to (3), the cost minimization problem considers the matching among cached content items, chs, crs, and cues, which is a four-dimensional matching problem."
"typically, the overall transmission cost for the caching based storage systems, includes both the cost of the first-time content download for the chs, and the cost of content download for crs. to better illustrate the effectiveness of the caching based storage systems, the overall transmission cost for content download is compared with the case where all the crs obtain their desired content items via bs dissemination. as shown in fig. 6, when the number of content requests is relatively small, the caching based systems are more costly since the cost for initial content distribution dominates the overall transmission cost. however, the benefits coming from the caching based system will be more significant and remarkable when it comes to large scale networks. with increasing number of requests, the caching based systems consume less transmission cost since the d2d content download is generally less costly than downloading from the bs. focusing on the caching based storage systems, the case using the ls algorithm achieves the best performance and the case using the ihm algorithm ranks second, while the hbm case is the most costly one. based on the numerical results, the caching based storage systems can provide users with their desired files with relatively low transmission cost for the scenarios with extensive content requests. furthermore, fig. 6 also provides the lower bounds of the transmission cost (based on the application of jensen inequality in section iii.b) for content download via bs dissemination (i.e., the curve labeled as \"bs dissemination, lb\") and caching based storage system with ls (i.e., the curve labeled as \"caching based, ls-lb\"). once again, these results demonstrate that the lower bounds are tight, and very close to the true cost for minimization."
"second, ovc dynamically lowers the associativity of l1 cache lookups. we note that the cache associativity constraint of a physical cache, described in section 2.1, need not hold true for virtually cached blocks. figure 2 shows an example of how a banked l1 cache organization can be leveraged to allow lowerassociativity cache lookup for a 32kb, 8-way set associative cache. the 8-way set-associative cache is organized in two banks each holding 4-ways of each set. for virtual addresses (i.e., ovc_enable and va 47 are set), the processor only accesses one of the two banks (i.e., 4 ways) based on the value of a single virtual-address bit from the tag (va 12 in the example). for other accesses using physical addressing, the processor performs a full 8-way lookup as in a conventional cache."
"the current focus on energy efficiency motivates reexamining processor design decisions from the previous performance-first era, including considering some optimizations that challenge compatibility across layers. to this end, this paper uses small virtual memory changes to save substantial translation lookaside buffer (tlb) and l1 cache lookup power."
"the page permissions must be stored with each cache block to check permissions on cache hits. however, when permissions change, these bits must be updated. this is harder than with a tlb because many blocks may be cached from a single page, each of which must be updated. in addition, when the os removes or changes a page mapping, the virtual address for a cache block must change."
let i r j be an index indicating whether user u j can successfully find enough chs to receive r . let (t) be the unit step function:
"these challenges hinder the adoption of virtual l1 caches despite their potential energy savings. in this work, we seek an ideal situation that provides most of the benefits of virtual caches by using it as a dynamic optimization while avoiding their complexities to an extent possible and maintaining compatibility."
"recall that, in a wireless distributed storage system, different crs may require different content items. since different crs can find different numbers of chs for content download according to the quality of physical communication links, it would be more practical and efficient to assume that different content items can be divided into different numbers of pieces. unlike most existing works that slice all content items into fixed k fragments [cit], in our work here, we optimize k for each content item to ensure that the corresponding crs can find sufficient number of chs for d2d content sharing, borrowing the concept of mbr [cit] . when more than one choices of k can satisfy the condition, the larger one shall be used to achieve better storage efficiency and lower energy consumption."
"to minimize the overall transmission cost in item sharing, crs should be matched with appropriate chs based on the cost of different communication links. since practically the network bs can only acquire statistical csi of active links to make such decisions, the mean transmission cost should be considered. basically, the transmission cost is proportional to energy consumption and spectrum resource usage for content sharing and can be expressed as p · t · b, where p, t, and b correspond to transmit power, transmission time duration, and link bandwidth, respectively. generally, t can be expressed in terms of the data block occupancy relative to the physical resource block (prb) duration of the available spectrum. therefore, when user u j requests for content item r, individual mean cost for data transmission from ch u i reusing cue u c 's spectrum, and the cost for downloading from the bs, can be written as"
"page permissions (e.g., read, write, execute, privileged) augment the coherence state permissions for each cache block and are checked along with coherence permissions. a page permission miss-match (e.g., write request for a block with read permission) triggers a cache miss, which results in access to the tlb. it is then handled appropriately as in conventional physical cache for page permission missmatches. page mapping or permission downgrades trigger a cache flush."
"as depicted in figure 3, ovc's space overhead in the l1 cache stem primarily from the addition of an asid (16 bits) and physical tag (28 bits) per cache block. the primary tag must be extended (8 bits) to accommodate larger virtual address tag. we also add page permission/privileged bits (3 bits) and a global bit. this totals approximately 10% space overhead for the l1 assuming 64-byte cache blocks. given that l1 caches comprise a small fraction of the total space (and thus transistor count) for the cache hierarchy, which is dominated by larger l2 and l3 caches, the overall static power budget (which is grows roughly in proportion to transistor count) of the on-chip caches barely changes: ~ 1% overhead for the cache hierarchy in table 6 . furthermore, the extra physical tag is accessed only for uncommon events: back invalidations, forwarded coherence messages and dirty evictions. l1 cache lookups and l1 cache hits do not accesses this physical tag. as a result, it leads ~ 1% energy overhead on l1 cache lookups, because most of the energy is spent on data access, which has not changed. we will show that this overhead is outweighed by the benefits of ovc. we also note that cycle time is not affected as data lookup latency overshadows the tag lookup latency."
"we insert two checks into the linux kernel for conflicting synonyms. within the page fault handler, we add code to check whether a virtually cached page is being mapped with write permissions at another address in another process. similarly, we put a check in the kernel routine that creates temporary kernel mappings to user memory to detect conflicting synonyms. if the above checks detect possibility of a conflicting synonym in the page-fault handler, the os marks the process with write access to a synonym as tainted, meaning that when it runs, it may modify synonym pages. we modify the os scheduler to flush the l1 cache before and after the tainted process runs. if hyper-threading is enabled, scheduler needs to prohibit tainted process from sharing the same core (and thus l1 cache) with another process. this ensures that address synonyms between the kernel and usermode code similarly: the kernel flushes caches before and after using kernel-space synonyms."
"practically, cellular linked storage nodes are individually unreliable, and the contact durations of d2d links may not be long enough to afford full content delivery. to improve content-caching reliability and efficiency, erasure coding can offer redundancy by splitting and encoding full content items into small pieces [cit] . for instance, maximum distance separable (mds) codes can achieve optimality in terms of the redundancy-reliability tradeoff [cit] . in addition, when considering performance metrics such as storage efficiency and repair bandwidth, there exist two special coding schemes: minimum storage regenerating (msr) codes and minimum bandwidth regenerating (mbr) codes [cit] . the msr codes can require the minimum storage at storage nodes while the mbr codes consume the least repair bandwidth among all regenerating codes."
"basically, there exist scenarios that allow multiple d2d users to reuse the same cue's spectrum. this case is a direct generalization by first considering a one-to-one matching before iteratively pairing the remaining d2d links with sinr constraint. the matching of the remaining d2d links can be tackled iteratively by forming \"virtual cue links\" that view each physical cue link with its associated d2d underlay link as a \"virtual cue\". however, the more representative case should consider the one-to-one matching between d2d links and cues in this work, thus the problem mentioned above reverts back to a one-to-one matching problem between virtual cues and d2d links. conversely, the optimization problem formulated in (3) assuming one-to-one matching between d2d links and cues can also be extended to more general cases. particularly, if multiple cues' spectrum resources can be reused by one d2d link, our problem formulation is also applicable by regarding the d2d link as several virtual d2d links, likewise."
"range allocator to allocate addresses from two nonoverlapping address pools, partitions p physical and p virtual (described in section 4.1), depending on whether physical or virtual caching is to be used."
"we set out to determine how often the expensive or complex virtual cache events actually happen in the real world by studying several modern workloads running on real x86 hardware under linux. first, we measure the occurrences of virtual-memory synonyms to determine how often and where they occur in practice. as noted in section 2.2, synonyms pose a correctness problem for virtual caches. second, we measure the frequency of page protection/mapping changes, as these events can be more expensive with virtual caches."
"for simplicity and practicality, the distribution of content items among storage nodes is assumed to be completed in advance. the content distribution may also be updated and adjusted to improve the system performance, forming a natural closed-loop feedback system of the wireless distributed storage via d2d links. however, for simplicity, we only analyze and optimize the radio transmission cost for content download, therefore the fundamental optimization problem is transformed and reduced to a three-dimensional np-hard matching problem [cit] . to the best of the authors' knowledge, three-dimensional matching problems have not been widely studied in content sharing scenarios, though related problems in graph theory have utilized the concept of hypergraph [cit] and solutions for k-set packing problems 1 [cit] ."
"in fig. 9, in addition to our proposed scheme with optimized k r for each content item r, the performance of another case with fixed k r for all r is also discussed for comparison. furthermore, the impact of the number of crs is also discussed. as can be seen from both fig. 9(a) and fig. 9(b), cost consumption increases with a larger η c min . when η c min increases, cues have higher qos requirements and may refuse to share their spectrums to ensure their own transmission quality. therefore, the number of accessible cellular spectrums reduces, leading to a lower success probability of d2d communication, further a higher overall transmission cost. in addition, our scheme with optimized k r always outperforms the case with fixed k r for all content items, because we can adjust the value of k r to guarantee that more crs can get desired content items via d2d based content sharing. by comparing fig. 9(a) and fig. 9(b), we can find that more transmission cost is needed when the number of crs ascends. meanwhile, the gap between the case with fixed k r and our scheme with optimized k r is enlarged. when the number of chs and cues keeps unchanged, if k r is fixed, the increasing number of crs indicates that there always exist some crs that can not find enough chs for content sharing. however, the case with optimized k r can always try to guarantee the number of crs that can get access to enough chs, further leading to a slower rise in transmission cost. apparently, the case using the ls algorithm consumes the least transmission cost, the ihm algorithm ranks second and the hbm algorithm achieves the worst performance. the impact of the value of q out max and δ max on the achievable performance is depicted in fig. 10 . intuitively, with the increasing of q out max, lower transmission cost is consumed, since a larger q out max indicates a looser outage probability constraint. in addition, a larger δ max represents a looser delivery delay constraint, thus it is more likely that data transmission can be accomplished during δ max . therefore, the increasing of q out max and δ max can both lead to a higher probability that a cr can find enough qualified chs, thus more crs can get content items via d2d links, further resulting in a lower transmission cost. again, the ls algorithm performs the best and the hbm algorithm consumes the most transmission cost."
"however, virtual caches present several challenges that have hindered their adoption. first, a physical address may map to multiple virtual addresses (called synonyms). an update to one synonym must be reflected in all others, which could be cached in different places. thus it requires additional hardware or software support to guarantee correctness. second, virtual caches store page permissions with each cache block, so that these can be checked on cache hits without a tlb access. when page permissions change, associated cache blocks must be updated or invalidated beyond the normal tlb invalidation. third, virtual caches require extra mechanisms to disambiguate homonyms (a single virtual address mapped to different physical pages). fourth, they pose challenges in maintaining coherence, as coherence is traditionally enforced using physical addresses. finally, virtual caches can be incompatible with commercially important architectures. for example, the x86 page-table walker uses physical addresses to find page-table entries [cit], which creates problem for caching entries by virtual address."
"in our evaluation we focus on dynamic (lookup) energy as tlbs and l1 caches are frequently accessed, but relatively small, making ovc's static-energy impact insignificant. table 7 shows the percentage of l1 data and instruction tlb dynamic energy saved by the ovc. we observe that more than 94% of the l1 data tlb energy and more than 99% of l1 instruction tlb lookup energy is saved by ovc. to analyze this result, we first note that the cache accesses that use virtual addresses and hit in the l1 cache avoid burning energy for tlb lookups. table 8 shows the percentage of data and instruction accesses that can complete without needing address translation, while the l1 cache hit rates for accesses using virtual addresses are listed in table 9 . we observe that on average 97% of data accesses and 100% of instruction accesses complete without needing address translation, while a very high fraction these accesses (0.96 and 0.99 respectively) hit in the cache, saving tlb lookup energy."
"coherence: l2 caches and beyond typically process coherence with physical addresses. to access virtually-tagged l1 blocks, incoming (initiated by other cache controllers) back-invalidations and forwarded requests may require reverse address translation (physical to virtual). reverse translation can be avoided by serially searching physical tags (added for cache block eviction) for all sets that might hold a block. since ovc already provides the processor with an associative lookup on physical addresses, it figure 2, would simply access the physical tags in both banks (8-way total). further, this action may be handled with an auxiliary structure (option (b) for handling eviction) and our empirical results find this occurs less than once per 1k l1 cache accesses due to high l1 hit rates and low read-write sharing. note that, coherence messages received due to local cache misses (e.g., data reply, acks) use miss-status handling register entries to find the corresponding location in the cache and hence do not require reverse translation lookup."
"we quantify the performance implications of ovc in table 11 which show the number of misses per 1k cache reference (mpkr) for the baseline and the ovc l1 data and instruction caches. for the l1 data cache, the change in the number of misses is within a negligible 0.7 misses per 1k cache reference, while changes for instruction caches are even smaller. two of the workloads (specjbb, memcached) experience larger l1-d cache miss rate decrease with ovc (~2 misses per 1k reference, which translates to a minuscule hit-rate difference), while the l1 i-cache miss rate increases for one workload (bind). we note that cache hit/miss patterns are slightly perturbed due to use of a single bit from the virtual page number in selection of the bank where an access should go when virtual address is used under ovc. more importantly from table 11 (right-most column), we observe that ovc hardly changes run time compared to the baseline system (within 0.017%). the unchanged run time, coupled with ovc's small static power overhead to the whole on-chip cache hierarchy (section 4.1) indicates that ovc leaves the static power consumption of the onchip memory subsystem largely unchanged while saving substantial dynamic energy. furthermore, for these workloads, the operating system never needed to use the taint bit (section 4.2) as they do not use direct i/o or make system calls to change page protection. moreover, there were no cache flushes due to memory-mapping changes."
"the process for obtaining maximum weighted three-dimensional matching based on ls algorithm is presented in step 4. we have shown that it is not possible to find a 1-claw to have a better system performance in lemma 3. thus, the process of finding a 1-claw can be ignored here."
"specifically, the ihm algorithm projects the threedimensional matching problem into the iterative twodimensional matching problems. for each two-dimensional matching, the hungarian algorithm with a complexity of o(l 3 ), e.g., the kuhn munkres (km) algorithm, is used to find the optimal matching. therefore, the ihm with j iterations has the complexity of o( j · 3l 3 ) [cit] . similarly, the hbm algorithm decomposes the three-dimensional matching into two layers of bipartite matching. thus the hbm algorithm with j iterations has a complexity of o( j · 2l 3 )."
"we also found that the os kernel sometimes uses synonyms in the kernel virtual address space to access user memory. for example, to process a direct i/o request that bypasses the operating system's page cache (used by databases), the kernel copies user data using a kernel address-space synonym for the userspace page. kernel space synonyms are also used during a copy-on-write page fault to copy content of the old page to the newly allocated page. these kernel-space synonyms are temporary but can introduce inconsistency through read-write synonyms."
"motivated by above reasons, this work investigates the wireless distributed storage systems based on d2d links by using (n, k, d) codes, and minimizes the transmission cost for content download process. different content requesters (crs) may require multiple content helpers (chs) for distributed content download. particularly, content download process for each cr requires connections to k chs to recover desired content items by establishing k eligible d2d links with the chs. notice that, once the content sharing via d2d links fails, the crs have to retrieve their desired content items from the associated bs instead, which typically requires higher energy and spectrum resources. in addition, since we assume that one ch can serve for multiple crs demanding different content items, but two or more crs requiring the same content item cannot connect to the same ch simultaneously. thus the matching between crs and chs is a many-to-many problem. however, for a given content item r, the matching between a cr and chs is a one-to-many problem, and can be solved by using an f-matching algorithm [cit], which can be further transformed into one-to-one matching problem [cit] . thus, a four-dimensional matching problem can be formulated by considering the distribution of content items in chs, the pairing between chs and crs, and the reuse of the resources of cellular user equipments (cues) for content sharing links."
"combining (10), (12), and (15) leads to the conclusion of lemma 1. fig. 2 illustrates an example that all the constraints in lemma 1 are satisfied and feasible power region exists. the power region is depicted based on (9) . assume the coordinates of the intersection point a is (p"
"to reap the benefits of ovc, the operating system must enable virtual caching for memory regions that are amenable to the use of virtual caching (section 4.2). importantly, we find that the os kernel (linux in this study) already possesses most of the information needed to determine which memory regions are suitable for virtual caching and which are not. while ovc defaults to physical-only caching to enable deployment of unmodified oses and applications, changes to support virtual caching affected only 240 lines of code in the linux kernel (version 2.6.28-4)."
"to show equivalence, we express the achievable data rate r c i, j for link from user u i to u j while reusing the spectrum of u c as"
"where b corresponds to the communication bandwidth, z r is the total size of the content item r, and p b s is the transmit power of the bs. recall that, for simplicity, the p b s can usually be treated as a constant. however, it is difficult to find the closed form of the mean transmission cost (17a). instead, we can relax our problem according to the following lemma."
"we find, though, that many of these problems occur rarely in practice. we analyze the behavior of applications running on real hardware with linux operating system to understand how synonyms are actually used and to measure the frequency and character of page permission changes. as detailed in section 3, we find that synonyms are present in most processes, but account for only 0-9% of static pages and 0-13% of dynamic references. furthermore, 95-100% of synonym pages are read-only, for which update inconsistencies are not possible. we also find that page permission changes are relatively rare and most often involve all pages of a process, which allows permission coherence to be maintained through cache flushes at low overhead. thus, a virtual cache, even without synonym support, could perform well, save energy, and almost always work correctly."
(1) how much tlb lookup energy is saved? (2) how much of l1 cache lookup energy is saved? (3) what is the performance impact of the ovc?
"targeting on the objective function in (23), the optimization of our objective considers the optimization of the coding parameters (k r ) and the four-dimensional matching among cached content items, chs, crs, and cues. however, for simplicity, in this section, we will discuss the content distribution and the optimization of k r for every content item r first, leaving three-dimensional matching problem among chs, crs, and cues. however, the three-dimensional matching problem includes a many-to-many matching between crs and chs and a one-to-one matching between cr-ch links and cues, which is hard to solve. thus, we further transform the original problem into a one-one-one three-dimensional matching problem by extending virtual crs and chs."
"to benefit from virtual caches and while sidestepping their dynamically rare issues, we propose opportunistic virtual caching (ovc). ovc hardware can cache a block with either virtual or physical address (section 4.1). virtual caching saves energy (no tlb lookup on l1 hits and reduced l1 associativity). physical caching provides compatibility for readwrite synonyms and caching page-table entries (and other structures) accessed by the processor with physical addresses."
"lemma 3: for a given k-dimensional maximum weighted matching problem, if the initial matching (denoted by g) is obtained by using the greedy algorithm, it is impossible to find a 1-claw to improve the overall performance."
"we used x86 full system simulation with gem5 [cit] to simulate a 4-core cmp with the configuration listed in table 6 . we modified the linux 2.6.28-4 kernel to implement the operating system changes required for leveraging ovc. we used cacti 6.5 [cit] with the 32nm process for computing energy numbers. for tlbs, l1 caches, and l2 caches, we used high performance transistors (-itrs-hp‖), while low static power transistors (-itrs-lstp‖) were used for l3. l1 and l2 caches lookup both tag and data array in parallel for providing faster accesses. however, l3 caches lookup the tag array and data array in sequence."
"traffic can be attributed to duplicated downloads of a small fraction of popular content items. in light of the rapid growth as well as the general under-utilization of storage capacity in mobile devices, distributed storage based schemes have been introduced to reduce wireless capacity bottlenecks by caching popular content items in mobile users [cit] . by enabling direct content sharing between mobile users without routing through base stations (bss), distributed storage systems can achieve higher energy efficiency and lower content delivery delay."
"in this paper, we would like to determine an appropriate value of k r such that all crs requesting content item r can find at least k r qualified cr-ch links. intuitively, the value of k r is dominated mainly by the physical condition of the worst-case cr, i.e., cr u j that only can find the fewest potential chs. for a specific content item r, define k wc r as the number of qualified chs the worst-case cr can find. in addition, we denote by h r j the set of chs that have stored a coded fragment of content item r and can be accessed by cr u j requesting content item r, i.e., therefore, we can determine the value of k r based on the following heuristic principle."
"without loss of generality, the number of chs for each content item in connection is assumed to be at least k r during download, and d r during repair, respectively. in other words, the download (and repair) process is executed fast enough such that none of the serving nodes would disconnect from the requesting nodes or power off before completing the content sharing."
"eviction of a dirty l1 block invokes a write-back to a physical l2 cache. ovclike most virtual caches-logically augments each virtually-tagged block with a physical tag to avoid deadlock issues with doing an address translation at eviction. this physical tag adds a small state (e.g., 28 bits on 544 bits state, tag, and data) and can either be stored (a) in the l1 cache or (b) an auxiliary structure (not shown) that mirrors l1 dimensions, but is accessed only on less frequent dirty evictions."
"the main contributions of this paper are three-folds: 1) a four-dimensional matching problem based on erasure coding for wireless distributed storage is formulated under cellular d2d spectrum underlay. the problem minimizes the transmission cost by considering the distribution of cached content items and physical quality of d2d links. for simplicity, the four-dimensional matching problem is reduced to a three-dimensional matching problem in which we assume the content distribution can be solved in advance. for the three-dimensional matching among crs, chs, and cue resources for d2d links, the activation of d2d links between crs and chs is a many-to-many-matching problem, while the matching between activated d2d links and cue resources is one-to-one. to reduce complexity, we propose a solution to transform the many-to-many based three-dimensional matching into a one-to-one based three-dimensional matching problem. 2) for the np-hard, one-to-one based three-dimensional matching problem, our approach considers a feasible region based on transmit power for cues and d2d nodes, as well as channel gains for the physical links involved. we propose an ls based scheme to numerically solve the three-dimensional matching at low complexity for content sharing in distributed storage and spectrum sharing through cellular d2d underlay. 3) based on the failure rate at crs when retrieving content items from neighboring chs, content distribution among chs can be updated for better performance. this contribution presents a new concept of closed loop feedback for content redistribution in wireless distributed storage. dynamic content redistribution makes it possible to achieve a higher success probability and to improve the robustness of distributed content storage."
"with all the background knowledge given above, we can apply the ls algorithm to solve the three-dimensional algorithm 2 maximum weighted three-dimensional matching based on local search [cit] g: initial matching achieved by greedy algorithm. b i : the set of all the neighbors of g(i)."
"under all the constraints (3b)-(3f) of p 1, the new constraint (16c) of p 2 will reduce the feasible set and lead to lower optimization complexity."
"generally, content sharing among mobile users in distributed storage systems may be ad hoc [cit] . recently, device-to-device (d2d) communications have emerged as a potential candidate to facilitate the distributed storage system to offload traffics from bss [cit] . accordingly, scalability and limit of caching in terms of d2d direct communications and multihop communications have been discussed [cit] . operating on cellular spectrum, d2d links can provide better user experiences when properly coordinated. d2d links in the underlay mode, through careful interference management, can achieve significantly improved spectral efficiency and system throughput, among other advantages [cit] ."
notice that the gap of the objective functions between p 2 in (16a) and the reformulated problem p 3 in (23a) is upper bounded by
"ovc saves l1 cache lookup energy by accessing only a subset of the ways in a set when using virtual addresses (section 2). table 10 presents percentage savings in dynamic energy by ovc from opportunistic use of partial lookups (4-ways out of 8-ways) in the l1 cache. the second column shows that on average more than 22% of the dynamic energy spent on l1 data cache lookups is saved, while the third column shows similar savings for an instruction cache. the rightmost column provides a more holistic view of the energy savings in the chip by showing how much of dynamic energy of tlbs and all the three levels of on-chip caches taken table 11 . miss ratio and runtime comparison between baseline and ovc together is saved. on average, more than 19% of the dynamic energy spent on the on-chip cache hierarchy and the tlbs is eliminated by the ovc. the savings can be as high as 32% (swaptions) for applications with small working sets that rarely access l2 or l3 caches. in total, ovc saves a considerable portion of on-chip memory subsystem dynamic energy through lower associative l1 cache lookups and tlb lookup savings as these two frequent lookups account for most of the dynamic energy in the on-chip memory."
"there has been decades of research on implementing virtual caches, which are summarized by cekleov and dubois for both uniprocessor [cit] and multiprocessor systems [cit] . here we discuss a few of the most related work on virtual caches. we also discuss relevant work on reducing tlb power."
"since for rayleigh fading channels, g i, j, g c,b, h i,b, and h c, j are independent exponentially distributed with means γ i, j, γ c,b, β i,b, and β c, j, respectively, we can use the jensen's inequality to rewrite the sinr requirements in (6b)-(6c) to get relatively more strict constraints with lower complexity, which can be expressed as:"
"based on the scenarios and assumptions discussed above, content sharing among the mobile users can be categorized into three cases: 1) a cr desires to download a certain content item. in this case, the original item can be recovered from any k of the n chs storing the requested content item. 2) a ch, already having one fragment of a content item, wants the entire content item. since k distinct fragments of the coded content items are sufficient to recover the original item based on the encoding redundancy, the ch can obtain the content item by connecting to at least k − 1 chs. 3) when one of the n chs becomes invalid, a new ch needs to be selected to repair the lost coded fragment from d existing chs. generally, for these different cases, the number of necessary chs for content sharing can vary, and the transmitted packet lengths in each d2d link may also vary. however, both the physical and mathematical models are essentially the same. hence, without loss of generality, this paper shall only focus on the general content download process described in the first case."
"we investigate user content sharing in a d2d based distributed storage system by focusing on a single cell without loss of generality. within each cell, m mobile nodes are randomly located. generally, users can use either d2d mode or cellular mode. when working in cellular mode, different cues are assigned orthogonal spectrum resources, and unit bandwidth is allocated for each cue. when working in the d2d mode, mobile users can serve as chs to cache (store) popular content items, or act as crs requesting desired content items from adjacent chs via d2d links. for spectral efficiency, cellular d2d underlay is investigated here in which uplink cellular resources are reused by d2d links by effectively mitigating co-channel interference."
"since all the content items can be encoded and stored in n chs, for analytical simplicity, we can select n nodes from all m users to act as chs and store all necessary (popular multimedia) content items. intuitively, mobile users with better physical conditions and higher connectivity (centrality) in the networks should have a higher probability to serve for more requesters. in this paper, centrality of user nodes can be defined based on the average distance from other user nodes [cit] ."
the operating system for ovc hardware has three additional responsibilities: (1) predicting when virtual caching of an address is desirable (safe and efficient); (2) informing the hardware of which memory can use virtual caching; and (3) ensuring continued safety as memory usage changes. we extend the linux virtual-address allocator to address the first two and make minimal changes to the page-fault handler and scheduler for the third.
"proof: assume that there exists one such 1-claw whose center vertex is in g and the talon (denoted by e 1 ) is not in g. then the performance would be improved if we add e 1 into g and remove all its neighbors from g. this can be possible only when the weight of e 1 is larger than the sum of the weight of all the neighbors of e 1 in g. in this case, e 1 should have been selected when applying the greedy algorithm, and e 1 should have been existed in g, which is a contradiction."
"ovc requires that hardware provide the following services to realize the benefits of caching with virtual addresses -(1) determining when to use virtual caching and when physical caching, (2) reducing power when possible by bypassing the tlb and reducing cache associativity (3) handling-virtual memory homonyms and page permission/protection changes, and (4) handling coherence requests for virtually cached blocks."
"from our analysis and numerical results, we observe that in both applications of jensen's inequality, the obtained lower bounds for costs c j,r i,c and bs j,r are quite tight. given the small gap between the original d2d transmission cost and its lower bound. therefore, we can relax and transform our cost minimization problem into optimizing the lower bound on the original transmission cost."
"opportunistically reducing lookup energy: when data can be cached using virtual address we take advantage of it in two ways. first, we avoid tlb lookups on l1 cache hits. second, we allow lower associativity l1 cache lookups. as shown in figure 1, when cache lookup address falls in partition p virtual (i.e., ovc_enable and va 47 are set), the tlb lookup is disabled and part of the virtual address is used for cache tag match. otherwise, conventional physical cache lookup is performed where the tlb is performed in parallel with indexing into the l1 cache. on a miss to an address in p virtual a tlb lookup is required before sending the request to the next cache."
"in the remainder of the paper, we focus on the second commonly used design. henceforth we use the term physical cache to refer to a virtually indexed and physically tagged cache."
"moreover, the number of neighbors that any hyperedge in the expanded hypergraph can find is at most n e . thus the complexity of the sorting of b i in step 3) is o(n e log n e )."
"the hardware defines a one-bit register named ovc_enable that an operating system can set to enable ovc (default is unset). when ovc is enabled, we take advantage of large virtual address space of modern 64-bit oses to logically partition the address space into two non-overlapping address ranges (partition p physical and p virtual ). the highest order bit of the virtual address range (e.g., va 47, the 48 th bit in linux for x86-64) determines the partition in which a virtual address of a cache lookup belongs to (in p physical if va 47 is unset and p virtual otherwise). only cache lookups with virtual address in the partition p virtual can use the virtual address to cache data. thus, there is no added lookup cost to determine how an address is cached."
"with simple modifications to linux (240 lines of code), our evaluation shows that ovc can eliminate 94-99% of tlb lookup energy and saves more than 23% of l1 cache dynamic energy compared to a virtually indexed, physically tagged cache. this paper makes three contributions. first, we analyze modern workloads on real hardware to understand virtual memory behavior. second, based on this analysis, we develop policies and mechanisms that use physical caching for backward compatibility, but virtual caching to save energy by avoiding many address translations. third, we develop necessary lowlevel mechanisms for realizing ovc."
"we modeled a 4-core system with an in-order x86 cpu detailed in table 6 . the simulated system has two levels of tlb and three levels of caches. each core sports separate l1 data and instruction tlb and a unified l2 tlb. the cache hierarchy has a split l1 instruction and data cache private to each core. each core also has a private l2 cache that is kept exclusive to the l1 cache. the l3 cache is logically shared among all the cores, while physically distributed in multiple banks across the die."
"practically, content item size varies, and the content requesting users may experience various channel conditions to chs. therefore, it would not work well unless the number of chs necessary for content download, k r, is determined in consideration of specific content size and user channel conditions."
"in this section, we focus on the cost minimization problem of content download in a distributed storage system. notice that content download from the bs is assumed to cost more than downloading via d2d links in this paper. therefore, to achieve a low overall transmission cost, the success rate for finding enough eligible chs should be fulfilled, i.e., the number of crs obtaining content items from chs via d2d links should be guaranteed."
"this paper have investigated the transmission cost minimization problem while guaranteeing users' qos requirements in d2d based distributed storage systems. exploiting a coding scheme, we have optimized the coding parameter k r and the set of chs for each interested content item such that all the crs can find enough eligible ch-cr links for d2d content sharing. the optimization of the requisite coding parameter and the content distribution has been executed based on mobile users' physical conditions and their preset qos requirements. in addition, the three-dimensional matching among crs, chs, and cues have been solved by leveraging a local search algorithm based on the concept of hypergraph. numerical results have shown the effectiveness of our proposed scheme."
"referring to the expression for outage probability in (8), whether a link is eligible is greatly affected by α r (the size of transmitted data fragments). further, α r is affected by content item size and its coding parameters. in this subsection, we focus on determining the coding parameter, k r, to guarantee that more crs can obtain desired content items via d2d links."
"for reliability and efficiency, content items are encoded using (n, k, d) coding schemes in this paper. with (n, k, d) codes, each content item is partitioned into k pieces, encoded and stored in n chs, with each ch storing α bits. the content download process requires connections to no fewer than k chs, each of which transmits α bits to the cr. when a ch becomes invalid, a new node can be selected to replace the invalid one and repair the lost data by connecting to d existing chs, each of which transmits β bits to the new ch. the process of content download and content repair is depicted in fig. 1 . when a cr or a new ch fails to find enough chs for content sharing or repairing, it has to retrieve the content items from a serving bs."
"when power is a first-class design constraint, two aspects of this design lead to higher energy consumption. first, tlb lookups are energy-hungry: they occur frequently and often use a highly associative (or even fully associative) design [cit] ."
"a physical l1 cache requires the address translation to finish before a cache lookup can be completed. in one possible design, the address translation completes before l1 cache lookup starts, which places the entire tlb lookup latency in the critical path. however, a more common design is to overlap the tlb lookup with the cache access [cit] . the processor sends the virtual page number to the tlb for translation while sending the page offset to the cache for indexing into the correct set. then the output of the tlb is used to find a matching way in the set. such a design is termed a virtually indexed/physically tagged cache. in both designs, all cache accesses, both instruction and data, require a tlb lookup. when latency (and thus performance) is the single most important design objective, a virtually indexed/physically tagged design is attractive as it hides the tlb lookup latency from the critical path of cache lookups while avoiding the complexities of implementing a virtual cache."
"our empirical analysis shows that virtual cache challenges are real, but occur rarely in practice. to benefit from virtual caches and yet sidestep their rare correctness issues, we proposed the opportunistic virtual cache (ovc) that can cache a block with either a virtual (saving power) or physical address (ensuring compatibility). we show that small os changes enable effective ovc virtual caching, while ovc facilitates its own adoption by operating correctly with no software changes."
"for frequent and performance-sensitive synonym uses, such as direct i/o, a program can prevent these flushes by mapping i/o buffers using the map_dynamic flag, which will use physical caching. however, even if a user fails to do so, the above mechanism ensures correctness anyways. we also note that it is possible to have read-write synonyms within single process's address space (e.g., if same file is simultaneously memory mapped by a single process at different places in writable mode). if such cases ever occur (we have encountered none), we propose to turn off ovc capability (unset ovc_enable) for the offending process."
"similarly, denote by¯ bs j,r the lower bound on bs j,r, thus the transmission cost for u j to acquire content item r from the bs can be derived as"
"while predicting future memory usage may seem difficult, we observe that the os already possesses much of the information needed. the kernel virtualaddress allocator defines flags specifying how the memory region will be used, which guides its assignment of page permissions for the region. table 6 . baseline system parameters."
"the remainder of this paper is organized as follows: in section ii, the scenario for content download over distributed storage system is described and corresponding notation is defined. in section iii, the problem to minimize the overall transmission cost for content download is described that considers erasure coding. the problem is further transformed into a simpler problem to facilitate solutions of low complexity. given a priori distribution of content items among the storage nodes, section iv transforms the original problem into a threedimensional problem for optimization. the three-dimensional matching problem is analyzed in section v by utilizing the concept of hypergraph, and an ls based solution is presented as well. theoretical complexity analysis and numerical results for our proposed solution are given in section vi, to demonstrate the performance against some benchmarks. the conclusions are presented in section vii."
"backward compatibility: virtual caches can break compatibility with existing processor architectures and operating systems. for example, oses on x86, such as linux, update a page table entry (pte) using cacheable virtual addresses. however, the x86's hardware page-table walker uses only physical addresses to find ptes in caches or memory [cit] . with a virtual cache, it is unclear how to make x86's page table walker work both correctly (a virtual l1 cache entry is like a synonym) and efficiently (if caching of ptes is disabled). moreover, virtual caches often break compatibility by requiring explicit os actions (e.g., cache flushes on permission changes) to maintain correctness."
"cache coherence is generally performed with physical addresses. with a virtual cache, the address carried by the coherence messages cannot be directly used to access the cache. thus a reverse translation (physical-to-virtual) is logically required."
"we, an open source in-memory object store used by many popular web services including facebook and wikipedia; and bind, the bind9 domain name service (dns) lookup service [cit] . we also analyzed the open-source web browser firefox [cit] synonym usages and tlb invalidation characterization. however, as an interactive workload, it does not run on our simulator."
"the operating system maintains coherence between the page-table permissions and the tlb by invalidating entries on mapping changes or protection downgrades, or by flushing the entire tlb. table 5 presents the average inter-arrival time of tlb invalidations for our workloads (and its reciprocal -the tlb invalidation request per sec). the inter-arrival time of tlb invalidations varies widely across the workloads, but we make two broad observations. first, even the smallest inter-arrival time between invalidations (2.325ms for memcached) is an order of magnitudes longer than the typical time to flush and refill a l1 cache (~ 5µs). hence, flushing the cache is unlikely to have much performance impact. second, we observe that almost all tlb invalidations (97.5-100%) flush the entire tlb rather than a single entry. most tlb invalidations occur on context switches that invalidate an entire address space, and only a few are for page protection/permission changes. consequently, complex support to invalidate cache entries from a single page may not be needed."
"ensuring correctness: while the kernel only uses virtual caching when it predicts that conflicting synonyms will not arise, they may still be possible in some rare cases. first, the kernel itself may use temporary kernel address space synonyms to access some user memory (section 3.1). second, the kernel allows a program to later change how a memory region can be used (e.g., through linux's mprotect() system call). we provide a fallback mechanism to ensure correctness in these cases by detecting when the change occurs, and then flushing the cache between conflicting uses of memory."
"in this section, we tackle the three-dimensional matching among crs, chs, and cues based on the hypergraph theory. although the three-dimensional optimization problem is still np-hard, already there exist approximation algorithms to solve such type of matching problems [cit] ."
"in a pure streaming setting the data come in sequentially, and the number of activities depicted in each video is not known a priori. in order to show the importance of all the modules proposed in sec. 3 for dealing with the hard constraints of such a streaming context -see sec. 2 -we conducted extensive testing exploring the following scenarios, which correspond to different variants of fiver:"
"finally, since fiver is oblivious to the actual number of classes in the training data, in all our experiments the algorithm is learning the classes truly on the fly."
"however, brings to the table crucial new features that makes it uniquely suitable for dealing with streaming data. firstly, it does not rely on any input parameters, which are inconvenient to tune in streaming settings. secondly, it limits the model size, thus allowing the tracking of drifting concepts. more precisely, when the number of allocated balls exceeds a given budget, fiver discards each ball with a probability proportional to its error rate. thirdly, it dynamically adjusts the ball centres, thus yielding very compact models while improving performance."
"we assessed our method on the following datasets: kth [cit] ) (all scenarios), ucf11 [cit] and virat [cit] for action recognition, skig [cit] and msrgesture3d [cit] for gesture recognition, japvow (m. [cit] ) and auslan [cit] for sign language recognition (uci repository [cit] ). table 2 shows the number of videos and classes of our benchmark."
"note that fiver does not need any validation set as it has no parameters to tune. this is very important in the streaming context, where non-adaptive methods which tune their parameters in an initial validation stage may perform suboptimally on unseen data. plots a to g in fig. 2 illustrate the recorded performance on the various benchmarks for all the presented scenarios. the figure shows that varun performs as well as full on most datasets, even though it queries only around 50% of all the labels. on kth, for example, varun achieves 90% online accuracy while accessing only less than 20% of the labels. the rnd method performs typically worse and needs all the labels to reach the performance of full. varunfix works almost as well as varun on the simplest datasets and slightly worse on the complex ones; this is due to the fixed budget control that has to discard information in order to keep the model size fixed. for example, both varun and rnd use 4% of the input data around the 50% query rate for ucf11, whereas varunfix use only 0.2% of the data -this is shown in the red and green boxes in fig. 2 as final percentage of input examples used as model centres. therefore, varunfix is extremely good at compressing the data, and allows for efficient computation at the cost of limited performance degradation. it is worth stressing that the rnd setting can be compared only against the varun setting as they can freely grow the ball covering, as opposed to varunfix which can use only a fixed amount of support centroids."
"the main contribution of this paper is an approach for dealing with human activity recognition in a streaming context. to the best of our knowledge, our approach is the first one to address all the challenges listed above in a principled manner. our starting point is a recently proposed local algorithm for classification of data streams [cit] which is incrementally trainable and nonparametric (i.e., the model structure is not specified a priori, but determined by the data in such a way that the number of parameters is not fixed in advance), while exhibiting theoretical guarantees on its performance. here we leverage on this result, and extend it to the active learning setting. this leads to a framework that meets all the above requirements: (1) it incrementally and efficiently learns the incoming data stream while being robust to the addition of new classes; (2) the active learning component evaluates the informative content of the incoming data items with respect to the current level of confidence, thus allowing to decide when the cost of manual annotation is worthwhile; (3) the nonparametric nature of the approach allows for fully data-driven learning."
"how to optimise the algorithm heuristics dynamically? system components, such as the chosen feature representation and the learning algorithm parameters, have a crucial impact on the final performance of any framework. in a continuous learning setting, however, design choices and parameter tuning are -if possible at all-more difficult than in offline settings, as we just cannot anticipate what new activities the system will be asked to learn."
"measuring prediction confidence. eq. (1) shows that class estimates p π(t) (y), associated with ball centres near the current input instance, should be considered more reliable than those associated with faraway centres, as the corresponding region of the feature space has already been explored -see fig. 3 -right. we thus adapt the rbf kernel [cit] to scale ball estimates based on their distance from the input examples:"
"to emphasize the versatility of our approach, we tested fiver in both batch and streaming learning settings. in the batch setting, we followed the standard evaluation protocol for each dataset: specific train-test splits or k-fold crossvalidation with specific values of k -see below for details. we then compared fiver's results to those of competing incremental and offline methods. in the streaming setting, instead, we assessed different variants of fiver using the online accuracy -or sequential risk [cit] )-as evaluation measure. this measure captures the average error made by the sequence of incrementally learned models in a procedure where we first predict the test item on the current model, and then use the result to adjust the model itself."
"how to minimise the required annotation effort? the issue of how many video fragments should be annotated is strictly related to the ability of learning new activities. while for newly observed activities one might assume that all video frames should be -at least initially-manually annotated, when analysing footage of known action classes only a fraction of the video input will likely bring in new information. in this context, the system should automatically select which video fragments are the most informative, and asks human annotators for help only in those cases."
"which integrates over all the local features of the video. in our experiments we used the log-likelihood in order to avoid numerical problems (note that, as the logarithm is a monotonically increasing function, replacing probabilities with log-probabilities in (1) does not change the argmax)."
"the dynamic nature of the streaming video setting implies that, at each time instant, new data is made available to the system, which needs to incrementally learn from it. this implies both refining the current models of known human activities and adding on the fly new models of previously unseen activities."
"we ran a first set of experiments in a batch setting, i.e., running fiver on a random permutation of the given training set and then applying the resulting classifier on the test set. here, fiver is evaluated without the active module. 3 we compared fiver against incremental and batch approaches: incremental algorithms: [cit] b,a; [cit] ), which follow an incremental learning approach similar to ours. batch algorithms: [cit], which have unrestricted access to training data for learning, as opposed to incremental methods that can access the data only sequentially. note that the performance of incremental algorithms is typically poorer than that obtained using the corresponding batch versions [cit] ."
"1. each video is associated with a variable number of feature vectors in a given feature space. 2. the feature space gets sequentially covered with balls centered on samples selected from the stream. 3. each ball is associated with an estimate of the conditional class probabilities obtained by collecting statistics around its centre; a new unlabeled sample is predicted using the estimate of the closest ball. 4. a sample falling outside its closest ball becomes the center of a new ball. 5. the radius of each ball is adjusted according to how well each ball predicts the class label of the new samples that fall close to it. 6. ball centres are incrementally adjusted to fit the actual data distribution. 7. the set of balls is organized in a tree-like structure [cit], so that the ball nearest to the current sample can be found in time logarithmic in the number of balls."
"end for 31: end for from the closest ball. these settings result in a parameterless algorithm. unlike, we set the feature space intrinsic dimension parameter to 2, as we empirically found that it does not affect performance significantly. a low value of intrinsic dimension corresponds to a \"flat\" data manifold when viewed in feature space. as this flatness is an intrinsic property of the data, we expect it to remain valid, at least to some extent, even when more complex features are added."
"we also tested these features with linear svm and 1-nn with dynamic time warping, two standard approaches generally used for these kinds of problems. as these two methods are highly inefficient, we did not calculate the performance on the larger ucf11 and virat datasets."
"we defined a truly streaming context for human activity recognition. we presented an incremental active recognition framework, well suited for streaming recognition problems, especially when the amount of data to process is large. our approach is simple and exhibits a number of desirable features: it deals with sets of local descriptors extracted from videos, it learns in an incremental fashion, it embeds an active learning module, it is capable of learning new classes on the fly, it limits memory usage, and it predicts new data in real-time. in addition, the method is nonparametric and does not require expensive validation sessions for training, as it has no parameters to be tuned. results demonstrate its competitiveness in terms of accuracy with respect to traditional batch approaches, as well as promising performance in a truly streaming scenario. the main two time consuming procedures are: 1) the feature extraction and 2) the nn search for the ball centroids. as previously mentioned, our method is independent from the feature extractor, that could be chosen based on the final application. in our experiments, we used dense trajectories, which is a real time feature extractor. on the other hand, the nn search could be easily parallelized using modern big data technologies (such as spark, mahout, etc.). there are several possible directions for improvement: [cit] (e.g for high-dimension features as for cnn-feature extractors), deriving a more so- fig. 3 . left: evolution of 10 action class probabilities over time for a test sequence containing three actions. middle top: the pink line represents the standard deviation of class probabilities for each frame of the same sequence. the cyan curve is their average standard deviation computed over a short interval of frames. for each segmented activity, we computed a confidence measure c i (y); the predicted activity label is discarded when confidence is below an adaptive threshold θ. right: examples of three time series associated with low, medium and high confidence, respectively. phisticated confidence measure, using an ensemble of our base learner (like random forest for decision trees), to name a few. future research will explore the use of confidence measures to automatically discover new activity classes by associating them with low confidence trajectories (see fig. 3, right), [cit] ."
", where the variance is set to the current ball radius π(t) . this confidence function is quite simple, and we are aware that more complex measures do exist. however, we view it as a plus that we manage to have good performance using a simple method, as we show in the experimental section. the local bandwidths of the rbf kernel are directly related to the complexity of the problem in each neighborhood of the feature space. indeed, in more difficult areas (small radii) the kernel penalizes distant samples from the current ball covering, when compared to less troublesome ones (big radii). given a test video v i, we thus define a confidence measure c i (y) on the estimate of the expected class conditional probability for any given class y as:"
"the first five datasets contain mostly footage material: we decided to extract efficient local features at frame level in order to focus on truly real-time prediction. in particular, from kth, ucf11, and virat sequences we computed improved dense trajectories [cit], due to their outstanding performance in action recognition tasks. for each video, three types of features were extracted, namely histogram of oriented gradient (hog), histogram of optical flow (hof) and motion boundary histogram (mbh). we ran the code published on the inria website 2, keeping all the default parameters except for the trajectory length, set to 8 frames, and the number of descriptor bins (16 bins for hog, mbhx and mbhy, and 18 bins for hof). every 8 frames we obtained a variable number of active trajectories. we then accumulated all the trajectories for each descriptor, and concatenated all the descriptors, obtaining a collection of vectors of 66 dimensions for each video. in this setting, each vector is a summary of the three local descriptors extracted from each video frame. for virat, we initialised the improved trajectory algorithm using the bounding boxes released along with the dataset. for the other datasets, we did not rely on any initialisation."
"the pervasive presence of cameras and mobile devices in our everyday lives has created a strong demand for automated methods able to analyse data streams in real time. this is especially challenging in the case of videos capturing human activities, as in tv footages and videos from surveillance cameras. another natural application is human robot interaction, which requires the machine to learn and recognise human behavioural patterns in real time. nevertheless, the mainstream approaches to action and activity recognition are typically based on an offline training phase (for a review of previous work in activity recognition we refer the reader to sec. 2). such a setting leads to several critical issues when dealing with streaming videos: how to incrementally learn activities from the incoming data?"
"note that the streaming setting used in our experiments is very strict: we do not use seed training sets, mini-batch training, cross-validation sets, or assume any preliminary knowledge on the number of classes. as the other incremental methods rely on much richer sources of information than those allowed in our streaming setting, we could only evaluate them in the batch and mini-batch setting."
"the resulting covering resembles a visual dictionary, learned incrementally and directly usable for predictions, where the balls play the role of visual codewords. finally, the active learning module defines the interaction between the learning system and the labeler agent, limiting the number of annotations requested."
"the fiver algorithm (see alg. 3) combines all the elements described above. namely, fiver trains the model over the video stream via alg. 1, while controlling the memory footprint as described in sec. 3.2. alg. 2 is the active learning module, which asks only for the most informative instances while not exceeding the budget rate. ignoring the cost of extract- end if 13: end for ing features (see sec. 4.1 for a discussion on this issue), the prediction and update time of fiver is dominated by the nn search, whose cost is small (i.e., logarithmic in the number of balls). the space requirement, instead, is clearly linear in the same quantity. however, the experiments we report reveal that a good classification accuracy can be achieved using a number of balls which is quite a small fraction of the training set size. this implies that, in practice, fiver really runs in real time."
"cardiovascular disease results in one death every 40 s in the united states [cit] . improved diagnostic, surveillance, and intervention methods would help reduce mortality [cit] and extend lives. heart disease may be detected using many non-invasive methods including manual auscultation of heart sounds, which is a common component of physical examinations and known to provide useful diagnostic information. however, simple auscultation of heart sounds is of limited utility. detection and processing of very-low-frequency heart sounds (\"infrasounds\") below the limit of human ear detection may extend the diagnostic power of auscultation. new studies of cardiac-generated sounds using computational fluid dynamics [cit] and advanced signal processing methods suggested increased potential to provide quantitative information that may be helpful for patient monitoring and diagnosis. seismocardiography (scg) is a noninvasive technique that measures cardiac-induced mechanical vibrations at the chest surface including those below the human hearing threshold. the reader is referred to previous scg reviews [cit] describing earlier studies, while this paper reviews more recent scg studies including advances in instrumentation and signal processing that hold the promises of increased clinical utility. since previous reviews of scg [cit] described early scg studies, this article is more focused on the developments in the field during the last few years. [cit], we conducted a search of the scientific journals and conferences using medline, as well as the google scholar search engine, for the following expressions: \"scg\", \"seismocardiography\", and \"seismocardiogram\". reviewing the reference section of the initial results led to additional articles. more resources were added during the manuscript preparation and revision. the final sampling period includes studies that were published after [cit] ."
"we can estimate how much each of these changes improved the evaporation model by including only one of these changes and fitting the parameters of these models individually; see figs. 4b, e and 8."
"early use of scg for cardiac diagnosis faced obstacles such as the large instrumentation size and unclear understanding of the signal characteristics and inter-and intra-subject variabilities. however, recent advances in sensor technologies and signal processing methods led, at least in part, to new numerous studies that provided better insight into these issues. the high morbidity and mortality associated with cardiovascular disease and the high cost of care may have provided motivation to more studies that re-evaluated the feasibility and utility of seismocardiography for diagnosis and monitoring of cardiac function [cit] . some of the studies reviewed here focused on telemonitoring of cardiac time intervals and heart rates."
"in conclusion, signal processing techniques and physiologic understandings rigorously applied may transform scg signal analysis from a research interest to a powerful bedside or home monitoring tool."
"however, moisture convergence, as it occurs, for example, in the itcz, is important for the transport of moisture in these regions. the mean convergence by advection including the moisture convergence term is"
we now test the new hydrological model in a series of three different sensitivity experiments. the discussion focuses on evaluating the new model. the three examples test the hydrological cycle model response to changes in the boundary conditions. these changes are beyond those used to fit the model parameterisation and can therefore be a test of the model's skill. we will leave more in-depth analysis of some of these experiments to future studies.
"while scg signals can contain useful diagnostic information, they are often contaminated by noise from different sources including sensor mechano-electronics, motion artefacts, and environmental vibrations. this signal contamination might result in errors in calculating scg features and eventually inaccurate signal classification, especially if automated scg processing is performed (i.e., without human supervision). for example, a recent study [cit] showed that, when"
"depending on the sensors that are used, scg signals might consist of one or more axial and rotational components. for example, a uniaxial accelerometer can be used to measure scg component in the dorso-ventral direction. however, combination of a triaxial accelerometer and triaxial gyroscope can provide information about axial and rotational heart-induced motion in three different directions. this review focuses on the dorso-ventral component of the scg signal, unless otherwise stated."
"applications of predictive methods such as machine learning are increasingly being used in biomedical signal processing, including for scg analysis. much inter-and intra-subject variability exists in scg signals and machine learning can be used to automatically recognize the underlying patterns. some of the applications of machine learning techniques include detection of cardiovascular disease, cardiac mechanics, and parameters affecting scg waveform such as respiration cycles."
"analysis of scg data recorded from the sleep patterns of a subject aboard the international space station (in microgravity) resulted in accurate identification of cardiac time intervals and scg fiducial points (such as ao, ac, mo, mc, lvet, and pep) with implications for future clinical application [cit] . as described earlier, scg morphology is affected by different factors such as the sensor location and respiration. investigating the effect of these factors on the estimation of cardiac time intervals from scg signals can possibly reveal clinically useful information."
"the greb precipitation model without ω sd has still fairly weak mean precipitation in the midlatitude storm track regions (compare figs. 5g and 2g ) and has a weak seasonal cycle with the wrong sign in these regions as well (compare figs. 5h and 3g) . the transient pressure systems in these regions lead to large vertical motions (ω) on shorter, daily timescales that result in large precipitation but have a nearzero ω mean . thus, to capture the precipitation in regions with strong variability in ω, but weak ω mean, we include ω sd . this mainly enhances rainfall in the midlatitudes and high latitudes (figs. 2g and 3g) ."
"one topic in climate change that deserves urgent attention is the changing pattern of the hydrological cycle [cit] . changes of rainfall have direct impact on the environment and on human health [cit] . the projections on how rainfall is changing are primarily based on coupled general circulation models (cgcms). cgcms evaluated by the intergovernmental panel on climate change (ipcc) for the fifth assessment report are among the most complex simulations of the climate system. however, it is far from trivial to understand even simple aspects of the climate system, as several processes interact with each other [cit] ."
"we estimate the effect that the change in reference climatologies will have on the new greb hydrological cycle model by fitting the parameters of the new model as described above to both the ncep and era-interim reanalysis. the resulting hydrological cycle models are evaluated against observations (gpcp and era-interim) in taylor diagrams for the annual mean. changing the reference climatology does not lead to major improvements in the representation of the hydrological cycle in the greb model, but it increases the correlation of precipitation, evaporation and circulation and reduces the rmse (fig. s1 in the supplement). [cit] ."
"today, smartphones and smartwatches are common and can be used as part of telemedicine for real-time patient monitoring at a relatively low cost. smartphones used scg for continuous monitoring of heart rate variability [cit], and cardiac activity of patients suffering from heart disease [cit] . in a recent study, the feasibility and accuracy of measuring heart rate using a smartphone accelerometer was assessed in different postural positions [cit] and suggested utility of scg for heart rate estimation."
"rainfall is generated by a multitude of different systems (e.g. midlatitude cyclones, tropical convection), which makes it one of the most complex processes in the climate system to model and thus to forecast. yet many aspects of the hydrological cycle (i.e. high precipitation in the intertropical convergence zone; itcz) seen in complex cgcms can be found in models with intermediate complexity such as the climber-2 [cit], the uvic earth system climate model [cit] . additionally, idealised models such as the ω [cit] are capable of representing many aspects of the climate change response seen in complex cgcms. simplified climate models and energy balance considerations are capable of explaining the large-scale features of the climate system and climate change (e.g. arctic amplification and land-sea contrast [cit] . they provide a framework to conceptually understand the hydrological response to climate change. because of their simplicity, they help to develop hypotheses about the processes involved."
"the simulation of precipitation and transport of moisture in the new hydrological cycle model is now comparable in skill to cmip models in terms of annual mean and the seasonal cycle of rainfall. the simulation of precipitation in the greb model is closer to the observed precipitation pattern than any cmip5 model in both the annual mean and the seasonal cycle. this is directly related to the fact that the greb mode has a prescribed atmospheric circulation, which is the main driver of the global precipitation pattern."
"the second phase corresponds to the development phase which begins with coding. the provided design is transformed into a programming notation by a programmer. unit testing and risk analysis are simultaneously performed, and testing is conducted to ensure that the developed code provides results based on the software requirement specification (srs) along with usability. after obtaining satisfactory results from the testing team, the product is deployed, and a deployment test is conducted with respect to the customer's environment."
the original greb model transport of moisture was very weak and had little agreement with observations (figs. 2f and 3f ). atmospheric transport of moisture in greb (eq. 4) is controlled by diffusion and advection with mean winds. this model considered a divergence free two-dimensional flow.
"a few studies addressed scg variability, e.g., the consistent effects of respiration [cit] . one recent study [cit] reported that the scg morphology appeared to mainly depend on the lung volume (and, hence, possibly the intrathoracic pressure), rather than dependence on negative or positive airflow (i.e., inspiration or expiration). this scg morphology variation can also be used to automatically identify the lung volume states and respiratory phases by employing machine learning [cit] . another study used support vector machines to classify the scg cycles occurring during the high and low lung volumes [cit] . successful grouping of scg cycles into two groups, where scg events in each group are more similar to each other and dissimilar to the events in the alternate group, would improve the signal-to-noise ratio in calculating the scg ensemble average. this would result in more accurate estimation of diagnostic information from the scg ensemble average."
"thus, the greb model is conceptually very different from the cgcm simulations in the coupled model intercomparison project phase 5 (cmip5), as atmospheric circulations, cloud cover and changes to soil moisture are not simulated but prescribed as external boundary conditions in the model. this leads to some parts of the hydrological cycle not being simulated in the greb hydrological cycle model (i.e. runoff). the effect of ocean circulation on the atmosphere is represented only through the sea surface temperature but is not explicitly simulated. additionally, the greb model has no internal variability, as atmospheric fluid dynamics (e.g. weather systems) are not explicitly simulated. subsequently, the model will converge to its equilibrium points (all tendency equations converge to zero), if all boundary conditions are constant. the control climate or response to forcings can therefore be estimated from a single year."
"in the seasonal cycle, each included variable improves the simulation of evaporation in the greb model (fig. 4e) . the seasonal cycle of flux corrections caused by evaporation in the original greb model is large over land and large over oceans. there are positive flux corrections around the equator and negative flux corrections over the oceans north of figure 5 . annual mean precipitation for four development steps of the greb precipitation parameterisation (a, c, e, g) and their corresponding seasonal cycles (b, d, f, h) in mm day −1 . the first step was changing the specific humidity boundary climatology (a, b). then subsequently more variables have been added to the precipitation parameterisation: adding only relative humidity (c, d), adding only ω (e, f), adding relative humidity and ω (g, h)."
"software development methodology is extremely important to enable each software development organization to develop a quality project within a given time period and budget. a review of extant literature reveals that it is necessary to determine the limitations and gap between different software development methodologies. the state of the art indicates that the main contradiction between different methodologies should include adoptability and predictability, should be people-oriented and process-oriented, requirements collection and requirement change management. in order to * fill the gap and eliminate the limitations of methodologies, a comprehensive software development methodology ''az-model'' is introduced. the proposed model is broadly divided into three phases, namely the customer involvement phase, development phase, and releasing phase. according to the proposed model, the customer is involved in sdlc until the completion of satisfactory design of the project. the effective involvement of the customer minimizes the occurrence of risks due to the changing requirements. furthermore, prior to commencing the development phase, unit testing is conducted in the design phase to analyze and minimize the risk in conjunction with usability testing. strong time-boxing and specialized project management ensures in-time delivery for proper workflow of the project as well as organization. specialized project management ensures that each phase and even each task of the project is well managed and available resources are maximally utilized. hence, effective timeboxing and good project management improves throughput of the organization."
"the simple globally resolved energy balance (greb) model was originally developed to simulate the globally resolved surface temperature and in particular its response to a co 2 forcing [cit] . the greb code computes about one model year per second on a standard personal computer. it therefore is a relatively fast tool, which allows conducting sensitivity studies to external forcing within minutes to hours [cit] . the hydrological cycle in the greb model was only needed as a zero-order estimate to model the latent heat in the energy balance and the atmospheric water vapour levels. this paper introduces a simple hydrological cycle model for the greb model. the aim of this hydrological cycle model is to present a simple and fast model for studies of the large-scale climate in precipitation, its response to climate variability (e.g. el niño or climate change) and external forcings. we improve three separate parameterisations in the model: precipitation, evaporation and the circulation of water vapour. the model is based on the dynamical variables (surface temperature, atmospheric temperature and humidity) in the greb model and on the boundary conditions of the greb model (horizontal and vertical winds)."
"while scg signals can contain useful diagnostic information, they are often contaminated by noise from different sources including sensor mechano-electronics, motion artefacts, and environmental vibrations. this signal contamination might result in errors in calculating scg features and eventually inaccurate signal classification, especially if automated scg processing is performed (i.e., without human supervision). for example, a recent study [cit] showed that, when determining the instantaneous frequency of scg signals using different time-frequency distributions, estimation accuracy differed significantly with the signal-to-noise ratio. these results indicated that some time-frequency distributions performed poorly in noisy conditions and would lead to inaccurate time-frequency features. it was then concluded that feature extraction methods might fail or, at a minimum, perform inaccurately for low signal-to-noise ratio conditions."
"the original greb precipitation model captures some largescale aspects of the mean and seasonal cycle of observed precipitation, such as more precipitation in the tropics and during warm seasons over land (figs. 2 and 3) . it has, however, substantial differences from the observed precipitation, as it cannot capture the high rainfall in the itcz and the enhanced precipitation over the midlatitude storm track regions, and misses many aspects of the seasonal cycle. the root mean square error for the annual mean of the original greb model precipitation parameterisation is 1.46 mm day −1 . the new parameterisation of precipitation in the greb model is assumed to be proportional to q air, as in the original greb model. we further assume that relative humidity, rq, and upward air motion, ω, increase rainfall. the latter is assumed to be a function of the mean and the standard deviation of the daily mean variation, ω mean and ω sd, respectively."
"for the new evaporation model, we retained the original bulk formula approach and included a few minor changes by considering land-sea differences, revised wind (u * ) estimates, scaled effectivity and skin temperature. the new evaporation model is"
"the sensitivity of the scg signal to sensor location is well known and, therefore, needs to be taken into account when comparing results from different studies. historically, investigators placed accelerometers at different anatomical locations, including the clavicle, the sternum, and various intercostal spaces [cit] . a recent study [cit] investigated the differences in scg signals morphology at the common auscultation sites of the four heart valves (aortic, pulmonary, tricuspid, and mitral), and found significant differences in scg morphology. that study also concluded, with the aid of sonographic measurements, that more feature points can be defined from multi-point scg measurements."
"the response of the hydrological cycle to global warming is one of the potential applications of the greb model and a comparison of the greb model with the cmip model simulations response to global warming provides a good test. the cmip5 ensemble mean response of precipitation shows a distinct increase of rainfall in the equatorial pacific, decreases of mean rainfall in some subtropical regions (i.e. east pacific) and increases in some areas of the midlatitudes; see fig. 11a . this pattern is normally referred to as the wet-getwetter paradigm [cit] . although this approach has been questioned by more recent studies [cit], it is still a good first-order approach to the changes in the global hydrological cycle, although changes over land might be muted or even reversed [cit] ."
"classification was also used to detect the respiration cycles (inspiration and expiration) [cit] and lung volume (high and low lung volume) [cit] . in one study [cit] classifying respiration cycles, two different training scenarios were implemented. the first was a leave-one-subject-out (loso) approach, which trained the svm on all but one subject, and tested on the subject who was left out. the second was a subject-specific (ss) approach, which trained and tested on each subject individually. the average accuracies for loso and ss were 88.1% and 95.4% respectively. other studies [cit] sought to classify scg signals according to the lung volume phases as opposed to inspiration/expiration. classification methods were also utilized to help identify fiducial points on the scg signals [cit], artefact presence in the scg [cit], and identification of the sensor location [cit] ."
"the ldv approach compares the frequency shift between the outgoing and reflected laser beams and determines the corresponding vibration velocity of the surface that reflected the beam [cit] . considerations when using ldv for scg measurements include the following: (1) the chest surface needs to be reasonably reflective for accurate ldv measurements, (2) the laser beam should be perpendicular to chest surface, and (3) chest movement due to respiration needs to be accounted for, since breathing causes the point of measurement to be displaced in the chest plane. one solution to this issue is to develop an algorithm that can automatically have the beam follow a measurement point on the chest surface. other ldv limitations include their cost and size."
"with each term on the right-hand side representing the fraction of the flux corrections attributed to precipitation, evaporation and circulation biases, respectively. each term is estimated as the difference between the observed and the greb model tendencies of the humidity resulting from precipitation, evaporation and circulation biases:"
"customer involvement significantly impacts sdlc. while comparing both methodologies, a customer is involved in heavyweight methodology only until requirement gathering, while customer is involved throughout the sdlc in lightweight methodology. often both the fore-mentioned methodologies exhibit negative impacts. therefore, according to the proposed model, a customer is involved until completion of a satisfactory design."
"extreme programming (xp) is the most commonly used method in agile methodology and involves the advanced form of the problems encountered in long development cycles of traditional development models [cit] . the xp procedure is described by short development cycles, incremental arranging, constant feedback, dependence on communication, and a transformative outline [cit] ."
"the quality factor is directly related to the inclusive success of the project. the quality factor is included in the survey questionnaire to examine a stakeholder's satisfaction, check the requirements for quality projects, and determine project success. a sanity check is performed to ensure the positive correlation of the quality factors with respect to the other five factors of project management. hence, the positive correlation among all other factors with the quality factor indicates that the proposed model with each individual factor affects the overall quality of the project."
the development of the new hydrological cycle model of the greb model is based on the existing zero-order hydrological cycle model of the greb model. the following section outlines the development of each of the three models and discusses how the change in the reference climatologies from ncep to era-interim has affected the model. all variables are summarised in table 2 .
"a few studies addressed scg variability, e.g., the consistent effects of respiration [cit] . one recent study [cit] reported that the scg morphology appeared to mainly depend on the lung volume in some applications, such as burn patients, highly infectious patients, and premature babies, attaching adhesive ecg electrodes or scg sensors would not be feasible. therefore, development of efficient contactless scg detection techniques are under investigation. these techniques include laser doppler vibrometry (ldv), microwave doppler radar, and airborne ultrasound imaging [cit] . a non-contact scg measurement might also reduce skin coupling artefacts that may be present in the scg signals acquired by the contact sensors attached to the skin."
"the ldv approach compares the frequency shift between the outgoing and reflected laser beams and determines the corresponding vibration velocity of the surface that reflected the beam [cit] . considerations when using ldv for scg measurements include the following: (1) the chest surface needs to be reasonably reflective for accurate ldv measurements, (2) the laser beam should be perpendicular to chest surface, and (3) chest movement due to respiration needs to be accounted for, since breathing causes the point of measurement to be displaced in the chest plane. one solution to this issue is to develop an algorithm that can automatically have the beam follow a measurement point on the chest surface. other ldv limitations include their cost and size."
"a main challenge in scg studies is that scg signal morphology appears to vary significantly, not only by cardiovascular pathology, but also normal inter-subject variation. these changes are affected by several factors including respiratory cycle phases, gender, age, sensor chest location, health conditions, cardiac contractility, heart rhythm, and postural positions [cit] . while these changes can lead to undesirable scg variability, deeper understandings of these processes will enhance our understanding of scg signals, help aggregate scg cycles into groupings with similar scg events to reduce scg signal variability and noise, and hopefully lead to more accurate definition of scg features for diagnoses and monitoring."
"if the result from the deployment test is satisfactory, then the product enters into third phase of az-model in which it is released in the market based on the nature and ownership of the product. the project manager releases the phase deal based on the nature of the stakeholder. time-boxing and strong project management are always involved very actively in conjunction with all processing phases of az-model. the hierarchical chart of az-model is listed in figure 2 ."
"scg is used to estimate different cardiovascular parameters such as cardiac time intervals, pulse transit time, and blood pressure. for example, non-contact scg was used at different body locations for estimating central arterial pressure and carotid arterial pressure waveforms [cit] . pulse transit time might be estimated from the time difference between ao point on the xiphoid scg and ao point on the carotid scg [cit] . blood pressure changes can be monitored using pulse transit time. for this purpose, the pulse transit time, which was defined as the time required for the blood pressure wave to travel from one location to another [cit], was first measured from the scg signals [cit] . the measured pulse transit time was then used to estimate the patient blood pressure [cit] . based on similar techniques, a wrist-watch, consisting of an accelerometer and an optical sensor, was developed to monitor blood pressure [cit] . in this \"seismowatch\", the blood pressure was estimated from the travel time of the micro-vibrations propagating from the heart to the wrist when the watch was held against the subject's sternum. in a different study, di [cit] developed a system that measures scg and ppg at multiple locations alongside the ecg signal. the pulse transit time may then be derived from the ppg."
"the six-pointed star model's factors are divided into two triangles. the scope, schedule, and budget factors on a triangle (fig. 6 ) are termed as input or output project factors. however, the risk, resources, and quality factors on the other triangle are termed as process factors (fig. 7) . the schedule factor handles in-time completion of a project. project milestone and srs implementation is controlled by the scope factor, and the budget factor is used to satisfy the budget, requirements, and return on investment. risk is analyzed and managed by the risk factor, the availability of resources is managed by the resource factor, and finally, the overall project success and satisfaction is maintained by the quality factor. hence, it is considered that the aim of all the factors involves checking the validity and efficiency of the proposed model [cit] . in table 1 . in order to elicit respondent opinions, the likert scale was used, as shown in table 2 . each response is associated with a numerical score that generates the numerical response of the collected data. thus, the proposed model is implemented in 24 software development organizations, and responses are obtained from 22 of the fore-mentioned organizations (91.67%) that nearly completed the projects using az-model. table 3 describes the size of a respondent organization. table 4 represents the model that is used by the respondent organization before or along with the proposed model. while developing the project, (i) the confidence of team members and the confidence for moving the project into the next phase significantly impacts the project quality. therefore, in order to produce a quality product, team confidence between the team members and between the different phases is extremely important. table 5 shows the confidence of team members while developing the project at each step while developing a specific phase or moving to the next phase by using the proposed model. survey respondents were related to each field of development or included specialist members such as project managers, requirements engineers, analysts, designers, coders, testers, and marketer. experience of the respondent is also important with respect to project expertise. thus, based on the general survey information, 41.5% respondents possess less than 2 years of work experience, and 33.0% respondents possess 3 years to 5 years of work experience, and 25.5% of the respondents possess more than 5 years of work experience. hence, table 6 shows the size of the respondent organizations. the organizations with 10-49 employees have a response rate of 36.4%, organizations with 50-250 employees have a response rate of 31.8%, and the organizations with more than 250 employees have a response rate of 31.8%."
"digestive state and mood may affect cardiac function through similar physiological mechanisms, thereby possibly affecting scg signal morphology. systematic investigations of these effects are lacking, and future studies are needed to determine the magnitude and nature of these effects on the scg signal."
"feature extraction is yet another step of scg signal processing. identifying the most significant signal features can result in efficient signal classification since these features are eventually the inputs to machine learning algorithms. determining the most effective and accurate techniques to extract specific signal features is a necessary step that should be done before identification of useful features. for example, there are different methods for estimating the time-frequency distribution of the scg signal. every method has its own advantages and disadvantages, and might be suitable for certain types of signals or under certain conditions. several studies were done to determine the most accurate methods for extracting time-frequency features of the scg signals [cit] . in these studies, different time-frequency distribution techniques were utilized, including short-time fourier transform [cit], polynomial chirplet transform (pct) [cit], wavelet transform with different mother functions [cit], wigner-ville distribution, and smoothed pseudo wigner-ville distribution (spwvd) [cit] . pct and spwvd were found to have the most accurate time-frequency distribution estimations and appeared more suited for determining the frequency content of scg signals. using these methods, scg signals of healthy subjects were found to contain three main spectral peaks below 100 hz."
"the precipitation response in the original greb model is positive in all locations and it closely follows the pattern of specific humidity in the control simulation (see eq. 1 and fig. 11d ). this is mainly due to an increase in the saturation water vapour pressure of about 7 % per degree of warming (clausius-clapeyron). the original greb precipitation response pattern is not correlated to the cmip5 ensemble mean response pattern (fig. 12a), suggesting that local differences in the precipitation response are very different from those in the cmip simulations."
"quality describes the customer satisfaction as well as development organization. while determining the customer satisfaction, a triangle that consists of the time, budget, and customer expectation requirements is considered. thus, from an organizational viewpoint, another triangle of workflow, goodwill, and business is considered. hence, the proposed methodology can produce quality products for both customers and organizations."
"owing to specialized project management in the proposed model, each phase and even each task is well managed. hence, maximum utilization of available resources and strong time-boxing aids in increasing the throughput, and this is extremely effective for an organization from the business prospective."
"based on the literature review, software development methodologies are broadly divided into two categories, namely heavyweight and lightweight methodologies. both the methodologies are not yet satisfied because heavyweight methodologies are process-oriented, predictable, and less accepting of changes, while lightweight methodologies are people-oriented, adoptable, and easily accept changes in requirements. however, both methodologies are important for software development organizations."
"in order to overcome the key limitations of the waterfall model, an iterative model of software development was introduced [cit] . in this approach, requirements are collected, and the project is developed and delivered to the customer through iterations. every delivered iteration is in addition to the already delivered iterations [cit] ."
"growth in the field of seismocardiography accelerated during the last decade. however, open issues and limitations hamper its clinical application. reviewed here are some of the current limitations along with potential future work."
"software engineering is a growing and emerging field in the world since software makes life more comfortable. figure 1 illustrates software engineering conceptions. the importance of software is undeniable. specifically, in the present time frame, the fact remains that computers are indispensable in today's world due to their widespread use in almost every field of life, especially in commerce, industry, medicine, education, engineering, and agriculture."
"sensors are most commonly placed on (or directed to) the sternum or its left lower border. however, in some studies, other locations were used for scg signal acquisition, including over the heart apex (lateral left lower chest) and the \"aortic valve listening area\" at the right upper sternal boarder [cit] . information about sensor type, model, and placement location in recent studies is summarized in table 2 and figure 2 . in some applications, such as burn patients, highly infectious patients, and premature babies, attaching adhesive ecg electrodes or scg sensors would not be feasible. therefore, development of efficient contactless scg detection techniques are under investigation. these techniques include laser doppler vibrometry (ldv), microwave doppler radar, and airborne ultrasound imaging [cit] . a non-contact scg measurement might also reduce skin coupling artefacts that may be present in the scg signals acquired by the contact sensors attached to the skin."
"the utility of ultrasound imaging in estimating non-contact two-dimensional (2d) scg maps of the body surface was also investigated [cit] . in addition to the advantages of other contactless measurement methods, this technique can collect scg data from multiple locations through different channels resulting in a potentially higher reliability. however, this method requires a planar measurement surface that is parallel to the emission panel."
"wearable scgs might be contaminated with different type of noise. therefore, investigating the effective noise removal techniques for ambulatory subjects is needed. a few ongoing studies are addressing this question. these studies were described in section 2.1."
"we applied the new hydrological cycle model to a number of sensitivity studies, which illustrated that the new hydrological cycle model is much improved over the original greb model. the annual cycle simulation without any correction terms is very realistic with the new model, and the precipitation response to enso events is now very similar to the observed, due to the much-improved transport of moisture. finally, the response to global warming now shows a precipitation response pattern that is comparable to that of the cmip models. again, a limiting factor in this sensitivity experiment was the evaporation response of the greb model in comparison to that of cmip models."
"in summary, machine learning algorithms were used for different purposes in scg studies, including scg classification into different phases of respiratory cycle (e.g., high vs. low lung volume), determining fiducial points (e.g., im and ao) and cardiac time intervals (e.g., pep), and classification of subjects into patients and low risk/normal. a summary of the machine learning algorithms used for scg analysis is listed in table 5 ."
"abstract. this study describes the development of the hydrological cycle model for the globally resolved energy balance (greb) model. starting from a rudimentary hydrological cycle model included in the greb model, we develop three new models: precipitation, evaporation and horizontal transport of water vapour. precipitation is modelled based on the actual simulated specific and relative humidity in greb and the prescribed boundary condition of vertical velocity. the evaporation bulk formula is slightly refined by considering differences in the sensitivity to winds between land and oceans, and by improving the estimates of the wind magnitudes. horizontal transport of water vapour is improved by approximating moisture convergence by vertical velocity. the new parameterisations are fitted against the global precipitation climatology project (gpcp) data set and reanalysis data sets (era-interim). the new hydrological cycle model is evaluated against the coupled model intercomparison project phase 5 (cmip5) model simulations, reduction in correction terms and by three different sensitivity experiments (annual cycle, el niño-southern oscillation and climate change). the skill of the hydrological cycle model in the greb model is now within the range of more complex cmip5 coupled general circulation models and capable of simulating key features of the climate system within the range of uncertainty of cmip5 model simulations. the results illustrate that the new greb model's hydrological cycle is a useful model to study the climate's hydrological response to external forcings and also to study inter-model differences or biases."
"in order to prove the effectiveness of the proposed model, a survey was conducted to collect the opinions of the proposed model from the users, and statistical tools are applied to analyze the collected data. the designed questionnaire contains different queries regarding general information on the respondent organization, and six-pointed star models were used to collect the technical feedback about the proposed model. hence, the proposed model is implemented in 24 well-known organizations although responses were obtained from 22 organizations that almost completed the project by using az-model. statistical tools were applied to calculate the mean, standard deviation, and correlation at 95% confidence level between the factors of the six-pointed star model to present the effectiveness of az-model. statistical results listed in table 8 show the cumulative mean and percentage for each factor. hence, the percentage of agree and strongly agree significantly exceed other elements of the likert scale, and this indicates the positive impact of az-model on the six-pointed star model. table 9 presents the correlation of quality factor with respect to other factors of the six-pointed star model, and table 10 shows the most positive effective factor of the proposed model. the statistical results reveal that az-model is extremely effective for software organizations to produce a quality product within a given time and budget."
"figs. 2 and 3 for the original greb model as discussed above. the diffusion term is only one-fifth of the magnitude of the advection term in global average (not shown) but is more important in some locations and therefore not ignored in the greb model. the original greb model simulated some of the main features of the regional differences in the precipitation and evaporation, but many important details are missing (e.g. itcz, subtropical dry regions or extratropical storm tracks). however, horizontal moisture transport is not simulated well by the original greb model."
"while the relationship between scg waves and cardiac activity is not fully understood, several studies investigated this relationship. for example, scg was reported to contain a low-frequency wave during atrial systole, a high-amplitude wave during ventricular systole, another wave during early ventricular filling, and some relatively high-frequency waves at the time of the first and second heart sounds [cit] . simultaneous recording of scg and electrocardiogram (ecg) indicated that the peaks and valleys of the scg correspond to known physiological events including mitral valve opening (mo) and closure (mc), isovolumetric contraction, ejection, aortic valve opening (ao) and closure (ac), and cardiac filling [cit] . the utility of scg in estimating cardiac intervals such as electromechanical systolic pre-ejection period (pep) and left-ventricular ejection time (lvet) was also shown [cit] . multi-channel partially simultaneous scg, ecg, and sonographic measurements were used to identify the feature points in a cardiac cycle corresponding to the four common valvular auscultation locations. using these measurements, new feature points (including left-ventricular lateral wall contraction peak velocity, septal wall contraction peak velocity, trans-aortic valve peak flow, transpulmonary peak flow, trans-mitral ventricular relaxation peak flow, and trans-mitral atrial contraction peak flow) were reported. table 1 lists all the scg feature points and cardiac time intervals (ctis) that were reported in the literature, while figure 1 shows a modified wiggers diagram [cit] where a sample scg signal (in the dorso-ventral direction) is plotted along with aortic blood pressure, ventricular volume, and the electrocardiogram. during the cardiac cycle of healthy individuals, the apex and base rotate in opposite directions, which results in a twisting motion of the left ventricle [cit] known to be affected by different factors such as aging and diastolic dysfunction. investigating the rotational vibration induced by this heart twisting motion might provide complementary information to the current scg analysis of uni-and triaxial accelerations. in recent studies [cit], a three-axis micro electromechanical systems (mems) gyroscope and a three-axis accelerometer were used simultaneously to measure the rotational and axial components of chest vibrations. the potential utility of the combined analysis of axial and rotational heart-induced vibrations were suggested for the ecg-independent identification of systolic points (such as ao and ac) and cardiac time intervals (such as lvet and pep) [cit] ."
"the original greb model was evaporating too much on the annual mean (see fig. 2e ) especially over the equatorial pacific and atlantic. the new hydrological cycle model parameterisation largely decreases evaporation over these regions and the flux corrections are reduced over the globe in the annual mean (fig. 6e, f) . the correlation of the annual mean experiences the largest changes from changing the reference climatology (fig. 4b) ."
"in summary, the new greb model does simulate the precipitation and circulation response to enso conditions fairly well, whereas the original greb model has very little skill, illustrating the significant improvement of the new greb model over the original greb model. however, the evaporation response in both models is not as well simulated as the precipitation and circulation responses."
"while scg signals can contain useful diagnostic information, they are often contaminated by noise from different sources including sensor mechano-electronics, motion artefacts, and environmental vibrations. this signal contamination might result in errors in calculating scg features and eventually inaccurate signal classification, especially if automated scg processing is performed (i.e., without human supervision). for example, a recent study [cit] showed that, when figure 3 . map of root-mean-square (rms) amplitude of scg waves at the chest surface using scanning laser vibrometry. there were local amplitude maxima that coincided with the aortic, pulmonary, tricuspid, and mitral auscultation areas. these data suggest that sensor location and size need to be chosen with care and that the effects of sensor misplacement need to be quantified."
"the above-stated methodologies are plan-driven and strong document-oriented methodologies. hence, highly rigged methodologies should be considered. [cit] . these methodologies are very flexible and also known as lightweight methodologies [cit] . many lightweight methodologies exist, and a few of these are discussed in the present study."
"exercise and the following period of recovery was also demonstrated to produce changes in scg signals. not surprisingly, exercise is associated with an increase in the overall amplitude of the scg signal, measured as the root-mean-square (rms) power [cit] . this increase in signal amplitude was shown to correlate with increased cardiac output observed during exercise [cit] . this cardiac output increase is a result of increased heart rate and stroke volume. exercise also produces changes in the left-ventricular ejection time (lvet) and the pre-ejection period (pep) [cit] . as exercise increases the heart rate (also seen as the r-r interval decrease in ecg), it generally causes a decrease in other measured time intervals such as lvet and pep. lvet correlates with both heart rate and contractility and, hence, decreases with exercise. pep is less affected by heart rate but does decrease during exercise due to the increased contractility (inotropy). these changes in lvet and pep were detectable by scg [cit], and exercise-induced decrease in pep was found to shift the scg signal power spectrum toward higher frequencies [cit] ."
"in summary, despite many studies conducted about scg genesis, the relationship between scg waves and cardiac activity is not yet fully understood. this is possibly because of the waveform variations in different studies and lack of understanding of the exact scg waves sources. thus, there is still a need for widely accepted universal labeling (i.e., valid for all/majority of patients) analogous to pqrst labeling in ecg."
"a rational unified process model (rup) was introduced with a parallel working style in which the new iteration commences prior to releasing the current iteration, and this is extremely time effective [cit] ."
"the observed evaporation response to enso events in the tropical pacific somewhat counteracts the precipitation response, as we observe mostly decreased evaporation over regions with enhanced precipitation and increased evaporation over regions with reduced precipitation (fig. 10a and b) . these evaporation changes are mostly caused by changes in winds, with decreased evaporation over regions where the winds have weakened (e.g. nino3.4 region). the new greb model somewhat captures this pattern but shows a stronger evaporation response, which partly explains the weaker precipitation response. however, both the original and the new greb model evaporation simulations have only a weak spatial correlation (0.3) with the observed evaporation changes overall."
"various other approaches are introduced for project management and include rapid application development (rad) [cit], crystal clear [cit] s), [cit], prince 2 [cit] . these are generic methods of project management adopted by different organizations based on projects requirements."
"to evaluate the greb hydrological cycle model independent of the other greb model components, such as the t surf tendencies, we force the original and new greb models with rcp8.5 equivalent co 2 concentrations and all other input variables for the hydrological cycle model taken from cmip model simulations. that is, we add t surf, horizontal winds and vertical velocity rcp8.5 cmip5 ensemble mean anomalies from the models described in table 1 on top of the greb control reference climatologies. in the control run, the reference boundary conditions of t surf, horizontal winds and ω are taken."
"in addition to human studies on healthy populations, there were several studies that focused on the application of scg in patients with cardiovascular disease. scg signals were used for diagnosis and monitoring of different clinical conditions such as atrial fibrillation [cit], atrial flutter [cit], heart valve disease [cit], coronary artery disease and ischemia [cit], myocardial infarction [cit], heart failure [cit], structural heart disease [cit], and heart stress testing [cit] ."
"the survey response of the six-pointed star model is summarized in table 7, which presents all the achieved frequencies from the respondents. the frequencies and percentage of computed frequencies are declared in the form of a likert scale. each part of the likert scale significantly impacts the analysis of the significance of the proposed model. hence, volume 6, 2018 +strong agree is presented in figure 8 . in table 8, the average and percentage of each likert scale for all the six-pointed star factors are provided to analyze the significance of the proposed model based on the respondents. the average score and percentage is calculated as related to 2-3 questions for each factor. the likert scale is shown in table 2 . for example, the schedule factor corresponds to question numbers 2.1, 2.2, and 2.3, and thus, the accumulative average and percentage of all the questions is computed. the schedule factor represents that 25.76% respondents are neutral while 36.36% agreed and 31.82% respondents strongly agreed with respect to the scheduling of the proposed model. thus, the average and percentage of all the factors indicate positive responses from the respondents with respect to the proposed model."
"the mean vertical air motion (ω mean ) provides a substantial improvement of the precipitation model ( fig. 4a and d comparing marker \"0\" to \"c\"). ascending air masses in the itcz lead to increased precipitation, whereas descending air masses (i.e. in the subtropics) suppress precipitation. it creates a sharper and more realistic gradient in precipitation than the original greb model (compare figs. 2d and 5e ). with the addition ω mean, greb is in the range of uncertainty of more complex cmip5 models in the annual mean and the seasonal cycle ( fig. 4a and d) ."
"in this study, we introduced the newly developed hydrological cycle model for the greb model. it consists of three parts: precipitation, evaporation and transport. the development of these models started from the existing zero-order hydrological cycle model of the greb model and used physical reasoning and observations for fitting parameters."
"a main challenge in scg studies is that scg signal morphology appears to vary significantly, not only by cardiovascular pathology, but also normal inter-subject variation. these changes are affected by several factors including respiratory cycle phases, gender, age, sensor chest location, health conditions, cardiac contractility, heart rhythm, and postural positions [cit] . while these changes can lead to undesirable scg variability, deeper understandings of these processes will enhance our understanding of scg signals, help aggregate scg cycles into groupings with similar scg events to reduce scg signal variability and noise, and hopefully lead to more accurate definition of scg features for diagnoses and monitoring."
this section discusses statistical analyses based on numerical results collected from the survey respondents that determine how the management factors are related to each other and how az-model affects each factor of the six-pointed star model. the collected results from the respondent are discussed in a statistical form.
"with the diffusion term, κ · ∇ 2 q air, the advection term, u · ∇q air, and the flux correction term, q correct . the simulated annual mean and seasonal cycle for precipitation, evaporation and mean horizontal moisture transport are shown in"
"with the new parameterisations for precipitation, evaporation and circulation, the new greb model resolves the seasonal cycle better than the original greb model (fig. 9) . the seasonal cycle of the original greb model was too weak in the northern hemisphere when compared to observations, and throughout the year the greb model was too dry (fig. 9b) . for the southern hemisphere, the original greb model was too wet. the new greb model captures the high humidity in northern hemispheric summer and the low values in winter (fig. 9c) . this makes the seasonal cycle stronger in the new greb model and it is closer to the reference climatology. in summary, the new greb hydrological cycle model simulates the seasonal evolution of the atmospheric humidity very well and significantly better than the original greb model."
"some studies sought to use classification tools such as support vector machines (svm) and neural networks (nn) to automatically detect cardiovascular disease. an early study using nns [cit] classified patients based on their scg as either having coronary artery disease (cad) or as low risk/normal. they predicted cad with a sensitivity of 80% and a specificity of 80%. recent studies [cit] sought to classify cardiovascular conditions with scg signals obtained via a smartphone's inertial measurement unit (imu). a multi-class classifier was used [cit] to classify subjects as either having st-elevation myocardial infarction, having atrial fibrillation, being preferred for percutaneous coronary intervention procedure, or normal. the proposed classifier achieved classification accuracies between 70 and 79%. however, the same study [cit] created a binary classifier (normal vs. atrial fibrillation) and achieved an accuracy of 98.7% using an svm."
"the waterfall model is the first, most influential, and most commonly used process model [cit] . this model was recommended by royce and includes a linear or sequential execution of stages in a way such that the previous phase provides feedback to the subsequent phase, and this typically follows the system design corresponding to the most significant process model [cit] ."
"the response of the hydrological cycle to seasonal changes is a good test for evaluating the skill of the hydrological cycle model. the greb model applies monthly flux correction terms to maintain a mean atmospheric humidity as observed. thus, by construction, the specific humidity in each calendar month in the greb model is identical to the observations; see fig. 9a ."
"scg signal processing usually consists of several steps including preprocessing (e.g., downsampling and denoising), signal segmentation, feature extraction, and classification ( figure 4 ). there were several recent studies that focused on noise removal, segmentation, and feature extraction of scg signals. these studies are reviewed in this section."
"the proposed model includes highly calculated tasks. the developing team works within the prescribed errands defined by the project management team and follows all the instructions or standards. with respect to requirement gathering, the engineers simply collect the specified requirements. the design is developed under the required boundaries, and thus, all the phases include extremely calculated tasks."
"other frequency-domain features include frequency coefficients such as amplitudes and frequencies. features were either obtained by taking the frequency amplitudes across a range of the frequency spectrum (0-512 hz) [cit], or by taking the frequencies and amplitudes at specific peaks of the spectrum, such as the first, second, and third peaks [cit] ."
"the following section presents the data sets used, the original greb model and the methods. in sect. 3, the new parameterisations of the hydrological cycle model in the greb model are described. section 4 presents three different sensitivity experiments to test the new hydrological cycle model. finally, we give a discussion and summary of the results."
"with eq. (1), which corresponds to an autoregressive model with a decorrelation (recirculation) time of about 14 days [cit] . evaporation, q eva, in the original greb model is calculated using an extended bulk formula:"
"the utility of ultrasound imaging in estimating non-contact two-dimensional (2d) scg maps of the body surface was also investigated [cit] . in addition to the advantages of other contactless measurement methods, this technique can collect scg data from multiple locations through different channels resulting in a potentially higher reliability. however, this method requires a planar measurement surface that is parallel to the emission panel."
"project management plays a vital role in the success of a project. model development and project management are equally important for the successful completion of a project. they blend together to form a complete methodology to deliver a high-quality product to the customer. throughout system development, the pm and software development model (sdm) collectively function to achieve milestones. the overall flow of the project is defined beginning with planning, and this is followed by dividing the entire project in to small and manageable segments. good project management requires specialization of a project manager in relevant areas. this is because a skilled project manager can efficiently manage any required changes during monitoring, given the complexity of the phase for achieving milestone [cit] . the four major variables in a project include time, cost, quality, and scope [cit] . the project diamond is shown in figure 3, in which only scope is free in time-boxing."
"the measurements of heart-induced motion, including displacement, velocity, and acceleration, were performed as early as the turn of the 20th century [cit] . these approaches can be categorized into two classes [cit] : (a) the measurement of whole-body recoil forces in response to cardiac ejection, usually termed ballistocardiography (bcg); and (b) the local chest surface measurement of cardiac-induced vibrations, typically referred to as scg [cit] . these vibrations are usually measured in the form of acceleration (m/s 2 ). this article focuses on the latter."
"the constant c eva modifies the evaporation efficiency for a given mean wind speed, u * . q sat−skin is an estimate of saturated humidity considering skin temperature. it is calculated using"
author contributions. cs developed the new hydrological cycle model code and together with dd designed the sensitivity experiments. dd provided the original greb model code and nl performed preliminary tests with precipitation.
a future study will involve examining the effect of different software development methodologies on the six-pointed star model and a brief comparison of az-model with other software development methodologies in statistical form.
"early bcg studies [cit] suggested that heart-induced motion may be used to estimate changes in cardiac output, and reported certain signal patterns in patients with myocardial infarction [cit] . these signal patterns were also found to correlate with the strength of myocardial contractions [cit] and contain detectable waveform changes during heart disease resolution [cit] s [cit], recent studies suggest possible scg utility for monitoring left-ventricle function, coronary blood flow during balloon angioplasty [cit], heart and breathing rates [cit], and ventricular filling, cardiac valve closure, and ventricular ejection preceding the onset of ischemic symptoms [cit] ."
"therefore, the results of the correlation indicate that the quality factor is positively correlated with other factors of the six-pointed star model that represents project success. the range of correlation values from 0.63 to 0.82 reveals that the factors are highly correlated. hence, the positivity of the correlation describes the effect of the proposed model on the five factors of the six-pointed star model for the quality factor. table 10 analyzes the effect of individual factors of the proposed model. for this purpose, the mean and standard deviation of each factor of the six-pointed star model is computed. there are 22 respondents and 2-3 questions for each factor. only 15 questions are related to the six-pointed star model. therefore, the collected data is used to compute the average and standard deviation values. mean and standard deviation shows the factor-wise levels of satisfaction. therefore, the results of the schedule factor show the extent of satisfaction with a factor relative to the other factors."
"microwave doppler radar is another non-contact method that can be used for scg measurements. when recording the scg signal using microwave doppler radars, the scg will exhibit in the phase variation of the microwave signal. scg signals can then be extracted from this phase variation. like ldv sensors, doppler radar approaches have the benefit of contactless signal acquisition, but suffer from the reflection of background microwave signals (called radar clutter) lowering the signal-to-noise ratio [cit] ."
"piezoelectric and mems sensors are smaller and lighter than contactless sensors. therefore, these sensors might be used in clinical settings for everyday and continuous screening subjects suspected of different cardiovascular diseases."
"the observed humidity tendencies resulting from circulation, q circul−greb, are defined by the residual of the total humidity tendency minus the precipitation and evaporation tendencies. by construction, all three flux correction terms (evaporation, precipitation and circulation) sum up to the total flux correction term."
"the evaporation has only improved slightly but does simulate the annual mean values fairly well. however, it is still different from the observed seasonal cycle and the skill is much lower than that of the cmip model. this suggests that the evaporation model is still a limiting factor in the greb model."
"the original greb hydrological cycle model, which is the starting point for this study, is shortly presented below. all variables and parameters are listed and explained in table 2 . the precipitation is proportional to the specific humidity,"
"in summary, the new greb precipitation model is significantly better than the original model. the rmse is reduced by 0.65 to 0.81 mm day −1 in the annual mean and by 1 mm day −1 in the seasonal cycle. greb precipitation now has a comparable skill to more complex cgcms and lies within the range of uncertainty of cmip5 modelled precipitation. introducing the new precipitation parameterisation globally reduces the flux corrections of specific humidity caused by precipitation; see fig. 6c and d. the root mean square of the flux corrections caused by precipitation is reduced by more than 40 %, indicating that the new parameterisation has indeed improved the simulation of the hydrological cycle in the greb model. similar improvements are gained for the seasonal cycle ( fig. 7c and d) . the original greb model showed large flux corrections, especially in the tropics where the itcz moves with seasons and in the midlatitudes. the pattern of the flux corrections of the new model still looks similar to the original model but is only half as large in amplitude (figs. 6c, d and 7c, d )."
"scg signals are believed to be caused by cardiac mechanical processes including cardiac muscle contraction, cardiac valve movement, blood flow turbulence, and momentum changes. the characteristics of these signals are likely to contain useful information that correlate with cardiovascular physiologic [cit] and pathologic processes [cit] . such information may powerfully complement methods of detecting heart electrical activity (such as electrocardiography), serologic testing, and imaging modalities (e.g., echocardiography, cardiac magnetic resonance imaging (mri), and catheterization)."
"in the original greb model, evaporation is calculated using a widely used bulk formula approach (see eq. 1 [cit] ) . this model does capture the main aspects of the regional differences in the annual mean evaporation in greb, with enhanced evaporation over subtropical oceans and weaker evaporation over land (fig. 2e) . the seasonal cycle (fig. 3e) is, however, very different from observed, and the land-sea differences are too strong."
"an interesting aspect of the greb model is that it has the atmospheric circulation (vertical and horizontal winds), humidity and surface temperatures as boundary conditions. this allows the greb model to be used as a diagnostic tool to understand how different boundary conditions affect aspects of the climate system, such as the hydrological cycle's response to global warming. it may also help to study how biases in the hydrological cycle in cmip models related to different boundary conditions from the atmosphere, such as biases in the vertical winds. [cit] links circulation biases in cmip models to biases in precipitation and moisture. forcing greb with the circulation of cmip models could shed light on how discrepancies in circulation between cmip models affect the hydrological cycle. the new greb hydrological cycle model is therefore a good tool in helping to conceptually understand the hydrological cycle and its response to global warming or other external forcings. it will further help in understanding cmip model biases in the simulation of the hydrological cycle. figure 11 . response of the hydrological cycle to an rcp8.5 forcing in the cmip5 ensemble mean for precipitation (a) in mm day −1, evaporation (b) and circulation (c) in kg m −2 s −1, original greb model for precipitation (d), evaporation (e) and circulation (f) and the improved greb model for precipitation (g), evaporation (h) and circulation (i). greb uses prescribed anomalies from the cmip5 ensemble mean of surface temperature, horizontal winds and vertical winds (ω). all responses are shown per degree of warming. figure 12 . rcp8.5 response of cmip5 models (blue), original greb (0) and improved greb (red ) per degree of global warming against the cmip5 ensemble mean (black star). precipitation (a), evaporation (b) and circulation (c) are shown. greb uses prescribed anomalies from the cmip5 ensemble mean of surface temperature, horizontal winds and vertical winds (ω). the correlation of the original greb model precipitation response with the ensemble mean is zero. the original and improved greb models have zero correlation with the ensemble mean evaporation and the standard deviation is 1 for both."
"this effectively corrects the greb model to have a climatological specific humidity as observed. the flux correction term, q correct, can help to evaluate the improvements in the hydrological cycle model. the better the model the smaller the correction term should be in eq. (4). we can therefore split the flux correction into three diagnostic terms:"
"the equator (fig. 7e) . the improved evaporation seasonal cycle mainly removes this distinct pattern over the oceans and reduces flux corrections over most land areas. (fig. 7e, f) . overall, the new evaporation model is slightly better than in the original greb model, but it still has substantial limitation in simulating the seasonal cycle correctly (figs. 2h, 3h )."
"the original greb model used climatological fields from the national centers for environmental prediction (ncep) [cit] for surface temperature, t surf, specific humidity and horizontal winds. the cloud climatology is taken from the international satellite cloud climatology project [cit] . topographic data are taken from the echam5 atmosphere model [cit] . for the development of the new greb hydrological cycle model, we replaced the ncep reanalysis boundary conditions for t surf, [cit] . era-interim reanalysis has a higher accuracy than ncep and a better agreement with observations [cit] . the reasoning for the changed data sets is further explained in sect. 3.4. precipitation from reanalysis products is influenced by the underlying cgcm [cit] and is therefore taken from observations from the global precipitation climatology project (gpcp) [cit] . the climatological boundary conditions and constraints for the greb model are summarised in fig. 1 . in the following, we refer to these data sets as observations."
"in summary, successful feature extraction from scg signals results in a more efficient classification of these signals. different studies that investigated the utility of various feature extraction methods/algorithms in both time and frequency domains were described in this section. however, more studies can possibly lead to improve the available methods and define more effective features. in addition, the features currently extracted from scg signals can be categorized into intuitive (e.g., lvet) and non-intuitive (e.g., skewness) features. future studies can also address the question of which intuitive or non-intuitive features can be more useful in classification of scg signals. a summary of these features are listed in table 4 . table 4 . summary of the features used in machine learning algorithms for scg signal analysis."
"statistical time-domain features include those based on the entire signal, and those from divided segments of the signal. features from segments of the scg signal were obtained by dividing the scg signal into a specific number of equal-sized bins and calculating the arithmetic mean of each bin as a feature [cit] . similarly, one study divided the signal into bins; however, binning of the signal was performed discriminately, where the signal portions corresponding to higher variation received a higher concentration of bins [cit] . that algorithm divided the bin corresponding the highest standard deviation in a recursive fashion, until some criteria, such as reaching the desired number of bins, was met. other statistical time-domain features, such as mean, kurtosis, skewness, and standard deviation were also extracted from the scg signal [cit] . time-domain features also included features related to cardiac mechanics, heart rate and heart rate variability, and turning point ratios [cit] . in addition, when the ecg r and q information is concurrently available with the scg fiducial points (ao, ac, mo, and mc), certain intuitive time-domain features can be determined. these include ctis (e.g., pep, isovolumic contraction time (ivct), lvet, and isovolumic relaxation time (ivrt)) and other metrics such as pep/lvet ratio, (ivct+ivrt)/lvet (also called myocardial performance index), and the lvet/r-r-interval ratio [cit] . amplitudes and slopes associated with the fiducial points, such as mc to ao slope, were used in some studies [cit], as well as features of the scg signal that do not depend on specific fiducial points, such as maxima, minima, and their associated widths of specific segments of the scg signal [cit] ."
"the quality of the software mostly depends on the software development life cycle (sdlc). the sdlc is a route used by software development industry to design, develop, and test high-quality software. the aim of sdlc involves producing high-quality software that meets or exceeds customer expectations, reaches completion within time, and cost estimation, and is directly related to the customer as well as organizational satisfaction. it is a priority for every organization to adopt a low-cost software development model. if the low-cost model can effectively produce high-quality software, then it should be adopted to enjoy long-term benefits [cit] . it is necessary for every organization to search for high-quality and low-cost software development models. hence, it is considered that a good sdlc captures, verifies, and implements user requirements within the time-box and bought [cit] ."
"we therefore conducted a set of sensitivity experiments with the greb model forced by the mean conditions for strong el niño and la niña events. the greb model was forced with mean composites of t surf, horizontal winds and ω from observations for four el niño (1982 /83, 1987 /88, 1991 /92, 1997 /98) and la niña (1988 /89, 1999 /2000, 2007 /08, 2010 events. the anomalies are calculated around el niño/la niña from may before the peak in december to april in the following year and against the climatological mean. in the greb model simulation, they are added on top of the reference climatology. the observed anomalies in the hydrological cycle during these el niño events are shown in fig. 10a-c . the skill of simulating la niña events are qualitatively the same. we clearly note strong regional changes in the precipitation in the tropical pacific that match changes in moisture transport (fig. 10c), illustrating that el niño-southern oscillation (enso) events mark strong regional changes in the hydrological cycle related to changes in the circulation."
"to illustrate that the seasonal cycle is not a feature of the seasonally varying flux corrections, we changed the flux corrections to an annual mean value for the original greb model (fig. 9b, e) and for the new greb model (fig. 9c, f) . this annual mean flux correction value is added on every time step to the tendency equation of specific humidity (eq. 4)."
"presently, global business is the key interest of every organization. therefore, software development organizations currently conduct business all over the word for economic feasibility. specifically, in developing countries, software development cost is extremely low and is almost one-third lower than that in developed countries [cit] . there are several other causes of outsourcing by software organizations such as availability of skilled human resources and reduced work load [cit] . nevertheless, huge risks are involved in outsourcing and include progress incompatibility, coordination problem, cultural differences, and slightly hidden costs [cit] . however, several solutions and causes exist with respect to outsourcing software projects [cit] . in this situation, heavyweight methodologies are most effective because there is no need to involve customers throughout the sdlc in heavyweight methodologies. requirements are collected in the parent country and are handed over to the contractors of developing countries after analysis to implement the next required phases of the sdlc. in agile methodologies, informal communication is considered more valuable than formal communication, and thus, there is limited opportunity for agile methodologies [cit] ."
"historically, feature extraction of scg signals mostly focused on the time domain and the frequency domain, separately. the time domain features include statistical features, such as mean, median, and standard deviation, and features related to cardiac mechanics, such as cardiac time"
"new advances and availability of lightweight low-noise accelerometers improved the quality of recorded scg signals. different methods were used for scg measurement in the recent studies, including the following:"
"other machine learning methods were used on scg signals such as hidden markov models (hmm) and graph similarity analysis. an hmm-based method was used in one study [cit] to estimate the heart rate, heart rate variability, and ctis from an scg signal. a graph similarity analysis [cit] was used in another study through the use of k-nearest neighbor graphs on scg signals from hf patients to identify them as compensated (outpatient) or decompensated (hospitalized)."
"wearable technologies can continuously monitor cardiac activity outside clinics and hospitals. this continuous monitoring might help in early detection of serious cardiac conditions, which can enable timely intervention and potentially reduce healthcare costs. most current wearable cardiac activity monitoring techniques are based on ecg measurements. however, recent studies proposed wearable scg systems for the assessment of the mechanical aspects of cardiovascular function, including relative changes in cardiac output, contractility, and blood pressure [cit] . scg wearable monitors might be used to assess myocardial contractility via pre-ejection period (pep) [cit] . another wearable system utilized triaxial accelerometers and gyroscopes to record all six axial and rotational components of the scg signals [cit] . the rotational vibration about the longitudinal (head-to-foot) axis showed a lower sensitivity to walking noise than other components, which might be useful for annotation of scg signals in ambulant subjects [cit] ."
(17) the second term on the right-hand side was not considered in the original greb model but is now considered in the new model. the moisture convergence term can be approximated by knowing the vertical air flow assuming continuity and hydrostatic balance:
"microwave doppler radar is another non-contact method that can be used for scg measurements. when recording the scg signal using microwave doppler radars, the scg will exhibit in the phase variation of the microwave signal. scg signals can then be extracted from this phase variation. like ldv sensors, doppler radar approaches have the benefit of contactless signal acquisition, but suffer from the reflection of background microwave signals (called radar clutter) lowering the signal-to-noise ratio [cit] ."
"traditionally, the success effecting factors of projects include time, cost, and scope [cit] . the project management body of knowledge (pmbok 4.0) introduced an advanced model of triple constraint that is based on six factors, namely schedule, scope, budget, risk, resources, and quality, which are extremely important for the success of a project [cit] ."
the proposed methodology is highly visible to present the tasks that should be followed by the developing team members. the methodology also possesses the capacity to control the overloading and enables in adjusting the workflow gap. visibility is also helpful in determining the time and cost of the project.
"the improved greb model response pattern is similar to the cmip models with enhanced and reduced response roughly at similar locations, which leads to a much improved correlation (fig. 12a and c) . this is strongly related to the moisture transport changes. however, the overall global mean precipitation response in the new greb model is shifted upwards compared to the cmip5 ensemble mean, which is related to the much stronger response in evaporation (compare fig. 11b and h) . in cmip5 models, we see a muted response of evaporation mainly due to changes in surface relative humidity and surface stability [cit] ."
"in order to improve the existing software development methodologies, several initiatives were recently adopted in different organizations, although they were not very successful. extant studies indicated that various organizations do not use formal software development methodology and instead only use self-created methodology [cit] . the most important causes for the non-adoption of formal software development methodology include the unavailability of suitable, adjustable, and flexible methodology according to the organizations and project requirements [cit] . furthermore, a review of previous studies described the reasons for lack of adoptability of development methodologies as pertaining to the non-suitability for specific social and cultural characteristics with respect to development teams and organizations [cit] ."
"this section describes the collection and analysis of the data. during the selection of the sdlc method, only experiencebased selection criteria should be followed as opposed to any other numerical formula."
"a spiral model is another example of the iterative model in development and from the delivery viewpoint. in the spiral model, prototyping and design elements are combined in a stage [cit] . four major phases are involved in this model as follows: i) objective, ii) risk, iii) development and validation, and iv) planning [cit] ."
"in order to determine the validity and efficiency of az-model, individual opinions are collected. furthermore, the data is collected from organizations after applying the proposed model. the popular six-pointed star model of project management is used to describe the effect of proposed model to the project, as shown in figure 5 ."
"strong el niño and la niña events lead to significant changes in the tropical precipitation and associated hydrological cycle changes. since these natural modes of climate variability are well documented, they present a good test case for the greb model."
"the observed strong changes in the circulation of atmospheric humidity (fig. 10c) is mostly due to changes in the convergence of moisture (e.g. ω). since convergence of moisture was not considered in the original greb model, the simulated changes in the circulation are very weak in the original greb model (fig. 10f) . the new greb model does consider convergence of moisture and simulates the changes in the circulation of atmospheric humidity very similarly to the observed (fig. 10i) . the new circulation parameterisation in the new greb model improves the correlation between the observed and the simulated circulation tendency from 0.3 (original greb) to 0.95."
"ecg is currently the main diagnostic method of atrial fibrillation (af). a preclinical study [cit] investigated the usefulness of scg for af detection. results suggested that the amplitude of the scg signal correlates to beat interval and significantly varies from beat to beat during af. this study also suggested that the combination of scg and ecg may reveal certain behavior in the electromechanical delay characteristic of af, which may lead to extra indicators for early detection of af."
"fitting the evaporation efficiency c eva and the turbulent wind factor improves evaporation over land, especially in the seasonal cycle (fig. 8d), and reduces the strength of evaporation over the ocean. the increase in evaporation over land is caused by the increase in the turbulent wind factor. c eva would decrease the evaporation in the annual mean and the seasonal cycle. by including the new estimate of monthly mean wind speed u *, the pattern of evaporation is getting closer to observations, especially over the oceans (i.e. fig. 8f, north atlantic), and by including the new estimate of skin temperature the seasonal cycle is improving slightly (fig. 4e) ."
"piezoelectric and mems sensors are smaller and lighter than contactless sensors. therefore, these sensors might be used in clinical settings for everyday and continuous screening subjects suspected of different cardiovascular diseases."
"scg signal processing usually consists of several steps including preprocessing (e.g., downsampling and denoising), signal segmentation, feature extraction, and classification ( figure 4 ). there were several recent studies that focused on noise removal, segmentation, and feature extraction of scg signals. these studies are reviewed in this section."
"code availability. the greb model source code used in this paper as well as the data used to run the model are available on github: https://doi.org/10.5281/zenodo.2232282. the github repository contains detailed documentation on how to download the source code and installation instructions, along with an example script on how to plot data obtained from greb model simulations. the greb source code is tested on recent-generation mac platforms."
"vision is an important driver of our posture [cit] . this has implications for contemporary digital device usage: if a work environment does not facilitate good vision and good visual ergonomics, then there is an increased risk that a worker will adopt awkward postures to see their work [cit], experience visual discomfort (also referred to as computer vision syndrome) [cit], or be less productive [cit] ."
"from (6), we see that a captured image can be represented by a number of p-frames, either ct-predicted or wt-predicted, each having a comparatively small transmission cost q( p i ��») i p i �� \\ but all together comprising a large storage i: lpi(� ) i . when a is small, h ' the penalty on large storage is negligible and multiple p-frames are attractive. on the other hand, when a is large, the penalty on large storage cost becomes expensive and one single representation of the picture as 1-or m-frame with relatively large transmission cost but small storage is more preferable."
"portability is a key feature of mobile devices. one way this is achieved is by locating the input area (e.g. keyboard, touch screen) in close proximity to the visual display. this then poses a dilemma for the user. if they choose to maintain a comfortable arm posture, then they will need to flex their head and neck forward to view the display, as has been shown to occur with laptop computers [cit], tablets [cit] and smartphones [cit] (see fig. 1 ). if the user opts for a comfortable head and neck posture by holding the device higher, then there is increased static muscular load on their shoulders and arms [cit] (see fig. 2 )."
"it is possible to separate (or de-link) the input and visual display area to facilitate good physical and visual posture. for example, when working with a laptop, use a laptop-station together with an external keyboard and mouse [cit] (see fig. 3 ). however, making these modifications means that the user needs to purchase and carry additional accessories. these accessories will add to the weight and bulk of what needs to be carried, which can negate the advantage of having a portable device [cit] ."
"there is evidence that 60 minutes continuous reading from a smartphone is associated with visual discomfort [cit] and ocular surface disorders such as dry eye [cit] . there also appears to be an association with hours of smartphone / computer use and dry eye in children [cit] but this work only reports total hours use per day, not continuous use. the impact of intermittent device use and multiple device use on visual comfort and ocular health and the maximum recommended use time requires further investigation [cit] ."
"fps games are one of the most popular types of game being played in the current market [cit] . fpss are categorized for their combat nature and fast paced action. these games are generally made up of bots that navigate the environment, shoot at enemies, and pick up items of interest. ai research using fps games has gained considerable attention in the research [cit] . during the last decade, research into fps games has continued to increase. a neural network (nn) was used to train the weapon selection module of unreal tournament bots [cit] . the results showed that the bots trained against the base ai had improved performance, and the bots trained against the harder ai were not as competitive, but had improved slightly. in a similar environment an evolutionary nn was used to train hierarchical controllers for fps bots [cit] . three controllers for shooting, exploring and path following were evolved individually, and then an evolutionary algorithm (ea) was trained to decide when to use each of the controllers. the results showed that bots using the evolved controllers were not able to outperform the hard-coded full knowledge bots, but they were able to play the game quite well. a nn was also used to learn sub-controllers for the movement of an fps bot and found that it was able to outperform controllers using decision trees and naïve bayes classifiers [cit] ."
"the foundation underpinning many of the visual ergonomics principles we apply to today's technology were established several decades ago. these principles can be used to prevent discomfort and injury, improve health and increase work performance."
"unlike conventional multiview video coding (mvc) schemes that focus on compression of all frames, in this paper, we address the frame structure design problem for interactive multiview video streaming with view synthesis (lmvs-vs), where a user watches a single video view at a time, but can request view-switch from the server to an arbitrary view every h frames. operationally, the server sends two closest coded views to the client so she can synthesize the desired intermediate view using image-based rendering (lbr) techniques. using 1-, p-and m-frames (distributed source cod ing (dsc) frames are our chosen implementation of m-frames) as building blocks, we formulate the structure design problem as a la grangian minimization, and develop a greedy algorithm to generate good structures. the key observation is that unlike previous imvs structures that relies on cross-time p-frames (each predictively coded using a frame of previous time instant in a decoding path), our struc ture optimization judiciously adds within-time p-frames as well, so that a frame in one transmitted view can be predictively coded using a frame in the other transmitted view of the same time instant, given server transmits two coded views for client's view synthesis. experi mental results demonstrate our optimized structures offer noticeably better streaming rate / storage tradeoffs than i-frame-only structures and previous imvs structures."
"as we age, our ability to accommodate (change focus between viewing distances) decreases. this is called presbyopia, and typically manifests at about the age of 40 as a difficulty focussing at close distances. presbyopia can be corrected with spectacles or contact lenses."
"one shortcoming for imvs is that available views for a client are limited by the few discrete number of camera-captured views pre-encoded at server, which means a view-switch can appear abrupt and unnatural to a viewer. in this paper, we introduce arbitrary view switching: in addition to camera-captured views, virtual views in-between captured views can also be requested by clients. theo retically, arbitrary view switching offers viewers an infinite number of views, so that a view-switch can now take place between views as close/far as the user desires. a virtual view can be synthesized using images at two nearest captured views via image-based ren dering ( i br) [cit], or recently popular depth-image-based rendering (dibr) [cit] . in the latter case, both texture and depth images at captured viewpoints need to be available for view synthesis. en coding both texture and depth images at multiple camera-captured viewpoints is commonly called the video+depth format [cit] . though we focus in this paper on ibr and encode texture images only, our proposed structure optimization can potentially handle video+depth format as well: we can use the same optimized structure to encode texture and depth images of respective viewpoints separately."
"a convention for lighting design in office environments is to locate overhead luminaires to the side of workstations [cit] . in this way, luminaires are not located in front of or above the worker, shining in their eyes and creating discomfort from direct glare [cit] . nor are the luminaires directly behind the worker casting shadows on the workstation [cit] or producing specular or diffuse reflections on the computer displays which can reduce contrast and task visibility and contribute to visual discomfort [cit] ."
"the second design convention is to not have large differences in luminance contrast between objects; otherwise the brightness difference between objects can be distracting [cit] and contribute to visual discomfort [cit] . examples of non-optimal conditions which could be experienced by mobile workers include working near windows or luminaires which are significantly brighter than the digital display, or the reverse: using a digital display in low light conditions where the display is significantly brighter than the surroundings. in a study comparing smartphone use in dark versus lit conditions, visual symptoms and eye discomfort was greater when reading from a smartphone in the dark [cit] . antona and coauthors comment that although smartphone devices may have an auto-brightness function which automatically adjusts the screen luminance to match the ambient lighting conditions, they have observed that the screen luminance is too great when using the smartphone in the dark [cit] ."
"to achieve portability, many mobile devices have a small visual display. therefore users are faced with a trade-off between enlarging the font size so that it is comfortable to read versus reducing the font size so that they can view the entire screen page without excessive scrolling. short of ensuring that digital interfaces are designed to maximise font size and usability (which will be discussed in section 3 on design strategies), if an individual needs to read from their device for prolonged periods then they should use a larger size device which allows them to view a larger size font [cit] ."
"much of the rest break literature for computer use relates to data-entry type tasks performed on a desktop computer. this body of knowledge has demonstrated that eye and vision comfort can be improved by taking rest breaks without any decrement in productivity or performance [cit] . the rest break schedules reported in these experiments include 5 minute breaks every 30 minutes [cit], a micro-break every 15 minutes [cit], 30 second breaks every 15 minutes and 3 minute breaks every hour [cit] and regular breaks scheduled over an 8 hour period [cit] ."
"this section looks at the results from the five game designers using the interactive training tool to train bots to play a first person shooter game. feedback was gathered from the users to find out what type of bot they tried to train. the first section will compare the feedback with the data from the training phase. the second section will investigate the results from the five user trained bots playing against each other. figure 3 displays the state-action value functions represented by a colour scale visualisation to show the similarities and differences between the trained bots. user 3 clearly has the most active policy with the majority of states having adjusted values, and very few areas where the state action pairs have not been visited. this activity user 1 tried to create an item collecting bot that shot at range then moved into melee. table 2 shows an even number of ranged and melee actions being used, with melee (six actions) being more successful than the ranged action (two successes). the health and ammo item actions were also evenly used with 11 health and six ammo item successes. therefore, overall a general type bot was attempted to be trained."
"in words, the transmission probability of a coded frame fi,j is the sum of probability ji (x, 2i) of synthesized view x, where view x is interpolated using coded frame pairs 2i's that include fi,j. b). storage cost: for a given frame structure t, we can define the corresponding storage cost by simply adding up the sizes of all the coded frames fi,j's in t, i.e.,"
"there are many visual ergonomics issues associated with mobile work environments. in this article they are grouped into the following categories: device location (viewing height and horizontal location relative to the user), display characteristics (font size), lighting (reflections, glare and ambient light), agerelated needs (older age workers, children and youth) and rest breaks."
"this article has presented an argument for visual ergonomics health literacy, based on the success of general health literacy programs. further research is required to progress this concept. for example:"
"this section looks at the results from a game played with five trained bots, one from each user, fighting against each other. no ai controlled bots were included in the games. the simulations were run for 12000 game ticks or iterations. an iteration was a complete update cycle of the game and was used to be consistent over all replays. due to there being multiple rl bots, using rl iterations would only be relevant to one of the five bots. 50 games were played and the results averaged. figure 1 shows the five user trained bots playing against each other. figure 4 maps the number of kills versus deaths to represent an overall combat strategy based on maximising kills and minimising deaths. the number of deaths scale on the y axis was reversed as the best strategy has the lowest death count. the figure indicates that user 5 has the best combat strategy bot, being the only one in the first wave on the pareto front. user 5 had the highest number of average kills of 4.8, and although they did not have the lowest number of deaths of 3.5, they were still able to dominate the other bots in combat. the next front had user 3 in it with an average of 3.9 kills and 3.7 deaths, with user 1 in the next front with 2.7 kills and the lowest number of deaths of 2.2. users 2 and 4 were dominated by the other three bots with average kills of 1.8 and 2.8, and deaths of 3.0 and 3.5 respectively."
"as yet there are no publications reporting comfort and ability for people with presbyopia using handheld mobile devices, but logic suggests that if the device is held in the reading plane (i.e. where one would hold a book to read) then general purpose progressive and bifocal lenses would be suitable for use, just as they are for reading books. the key difference between reading from hardcopy documents and reading from small devices such as smartphones is the viewing distance: hardcopy documents are typically held about 40cm from the eyes, whereas smartphones may be held at 35 cm or less [cit] . therefore, if an olderage worker reads from the phone at a close viewing distance, then they may require a stronger prescription in their spectacles to clearly see the display."
"the tradeoff points of the proposed algorithm are plotted in fig. 7, where different view-switch distance l's are used. we can see that increasing l makes the performance deteriorate dramati cally. this is intuitive; more coded frames are generally required to handle larger view-switch distance, resulting in larger storage."
"user 4 aimed to create a bot that fled the stronger enemies but attacked the weak ones. the data does not reflect this training as the hide action was only selected once by the user. however, this failure may be an indication of why the user felt the training did not work well for them. improvements need to be made on the hide behaviour so that it is useful for the intended purpose as user 4 assumed."
"in our proposed imvs-vs system, videos from k o closely spaced cameras in a id array capture a scene of interest for later streaming and playback. given k o captured views, one can design a frame structure at encoding time to optimally trade off expected transmis sion rate at stream time and storage size of the entire structure. note that given client has the ability to synthesize any intermediate view using images of two closest captured views via ibr or di br, one can choose a subset of k coded views from k o captured views for coding at server instead, and still enables arbitrary view switching at client. doing so would mean coding fewer total views, result ing in a better transmission rate / storage. however, it also results in higher synthesized view distortion, since the two closest captured views used for view synthesis are now further apart. we assume k of k o views were pre-selected a priori based on minimum synthe sized view quality requirement as specified by the application."
"a client can request video of an arbitrary viewpoint, and can switch viewpoint every h frames. to facilitate view synthesis at the client side, the server always transmits two neighboring coded views, and the client uses view interpolation to generate the virtual view [cit] . the two boundary views, view 0 and view ko -1, must be pre-selected as coded views to enable synthesis of any virtual view between view 0 and view ko -1. in addition, the server encodes every h'-th frame of each coded view as an i-frame, h' » h, to permit some required level of random access."
"observation of the games showed that user 1 was extremely competent in health item collection, and often avoided ammo items in favour of health. this behaviour is also reflected in the values recorded for health and ammo collection. user 1 was the second best at collecting health items with an average of 18.0 health items per game, whereas they were the second lowest in ammo collection at 8.7 items per game (see figure 5a ). this bot rarely used the wander behaviour, which corresponds to feedback from user 1 that they wanted their bot to always try to move with an intention."
"there is scope for incorporating visual ergonomics principles into the design of devices to improve comfort and efficiency. however, if digital device use is ubiquitous, then individuals require the knowledge (and the motivation) to improve their own visual ergonomics work conditions. it is important to address this now before we have a widespread epidemic of discomfort and injury from not applying sound visual ergonomics principles to work environments."
"given a fixed number of coded views k, one can construct a redun dant frame structure t of the multiview sequence to enable arbitrary view switching for imvs-vs. t is redundant in the sense that an original captured image can be represented by multiple coded frames f i �� ) 's. for example, in fig. 5(a), captured image of instant 2 and view 0 is represented by two coded frames, f�, � and f��j."
"the human eye's ability to converge and to focus its crystalline lens to see an object at a near distance is better when looking in a downward gaze compared to an upward gaze [cit] . this has implications for the vertical location of digital displays: if a computer display is located above eye height then a worker is more likely to tip their head backwards to view the display [cit], increasing their neck flexion, which in turn can contribute to physical discomfort. the literature reports that visual and physical comfort can be improved if visual displays are below eye height [cit] but they should not be located so low that it induces increased neck flexion and head tilt to view the display [cit] ."
general purpose progressive lenses and general purpose bifocals are contraindicated for desktop computer monitors because they promote a chinup/head-forward posture for the wearer in order to see through the reading portion of their spectacles [cit] . this can contribute to physical discomfort from an increased postural load in the neck and shoulder region [cit] .
"as shown above, the last augmentation does not increase the number of representations of a given coded view, while each of the first two increases the size by one p-frame. the above process re peats to find the most locally beneficial augmentation at each it eration, update the corresponding schedule and compute local la grangian cost, until no more lagrangian cost reduction can be found. note that after updating the local schedule at each iteration, it is pos sible that some coded frames in ti are not used by any view-switch. in this case, those unused coded frames will be removed from the structure to save storage."
"in details, we describe the algorithm as follows. first, as ini tial solution for ti, we construct an m-frame for each coded view j at instant i, where all viable view-switches to view j from coded frames fi-1,k's in 7i-1 could transition. we then determine the corresponding schedule 9i and compute the local lagrangian cost in (6) . given the initial solution, we improve the local structure ti by iteratively making augmentations: selecting a candidate from a set of structure augmentations that offers the largest decrease in local lagrangian cost. the augmentations include:"
"a major consequence is that office workers are no longer constrained to a designated workstation. activity based work areas are a feature of many offices, allowing workers the flexibility to work from a variety of workstations according to their task. workers may also have the flexibility to work from home, in transit and in vehicles, or be required to work from digital devices in-the-field. in some cases, workers are termed \"nomadic\" if they travel most of their work time to meet and/or collaborate with others inside and outside of their organisation [cit] ."
"the user trained bots have shown greater diversity in their policy landscapes and behaviours than in previous research of automatically trained bots [cit] . the policy landscapes were especially varied in user 3 and 5's bots, both of which spent more time training than the other three users. the bots produced from the varied policies appeared better in their behaviours than the other three bots. an example of the diversity in bots can be seen by the bot user 1 trained, which was very good at health collection but not as competitive in combat, whereas user 3 also produced a very good health collecting bot and was also very good in combat."
"we can see that (9) is the discrete domain equivalent expression of (2). it is clear that the larger the segment number n is, the closer (9) approximates its real value defined in (2)."
"the purpose of this article is to provide an overview of visual ergonomics issues related to mobile work environments, identify some of the gaps in our knowledge, discuss potential opportunities for incorporating visual ergonomics principles within design standards, and propose the concept of visual ergonomics health literacy as a way to mitigate the physical challenges associated with mobile working."
"the world health organisation defines health literacy as \"the cognitive and social skills which determine the motivation and ability of individuals to gain access to, understand and use information in ways which promote and maintain good health\" [cit] . people with low health literacy are more likely to have difficulty understanding and assimilating health information, for example, being able to read the label of a prescribed drug [cit] . low health literacy is associated with poorer health outcomes and higher mortality, increased health costs for individuals and in the united states, 3-5% additional cost to the healthcare system [cit] . conversely, high health literacy is associated with better health outcomes."
"xes'm yesn assuming all virtual views within one segment use the same coded frame pair for synthesis (true for large enough n), we can define segment display probability q: (m, bi) as the probability that a synthesized view in segment sm is synthesized at the client using coded frame pair bi at instant i. similar to (1), we can recursively compute this probability using ai [n -m], i.e.,"
"therefore, in addition to implementing strategies to improve the design of digital devices, we also need to communicate recommended practice guidelines to end users so that they can make informed choices when working in varied environments [cit] . this could be termed \"visual ergonomics health literacy\"."
"office workers in \"traditional\" offices with designated workstations may benefit from this knowledge by having ergonomics assistance to adjust their workstation, or they may be exposed to ergonomics guidelines or information through education programs within their workplace. unfortunately, this time-honoured visual ergonomics knowledge is not reaching a new and rapidly growing body of the population who use digital devices in mobile (or flexible) environments. consequently, these people are at risk of developing visual-related occupational health issues due to exposure to adverse physical work environments."
"example of imvs frame structures for two views, where circles and rectangles denote 1-and p-frames, respectively. (i, j) denotes a frame at instant i of view j. (a) structure with i-frames only; (b) structure with an initial i-frame and all p-frames. different tradeoffs between those two extreme structures in terms of storage rate and expected streaming rate."
"emails were sent to five game designers working in the computer games industry that have worked on commercial shooter games. the designers were asked to train a bot using the supplied training tool, and to email the results back along with answers to some questions regarding their training experience. data was recorded for all user actions, including reward and penalty frequencies, and whether the guide actions failed or succeeded after the action was pressed."
"there have been studies investigating the postural implications for the horizontal location of portable devices with desktop computer monitors. for example, szeto and co-authors demonstrated increased static muscle activity in the neck and shoulder muscles of subjects who turned their head to view an angled computer monitor for 20 minutes [cit] but less static muscle activity when subjects actively worked between dual computer monitors [cit] . szeto pos- tulates that varying the head posture to view dual desktop monitors may reduce static muscle tension in the shoulder and the neck [cit] ."
"if a person views small size characters on a visual display, then they have two options: maintain a long viewing distance with the risk of not being able to discern detail within the characters (i.e. the character size is too small to read) or make a postural adjustment to increase the character's angular size on the eye's retina (i.e. so that the character size is large enough to read). the latter can be achieved by a person moving their head and torso closer to the desktop display [cit] or by holding the display at a close viewing distance [cit] ."
"one issue which has recently captured attention in the scientific literature is a possible link between myopia and sunlight exposure. for many years there has been debate whether myopia has a genetic basis or is the result of environmental exposures such as performing close work [cit] . high proportions of myopia have been linked to education levels [cit] with associated blame on using digital devices. emerging evidence indicates that childhood myopia can be slowed by spending time outside in daylight [cit] and there is conjecture that increasing the amount of daylight through school windows may reduce the risk [cit] . so far, this evidence applies to myopia in children, not adults. this research has implications for children using digital devices: if children are encouraged to work outside, then they will need to be mindful of postural and lighting issues as discussed in sections 2.1 to 2.4 above."
"the results showed that user 5 only using manual rewards seemed to perform better than using the automatic reward function in regards to training the bot that they wanted and seeing immediate feedback during the training session. forcing manual rewards only should improve some of the issues with training not seeming to be working for some types of bots, especially those that differ from the path the automatic reward system steered them towards. this issue is clearly seen in the policy landscapes of the trained bots. users 1, 2 and 4 did the least amount of training (as seen in the training results listed in table 2 ) and the policy landscapes were all very similar. user 3 performed extensive training using automatic rewards, and was able to produce a more varied landscape for their trained bot. user 5 had an extremely different policy landscape to the other users due to only manual rewards being used, and they were the most successful in creating a bot that they wanted in accordance to their feedback. these results imply that the automatic training reward feature is too forceful for allowing full customisation for user guided training."
"users 2, 4 and 5 produced bots with a similar health collecting ability with averages of 6.0, 10.7 and 7.7 respectively. while these bots were capable at health item collection, they frequently chose different actions during non-combat states such as wander, and ammo collection. although user 2 had the lowest health collection rate, they scored extremely high in the ammo item collection task with an average of 38.1, almost double that of the next highest scoring bot from user 5 who had an average of 19.9 (see figure 5b) . observation of one game showed user 2 spending a lot of time wandering around an area of the map with number of ammo items, which could be attributed to this very high number."
"game ai has many parameters which are usually balanced or fine tuned by developers once all the features of the game are complete. the job of tuning parameters can be time consuming due to the large number of them. some researchers have attempted to tune these parameters using genetic algorithms [cit] . an fps environment was used with the parameters being the behaviour of the bots [cit] . bots were tuned from unreal tournament (epic games) and found the tuned bots were better, in terms of kills, than the standard ai in the game [cit] . the popular counter strike (valve) game has been used as a test bed for fps research [cit] . they found that evaluation times were extremely long as the rendering could not be \"turned off\", and therefore only 50 generations were completed. results showed that after only 50 generations the evolved bots had a slight advantage. other research evolved the behaviour of bots in an open sourced fps engine called cube [cit] . evaluation was performed by the author manually playing against the bots with an evaluation function consisting of the bots health, kills and deaths. results showed that the bots evolved into capable enemies from initial useless ones."
"if the end user has visual ergonomics knowledge (and by extension, visual ergonomics health literacy) then there is potential for them to apply basic visual ergonomics principles to future and novel work scenarios. this concept is similar to the participatory ergonomics philosophy whereby workers become ergonomics literate and have the skills to become their \"own ergonomist\", are able to make appropriate ergonomics interventions for current and future problems within their workplace, are aware of their knowledge limitations and are therefore able to seek additional help when necessary [cit] . although this type of strategy has been reported as successful for office workers [cit] and for teaching healthy computing skills to school children [cit], hägg cites an example of workers becoming injured when ergonomics information was misapplied: therefore, if workers are being their \"own ergonomist\", then they needs to have access to good quality ergonomics knowledge and understand how to apply it correctly [cit] ."
"we now present a greedy algorithm to generate a good frame struc ture for imvs-vs problem defined in sec. 3. we first introduce the concept of segment to facilitate the calculation of frame transmission probability q( fi,j), then discuss the algorithm in details."
"fi,jet c). transmission cost: given a frame structure t and associ ated schedule g, transmission cost is defined as the sum of the sizes of all the coded frames fi,j's in t, scaled by the corresponding frame transmission probabilities q( fi,j) 's:"
"visual ergonomics is a particular challenge because digital technology is developing at a fast pace and people interact with devices in a wide variety of ways [cit] . although guidelines exist for healthy use of digital devices, for example, notebook personal computers [cit], computers for use by children [cit], electronic games for use by children [cit], it is difficult to predict future forms of technology and the ergonomics challenges these may pose. hence it is difficult to develop prescriptive advice which can be applied to all the different ways people may interact with devices and to future technology designs."
"depending on the decoding path traversed by the user, differ ent coded frames representing the same original captured image will be transmitted. hence a transmission schedule g associated with a given redundant structure t is needed. let 2i be the pair of coded frames cached at the decoder at instant i when the user requests a virtual view y for instant i + 1. the schedule g dictates which pair of coded frames, 2i+1o should be transmitted at instant i + 1, such that virtual view y can be synthesized at decoder. in this paper, we denote a scheduled transmission to view y by g as: 2i � 2i+l,"
"multiview video refers to videos captured synchronously by multiple closely spaced cameras. for the sake of raw compression gain, tradi tional multiview video coding schemes (mvc) [ 1, 2] exploit inher ent inter-view and temporal correlation of all captured video frames across time and view, resulting in complex inter-frame dependencies in the mvc frame structures. these complex inter-frame depen dencies translate to a high transmission rate for applications such as interactive multiview video streaming (lmvs) [cit], where a client watches only a single view at a time, but can periodically request switches to neighboring views from server every h frames, as the single-view video is streamed and played back in time. this is be cause typical mvc frame structures are not optimized to provide sufficient decoding flexibility to support this view-switching inter action, and hence often multiple frames need to be transmitted just so a single desired frame can be correctly decoded."
"to enable arbitrary view switching but maintain reasonable workload at server, the server transmits two nearest coded views to the client as references for view synthesis of the requested vir tual view at client. we call this system imvs with view synthesis (lmvs-vs); see fig. 3 for an illustration. our goal is to design an efficient pre-encoded frame structure of a multiview video se quence at the server to facilitate arbitrary view switching. a natural approach to enable arbitrary view switching in imvs-vs is to use the same imvs frame structure composed of i-frames, m-frames and cross-time predicted (ct) p-frames, where a p-frame is encoded using a frame in previous time instant as predictor. however, in imvs-vs the server always transmits two frames from two neigh boring views of the same instant, and those two frames typically exhibit high spatial correlation. we can hence achieve better per formance by enabling within-time (wt) prediction also, where one transmitted frame can be predicted using the other transmitted frame as predictor, as done in mvc."
"the increased activity seen in the policy height field representations of user 3 and 5, seemed to have paid off as these bots stood out in combat. observation of the combat strategy of user 3 and 5's bots showed intelligible behaviours, and they would only break out of combat to collect health items. bots from users 1, 2 and 4 had more erratic behaviour with rapid state changes and selection of strange actions during combat. for example, in the replay, user 1 is fighting user 2 and user 1 breaks out of the combat and wanders away even though the enemy is still in sight. they re-engage in combat after a time, but the behaviour looks erratic and is not what would be expected of a commercial fps bot."
"improving health literacy entails more than providing health education to an individual. instead, health literacy considers an individual within a social, environmental and political context and encourages an individual's active participation in their healthcare [cit] . as such, health literacy is not only the responsibility of individuals, but includes all stakeholders within the health system, such as healthcare providers, organisations that provide healthcare services, government bodies, regulators and policy makers [cit] to ensure a consistent message is communicated to end users. in the context of work, health literacy could also include the employer who is obligated to provide safe and healthy work conditions for employees."
"user 3 followed a similar strategy to user 1 of favouring health items over ammo items. they were successful in this goal, and achieved the highest health collecting bot with an average of 23.0 health items per game. user 3 not only trained a very good health item collecting bot, but also a competitive combat bot, proving that the extensive training they did produced a bot that was well rounded in all the game objectives."
"where s m o is the segment which includes the starting virtual view k o /2 in (1). note that transmission schedule g now dictates trans mission in terms of segments instead individual virtual views. cor respondingly, (2) is approximated by"
"over the past decade there has been a dramatic increase in using computer games for artificial intelligence (ai) research. in particular, first person shooter (fps) games are an increasing popular environment in which to test traditional machine learning techniques due to their similarities to robotics and multi-agent systems (mas). for example, fps game agents, termed bots, are able to sense and act in their environment, and have complex, continuous movement spaces. fps bot ai generally consists of hard-coded techniques such as finite state machines, rule-based systems, and behaviour trees [cit] . these techniques are generally associated with drawbacks such as predictable behaviours [cit] and time consuming tuning of parameters [cit] . reinforcement learning (rl) is a class of machine learning algorithms which allow the agent to build a map of behaviours by sensing and receiving rewards from the environment. an extension to the standard rl algorithm is interactive rl, or interactive training, a method that allows human users to interact with the learning algorithm by providing rewards or punishments instead of using a fixed reward function."
"szeto's dual monitor work [cit] augers well for people who concurrently work from multiple digital devices, such as a laptop, tablet and smartphonethe dynamic head motion required to view multiple devices may reduce the risk of musculoskeletal discomfort. similarly, szeto's angled computer display work [cit] has implications for people who hold a digital device (such as a smartphone or tablet) in one hand to the side of the body, requiring a subtle head turn to view the device (see fig. 4 ). this may increase the static muscular load in the neck and shoulders and contribute to physical discomfort, particularly if the device is used for prolonged periods. both these issues require further investigation with people using mobile digital devices."
"one way to mitigate the visual ergonomics health and performance problems associated with mobile work is to design equipment which facilitates comfortable and effective use. for example, set the default font size on digital devices to 3.6 mm because this has been shown to improve productivity, accuracy and ease with performing computer tasks [cit] or include an adaptive feature on digital displays which automatically adjusts the luminance contrast between text and the background to assist reading comfort [cit] . these types of engineering controls would remove or at least minimise the burden placed on workers to adjust their workstation and display for optimal comfort and use."
"it can be seen from (6) that to calculate the lagrangian cost, we have to first calculate frame transmission probability q( fi,j) in (2) using multiple integrals of synthesized view display probability /i (x, bi). to simplify the calculation of /i(x, bi) then q(fi,j), we approxi mate in discrete domain instead. in particular, we divide the interval between two boundary coded views, 0 and k o -1, into n evenly spaced segments. more specifically, segment sm represents vir- as similarly done in sec. 2, we now define segment transition probability adn -m] as the probability that upon watching a view x inside segment sm at instant i, a client requests virtual view y inside segment sn. it is straightforward to derive the expression of"
"the state sensors of the bot were designed to capture local information that the bot can use to sense the environment. the input states for the bot are as follows: the output states were the actions the bot can perform in the world as follows: melee (0), ranged (1), wander (2), health item (3), ammo item (4), dodge (5), and hide (6) . therefore the number of state action pairs in the policy table is 756."
"secondly, the traditional office ergonomics model assumes that a worker has a relatively constant exposure to elements at their workstation. for example, if the worker has a short stature, then their feet may not reach the floor when seated at their workstation. if the adverse element is addressed (in this case, by the provision of a footrest) then the worker's comfort will improve. however, if a worker uses multiple work areas throughout the day, then they may be exposed to a variety of different elements which hinder their performance or comfort. this can pose a challenge for ergonomists and rehabilitation workers who need to isolate the cause of discomfort in an injured worker."
there is evidence that the small size font displayed on handheld devices such as smartphones drives users to hold the device at a close viewing distance [cit] and that this can contribute to visual discomfort [cit] .
"this section has shown that the users were able to use the interactive training tool to train the types of bots they wanted. the policy visualisations showed that there were three distinct types of bots that were trained, where user 1, 2 and 4 had similar trends, and user 3 and 5 had distinctly different trends. the next section will continue investigating the varied nature of the bots from the different users."
"firstly, the traditional office ergonomics model assumes that an individual has their own workstation and chair. with the assistance of an ergonomics advisor, their workstation can be optimised for comfort by making adjustments to the workstation or chair, or by providing ancillary equipment such as document holders and footrests [cit] . however, if a worker does not have a designated workstation, or if their work locations are transitory, then it is not feasible to provide individual ergonomics advice for each of their work locations."
"we can now define the design of redundant frame structure for imvs-vs as an optimization problem: given a fixed number of coded views, how to find a structure t* and associated schedule g*, using a combination ofi-, p-and m-frames, that minimizes the transmission cost c(i) while a storage constraint b is observed:"
"one of the major concerns with the combat system was that the bots did not have the ability to shoot and move to a designated position at the same time. user 4 was not able to train the type of bot that they wanted to due to this restriction and the limited actions that could be performed in combat. a solution to this issue is to add guide actions which are able to add to the combat experience. these actions could include a flee to health item action, an action which allowed the bot to kite by staying in ranged attack distance, and a separate ranged action which moved into melee range."
"the user with the second most active training session was user 5. their feedback quoted them trying to create a bot that was primarily ranged but also used melee attacks, and attempted to collect health and ammo items. the guide action data back up this statement as the ranged attack was focused on with nine successes and 15 failures, while the melee attack was used nine times successfully. health and ammo items were selected 31 and 14 times successfully successively. user 5 tried training the hide action more than the rest of the users with six successful attempts and two failures."
"in fig. 6, we compare the performance of frame structures generated using our proposed algorithm using 1-, dsc and p-frames with wt prediction (ipm w / wt), without wt prediction (ipm w / 0 wt), using 1-and p-frames without wt prediction (ip w/o wt), and using only i frames (i-only). the view switching distance l is set to be 1.5. first, we observe that i -only had a single tradeoff point, because placing i-frames at all switching points results in no flexibility to trade off between storage and transmission rate. sec ond, for the same storage, i pm w / wt offers lower transmission rates than i -onl y by up to 40% for akko&kayo and 30% for ball room, due to using both wt and dsc coded frames. further, we observe that structures using wt prediction can offer a noticeable"
"the contribution of this paper is twofold. investigating how human users interact with learning algorithms is an interesting and novel idea, and may provide insight into the underlying algorithm itself. the results from the paper also benefit the game industry as a new method for designing and training bot ais may be established."
"in this paper, using 1-, p-and m-frames as building blocks, we formulate a lagrangian problem to find optimal frame structures that enable arbitrary view switching in imvs-vs. the crux of the opti mization lies in finding the right mixture of the two prediction types (ct and wt) for p-frames that offers the best storage / streaming rate tradeoff. experimental results show that structures using appro priately added wt-predicted p-frames can lower streaming bitrate of imvs-vs over i-frame-only structure by up to 40%, and over struc ture with m-frames but only ct-predicted p-frames by up to 9% for the same storage constraint. the outline of the paper is as follows. sec. 2 overviews our proposed imvs-vs framework and frame structure. in sec. 3, we formulate the problem of generating the optimal frame structure for imvs-vs. a greedy algorithm is then developed in sec. 4 to gen erate the structure. experiment results and conclusion are given in sec. 5 and sec. 6, respectively."
"creating design standards would require a body of evidence-based literature to support the design parameters. this is within the realm of possibility because there are already publications which report links between visual posture, comfort and productivity (as described in section 2). these design standards could be used as a benchmark for manufacturers when designing new equipment [cit] ."
"this paper has clearly shown that interactive training is a viable option for designing bots in an fps game. all the users, who have experience working on big budget, high quality fps games, felt that the tool had potential to be used during the development of a fps game. the secondary aim of this paper was to show that a diverse set of bot types could be trained by different users. the results showed that the bots were all different from each other, and particularly the bot which was trained with manual rewards only was unique from the others and performed well in the game objectives. a number of improvements have been identified based on the feedback from the users which will make the tool more suitable for commercial fps game needs. a number of the users made points about the inadequacy of the wander behaviour, and commented on how it is not what bots should be doing, rather they should be moving with intention around the level to known item positions and known pathways. to address this issue bot patrol paths will replace the wander behaviour."
"in light of the challenges outlined in this article, we propose that visual ergonomics health literacy is imperative if we are to avoid increased health costs associated with mobile working environments. we propose \"visual ergonomics\" health literacy rather than simply \"ergonomics\" health literacy because many of the awkward postures associated with digital device use are linked to how a person looks at the visual display. we also believe that if a person has an understanding of basic visual ergonomics principles (some of which were outlined in section 2) then they will be able to apply these principles wherever and whenever they use their digital devices."
"the difference between what the bot knows and what the user knows caused some frustration with one of the users. to address this issue, items and enemies that are visible will be marked so that the user can immediately see what the bot can see. also the bot's vision will be modified from a distance based system to a line of sight based system to be closer to what a human player could see. similarly the ranges for the ranged and melee attack behaviours were not obvious. some of the users failed the melee action many times during training and this could be improved by having clear ranges visible on screen. in addition to this visual feedback, the actions that are not available (i.e. would fail due to parameter constraints) will be disabled on the ui. further improvements will also be made to allow the designers to hand initialise the policy before training commences."
"school children can also be classified as mobile workers because they work at multiple locations throughout the day, for example, in the classroom, library, school grounds, in transit and at home. one of the biggest challenges for ensuring that children are comfortable when using computers and other digital devices is providing and selecting appropriate size furniture for them [cit] and, if the furniture is adjustable, then ensuring that it has been correctly adjusted for their size [cit] . this has implications for the location of visual displays relative to the eye height of the child."
"it is estimated that 60% of australian adults have low health literacy, and that this is similar to health literacy estimates in other developed countries [cit] . although we have no specific knowledge about visual ergonomics health literacy within world populations, there is evidence that the general population only has limited knowledge of vision and eye care issues and limited awareness of eye disease [cit] ."
"as an illustration, fig. 1 shows one mvc frame structure pro posed in [i], where i-frames are periodically inserted every h' frames to permit some level of random access. in order to facilitate view switches every h frames, the structure in fig. 1 can be gener ated with h' set to h. however, for a small desired view-switching 978-1-61284-350-6/11/$26.00 ©2011 ieee fig. 1 . example of mvc frame structure, where circles and rectan gles denote 1-and p-frames, respectively. (i, j) denotes a frame at instant i of view j. frames in the shaded region are transmitted to receiver to correctly decode and observe view 1. period h, this leads to high transmission costs due to frequent 1-frame insertion. alternatively, one can first select a compression efficient frame structure with h' » h, and then send to client all the frames required to enable decoding of frames in a single re quested view after a view-switch. for instance, in order to switch from frame (2, 1) of time instant 2 and view 1 to frame (3, 2) of next instant 3 and neighboring view 2, server would send frames (0,2), (2, 2), (3, 2) and (4, 2) to client, but only frame (3, 2) is displayed. besides a large overhead in decoding complexity, this incurs a large transmission cost."
"this paper is organized as follows. section 2 will provide background on fps games and relevant research. an introduction to rl and interactive training will then be given. section 3 will outline the game test bed used for the research, including the interactive training tool interface. section 4 presents the data gathered from game designers' training sessions and results from playing the trained bots against each other. the final section concludes the paper with ideas on future work."
"this paper extends previous work in interactive training [cit] with the aim of further investigating its suitability for commercial first person shooter games. the research is continued through experiments involving commercial computer game designers using the tool to train a bot. this aim will be achieved by examining the results from the training session, to see if they match the intention of the designers. the secondary aim is to prove that diverse types of bots can be created by different users."
"although these data provide a guideline for the frequency with which people should take rest breaks from computer work, data entry tasks are not the only way people use a computer. [cit] where people may use multiple digital devices to access information, interact with social media, send and receive emails, perform creative work (e.g. write, draw), read books, watch videos and play games. further, when taking a rest break from their formal work, people may use other digital devices such as smartphones or tablets to engage in these myriad activities."
"user 2 tried to create an aggressive melee combatant and this can clearly be seen by the number of melee actions that were selected. unfortunately a very high number of these guide actions failed, indicating that either the failure condition for the melee action was unreasonable (i.e. having to kill the opponent), or that the range for using the melee action was not clear to the user."
"discomfort associated with lighting is potentially worse when working in cafes, in vehicles on in transit (e.g. on trains, planes, airport lounges). this is because chairs and tables used by workers might not be optimally placed relative to overhead luminaires, or the ambient light might be unpredictable or quickly change, for example, if travelling in a vehicle which has direct sunlight streaming through the window in some directions of travel. unlike an office environment where these problems may be solved by appropriate location of furniture or by installing window blinds [cit], mobile workers have less control over these aspects of their environment."
"despite this, there remains a risk that users will override default settings on equipment (for example, make the font size smaller) or use devices in nonrecommended ways (for example, view their digital device with a sustained head-turn posture)."
"we now derive a greedy optimization algorithm to generate good frame structures, based on the optimization problem defined in sec. 3.2. more precisely, we iteratively build one \"slice\" of the struc ture at each instant from front to back, i.e., starting with two i-frames synthesizing view k o /2 at instant 0, we construct the local structure ii and corresponding schedule 91 at instant 1, then t2 and 92 at in stant 2 and so forth. at each switching instant i, the key question is: given the scheduled structure 7i-1(gi-1) constructed up to instant i-i, how to optimally construct coded frames of ti and its schedule 9i at instant i to minimize (6) for a given a. to construct locally optimal structure slice ti at instant i, we ini tialize slice ti with one m-frame for each view. this initial slice has no redundant representation (one frame per captured image). thus, it has minimum storage, while large sizes of m-frames will lead to a large transmission cost. next, to methodically reduce transmis sion cost, we can incrementally add the most beneficial redundant p-frames one at a time, resulting in an increase in storage. we ter minate when no more beneficial redundant p-frames can be added to further lower local lagrangian cost."
"thirdly, mobile (or flexible) work environments have shifted the onus onto workers to set up their own work area for comfort. workers need to have an awareness of ambient light, noise, airflow and temperature, particularly when these parameters affect their ability to work comfortably and easily. they also need to have the resources and ability to make modifications to improve their work environment [cit] . although this sounds straightforward, it may be limited by a worker's ability to identify deficient elements in their environment or by their motivation to modify their work environment [cit] . it may also be hampered by the remarkable ability of humans to adapt to and continue to work in deficient work conditions, even if it has poor consequences for their health or comfort, for example, adjusting one's posture to see around a reflection on a computer display [cit] ."
"the other major consequence of aging is our response to light, for example, a need for more light to see detail, colour and contrast, and an increased risk of ocular health disorders such as cataracts which can contribute to increased sensitivity to bright light sources [cit] . there are also age-related physiological and neural changes which slow our dark adaptation as we get older: a study of 94 adults without ocular disease showed that 70 year olds can take up to 10 minutes longer to adapt to dark conditions after being in lit conditions compared to 20 year olds [cit] . prolonged glare recovery after exposure to a bright light can impact the ability to discriminate contrast (i.e. detect a dark object against a light background, as required for reading) [cit] and may be even more prolonged in the presence of ocular diseases such as macular degeneration [cit] . these changing responses to light are potential issues for all workers, but may be more problematic for mobile workers who have less ability to control their working environment."
"user 3 attempted a health collecting ranged/melee combination bot that favoured melee. the figures in table 2 reflect what the user attempted to do. the melee action was focussed on with 15 successes and 13 failures, and the ranged action was selected less frequently with three successes and one failure. the health item action was used as a guide 64 times successfully, and six times unsuccessfully. these figures show that user 3 performed more interactive training than all the other users, which were also seen by the height field representation of the policy in figure 3 as the landscape was more active, compared to the others."
discomfort from lighting can also be an issue for people working from home where there may be a conflict between the task lighting requirements to perform work and the aesthetic requirements for home lighting. a home based worker may be unable to modify the lighting to improve their visual comfort and ability [cit] or for aesthetic reasons they may not wish to change their home lighting.
"teaching people visual ergonomics principles after they commence employment is too late. if infants use devices, then the education should commence at that age. similarly, if school children use digital devices, then visual ergonomics principles should be reinforced during the school years. but to achieve this goal, parents, caregivers and teachers need to have good visual ergonomics health literacy themselves so that they can share accurate information with the children. a similar argument was put forward by legg and jacobs in their article supporting the introduction of ergonomics education modules within teacher training colleges [cit] . this is not to say that it is redundant to reinforce visual ergonomics principles after people commence employment. if people are familiar with the concepts from a young age, then it might gain more traction within workplaces if the visual ergonomics principles are presented in terms of work efficiency or if they are integrated within general company policies, rather than only badged as \"health and safety\". similar sentiments have previously been proposed for general ergonomics [cit] . this approach is also consistent with health literacy principles which advocate a three pronged strategy: embedding health literacy into systems, ensuring effective communication and integrating health literacy into education [cit] ."
"the interactive training algorithm updates when a state change occurs and when the user has selected a guide action or reward. when the user selects a guide action, it immediately overrides the current action. the user chosen action continues until it either succeeds or fails, or the user selects another action."
"there is very little literature reporting the effect of digital device use on the vision and ocular health of children and teenagers [cit], despite reports that children as young as 2 years use digital devices [cit] . associations have been reported between smartphone/computer use and dry eye in children [cit] (see section 2.7 below)."
"speech data were extracted from special broadcasting service australia (sbs) radio podcasts 7 . native transcriptions were acquired for 40-60 minutes of speech in seven different languages (arabic, cantonese, dutch, hungarian, mandarin, swahili, urdu). the cantonese transcriptions were collected at i 2 r, singapore as part of a collaborative research project and transcriptions for the remaining six languages were collected from paid student volunteers at the university of illinois, urbana-champaign who were native speakers of these languages."
"a bigram phone language model for greek is needed to represent prpπq in equation (1) . in order to build this language model, we extracted text documents from wikipedia for greek. this text consisted of 100,000 sentences and 20,000,000 greek word tokens. we used the cmu clmtk toolkit to train a bigram language model in the arpa format 12 . this was further represented as a finite state acceptor using the arpa2fst utility in kaldi 13 . in order to train a phone language model, the greek word sequences were first mapped to phone sequences using the above-described greek pronunciation dictionary, along with applying the greek g2p rules for any out-of-vocabulary words."
"second, a greek pronunciation dictionary was constructed using data available from the translation as a service (taas) project 11, which has about 200,000 greek words with corresponding pronunciations available for some words. these pronunciations use an ascii-based alphabet which we further map to ipa using a deterministic phone mapping. table 2 shows a subset of these phone mappings."
"speech data from podcasts in the test language, and in six other talker languages, were each transcribed by native and non-native transcribers. probabilistic transcriptions were created from mismatched crowdsourcing using the methods 4 . auxiliary information including a word list and a rudimentary g2p were mined from the internet in the target language, and used to post-process the probabilistic transcription"
"evaluation data included native transcriptions for roughly 20 minutes of greek speech. these were obtained by finding greek native speakers on the freelance platform upwork which explicitly allows online workers to list their language skills 8 . the greek data were randomly split into a development set and an evaluation set, of roughly equal size. all the native transcriptions were converted into phonemic sequences using a universal phone set. this universal set was constructed manually starting from ipa symbols appearing in canonical descriptions of all seven languages and merging phones (with the closest phone differing in only a distinctive feature) to ensure that each phone was covered by at least two languages."
"the native transcribers for all eight languages transcribed short 5-second speech clips that were spliced out of sbs radio podcasts to be largely homogenous in the target language. before collecting mismatched transcriptions, these clips were further split into 1-second segments to make the transcription task easier for the mismatched crowd workers. the mismatched transcriptions were obtained from workers on amazon mechanical turk 9, using the methods in the paper 4 . ten distinct crowd workers transcribed each clip; each worker was asked to listen to a speech clip and provide a sequence of nonsense english syllables that is a closest match to what they heard."
we choose greek to be our target language. greek has limited asr support and is one of the many languages that are not well represented on crowdsourcing platforms (as shown in a language demographic study on amazon mechanical turk 6 ).
"current data-driven speech recognition technologies have proven to be very successful for well-resourced languages (such as english and mandarin chinese). however, building these systems involves an expensive and laborious process of transcribing large amounts of speech recordings. this process of acquiring speech transcriptions is the main bottleneck to building automatic speech recognition (asr) technologies for under-resourced languages 1 . crowdsourcing, wherein the task of speech transcription is distributed among a large community of online workers, is a viable option to derive speech transcriptions 2 . this technique, however, requires the crowd workers to be native speakers of the language being recognized. this limits crowdsourcing for speech transcriptions to be applicable only to a small fraction of the worlds languages. mismatched crowdsourcing was proposed to address this limitation 3 ."
"though a proper pronunciation model may be unavailable, it is often possible to estimate the set of all possible grapheme-to-phoneme (g2p) mappings for the target language by downloading and appropriately reformatting the wikipedia page titled xxx alphabet, where xxx is the test language (in this case, greek). the resulting g2p can be constructed to generate, for any given grapheme sequence in the test language, all phone sequences that are attested on wikipedia as possible pronunciations, with equal likelihood for any of the attested pronunciations. an unweighted g2p of this kind is likely to be useless for automatic speech recognition, but may be useful for the purposes described in this paper, as it imposes a loose upper bound on the set of possible phone sequences that might correspond to any given orthographic sequence. such g2ps have been constructed for seventy languages, and are available at here 10 . let us denote the resulting fst mapping graphemes to phonemes as g2p."
"in our experiments, we regard phone sequences directly generated through probabilistic transcriptions (pts) derived from mismatched transcriptions as a baseline, and use a greek g2p and a pronunciation dictionary as constraints onto phone sequences to make results more accurate. from experiments, we can see our methods can significantly improve performance of pts. phone error rates can be reduced up to about 22%."
"suppose that a pronunciation lexicon is unavailable in the target language, but that it is possible to acquire a long list of attested orthographic word-forms in the test language. using this list, it is possible to create an orthographic language model in the target language. if the available text data are sparse, as assumed in this paper, then the language model may even be a 0-gram, in which every attested word form is considered to be equally likely. this language model can also be represented in the form of an fst, which can be denoted with the symbol lm."
"though mismatched crowdsourcing was invented for zero-resource scenarios, it can only currently be evaluated in languages for which a reference evaluation-test transcription exists. experiments in this paper evaluate the probabilistic transcriptions by comparison to an orthographic transcription provided to us by a native speaker of greek. a probabilistic transcription is a distribution over possible phone sequences, therefore it may be evaluated by computing the phone error rate (per) of the 1-best transcription, or of the n-best transcription, or by computing the probability of the native transcription given the probabilistic transcription. previous studies 5 showed that these measures are highly correlated, therefore this paper reports per of the 1-best transcription:"
"for the sake of comparison, we also compute phone error rates using the greek dictionary (described above) in conjunction with the g2p rules (computing the x pt transducer of eq. (3)). greek words appearing in the dictionary are mapped to their corresponding pronunciations and any remaining out-of-vocabulary words are mapped to phones using the greek g2p rules. this constraint is referred to as \"g2p + dict\"."
"first, a simple set of g2p mappings were compiled for greek based on the description of its orthography 10 . these rule sets can be fairly easily generated for a range of different languages 10 . table 1 shows a subset of these mappings for greek. table 1 . subset of g2p mappings for greek 10 ."
