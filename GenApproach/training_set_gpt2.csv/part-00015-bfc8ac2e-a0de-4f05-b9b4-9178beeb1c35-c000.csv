text
"communication under tcp/ip protocol can be regarded as information exchange between ports, the process of information exchange between ports is called the session. in the process of data interaction, session presents a specific interaction mode over time, ideally, this mode strictly abides by figure 1. formation mechanism of pimdl: data flow is shown by arrows. sessions between ports pass through constraints and are transmitted through data packets at the data link layer, the mode of port interaction can be found at the data link layer."
"an etn with 19 points is introduced in this section, where v 1, v 2, and v 3 are the storage centers, v 18 and v 19 are the disaster points, and others are the crossover points. the etn in case study is shown in figure 8 ."
"at present, abnormal traffic data sets have little consideration for long-term interaction information between ports, and cannot fully summarize the temporal characteristics of abnormal interaction. the accuracy of deep learning depends greatly on the size of the training set. if input samples cannot volume 7, 2019 summarize the characteristics of all samples in this category, it will appear obvious over-fitting. in addition, the existing datasets [cit] have a large number of longterm abnormal sessions lasting more than 24 hours, and the features of longer and shorter sessions are different, unified processing is prone to data waste. to solve this problem, based on the separability of the network traffic [cit], a data enhancement method of interaction mode between abnormal traffic ports is proposed. a long session is segmented into several short sessions by time series to obtain the characteristics of different time periods. the method is shown in figure 13 ."
"where, is the length of the k-th section and, is the middle distance from the k-th section to v . according to (5), the probability, of predicted secondary disasters on each edge, can be further conjectured by"
"from figure 9, we can see that the probability becomes different along with the edge gets close to corresponding disaster point. however, the situation that the nearer it is to disaster point, the higher the probability of time delay is not reflected vividly, the reason is that the result takes the length of each edge into consideration additionally. if we eliminate this factor, the probability of unit length on each edge is shown as in figure 10 ."
"( ) crossover point. we appoint a gene (except disaster point), which is contained both in father chromosome and in mother chromosome, as a crossover point. especially, if there is no crossover point, we grant those parent chromosomes are irreproducible. if there are several crossover points in parent chromosomes, we can select one stochastically as a real crossover point."
"as can be seen, with the window sliding, there is no obvious increasing or decreasing trend in the std and mean of sample differences within the group. when the window exceeds 100s, the image fluctuation tends to be similar. let the time series of std and mean be m diff and s diff, respectively, and quantify their fluctuation intensity by their total variation degree. that is, figure 10 shows how df m and df s vary with the value of the window w."
"among the many problems involved in intrusion detection, the anomaly detection method is the most important one, and its key point is to design a feature set that can accurately describe network traffic [cit] . at present, many data sets, such as kdd'99 [cit], nsl-kdd [cit], unsw-nb15 [cit], iscx [cit], which are widely used in intrusion detection systems, have a large capacity and rich characteristics, and the neural network can be used to mine the associate editor coordinating the review of this manuscript and approving it for publication was muhammad imran tariq . the internal rules of these data sets to realize the intrusion detection. there are a lot of achievements in previous studies, while ignoring several problems. firstly, to obtain the previous feature set from the initial traffic, it is necessary to check all the traffic data in the first two seconds and the first 100 connections at the end of the session, however, the intrusion detection system cannot be too complex because of the massive and high-speed traffic characteristics, in practice, according to previous research methods, building feature sets from the real-time generated initial traffic will cause a lot of computational burdens. secondly, previous studies have trained neural networks based on a large number of high-level protocol information (e.g. logon status, flag). when attackers camouflage these attributes, the classification accuracy of neural networks will be greatly affected. thirdly, in reality, the scale of abnormal traffic is usually smaller than that of normal traffic, but the abnormal traffic of data sets used in previous research accounts for a large proportion. for example, in the training set provided by nsl-kdd, the abnormal traffic accounts for 46.52% of the total traffic, but it is almost impossible for the abnormal traffic of this scale to appear in reality (by examining 16.7+ billion visits to 100,000 [cit] attack traffic report said abnormal traffic accounts for 21.8% of total traffic, and in some specific scenarios, such as small local area networks, the potential threat is less, and the proportion of abnormal traffic will be smaller.)."
"(c) an improved genetic algorithm is specially designed to obtain the optimal solution. technically, bidirectional search strategy is designed based on a newfangled path cluster that integrates the specific paths to connecting each storage centers and disaster points."
(b) a discrete robustness optimization model is proposed based on an etn with multistorage center and multidisaster point. the purpose of this model is to find several paths with the better timeliness and better robustness to transport emergency materials.
"in figure 1, the dotted line frame above is the ideal data transmission channel. its essence is the session between ports, which follows the tcp/ip mode. however, since the real data stream is shown by the real line, the session will project an interactive mode represented by the arrival distribution of data packets in the link layer in the dashed frame below the graph. this interactive mode is projected by the high-level session in the data link layer, that is, the model proposed in this paper, pimdl. the process of building pimdl model based on initial traffic is as follows:"
"(3) based on the objective function, the optimal solution with the smallest objective value entails that the path in optimal solution must be the shortest path from v to v . otherwise, it will be replaced by the better one."
"in terms of the optimal path cluster, the convergence procedure based on predicted time, time delay, and nominal time are shown in figures 11 and 12, respectively."
"as shown in figure 1, transportation network planning of emergency materials (tnpes) is a vital step in emergency planning system. the significance of tnpes is to prearrange suitable paths from existing transportation network, and emergency materials will be dispatched immediately to the disaster area through those paths, once unexpected disaster takes place. unquestionably, if the suitable path is not preplanned, emergency materials may be delayed, which may have a bad effect on rescuing and lead to additional loss."
"the horizontal axis in figure 10 shows the size of the window w, and the vertical axis is the corresponding values of df m and df s . it can be seen that the fluctuation intensity shows a slow attenuation state with the increase of the window. when the window exceeds 100s, the fluctuation intensity tends to converge."
"because the ideal state of all samples in the container needs to satisfy the requirements of rich characteristics and small bias at the same time, the window size is selected to be 100s based on the above two experiments. in addition, because the richness and bias of the fixed window have no obvious increasing or decreasing trend, there is no special requirement for selecting the starting position of the window, that is, the optimal range constraint of sample acquisition only considers the size of the window to be 100s."
"aiming at the difference of interaction modes between traffic ports in the complex network environment, a pimdl model is proposed to quantify the mapping of interaction modes between ports at the link layer. on the basis of verificating volume 7, 2019 the feasibility of pimdl, and neural networks construction method based on cnn and lstm are designed to recognize normal and abnormal pimdl, and intrusion detection system is carried out through multi-model evaluation mechanism. compared with the traditional feature sets that previous studies depend on, this paper trains by acquiring the arrival time of traffic data packets, improves the computational efficiency, detection rate in the case of camouflage information, the accuracy of anomaly detection in small samples. however, this method has high spatial complexity because it maintains a time series for each session. next, the mapping model of sessions in different autonomous domains at the data link layer will be established by bifurcation and chaos theory, and the effect of actual distance on pimdl will be explored to classify abnormal traffic more accurately."
"auc (area under curve) is the area under the roc curve, which is between 0 and 1. when a positive sample and a negative sample are randomly selected, the probability that the current classification algorithm ranks the positive sample ahead of the negative sample according to the calculated score value is the auc value. as a numerical value, auc can evaluate the quality of classifier intuitively."
"it is necessary to predict the duration of the session in order to get its complete information. in real traffic generation, the duration of the session cannot be predicted in advance, which makes it possible to use the neural network to classify the traffic only when the session is over, which greatly reduces the real-time of intrusion detection;"
"the running time of the system is 3500s and the detection period is 100s, and the statistics start from 500s. each detection period counts the above three indicators. the results are shown in figure 18, it shows the changes of acc, dr and far with running time, their mean values are 98.90%, 90.13%, and 0.96%, respectively. at the end of the system, the number of normal sessions and abnormal sessions is 16 442 and 405, respectively, and the abnormal traffic accounts for 2.4% of the total traffic. this shows that the improved algorithm can still have higher accuracy and detection rate and lower false alarm rate when the abnormal session size is much smaller than the normal session size."
"from figure 13, we can see the convergence procedures of each path are different, but the trends are strictly consistent with the convergence procedure in figure 11 . however, a peculiar phenomenon is that the predicted transportation time of p 1,19 is finally beyond the required arrival time, which is also mentioned in table 1 . further analysis shows that the reason is that all the paths from v 1 to v 19 are much great and comprehensively influenced; consequently, both the timeliness and robustness have to be yield as much as possible."
"the tcp/ip protocol, but in reality, because the actual data transmission is transmitted through the data link layer in the form of data packets (or as a frame, the term of data packet pays more attention to the specific description of frame data, which includes ip, port, etc.), the interaction mode is affected by high-level protocols, routing topology, uncontrollable network delay, etc., it will show a more flexible rule in the data link layer. from the point of view of the data link layer, the arrival time distribution of data packets represents the port interaction mode to some extent. the definition of pimdl is derived from this, its formation mechanism is shown in figure 1 ."
"according to the comparison in figure 14, the convergence of improved genetic algorithm tends to table when iteration is at 104; however, the table convergence of traditional genetic algorithm is at 184. besides, the results of improved genetic algorithm are much more obvious than traditional genetic algorithm before the iteration at 30, which reflects that the bidirectional search strategy has higher search ability. combined with analysis above, the results demonstrate the improved genetic algorithm is more reasonable and efficient to solve the path optimization problem in the network with multisource and multiterminal."
"traditional genetic algorithm figure 14 : the comparison of convergence procedure based on two genetic algorithms. time delay predicted time 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 time ( tables 4, 5, and 6, respectively."
"generally speaking, research on optimization of etn, especially path planning, belongs to the category of decisionmaking. those literatures give us some new insights to analyze the problem. above all, the introduction of decision preference and scenario planning makes the research closer to the real world."
"when processing samples, the neural network needs to unify the samples into the same shape of tensor for input. therefore, before training the neural network, it is necessary to ensure that all samples have the same shape. ideally, all pimdls are input into a set of neural networks for training, because durations of the sessions are different, when acquiring their pimdl for shorter sessions, 0 is used to fill the missing bits in order to conform to the shape of longer sessions. however, this method will result in a large bias in shorter sessions under unified training. to avoid this bias, the samples are grouped according to the duration of the session, and each group trains different neural networks, as shown in figure 8 ."
"now, we give the max cycle time as the terminal condition of the algorithm. according to the description above, the flow chart of improved genetic algorithm is given as figure 7 . an improved genetic algorithm is designed in this section. the main improvement is that a bidirectional search strategy is proposed based on the etn with multisource and multiterminal. besides, chromosome cluster is also newfangled one. additional illustration is that mutation operator is omitted in that the weakness of falling into local optimum is avoided by introduction of father search and mother search."
"the subsequent structure of the paper is arranged as follows. section 3 is the description of discrete optimization model of etn, and prospect theory is also introduced in this section. section 4 is the algorithm design. a series of rationalities are analyzed based on a case study in section 5. section 6 is some conclusions."
"for each sample group, because the bias is mainly determined by the complement position, the difference value set between the upper limit of container duration and all session durations are counted to indicate the session bias in this sample group;"
"equations (5) and (6) show that, is just an approximate value that reflects the probability of predicted secondary disasters. however, the time delay, which is mainly caused by those predicted secondary disasters, still cannot be predicted precisely by decision-maker because of limited rationality. in fact, in the behavior theory, prospect theory reveals that no matter how precise the probability is, the decision result is finally made based on the subjective reflection of probability, not the probability directly. just as the weight function in prospect theory [cit], it reflects that decision-maker usually overweight the smaller probability and underestimates the higher one. therefore, we can use the weight function to mimic the decision behavior of decision-maker under limited rationality. that is we have the following."
"in figure 12, the horizontal axis represents the size of the window w, and the vertical axis represents the corresponding bias degree. it can be seen that the bias presents a form of an approximate linear function with the increase of the window."
"as can be seen in figure 7, with the increase of τ, there is no ''cut off'' phenomenon in the series autocorrelation function graph, that is, the series does not show good stationarity. the value of autocorrelation function appears periodically outside the confidence interval, which indicates that a longer session sequence can be approximated to an ''autocorrelation series'', that is, the time series has the properties of front-back correlation. similar to the above, the samples under other attributes show same properties."
"among them, tp denotes the number of abnormal sessions correctly identified, tn denotes the number of normal sessions correctly identified, fp denotes the number of normal calls incorrectly identified, and fn denotes the number of abnormal sessions incorrectly identified."
"to avoid the serious losses caused by network attacks, it is important to build an effective intrusion detection model to explore the existing characteristic rules in mass traffic data. as a branch of machine learning, deep learning can recognize the internal law of a certain kind of things to the maximum through training multilayer neural network, so it has a unique advantage to explore the internal law of abnormal attack traffic in massive network traffic data."
"the horizontal axis in figure 3 represents the lag and the vertical axis represents the mutual information between two successive sub-series with corresponding lag values. the results show that when the mutual information reaches the second minimum, the lag of the random sample is concentrated around 1.8s. therefore, according to the above results, the lag selection of phase space reconstruction is 1.8s."
", considering the element as a pixel, the corresponding four attributes are scaled to the parameter range (0, 255) of rgba color format (red, green, blue, opacity) according to the maximum value. to make the image clearer, add 1 at the position of the fourth element t − j, that is, converting the original four elements:"
"in order to test the improvement of the proposed algorithm in computing speed, nsl-kdd-type datasets are attempted to build from initial traffic, using mawi data set (from 00:00 to 02:00 on april 9) as experimental data. as benchmark algorithm, all 41-dimensional data (e.g. first two seconds traffic information, first 100 connection traffic information) are obtained in the process of real traffic generation. when an attribute cannot be acquired due to incomplete information, it is skipped to acquire the attribute. the computational time required to acquire attributes in the process of generating simulated traffic is counted. the experimental results are shown in figure 19 . figure 19 shows the calculation time of the improved algorithm. the horizontal axis represents the flow generation time, and the vertical axis represents the solution time of the algorithm with a time interval of 0.1s before the last one. the average processing time of the benchmark algorithm and the improved algorithm is 1.6276s and 0.8899s per unit under 900s, and the standard deviation is 0.4720 and 0.1595, respectively. the experimental results show that compared with the benchmark algorithm, it needs to maintain traffic information lasting for two seconds, including 100 connections, and frequently performs queries and calculations, which results in a large amount of computational burden. the improved algorithm only obtains identity information and arrival time, so it has less computational burden and volatility. in addition, some attributes are skipped in the experiment, so the benchmark algorithm will face a more serious computational burden in the real situation."
"this paper focuses on the discrete robustness optimization of emergency logistic network based on a situation that the nearer to the disaster area, the higher probability the time delay. thereinto, prospect theory is especially introduced to describe the subjective reflection of decision-maker under limited rationality. in addition, an improved genetic algorithm is also designed based on the optimization model to obtain the optimal path cluster with the better timeliness and robustness from etn with multistorage centers and multidisaster points. according to the analysis of case study, some conclusions can be summarized as follows."
"because these high-level protocols are not used for training, the accuracy of the improved algorithm is not affected by the camouflage protocol, which is stable at 98.90%. because this experiment only needs to prove the problem of accuracy decline, there is no evaluation of dr and fpr indicators."
"in order to reduce losses and improve rescue effectiveness, emergency response and guarantee system is scientifically constructed during the normal condition. roughly, emergency response and guarantee system includes policies and regulations, institutions and organization, emergency prearranged planning, and emergency planning system. obviously, emergency planning system is the precondition to enact emergency prearranged planning, and, also, it is supported by policies and regulations. therefore, emergency planning system plays a fundamental and essential role in emergency response and guarantee system. the main content of emergency planning system is shown in figure 1 ."
"(2) based on the realistic situation that the nearer to the disaster area, the higher the probability of time delay, decision-maker who is under limited rationality, holds the attitude of risk aversion when he faces an edge with lower probability of time delay, and the attitude turns risk seeking when he faces an edge with higher probability."
"aiming at the above problems, a collaboration mechanism between neural networks is designed to evaluate the network traffic synthetically to determine the abnormal traffic. that is to say, session pimdl is graded sequentially by three groups of neural networks (score is between 0 and 1, calculated by sigmoid activation function of the last layer). according to the three groups of model scores of session pimdl, sessions are mapped to the three-dimensional coordinate system, and traffic is classified by pre-trained an svm [cit] model. the specific flow chart is shown in figure 16 (for clearer typesetting, fig 16 is on the next page) ."
"cnn can get features from shorter fragments, and the position of the features in the data fragments is not highly correlated, that is to say, cnn is a very effective prediction method for stationary time series [cit] . lstm is a cyclic neural network, which is more suitable for dealing with and predicting events with relatively long intervals and delays in time series, namely autocorrelation time series [cit] . therefore, in view of the conclusion above: the stability of shorter sessions and the self-correlation of longer sessions, a neural network is constructed based on the cnn layer and the lstm layer, respectively, to classify normal traffic and abnormal traffic."
"for stochastic processes with fixed length in formula (5), when τ increases, the capacity of x s and x e decreases in order to ensure the same and continuous capacity."
"based on the flow chart of the improved intrusion detection algorithm in figure 17, a simulation experiment is designed to verify its detection index. [cit] and ctu-13 were injected into mawi network traffic. in order to keep a gap with training data, the abnormal traffic was measured by 40% of the initial traffic, and the normal traffic was selected by mawi from 00:00 to 02:00 on april 9. according to the algorithm flow, the accuracy (acc), detection rate (dr) and false alarm rate (far) are counted in each detection period. the three definitions are as follows:"
"in addition, genetic algorithm has better advantage in obtaining path cluster from etn with multistorage centers and multidisaster points. the shortage in this paper is that reference point is only just based on the timeliness. in fact, the optimization resulting from multireference point, such as capacity of each edge and path, is more practically significance. besides, group decision can also be used in this paper, by which universality of optimal solution is more general. those will be the next step to further study."
"from the analysis, when the scale of ent is bigger enough, the optimal solution can hardly be obtained by traditional classical algorithm because of the parallelism and complexity."
"dsd especially is all equal to dnd in table 5, but it does not violate the conclusion obtained above. the reason is mainly caused by the weight factor of prospect valve in time delay in equation (19) . further analysis shows that the tolerant time delay in table 4 is so plenty that subjective reflection turns invalid. in fact, it is coincident with decision behavior in the real world."
"(3) the advantage of discrete robustness optimization in this paper is that the uncertainty scale is controllable, by which the uncertainty environment of etn can be vividly mimicked. robustness especially in the model is correspondent to the attitude of risk aversion in prospect theory, and the optimal solution under different scale of uncertainty means the better risk aversion attitude of decision-maker with consideration of timeliness."
"(1) with the required arrival time, prospect theory can well describe the subjective reflection of decision-maker under limited rationality. generally, decision-maker who faces a path with more nominal transportation time and less tolerant delay time holds an attitude of risk seeking, which can be described as a proverb that what is hopeless and bold is often successful. on the contrary, the attitude will turn to risk aversion when decision-maker faces a path with less nominal transportation time and more tolerant delay time, which also can be described as a proverb that caution is the mother of security."
"as there are several paths obtained as the optimal solution to realize the emergency materials transportation from v to v, chromosome cluster can be introduced, and it can be recorded in a two-dimensional matrix. specifically, the column subscript of this matrix has same means of loca in chromosome. the rower of matrix is consistent with the subsequence of obtained chromosome. now, we give an example to illustrate the chromosome cluster as figure 3 shown."
"in addition, if the values of the acf of two stochastic processes are in the confidence interval, it is considered that the two stochastic processes are not correlated under a certain confidence level. the definition of the confidence interval of the autocorrelation function is as follows [cit] in formula (6), acc denotes degree of confidence, l denotes capacity, and erf () denotes error function, which is defined as follows"
"in order to further explore the accuracy improvement effect of the proposed improved algorithm in small sample abnormal traffic classification task, reduce the scale of abnormal traffic and detect its accuracy, the above benchmark algorithm is also used as a reference, and roc curve and auc are used to evaluate its detection accuracy. the roc curve is the representation of tpr (true positive rate) and fpr (false positive rate) under different thresholds. they are valued in the same way as dr and far."
"the research about esc belongs to the field of emergency planning system. those researches provide the foundation of tnpes to further study this problem. beyond doubt, tnpes must obey operation mechanism of esc. the excellent framework, management, and planning especially should be referenced during the research of tnpes."
"the standard deviation and mean value of the difference value set corresponding to each sample group are calculated, drawing the changing trend of them along with window sliding on the coordinate system;"
"segmented data enhancement of abnormal traffic makes the neural network learn a lot of abnormal characteristics of segments, so that three groups of neural networks can fit the characteristics of pimdl duration in (0,100), (100,200), (200,300), respectively. however, the following problems need to be solved when it is used in intrusion detection:"
"obviously, after each father search and mother search, all the storage centers and disaster points are traversed. the proportionality and availability of solution resulting from two searches can also be guaranteed."
"is a quaternion, it can be expressed as a fourdimensional time series s i, which can represent the interaction mode of its high-level ports to a certain extent."
"in the training process of the neural network, the abnormal flow is enhanced by piecewise data. the three groups of neural networks obtained only fit the piecewise characteristics of pimdl, not the overall characteristics. its application needs further treatment."
"according to the duration of network sessions, traffic can be classified into two categories: longer sessions and shorter sessions. because the data characteristics determine the selection of the types of neural networks, the self-correlation of time series is explored for two types of sessions, and then the neural networks are constructed according to their respective attributes."
"to solve the problems above, this paper proposes pimdl, which reconstructs the traffic feature set from the initial traffic to quantify the network traffic. consider the arrival time distribution of data packets as time series, the traditional training method based on high-dimensional traffic feature set (e.g. kdd'99) is improved. the main work is as follows:"
"obviously, figure 10 can well reflect the phenomenon we appointed. especially, for three conditions in figure 10, the value of the edge that is nearest to the disaster point is beyond triple to the farthest one. in addition, both in figure 9 and in figure 10, the synthesis probability is caused by disaster points v 18 and v 19 commonly, and the result is also in accordance with the real word."
"as mentioned in section 4, the main improvement of genetic algorithm is that a bidirectional search strategy is proposed according to the etn with multisource and multiterminal. in some of the other researches, the network is usually transformed as the one with single-source and singleterminal. in order to verify the effectiveness of the improved genetic algorithm in this paper, we transform the etn as the one with single-source and single-terminal and solve it by the traditional genetic algorithm. then, the comparison of convergence procedure based on two genetic algorithms is shown in figure 14 ."
"in the above two experiments, fig. 10 shows that the fluctuation intensity decreases slowly with the increase of the window. when the window is 20s, the characteristic diversity of samples in each time window is relatively narrow, and there is almost no significant difference between samples. when the time window slides, diversity fluctuates present violently because the window is too small to accommodate a large number of characteristics. with the increase of the window, the sample characteristics in the time window are more abundant, and the variation of the degree of diversity is gradually smooth. when the window is larger than 100s, the mean value of the fluctuation degree tends to converge and the variation of diversity degree tends to be the same, which means that the richness of characteristics in the window has reached the ideal value. figure 12 shows that the deviation increases linearly, with the increase of the window, the bias in all samples in the container increases linearly, and there is no obvious inflection point or extreme value. when the window is larger than 100s, the change of bias is approximate when the deviation is less than 100s, that is to say, only considering the bias cannot get a better choice."
"obviously, the problem of tnpes belongs to the category of decision-making under uncertainty and risk. supported by the policies and regulations, the selected path will be specialized for transporting emergency materials [cit] . without the interruption of other traffic during the emergency period, the transportation of emergency materials can be guaranteed in a great degree. however, the interruption caused by other uncertainty factors, like secondary disaster, cannot be predicted and averted. especially, if the unexpected interruption is serious enough, the preplanned path may lose the value of use. therefore, how to select out a suitable path from existing transportation network under complicated and uncertain environment is a difficulty and hot topic."
"the common attitudes held by those literatures are that timeliness is vital to emergency rescue, and stochastic factors caused by unexpected disturbance make the environment of etn more complex and uncertain. the conclusions especially resulting from the researches can improve the tolerance and sensitivity of etn to complex environment. however, the decision behavior under uncertainty and risk is not considered in those researches."
"chaotic time series is a kind of irregular movement in determining the system. pimdl is affected by the high-level network protocol, network topology, and other determinant factors, showing a more complex form of expression. therefore, it can be approximated as a chaotic time series to a certain extent. based on the embedding theorem, a phase space is reconstructed from chaotic time series in the same topological sense as the original dynamic system. the process is as follows:"
"in the experiment, abnormal traffic in the nsl-kdd data set is sampled randomly, and its scale is reduced to 2.40% in the above experiment. the accuracy of the traditional and improved algorithms for small sample abnormal traffic classification is explored. the experiment was conducted 10 times to ensure that random sampling does not introduce bias. the experimental results are shown in figure 21, the auc performance of each numbered experiment is shown in table 7 ."
"the algorithm flow shown in figure 16 is divided into two modules: session scoring module and abnormal detection module. the main task of the session scoring module is to use model_1, model_2, and model_3 for scoring when the session exceeds 100s, 200s and 300s. if the hopping interval of session data packets is too long (e.g. the data packets do not appear in 100-200 s), the lower model (e.g. model_1) should be re-used for further prediction to ensure that all sessions are scored in the order of model_1, model_2, and model_3. when the session exceeds 300 seconds, all arrival time records of the session are cleared to keep the session duration in the (0s, 300s) interval. for long sessions, one of the models may be scored more than once because of repeated emptying of arrival times. the main task of abnormal detection module is to obtain all the sessions with three model scores at the end of the detection period, map three scores (if the session scored more than once, then take its average as the final score) to the three-dimensional coordinate system, and classify them using the pre-trained svm model to achieve the purpose of intrusion detection. pre-trained svm model acquisition method is: the above training data (60% of the initial training traffic is injected into the normal traffic) is trained by improved intrusion detection algorithm, and the mapping of normal and abnormal speech annotated in the three-dimensional coordinate system is obtained periodically. the mapping of normal and abnormal sessions for 10 min, 20 min, 30 min and 40 min in the three-dimensional coordinate system is shown in figure 17 ."
"( ) crossover mechanism. for each chromosome in father cluster, it will be taken out to match each chromosome in mother cluster. once matched successfully, they will take part in the crossover with the mechanism that the crossover point together with the subsequent genes replaced each other both in father chromosome and in mother chromosome. this process can be denoted as ⊗."
"where, is the distance from v to disaster point v . and are the parameters that can be simulated by matching the variance of number of secondary disasters."
"(a) with the consideration of uncertainty of secondary disaster that may lead to the time delay, prospect theory is introduced, based on the required arrival time of emergency materials, to reflect the subjective decision of decision-maker under limited rationality."
"all dna extracts were diluted in sterile molecular biology grade water (5 prime, inc., gaithersburg, md) so that 10 -15 ng of genomic extract dna was added to each qpcr reaction. all qpcrs were performed on the realplex 4s system (eppendorf) in a total volume of 25 μl using the perfecta ® qpcr supermix (quanta bio-sciences, gaithersburg, md) following the previously published thermocycling conditions and final primer/ probe concentrations ( table 1) ."
"the algorithm to compute the set of parameters that escape (α esc ) from a method m is presented in figure 7 . the algorithm takes as input a ssa-based control flow graph of the method and returns a set of indices which refer to the positions of parameters in the method's signature which might have escaped 1 . lines 1-10 populate a worklist with vari- ables that either escaped through a store or through a function call from within m. the algorithm then proceeds through each variable v in the worklist. using the ssa property that each variable has a single reaching definition the algorithm retrieves the unique definition def of v (line 15). if def represents the start node then v is a receiver or a parameter and the appropriate index is added to the mayescape set. for a copy instruction v ← s, s is added to the worklist, since v and s both point to the same escaped object. notice that the order between the instruction that escapes v and the copy from s to v does not matter, since in ssa-form once a variable is defined its value remains unchanged. if variable v is assigned the return value from a function call then all arguments corresponding to the parameters that might be returned are added to the worklist since these might have escaped (lines [cit] . a ssa φ instruction acts as a multi-variable copy statement. figure 8 presents the algorithm to compute the return value summary for a function m. the algorithm maintains a worklist of variables that might be returned. the worklist is seeded with the unique return variable of m. for each variable v in the worklist, depending on its unique definition, the return value summary and the worklist are updated. in lines 12-14, if v is defined at the start node then, since a start node defines the receiver or parameters of method m, the corresponding index of the parameter is stored in params. this represents the situation when the receiver or a parameter to m might be returned."
"a possible tool extension might exploit a new, recently developed promot feature: the process interaction model [cit] (pim) concept that gives a compact specification of a rule-based model and has been applied to the modeling of signaling pathways. this could lead to a fully rule-based, modular design of synthetic signaling networks, from the receptor membrane proteins down to the genes regulated in the nucleus."
"although precise, the alias set analysis in its original form is expensive to compute. using efficient data structures [cit] and algorithms [cit] only improves the efficiency to some extent. in situations where a faster running time is desired we propose the use of method summaries. in this section, we discuss the use of callee summaries that decrease the computation load, without any effect on a client analysis."
"the key insight is that clients of a flow-sensitive whole program analysis often need precise information at a small subset of program points. on the other hand, a flowsensitive program analysis computes precise information at all program points and therefore computes a lot more information than required. computing this unnecessary information is wasteful and should be avoided. we use callee summaries to achieve this."
"to evaluate the effect of using caller summaries on the precision of the analysis (callee summaries have no effect on precision) we executed the client analysis using the orig and ccs abstractions. as per our discussion in section 4, we expected a decrease in precision since caller summaries cause the alias set abstraction to compute fewer aliasing facts. however, the results surprised us; none of the 54 test cases showed any degradation in the client analysis. the ccs abstraction contained sufficient must and must-not aliasing at each shadow of a test case to produce the same transitions on the abstract state machine."
"as stated above, only promoters, bacterial rbss, and eukaryotic coding regions (or, to be more precise, the pools of their corresponding mrna) require a rule-based modeling approach. part structure is specified into an input file. here, the binding site number, the kind of regulatory factors acting on the part, their mutual interactions (i.e. cooperative or not), and their activation or inhibition via chemicals have to be specified. moreover, a set of kinetics parameters is required too. our program converts this information into rules that represent generic descriptions of the reactions that take place in the part. kinetic parameters, molecules, seed species (i.e. the molecules present at the beginning of the computation together with their initial state and concentration), and reaction rules are written to an bngl file (part.bngl). bionetgen then reads the part.bngl file, calculates all the species and reactions involved in the part, and writes them to the part.net file. at this point, our program takes part.net as an input and converts its content into an mdl file (part.mdl)."
"the handling and consumption of poultry or poultry products have been repeatedly associated with the transmission of bacterial pathogens to the human population. incidences of zoonoses originating from poultry products and processing environments have been reported for salmonella spp. [cit], campylobacter jejuni [cit], and listeria monocytogenes [cit] . considering the high environmental, economic, and public health costs of these zoonoses, a comprehensive understanding of the poultry production and processing parameters that allow for the survival/transmission of these bacterial pathogens is essential."
"our software is a set of perl and python scripts. scripts for promoters, bacterial rbss, and eukaryotic coding regions call bionetgen to compute all the part or pool reactions and species. eukaryotic coding region and sirna scripts generate two mdl files: one for the nuclear part and the other for the corresponding mrna/sirna pool in the cytoplasm. once written, part and pool mdl files can be loaded into promot and the cellular nucleus and cytoplasm designed, separately, in a drag-and-drop way. both the nucleus and the cytoplasm have to be saved as modules. afterwards, they are converted into compartments (a different object class in promot) via the python script \"compartment_parser.py\". finally, by running another python script, \"link_compartment.py\", the two compartments are connected to form a cell that, in promot, is an instance of the class sbml-model. all the circuit components (parts, pools, and corresponding reactions) are written in a single mdl file that can be loaded into promot to visualize, modify if necessary, and finally export the system to sbml or matlab format. all the simulations presented in this paper have been performed with copasi [cit] . the software is available on request from the authors."
"to leverage callee summaries in the alias set analysis the transfer functions from figures 3 and 5 are modified. these modifications are presented in figure 9 . we denote the set of methods that contain shadows or transitively call methods containing shadows by m * . the function call 1 o ♯ is modified so that arguments in the caller are mapped to parameters in the callee only for methods in m *, the methods that are still analyzed flow-sensitively. since return instructions are only encountered in methods not using callee summaries no change is required to the return function return"
"rna polymerase binds the dna in the absence of any repressors. if it is recruited by activators, two scenarios are possible: a) if the activators do not bind cooperatively, only one of their operators has to be occupied in order to let rna polymerase bind; b) if the activators bind cooperatively, all their operators must be bound to get transcription started. chemicals can bind and inactivate transcription factors anchored to their operators. depending on the presence or absence of cooperativity, the binding of a co-repressor [cit] to an activator can have a b figure 3 synthetic eukaryotic promoter and mrna. a) in the configuration here shown, a promoter is bound by a repressor r 1 and an activator a 1 . every operator is labeled with the name of the corresponding transcription factor and the position with respect to the tss (the lower the integer, the closer the operator to the tss). a star marks the operators with the highest affinity in case of cooperativity. b) mrna with four riboswitches along the 5'-utr (three of them are tandem ones) and two sirna binding sites on the 3'-utr region. the ribosome binding site is sequestered by the riboswitches nearby when they are in their inactive configuration. a different repercussion on rna polymerase bound to the dna. without cooperativity, all the activator's operators must be free to let polymerase leave the double chain. in contrast, in case of cooperativity it is enough to free the rightmost operator to destabilize the polymerase-dna bond (see additional file 1 for figures illustrating these interactions)."
"the concentration (mg/l) results from the final scalder and chiller tank processing water samples from all time points throughout the study are shown in tables 2 and 3, respectively."
"processing water samples were collected from a commercial broiler processing facility. small (~4.40 pounds) cobb ® broilers were processed at an average line speed of 364 birds per minute for 18 hr each day. for scalder water samples, 3 sterile 1-l plastic nalgene ® bottles (fisher scientific, pittsburgh, pa) were used to collect 3 l of water from ~5 cm below the surface at the turnaround (midpoint) of the final scalder tank of a triple tank counterflow system. for chiller water samples, sterile 1-l tri-pour beakers (fisher sci.) were placed in the basket end of a metal pole to retrieve 4 l of water from the posterior end of the counterflow chiller tank. in total, 3 sterile 1-l plastic nalgene ® bottles and 1 sterile 1-l glass mason jar (for oil and grease content analysis; see below) were used to collect these samples. samples were collected from these two tanks at three times during the processing shift: 1) prior to the first birds entering the cleaned and disinfected tanks (start); 2) after 9 hours (~half of the processing day) of processing (mid); and, 3) after the last birds left the tank and the waters were considered \"dirtiest\" (end). samples were taken from these three time points on three successive days, and were placed on ice for transport back to the laboratory for further sample processing and preparation."
"coding region, sirna and mrna pools, terminators as already mentioned above, eukaryotic cells do not have an rbs. therefore, translation regulation-together with gene expression-concerns the coding region. here, in contrast to our bacterial framework, each coding region has a corresponding mrna pool in the cytoplasm. mrna pools are connected to the ribosome pool and, potentially, to chemical and sirna pools as well. in the nucleus, mrna is transcribed and spliced, and it becomes mature inside the coding region part. free molecules of the spliceosome have their own pool and they interact with the immature mrna by following a michaelis-menten (enzyme-like) scheme. all the other steps of mrna maturation and transport into the cytoplasm are lumped into a single reaction to minimize the model's number of kinetic parameters."
"from tables 2, 3 and 4 we can see that the performance of the system was higher with a higher number of folds for the cross-validation, since a higher number of folds means that the number of training data, compared to the test data, is larger. therefore, in this section we will base our discussion on the accuracy obtained by the system with 10-fold cross-validation. table 2 shows that for input images with a uniform white background, the use of edge detection as part of the pre-processing step lowered the accuracy. for example, the accuracy of pp1 was higher than that of pp6. the difference between these two pre-processing steps is in the use of edge detection. this can be explained as follows. the output of an edge detector contains only the edges of the input image without the areas within the objects in the image. therefore, a slight variation in object's position will greatly influence the result of the classification process. the proposed system uses a nearest neighbor classifier. this type of classifier only performs a substraction operation between the training and testing vectors. since the vectors containing image edges are very sparse, even a slight variation of the object position (and hence, the edge positions) will result in a large difference. an exception is observed, however, for pp2 and pp7 in which the accuracy of pp2 is lower than pp7, although the difference is not significant and can be attributed to the randomization performed in the cross validation process. a similar trend is also observed in tables 3 and 4, i.e. the use of edge detection tends to lower system accuracy except in the case of pp2 and pp7."
"the interprocedural control flow graph is created in the standard way; nodes represent instructions and edges specify predecessor and successor relationships. each procedure begins with a unique start node and ends at a unique exit node. by construction, a call instruction is divided into two nodes; call and return. a call edge connects the call node in the caller with the start node in the callee. a return edge connects the exit node in the callee with the return node in the caller. a callflow edge connects a call node to its return node completely bypassing the callee (figure 2 ). this edge is parameterized with the method it bypasses and the variable the return from the call is assigned to."
section 3 proposed the use of callee summaries for methods not in m * and section 4 proposed the use of caller summaries for all methods thereby requiring flow-sensitive analysis of only methods containing shadows (s). we measured the percentage of reachable methods that are in m * and s and present these in table 4 . the maximum percentage of methods in m * was for the test case jython-fsi where 59.9% of the methods are in m * . notice that the methods containing shadows for jython-fsi are only 0.6% indicating that most methods are in m * since they call methods containing shadows. on average (geometric mean) m * contains 11.9% of the reachable methods implying that callee summaries are used for the remaining 88.1%. when using both callee and caller summaries a mere 0.3% of reachable methods (average of set s) require flow-sensitive analysis.
this paper presented callee and caller summaries as a means to improve the efficiency of an alias set analysis. we described the information required from a callee summary to ensure that their use does not decrease precision at a callsite. algorithms to compute the callee summary and the alias set transfer function leveraging the summaries were also presented. through experimental evidence we showed that a client analysis and alias set precision metrics are unaffected by the use of callee summaries. on average a 27% reduction in the running time to compute the abstraction was witnessed.
"as an example application, we subsequently show how to build within this framework an rnai-based logic evaluator [cit] and a possible counterpart based on transcription http://www.biomedcentral.com/1752-0509/7/42 regulation. the discussion of simulation results is accompanied by considerations about the improvements with respect to prior work and future perspectives."
"to model promoters, operator position is not explicitly taken into account, but activator binding sites are supposed to be placed upstream of the tata box whereas repressors bind the dna between the tata box and the tss (transcription starting site). we take a prokaryotic repression model based on competition between repressors and rna polymerases, where dna-bound repressors prevent rna polymerases from reaching their binding sites and initiating transcription. the use of bacterial transcription factors in eukaryotic cells is a way of combining orthogonal systems that is broadly exploited in synthetic biology [cit] ."
"the authors would like to acknowledge the expert technical assistance of latoya wiggins, nicole bartenfeld, and kathy tate for their assistance in sampling and cul-tural work, as well as john gamble and laura lee rutherford for their assistance in sampling and molecular analyses. these investigations were supported equally by the agricultural research service, usda cris projects \"pathogen reduction and processing parameters in poultry processing systems\" #6612-41420-017-00 and \"molecular approaches for the characterization of foodborne pathogens in poultry\" #6612-32000-059-00."
"the rest of this paper is organized as follows. section 2 discusses the image processing operations we used as pre-processing methods. in section 3, a more detailed discussion of the nearest neighbor classifier is presented. section 4 describes the overall system proposed in this paper. experiment design and results are presented in section 5. an analysis of the results is presented in section 6. finally, our conclusions are given in section 7."
"in our experiments we have used a static analysis that verifies conformance to temporal properties specified using a statemachine-based specification [cit] as a client of the alias set abstraction. in previous work [cit], we presented a two stage approach to verifying such properties. in the first stage an alias set abstraction of objects in the program is computed. the second stage uses this abstraction to compute an abstraction for the state an object, or group of objects, is in. this enables statically verifying whether the state machine might end up in an error state indicating a violation of the property."
"however, rule-based languages cannot be used to generate directly models for single composable parts (and pools) that interact via the exchange of fluxes of molecules such as signal carriers, as in our model."
"inferring properties of pointers created and manipulated by programs has been the subject of intense research [cit] . a large spectrum of pointer analyses, from efficient points-to analyses to highly precise shape analyses, have been developed. a useful tradeoff between the two extremes, and an increasingly used abstraction, is the alias set analysis. this static abstraction represents each runtime object with the set of all local pointers that point to it, and no others. the abstraction is neither a may-point-to nor a must-point-to approximation of runtime objects. instead, each alias set represents exactly those pointers that reference the particular runtime object. as a result, like in a shape abstraction [cit], every alias set (except the empty one) corresponds to at most one concrete object at any given point in time during program execution. this ability to statically pinpoint a runtime object enables strong updates which makes the abstraction suitable for analyses that track individual objects [cit] . we discuss the alias set analysis in more detail in section 2."
the key requirement we put on a callee summary is that it should enable the analysis to bypass flow-sensitively analyzing a method without impacting precision in the caller. table 1 provides a summary of the contents of such a summary. the summary is divided into escape (α esc ) and return value (α ret ) information.
"translation regulation occurs either via riboswitch activation/deactivation or rna interference. as in the promoter case, position along the mrna is not explicitly taken into account. however, riboswitches are normally placed on the 5'-utr (untranslated region) [cit] whereas sirna binding sites lie on the 3'-utr [cit] (see figure 3b )."
"where a stands for not(a), ∧ for and, and ∨ for or. as inputs, we considered four external chemicals that interact with promoters and rbss. however, since our composable parts accommodated at most two binding sites on dna or mrna, we could not predict a circuit design that only employs transcription or translation control, respectively. with the new set of eukaryotic parts and pools, we are now able to reconstruct an rnai-based logic evaluator that is close to the original one, and to design an alternative circuit that performs the same boolean function via transcription regulation alone."
"in contrast to the related work discussed above, we propose a technique to reduce the number of methods that must be analyzed using any of the approaches discussed above (our implementation uses the ifds algorithm [cit] to compute the alias set abstraction). under certain conditions, instead of computing expensive procedure summaries through ifds, our analysis uses cheaper callee summaries without a loss of precision."
"as discussed earlier we chose a static analysis that verifies conformance to temporal properties as a client of the alias set analysis. the analysis represents temporal properties as state machines where operations on object(s) transition the state machine associated with the object(s). to distinguish the objects on which operations are performed, the analysis uses an alias set abstraction. the result of the analysis is a list of shadows that cannot be verified by the analysis; these include actual violations and false positives."
"of the 54 test cases, only 9 showed a degradation in the ma precision metric. the four highest degradations were for luindex-r (75%), luindex-w (73%), lusearch-r (22%) 2 due to space limitations we do not present results for antlr, fop and hsqldb. and lusearch-w (21%). the average (geometric mean) degradation for the 9 test cases was 8%. 31 of the 54 test cases also noted a decrease in the mna metric. the maximum decrease was 17% for bloat-w with an average decrease of 4%."
"in our previous software version [cit], mdl files for parts and pools contained ode systems. here, we adopted a new representation [cit] where odes are substituted by reactions, species (called storage-intras), and adapterfluxes, i.e., entities that handle the exchange of fluxes of molecules with external modules and permit to build the part interface. indeed, adapter-fluxes have two terminals: one is connected to either an internal storage-intra or a reaction, the other to a part terminal. the export function from part.net to the mdl format converts species into storage-intras-if they belong to the part-or into adapterfluxes-if they belong to a pool, such as rna polymerase and ribosomes, or they are fictitious (such as pol cl, rna polymerases in the promoter clearing phase, see [cit] for a detailed explanation). moreover, a library of all the reactions present inside the parts is created such that the part.mdl file can be loaded into promot. finally, links among storage-intras, adapter-fluxes, and reactions are automatically computed: they follow in a straightforward way from the definition of reaction products and educts. figure 2 provides a schematic representation of this computational architecture."
"when the client analysis did not show a loss of precision, we set out to develop a finegrained metric for evaluating precision of alias sets. using the alias set abstraction we compute must and must-not alias pairs for variables live at the shadows of each test case. then we sum the alias pairs for all shadows in a test case to give us two precision metrics: ma the aggregated must-alias pairs and mna the aggregated must-not alias pairs. as expected, the metric values for orig and cs are identical indicating that no precision is lost by using callee summaries. table 6 . alias set abstraction precision in terms of aggregated must aliasing (ma) and must not aliasing (mna) metrics computed at the shadows for each test case."
"rule-based modeling approaches tackle this combinatorial explosion of species and reactions. a biological system is specified by: a) a general description of the species types and their possible states (in our example: a transcription factor that can be bound or unbound and a promoter with six operators that can be either free or bound); b) a list of seed species, i.e., the ones present before any interaction takes place (an unbound transcription factor and a promoter with six free operators); c) a set of rules that describe the interactions between the species (unbound transcription factors bind free operators). according to these input specifications, software such as bionetgen [cit] and the kappa calculus [cit] compute all the species and reactions of the system under consideration (see additional file 1 for a schematic representation). these tools have been proved to be extremely efficient at giving a compact representation of biological systems. for instance, the cbngl (compartmental bionetgen language) has provided a detailed description of the egf signaling cascade in mammalian cells [cit] ."
"the experiment conducted in this study employed cross validation to measure the performance of the system using the various pre-processing variants. we used 10 runs of 2-fold, 5-fold and 10-fold cross validations."
"our choice of client analysis was dictated by two reasons. first, each temporal property specifies its own points of interest; only events that transition the state machine of that property are considered shadows. by choosing different properties we ensure a varying set m *, the set of methods for which callee summaries are used. the properties we experimented with are presented in table 3 . the code fragment from figure 1 uses the fsi property. shadows for fsi are the next operation on an iterator and updates on the collection type e.g. add, clear, remove. a second reason for choosing this client analysis is that it cleanly teases apart the computation of the alias set abstraction and its use in computing the state abstraction. this enables us to measure the precision and efficiency of the alias set abstraction in a real-world scenario. table 3 . temporal properties investigated to obtain a varying set m * ."
"in this section we present caller summaries as a mechanism to speed up the interprocedural context-sensitive alias set analysis. although static analyses that infer properties of pointers can be useful even when the analysis is carried out locally on individual methods, such analyses shine most when computed interprocedurally. the added ability to carry forward computed pointer and aliasing information from a caller into a callee by mapping arguments to parameters can significantly improve precision. however, when efficiency is a bigger concern a natural trade-off is to forego some precision by conservatively assuming initial pointer and aliasing relationships for the parameters of a method. the reason using caller summaries improves efficiency is that it decreases the number of the methods the must be analyzed flow-sensitively. let us revisit the example in figure 1 . using callee summaries enables the alias set analysis to discard flow-sensitive analysis of methods 3-6 since they do not contain any shadows. however, even though methods 1 and 7 do not contain shadows, they are analyzed flow-sensitively to ensure that at a callsite to a method containing a shadow, precise information can be mapped into the callee. in an analysis that uses caller summaries to make conservative assumptions at every method entry, flow-sensitively analyzing such methods is un-needed since the precise information computed at the callsite will never be propagated into the callee."
"many people still find the interaction with machines an uncomfortable experience. using keyboard and mouse are cumbersome ways to interact. efforts should be made to adapt machines to our natural means of communication [cit] . hand gestures, as a part of human body language, are used for many purposes. hand gestures as a communication tool with machines can be used in many fields. examples of such uses are socially assistive robotics, computer interfaces, game technology, assistance of surgeons and also military robotics [cit] ."
"of more detailed models for rnai [cit] might further improve our results. in this paper, however, we want to show that our framework based on composable parts and pools generated via a rule-based modeling approach is applicable to the design and analysis of eukaryotic cells, and a more detailed analysis of the parameter space of such systems is left to a future work."
"mrna half life strongly influences the dynamics of synthetic gene circuits. terminators introduce loop structures at the end of the mrna sequence which may considerably alter the mrna's stability [cit] . therefore, in contrast to bacteria, eukaryotic terminators are characterized by specifying the decay rate of the mrna (or sirna) produced by the transcription unit they belong to."
"our reason for using a naive caller summary was to investigate the maximum precision degradation due to such summaries. whereas the callee summaries presented in the preceding section do not affect precision, caller summaries do. as an example let us look more closely at the example in figure 1 . the method bar receives two list references, a and b. an alias set analysis which does not utilize caller summaries is able to differentiate between the two references. in particular, at the start of method bar the analysis infers that a and b must-not alias (two separate lists were created at lines 1 and 3 and assigned to a and b respectively, and a reference of one is never copied to the other). however, the naive caller summary assumes that a and b could be aliased. hence the precision of the alias set analysis degrades i.e. fewer must-not facts are computed."
"in this study, we have used the 1-nearest neighbor classifier. the 1-nearest neighbor (1-nn) classifier classifies the input vectors based on the class of the closest neighbor in the database. in this study, the distance between vectors is calculated using euclidean distance. if a and b are two vectors of length n, then according to the euclidean distance criterion, the distance between a and b is calculated using eq. (1) as follows."
"in our bacterial part model, we considered only promoters and rbss regulated by no more than two regulatory factors [cit] . this assumption turned out to be sufficient to reproduce in silico most of the synthetic circuits realized in e. coli during the first half of the last decade [cit] . however, in recent years, the emphasis has shifted to synthetic biology applications in more complex organisms and several circuits in eukaryotic cells have been engineered [cit] . therefore, a detailed description of eukaryotic gene parts is necessary to properly model and http://www.biomedcentral.com/1752-0509/7/42 simulate in silico synthetic constructs for yeast and mammalian cells. moreover, cellular compartments of various volumes, such as nucleus and cytoplasm, have to be taken explicitly into account. since they host reactions without counterparts in prokaryotes, new pools have to be introduced."
"the alias set abstraction employs abstract interpretation to summarize all possible runtime environments. the abstraction contains an alias set for every concrete object that could exist at run time at a given program point. the merge operation is a union of the sets of alias sets coming from different control flow paths. a given alias set o ♯ is exactly the set of local variables that point to the corresponding concrete object at run time. individual alias sets do not represent may-or must-points-to approximations of runtime objects, although the abstraction subsumes these relationships. if two pointers must point to the same object at a program point, then all alias sets in the abstraction for that point will either contain both pointers or neither. similarly, if two pointers point to distinct objects at a program point then the abstraction at that point will not contain any alias sets containing both pointers."
"future directions include experimenting with other client analyses of the alias set abstraction, using callee and caller summaries for the standard library, and developing less conservative caller summaries such as those briefly mentioned in section 4."
"in the transcriptional version of this circuit, sirnas are replaced by repressors (see figure 6a-b) . every repressor binds non-cooperatively to two operators. therefore, symmetrically to the original circuit, and 1 is a transcription unit whose promoter (p and1 ) is regulated by three repressors and it contains a total of 6 operators; and 2 's promoter (p and2 ) is controlled by two repressors and hosts 4 operators. these promoter configurations show a high degree of complexity: p and1 hosts 65 species, exchanges 12 fluxes, and contains 834 reactions; p and2 hosts 17 species, exchanges 7 fluxes, and contains 130 reactions. the promoter that leads to the synthesis of the repressor associated with a has a configuration (2 operators) close to the most complex one we could achieve with our old set of bacterial parts: 5 species, 6 fluxes, and only 22 reactions. although p and1 and p and2 have the same number of binding sites as the mrnas for and 1 and and 2 in the rnai-based circuit version, the promoters' species are much more numerous than the corresponding mrnas', because mrna is degraded as soon as one sirna binds (states where more than one sirna is bound are forbidden). overall, the transcription-based version of the circuit is made of 187 species and 1165 reactions. hence, our modular, rule-based modeling approach turns out to be extremely useful in designing systems with complex promoters."
"cherem and rugina [cit] present a flow-insensitive, unification-based context-sensitive analysis to construct method summaries that describe heap effects. the analysis is parameterized for specifying the depth of the heap to analyze (k) and the number of fields to track per object (b). varying the values for k and b results in different method summaries; smaller values produce lightweight summaries whereas larger values result in increased precision. method summaries were shown to significantly improve a client analysis that infers uniqueness of variables i.e. when a variable holds the only reference to an object."
"gene circuits in eukaryotic cells. in the circuit, fluorescence expression is under the control of an activator and an sirna. for the sake of simplicity we do not show all the terminators and all pools; rna polymerase, ribosome, spliceosome, dicer, and risc pools were removed. every full arrow represents a transcription process."
"to measure the effect of summaries on the time required to compute the alias set abstraction we computed the abstractions using the three versions of the transfer functions. in table 5 we show the running time of the original alias set abstraction (orig), the alias set abstraction which uses only callee summaries (cs) and the abstraction using both callee and caller summaries (ccs). the times for cs and ccs include the time for computing the callee summary and ccs also includes the time to compute the caller summary. for all test cases, the time required to compute the abstraction is reduced when callee summaries are used for methods not in m * . the greatest reduction is for pmd-writer which takes 99.6% less time to compute (6670 vs 29 seconds ). the reason for this is quite obvious; for pmd-writer, m * contains only 12 methods out of the 9365 reachable methods. on average the use of callee summaries reduces the time to compute the alias set abstraction by 27%. introducing caller summaries has a more significant impact; an average reduction of 96% is witnessed over the entire test set. table 5 . time taken to compute the alias set abstraction for the original transfer functions (orig), the transfer functions leveraging callee summaries (cs) and the transfer functions employing both callee and caller summaries (ccs)"
"for each respondent and each background condition, the visio hand gestures database contained 48 hand gesture images belonging to 6 classes. each class consisted of 8 images with variations in the angle of the hands. in total, 2880 images were used as the training and testing data set for cross validation. some examples of these images are presented in figure 3 ."
"different transcription factors can bind a promoter, each on n operators, in principle. transcription factors of the same species can bind cooperatively. here, as in our http://www.biomedcentral.com/1752-0509/7/42 previous work [cit], we assume that the affinity between dna and transcription factors varies with the relative position of the operators with respect to the tss. as for repressors, the strongest operator is the one closest to the tss. in contrast, activators bind with higher affinity to the one furthest from the tss and the tata box (see figure 3a) . the binding of a transcription factor to an operator causes a rotation of the dna such that the binding rate constant of the adjacent operator is increased [cit] ."
"prior to analysis, all quantification (cultural and molecular) data was log 10 -transformed to ensure the data was normally distributed. prism 6.0b (graphpad software inc., la jolla, ca) was used to perform all regression analyses, means comparisons (t-tests), and anovas on the microbiological data. for one-way anovas, tukey's post-tests were used to determine significant differences between pair-wise combinations. an alpha level of 0.05 was used to determine significance in all analyses."
"with the joint usage of mdl and bngl, we propose models for eukaryotic parts and pools that arise, when possible, from the corresponding bacterial modules [cit] . we aim at giving part descriptions that are useful for synthetic biology applications and do not pursue an exhaustive representation of all the possible interactions that govern transcription and translation in eukaryotes. moreover, not all the mechanisms behind mrna and protein synthesis are well known, and the values of several kinetics parameters have not been measured yet. therefore, despite the power of a rule-based modeling approach, one has to find a proper trade-off between model granularity and available knowledge in order to obtain a meaningful model that can predict synthetic gene circuit dynamics and performance. specifically, among the set of composable parts and pools, only promoters and coding regions require a rule-based modeling approach because of their potentially complex structure where several binding sites for regulatory factors are present together with either an rna polymerase or ribosome binding site, respectively."
the transcriptional version of the logic evaluator also reproduces the circuit truth table faithfully (see figure 6c ). the final 1 and 0 output levels (concentration of the reporter proteins) are higher than the ones in the rnai-based version of the circuit because more mrna is transcribed and more proteins are expressed when all the regulations occur at the dna level (see additional file 1).
"in figure 4 we provide a graphical representation of a simple gene circuit made of parts and pools in a eukaryotic cell. a more detailed model description is available in additional file 1, including all the circuit reactions and rules in bngl."
"a more exhaustive description of eukaryotic systems might take into account mechanisms that have been neglected here. for instance, cell metabolic reactions can be described by a network of pools that either store free molecules (e.g. kinases and phosphatases) or that represent enzymatic reactions (e.g. phosphorylation and dephosphorylation). furthermore, part models presented below might be enriched by considering also operator positional effects and transcription squelching, for instance. such a precise picture might be useful for the analysis of specific cellular phenomena-and the evaluation of the corresponding kinetic parameter values-on rather simple gene circuits."
"in terms of zoonotic bacterial pathogens, higher gene abundances were consistently detected from the 1 ml raw samples as compared to the 100 ml filtrate samples using both ddpcr and qpcr (table 4), with raw sample values being significantly higher in 33% of the possible scalder water combinations for both ddpcr and qpcr. high levels of organic material in scalder tanks, like those observed during the mid and end sampling times ( table 2), have allowed for the persistence and cross contamination of salmonella and other bacterial pathogens during the scalding process [cit] . the removal of this organic material via filtration, and the bacteria associated with this material, could explain the significantly higher bacterial pathogen detection within the raw scalder samples. in only four possible pathogen:sampling time combinations for the chiller tank water samples were the 100 ml filtrate values higher than the 1 ml raw water samples, but none were significantly higher. considering this demonstrated enhanced detection for both total bacteria and the zoonotic bacterial pathogens, all results discussed below represent analyses of the 1 ml raw water samples."
"the return value summaries of all possible target methods at the callsite are consulted and the fresh, heap and null fields of the summary of m appropriately updated. if any of the return value summaries indicate that a receiver or parameter might be returned the corresponding argument is added to the worklist. copy and p hi instructions add sources of assignments to the worklist. load, n ew and n ull instructions require an update to the corresponding heap, fresh and null fields of the return value summary. two special cases must also be handled; if any possibly returned variable was stored in a field or escaped by a function called by m, escaped is set to true."
"unlike a shape analysis which emphasizes the precise relationships between objects, and is expensive to model, an alias set analysis, like a pointer abstraction, focuses on local pointers to objects. this makes computing the alias set abstraction faster than shape analyses. however, since the analysis is flow-sensitive and inter-procedural it is still considerably slower than most points-to analyses. in this paper we propose two ways to further speed-up the alias set analysis; callee summaries providing effect and return value information and caller summaries that make conservative assumptions at method entry."
"to determine the contents of a callee summary one must understand the effect of a method call on the alias set abstraction. first, the callee might escape the receiver or arguments of the call. this might occur directly, when a callee's parameter is stored in a field, or indirectly, when a parameter is copied to a local reference which is then stored. in escape information (αesc) params set of parameters (including receiver) that may be stored into the heap by m or procedures transitively called by m return value information (αret) params set of parameters (including receiver) that might be returned by m. heap might an object loaded from the heap be returned? fresh might a newly created object be returned? escaped might a newly created object be stored in the heap before being returned? null might a null reference be returned? table 1 . callee summary for a callsite with target method m figure 6 the function foo escapes both its parameters, p directly via a store to field f of class foo and q indirectly by first copying the reference to y and then storing in f oo.f . therefore, a callee summary analysis must track such copies and ultimately provide a list of all parameters that might have escaped. second, the return value from the callee might be assigned to a reference in the caller. to see how this might affect aliasing in the caller consider once again the example in figure 6 . the function foo returns the pointer y which is a copy of q, one of foo's parameters. therefore, the returned reference is the argument which is mapped to q, in this case variable b. at run time, the effect of calling foo is that after the call, a and b must point to the same object. let us examine the effect on the abstraction at the callsite if the interprocedural transfer functions from figure 5 were used. call determines that b and q point to the same location and foo determines that q and y point to the same location. this leads return to infer that since b and y point to the same location and y is assigned to a, b and a must point to the same location after the call; an alias set containing both a and b is created in the caller. in order to forego flow-sensitive analysis of foo in favour of a callee summary, the summary must specify which of the callee's parameters might be returned so that similar updates can be made at the callsite. other possible returned references include references to newly created objects or those loaded from the heap."
"all processing water characteristics significantly increased after the beginning of the processing day for the final scalder ( table 2 ) and chiller ( table 3) tanks. this was expected since a variety of organics/particulates are introduced into these water tanks throughout the processing day from the carcasses. all measured final scalder water samples reached a plateau value by the mid sampling time that did not significantly change by the end sampling ( table 2) . conversely, only 2 of the tested variables in the chiller tank (ts and tkn) reached this plateau, with all other variables significantly increasing throughout the sampling day ( table 3) . these values are consistent with previous scalder and chiller tank assessments from within commercial processing facilities [cit], indicating that this study was run under normal industry conditions."
"before we explain the contents of a callee summary let us see how the alias set analysis can use such summaries. consider a callsite, with a target method m. if an oracle predicts that a client of the alias set analysis never queries any program point within m or any methods transitively called by m, then computing flow-sensitive alias results for all methods in the transitive closure of m is unnecessary. instead a callee summary, which provides information regarding the parameters and return value, could be used. for many client analyses such an oracle exists. in section 5 we discuss one such client analysis that leverages alias sets in proving temporal properties of objects. the points of interest for this analysis i.e. the shadows, are operations that change the state an object is in and are statically known ahead of time. additionally, callee summaries can be used for methods in the standard library; the alias set analysis can be seeded to use callee summaries for all chains of calls into the library. analyses such as those detecting memory leaks and automatically deallocating objects [cit], that already use alias sets, could benefit from such summaries to only analyze application code."
the k-nearest neighbor (k-nn) classifier is a method that classifies objects based on the class of k-closest neighbors [cit] . the input vector for this method is the vector obtained by reshaping the input image into a column vector.
"additional file 1: supplementary material. supplementary material contains the list of parameter values we used for the simulations of the two logic evaluators and the simpler circuit where a reporter protein is regulated by an activator and an sirna. this small circuit is described in details: for each of its parts and pools we give all the reactions, the corresponding bngl rules, and the parameter values that have to be specified as inputs. results from the simulations of both kinds of circuits are reported. moreover, figures that elucidate some interactions (at dna and mrna level) considered in our framework have been inserted."
"unlike callee summaries, caller summaries can affect the precision of the alias set abstraction since important aliasing information available at a particular callsite might not be propagated into the callee and instead some conservative assumption is made. the degree to which the use of caller summaries affects precision is dependent on the choice of caller summary as well as the client analysis."
"the final image processing operation used during pre-processing is desaturation. desaturation is a process that converts an image into a grayscale image by removing the chromatic (color) information. in other words, we only preserve the intensity information of the pixels. we can do this most easily by using the hsi color space, since it can be achieved by simply taking the intensity layer (i). the conversion from rgb color space to hsi color space is performed before the edge detection operation (if applicable). in this study, we did not combine the desaturation operation with low pass filtering, histogram equalization or thresholding."
"modifying the alias set analysis to use caller summaries is straightforward. figure 10 shows those transfer functions which have been modified from their earlier version (fig- fig. 10 . updated transfer functions for the alias set analysis using callee and caller summaries. ure 9). first, the call function is modified. instead of mapping arguments to parameters, the caller summary provides the set of alias sets to seed the callee's analysis. second, callee summaries are used for all methods instead of only those not in m * ."
"the advantage any static analysis derives from interprocedurally analyzing a program is that the analysis need not make conservative worst case assumptions at method entry. this certainly holds true for the alias set analysis. at a callsite, the analysis ensures an appropriate mapping from the caller scope arguments to the callee scope parameters so that alias sets in the callee precisely represent aliasing at the start of the method. however, when efficiency is a bigger concern, we propose the use of caller summaries which are conservative and sound approximations of incoming alias sets. a direct benefit of using such summaries at method entries is that methods that were previously analyzed flowsensitively only to obtain precise entry mappings for methods containing shadows no longer require flow-sensitive analysis. for example, since methods 1 and 7 in figure 1 were analyzed flow-sensitively only because they contain calls to methods 2 and 8, with the added use of caller summaries this is no longer required. only methods 2 and 8 will be analyzed flow-sensitively with caller summaries used to seed their initial alias sets and callee summaries used at all callsites."
"flow sensitive analyses take into account the order of instructions in the program and compute a result for each program point. although typically more precise than those that are insensitive to program flow, flow-sensitive analyses often have longer execution times than their insensitive counterparts. computing such precise information for each program point is often overkill; clients of the analysis need precise results only at specific places. long segments of code might exist where a client neither queries the analysis nor cares about its precision. as an example, consider a static verification tool that determines whether some property of lists and iterators is violated by the code in figure 1 . the verification tool is a client of the alias set analysis as it requires flow-sensitive tracking of individual objects to statically determine runtime objects involved in operations on lists and iterators. notice that precise alias sets are required only when operations of interest occur. for the example, these are the two calls to next at lines 7 and 10 and the call to add at line 11. on the other hand, a typical alias set analysis computes flow-sensitive results for all program points irrespective of the fact that it is likely to be queried only at a few places. in such situations, we propose the use of a selectively flow-sensitive alias set analysis that uses callee method summaries as a cheaper option. only methods that contain a point of interest (which we call shadows), or transitively call methods containing shadows, are analyzed flow-sensitively. for all other methods, callee summaries providing effect information for the parameters of a method invocation and the possible return value are used. if callee summaries were available, only methods 1, 2, 7 and 8 from figure 1 would have to be analyzed flow-sensitively since they contain shadows or call methods containing shadows. for the entire segment of code represented by methods 3-6, flow-sensitive information is not required and callee summaries can be used instead. in particular, while analyzing method 2 the alias set analysis need not propagate the analysis into method 3 at line 8 and instead its callee summary can be used. from the client's perspective this is acceptable since it does not query any program point within methods 3-6. in fact, as long as callee summaries contain sufficient information so that foregoing flow-sensitive analysis of methods without shadows does not affect alias set precision in methods with shadows, the client's precision will be unaffected. details of the construction of callee summaries and their use in the alias set analysis are given in section 3."
"transcription factors and fluorescent proteins are synthesized inside the cytoplasm. the former are imported into the nucleus where they exert their regulatory action on the dna, and the latter flow into a pool placed in the cytoplasm, since they are not normally localized into the nucleus."
"when we compare the results of tables 2 and 3, we observe that for most cases, the system's performance was higher when the hand gesture was captured against an environmental background. in the case of pp1 and pp6, the performance improvement was very small and can be attributed to the randomization done in the cross validation process. however, in other cases, such as pp3, pp4, pp8 and pp9, the performance increase was quite significant. in the case of pp3, pp4, pp8 and pp9, the main contributing factor to this phenomenon is the hue of the background. the environmental background used in these experiments was dominated by a green hue, which is very different from that of the human skin. therefore, the thresholding operation was capable of separating the object (i.e. the hand) from the background. this in turn improved the performance of the classifier. table 4 shows that the performance of the system when the input consisted of images with both uniform and environmental backgrounds falls between the performances of the system as presented in table 2 and 3. this is because the classifier used in this investigation is a nearest neighbor classifier. in this case, the system operates as if we have two parallel classifiers, one for the images with a white background and one for the images with an environmental background. the overall system performance then falls between the performances of these two classifiers. finally, we can also conclude that the best pre-processing variant (in terms of system accuracy) was pp5. this holds true for all types of input image background."
"we have presented a new version of our computational framework based on composable parts and pools for the design of synthetic gene circuits. as a novelty, we combined a rule-based modeling approach-via the bionetgen language-with the modular design of biological systems-through the mdl coding. this method allows for the construction of interconnectable genetic modules with high numbers of species and reactions such as promoters, bacterial rbs, and eukaryotic mrna pools. we provided evidence for the validity of our approach by designing and simulating complex eukaryotic boolean circuits such as the rnai-based logic evaluator [cit] . this is the procedure followed by rinaudo and co-authors (in their case, the lowest 1 concentration is at the entry \"0010\"). they, however, measured fluorescence. alternative configuration based on promoter regulation only. with both circuits we were able to reproduce the truth table faithfully."
"to assess pathogen survival and transmission, accurate, sensitive, and highly specific quantification methods are needed. historically, the quantification of foodborne pathogens in food production systems was based either on cultures or quantitative pcr (qpcr). while these methods have been used successfully, both come with caveats; either being time consuming and too ineffectively selective (culture-based) or dependent upon the proper standards and assay efficiencies (qpcr-based). to circumvent these issues, third generation pcr technology, known as droplet digital pcr (ddpcr) was introduced to provide absolute quantification of target genes and the pathogens to possess those genes [cit] . the advantages of ddpcr over qpcr-based assays are threefold: ddpcr is based on endpoint pcr (efficiency of primer/probe annealing is minimized); ddpcr does not require the use of standards for accurate quantification; and most importantly, ddpcr is a high throughput (15,000 -20,000 pcr reactions per well) assay."
"recent advances in transcription factors engineering [cit] and rna-based synthetic biology [cit] raise a further demand: the model of parts such as promoters and coding regions has to take into account an increasing number of operators and mrna binding sites, respectively (in principle, this requirement holds also for bacterial systems). however, this means an exponential growth in the number of species and reactions for these parts. for example, in our previous model, bacterial promoters had no more than two operators. since each operator can assume two states (free and taken by a transcription factor), only four possible configurations were possible, and for a unique transcription factor, only four binding reactions were present. if, for instance, we increase the number of operators to six, we will have 64 possible configurations and 192 binding reactions. this is what is referred to as the combinatorial explosion problem."
"promoter leakage is proportional to all the configurations where at least one repressor is bound or, in the absence of repressors, where there is no activator whatsoever on the dna (without cooperativity) or all the right-most operators (with cooperativity) are free."
"since the callee summary of a function m depends on summaries of functions called by m, the algorithms presented must be wrapped in an interprocedural fixed-point computation. a worklist keeps track of all functions whose summaries may need to be recomputed. whenever the summary of a function changes, all of its callers are added to the worklist. the computation iterates until the worklist becomes empty."
we call a pair containing a benchmark and temporal property a test case. since not all benchmarks exercise all temporary properties we have chosen to present results only for test cases when a temporal property is applicable for a benchmark e.g. the antlr benchmark never uses a writer and hence the corresponding temporal property is inapplicable.
"pre-processing is a process of preparing data for another procedure. this preprocessing step aims to transform the data into a form that can be more easily and effectively processed [cit] . in this paper, the pre-processing steps are built on the basis of several combinations from the following image processing operations: edge detection, low pass filtering, histogram equalization, binary image processing (i.e. thresholding) in hsi color space, and desaturation. these image-processing operations are discussed in more detail below."
"molecular quantification was applied to raw water taken directly from the initial water samples and from the filtrate recovered from the filtration of the processing water samples. for the raw samples, dna was extracted from two 0.5 ml aliquots using the fastdna spin kit for feces according to manufacturer's specifications (mp bio, solon, oh). for the filtrate samples, sterile pre-moistened (in 1x pbs) cheesecloth was used to initially filter 100 ml of the homogenized processing water samples into a fresh 1-l tri-pour beaker. the cheesecloth was rinsed in 20 ml of 1x pbs and the resultant filtrate was divided into 3 -40 ml subsamples and filtered simultaneously through 3 separate 0.8 μm nalgene filter units (fisher sci.). the three filtrate samples were combined in 1 sterile 250-ml centrifuge bottle (beckman coulter), and the cells were pelleted at 10000 rpm for 20 min at 4˚c. the pellet was re-suspended in 2 ml of 1x pbs and dna was extracted from four 0.5 ml aliquots using the fastdna spin kit for feces (mpbio). for both the raw and filtrate samples, all individual dna extracts were dry-pelleted using a vacufuge tm plus (eppendorf, hauppage ny), and all extracts coming from a single sample were combined in 100 μl sterile molecular grade water. the dna concentration in each sample was determined spectrophotometrically using the take3 ® plate with the synergy h4 multimode plate reader (biotek, winooski, vt)."
"as mentioned in the previous section, the pre-processing methods implemented in this study consist of combinations of two or more of the five image processing operations described above. these combinations will be discussed further in section 4."
"other frameworks for computing procedure summaries have also been proposed. gulwani and tiwari [cit] developed procedure summaries in the form of constraints that must be satisfied for some generic assertion to hold at the end of the procedure. their key insight was to use weakest preconditions of such generic assertions. furthermore, for efficiency they use strengthening and simplification of these preconditions to ensure early termination. the approach has been used to compute two useful abstractions; unary uninterpreted functions and linear arithmetic. recently, [cit] introduced an algorithm which also computes weakest preconditions and relies on simplification for termination. they describe a class of complex abstract domains (including the class of problems solvable using ifds) for which they can generate concise and precise procedure summaries. their approach uses symbolic composition of the transfer functions for the instructions in the program to obtain a compact representation for the possibly infinite calling contexts."
"our conclusion from this experiment is that even though caller summaries cause a theoretical decrease in precision, this does not automatically translate into precision loss for the client analysis. situations exist where the benefits of using caller summaries heavily outweigh the slight chance of losing precision."
"the overall block diagram of our system is presented in figure 3 . the system operates in two phases, namely the training phase and the testing phase. during the training phase, the input images of the system are training images, while during the testing phase the input images of the system are test images. as shown in this figure, the input images are first processed using various combinations of the image-processing operations described in section 2. we have used ten pre-processing combinations in order to compare their performances. these combinations are presented in table 1 . the pre-processing operations were performed sequentially, that is from the leftmost entry of this table to the rightmost entry. the pre-processing output is shown in figure 4 . after pre-processing, the input image was fed into the nearest neighbor classifier. the classifier then classified the input image into one of six classes."
"droplet digital pcr was performed as previously described [cit] using the bio-rad qx100 system (bio-rad, hercules, ca). in short, 1:10 dilutions of the dna extracts were used as templates for general or pathogenspecific pcr assays using primer/probe sets listed in table 1 . taqman-based pcr reaction mixtures (composed of 2x ddpcr mastermix (bio-rad), 900 nm primers, 250 nm probe, 10 -15 ng template dna in a final volume of 20 μl) were mixed with droplet generation oil (bio-rad) and loaded into an 8-channel disposable droplet generator cartridge (bio-rad). the cartridge was placed into the droplet generator (bio-rad) to create the ~20,000 droplets, which were collected from the droplet well of the cartridge and manually transferred to a 96-well pcr plate. the plate, after heat-sealing, was placed on a conventional thermal cycler (s1000; bio-rad) and amplified to end-point (40 cycles for all reactions). upon completion, the 96-well plate was transferred to the droplet reader (bio-rad), and the droplets were automatically scanned from each well at a rate of ~32 wells/hr. analysis of the ddpcr data was performed with the quantasoft analysis software package (bio-rad)."
"at present, part and pool models are based on full massaction kinetics. however, since several parameter values are still not known, we plan to perform a detailed investigation of the parameter space of mechanisms such as rna interference and mrna splicing. moreover, in some cases reactions could be lumped into hill function-based kinetics (already supported both by bionetgen and promot) in order to simplify some interaction schemes and to reduce the number of reactions and unknown parameters in the system."
"for methods not in m *, we define the transfer function callf low for the similarly named edge connecting a call node to a return node in the caller. the callf low function uses two helper functions mustreturn and mightreturn which employ the return value summary α ret to update alias sets by simulating the effect of analyzing the callee. the function mustreturn is true only when the object represented by o ♯ before the call is returned by the callee. therefore the null, fresh and heap flags of the return value summary should be false since a non-null object, that was not allocated in the callee nor loaded from the heap should be returned. additionally, o ♯ must contain the corresponding arguments of all parameters that might be returned by the callee. parameters that might be returned are given by α ret .params and the corresponding arguments are retrieved through the inverse function r, where r is the function mapping arguments to parameters."
"in situations where some loss of precision is acceptable in favour of larger gains in efficiency, we showed how caller summaries that make assumptions about pointer and aliasing relationships at method entry can be employed. in order to gauge the maximum decrease in precision, we chose to use a conservative caller summary which assumes that any two parameters of a method might be aliased. empirical evaluation of the effect of using caller summaries on the precision of the client analysis revealed no decrease in the abilities of the client analysis. for a fine-grained evaluation of precision, two metrics deriving aggregated must and must-not aliasing between variables were calculated. the average decrease was 8% for the must-and 4% for the must-not alias metric. the running time for computing the alias set abstraction decreases by 96% on average if both callee and caller summaries are used."
"as shown in figure 5c, deterministic circuit simulations correctly reproduce the circuit's truth table in terms of high/low reporter outputs, but also with respect to the quantitative outputs, without specific tuning of model parameters. the choice of deterministic simulations is justified by the fact that both logic evaluators exceed 100 proteins in signal separation (see additonal file 1), which we shown to be a condition for large boolean networks to be insensitive to stochastic noise [cit] . in our simulations, we first let the circuit get to the steady state in the absence of chemicals (96 hours). then we fed it with the inputs in order to calculate all 16 entries of the truth table. after 48 hours (i.e, the time considered in the original work), a clear signal separation is already reached. the separation does not improve substantially if we simulate the circuit for 96 hours. discrepancies with the published measurements mainly concern the logical 0 levels. these probably reflect the fact that our circuit is not identical to the reference circuit since we had to choose arbitrarily how to design the sirnas' expression. moreover, our knowledge of rnai kinetic parameter values is still quite limited and adaptations of parameter values or the implementation http://www.biomedcentral.com/1752-0509/7/42"
-we describe callee method summaries for the alias set analysis which provide sufficient information at a method callsite to forego flow-sensitive analysis of the callee without a loss of precision in the caller. we present algorithms to compute such summaries and a transfer function that employs the computed summary. (section 3) -we present the simplest caller summary as a proof of concept to using such summaries to flow-sensitively analyze even fewer methods. a transfer function for the alias set abstraction that uses both callee and caller summaries is also presented. (section 4) -we empirically evaluate the effect of caller summaries on the precision of a realistic client analysis and present precision metrics for the alias set abstraction. the effect on the running time of different incarnations of the alias set analysis is discussed. (section 5)
"the instructions copy pointers between variables, store and load objects to and from fields, assign null to variables, create new objects and call a method m. for method calls, the receiver is specified as the first argument p 0 followed by the arguments p 1 to p k . φ instructions, introduced during ssa conversion, act as copy instructions with a different multi-variable copy for each incoming control flow edge."
"in this paper, we will use hand gesture images that represent different commands to a robot, in particular a military robot. the system identifies six different classes of commands: move forward, turn right, turn left, stop, move backward and a class that consists of gesture variations that do not fall into any of the five previously mentioned classes. these variations of hand gesture images refer to military hand signals [cit] and previous results in the literature [cit] ."
"these findings represent the first report of the use of third generation pcr technology to detect zoonotic bacterial pathogen signatures in environmental samples along the poultry production continuum. while more validation of this ddpcr method needs to be performed on more poultry-related environmental sample types, the results of this study highlight the advantages of ddpcr and the potential for the integration of this highly sensitive and specific method into future poultry food safety research. given the much higher throughput and absolute quantification of ddpcr while producing statistically similar results to qpcr in this study, this third generation technology represents a significant improvement in the molecular detection and quantification of zoonotic pathogens in commercial industry environments. obtaining cultural isolates is still essential within the regulatory framework of food safety research, but ddpcr represents a significant improvement in the ability to determine the presence and possible transmission of pathogen-specific genes within the poultry production environment, especially given the low infectious dose of some of these zoonotic bacterial pathogens."
"rnai interference is a regulation mechanism typical of higher eukaryotes such as mammals, but it has also been engineered into budding yeast [cit] . in our framework, http://www.biomedcentral.com/1752-0509/7 /42 we suppose that a sirna-coding region drives the formation of double-stranded small interfering rnas in the nucleus. they undergo a splicing operation after interacting with the dicer enzyme and are then exported to the cytoplasm as a single strand. as in the mrna case, all the nuclear maturation processes and transport are lumped into a single reaction. free dicer molecules (from a distinct pool) act on double stranded rnas following a michaelis-mentes scheme (analogous to the mrnaspliceosome interaction above). in the cytoplasm, sirna pools are connected both to the mrna and the risc pools. despite its complex structure, risc is here treated as a single molecule that binds an sirna in the sirna pool and brings it to its target mrna. once the sirna is bound to the mrna, the mrna is cleaved and rapidly degraded, and any ribosome along the mrna is released. each sirna can bind to any of n different sites placed on the mrna's 3'-utr (see additional file 1)."
"several other features distinguish eukaryotic from prokaryotic transcription and translation, and corresponding modular mathematical descriptions are missing to date. once transcribed, mrna undergoes splicing and maturation in a eukaryotic cell's nucleus before being transported into the cytoplasm where it is translated. moreover, eukaryotic mrna does not have a unequivocal sequence (such as the shine-dalgarno one in bacteria) recognized by the ribosomes. therefore, the rbs as a part per se is no longer necessary and the binding site for the ribosomes is embedded in the protein coding region. furthermore, the nucleus has a spliceosome pool, and the cytoplasm contains as many mrna pools as there are coding regions in the circuit. while eukaryotic riboswitches/ribozymes-mediated translation regulation does not show any particular difference to bacteria, rna interference (rnai) includes more steps than the sole antisense rna base-pair binding. indeed, sirnas (small interfering rnas) are processed in the nucleus and exported to the cytoplasm where they bind the so called risc (rna induced silencing complex). in this configuration, they bind and cleave their targets on mrna, after which the mrna is degraded rapidly [cit] . we therefore include a pool for the dicer enzyme in the nucleus (see the modeling below for more details) and one for the risc in the cytoplasm."
"in this work, we present an extension of our modeling tool [cit] to build combinatorial composable biological parts for both complex prokaryotic and eukaryotic systems via the joint usage of bngl and mdl. figure 2 shows a schematic of the framework (see also methods). a high-level description of part structures (made of, e.g., binding molecules and sites) is converted into rules that serve as inputs for bionetgen. bionetgen elaborates a list of species and reactions that is parsed into an mdl file containing the proper interface a part needs to be connected to other parts and pools. within promot, parts and pools can be wired into circuits. the circuits can then be exported into formats suitable for simulations such as sbml [cit] and matlab (mathworks, nantucket / ma)."
"escape analysis has been widely studied [cit] and used in a variety of applications ranging from allocating objects on the stack to eliminating unnecessary synchronization in java programs. to determine whether an object can be allocated on the stack and whether it is accessed by a single thread, [cit] compute object escape information using connected graphs. a connected graph summarizes a method and helps identify non-escaping objects in different calling contexts. in their work on inferring aliasing and encapsulation properties for java [cit], ma and foster present a static analysis for demand-driven predicate inference. their analysis computes predicates such as checking for uniqueness of pointers (only reference to an object), parameters that are lent (callee does not change uniqueness) and those that do not escape a callee."
"a recognition system typically consists of pre-processing steps and a classifier. the classifier used in the proposed system is a nearest neighbor classifier. in our previous work [cit], we have presented a hand gesture recognition system using images against a uniform white background. in this paper, we improve the system using images taken against a different background, namely an environmental background, because this type of background is mostly found in military situations. we have also implemented more pre-processing methods compared to our previous work [cit], in which we have only used combinations of histogram equalization and desaturation. the additional pre-processing methods implemented in this paper are: edge detection, low pass filtering and image binary processing in hsi color space."
"the paper is structured as follows: in the next section, we provide a detailed description of our novel models for eukaryotic parts and pools (see methods for details)."
"this modeling framework represents a novelty in the field of computational synthetic biology. several other computational tools for the modular design and analysis of synthetic gene circuits are available [cit] . they present various features such as simulation environments, connection to dna-sequence databases, internal languages (e.g. eugene [cit], antimony [cit] ), and rule-based grammars [cit] for circuit specification and design automation. however, they have been tailored to prokaryotic systems only and none of them implements models for eukaryotic parts."
"bacterial synthetic gene circuits can be designed in an electronic fashion by wiring together standard biological parts and pools of signal carriers [cit] . following the classification given by the mit registry (http://partsregistry. org/), standard biological parts are dna segments such as promoters, ribosome binding sites (rbs), coding regions, small rnas, and terminators (see figure 1 ). each part is characterized by a well-defined function either in transcription or translation. http://www.biomedcentral.com/1752-0509/7/42 ribosome per second) might be adopted to quantify the rbs strength [cit] . in our representation [cit], three more kinds of molecules act as signal carriers because of their role in transcription and translation regulation, namely transcription factors, small rnas, and chemicals. transcription factors are proteins that bind promoters and either prevent (repressors) or enable (activators) rna polymerase binding. small, antisense rnas, on the contrary, regulate translation by binding the mrna and forming or removing hairpin loops that are hurdles to ribosome flux. chemicals carry out a regulatory action both a) in transcription by binding transcription factors and modifying their spatial conformation and, therefore, their activity, and b) in translation by binding and altering mrna secondary structures such as riboswitches and ribozymes [cit] . we associated a pool and a flux to each of the five signal carriers. pools represent the cellular 'storage' for free molecules of signal carriers; they can be seen as bio-batteries, since it is their content that drives circuit activity. furthermore, in a gene circuit design, transcription factor and small rna pools connect transcription units, whereas chemical pools are interfaces between the whole circuit and the extra-cellular environment (see figure 1 )."
"poultry processing water samples were analyzed using the appropriate standard method [cit] for cod (chemical oxygen demand method 5220d), o & g (oil and grease method 5520d), ts (total solids method 2540b), tss (total suspended solids method 2540d), and tkn (total kjeldahl nitrogen method 4500-n org c and 4500-nh 3 c)."
"riboswitches are, essentially, rna hairpin loops that can prevent ribosome binding. in our framework, they assume two different states: active (on) and inactive (off ). only the active state allow ribosome binding to the mrna. riboswitches change their state upon chemical binding to their aptamers. only when all the riboswitches' aptamers are on, ribosomes are allowed to bind the mrna and to start translation. as an improvement of our previous representation [cit], here we explicitly consider single as well as tandem riboswitches with one or two aptamers, respectively. tandem riboswitches can be bound by a unique chemical species or by two different species. since homo-and hetero-cooperativity have been reported in literature [cit], both have been taken into account in our model. in principle, n different riboswitches can be placed along the 5'-utr."
"also related are analyses which traverse the program callgraph (mostly bottom-up but some top-down analyses have also been proposed) and compute a summary function for each procedure [4, 5, 30 ]. this summary function is then used when analyzing the callers."
"transcription and translation efficiency depend on promoter and rbs features, respectively. transcription is modulated by various affinities: between rna polymerases and promoter sequences; between transcription factors and their corresponding dna binding sites (operators); between chemicals and transcription factors. analogously, the accuracy with which both small rnas and chemicals modify the mrna secondary structure influences translation strength. moreover, transcription factors and chemicals can bind dna (the former) and mrna (the latter) cooperatively."
"this decrease in precision can cascade into client analyses. for example suppose a client of the alias set analysis is a verification tool for the property that an iterator's underlying list structure has not been modified when its next method is invoked (executing such code results in a runtime exception). if caller summaries are not used, the analysis infers that the iterator's underlying list i.e the list referenced by a, is never modified since a and b must-not point to the same object and the code only modifies the list referenced by b. hence, the client analysis can prove that line 10 is not a violation of the property. however, when caller summaries are used, the client analysis infers that the list pointed to by reference a might be modified (the caller summary suggests that a and b might be aliased and an element is added at line 11 to the list pointed to by reference b). hence the client analysis loses precision since it can no longer prove that the next operation at line 10 is safe w.r.t. the property being verified. we empirically evaluate the loss in precision of using caller summaries on the alias set analysis and a client analysis in section 5."
"in this study, we have compared ten variants of pre-processing methods applied to a hand gesture recognition system. our experiments showed that the best preprocessing variant was pp5, giving a system accuracy of up to 83.78%. in future work, we will expand our hand gesture database to include images with more varied backgrounds, both in terms of complexity and hue variation."
"while ddpcr represents the newest quantification technology, only relatively pure bacterial or cell culture samples have been analyzed [cit] . to our knowledge, this emerging third generation pcr technology has not been applied to complex environmental samples containing mixed microbial populations, as well as organic/inorganic particulates/contaminants. considering zoonotic foodborne pathogens can be detected in poultry carcasses but many times not within the limited sample volumes from the high capacity processing water tanks, the goal of this study is to determine the utility of ddpcr for the detection of salmonella spp., c. jejuni, and l. monocytogenes from the commercial poultry processing tank waters, and compare these results to common pathogen detection assays (cultural, qpcr). water from the final scalder and chiller tanks in a commercial poultry processing facility was sampled at three time points during the processing day (prior to the introduction of the first carcasses, halfway through the day, and after the last carcasses leave the tank) over three consecutive days. additionally, the effect of water sampling method (raw water versus filtered samples) on-molecular-based detection efficacy was also determined."
we define group convergence (gc) as the event by which the majority of the group chooses the correct solution. a necessary and sufficient condition for gc requires that
"given the current scenario of the web, where users can provide content by producing annotations, comments, *correspondence: rdaddio@icmc.usp.br 1 institute of mathematics and computer science, sao paulo university, sao carlos, sp, brazil full list of author information is available at the end of the article and reviews about any subject, there is a great amount of rich and detailed information available that is created collaboratively by the community. in spite of its unstructured and uncontrolled nature, user-created descriptions can be exploited by information retrieval and recommender system tasks, lessening the need of domain experts to create structured metadata about the items (indexing). moreover, one can always obtain updated descriptions about newly added items, which can vary over time depending on the context they are inserted (e.g., news about determined event are bound to vary very fast, while descriptions about movies and books may have little variation through time) [cit] ."
"the results of our study contribute to the literature on group decision making regarding strategic/combinatorial problems. they relate to the ongoing debate as to when groups are better than individuals, and demonstrate that the nature of the problem to be solved might be as important as the characteristics of the group members attempting to solve it. groups have been shown to outperform individuals in lab settings [cit] and in certain prediction and estimation tasks, typically involving a relatively straightforward quantitative judgment (the \"wisdom of the crowd\" effect [cit] ). in another study of quantitative judgment tasks, it has been shown that the improved performance of groups can sometimes be attributed to learning that occurs in individuals as a result of group discussions [cit] . our results show that large groups might not be preferable when solving more complex problems which are characterized by low demonstrability. on the other hand, many studies have documented that certain dynamics (polarization, free-riding and \"groupthink\") may seriously inhibit the group's overall performance [cit], and that coordination costs further inhibit performance as the team gets larger [cit] ."
"this module is divided into two steps. first, we apply a sentiment analysis algorithm in the item's reviews, obtaining the sentiment for each sentence. the main reason for using a sentence-level sentiment analysis is that most of the features extracted from the reviews are nouns, especially in a movie recommendation domain. nouns have neutral sentiment; hence, we rely on the context and sentiment existing in sentences containing these nouns. in the second step, for each feature, we select all sentences that relate to it and calculate its average sentiment. these steps will be better detailed in the following subsections."
"the values displayed as results are the average values of the iterations. in order to check the significance of the results, we applied the student's t test [cit] ."
"however, dealing with unstructured text raises a set of challenges, especially when considering user-provided reviews [cit] . first, reviews are prone to the occurrence of noise, such as misspelling, false information, and personal opinions that are valid only for the reviewer. secondly, there is a requirement for natural language processing (nlp) tools to analyze, extract, and structure relevant information about a subject from texts. finally, there is a lack of research about how to organize and use additional data provided by users in order to enhance items' representations, and consequently, to improve the accuracy of recommendations."
"this method considers that each document is represented by two vectors located in their respective spaces: technical and privileged. we utilize a consensus-based clustering method that analyzes several clusters produced by different algorithms or the same algorithm with different parameters and combines them into a single clustering model. for our proposal, we use the well-known k-means algorithm [cit], applying it repeatedly with 50 different parameter settings (i.e., varying k from 1 to 50), into both documents representations."
"as observed in the results, in general, the machine learning approaches provided better results than their heuristic counterpart. this can be better observed in relation to the aspect approaches, where the results are statistically different in every metric and database used in this study. in relation to the term approach, which provided the best results, although the classification approach was not statistically superior to the heuristic approach for the ml-100k database, the experiment carried out in the hetrec ml database indicates that the usage of the tlate can produce more accurate representations with a larger set of reviews."
"firstly, the reviews go through a pre-processing step (detailed in the \"text pre-processing\" section) in order to reduce noise and provide a structured version of them. next, the feature extraction module obtains relevant characteristics about the domain of the items. this module is the main challenge addressed in this work, and four different techniques were tested, being two of them based on terms and the other two based on aspects. also, for each granularity (terms and aspects), we explore one technique based on heuristics and one based on machine learning and verify which ones provide the best results. this module will be detailed in the \"feature extraction\" section. in the item's representation creation module, the features extracted previously are represented as the positions of each item's vector, and their scores are computed as the overall sentiment toward them obtained in the item's reviews. this step, which is detailed in the \"sentiment analysis and item representations construction\" section, is performed for each item present in the database, producing a set of items' representations. these representations feed a neighborhood-based collaborative filtering algorithm, which is detailed in the \"recommendation\" section."
"we have enhanced both databases by collecting structured metadata such as genres, actors, and directors from the imdb web site. we constructed binary items' representations for each of these metadata, where 1 means that the item has the metadatum while 0 means it does not, and used them as baseline representations, applying them to calculate the item similarity by the same means that our approaches are used. in our evaluation, we compare the descriptive power of our representations against those binary, baseline representations. table 1 shows the total and average number of features for each structured metadatum considered."
"at the end of the feature extraction module, the resulting set is used by the items' representations generation module. in this module, the sentiment value for each item's feature is computed. thus, an item is represented by the average sentiment of many users' reviews toward each of its characteristics."
"in this paper, we compared four different techniques of text feature extraction for item representation construction and analyzed the impact they produce in a neighborhood-based recommendation algorithm. the results showed that the techniques based on terms provide better results, since they produce a larger set of features, hence detailing better the items. another point worth noticing is that, for both characteristic granularities, the techniques based on machine learning provided better results. among all techniques addressed, the technique based on transductive learning provided the best results, being statistically superior in a larger database."
"in contrast, increasing group size for psc problems may be detrimental to group performance, because non-solvers might not recognize the correct solution. that is, for at least some psc type problems, we expect the following to hold:"
"to further analyze the differences among our approaches, table 7 presents the top 10 most frequent features extracted with each technique in both datasets used in our experiments. as explained before, some techniques use the lemmatized version of words, while others use the stemmed version. we maintain this format here."
"as future work, we plan to analyze the proposed set of feature extraction techniques with other recommender algorithms, including those specific for the item recommendation task and optimization of personalized rankings, such as the bpr (bayesian personalized ranking) [cit], climf (collaborative less-is-more filtering) [cit], and tfmap (tensor factorization for map maximization) [cit] . another possibility of extension is to apply the items' representations in different attribute aware recommendation algorithms or to apply the system into different data domains. finally, we plan to build user profile vectors based on these techniques, by using their own reviews or inferring their sentiment toward the features by analyzing the vectors of the items he/she evaluated as interesting. we then plan to apply these users' vectors alongside the items' representations in the recommendation process."
"a body of social science research has shown the effect of the demonstrability of a problem on a group's ability to collectively solve intellective problems [cit] . a problem is considered to be of high demonstrability if group members who failed to solve the problem are still likely to recognize correct solutions proposed by others. according to the \"truth-wins\" process [cit], when solving problems of high demonstrability, groups are likely to converge to a correct solution as long as there is at least one group member who is able to solve the problem. in contrast, for problems of low demonstrability, members who were not able to correctly solve the problem may not be able to recognize solutions proposed by those who did; thus, the majority a1111111111 a1111111111 a1111111111 a1111111111 a1111111111 of the group might not converge to a correct solution, and the \"truth-wins\" process does not apply [cit] ."
"our proposal focuses on the development of methods that produce items' representations based on users' reviews for recommender systems. for that, we propose an architecture that ranges from text pre-processing to generating recommendations, making use of feature extraction techniques (applied in two granularities: terms and aspects), coupled with sentiment analysis techniques for assigning polarities to the various themes portrayed by users in their reviews. thus, the items' representations aim to describe items by their characteristics and the collective appreciation of users toward them. finally, the representations feed a hybrid recommender system that combines a content-based approach with an item-based neighborhood model to produce suggestions to the user."
"we explored this technique in a previous experiment [cit] . the results showed that the produced aspect set was very small, which affected the capability of the recommender to distinguish the items and hence to locate appropriate neighbors to produce adequate suggestions."
"recent works focus on extending the traditional recommendation paradigms by using this user-provided unstructured information [cit] . this is a great source of information, since it is able to describe the item and provide feedback about the opinion of the user regarding the item and its features. indeed, it is common for users to analyze reviews from other users to decide whether or not to consume a certain product. when reading the reviews, a user can verify whether the item meets his/her expectations in certain aspects, analyzing if the majority of the reviewers consider or not positive the features that he/she thinks interesting."
"the filtering step aims to remove words that have less chance to be terms. in a previous work, we tested two different filters [cit] : (1) filter_df, which removes the words that occur only in one document in the database and (2) filter_df_n that also deletes those words that are not nouns. in our experiments, we found out that the filter_df_n performed better since it provides a significantly smaller set of candidate words to be classified by the transductive learning step, and hence the overall outcome contains a smaller but more descriptive terms set."
"all experiments were carried out in a 10-fold crossvalidation setting. the rating database was randomly divided into 10-folds, where each fold contained at least one interaction of each user. this way, we guarantee that the system can generate recommendations for every user in the database, and that every user can be inserted at the same time in the test set. we define as training set 9 out of the 10-folds and use the remaining as test. we iterate 10 times this procedure, varying the fold used on the test set. since the reviews used are collected from outer sources, they can be viewed as additional content and thus the representations are not divided into 10-folds."
"in this section, we present the results related to the item recommendation scenario. table 5 exhibits the baseline results for both databases, while table 6 presents the results for the proposed approaches. figure 6 presents graphics with the results of prec@10 and map of the baselines and the proposed approaches for the ml-100k database, and fig. 7 shows the comparison in the hetrec ml database."
"as it can be observed, term-based approaches produced better results in both databases and recommendation scenarios. this implies that a greater number of features tend to describe better the items than a significantly smaller set, considering the recommendation algorithm used in this work. while the term-based approaches contained thousands of features, aspect-based approaches contained on average a few hundred due to their nature of grouping topic-related words."
"next, we generalize this result by characterizing the phase transition between two types of problems: problems for which increasing group size monotonically improves performance, and problems for which performance peaks at a finite optimal group size and decays thereafter."
"in order to find similar items, a similarity measure is employed between the previously described items' representations. this similarity can be based on several correlation or distance metrics, and through experimentation, we found out that the pearson correlation coefficient, p ij, performed better in most of the settings. in this metric, a value corresponding to 1 means total correlation, in which the vectors coincide and are in the same direction, whereas a value of −1 corresponds to the case where the vectors coincide but are in opposite directions. the pearson correlation is defined as follows:"
"as it can be seen, both top 10 lists generated from the term extraction techniques are almost equivalent, with differences on the order or with few words. in fact, the first three words are the same in each technique, and they are indeed relevant: the first two regard the opinion of users toward the item as a whole, while the third regards the time, or duration, of a movie. this is also reflected in the heuristic aspect technique, where both words \"film\" and \"movie\" are part of the \"cinematography\" aspect, which is the most frequent in both datasets. another difference worth noting is that classification terms removed words such as \"way\" and \"thing, \" which are very common nouns but can be considered of little impact on describing movies. lastly, as it can be seen, hierarchy aspects produces not semantically table 5 the average values of prec@10 and map for the structured metadata baseline applied in both databases regarding computation time and resources, we argue that the main difference between the approaches lie in the item similarity calculation, since the size of the representations is significantly different. having the item similarity calculated, which can be done offline periodically, the system will perform within the same time."
"in our work, these routines are supported by the wellknown stanford corenlp 1 [cit], a natural language processing toolkit that contains several nlp routines. the corenlp also supports a sentence-level sentiment analysis algorithm [cit], which we also apply during this preprocessing step. more details about this algorithm can be found in the \"sentiment analysis algorithm\" section. finally, the stemming was performed through the traditional porter algorithm [cit] ."
"in this work, we compare the effect that these two representations causes to a recommendation algorithm that uses information about items. therefore, we apply two term extraction and two aspects extraction techniques in the set of reviews, and the result of each is evaluated on the recommender system. our evaluation also presents a discussion about which technique performed better in the particular domain of movies recommendation. in the following sections, we introduce each of these techniques."
"in the final step of the item's representation creation, the system analyzes the feature set and checks whether they are terms or aspects. if they are terms, the system verifies whether they are represented as a stem or lemma and converts them into a set of words that the stems/lemmas comprehend. then, for each item, the system looks up in the xml files containing its pre-processed reviews which sentences contain these words. if the feature set are aspects, the system finds the set of terms that the aspect represents, and then applies the same stem/lemma recognition for each term, finally converting them into words and checking the sentences which contain them."
"we performed our experiments on two databases related to movies, generated from the movielens web site 3 and enhanced with information contained in the internet movie database (imdb) web site 4 ."
"in order to do that, we developed a system's architecture with concise and specific modules. as depicted in fig. 1, the main goal of the system is to produce a set of items' representations that will feed, alongside the ratings provided by the users, the recommendation algorithm."
"after having obtained the sentences related to each feature, the next step is the sentiment attribution to them. for each feature of each item, it is calculated as the average sentiment of the related sentences. thus, the final value represents the collective level of appreciation or depreciation of a certain attribute of an item. as mentioned before, the sentiment values range between 1 and 5, being 1 equals to \"very negative\" while 5 means \"very positive. \" a zero value indicates that an item simply does not have that feature."
"hypothesis 2: increasing the group size for npc problems will improve group performance (i.e., facilitate convergence to the correct solution) because non-solvers will be able to recognize a correct solution when it is presented to them."
"regarding the study's limitations, we have based our model and empirical studies on noninteracting (nominal) groups, which is distinct from situations in which groups solve problems together [9, [cit] . studying nominal groups enabled us to focus on the inherent characteristics of the problem and their effects on group performance, eliminating factors related to group dynamics. we note that such groups are becoming prevalent in online communities such as crowdsourcing platforms and citizen science [cit] ."
"we also collected up to 10 reviews per item for the ml100k database, resulting in a total of 15,863 documents and up to 100 reviews per item for the hetrec database, resulting in a total of 656,031 documents. the reviews were selected as the top-n from imdb, ordered by their helpfulness. unfortunately, not every movie had the maximum number of reviews, in fact, there were some movies that did not have reviews at all. the reason we selected only 10 reviews for the ml-100k database is that this database was used in our preliminary experiments; hence, we needed a smaller set of reviews to guarantee speed in these experiments. with the preliminary experiments done, we were able to test the best configurations in a larger setting, which demands more time."
"the lihc makes use of technical and privileged information to perform the document grouping task. the technical information used is a traditional bag-of-words representation, containing the frequency of the terms present in the document. privileged information in text processing domain consists of information besides traditional term frequency (tf) or term frequency-inverse document frequency (tf-idf) [cit] . in this work, we use the part-of-speech tag of words as privileged information, since they represent a linguistic information about the terms located in the documents."
"in the term-based approaches, considering the rating prediction scenario, it can be seen that as the number of neighbors increases, the accuracy of the predictions decreases. one possible reason for this effect is that as the items are described in greater detail, there is a possibility that as soon as one increases the number of neighbors, unrelated items will be included in the prediction calculation. the opposite effect, however, occurs in the aspectbased approaches. it can be observed that if the number is increased to a certain number of neighbors, the prediction accuracy improves. it is speculated that this is due to the fact that the representations contain a very small number of features, making it difficult to tell them apart, and hence, it needs more neighbors to calculate the rating prediction."
"to evaluate our proposal, we selected the most promising configuration of each of the aforementioned feature extraction techniques and compare them with each other as well as with some traditional structured metadata representations."
"for example, in the reviews about the movies that a user u rated positively it was reported that they have strong science fiction elements (positive sentiment), but have weak or absent (negative or neutral/absent sentiment) romance. another user v has evaluated positively movies that have direction and photography praised in their reviews and evaluated negatively movies that contain positive critics about their suspense. thus, the system will most likely suggest movies that relates to the preferences of those users: for user u, it will suggest science fiction movies with no aspects of romance, while for user v the suggestions will have positive direction and photography, and negative or absent suspense."
"as an alternative for the vector construction, the system can also produce a binary sentiment vector. in a previous work, we defined the best strategy to perform the binarization of the sentiment vectors [cit], which we describe as follows: each feature is divided into two portions, corresponding to its positive and negative parts. features that have positive sentiment are attributed value 1 to its positive portion, while those with negative sentiment have their negative portion defined as 1. we used a threshold α to adjust the relevance of the intensity of the sentiment in the binarization process, and through experimentation, it was determined that tighter intervals produced the best results. with the aspect-based approach described in the \"extracting aspects through heuristics\" section, the recommender performed better with the binary representations. we argue that since the binary scoring duplicates the number of features, the aspect-based approach has bigger vectors to describe the items, thus producing better results."
"in this section, we present a comparative study about the applicability of the four aforementioned feature extraction techniques in a recommender system. for each technique, we selected the best configuration and compared them with each other and to structured metadata. we named the four techniques as:"
"finally, we also argue that our approach can help in minimizing problems regarding new items. in the traditional item k-nn, item representations were constructed with the ratings the item has received, thus hurting the prediction of newly added items. our approach overcomes this drawback by constructing representations that rely on reviews external to the database. in our experiments, we used reviews from imdb, but reviews from several other websites can be extracted, thus minimizing the chances that an item would remain without any description."
"second, in our study each participant was presented with a single wrong solution. in general, there could be many wrong solutions, and our formula p(rw) can easily be extended by adding similar elements for each of these wrong options. having multiple wrong solutions will actually make group convergence more difficult for psc problem types, because solvers can be led astray by more options. in this respect, the likelihood of convergence for psc problems shown in fig 4, when there is just one wrong solution, represents an upper bound."
"recommender systems emerged to deal with the information overload problem by producing personalized content suggestions to their users. these systems can be traditionally divided into two main strategies: content-based and collaborative filtering [cit] . in the content-based approach [cit], users' profiles are matched with items' representations using a similarity measure. in collaborative filtering [cit], there are two main fields addressed: (i) neighborhood models (also called memory-based models), which find and match ratings from clusters of similar users or items to predict unknown ratings and (ii) latent factor (or model-based) models, which have comprised an alternative path to transform both items and users into the same latent factor space, allowing them to be directly comparable. beyond these two strategies, there is an effort to combine them into a third hybrid approach, where the flaws of each other are compensated by their strengths [cit] ."
"to summarize, our results show that the benefit from increasing the group size (p(ecs) increases with n) can be offset by the fact that its members may not recognize correct solutions (p(vc) is low). one possibility for mitigating the detrimental effect of increasing the group size, due to the inability of group members to identify correct solutions, is to separate the group that generates solutions from a group of experts that choose the best solutions. this design choice is exhibited in an open innovation platform that uses a group of experts to choose the winning ideas posed by the crowd [cit] ."
"as the world becomes more connected, groups are increasingly able to solve problems collaboratively by utilizing participants of diverse backgrounds and expertise [cit] . this study improves our understanding of the mechanisms that underlie group problem-solving processes, and can inform the design of systems for helping groups make good decisions collectively. tables. table a, solvers' acceptance and rejection of solutions for tsp-e, geo-e, geo-h and tsp-h. table b, average verification time and standard deviation (in parentheses) in seconds for each problem instance. table c, number of subjects from ben-gurion and p(s) measures (no interaction groups). table d"
"as previously stated, the proposed system uses users' reviews to produce representations about the items, containing their most relevant characteristics and the overall sentiment toward them. thus, the preference of a user is built based on an average of opinions."
"the second aspect extraction technique was proposed in order to eliminate the human intervention required in the previous technique. we use the lihc (lupi-based incremental hierarchical clustering) [cit] for the automatic generation of a hierarchical clustering of texts, and through this, produce a topic hierarchy. this topic hierarchy is then processed so that the most representative topics are used as aspects of the items."
"where o u is the predicted items set that the user u evaluated,r ui is the predicted rating, and r ui is the real rating. item recommendation evaluates the capacity of the system to generate personalized rankings of suggestions. we evaluated this scenario by selecting the top 100 items with the highest predicted ratings for each user and applied the precision at n (prec@n) and the mean average precision (map) measures. the prec@n measures how many relevant items are returned in relation to a small n sample of the total ranking:"
"the clusters produced are combined using a coassociation matrix m. thus, for each type of information, we generate its respective co-association matrix: m and m * . these matrices, in turn, are combined into a final co-association matrix m c ."
"rating prediction evaluates how much the ratings predicted by the system deviate from real ratings assigned by users. for that, we used the root mean square error (rmse) metric, which is defined as:"
"using numerical simulations for different parameter values of p(s) and p(vc j ns), we found two different phases of group behavior, a region where performance improves monotonically with group size (fig 4(a), blue region) and a region where increasing group size decreases performance after a peak at a finite group size (fig 4(a), orange region) . fig 4(b) and fig 4(c) show the group performance for example problem instances that lie in the different regions."
"we used a nominal group setting in which participants first solved a problem on their own and were then presented with solutions proposed by other group members. we say that a participant is a solver (s) of a given problem if the participant was able to solve the problem in a predesignated amount of time (and conversely for a non-solver (ns)). a participant has recognized a given solution to the problem if the participant was able to accept the solution if it is correct (ac), or reject the solution if it is wrong (rw)."
"where μ is the global average rating and b i and b u are the item's and user's deviations from the average. to estimate b u and b i, one can solve a least squares problem. we adopted a simple approach which will iterate a number of times the following equations:"
"to isolate the effect of the computational complexity of the problem itself on group performance from other aspects that have been shown to affect group performance (e.g., social dynamics in the group), we study nominal (non-interacting) groups. this enables us to understand the computational limitations each individual carries in group interactions. we show through empirical studies and analytical derivations that group performance-and in particular, the effect of group size on performance-depends on both solution and verification complexity. notably, we show that for problems of particularly low demonstrability, increasing group size can be detrimental to group performance."
"in this section, we present the results obtained in the rating prediction scenario. table 3 shows the results for the structured metadata baseline for both databases, while table 4 presents the results related to our proposed approaches. figure 4 presents a graphic comparing the baselines and the approaches for the ml-100k database, and fig. 5 shows the comparison in the hetrec ml database."
a common approach of collaborative filtering algorithms is to adjust the data for accounting item and user bias. these effects are mainly tendencies of users to rate items in different manners (higher or lower ratings) or items that tend to be rated differently than the others. we encapsulate these effects within the baseline estimates. a baseline estimate for an unknown ratingr ui is denoted by:
"therefore, we select from the set of candidate words those that are more common among the item reviews, assuming that these may be, in fact, features. since an item has n reviews, instead of using the document frequency (df) [cit], we decided to use a similar metric called item frequency (if) [cit] . considering f as the candidate words set and i the items set, the item frequency if f of a candidate word f is given by"
"our work complements these previous studies which describe deleterious effects of small group dynamics. our results underline the important role of problem complexity in group processes, even before considering the different personal dynamics such a group may display. we show that in addition to potential detrimental effects of social dynamics on group performance, there are also detrimental effects stemming from the difficulty individuals have in assessing the correctness of proposed solutions."
"first, we select only words with the noun pos tag as candidate terms. one of the problems with part-of-speech taggers is that unknown words tend to be classified as nouns. this problem is aggravated when using texts produced by users, due to misspellings, internet slangs and abbreviations."
"in the recommendation module, the items' representations are finally analyzed alongside the ratings provided by the users. we use a neighborhood-based recommendation algorithm because of its simplicity, efficiency, stability, as well as the easiness for extending the traditional model into using items' feature vectors [cit] . we opted to use an item k-nn collaborative filtering algorithm that takes into account the bias of users and items [cit], and this one was adjusted to use the items' feature vectors in the process of obtaining the neighbors instead of the traditional users' ratings vector. with that, our approach can be regarded as a hybrid recommender, since it uses content-based representations in an item-based collaborative filtering algorithm."
"for the npc class, we used the traveling salesman problem (tsp), which requires the solver to form a closed loop through the graph that visits each node exactly once. for the psc complexity class, we used a strategic game called geography (geo), in which players traverse a path on the graph by selecting a node at each turn, starting from an initial node. the first player to reach a node which does not have outgoing edges, or only has outgoing edges to nodes that were previously chosen (a.k.a \"sink\") loses the game. for both problems, we generated an easy instance (denoted tsp-e and geo-e respectively) and a hard instance (denoted tsp-h and geo-h respectively). fig 1 (top) shows a visualization of a possible solution to the tsp-h problem with a solution emanating from the node labeled 29 and terminating with the node labeled 47 having traversed the entire graph with no cycles . fig 1 (bottom) shows the geo-h problem instance in which the green player is positioned at node 26 and is asked to choose the next node which will guarantee a win over the blue player."
"where k if is equal to 1 if an item i has the candidate word in at least one of its reviews. the if f is then compared to a threshold, and if its value is greater than it, the candidate word is maintained in the term set. in an earlier experiment, we considered four different thresholds for constructing lists of terms: 1, 30, 100, and 200 [cit] . by considering these threshold values, we verified the impact of different sizes of term sets, as well as what types of terms should be regarded: those more specific, i.e., that may appear in at least two items (threshold 1) or those more general, i.e., that may appear in a larger set of items (threshold 200). the results indicated that the threshold of 30 is a good value since it provides the best tradeoff between term set size and recommendation accuracy, performing better than the baseline."
"we focus on intellective problems with complete information, which require at least some computation and for which there is a ground truth and solutions can be verified for correctness. we distinguish such tasks from judgment tasks where there might not be sufficient information to determine the ground truth during the group's decision-making process (e.g., a jury's decision), and quantitative assessment tasks, such as the famous task of assessing the weight of an ox [cit], where statistical convergence to the mean makes a larger number of team members beneficial (i.e., the wizdom-of-the-crowd phenomena)."
"the article is structured as follows: in \"related work\" section we overview some works related to the use of users' textual reviews in the recommendation process; in \"the proposed system\" section we describe the proposed system. in \"empirical evaluation\" section we present our empirical evaluation, detailing the experimental setting, databases and the results. finally, in \"conclusion and future work\" section, we present our conclusions, and discuss current limitations and future work."
"the algorithm splits the tokens of a sentence and calculates the sentiment by combining the tokens and constructing a tree in a bottom-up approach, where the root node is the final sentiment for the whole sentence and may contain one of the five sentiment levels: \"very negative, \" \"negative, \" \"neutral, \" \"positive, \" and \"very positive. \" we convert this classification into a [ 1, 5] rating system, being 1 equals to \"very negative\" and 5 equals to \"very positive. \""
"in this approach, we apply a set of heuristics similar to those applied in the term extraction, to reduce the number of candidate words and create aspects from the reviews. we apply in the set of lemmatized word both linguistic and statistical filters applied in the term extraction technique: first, we select only nouns and then apply the item frequency, discarding the candidate words that have an if f value lower than a certain threshold. based on the results obtained in the previous approach, we apply the threshold of 30 since the terms that appear in a number of documents smaller than 30 do not heavily affect the results and thus can be regarded as noise."
"regarding the results for the item recommendation scenario, we observed that those produced by all approaches outperform the other results, on average, in 1% on prec@10, while for the map there are no conclusive results. although the results for the ml-100k database are favorable, those obtained for the hetrec ml do not exceed those obtained by the baselines. despite that the prec@10 results are favorable, there is not a significant contribution to the item recommendation scenario. this is due mainly to the nature of the implemented algorithm. since it performs the rating prediction task and then through the ratings, it generates a ranking of the 100 items with the highest predicted rating, it is assumed that the final ranking contains items which are not in the test set, since it is a drawn portion of the 10% of the database. since the goal of this article was to evaluate the impact of the feature extraction techniques, and for this, we chose a k-nn algorithm initially developed for rating prediction tasks, we leave for future work the extension of this model to produce better rankings."
"finally, we apply the classical hierarchical clustering algorithm upgma (unweighted pair group method with arithmetic mean) [cit] in the matrix m c, producing a hierarchical clustering model of documents."
"the resulting output of this procedure are extensible markup language (xml) documents containing the texts in a more structured form. in those files, each of the documents is described in a tree structure that segments them into sentences, in which each one of them may contain its syntactic parsing, a sentiment and the tokens. each token, in its turn, contains its lemmatized and stemmed form and its pos tag."
"from the set of remaining terms, we group those that contain the same or similar stem. for example, when performing the pre-processing step, we may obtain the lemmas \"director\" and \"direction, \" but both share the same stem \"direct. \" by grouping the lemmas that share the same stem, we often reduce the number of features that have relation to the same topic."
"within the broad category of intellective tasks, we studied two types of problems that are computationally hard, in the sense that the number of possible solutions to consider grows exponentially with problem size. however, they differ in the amount of computation required to verify solutions, hence they should exhibit different levels of demonstrability [cit] . the first problem type belongs to the np-complete (npc) computational complexity class, for which solutions can be verified in polynomial time (with respect to the size of the problem). the second problem type belongs to the pspace-complete (psc) computational complexity class, for which verifying solutions requires exponential time [cit] provided preliminary evidence regarding the relationship between demonstrability and computational complexity. we extend their study in the following ways. first, by showing that for some problems of low demonstrability, groups may fail to converge to the correct solution. second, by deriving the boundary that distinguishes these problems from others for which group performance monotonically improves with group size. third, by providing a new empirical design for showing the effects of demonstrability on performance."
"the empirical results and simulations supporting hypothesis 2 show that for problems of particularly low demonstrability (such as hard psc problems), increasing group size beyond a certain finite number is detrimental to group performance. intuitively, the reason for this detrimental effect is that the benefit of adding group members is marginally decreasing, because at some point the likelihood of having at least one group member who correctly solves the problem converges to 1, and beyond this point adding more group members is no longer beneficial. at the same time, increasing the group size monotonically decreases the likelihood that a majority of group members will accept the correct solution and reject the wrong solutions. therefore, beyond a certain optimal finite group size, this negative effect will outweigh the positive effect of increasing the likelihood of generating a correct solution."
"we address this gap by studying the joint effect of demonstrability and group size on group performance. we formalize the intuitive notion of demonstrability by drawing on computational complexity theory [cit] . specifically, computational complexity considers two factors: (1) solution complexity, how the computational resources required to solve a given problem grow with problem size, and (2) verification complexity, how the computational resources required to verify the correctness of a given solution to the problem grows with problem size."
"in addition, we provide a more in-depth evaluation regarding the four techniques proposed by comparing them in two different recommendation scenarios: rating prediction and item recommendation. as part of this work, we still analyze the results in two movie databases that differ greatly in size, being one very small and the other very large. based on the experiments, we elect and discuss the feature extraction technique for items' representation that performed better among the others. according to our findings, the machine learning techniques produce better results than their heuristic-based counterparts in the majority of the cases."
"in order to evaluate which technique describes better the items for a recommender systems, the resulting recommendations were evaluated in two main scenarios: rating prediction and item recommendation [cit] ."
"even though the classification terms technique provided the best results, the other term-based technique also produces interesting results, being significantly easier to implement since it is based on simple heuristics. those heuristics also require fewer computational resources, being an interesting approach for limited recommender servers. the hierarchy aspects approach also provided interesting results in a larger database setting, providing a much smaller set of features than the term-based approaches, being interesting for systems that performs the item similarity calculation online and requires faster results. finally, the heuristic aspects approach provided competitive results with the baseline, being a good alternative for systems that have restrictive computational resources and do not have available items' metadata."
"the most common way to represent characteristics of a text is through terms. terms are words extracted from a text describing the subjects covered in it [cit] . especially in product reviews, terms are predominantly nouns, due to the fact that these reviews refer to nominal characteristics of the products, for example: \"the resolution of this camera is good\" or \"this actor was not convincing\". in the sentiment analysis area, in turn, characteristics tend to be represented as aspects, which are collections of terms that represent the same topic [cit] . figure 2 illustrates the difference between terms and aspects. in the figure, the review was processed and only the most important words were maintained as terms (first column). from them, we could produce aspects, which are depicted in the second column and may contain more than one term. for instance, the aspect \"acting\" contains the terms \"act\" and \"performance\"."
"in preliminary experiments [cit], we used only the well-known movielens 100k (ml-100k) database due to the fact that it is smaller and therefore simpler to find additional information about items, as well as being quicker to perform the experiments. the ml-100k consists of 100,000 ratings (from 1 to 5) performed by 943 users for 1682 movies."
"as stated earlier, we perform in the pre-processing step a sentence splitting in the reviews, so they can be processed by the sentiment analysis algorithm, resulting in a set of sentiment information of all reviews' sentences. in the adopted approach [cit], recursive neural networks models are used to build representations that capture the structure of the sentences, obtaining in this way their sentiment based on the meaning of each word."
"the last step, performed semi-automatically, is to group synonymous topics. we use the wordnet 2 lexical database as a basis to obtain the synonyms of the lemmas for each existing topic and group those topics who share the synonyms. after performing this step, we make a manual check to remove errors and noise."
"to illustrate this step, fig. 3 presents the sentiment assignment to the aspect \"acting, \" extracted from the review sample in fig. 2 . considering that an item i in the system has only the review in fig. 2, the system finds the sentences that make reference to \"acting, \" obtains their polarities, and calculates the average sentiment. finally, if necessary, the system performs the binarization step."
"coincide with the equation. the problems from our empirical problems fall in different regions of this space. the npc type problems (tsp-h, tsp-e) fall within region b (in blue) whereas the psc type problems (geo-h, geo-e) fall within region c (in orange). the supplementary information also includes analysis of the optimal group size for problems within this region (fig b in s1 file) ."
"where w i and w j correspond to the average value of the features of i and j, respectively. the final similarity measure is a shrunk correlation coefficient, s ij :"
"de novo whole genome assembly reconstructs genomic sequence from short, overlapping, and potentially erroneous fragments called reads. we use optimized parallelized program of the most timeconsuming phases of meraculous, a state-of-the-art production assembler [cit] . it is a novel algorithm that leverages one-sided communication capabilities of upc to facilitate the requisite fine-grained parallelism and avoidance of data hazards. a lightweight synchronization scheme is the heart of the parallel de bruijn graph traversal."
"finally, we also instrument the shared memory allocation function and always set the content of newly allocated object to zero. this is to avoid the occasional missed log values because of the same values in shadow memory."
"log processing. the value and order logs generated in full execution are processed to obtain the required event order in replay. based on the distributed event order log, this pass generates a replay order log for each thread in sub-group. the event orders are translated into wait and wake operations so that threads in subgroup could collaboratively enforce the order present in the original execution. in addition, a write check log is generated for each thread so that it could try to match its own written values with remote read values in certain ranges at correct points in replay phase. we use this value-based approach to infer communications between threads in sub-group because there is no explicit matching between senders and receivers in one-sided communication."
"sub-group replay sreplay only executes the threads in subgroup in replay phase. the effects (e.g. remote writes) of any other tasks can be reconstructed from the logs. each thread in subgroup reproduces the same execution by injecting the values in its value log at correct points. the operations from different threads are scheduled to execute in an order according to the replay order log. in addition, after a thread performs certain writes, it needs to check whether all the local writes so far could contribute to some read value log entries of remote threads. on a value match, a com- case ei is a read of range (a,len)"
"radtransceiver also has a priority system that affects the order of processing of the images. transfer processes with a higher priority always appear in the top rows of the queue and are thus handled right after the current active process has finished. when there is more than one process with the same priority they will be sorted based on their time of requests, following the 'first in -first out' principle."
"with sreplay, we could provide smt solvers with only constrain formulations for threads in sub-group. if a solution is found, it means that a schedule of threads within sub-group will reproduce the bug. in another word, we could conclude that such a bug is only caused by threads in sub-group. in this case, the users could inspect the schedule and replay the buggy schedule repeatedly. if a solution is not found, it means that some read values are not produced by any threads in sub-group, an smt solver, like z3 [cit], will produce an unsatisfiable (unsat) core which is a subset of constraint clauses that conflict, leaving the formula unsatisfiable. unsat core could localize the read values that are produced by threads outside subgroup. then, we could search the external threads that ever wrote the required value to these read addresses by a new execution, and add all those threads in a new sub-group. then we could let smt solver try to find a solution again. such iterative search continues until the smt solver finds a solution to the constraints of the extended sub-group."
". this approach generates a set of causal relationships between individual accesses to ensure that the replay and record enforce the same orders for all events. figure 4 shows the insight. from the vector clocks, t2 can identify the difference between gl0 and gl1. according to our rule, the second r(x) in t2 is causally ordered after w(x) in t0. in t3, there is no memory access performed between the two global layers, so there is no order generated. t4 performs a memory access w(z), but it is not conflicting with r(x) in t2, so there is no causal relationship between the two and also no order generated. for the example in figure 3, before r(x) in t1 is performed, the current vector clock in the thread is [cit], after the operation, the vector clock becomes [cit] . according to the rule, r(x) needs to be ordered after w(x) in t2 and w(y) in t3. note that w(y) in t3 does not conflict with r(x) in t1, but it is causally ordered before r(x) in t1. specifically, it is because the vector clock obtained in t1 at r(x) (most recently updated by w(x) in t2) include w(y) in t3 due to t2's r(y), -they are indeed conflicting accesses."
"in figure 2, the shaded region indicates sub-group. white dots indicate read accesses that do not have value log entries; black dots indicate read accesses that generate value log entries; brey dots indicate write accesses. we will discuss how we avoid logging the value for every read in section 4.1. the arrows indicate detected event orders, which is a superset of orders between conflicting accesses. a read could be ordered after multiple writes (such as the second read in the second thread) but it could only get value from one write ordered before. we infer the precise order between writes and reads in replay phase. some read could get values written by threads outside sub-group, such as the second black dot in the fourth thread in sub-group. in this case, there may be no write event ordered before the read in sub-group."
"finally, if the calculation is applied to a jet with sufficient symmetry (e.g., round nozzle, chevron nozzle with small penetration angles, 19 etc. ), microphones distributed along the azimuthal angle are statistically identical. as a result, the calculations of the spectra can benefit from the symmetry in these individual microphones and are ensemble-averaged to achieve better statistical convergence."
"we see that if two threads running on the same node, our algorithm practically does not produce any incorrect event order, while in theory, it is possible. when the threads run on different nodes, we do see a small percentage of mis-reported orders and it increases with system size. it is reasonable since the larger system produce more variances in memory access latency and the effects of nonatomic instrumentation become more significant. however, even with 1024 threads, we only have 15% of mis-reported orders. the consequence of such mis-reported orders is the potential imprecise information provided in the debugging tool, but the replay correctness is never affected. moreover, this result is from the test program that artificially generates a large number of data dependences together with each other, which is unlikely to be the case for real applications. therefore, we believe that our simplified vector clock algorithm does a good job in detecting event orders in large-scale executions."
"in uts, the upc_fence operations in line 10 and line 39 are inserted for this purpose. if we delete those fences, we indeed found incorrect behavior in a machine with powerpc processors, which supports a more relaxed memory consistency model than intel processors and allows the reordering of write operations."
"this paper attempts to answer this question by proposing the first scalable partial r&r tool, sreplay, combining the best of data-and order-replay. sreplay is a hybrid design in that it performs coordinated deterministic replay of a sub-group (i.e. a set of threads of interest) (instead of an individual isolated thread) and reconstructs event orders based on information logged in record phase. similar to data-replay, each thread in the sub-group generates value logs for loads, in addition, we also track event orders among threads in the sub-group. the value logs are not only used to ensure isolated thread replay, but also used to infer communications based on value matching assuming the logged event order. threads not in sub-group are not executed in replay."
"in this context, large eddy simulation (les), along with advancement in high-performance computing, is emerging as an accurate yet cost-effective computational tool for prediction of turbulent flows and their acoustic fields. as reviewed by bodony & lele, 1 reliable les prediction of jet noise requires particular attention to details in many different aspects, e.g., inclusion of nozzle geometry, appropriate treatment of the nozzle boundary layers, 2-5 sufficient grid resolution, low numerical dissipation and dispersion, appropriate inflow and outflow boundary conditions, 6, 7 shock capturing schemes, 8, 9 use of acoustic analogy methodologies [cit] best suited for jet aeroacoustics, etc. the present study is part of a broader ongoing effort to improve understanding and develop predictive capabilities of propulsive jet noise, through high-fidelity physics-based simulations with the unstructured compressible flow solver \"charles\" developed at cascade technologies. as a complement to experimental studies, the les provide access to complete flow fields, allowing in-depth probing of the physics of jet noise production. in previous work, charles has been used to investigate a wide range of high-speed unsteady flow processes for various complex configurations, including impinging flows, 13, 14 circular 15, 16 and rectangular 17, 18 jets, chevrons, 19, 20 and faceted military-style nozzle. 21 experience gained from these studies is currently used for the mesh design, numerical setup and acoustic post-processing steps of ongoing work, to continue advancing existing methodologies towards best practices for jet noise predictions with unstructured les. the paper is intended as a review of some of the lessons learned so far and as an additional step towards such best practices."
"memory consistency models [cit] specify the order in which memory accesses performed by one processor become visible to other processors. it is a central concept in shared memory parallel architecture [cit] and programming models based on shared memory (i.e. upc [cit] ). sequential consistency (sc) is a strong memory model mandates that the global memory order is an interleav-ing of memory accesses of each thread with each thread's memory accesses appearing in program order. sequential consistency violations (scvs) happen when non-sc behaviors are allowed by architecture or runtime system due to the lack of synchronizations (e.g. fences). it is critical to monitor and detect scvs as they likely indicate concurrency bugs. recent works [cit] show the techniques to dynamically detect scvs. unfortunately, these proposals rely on the ability to detect conflicting accesses (i.e. data races) based on cache coherence, which does not exist in implicit one-sided communication."
"it was shown that ensuring the correctness in relaxed memory consistency with synchronization operations is challenging [cit] . it is important to provide programmers of distributed memory with with one-sided communication with the ability to analyze these bugs. sreplay readily made it possible to analyze memory consistency based on values returned by load operations. for each thread in sub-group, sreplay provides the returned value for each load. the values in record phase could be affected by the semantics in relaxed memory model: a value returned and logged may not be possible in an sc execution. in replay phase, with much fewer number of threads, we could easily ensure that memory operations from different threads in sub-group are executed according to sc. the value returned in replay with an sc execution may be different from the value logged in record, and a user could consider this as a potential scv. it is possible that the value is produced by some threads not in sub-group, therefore, such analysis is not precise. nevertheless, it provides the users with good hints to pinpoint the root cause."
"the ability to reproduce a parallel execution is desirable for debugging and reliability purposes. in debugging [cit], a programmer needs to travel back in time and deterministically examine the same execution, while for resilience [cit] this is automatically performed by the the application upon failure. to be useful, deterministic record and replay (r&r) is required (i.e. replay faithfully reproduces the original execution). for parallel programs the main challenge of r&r is inferring and recording the order of acm acknowledges that this contribution was authored or co-authored by an employee, or contractor of the national government. as such, the government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for government purposes only. permission to make digital or hard copies for personal or classroom use is granted. copies must bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. to copy otherwise, distribute, republish, or post, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. conflicting operations (data races). this problem has been investigated intensively in the context of shared memory [cit] and distributed memory programs [cit] . our main interest is to enable r&r for programming models based on one-sided communication [cit] that are increasingly used in large-scale scientific applications."
"for far-field noise predictions, the accurate propagation of the small amplitude acoustic fluctuations from the near-field source region to the far-field microphones within the computational domain would be prohibitively expensive. the ffowcs williams-hawkings (fw-h) equation 31 is one of the most commonly used methods to overcome this difficulty. sound at a far-field location can be computed from flow information on an arbitrarily-shaped surface s and the volume-distributed sources outside of s. if s corresponds to a physical solid surface (e.g., an helicopter blade or aircraft landing gear) the fw-h formulation is referred to as solid (impenetrable), and as permeable (porous) otherwise. one advantage of the permeable formulation is that it allows for the acoustic sources in the volume v outside the solid surface but inside the data surface s to be taken into account."
"since the whole transient flow field is stored, these simulations yield large databases, between 10tb and 18tb each. these databases reside in archival storage at erdc and are currently continuing to be used for post-processing and analysis, probing the physics of jet noise production. for practical use where only a few specific measurements are of interest (e.g., thrust loss, near-field and far-field noise, etc. ), only a subset of the flow field needs to be recorded (e.g., mean measurements, transient data on fw-h surface, etc. ), significantly reducing the data storage requirement."
"(i) in terms of performance, processor load-balancing based on the shock sensor was found critical for the flow solver to retain linear scalability on large number of cores, when shocks are present in the flow. for the far-field noise solver, the frequency-domain formulation 33, 34 of the fw-h equation is a robust and efficient method ideally suited for jet aeroacoustics (including forward flight effects)."
debugging tools such as data race detectors [cit] or stack inspectors [cit] could help identify the initial sub-group. we leave the systematic exploration of this topic as future work.
"after processing all distributed event order logs, a map is generated for each thread in sub-group, it is then written to an order log used during replay."
"with different replay group sizes (2,4,8,16 ), we see that the record overhead only increases slightly or almost the same. the reason is two-fold. first, the main overhead is introduced by instrumentation of read and write accesses. they are local overhead and do not increase when the number of threads in replay group increases. second, the overhead due to vector clock does increase when replay group size increases. but sreplay size is not expected to be large."
"we propose a simplified mechanism to generate conservative causal relationship of events. consider vi(ei0), it captures the set of all accesses from all threads that causally happened before ei0. we could consider it as a global layer, denoted as gl[ei0]. it captures the boundary of most recent previous accesses in all threads that are causally executed before ei0. when ti performs the next memory access ei1, similarly, vi(ei1) represents a different global layer gl[ei1]. to reproduce the event orders in an execution, it is sufficient to execute ei1 after the accesses in each remote thread on gl[ei1]. these accesses are denoted as vi(ei1) ["
"as a demonstration case, les of supersonic jets issued from a converging-diverging round nozzle are performed at three different operating conditions: isothermal ideally-expanded, heated ideally-expanded and heated over-expanded. first, the numerical methods are reviewed in section ii. details on the flow configuration and computational setup are presented in section iii. comparisons with the available experimental measurements carried out at united technologies research center (utrc) for the same nozzle and operating conditions are shown in section iv. it is important to note that, to put the predictive capabilities to practice, all the simulations and post-processing were performed without prior knowledge of the experimental data, such that the initial results presented in this paper correspond to \"blind\" comparisons. finally, as one of the important aspect of the jet noise problem, the far-field acoustic formulations best-suited for jets are discussed in section v and appendix."
"this section focuses on the unheated ideally-expanded jet case a1 and the blind comparison with the corresponding experiment b118 (see table 1 for details). the experimental post-processing for the comparison with cases a2 and a3 is currently ongoing and will be presented in future publications. however, some les results for these cases are reported in section v."
"the digital imaging and communication in medicine (dicom) standard has been widely adopted for recording and sharing of digital medical images [cit] . a dicom image consists of a header with patient and exam data firmly connected with the image pixel data in a single file. the dicom header contains a large amount of information related to the pixel data it belongs to, including identifiable information about the patient, the study, and the institution. because of this inclusion of possible sensitive data, de-identification of dicom images is often required in data exchange for clinical research [cit] ."
"the lookup-table consist of an id-mapping between the original and the de-identified id and was used by the system to record and guide the de-identification process. however, besides the replacement of the patient id as used in the lookup-table a whole set of 50 dicom header tags are deidentified during the de-identification process (see table 2 ). these 50 tags are considered to be the possible cause of data breach when they are exposed to a third party either by the element itself or combination with other elements. all 50 elements are replaced by the de-identification id, a blank, or a more general representation. although this provides a proper de-identification of the dicom header, a more comprehensive de-identification, including blanking of part of the image, should be considered in accordance to the dicom supplement 142 clinical trial de-identification profiles to ensure the full protection of patient's private data [cit] ."
"after being logged in into radtransceiver, a user initiates and registers the transfer of dicom data through the web interface by entering a patient id and selecting the appropriate images to be transferred from the pacs. dicom transfer of the selected images to another dicom node without deidentification is performed instantly while all other transfers are pushed into a processor service. using the processor, images can be transferred from the pacs or modality into the predefined storage destination or sent through a process of image de-identification using a specific profile before they are stored."
"deterministic replay is a powerful technique for debugging hpc applications. in principle, replay tools for hpc applications typically fall into two categories [cit] . data-replay tools record all incoming messages to each thread during program execution, and provide the recorded messages to threads during replay and debugging at the correct execution points. this approach allows individual threads to be replayed in isolation. in contrast, orderreplay tools only record the order of nondeterministic events in inter-thread communication during program execution. since orderreplay does not record actual inputs to threads, it typically generate smaller logs than data-replay. however, detecting event orders at large-scale poses scalability challenge."
"sreplay uses the principle of data-replay to ensure the correct replay of each thread in sub-group based on value logs. we use a combination of order-replay and value matching to infer the communications between threads in sub-group. this idea is novel and has not been exploited in previous work. this design principle is critical for improving usability since purely relying on order-replay requires replaying all threads (not satisfying requirement of partial replay). due to non-atomic instrumentation, it is very challenging to generate precise event orders. our current approach does not rely on precise event order among threads in sub-group."
users can track the status of their own transfers by accessing the web interface for the associated process showing a list of jobs in 'waiting' (fig. 4) and 'processing' (fig. 5) state.
"(a) one of the first and more critical aspect of jet noise prediction is the careful design of the mesh for aeroacoustics: here, sufficient resolution in the shear-layer, as well as cell isotropy and limited grid stretching in the acoustic source-containing region, are strongly recommended. (b) likewise, the importance of the computational methods used in the flow solver needs to be stressed: numerical schemes with low dispersion and dissipation are essential to aeroacoustics predictions."
"for each application, we show the size of shadow memory allocated. it includes both read and write shadow memory. we see that different applications show drastically different characteristics. we found that shadow memory size increases after the executions start and then become stable after certain points. the largest shadow memory size appears in meraculous. it is due to large input data size (150 gb). sreplay also uses a separate shadow memory to keep written values. the final column shows the largest log size generated by a thread in sub-group for each application."
"the result of a patient id based search from the pacs provides a list of selectable items of available study entities and serie within those entities in dicom files. date, time and study descriptions are shown for each study entity. the date and time, modality type, series number, series description, number of images, and accession number are shown for each series. the user then selects the required series by ticking the checkboxes at the beginning of the corresponding row. further processing will be done after the submission button is clicked by the user. the de-identification id is entered by the user using an input box provided at the bottom of the page. when no de-identification id is provided a time-stamp based de-identification id will be automatically generated. the search results, study entity selection and entry of identification-id are shown in fig. 3 ."
"(j) other important aspects of the jet noise problem which were not addressed in details in the present study include characterization of the nozzle boundary layers (e.g., evaluation of the momentum thickness at nozzle exit), appropriate inflow and outflow boundary conditions, shock capturing schemes, effect of subgrid scale model, etc."
sreplay maintains a shadow memory in each thread in sub-group. the shadow memory indicates the current local view of shared memory of a thread. each address in the shadow memory is associated with a sequence number (sn). the contents of a memory address are logged either at its first read or when the value read by the execution differs from value stored in the shadow memory.
"to perform the processing of the images the services provided by the clinical trial processor (ctp) are utilized [cit] . ctp is a stand-alone dicom application entity (ae) developed by the radiological society of north america (rsna) [cit] . it has the capability of using multiple, independently configured, pipelines for running several processes in parallel. with this flexibility and configurability of pipelines the deidentification of selected patient data can be performed using a variety of pertinent rules and regulations [cit] . in our dicom distribution system, each pipeline runs its own ctp dicom import service. this import service consists of a storage service class provider (storage scp), accepting images from other dicom nodes (e.g., pacs or modality) and putting them in a queue for further process. fig. 1 two kinds of image transfer: a a direct transfers from one node to the destination node (such as from pacs to viewers); b a transfer through a processor before images reach the destination in our setup, three possible pipelines are defined (fig. 2 )."
"\"adapt\" is a massively parallel tool developed in cascade's solver infrastructure that can refine elements locally to match a target length scale. this target length scale can vary throughout the domain, and can even be different in each direction for non-aeroacoustic applications. specification of the target length scale can come from expert knowledge of the problem, a desired mesh size limit based on compute resources, or even from a solution on an unadapted or partially adapted mesh. the directional refinement capability dramatically reduces the overall mesh size and also prevents the addition of stiffness to the problem due to excessively small elements. the \"adapt\" tool also provides a surface projection algorithm to respect nonplanar mesh boundaries during refinement, ensuring accurate representation of the underlying geometry, and allowing the use of very coarse grids as starting point."
"overall, we found that the runtime overhead is mainly decided by: (1) instrumentation of local load/store or remote put/get; (2) shadow memory size. applications typically show a large difference on the two aspects, therefore, we see variations in record overhead. in particular, uts-upc has a large overhead, it is partially due to the instrumented shared memory accesses in busy wait. for applications with large shadow memory size, we see that the overhead could be large (as for ft). because shadow memory needs to be accessed on all instrumented reads, large shadow memory tends to have poor cache locality. this could explain why cg has lower overhead than ft, because the shadow memory size is much smaller. for meraculous, although the size of shadow memory is much larger than ft, the log size is in fact smaller than shadow memory size. this suggests that the data in shadow memory are mostly allocated and written once. in another word, when deciding whether some values need to be logged, we mostly find that chunk of data not appear in shadow memory. therefore, there are no byte level comparisons in those cases. since the overhead is depending on multiple factors, we cannot draw conclusion based on a single factor, for example, for both cg and lu, the ratio between log size and shadow memory is large and shadow memory size is small. however, the overheads are different. in this case, the different overhead is due to (1), -the instrumentation."
"based on previous grid resolution studies, 15, 16 the refined mesh used in all the simulations contains approximately 42 million unstructured control volumes, mostly hexahedral. in general, hexahedral grids 30 are found less dissipative than tetrahedral grids 37, 38 for unstructured les of jet noise. with the current standard in high-performance computing and unstructured les, the present grid would be considered of medium size, compared to larger grids 20, 21 used in recent studies."
"the measurements of the time-averaged streamwise velocity along the jet axis and in different cross-flow planes are compared to the azimuthal-averaged les predictions in figure 6 . the results show good agreement with experiments, as the centerline decay and length of the potential core are accurately captured in the simulation. the potential core length, taken as the distance up to which the flow velocity is greater than 95% of the jet exit velocity, is approximately 9.7d in the simulation and 10.2d in the experiment. in general, this quantity is challenging to predict numerically. under-prediction is often observed in simulations and is typically associated with lack of resolution in the jet plume 15, 40 and with initially laminar jets transitioning to turbulence outside of the nozzle."
"practically, sreplay makes it possible to debug a large-scale execution on a smaller (or even local) machine, relieving users from monitoring a large number of concurrent events from thousands of threads. at the same time, it provides the insights on communications between threads in the sub-group for debugging purpose. the scalability is ensured by several simplifications so that sreplay it could be used in large executions involving thousands of threads. moreover, partial replay is intrinsic to the scalability of resilience techniques [cit] using uncoordinated or quasi-synchronous checkpointing and recovery. in this paper, we focus on the usage of r&r in debugging."
"in sreplay, communication is inferred by matching values written by a potential producer with the values in remote threads' value log. consider the scenario in figure 5 . in record phase, there are three read accesses from t2 that incur new values logged (e21,e22,e23). the number indicates the return value of each read. when each one is performed, its vector clock represents a global layer that indicates the set of remote accesses that ordered before it. such global layers are denoted by dashed lines. the arrows indicate the remote accesses that produced the new values logged. the goal of value matching is to infer the solid arrows in replay phase."
"the managers of radtransceiver can create one or more user-specific module(s) related to the type of clinical research study and associate a transfer protocol for that user/study combination. the transfer protocol destination node defines how it has to be stored (file system or dicom) and whether it has to be de-identified or not. this allows to configuring and utilizing different modules for specific tasks and users. these fig. 2 data are transferred using one from three types of ctp pipelines set for the processor: 1) a pipeline without de-identification scheme to a local file system; 2) a pipeline with a de-identification process before the images are stored in a specific directory; 3) a pipeline with a deidentification process before the images are sent to another dicom node by the export service fig. 3 web interface. the web interface shows the results of a search for a patient from the pacs based on the patient id, selection of images (surrounded by the red rim on the left side), and entry of an anonymizationid (surrounded by the yellow rim) in the bottom part of the web page modules are linked to the user account and thus a user can only execute those modules assigned to his/her user profile."
"because event orders could be easily tracked in message passing, existing research has been mainly focusing on mpi r&r debugging [cit] . sub-group reproducible replay (srr) [cit] tries to find a good balance between data-replay and order-replay by considering a hybrid approach. srr divides all mpi ranks into disjoint replay groups, based on the insight that ranks communicate mostly with few other ranks in the same group. during the record phase, srr records the contents of messages across group boundaries using data-replay but records communication orderings within a group. each group could then be replayed independently. the idea of sub-group in srr is similar to our ideas in sreplay. however, due to the fundamental difference between two-sided and one-sided communication, the techniques in srr could not be applied to our context. for example, during the runtime of record phase, we could not determine whether a communication is within or between different sub-groups."
"due to the fundamental feature of one-sided communication, tracking event order is inherently much more challenging than in two-sided communication. the record and replay of individual thread in pinplay [cit] could be potentially directly applied in this context. however, as we discussed in section 1, it does not provide sufficient insight for debugging purpose. there is no previous work addressing this problem."
"shared memory r&r techniques either monitor thread scheduling [cit] by tracking synchronization apis, or log [cit] the memory accessed within each thread. in distributed memory, r&r techniques for mpi [cit] have been developed with emphasis on scalability. they track two-sided mpi_send/mpi_recv operations and ignore local memory accesses. unfortunately, none of existing solutions are sufficient to enable deterministic r&r for distributed shared memory with one-sided communication. this mode is the base for partitioned global address space (pgas) languages such as upc [cit], co-array fortran [cit], chapel [cit], x10 [cit], openshmem [cit] and an important feature of the new mpi-3 rma [cit] . existing deterministic r&r tools for shared memory [cit] supports two \"end points\" in the design space. on one end, a r&r tool could log the inputs (values) to loads to one thread, with these values injected into replay execution at the right points, this thread could be replayed in an isolated manner. it is called datareplay [cit] . on the other end, a r&r tool could detect and record the order of events from all threads, a deterministic replay could be achieved by scheduling events in the same order. it is called order-replay. however, the two designs could not simultaneously achieve the usability and scalability. data-replay incurs only local instrumentation overhead but provide little insights on communications between threads. order-replay incurs high overhead (increases with system size) to track event orders in largescale distributed memory. therefore, the question is how to design a useful and scalable r&r tool for one-sided communication in distributed memory?"
"an instantaneous snapshot of the vorticity field is shown in figure 4 and highlights the detailed turbulent structures resolved in the simulation. in this figure, the computational mesh is partially shown in planes normal to the axis of the jet, to illustrate the unstructured mesh capabilities. as expected, the thin laminar shear layer issued from the nozzle quickly transitions to turbulence, within one diameter from the nozzle exit. the turbulent flow mixes as it develops farther downstream of the nozzle, without significant grid transition effects. a more global view of the transient flow field is presented in figure 5, showing the flow (visualized by the contours of temperature in red scale) and sound (visualized by contours of pressure in gray scale). the dominant noise radiation towards aft angles is clearly visible. sound waves are emitted mainly in the vicinity of the nozzle exit, as well as downstream of the jet. note that the visualized sound waves far from the jet plume do not represent the actual sound that can be computed using the fw-h solver. as discussed in section ii, the coarser resolution and lower-order dissipative discretization outside of the acoustic sourcecontaining region (i.e., the zone enclosing the fw-h surface) leads to the intended smooth but quick damping of the pressure waves in the computational domain."
"we solve this problem by updating shadow memory for thread local stores. when later a thread reads some addresses written by itself, no value log is generated because the values from shared memory and shadow memory is unchanged. in our example, after (1), in shadow memory, @tmp1 holds the value returned by upc_alloc(). at (2), we find the value @tmp1 unchanged, as if the thread previous already observed it. no value for p_addr is logged. so replay phase will correctly use the address of actually allocated object. essentially we write the dynamically allocated addresses into shadow memory, so it will not be logged later."
"(d) for fair comparison with measurements, attention to the details of the experimental procedure is important (e.g., shear-layer correction, atmospheric attenuation, far-field assumption, consistent postprocessing of time signal for spectra computations, etc. ) (e) special care is also required for the time signal processing: windowing of the time data should be used to avoid spectral leakage; the total (statistically converged) simulation time should be at least 10 periods of the lowest frequency of interest, for reasonable convergence of the acoustic spectra; the transient flow sampling frequency should be at least twice the grid cutoff frequency, to capture high frequencies."
"the large number of images transferred without deidentification is caused by the fact that this setup is also used to facilitate the retrieval of image data into an epr. the dicom web-viewer integrated into our epr is limited to about 1 year of storage. to enable easy access to older data, epr users can automatically retrieve the historical data from their patient with one click within the epr system. requests through fig. 6 web based dicom distribution system with ctp integrated as its image processor fig. 7 finished task shown in the web interface. the status (surrounded by the yellow rim), is indicated by \"completed\". the buttons next to the status are used for restarting or removing the task the epr are considered as high-priority tasks and therefore, the data will immediately be transferred from pacs to the epr."
"two different methods are used to compute the noise at the experimental near-field probes. the first method corresponds to the computation and propagation of the pressure fluctuations directly in the simulation (i.e., recording the pressure signal from simulation at specified locations). the les pressure time histories are recorded over 48 near-field probes equally-spaced in the azimuthal direction and the spectra are azimuthalaveraged."
"in case of transfer without de-identification, the pipeline consists of a storage scp as part of the import service followed by a storage service class user (storage scu) as part of the export service. note that a service class provider (scp) is a dicom node or device that provides specific dicom network services such as for instance those offered by the storage class defined by dicom. a service class user (scu) is the client role which makes use of dicom services offered by an scp. the radtransceiver is an application entity (ae) that is an scu as well as an scp for the dicom storage service class. an ae is a node or an application providing dicom services within the network. in radtransceiver the storage service class mechanism saves images into specific folders defined in the configuration file."
"the initial comparisons between les and experiments are very encouraging in terms of flow field as well as near-field and far-field acoustic spectra. it is important to note that all the simulations and postprocessing were performed without prior knowledge of the experimental data, such that the results presented in this paper correspond to \"blind\" comparisons. additional analyses that leverage the large les database recorded are also presented in the paper, in particular parametric studies of the far-field noise predictions. in future work, guidance from the experimental results will be used to further improve the comparisons and continue the development of methodologies towards best practices for les-based jet noise predictions."
"record at full concurrency. the user first specifies sub-group, a subset of threads that need to be replayed. a modified compiler is used to build a binary with recording instrumentation, tracking both load/store instructions, as well as communication operations (e.g. put/get). the instrumented binary is then executed at full scale on a modified upc runtime system that records the execution. for any tasks within sub-group, we record load values of each thread in its value log, we also track the order of put/get operations from threads in sub-group in distributed event order logs. the event order log indicates the order of conflicting operations accessing the global memory at coarse-grain. the execution of threads outside sub-group and their communication with sub-group are not tracked."
"when required, a pipeline with an anonymizer is configured to de-identify images before they are stored in a dicom node. the execution of a de-identification task will trigger a service to add a record for each image to be de-identified into a confidential database, registering the identity mapping between original and de-identification id."
a constant plug-flow is applied to the inlet of the nozzle such that the desired mach number and the temperature ratio are achieved at the nozzle exit plane. it should be noted that we assume that the flow issued from the nozzle is laminar.
"in replay phase, by following order log, we can order the three read accesses after the accesses before the global layers specified by their vector clocks. the value matching could be done at producer side as follows. consider e21, both t1 and t3 could compare their last write value to x with the value in t2's value log. the communication is inferred when the two values match. in the example, t3 will conclude that its write value is consumed by t2. therefore, the purpose of the value check log is to give the potential producer threads information about, at which point, the thread should match its written values with which remote new read values in remote threads' value log."
"a qualitative comparison of the flow field results between the les performed on the stretched grid 137m and the adapted grid iso217m are presented in figure 2 . high frequency sound waves originating from far downstream of the jet can be identified in figure 2(a) . this spurious high frequency noise has been eliminated completely with the adapted mesh in figure 2(b) . the spurious waves may not be apparent to a casual observer at this image scale but a magnified view of the sound field makes them evident. the same trend is observed on much coarse grids, though the spurious noise in these cases has lower frequency (i.e., larger wavelength, directly related to the larger grid spacing). overall, the results show that this numerical artifact is likely associated with the grid stretching in the far downstream region and that mesh isotropy is desired."
"where the subscript 0 refers to the stagnation (total) properties. for all the cases, the nozzle exit diameter d is retained as reference scale. table 1 summarizes the operating conditions and parameters of the simulations. table 1 . summary of operating conditions and simulation parameters. the subscripts j refer to the fullyexpanded jet properties and δ θ /d is the momentum thickness of the boundary layer at the nozzle exit. the jet configuration is presented in figures 3(a) and (b), including the default fw-h surface used to compute the far-field sound (see section v for further analysis), and the sets of near-field probes (d2p 1 − d2p 17) and far-field microphones (d2m 1−d2m 12) in the utrc experimental configuration, which match the les microphone stations. note that the more recent experimental results used in this paper correspond to a nozzle size of 2 inches, while previous comparisons 15, 16 were done for a 3 inch nozzle 35, 36 at the near-field microphones (p 1 − p 8) and far-field microphones (m 1 − m 9). the new experimental setup has more probes and therefore better resolution of the near-field flow, and the microphone distance is increased from 40d to 60d for a better approximation of the far-field conditions."
"over the last 40 years, the noise of propulsive jets powering civilian aircraft has been reduced by approximately 20 db, in large measure by the introduction of the turbofan engine with progressively higher bypass ratio. yet, more stringent noise regulations, the long term vision of effectively \"silent\" aircraft, and environmental impact concerns continue to push the civilian (and military) aircraft towards design configurations that further reduce the noise, while maintaining other performance metrics (like nozzle thrust). until recently, the development of such designs has relied largely on laboratory and full-scale testing. however, cost constraints and the high complexity of the flow often limit the range of the parametric investigation and the success of the design optimization. flow and noise-prediction tools that are fundamentally rooted in the flow-physics are needed to help improve the understanding of the sources of jet noise and guide the analysis processes towards quieter designs."
"experience gained from previous jet noise studies with the unstructured large eddy simulation (les) flow solver \"charles\" are summarized and put to practice for the predictions of supersonic jets issued from a converging-diverging round nozzle. in this work, the nozzle geometry is explicitly included in the computational domain using an unstructured body-fitted mesh with 42 million cells. three different operating conditions are considered: isothermal ideally-expanded, heated ideally-expanded and heated over-expanded. blind comparisons with the currently available experimental measurements carried out at united technologies research center for the same nozzle and operating conditions are presented. the initial results show good agreement for both flow and sound field. in particular, the spectra shape and levels are accurately captured in the simulations for both near-field and far-field noise. in these studies, sound radiation from the jet is computed using an efficient permeable formulation of the ffowcs williams-hawkings equation in the frequency domain. its implementation in cascade's massively-parallel unstructured les framework is reviewed and additional parametric studies of the far-field noise predictions are presented. as an additional step towards best practices for jet aeroacoustics with unstructured les, guidelines and suggestions for the mesh design, numerical setup and acoustic post-processing steps are discussed."
"the information embedded in the dicom data should be handled carefully to ensure the security of patients' identity, especially when transfers are intended for a clinical trial or research. the importance of security issue had motivated research in the utilisation of health information within data exchange and the use of information system technology [cit] . however, the process of de-identifying the data should be implemented such that it provides ease of use and simple configuration in order to facilitate all users -including inexperienced ones -to adequately fulfil the task of ensuring the patient data protection. the imaging it infrastructure within an institution may consists of many smaller systems that support each other of these systems, some are often not integrated within the normal workflow but operated as standalone applications thus reducing the ease of use and increasing the risk of errors and mistakes. therefore, other than just the easeof-use of the systems introduced to handle a certain task, the ease of integration of those systems have also become an important issue to adequately fulfill the requirements of smooth integration into the daily work processes. therefore, besides providing an easy to use process, health institutions are also required to develop systems that are ready to be integrated into the normal workflow using current standards."
"some modules are triggered automatically from the epr and designed to transfer data from pacs directly to the epr without user interaction. since epr users can send requests of image retrieval through the system, from the image viewing process, the epr itself can be considered as a user that can initiate image transfers from pacs."
a dicom data distribution system was developed to provide a fast and easy environment to facilitate the distribution of medical images within an institution. the use of commonly known web based technologies and the flexible processor make it easy to implement and adapt to specific use.
"the ideas in sreplay can be applied to any programming models based on one-sided communication with memory access instrumentation. we built a prototype based on unified parallel c [cit] programming language. upc is a typical pgas language which is defined with a relaxed memory consistency model that allow memory access reordering for high performance. nondeterministic execution is common in upc applications with fine-grained parallelism. in upc, global shared memory could be accessed with either local load/store instructions or one-sided remote operations (e.g. put/get). we modified the compiler and runtime system to instrument all memory accesses to shared memory."
"a consequently, the grid resolution inside the nozzle is only adequate for laminar flow and the value of the reynolds number is reduced in the computations since there is no wall model inside the nozzle. for all cases, the experimental reynolds number is about 5.5 times higher than in the computations."
"because program order contributes to causal relationship, the event orders detected are conservative. it is why in figure 3 r(x) in t1 is causally ordered after w(y) t3: w(y) in t3 conflicts with r(y) in t2, r(y) and w(x) in t2 are ordered by program order, w(x) in t2 conflicts with r(x) in t1, so transitively, r(x) in t1 is also causally ordered after w(y) in t3."
a pipeline without de-identification sending data into a file system. 2. a pipeline with de-identification that exports the images in a specific directory or network attached storage. 3. a pipeline with de-identification sending the dicom images to a dicom node using the dicom export service.
"there are two kinds of image transfer that could be initiated by the users. first is a direct transfer from source to destination node, for example the transfer to a viewer in the electronic patient record (epr) system. with the second type of transfer the images have to pass through a processor before the images are transferred to the destination node (fig. 1) ."
"in this case, t2 happens to be in sub-group, so that sreplay could provide the insights. when t0 steals from t3, sreplay could only replay t0's execution but cannot gives information on which thread performed the writes, except that it is not from any thread in sub-group."
"the initiation of a de-identification task by the user will generate a mapping of the original, unique, identifier into a de-identification identifier both in the database and the lookup-table. the update on the database is required to register the transfer itself and to record the amount of images being sent to be matched in the last stage of the process. meanwhile, the creation of id-mapping on the lookup-table will be used in the de-identification process itself. the lookup-table is updated continuously and only contains the id-mapping of active transfers. the mapping will be removed when a transfer task is finished."
"in many institutions the production of de-identified data is performed from the pacs. thus, it requires the use of a full pacs workstation for the researcher. this involves a costly pacs license and blocks the workstation for actual clinical reporting use for the time required for the de-identification. alternatively, users get the non de-identified dicom data locally and perform the de-identification process manually. this leads to the possibility of use of a wide variety of not properly managed tools and thus cannot ensure patient data and identity safety. using radtransceiver, the access is provided through a simple, password-protected, webpage which can be accessed from any pc in a hospital; the deidentification process is controlled and supervised centrally and is vendor neutral."
"if for a read access, if there is a value log entry for it, then the value from value log is used (line 8 ∼ 9). the value is written to shared memory (line 10). such value may or may not be the same as the current values in shared memory. if the value is produced by a thread not in sub-group, then shared memory does not contain it because that thread does not execute in replay. in this case, value log is used to construct the partial states in shared memory."
"pacs integration with other information systems has made medical image distribution to play an important role in the entire clinical process including the interpretation, processing and analysis of imaging data [cit] . distribution of images and information to clinicians in a timely manner is therefore required to facilitate clinical work-up. although current highspeed networks allow exchange or transfer of data from one node to another at acceptable speed, optimization of the processing of digital images during the distribution is also a prerequisite to meet the clinical and research needs."
"the bold font denotes vector quantities and the summation convention is used for repeated vector component indices i,j. the subscript ∞ indicates a ambient quantity and c ∞ is the speed of sound. the unit outward normal to the surface isn, and u i are the local fluid velocities on s. the heaviside function h(s) and the dirac delta function δ(s) are present to account for the discontinuity of the functions at the surface interface."
"where the superscript † denotes the complex conjugate, and p ref is the reference pressure. recall that st max and st min are the maximum and minimum frequency (i.e., the narrowband width) accessible by this post-processing, respectively (see section iid). while acoustic analogy calculations are typically very efficient compare to the cfd near-field simulations, the computational cost may become not negligible for predictions involving a large number of time samples, surface elements and/or observers. in particular for jet noise, a long time record is usually needed to ensure statistical convergence of the low frequencies, leading to large les databases. in addition, directivity mapping in three dimensions can require several thousands of observers. special care is therefore needed in terms of memory management, efficiency and performance. fortunately, the method is well-suited for parallelization, since the calculations for each surface element, each frequency and each observers are independent. the current mpi implementation is done in the same massively parallel infrastructure than the flow solver \"charles\". it uses a standard load-balancing approach on the fw-h surface elements: each processor computes the noise contribution of only a portion of the surface, for all the observers and frequencies; the individual contributions are then recombined linearly as final output."
"from the results and analysis presented in this paper and in previous jet noise studies with high-fidelity unstructured les, comments and guidelines can be suggested:"
"similar to laboratory experiments, exact pressure matched condition cannot be achieved in simulation, and residual shock cells are formed in the vicinity of the nozzle. the shock cell structures are visible in figure 5, and their presence suggests that the nozzle is operating slightly off design. as discussed in ref. 15, the shocks were minimized in the experimental studies by varying the nozzle pressure ratio over a small range centered at the design value and determining when the minimum sound level was observed at the 90"
"listing 1 shows two functions related to work stealing. the first function, checksteal, is called by a thread which will potentially share certain amount of its own work to another thread. the thread first checks whether it has enough work to share (line 25). if so, it updates local stack information (line 29 ∼ 35). finally, it publicizes the work using one-sided communication and writes directly (put) to the work stack of the remote thread which requested the work (line 38 ∼ 40). the first write (line 38) indicates the stolen work amount. the second write (line 40) indicates the stolen work address."
"shocks, like sub-grid scale turbulence, are also sub-grid phenomena and thus require modeling to account for their effect on the resolved flow. however, unlike sub-grid scale turbulence, they are localized in the flow and a surgical introduction of modeling is potentially more appropriate. \"charles\" uses a hybrid central-eno scheme to simulate flows involving shocks. the scheme has three pieces: a central scheme (described previously), a shock-appropriate scheme 24, 25 for computing a flux across a shock and a hybrid switch 26 to identify the presence of flow discontinuities."
"the les code \"charles\" is designed and implemented using message passing interface (mpi) to function well in a massively-parallel, distributed memory environment. because the numerical method is purely hyperbolic and fully explicit, the code requires relatively little communication at each time-step, as compared to other methodologies. scalability tests on different systems showed that the code exhibits perfect scaling at the 20,000 core level 39 and nearly linear scaling up 160,000 cores. 20 here, the majority of calculations were carried out on cray xe6 machines at dod supercomputer facilities in erdc and afrl. each simulation ran for a week on 1024 cores, for a computational cost of approximately 172 kcore-hours."
(c) inclusion of the nozzle in the computational domain will only become more important as more complex geometries like chevrons and faceted nozzles are being investigated numerically. unstructured solvers seem ideally suited to handle such complex cases.
"to facilitate data distribution within a hospital environment, a web-based system called radtransceiver was built. the system was implemented as a fast and easy web-environment, providing an integrated rule-based distribution and optional de-identification of imaging data."
"the instrumentation of memory accesses is conducted in both upc runtime and upc compiler. for each local memory accesses that are casted from shared pointers, we add \"before\" and \"after\" instrumentation by compiler. for put/get operations, we modify the upc runtime to intercept them. both instrumentations increase the sn of the thread."
"we use fifteen upc benchmarks to evaluate sreplay. eight nas parallel benchmarks [cit] (bt, cg, ep, ft, is, lu, mg, sp) and three applications in the upc test suite (guppie, laplace, mcop) are deterministic. the rest are nondeterministic by design: two applications in the upc task library [cit] (fib, nqueens), unbalance tree search (uts) [cit] and parallel de bruijn graph construction and traversal for de novo genome assembly (meraculous) [cit] . table 1 shows the parameters and data sets used in experiments."
"we discuss usage cases of sreplay based on a real-world application. we consider unbalanced tree search (uts) benchmark [cit], a typical application that could leverage the advantages of onesided communication. uts exploits a synthetic tree-structured search space that is highly imbalanced. an efficient parallel implementation of the search relies on asynchronous work-stealing to keep processors engaged in the search."
"the overhead of algorithm 2 is high for following reasons. storage overhead. two vectors (v a x and v w x ) are associated with each shared memory location. atomic vector clock updates. it implicitly requires that the updates to vector clocks happen atomically with the actual memory accesses. in a large-scale distributed memory system, to satisfy this each memory access will be associated with a lock operation when modifying the vector clock. update order requirement. the updates of vector clocks associated with memory addresses (v w x and v a x ) (line 7 and 11) should be consistent with program order. because updates to vector clocks are ordinary memory accesses to shared memory, upc runtime may reorder them. strictly enforcing the order requires using fences, which is expensive."
"over the last decade, picture archiving and communication systems (pacs) evolved from a predominantly radiological service into the fundament of the image management system utilized throughout the entire healthcare enterprise for clinical practice and research [cit] . it integrates multimedia, information, and communication technologies, providing practically unlimited amounts of data for patient care, research and teaching [cit] ."
"we relax algorithm 2 to make it practical. to reduce storage overhead, we associate a range of addresses with a single vector clock. for upc, we naturally partition the shared address space according to the affinity (owner) of shared address and assign one vector clock for each partition. we choose to not maintain atomicity of instrumentation and not use fences to ensure vector clock updates order."
"deterministic record and replay (r&r) consists of monitoring the execution of a multithreaded application on a parallel machine, and exactly reproducing this execution later. r&r requires recording in a log all nondeterministic events that occurred during the initial execution. they include the inputs to the execution (e.g., return values from system calls) and the order of the inter-thread communications (e.g., the interleaving of the inter-thread data dependences). during the replay phase, the logged inputs are fed into to the execution at the correct times, and the memory accesses are forced to interleave according to the log."
"we are interested four aspects: (1) replay overhead in different sub-group size; (2) log size; (3) memory consumption and (4) quantify the affects of imprecise event order detection. for each experiment, we choose four different sub-group sizes: 2,4,8 and 16. sub-group size is expected to be small for partial replay. since each node in edison contains 24 cores, we make sure that threads in sub-group execute on different nodes (e.g. when sub-group is 2, the threads are t24 and t48). in total, we conduct 60 executions (4 for each application). the concurrency during the initial program run and the recording phase is given by the parameter np in table 1 . the replay correctness is verified manually by comparing the results and outputs. we use only one node of edison (24 cores) for the replay phase, down from the original 1,024 cores (∼ 40 nodes) in most cases. table 2 shows our results. for each application, we show the native execution time without any instrumentation, the overhead for different sub-group sizes, size of shadow memory allocated and the largest log size among all logs generated by threads in sub-group. in addition, we wrote a micro-benchmark program to quantify the inaccuracy in event order detection."
"the momentum thickness of the boundary layer at the nozzle exit δ θ /d is estimated and reported in the table 1. the use of such thin laminar boundary layers is a convenient and popular approach, 4, 10, 11, 30 as it leads to rapid transition to turbulence near the nozzle exit while enabling coarser resolution inside the nozzle."
"(f) to further improve statistical convergence of the numerical data, azimuthal averaging of the far-field spectra (over 18 to 36 positions) can be used for jet with sufficient level of azimuthal symmetry (e.g., round jets, chevron nozzle with small penetration angles, etc. ) (g) to limit spurious noise cause by vortices crossing the side parts of the fw-h surface, the pressurebased formulation, 12, 43 rather than the original fw-h formulation, is recommended, in particular for heated jets. its additional benefit is the reduction in file size, since the density on the fw-h surface is no longer stored."
"dicom de-identification covers the security of patientrelated data, but data trackback is still possible by retrieving the de-identification entry in the conversion table. therefore, the access to the clinical research study information system database and conversion table is limited to authorized personnel and can be obtained through an internal network only."
"the compressible flow solver \"charles\" solves the spatially-filtered compressible navier-stokes equations on unstructured grids using a control-volume based finite volume method where the flux is computed at each control volume face using a blend of a non-dissipative central flux and a dissipative upwind flux. this blending approach is often the basis of implicit approaches to les, where the blending parameter is selected as a global constant with a value large enough to provide all the necessary dissipation (and potentially much more). here, \"charles\" does not use the implicit les approach, but a heuristic algorithm 13 to compute a locally optimal (i.e. minimal) blending parameter, which is purely grid-based, and can be pre-computed based on the operators only."
"the rest of the paper is organized as follows. section 2 presents background of deterministic r&r and upc. section 3 explains the essence of one-sided communication and each step of sreplay by a concrete example. section 4 shows the value logging and simplified vector clock algorithm in record phase. section 5 describes the offline mechanisms to generate logs for replay phase. section 6 describes the communication inference mechanisms and the subgroup replay algorithm. section 7 presents several usage models of sreplay, it is followed with the implementation details in section 8 and the evaluation in section 9. the paper concludes in section 10."
"we present a overview of sreplay based on upc. the details of each components are discussed in the following sections. as shown in figure 2, it involves the three steps."
"experiments are conducted on edison, a cray xc30 supercomputer at nersc. edison has a peak performance of 2.57 petaflops/sec, with 5576 compute nodes, each equipped with 64 gb ram and two 12-core 2.4ghz intel ivy bridge processors for a total of 133,824 compute cores, and interconnected with the cray aries network using a dragonfly topology."
"(h) while different alternative exist for the treatment for the outflow disk, a closed fw-h surface with the method of end-cap 10 tends to lead to better acoustic results: it is a robust and reliable method, with limited performance penalty."
"traditional large-scale hpc applications are based on message passing and they typically use message passing interface (mpi) as communication mechanism. several modern programming models for distributed shared memory use one-sided communication abstractions that offers better performance with less synchronization. this model is particularly suitable for irregular applications [cit] . several partitioned global address space (pgas) languages, including unified parallel c (upc) [cit], co-array fortran [cit], chapel [cit], x10 [cit] and openshmem [cit], are based on one-sided communication. in addition, the new mpi-3 [cit] introduced efficient support for one-sided communication with remote memory access (rma) [cit] ."
"algorithm 1 shows the detail of the value logging mechanism in sreplay. each thread maintains its local shadow memory, vsm. on a read, v (a, len) is the value obtained from the current shared memory. if the value is the same as the current value in vsm, no log is generated. if not, a new value log entry is generated and vsm is updated, so that next time ti will not log the same value again. on a write, v (a, len) is the written value and it also updates the shadow memory. this could avoid logging the values generated by the local thread and avoid logging addresses of dynamically allocated objects (see section 8) . the sn (vi[i]) is updated on both reads and writes. vi [i] in an value log entry indicates that this value should be consumed by ti in replay phase when its sn is increased to the same number."
"c otherwise, the reference frame can be rotated to satisfy this condition therefore, for jet noise predictions in uniform subsonic coflow, the complex acoustic pressurep ′ at an observer x outside of s is given as a function of frequency by the following integral solution"
some applications have dynamically allocated objects in shared memory. their addresses could be different in record and replay phase. we cannot log any shared address of those objects as values to avoid bad pointer. consider the following code:
"ss_steal is called by a thread that has posted the stealing request and is waiting for stolen work that will be granted by a remote thread. the stealindex is initially waiting_for_work, indicating that it is waiting, then the thread busy waits on a whileloop, until the local variable stealindex is updated by a remote thread using one-sided communication. after this, the local thread will observe the update by a local read (line 7) and then leaves the loop. if some work is successfully stolen, the local thread will then read the second write performed by remote thread, stolen_work_amt, to find out the amount of stolen work. finally, it completes the work stealing by copying data from the stack of remote thread to its local stack."
"the simulation setups a1, a2 and a3 reproduce the unheated ideally-expanded jet (b118), heated ideally-expanded jet (b122) and heated over-expanded jet (b122 od, where od stands for \"off-design\") conditions tested at utrc's acoustic research tunnel facility, 35, 36 respectively. additional simulations of these conditions are currently ongoing, but only the initial les corresponding to the blind comparisons are reported in this paper."
"due to challenges associated with the computation of the volume-distributed sources, the surface s is often chosen such that it encloses flow-generating sound sources and that the volume term can be assumed small and neglected. in the current far-field noise solver, the third integral in equation 11 is not computed."
"munication is assumed to happen between the two threads. this process is driven by the write check log. for each read log entry of a thread in sub-group, sreplay could infer one of two possibilities: (a) the value is produced by a thread inside sub-group, if so, the specific thread is given; (b) the value is not produced by any thread inside sub-group. in figure 2, the question marks indicate the value matching operation."
"the method has been successfully applied to a wide range of problems, including aircraft fly-over and rotorcraft noise (for instance, see the review by brentner & farassat 32 ) . unlike these applications which involve arbitrary moving noise sources in a quiescent environment, jet flow configurations are typically considered in a fixed laboratory frame and, potentially in the presence of a uniform coflow to account for flight effects. since the distance between the noise sources (i.e., stationary surface s enclosing the jet) and the observers (i.e., far-field microphones) is fixed and time-independent, there is no doppler effect and an efficient frequency-domain permeable formulation 33, 34 can be used. the formulation and its implementation for the prediction of jet noise in cascade's massively-parallel unstructured les framework are discussed in appendix."
"a separate service monitors the amount of files being stored. when the amount of files in one task matches the amount as registered in the database, the service will mark the transfer task as a completed process."
"for some j, it means that tj did not perform any access after ei0 that is causally happened before ei1. in this case, no new causal relationship needs to be generated. therefore, condition for generating causal relationship is, vi(ei1"
"in some cases trace-back of the actual patient behind the de-identified data is required, for example in the case of incidental findings in scans made for a research study. therefore, our de-identification method, sometimes referred to as pseudonymization, uses a lookup-table which allows tracing back the original patient information. however, only necessary data are pseudonymized while the remaining fields are made anonymous. this is the reason why ctp is used within our system; besides transferring and processing the dicom data in proper manner to provide patient data security, integrity and confidentiality, it also provides simple yet flexible and extensive configuration of dicom de-identification profiles."
"using the value log, order log and the value check log, sreplay can replay the threads in sub-group without executing any other threads. the partial replay algorithm is shown in algorithm 4. in the replay phase, sreplay executes the memory accesses according to the order log. the correctness is always ensured by the value log."
"much like in the time-domain formulation with convective effects, 53-55 r represents the effective acoustic distance (rather than the geometric distance) between the source y and the observer x in terms of time delay between emission and reception, in the presence of coflow. the solution of equation 5 can be further simplified 33 by moving the green's function inside the spatial derivative operator, applying green's theorem, and using the properties of the dirac delta function. also, outside of the source region, density perturbations are expected to be small, so the term c 2 ∞ρ ′ is replaced by the acoustic pressurep ′ ."
"shadow memory is implemented as a hash map. each entry maps a key to a block of consecutive bytes. the size of the block is configurable, we choose 64-byte block. depending on the size of accessed address range, multiple blocks may be accessed for value comparison."
"the paper is intended as a review of some of the lessons learned so far on jet noise predictions with unstructured large eddy simulation (les), as well as a demonstration of the current predictive capabilities put to practice. the compressible flow solver \"charles\" developed at cascade technologies is used to conduct les of supersonic jets issued from a converging-diverging round nozzle at three operating conditions relevant to laboratory testing and tactical aircraft: isothermal ideally-expanded, heated ideally-expanded and heated over-expanded. in the numerical study, the nozzle geometry is explicitly included in the computational domain, using unstructured body-fitted mesh. noise radiation from the jet is computed using an efficient frequency-domain implementation of the ffowcs williams-hawkings equation, based on the les data collected on a permeable surface located in the jet near-field."
"user rights in accessing radtransceiver are divided into three types: superuser, managerial user and ordinary user. a superuser has unrestricted access to all features of the system. this category of users contains the administrators who are responsible to manage and supervise the whole system and take actions over unwanted activities or erroneous usage of the system. managerial users have access to the managerial features of the system to view all image processes and their status. ordinary users have limited rights, can only view and access their own initiated processes and have no access to the critical features of the system or information which belongs to other users. superusers and managerial users have the ability to create and make changes of the transfer rules while ordinary users are only able to perform image transfer based on the transfer rules that are related to their login credentials."
"five pipelines composed of a minimum of one dicom import service and either a dicom storage or export service are defined in the ctp server to provide different dicom nodes with images according to their specific needs. with the dicom export service set in the configuration, dicom images received by the ctp were stored at a different dicom node within our institution. the resulting images were all stored at the correct dicom node without any duplication occurring. the anonymizer was set only when the related transfer required de-identification. figure 6 shows the entire system. processes can be monitored by observing the status of the process in the web pages. when failures occur, the user can restart or remove the tasks using the corresponding buttons on the rightmost sisde of the task-row. figure 7 shows the finished process in the web interface. fig. 4 the web interface shows the selected images are queued and waiting to be processed. the status indicates that images are in the \"waiting\" list fig. 5 the web interface shows the selected images are being handled by the system. the status indicates that the images are being processed"
"the export and store services provided by the processor can also be utilized for image sharing between health enterprises. de-identified images can be directly uploaded using a dicom or hypertext transfer protocol (http) or stored into a specific directory of the system that will transfer images to outside the institution. therefore, this system would be adaptable for the implementation of a large multi-centre clinical trial study. the interface of the dicom distribution system was built using web technologies, therefore, wider access is possible. the de-identification profiles will be kept controlled since the transfer will be based on the existing modules in the system. however, going beyond the borders of the own institution would require proper protection of the database and table containing the relation between the deidentification id and the real patient credentials. furthermore, the system architecture and security should be considered carefully to ensure the safety of the system in delivering images with embedded patient data. for those reason, communication within our systems was still limited to one hospital only but currently in further development to enable collaborative studies involving many health institutions. in conclusion, we have devised a vendor neutral system that supports data distribution and de-identification tasks in an easy, robust and user friendly way, providing a fast process in an easy setup environment while still being able to maintain a high level of security and stability. its ability to run different de-identification processes in parallel pipelines accessible through a simple web interface is a major advantage in both clinical practice and clinical research setting."
"with regard to the knowledge relevant to the tasks, employees have personal control over it because the knowledge is acquired or created by themselves. they are also very familiar with the knowledge because they employ it every day in work settings. to acquire or create new knowledge, they also have invested much effort and time into the process. thus, individuals can easily experience ownership feelings over their knowledge relevant to their tasks; this is especially true for knowledge workers."
"territoriality was first observed and studied in animals [cit] s, scholars began to view human territoriality over physical space and objects as a means to organize people so that violence, aggression, and overt domination would become unnecessary. [cit] . the organizational perspective takes a much broader focus, no longer limited to territoriality over physical space and objects. according to this concept, individuals can experience territorial feelings and behavior over all aspects of organizational life [cit] ."
"in addition, just as the theory of reasoned action suggests that beliefs are very important to predict behavior [cit], territorial cognition may play an important role in predicting knowledge hiding. territorial cognitions include an individual's perception or belief of who should enter the territory, what goes on the territory, who should take care of the territory, or the types of activities that are allowed to take place in the territory [cit] ."
"in this study, data were collected from employees via a three-wave web-based survey. the measurement of predictor, mediator/moderator, and dependent variables were separated by a temporal lag to reduce biases (e.g. consistency motifs, illusionary correlations) that might occur in a cross-sectional study. however, if the time lag is too short, then it cannot reduce the salience of the predictor variable or accessibility in memory. if the lag is inordinately long, then it may allow contaminating factors to intervene between the measurements of the predictor and criterion variables, and subsequently could mask a relationship that really exists [cit] . thus, the present study adopted a two-month lag, which appeared to fit the criteria mentioned above, in that it is unlikely to be deemed either too short or too long."
"which can be easily computed from the code table. again, computing the scores for the candidate tags is done as by par and nar, except that code table ct is consulted for any conditional probabilities that cannot be obtained from the top-m's. in these cases, the support of a tagset x is estimated byŝ up d (x)."
"specifically, at time 1, 300 participants were asked to complete a questionnaire on knowledge-based psychological ownership and demographic variables. two hundred and sixty-eight participants finished the survey (89.3 percent response rate). about two months later, those respondents who finished the first-stage survey were asked to complete another questionnaire on territoriality and organization-based psychological ownership. two hundred and thirty-two participants finished the second-wave survey (86.6 percent response rate). about another two months later, those who responded in the previous two stages were asked to complete the third-wave questionnaire on knowledge hiding. the style of the three-wave questionnaire was the same. a separate letter stating that all information would be confidential and only used for academic purpose was also included in all of the three-wave questionnaires. in addition, in each wave two reminder e-mails were sent to those who did not respond in time. one reminder e-mail was sent one week after the questionnaire was sent, and another was sent two weeks later. finally, 190 participants finished all of the three stages of the surveys (63.3 percent total response rate). thus, 190 cases were used for analysis in the present study."
"in addition, a recent proposed territorial perspective may help to explain the underlying influencing mechanism in the relationship between knowledge-based psychological ownership and knowledge hiding [cit] . according to territoriality theory [cit], the stronger an individual's psychological ownership of an object, the greater the likelihood he or she will treat that object as his or her territory, and then protect and keep it (through defending) as his or her own. from such a viewpoint, if an individual experiences a strong feeling of ownership for the knowledge that he/she has acquired, created, controlled and invested in work, he/she will have a strong territoriality over his/her knowledge, which subsequently leads he/she to seek less interaction with others and to defend his/her knowledge territory (i.e. hiding knowledge). thus, the present study proposes that territoriality is a more proximal variable than psychological ownership for knowledge hiding. specifically, the present study proposes that territoriality will mediate the link between knowledge-based psychological ownership and knowledge hiding."
the results of the present study supported the hypothesized model of knowledge hiding based on psychological ownership theory and territoriality theory. this study can make a number of contributions to this new and emerging area.
"tagging systems also have their downsides though. apart from linguistic problems such as ambiguity and the use of different languages, the most apparent problem is that the large majority of users assign only very few tags to resources. sigurbjörnsson and van zwol [cit] did an analysis of tag usage on photo sharing platform flickr, and showed that 64% of all tagged photos are annotated with 3 or less tags. obviously, this severely limits the usability of tags for the large majority of resources."
"to overcome this problem, we propose to use the strengths of pattern selection. using an off-the-shelf pattern selection method based on the minimum description length principle, we have demonstrated that our fastar method gives a very favourable trade-off between runtime and recommendation quality."
"to construct, communicate, maintain, and restore territories, individuals may conduct marking and defending behaviors [cit] . marking involves the social construction of objects as territories, for example, public pronouncements of one's idea. defending entails that the individual takes a variety of ways to prevent or respond to territorial infringements, such as holding knowledge privately, complaining to superiors, or yelling at invaders. thus, those with a high level of territoriality are more likely to withhold knowledge than those with a low level of territoriality. knowledge-based psychological ownership impacts knowledge hiding via its effects on territoriality. hence:"
"using pattern selection, on the other hand, comes at the cost of longer offline pre-processing times. krimp requires up to 15 minutes to obtain code tables for delicious and youtube, and up to 10 hours for lastfm. however, this is mostly due to the fact that it needs to generate and test a large set of candidates, which could be avoided by using different heuristics to construct code tables [cit] . in the end, performing pattern selection once beforehand is well worth the effort if this results in much faster yet high-quality recommendation."
organization-based psychological ownership. organization-based psychological ownership was measured by using four items from van dyne and scale. an example of the items is: ''i feel a very high degree of personal ownership of this organization''. cronbach's coefficient a was 0.88.
"assuming that organization-based psychological ownership moderates the association between territoriality and knowledge hiding, it is also likely that organization-based psychological ownership will conditionally influence the strength of the indirect relationship between knowledge-based psychological ownership and knowledge hiding. since territoriality will be less strongly related to knowledge hiding when organization-based psychological ownership is higher, the following hypothesis is expected:"
"to test the hypotheses, a series of hierarchical regression models was used and examined. [cit] suggestions. [cit], four conditions are necessary for establishing mediation:"
"finally, all candidate tags are ranked according to score function s, providing the final recommendation. in the original paper [cit], the concept of promotion was introduced to weigh certain tags, which slightly improved recommendation quality. however, this requires parameter-tuning and to avoid unfair comparisons, we do not consider any tag weighing schemes in this paper. the principle of using conditional probabilities is very strong, and maybe it is for that reason that very few improvements on this technique have been proposed since (considering only methods that assume exactly the same task). we will compare to this baseline method in section 4. for this purpose, we will dub it pairar (or par for short), for pairwise association-based recommendation."
"third, the present study found that organization-based psychological ownership may weaken the link between territoriality and knowledge hiding. thus, organizations can reduce knowledge hiding by strengthening individuals' organization-based psychological ownership. there are many ways to improve individuals' organization-based psychological ownership, such as encouraging employees to participate in the organization's activities and decisions, giving employees stock ownership [cit], [cit] ) . in addition, this finding may imply that organizations can reduce individuals' knowledge hiding by promoting employees' higher-order territories (e.g. team, organization) than knowledge."
"this study also showed that organization-based psychological ownership may weaken the indirect effect of knowledge-based psychological ownership on knowledge hiding via territoriality. knowledge-based psychological ownership is an ownership of lower level targets, while organization-based psychological ownership is one of the higher-level targets. thus, the present study's findings may imply that the negative effects of lower-order target-based psychological ownership can be lessened by the positive effects of higher-order target-based psychological ownership. future research could test this proposition by investigating more higher-order ownerships' effects. for example, future research could examine whether the effects of territoriality on knowledge hiding can be inhibited by team-based psychological ownership."
"note that the code table itself can be computed offline. while computing recommendations, we only need to estimate supports. given that a code table is significantly smaller than the set of all (frequent) tagsets, this algorithm should be much faster than both nar and latre. we therefore dub it fastar (or far for short), for fast association-based recommendation."
"psychological ownership theory is very useful to explain knowledge hiding [cit] . according to this theory, psychological ownership refers to a ''state in which individuals feel as though the target of ownership or a piece of that target is 'theirs' (i.e. 'it is mine!')'' [cit] . psychological ownership is also said to exist when an individual is psychologically tied to an object and that object has become one with the individual, becoming a part of the extended self (cf. [cit] . in addition, it has been theorized that individuals can experience ownership feelings of a wide variety of targets, including material and immaterial objects or entities within the organizational context [cit] )."
"this study provides a first empirical attempt to understand when and how knowledge-based psychological ownership affects knowledge hiding. the present study found that knowledge-based psychological ownership is positively related to knowledge hiding. in addition, territoriality mediates the relationship between knowledge-based psychological ownership and knowledge hiding. moreover, the present study found that organization-based psychological ownership may weaken the link between territoriality and knowledge hiding. the present research can help academicians and practitioners to understand why and when people hide knowledge, and to sharpen their view on current knowledge transferring practice. organizations can reduce knowledge hiding by decreasing individuals' self-perception of possession of knowledge, or decreasing individuals' territoriality, or strengthening individuals' organization-based psychological ownership. future research can differentiate explicit and tacit knowledge hiding, and focus on how collective knowledge-based psychological ownership influence knowledge hiding. future research also can examine other moderators, such as team-based psychological ownership, conscientiousness, and knowledge sharing climate."
"many variants of tag recommendation have been studied in recent years [cit] . based on the used information, we split existing methods into three categories."
"to quantify computational demand we measure runtime, which reflects the time needed for the online computation of a recommendation for a single query (on average). note that this excludes the time needed for generating the associations needed by par/nar/far, as this can be done offline beforehand."
"to examine the discriminant validity of the measures, an exploratory factor analysis for knowledge hiding, knowledge-based psychological ownership, territoriality, and organization-based psychological ownership was conducted via using spss 19.0. results indicated a dominate four-factor solution based on the eigenvalues (i.e. larger than 1), with four factors explaining 79.3 percent of variance. the estimated factor pattern loadings for the model revealed that each item had large and statistically significant factor loadings on its hypothesized factor (0.81-0.93). in addition, the cross-loadings were relatively small (i.e. less than 0.35). therefore, the results suggest that these factors are measuring unique constructs."
"implementation prototypes of par, nar and far were implemented in c++. each experiment was run single-threaded on a machine with a quad-core intel xeon 3.0ghz cpu and 8gb [cit] ."
"apart from the quality of the recommendations, performance from a computational point of view is also important. tag recommendation is often implemented in online web services and thus needs to be fast. although we do not formalise this in the problem statement, we will evaluate this in the experiment section. the tag recommendation problem can be stated as follows."
"organizations also have taken many actions to facilitate knowledge sharing, but the effects cannot meet or surpass management's expectation [cit] . knowledge hiding is still prevalent in work settings today. it is reported that fortune 500 companies lose at least $31.5bn a year by failing to share knowledge [cit] . in a recent survey conducted in the usa, 76 percent of respondents admitted they once hid knowledge (cf. [cit] ) . similarly, 46 percent of respondents reported they once conducted knowledge hiding in work settings in a survey from china [cit] ."
"the development of the moderated mediation model of knowledge hiding and the empirical findings in this study provide a basic and solid foundation for future inquiry about knowledge hiding. as a start, the present study demonstrated that individuals' personal psychological ownership feeling of knowledge (i.e. ''it is mine'') is an important predictor for knowledge hiding. [cit] extended psychological ownership from individual psychological ownership to collective psychological ownership. they argued that collective psychological ownership is the collectively held sense (feeling) that this target of ownership (or a piece of that target) is collectively ''ours''. collective psychological ownership may lead to a pride in intra-group sharing (e.g. of ideas, time, and energy) because the individual will elevate the team's goals and success to a higher plane than one's own personal interests [cit] . therefore, compared to individual-level psychological ownership of knowledge, collective level psychological ownership of knowledge may have very different influences on knowledge hiding. future research should investigate empirically the relationship between collective knowledge-based psychological ownership (i.e. ''it is ours'') and knowledge hiding."
"according to self-consistency theory, people who have positive images of themselves will engage in behaviors, possess attitudes and choose roles that reinforce that positive image. [cit] found that mentors who have a strong sense of obse would be confident in their organizational role and more likely to mentor others. [cit] also observed that individuals with high obse will be motivated to engage in the mentoring role, as it will provide them with the opportunity to demonstrate their organizational competence. [cit] found that obse is positively related to public (other serving) and negatively related to private (self-serving) motive. thus, when employees feel the organization is their personal psychological property, they will put forth their efforts to those behaviors that will benefit their organizations so that they can maintain and/or enhance their self-image. since territoriality is mostly driven by a self-serving motive and may do harm to organizations, employees with a high ownership feeling for the organization will in turn inhibit the negative effects of territoriality via obse. thus, the relationship between territoriality and knowledge hiding will be weakened by organization-based psychological ownership. therefore:"
'' one way that organizations can reduce knowledge hiding is to focus on management practices that decrease an individual's self-perception of possession of knowledge. ''
"third, the present study found that organization-based psychological ownership could weaken the indirect effect of knowledge-based psychological ownership on knowledge hiding via territoriality. this result describes when knowledge-based psychological ownership will lead to knowledge hiding via territoriality. when individuals feel a strong psychological ownership feeling for the organization, the indirect effect of knowledge-based psychological ownership on knowledge hiding will be not significant. this study therefore extends the existing finding that organization-based psychological ownership is negatively related to knowledge hiding [cit] to a more confounding effect that organization-based psychological ownership may also moderate territoriality-knowledge hiding link."
"in practice, query i consists of very few tags in the large majority of cases, since most users do not manually specify many tags. therefore, high-quality tag recommendation is of uttermost importance for resources annotated with 3 or fewer tags. all the more so, since these input sizes potentially benefit most from recommendation: when more than 3 tags are given, recommendation is probably less necessary."
"in section 2, we will first provide the notation that we will use throughout the paper, and formally state the recommendation problem that we consider. after that, section 3 will explain association-based recommendation in more detail. first, the above mentioned method using pairwise associations will be detailed. second, both our own generalisation to larger associations and latre will be discussed. third, we will introduce fastar, for fast association-based recommendation, which needs only few associations to achieve high accuracies, making it much faster than its competitors. for this, fastar uses associations selected by the krimp algorithm [cit], a pattern selection method based on the minimum description length principle. given a database and a large set of patterns, it returns a set of patterns that together compresses the database well. these so-called code tables have been shown to provide very accurate descriptions of the data, which can be used for e.g. tag grouping [cit] ."
"after all methods have been introduced, they will be empirically compared in section 4. finally, we round up with related work and conclusions in sections 5 and 6."
"the overall good performance of pairar shows that the idea of summing pairwise conditional probabilities is hard to beat. it very much depends on the data table 2 . results. for each combination of dataset, query size k, method, and minsup, the obtained precisions, mrr and runtime are given. for naivear and fastar, the number of tagsets in f resp. ct are given. runtime is given per query, on average in milliseconds. at hand whether associations consisting of multiple tags can be exploited to improve recommendation quality. this is not the case for the delicious dataset that we used, which is probably due to the relative large number of tags, of which many occur only very rarely. for the youtube data, however, recommendation can be substantially improved using fastar. that is, for this dataset our method based on pattern selection beats the more 'exhaustive' methods with respect to both precision and runtime. whenever nar and/or latnc achieve higher precisions than par, far provides a much better trade-off between precision and runtime. the downside is that it does not always give the highest possible precision. mining all association rules on demand, such as latnc does, does not seem to be a good idea for realistically sized datasets, as this comes at the cost of long runtimes."
"although knowledge hiding may have positive intentions or outcomes in some contexts (e.g. it may be intended to protect the other party's feelings; cf. [cit] ), it is usually a negative perspective on an individual's knowledge contribution in most work settings. knowledge sharing, in contrast, is a positive one. just like counterproductive work behavior is different from organizational citizenship behavior [cit], knowledge hiding is not opposed to knowledge sharing [cit] . they are related but distinct constructs. knowledge hiding may have a different psychological emerging mechanism. even when antecedents are shared by both knowledge hiding and knowledge sharing, the underlying mechanism and effects of the antecedents are likely to be very different [cit] have identified several antecedents of knowledge hiding (e.g. perception of distrust, complexity of knowledge, task-relatedness of knowledge, and knowledge sharing climate). however, scholars still know little about the psychological mechanism of knowledge hiding. the present study will extend this growing literature by examining how and when knowledge-based psychological ownership affects knowledge hiding."
"conditional probabilities based on pairwise associations allow for high-quality tag recommendation, and this can be further improved by exploiting associations between more than two tags. unfortunately, doing this naïvely is infeasible in realistic settings, due to the enormous amounts of associations in tag data."
"online collaborative tagging platforms allow users to store, share and discover resources to which tags can be freely assigned. well-known examples include delicious (bookmarks), flickr (photos), lastfm (music), and youtube (videos). tags can be chosen completely freely, allowing both generic and very specific tags to be assigned. as a result, tags enable very powerful tools for e.g. clustering [cit] and indexing [cit] of resources."
"since there is some overlap between defending behavior and knowledge hiding, the present study emphasized the cognitive aspects of territoriality rather than its behavioral aspects."
"h3a. organization-based psychological ownership moderates the relationship between territoriality and knowledge hiding. specifically, organization-based psychological ownership will weaken the link between territoriality and knowledge hiding."
"several studies [cit], 2001) have linked psychological ownership with knowledge sharing and proposed that psychological ownership has a positive effect on knowledge sharing. these studies suggested that when employees believe they own information they are more likely to engage in knowledge sharing, because the sharing of what they possess makes them feel needed, wanted and appreciated. as a means of self-expression and to show self-consistency, sharing expertise might have personal benefits, such as pride, increased personal identification with co-workers or the organization, more respect from others and a better reputation, and reduced alienation or stronger feelings of commitment [cit] . thus, they suggested that the individual who believes knowledge belongs to oneself would be more likely to share knowledge. in their reasoning, self-enhancement motivation mechanism is thought as a good mediator to unfold the effect of knowledge-based psychological ownership on knowledge sharing. therefore, the above analyses may remind us that knowledge-based psychological ownership may affect knowledge hiding and knowledge sharing simultaneously, but via a very different mechanism. that is, it affects knowledge hiding via self-protection motivation (i.e. territoriality) and influences knowledge sharing via self-enhancement motivation. thus, future research could investigate these two mechanisms simultaneously in one study."
"fastar uses krimp, an existing pattern selection technique, to pick a small set of tagsets. however, the used coding scheme is not specifically designed for selecting associations that are useful for recommendation. despite this, the presented results are encouraging. by modifying the selection process to better reflect the needs of tag recommendation, e.g. by allowing overlapping tagsets, we believe that the results can be further improved."
"second, the present study found that territoriality mediates the link between knowledge-based psychological ownership and knowledge hiding. thus, organizations can reduce knowledge hiding by taking measures (e.g. adopting open work spaces) to decrease individuals' territoriality."
"finally, the scale used to measure knowledge hiding in this study consisted of just three items. although this scale is adapted from a validated scale, there is a potential threat that the construct may be under-represented because of the lack of a large number of items [cit] . however, the number of items to include depends on the length of the testing time. since the testing time is very limited for employed workers, a short scale is preferable. however, future research is still encouraged to replicate the present work using another scale to validate the findings of this study further."
"finally, the present study only investigated the moderating effects of organization-based psychological ownership. other individual and organizational variables may also moderate the link between territoriality and knowledge hiding. for example, conscientiousness may weaken the relationship since conscientiousness is a consistent predictor of the propensity to exert effort [cit] and withhold counterproductive work behavior [cit] . organizational knowledge sharing climate may also inhibit the negative effect of territoriality since in a strong such climate territoriality will be thought as unacceptable and those who transgress organizational norms are often sanctioned. hence, future research could investigate these potential individual and organizational moderators in the link between territoriality and knowledge hiding."
"as a solution to this problem, methods for a wide range of tag recommendation tasks have been proposed. in this paper we consider the most generic tag recommendation task possible. that is, we only assume a binary relation between resources and tags. because it is so generic, it can be applied in many instances, e.g. on platforms where heterogeneous resource types co-exist, making it impossible to use resource-specific methods. also, many collaborative tagging platforms maintain only a single set of tags per resource, irrespective of which user assigned which tag."
"the second category aims at tagging systems that allow users to individually assign their own tagset to each resource. the resulting ternary relation is often called a folksonomy. one of the first methods for folksonomies was folkrank [cit], based on the same principles as pagerank, and outperforms baseline approaches like collaborative filtering [cit] . also, a system composed of several recommenders that adapts to new posts and tunes its own parameters was proposed [cit] ."
"ownership can symbolize the self and show core value [cit] . it is common for people to experience psychologically the connection between the self and various targets of possessions [cit] . james (1890) also argued there is a fine line between ''me'' and ''mine''. thus, it is reasoned that psychologically owned target becomes part of the ''extended self'' [cit] wrote, ''what is mine is myself'' (p. 592). therefore, individuals who experience strong psychological ownership for the organization will find themselves present in it. individuals who are high in organization-based psychological ownership will also have positive self-assessments of themselves as organizational members, as there is value and/or importance attached to that ownership relationship. that is, they will come to believe that they are significant, worthy, and valuable to the organization, and subsequently form a strong feeling of organizational identification and obse [cit] )."
"the first attribute indicates that territoriality originates from psychological ownership. that is, psychological ownership is the psychological foundation of territoriality [cit] . psychological ownership over objects or entities inherently implies that individuals attach themselves to these objects and entities. the second attribute indicates that territoriality toward an object or social entity will lead to the social construction of the territory, such as signaling and defending it from others. consequently, territoriality provides valuable insights for disentangling the link between psychological ownership and employee behaviors."
1. the independent variable must be significantly associated with the mediator; 2. the independent variable must be significantly associated with the dependent variable; 3. the mediator and dependent variable must be significantly associated; and 4. the relationship between the independent variable and dependent variable should be non-significant or weaker when the mediator is entered.
"building on the above facts, researchers have recently begun to propose that knowledge hiding and knowledge sharing are not the opposite ends of a continuum, but are two distinct constructs [cit] . the two constructs may have very different antecedents. even though they may have similar behavioral manifestations, the underlying motivation and mechanisms are strikingly different [cit] . field observations also show that employees may conduct knowledge sharing and knowledge hiding simultaneously [cit] . they may share with colleagues some knowledge that is explicit and unimportant, but withhold other knowledge that is tacit and vital. thus, as a prerequisite of facilitating knowledge transferring in organizations, researchers and practitioners need to understand why and when people hide knowledge in work settings [cit] ."
"in an increasingly complex, knowledge-based and turbulent economy, the competitive advantages of more and more organizations depend on effective knowledge management. to gain and sustain competitive advantages, organizations need to develop systematic processes to create and leverage knowledge. one of the main tasks in knowledge management is taking steps to stop individuals hiding knowledge and letting them share their knowledge or information within their organizations."
"that is, we are looking for a pattern set that provides a complete description of (the associations contained in) source database d. in previous work [cit] we have shown that code tables, such as produced by e.g. the heuristic algorithm krimp [cit], provide such descriptions."
"individuals who experience knowledge-based psychological ownership will experience a strong attachment to the knowledge. the ideas, knowledge, and information created and held by individuals tend to be a particular extension of their selves. the products of the mind are intimately known by their creator and holder. thus, the ideas, knowledge, and information that the individual creates and holds are likely to be treated as his or her territories. those who are high in knowledge-based psychological ownership will be more likely to experience a sense of territoriality."
"2. emotion; and 3. behavior. [cit] emphasized the motivation aspects of human territoriality and defined it as the impetus of humans to establish permanent or temporary control over territory. [cit] focused on the behavioral aspects and defined it as an individual's behavioral expression of his or her feelings of ownership toward a physical or social object. they argued that territoriality includes behaviors for constructing, communicating, maintaining, and restoring territories around those objects in the organization toward which one feels proprietary attachment. they identified two kinds of territorial behavior:"
"second, although this study mitigated common method variance [cit] by collecting data from employees three times, it still had some risks since all the information was reported by the same respondents. however, it should be noted that self-report is an appropriate way to measure these variables. [cit] argued, self-report may be quite useful in providing a picture of how people feel. therefore, self-report is appropriate for measuring psychological ownership and territoriality. in terms of knowledge hiding, self-report may be the best way of assessment, because typically only the informer could know if knowledge hiding occurred. peers or recipients of knowledge can only report on the knowledge that is shared [cit] . in addition, this study has tried to minimize the costs of self-report via assuring the participants of anonymity. however, future research is still encouraged to replicate the present work through the employment of an experimental design to validate the findings of this study further."
"moreover, the present study proposes that individuals' psychological ownership for the organization will weaken the effects of territoriality. employees with a strong psychological ownership feeling for the organization will come to believe that they are significant, worthy, and valuable to the organization, and subsequently form a strong organization-based self-esteem (obse; [cit] ) . according to self-consistency theory, employees with high obse will put their efforts into those behaviors that will benefit their '' few studies have investigated the determinants of knowledge withholding or knowledge hiding. '' organizations so that they can maintain and/or enhance their self-image. since territoriality may do harm to organizations, employees with a high ownership feeling for the organization will in turn inhibit the negative effects of territoriality. thus, it can be argued that the relationship between territoriality and knowledge hiding will be weakened by organization-based psychological ownership. in sum, the present study aims to build and test a moderated mediation model of knowledge hiding."
"understanding how and when knowledge-based psychological ownership affects knowledge hiding has some practical implications. first, the present study found that knowledge-based psychological ownership is positively related to knowledge hiding. thus, one way that organizations can reduce knowledge hiding is to focus on management practices that decrease an individual's self-perception of possession of knowledge. for example, the organization can reduce individuals' knowledge-based psychological ownership by adopting some knowledge management tools, promoting teamwork, stressing the collective ownership of knowledge, and advancing individuals' organizational commitment."
"analogue to naivear, the set of tagsets provided by the first column of a code table is augmented with the top-m pairwise co-occurrences for each input tag. also, the set of candidate tags is obtained in exactly the same way: each tag that co-occurs with any of the input tags in the code table is considered a candidate tag, as are the top-m tags for each input tag."
"the descriptive statistics and correlations for all variables are presented in table i . results showed that age, gender, and organizational tenure were not related to territoriality and knowledge hiding. however, manager was negatively related to territoriality and knowledge hiding. technologist was positively related to knowledge hiding."
"h3b. organization-based psychological ownership will moderate the positive and indirect effect of knowledge-based psychological ownership on knowledge hiding (through territoriality). specifically, territoriality will mediate the indirect effect (of knowledge-based psychological ownership on knowledge hiding) when organization-based psychological ownership is low but not when it is high."
"the big problem, however, is that although 'on-demand' mining circumvents the need to cache millions of associations, the online mining of association rules is computationally very demanding. as such, latre provides better recommendations at the cost of scalability."
"there are different definitions regarding territoriality. [cit] defined it as an intertwined system of emotions, beliefs, and behaviors that are very place-specific, socially and culturally influenced, and linked to person-place transactions dealing with issues of setting management, maintenance, and expressiveness. thus, territoriality is believed to consist of three dimensions:"
"although experiments will show that naivear and latre can improve on pairar in terms of precision, the downside is that both methods are rather slow. unfortunately, most users would rather skip recommendations than wait for them. the question is therefore whether a small set of associations could already improve precision, such that recommendation is still (almost) instant."
"the first category is the most generic, and the one we consider in this paper: methods that only assume a binary relation between resources and tags. we already introduced and experimented with the methods in this category [cit] ."
"as an important entity of individuals' organizational life, the organization can also be experienced by individuals as an important target of ownership. individuals who have a strong ownership feeling for the organization will feel possessiveness and psychologically attached to their organization. the present study proposes that psychological ownership for the organization will moderate the territoriality-knowledge hiding association. the rationale here is that organization-based psychological ownership is causally and positively associated with organization-based self-esteem (obse), through which it inhibits effects of territoriality."
"1. add a new function to \"chartr-modelspec.c\" with the parameters needed to be estimated for the model. specify the model in c code, following the structure of the pseudo-code example given in algorithm 1. provide the new model with a unique name (i.e., not shared with any other models in the toolbox), preferably using the convention defined in table 2 . 2. add any new parameters of the model to the function makeparamlist, and to the paramsandlims function in script \"chartr-helperfunctions.r\". 3. add the name of the model to the function returnlistofmodels, in script \"chartr-helperfunctions.r\". 4. make sure additional parameters are passed to the functions diffusionc and getpreds, in scripts \"chartr-helperfunctions.r\" and \"chartr-fitroutines.r\", respectively. 5. finally, specify in function diffusionc the code for generating choices and rts to use for model fitting. for example, the code for generating the choices and rts for ddmsvs z s t is shown in listing 3. listing 3. r code for simulating choices and rts for the model ddmsvs z s t ."
"where b is the intercept of the urgency signal and m is the slope. as with the ddms described above, urgency signal models can incorporate any combination of variability in starting state, drift rate and non-decision time, giving rise to a family of different decision-making models. we also allow for the possibility of variability across decisions in the intercept term of the linear urgency signal,"
"third, a little knowledge of the different technical basis on which models are built can be helpful. the multitude of different modelling techniques can often appear overwhelming; we have offered a simple introduction to some of these, explaining the various questions they can answer, and outlining their strengths and weaknesses."
"computational models are reaching into domains beyond those where they have been traditionally applied (the physical and life sciences and engineering); they are being used for new purposes; and their complexity means that they have different properties from simpler models (such as those which can be completely checked using analytic methods). this extension has the potential for new application and utility across many aspects of our collective life, but it also means there is a greater potential for their misuse: misleading as to the current state of what is modelled and informing decisions where they are not suited. hopefully this paper will help educate all relevant stakeholders as to these opportunities and dangers, and thus help make these tools a positive force in the new areas in which they are being applied. this paper distinguishes some of the different purposes for a model: this has a significant impact on how the model should be developed, checked and used. it gives an overview of some of the different technical bases, to provide some understanding of their nature and properties. it also looks at some of the future directions in which modelling is developing. it includes two checklists, aimed at the full range of stakeholders: to help people ask the right questions of models and modellers and hence improve the whole modelling process. this paper is a condensation of the recent blackett review computational modelling: technological futures [cit] that was initiated by government office for science and the prime minister's council for science and technology. it is organized into five sections covering: where models are used, why model, making and using models, types of model and analysis and future directions. appendix a contains two checklists: making and using models and what users should ask about a model."
"as with the collapsing boundaries, the urgency signal can take many functional forms; we have implemented two such forms in chartr. the general implementation of the urgency signal is"
"l 1 adaptive pd augmentation due to the physical correlation between heading angle and turning rate, the architecture of the l 1 adaptive controller is based only on the turning rate dynamics. actually it is only the turning rate dynamics that can be uncertain, and that is directly affected by environmental disturbances, as shown in (1)-(2). hence the yaw dynamics (12)- (13) is reduced to"
"the methods are focused on the specification of various mathematical models of decision-making, and the parameter estimation and model selection processes. for reference, the symbols we use to describe the models are shown in table 1 . the naming convention for the models we have developed in chartr is to take the main architectural feature table 1 list of symbols used in the decision-making models implemented in chartr."
"in a very short period of time, the life sciences have become data-centric endeavors 1 . at the avantgarde of this trend, the omics sciences benefit from simultaneous rapid advances in computational systems and data acquisition technologies and now face data management challenges that go well beyond wet lab practice 2 . biological data in the omics sciences is usually curated by specially assigned professional scientists in a process often known as biocuration. it has been described as \"the activity of organizing, representing and making biological information accessible\" 3 to biologists. it is becoming a key task, given that expert-curated web-accessible databases are one of the main driving forces in current research in biology in general and bioinformatics in particular 4 . the responsibilities of curators may include data collection; consistency, incompleteness 5 and accuracy control; annotation using widely accepted nomenclatures; or evaluation of computational analysis, amongst others. biocuration requires broad expertise in the domain because of the vast amount of heterogeneous information available from literature, often lacking a unified and standardized approach for the representation and analysis of data. this often involves a previously unforeseen forefront role for text mining methods 6 . one of the challenges of biocuration is the unambiguous identification of biological entities from existing studies and literature. data trustworthiness can only be ensured through costly data management 7 . this task, when understood as \"manual\" expert curation, is uncertain and error-prone due to the complexity of the information involved, so that the development of computational procedures to assist experts in it is worth pursuing."
"according to the results in tables 2 and 3, [cit] dataset according to all the evaluation measures considered. [cit] version although, in this case, differences are comparatively minor."
"the most prominent sequential sampling model of decision-making is the diffusion decision model (ddm), which has an impressive history of success in explaining the behavior of human and animal observers (e.g., [cit],b) . however, recent studies propose alternative sequential sampling models that do not involve the integration of sensory evidence over time. instead, novel sensory evidence is multiplied by an urgency signal that increases with elapsed https://doi.org/10. 1016/j.jneumeth.2019.108432 decision time, and a decision is made when the signal exceeds the criterion [cit] . another line of research proposes that observers aim to maximize their reward rate and suggests that the decision boundary dynamically decreases as the time spent making a decision grows longer. such a framework has been argued to provide a better explanation for decisionmaking behavior in the face of sensory uncertainty [cit] ."
"for models that are intended to explain or predict the outcomes of processes that take place over time, we usually need data that have been collected over a period (referred to as time-series data, or longitudinal data). however, such data are often difficult to obtain, not least because of the time it takes to gather the dataset, but also because definitions may have changed in the intervening period, making data points measured at different times not strictly comparable. also, if one is using data collected at two points in time from the same individual or organization, one must consider the effects of those who stop participating during the data collection period, which may lead to a biased sample."
"classification performance metrics. several metrics were used for the evaluation of the classifiers' performance. at the subtype level, i.e. for the binary classifiers of each subtype, the performance was evaluated using the precision (as measure of quality), the recall (as measure of completeness), the matthew correlation coefficient (mcc) and the f-measure. the latter two are considered more robust evaluation measures as mcc takes into account all elements of the confusion matrix making it suitable for unbalanced datasets 53, while the f-measure involves only precision and recall. all these metrics are based on the concept of true and false predictions in binary classification according to 54 . the mcc, defined as correlation coefficient between the observed and the predicted classification, ranges from −1 to 1, where 1 corresponds to a perfect classification, 0 to a random classification and −1 to complete misclassification. the f-measure being the harmonic mean of precision and recall ranges from 0 (describing complete misclassification) to 1 (perfect classification). for the multi-class classifiers, at the global level, we report the accuracy (accu), which is the ratio of correctly classified sequence to their total number, but also the mcc and the f-measure as explained in 54, 55 . the reported measures are the mean values of the respective metrics over the five iterations of the 5-fold cv used to evaluate the classifiers."
"this is a single use case, but it represents a wide array of related ones. open source and open knowledge projects are full of decision making discussions available widely in textual form. rhetorical studies of them so far take place on a qualitative, discursive level. examples include dissent and rhetorical devices in bug reporting [cit] and how python listservs select enhancement proposals [cit] . interestingly, the role of a participant in the python community is related to the kinds of message they quote (syntheses, disagreements, proposals, or agreements), and syntheses and disagreements are the most quoted. the organizational relevance of these open decision making discussions in collaborative communities makes them a promising target for support, and argumentation mining technology is an appropriate tool to deploy towards that end."
"listing 2. the required raw data format for parameter estimation in chartr. the raw data are converted in \"chartr-processrawdata.r\" to generate 9 quantiles (10 bins) of correct and error rts to be used in the parameter estimation process. it also stores the data as a r list named dat, which includes four fields: n, p, q, pb."
"the gpcrdb 21, 28 is a curated and publicly accessible repository of gpcr databases and web tools for the analysis of membrane proteins including about 400 human specimens. overall, the gpcrdb dataset contains 14,951 proteins from 3,184 species."
"using a rbf kernel the svm needs to adjust two parameters, the error penalty parameter c and the γ coefficient, through grid search. our problem involves multi-class classification for which a \"one-against-one\" approach is used and implemented in the libsvm library 52 . naive bayes. nb 41 is a simpler model that provides a baseline for comparison. it is a probabilistic classifier which applies bayes' theorem with an assumption of independence of variables. under this assumption the probability of a class given the input data is expressed as"
"although aic and bic are provided and easily computed in chartr, their use for discriminating between models requires careful consideration from the researcher. our perspective is influenced by an excellent paper that describes the worldviews for the two metrics [cit] . ultimately, whether aic or bic are used depends on the goals of the researcher."
the availability of only the motion data gathered during full scale sea trials obviously precludes the possibility of identifying the coefficients of the matrices m and n(u 0 ). therefore the identification of the low-speed reversing dynamics relies on linear output models parameterised in terms of time constants and gains. hence the action of environmental forces/moments τ e is modelled as output disturbances.
"the models chartr ranked 2nd to 6th were also sensibly related to the data-generating model; they all assumed the decision-formation process was influenced by factors other than sensory evidence, such as growing impatience or other variants of the urgency gating model. the second case study reaffirms our conclusion from the first case study that model selection may not be put to best use when arguing for a single \"best\" model in an absolute sense. this is especially true when the datagenerating model is not decisively recovered from data."
"sequential sampling models of decision-making assume that rt comprises two components [cit] . the first component is the decision time, which encompasses processes such as the accumulation of sensory evidence and additional decision-related factors such as urgency. the second component is the non-decision time (or residual time), which involves the time required for processes that must occur to produce a response but fall outside of the decision-formation process, such as stimulus encoding, motor preparation and motor execution time."
"where i denotes an experimental condition; j denotes an exemplar trial; denotes the uniform distribution. chartr provides flexibility to the user such that they can assume the decision-formation process involves none, some or all of these random components. furthermore, it provides flexibility to assume distributions for the random components beyond those that have been typically assumed and studied in the literature. for example, one could hypothesize that non-decision times are exponentially distributed rather than uniformly distributed [cit] . (e) ugm with variable drift rate (s v ) and variable non decision time (s t ). in the standard ugm, the urgency signal is only thought to depend on time and thus starts at 0. the sensory evidence is passed through a low pass filter (typically a 100-250 [cit] . the sensory evidence is then multiplied by the urgency signal to produce a decision variable that is compared to the decision boundaries. (f) schematic of urgency signals with an intercept (top panel) and a variable intercept (bottom panel)."
"for each model and hypothetical participant, we repeated the parameter estimation procedure 5 times, independently. [cit] ditterich, 2006aditterich (2006a, dddms) [cit] ditterich, 2006aditterich (2006a [cit] ditterich (2006a [cit] ditterich (2006a [cit] ditterich (2006a [cit] ditterich (2006a . gray points denote data. lines are drawn for visualization purposes."
random forest. rf 36 is an ensemble learning method 37 using decision tree (dt)-based classifiers. the dt classifiers are trained to split an input space into homogeneous regions with associated class labels. the splits are typically axis-aligned and are selected to maximize the information gain.
"an important aspect of documenting and maintaining a model is to ensure that it is properly preserved for later access, regardless of institutional and personnel changes and the evolution of computing infrastructure. one increasingly popular solution is to make the"
"for reference, the models being considered, the parameters for the models and the number of parameters in each model are shown in table 2 ."
"chartr also provides functionality to transform the model selection metrics into model weights, which account for uncertainty in the model selection procedure and aid interpretation by transformation to the probability scale. the weight w for model i, w m ( ) i, relative to a set of m models, is given by"
"it is important to make sure that a model is dealing with the right issue and helping to ask the right question. even a high-quality model will not be helpful if it relates to an issue that is not the main concern of the user. conversely, asking a model to answer more and more detailed questions can be counterproductive, because it would require ever more features of the real system to be included in the model. in other words, models need to be 'requisite'-they must have an identified context and purpose, with a well-understood knowledge base, users and audience, and possibly developed within a particular time constraint [cit] ."
"these 5 steps are repeated for each model and each participant under consideration. in the next few sections, we elaborate on each of the steps with examples to illustrate their implementation in chartr. we note that use of chartr requires a basic knowledge of r programming, and if one wishes to design and test a new decision-making model then also c programming. owing to the many excellent online resources for both languages (a simple search of \"r program tutorial\" will return many helpful results), we do not provide a tutorial for either language fig. 3 . chartr flow chart. models are specified and once data is available, the parameters are estimated through the optimization procedure. once parameter estimation is complete, the final goodness of fit statistic is calculated for every model under consideration, which is used for subsequent model selection analyses."
"in our work, we have analyzed a corpus of debates, to understand how the english-language version of wikipedia makes decisions about which articles to include and exclude from the encyclopedia. we used two approaches to argumentation theory to annotate asynchronous messages in each debate, in iterative multiparty annotation experiments [cit] ."
"once the best-fitting parameters have been estimated from a set of candidate models, the final step is to use this information to guide inference about the relative plausibility of each of the models given the data. many different levels of questions can be asked of these models. [cit] ."
"playing with models in a creative but informal manner can provide new insights. here, the model is used as an aid to thinking, and can be very powerful in this regard. however, the danger is that people confuse a useful way of thinking about things with something that is true. if the purpose of a model is unclear or confused, this can lead to misunderstandings or errors. to give two examples, a theoretical model might be assumed to be a good way of thinking about a system, even though this might be crucially misleading, or a model that helps establish a good explanation be relied upon as a predictive model. making clear the purpose of a model is good practice and helps others know how to judge it and what it might be reliable for."
"all too frequently, one does not discover exactly what data one needs until the model has been built, so it often becomes an iterative process of finding data and developing the model. however, there are a few helpful distinctions to be made that will enable a model commissioner to ask model developers the right questions. the first distinction is between the data needed to specify and build the model; the data that will be used to check the model's output; and the data needed for day-to-day use of the model. the second distinction concerns the levels at which the model operates: the micro-level, describing how the smallest components of the model behave (for example, the cars in a traffic model); the meso-level, describing how the components are linked together (for example, the road layouts); and the macro-level, covering the properties of the system as a whole (for example, the funding for new road infrastructure). the micro-level may be determined by the science behind the model, by qualitative evidence, or by 'big data' analyses. the meso-level might reflect the structure of the system. and the macro-level may include data such as aggregate statistics over a long period of time. sometimes it is acceptable to use closely related proxies for these data."
"each frame (or frames) may require a different type of model and analysis, and all kinds of framing demand judgements about the scales to be adopted, from the coarse to the fine-grained. a model developed to address one frame of reference may not be suitable for another frame and can be positively misleading if this is attempted. for example, using a costing model for rail ticket sales to assess the order in which to upgrade signals or the impact of lengthening trains by adding carriages could give very misleading results. this is because the costing model would not include details of how signals depend on each other, or the loads that rails are designed to withstand. it is thus helpful to make these frames of reference explicit when developing or commissioning models."
"the default chartr qp plot displays 5 quantiles of the rt distribution: 0.1, 0.3, 0.5, 0.7 and 0.9 (sometimes also referred to as five percentiles: 10th, 30th, 50th, 70th, 90th). the .1 quantile summarizes the leading edge of the rt distribution, the 0.5 quantile (median) summarizes the central tendency of the rt distribution, and the 0.9 quantile summarizes the tail of the rt distribution. the goal of visualization with qp plots, or other forms of visualization, is to enable comparison of the descriptive adequacy of a model's predictions relative to the observed data."
"in chartr, we provide functions for converting the raw qmp statistic that approximates the likelihood. the likelihood is a goodness-offit statistic that can be combined with penalized model comparison metrics. this could entail comparison between two models at multiple levels of granularity. for instance, the question could be \"which of the models considered provides the better description of the data\", or \"is a ddm with variable baseline better than a ddm without a variable baseline\". it could also be used to compare between a model with collapsing boundaries and a model with drift-rate variability [cit] ) or between models with different forms of collapsing boundaries [cit] ) . all of these questions can be answered using chartr. as a guide, we provide illustrations of model selection analyses using chartr in two case studies presented in section 3.4. we also apply the model selection analyses to the behavior of monkeys performing a decision-making task [cit] ."
"such an approach may also be particularly important when cross validation is used instead of model selection metrics. given the necessity to run the model fitting code many times with different random seeds to avoid local minima, even a 10 fold cross validation would lead to enormous numbers of model fitting runs. for instance for five repeats and 10 fold cross validation, it would take nearly 50 such repeats which might be very time consuming if the researcher wishes to test every single model in this process. using the model selection metrics to pare down to the most likely set of models and then pursuing cross validation and different hyperparameter settings is likely advisable. we leave the judicious choice of these settings to the users of chartr. in all panels, np refers to the number of particles used in the differential evolution algorithm, n is the number of monte carlo replicates used for simulation of each of these models. higher values of n provide more precision but require more time."
"whose fit to the data set in identification is 92.3%, and in validation is 85.4%. figure 2 shows the comparison between the full scale data and the surge model m u,g for the identification and validation data sets."
"besides these patterns, we found statistically significant differences between how experts and novices in the community argued in our corpus of debates. experts were more likely to use argument from precedent, while novices (who had little experience in the debates and in the wider wikipedia community) were more likely to use several argumentation schemes that the community viewed as less sound bases for decision making. 2 these included argumentation from values, argumentation from cause to effect, and argument from analogy."
"in this paper, we illustrate this using a specific example of how machine learning methods can be used to assist the curation of a protein database. this example involves a web-accessible and publicly-available database of g protein-coupled receptors (gpcrs). these are membrane proteins responsible for numerous physiological responses by transducing the signals embodied in the chemical structure of hormones, neurotransmitters and synthetic ligands and also the energy encapsulated in light photons from outside to inside the cells 8 . gpcrs are . the first study that aimed to represent the overall map of the gpcrs in a single mammalian genome classified the human gpcrs in five main families or classes by phylogenetic analysis. these classes were termed glutamate, rhodopsin, adhesion, frizzled/taste2, and secretin (hence the grafs classification system) 11 . the glutamate family (also known as class c gpcrs), which is the subject of the present study, included the following receptor subtypes: eight metabotropic glutamate receptors, a gaba b receptor heteromer composed of two subunits, a single calcium-sensing receptor, and five receptors that were believed to be taste receptors 11 . in a subsequent study, which was focused in class c gpcrs and performed in human (22 sequences), mouse (79), fugu (30), and zebrafish (32) genomes, as well as in four invertebrate species, four main phylogenetic groups divided in eight subgroups were found 12 . namely, group i: v2r (pheromone receptor), tas1r (sweet taste receptor), gprc6a, and casr (calcium-sensing receptor); group ii: grm (mglu receptors), group iii: gaba b together with gpr158 and gpr158l and group iv: gprc5 12 ."
our experiments involve the supervised classification of transformed versions of unaligned 39 primary amino acid (aa) sequences of class c gpcrs. transformations are required to achieve fixed length sequence representations.
"the contribution of this paper is to bring together current thinking about, and experiences with, computational modelling. it does not reveal new research or results, but rather aims to serve as a guide for all those involved in modelling. it is of direct interest to a range of potential stakeholders for modelling: commissioners, owners, developers and users, but it is also important for those who may be affected by the insights that come from these models in the public, private, academic and not-for-profit sectors."
we use a systematic three step approach to analyze the svm classifier models built on the transformed dataset in order to assess the kind of classification error:
"finally, we could envision argumentation mining being used to summarize the debate. macroargumentation, such as the factors analysis described above, would be a natural choice for summarization, as it has already proven useful for filtering discussions. a more reasoning-intensive approach would be to calculate consistent outcomes [cit], if debates can be easily formalized."
"in the three other hypothetical observers that we simulated, the pattern of results returned by chartr was consistent with the results shown for the two hypothetical observers in fig. 6 : ddmsvs t was chosen as the best fitting model for all observers by aic. if we assume the set of observers are independent, which is true in the case of our hypothetical example and usually in experiments, we can average the individual-participant posterior model probabilities to obtain a grouplevel estimate. as shown in fig. 6f, across the set of observers ddmsvs t is identified as the most plausible model for the data, indicating reasonably good model recovery; the next-best models are the same as those described earlier. the results from bic were again consistent, preferring the ddms t model over ddmsvs t for this group of hypothetical observers. fig. 7 shows qp plots of the data from two hypothetical observers overlaid on the predictions from a range of models. the simple ddm predicted greater variance than was observed in data, and therefore provided a poor account of the data. when the ddm is augmented with s t and both sv and s t, it provided a much improved account of the data, capturing most of the rt quantiles and the accuracy patterns. three other models provided an almost-equivalent account of the data in terms of log-likelihoods (ddmsvs z s t, cfkddmsvs t, dddmsvs t ), but they did so with the use of more model parameters than ddmsvs t and ddms t . this led to a larger complexity penalty for those models and fig. 8 . model selection and parameter estimation outcomes from applying a range of cognitive models of decision-making to data from hypothetical observers (case study 2). decision-making in these hypothetical observers is controlled by the model bugmsv. (a) aic values as a function of model with the ddm model as the reference for one hypothetical observer, subj 3. color conventions as in fig. 6a. (b) bic values as a function of model with the ddm model as the reference for the same subject shown in a. (c) akaike weights and posterior model probabilities for the top six models that provided the best account of subj 3's behavior. (d) results for another hypothetical subject. (e) results for the population of hypothetical subjects. the most probable model for this set of hypothetical observers is the generative model, bugmsv. however, we note that other models such as bugm, ugmsv, and uddmsv provide quite good descriptions of the behavior. this result is in keeping with the general notion that model selection ought to be used as a guide to the most likely models and not necessarily to argue for a \"best\" model. (for interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) thus larger aics and bics in comparison to the ddmsvs t model, as shown in the model selection analysis in fig. 6 ."
"we anticipate that chartr will provide a pathway to standardizing quantitative comparisons between models and across studies, and ultimately serve as one of the reference implementations for researchers interested in developing and experimentally testing candidate models of decision-making processes. chartr also codifies the various parameters of decision-making models, which reflects the hypothesized latent constructs and how they interact, and provides easy access to many models of behavioral performance in decision-making tasks including variants of the diffusion decision model, the urgency gating model, diffusion models with urgency signals, and diffusion models with collapsing boundaries. chartr also offers pedagogical value because it allows the user to effortlessly simulate the many different models of decision-making and generate choice and rt data from hypothetical observers. chartr will also allow quantitative evaluation of the predictions of various decision-making models and help move away from qualitative intuition-based predictions from these models. finally, chartr is also sufficiently flexible that users can implement novel models with their own specific assumptions."
"-what verification procedures will be used to check that the model works as expected? -how will the model be validated, and what data will be used for doing so? -is there a schedule of reviews to ensure that the model remains up to date?"
"a common feature across all of the excellent toolboxes currently available is that they provide code to apply the ddm to data, or the ddm in addition to a few alternative models. as a consequence, the toolboxes provide no pathway for a researcher to rigorously compare the quantitative account of the ddm to alternative theories of the decision making process including models with an urgency signal [cit] ), urgency-gating [cit], or collapsing bounds [cit] b) . simply put, we currently have no openly available and extensible toolbox for understanding choice and rt behavior using the many hypothesized models of decision-making. we believe there is a critical need for examining how these different models perform in terms of explaining decision-making behavior."
"in order to deal with an increasingly complex world, we will need ever more sophisticated models. computational models have the potential to help us make decisions more wisely and to understand the complicated and often counter-intuitive potential consequences of our choices. however, as with all tools, they can be applied in wrong or misleading ways. thus a degree of understanding of their uses and properties is desirable. this paper brings together some of that knowledge in order to promote the better understanding of models. this is summarized by four points. first, it is important to be aware that models have different kinds of uses. effective deployment requires both the user and the modeller to be aware of their capabilities and limits. we have outlined some broad categories of model purpose and the key role that framing plays in balancing perspectives and getting the best out of a model. confusing or conflating model purpose can result in the inappropriate use of models, or a mistaken assessment of their reliability."
"steps 1-3 loaded the required data, identified the desired model to fit and specified the parameters of the model to be estimated. this information is now passed to the optimization algorithm (differential evolution). parameter optimization is an iterative process of proposing candidate parameter values, accepting or rejecting candidate parameter values based on their goodness of fit, and repeating. this process continues until the proposed parameter values no longer improve the model's goodness of fit. these are assumed to be the best-fitting parameter values, or the (approximate) maximum likelihood estimates. fig. 4 provides an overview of the steps involved in parameter estimation when using the differential evolution optimization algorithm [cit] ."
"the next step in model estimation is, for each model, to specify a list fig. 4 . flow chart for the parameter estimation component of chartr, which uses the differential evolution optimization algorithm [cit] ."
"there are a wide range of computational modelling techniques, but they differ principally along a few dimensions. selecting particular points along these dimensions implies a set of abstractions and assumptions about the system being modelled, which in turn determines how observations are represented."
"together, the results in figs. 9 and 10 highlight the ease with which chartr can be used to make insightful statements about the latent cognitive processes underlying behavior in decision-making tasks and ultimately may be a stepping stone for deeper insights into mechanism [cit] ."
"-what methods will be used to communicate with users? -what are their needs and abilities to appreciate the model and what it provides? -are visualizations, dynamic graphs and movies appropriate to convey the messages of the model and, if so, have resources been set aside to create these?"
"geographic: spatial and topological relationships, such as (static) locations of adjacent underground stations and the positions of emergency exits, or (dynamic) flows in a pipeline and networks of sensors on people, animals and objects. temporal: how the expected certainty of the model varies over time. for example, weather forecasting becomes less certain the further we look into the future, and navigation models become less precise as we move away from the position where we last verified our location. physical: underlying natural science, ecosystems and their governing laws, such as those that govern water flow, heat transfer or atmospheric physics. security: threats and their mitigations, such as access controls, which prevent unauthorized persons or systems from physically entering or digitally accessing a system, and encryption methods that encode data so they can only be accessed via keys. privacy: anonymity, identity, authentication of personally identifiable information, and controls on intended and unintended disclosures. legal: obligations, permissions and responsibilities for different components within the system and for human users of the system. social: communication and interaction relationships between humans involved in the system, and between humans and the physical/natural world and the underlying technologies. economic: quantitative aspects of resource consumption, production and discovery; typical resources are energy, money and communication bandwidth. uncertainty: what the acceptable bounds of uncertainty are for various aspects of the system, and how bounds are qualified, quantified and related to each other. failures: relationships between components that can fail or operate incorrectly, including fail-safe mechanisms and redundancies."
"personal water crafts (pwc) are capable of fast and agile manoeuvring and provide endurance over long distances making them suitable for complex autonomous missions. certain missions may require station keeping as one of the use-modes, which is achieved through heading and speed control at low speed forward and aft."
"another type of analysis uses logic to formulate questions and reasoning techniques to answer them. for instance, questions about the performance of a modelled telecommunications service such as after a request for a service, is there at least a 98% probability that the service will be delivered within 2 s? can be expressed in a probabilistic temporal logic. automated reasoning tools such as theorem provers and model checkers can be used to derive the answer."
"note that the main goal of this study is to illustrate the use of machine learning methods as protein database curation assistance tools. [cit] . this analysis concerns the ability of different machine learning methods to discriminate between class c subtypes from different transformations of the results of the analyses of the datasets using supervised classification methods, reported in the previous section, lead to some unequivocal conclusions."
previous research on gpcr class c from a data curation perspective. subtype classification of gpcrs has been attempted at different levels of detail 30 . our interest in the analysis of the evolution of this database from a data curation perspective stems from early experiments 23 in which we tested the extent to which class c gpcr first-level subtypes could be automatically discriminated from different transformations of their unaligned primary sequences.
where g β (s) is the laplace transform of cos(2β(t)); k u f is the steady state surge gain for a constant β; t u (x u /(m − xu)) −1 is the time constant.
"the desired closed loop step response should have an overshoot less than 5%, and a rise time of approximately the performance of heading controller is first assessed by step changes. figure 7 shows the scalability of the transient response of the l 1 adaptive heading controller. figure 8 shows the performance of the pwc during an elevator step simulation. the elevator steps and causes the surge to change sign and thereby the input gain."
"the collapsing boundaries, as implemented here, are symmetric, though they need not be; chartr provides flexibility to modify all features of the boundaries, including symmetry for each response option, and the functional form. for instance, one might hypothesize that linear collapsing boundaries are a better description of the decisionformation process than nonlinear boundaries [cit] . chartr also permits ddms with collapsing boundaries to incorporate any combination of variability in starting state, drift rate, and non-decision time (e.g., models of the form cddmsvs z s t and cfkddmsvs z s t )."
"data are observations that can provide evidence for a model. the exact role of data depends on how they were obtained, and the purpose of the model. for example, if the model aims to offer rigorous explanations or predict future outcomes of an existing system, then data are necessary to validate the model. if, on the other hand, the purpose of the model is to specify a system design, or define how an intended system is required to behave, then data are used to validate the system against the model. in other words, after the system has been implemented, one checks it behaves as it should."
"once the c code has been specified for the model, the code is compiled using the following command that uses the shlib framework [cit] at the terminal (usually iterm in mac, terminal emulator in linux). the command shown in listing 1 calls the appropriate compiler (clang on mac, gcc on linux), identifies the appropriate compiler to run, and loads the appropriate libraries and ensures the correct options are applied during compilation to create the architecture-specific shared object."
"the output of the compilation is a shared object called chartr-modelspec.so that is dynamically loaded into r for use with the differential evolution optimizer. we anticipate that future versions of chartr will use the rcpp framework (eddelbuettel and françois, 2011), which will obviate the need for compiling and loading shared object libraries."
"as the review of quality assurance of government models (commonly known as the macpherson review) [cit], once a model exists, it may be used for purposes beyond that for which it was originally designed, and it may continue to be used long after the time when it should have been replaced. there are at least three reasons for this: these dangers can be avoided, or at least ameliorated, by scheduling regular reviews of the model to check that it remains fit for purpose, and to ensure that the documentation remains relevant. the review may conclude that the model should be retired or re-written. to ensure that such reviews do take place, models should have long-term owners with responsibility for their continued maintenance."
"fourth, in chartr, we have largely focused on the use of modelselection metrics rather than predictive tests such as cross validation. however, cross validation is a powerful tool to guard against over-fitting and for predicting generalization performance on held out data and does not explicitly include a penalty term. if one intends to use cross validation, one could subset the data that is passed into the parameter estimation routines; say, retain 80% of the data and hold out 20% of the data. parameter estimation would then operate on the training data as described below. once the best-fitting parameters are identified they could be passed with the held out 20% of the data to the same functions as the training phase to calculate the out-of-sample goodness of fit. this process could then be repeated, say, 10 times, to obtain an estimate of the average out-of-sample goodness of fit. we anticipate implementing cross validation as an additional approach in future versions of chartr."
"none of these simplifying assumptions are likely to hold in experimental contexts. for example, the relative speed of correct and table 2 list of the 37 models available in chartr along with the individual parameters in each model and the total number of parameters. n refers to the number of stimulus conditions used. a u is the short form of a upper ."
"this idea was extended to protein sequences in 38, where it was shown to capture meaningful physical and chemical properties of the proteins. in the current work, 3-gram representations were first created from two different databases: swiss-prot and gpcrdb. the gpcrdb representation was created using the complete database (not only class c) [cit] versions. to train the model, each sequence was split into 3 sequences of 3-grams with offsets from 0 to 2 that were used in training set. a skip-gram version of window size 25 was used to train both models. for the final working represention of a sequence, the vectors corresponding to its 3-grams were summed up."
"for low speed operations such as station keeping where the speed is below 3 m/s the personal water craft behaves as a fully displaced vessel; hence the influence of the vertical dynamics into the manoeuvring characteristic is negligible. for larger ships it is assumed that the heave, roll and pitch are fairly small [cit] . this assumption is extended to the dynamics of the water jet vehicle; however this may be true only in confined waters."
"-does the model offer answers to the problems that i have? -are the assumptions it makes ones that i agree with? -if the model offers an explanation or prediction, has the model been validated sufficiently against empirical data (or in any way at all)? -if the model has no or weak empirical basis, is this adequate to my needs? -is the model documented so that i can understand how it works? -is the model output clear and comprehensible? -does the model output seem plausible when compared with other sources of information? -has the degree of uncertainty in the model output been properly recorded and its implications recognized? -is the model being used for its original intended purpose or, if not, is the new purpose compatible with the design of the model? -have other stakeholders or users been involved in the model design and use and, if so, do they agree that the model is useful?"
"we can also conclude that svm classifiers show a very consistent overall advantage when compared to rf and nb for all three datasets and for all five data transformations. [cit] datasets. this is a relevant result for two reasons: first, because it reveals svm performance to be more robust in datasets with limited class separability; [cit] datasets, almost any classifier will do reasonably well, even the baseline nb classifier. this is further evidence that sound biocuration, when dealing with the label noise problem adequately, helps to reduce the uncertainty associated to model-based decision making, in this case by limiting the impact of the choice of data analysis methods (here, the choice of classifiers) on the results."
"to apply it to protein sequence classification, the aas are considered as letters and the whole sequences as sentences, with n-grams acting as words. in nlp, this representation is understood as \"distributed\" because one \"concept\" in the domain is represented in several dimensions and one dimension gathers information about several \"concepts\". in nlp, these distributed word representations are learnt using an artificial neural network model and have been refined in the form of continuous bag-of-words (cbow) and continuous skip-gram (csg) models 47 ."
"second, we considered the number of particles used to explore the parameter space in the differential evolution optimization algorithm. in general, a higher number of particles is better as it minimizes the risk of falling into local minima, though this comes at the cost of computational time. we reduced the number of particles from 400 to 200 and found that computation time again halved ( fig. 11a-c) . the general rule of thumb proposed for the number of particles is 10 times the number of parameters to be estimated; 200 particles is consistent with the rule of thumb for the models implemented in chartr."
data availability statement. the gpcr datasets analyzed in this study are publicly available from gpcrdb (http://gpcrdb.org). the remaining generated datasets are available upon request to the corresponding author.
"if a researcher believes that all of the models implemented in chartr or novel models they develop are all wrong but provide useful descriptions of choice and rt data, then aic is more appropriate for model selection. in this scenario, the goal of model selection is to assess which model will provide the best predictions for new data. in this sense, aic is closely linked to cross validation. as more and more data are collected, the assumption under aic is that the model that produces the best predictions will become more and more complex."
"it is clear from fig. 5 that various features in data discriminate between various features of the decision-making models: the relative speed of correct and error rts, and critically the shape of complete rt distributions. we now provide three illustrative case studies that take advantage of the differential predictions of the models, demonstrating the use of chartr for parameter estimation and selection amongst sets of competing models."
"the science of urban modelling is rapidly developing, and modelling is routinely used in the retail and transport sectors. however, substantial research challenges and opportunities remain, particularly in dynamics and in deploying new data sources. greater research coordination, and policies that make high-quality urban models available to local authorities, could help to realize the tremendous potential of 'urban analytics'."
"last, modelling is changing fast. this presents a range of future opportunities, which could transform policymaking and business operations. we have outlined some of those opportunities and also the fresh challenges they provoke. there is a consequent increasing need for the new skills and collaborations that will underpin the future of modelling."
the diffusion decision model (or ddm) is derived from one of the oldest interpretations of a statistical testthe sequential probability ratio test [cit] )as a model of a cognitive processhow decisions are formed over time [cit] . the ddm provides the foundation for the decision-making models implemented in chartr and assumes that decision-formation is described by a onedimensional diffusion process ( fig. 1a ) with the stochastic differential equation
"note that only limited information regarding these sometimes drastic database changes is publicly available. in fact, little detail is known regarding the rationale behind those changes."
"implementation of the ugm in chartr uses the exponential average approach for discrete low-pass filters (smoothing). the momentary evidence for a decision is a weighted sum of past and present evidence, which gives rise to the ugm's pair of governing equations"
"where m is the mass of the vehicle, i z is the rigid body inertia around the z-axis, x g is the horizontal coordinate of the centre of gravity w.r.t. the origin of the body-fixed manoeuvring frame (z-down), and x (·), y (·), n (·) are the hydrodynamic derivatives."
the ddm is a special case of most of the model variants considered and will almost always fit more poorly than any of the other variants. we provide model selection methods that determine if the incorporation of additional components such as urgency or collapsing bounds provide an improvement in fit that justifies the increase in model complexity.
"another challenge is to create scaleable architectures for real-time or batch reprocessing of argumentation mining on the web. in our scenarios above, support for authoring arguments would require real-time feedback (i.e. within minutes). slower batch processing would be useful for the two other scenarios (support in challenging arguments with critical questions; support for summarizing debates) since wikipedia's debates are generally open for 7 days."
"in previous work, argumentation schemes have been classified in constrained domains, especially in legal argumentation [cit] and by using [cit] the araucaria corpus [cit] . 4 each of our envisioned applications of argumentation has certain requirements. automatically detecting the argumentation schemes used in a message could be used for supporting authoring and finding weaknesses of arguments, which focus on the interior of each message. in order to ask the appropriate critical questions, the premises, conclusions, and inference rules would first need to be detected. to get at the point of each message, the macro-level argumentation (for instance using factors and dimensions) would be useful for summarizing the debate, especially if we record rationales."
"despite the apparent benefits of model selection, there are technical and computational challenges in the application of decision-making models to behavioral data. some researchers have surmounted these issues by simplifying the process: using analytical solutions for the predicted mean rt and accuracy from the simplest form of the ddm, applied to participant-averaged behavioral data [cit] . however, the complete distribution of rts is highly informative, and often necessary, to reliably discriminate between the latent cognitive processes that influence decision-making [cit] . until recently, applying sequential sampling models like the ddm to the joint distribution over choices and rt required bespoke domain knowledge and computational expertise. this has hindered the widespread adoption of quantitative model selection methods to study decision-making behavior."
"ensemble modelling is an important approach to model combination that involves running two or more related (but different) models, and then combining their results into a single result or comparing them. when results within the ensemble disagree, this can contribute to an understanding of whether uncertainty is present as a result of the type of model (and so the choice of model is crucial), or exists within the system. as an example, ensembles are widely used in weather forecasting, to show the different ways in which a weather system can develop."
"the bic scores/weights also suggest that ddmsvs t and ddms t are the best models for describing the data. however, interestingly, bic ranks ddms t higher than ddmsvs t . this result does not suggest that chartr is failing to recover the data generating model. instead, our interpretation of the results is that both ddmsvs t and ddms t should be considered candidate explanations for the data and that they are very close in terms of explanations for the choice and response time data. that is, the most likely explanation for the data is a ddm with variable non-decision time. there might also be a contribution from drift rate variability. as we explained in the methods, aic is more focused on false negatives and thus places a lower penalty on complexity. bic is more focused on false positives and thus places a higher penalty on complexity."
"the realisation of station keeping for the pwc is certainly a challenge since the system is under-actuated. the vehicle has an azimuth impeller equipped with an elevator, which can be used to change the vertical direction of the impeller jet. this actuation is perfect at high forward speed to execute fast manoeuvres; however at low speed the controllability of the system reduces."
"-large-scale availability of data about individuals will transform modelling. when we model a population of individuals today, we often attempt to make predictions using aggregate models based on assumptions about hypothetical, 'average' members of the population. in future, it may be easier to eliminate these assumptions by modelling the individuals directly. -models will require more extensively linked data. some data may be derived not from measurement but from other models, requiring additional links to derived data. -modelling will span many scales, and many levels of detail. as various modelling communities come together, bringing expertise from different disciplines and sharing approaches to model design, we will see more sophisticated ways to link models in ways that describe systems at multiple levels of detail. -more models will be built by computers. models may be constructed from data by automated or semi-automated inference. these models will have the capacity to reveal unexpected results, but it may be hard to guarantee that their mechanisms continue to operate reliably in the face of new evidence. -models will help to train computers. when computers learn from real-world data, they need to be exposed to both positive and negative examples. the latter can be difficult to find: models may be able to generate verisimilar data representing failures. -more systems will become part of models and more models will become part of systems. more components of engineered systems will be software: that software may be incorporated into models used to predict the behaviour of the aggregate system built from components and embedded models may drive aspects of system behaviour. this will change the dynamic between modelling and deployment of systems. -new technologies will change modelling paradigms. specialist quantum simulators will soon become available. they may allow us to develop models that predict properties of materials or pharmaceuticals, or make scenario planning for finance, defence and medical diagnosis more tractable. -ubiquitous sensors will require new forms of modelling. sensors, actuators and processors are becoming more ubiquitous and more intelligent, yet sensors decalibrate and degrade over time both individually and as networks. the unreliability of data from sensors will require more spatial, dynamic and probabilistic styles of modelling. -modelling will be used more often for strategic and policy-level issues. modelling will increasingly be used for high-level organizational planning and systems thinking, adding more detail to potential future scenarios, and allowing analysis of possible outcomes of policy interventions. -senior decision-makers will increasingly become involved in modelling. senior decision-makers will participate more often in building and using models. a willingness to engage directly in modelling, for example, by bringing modelling into the boardroom, will increasingly be seen as a sound approach to managing complexity. -some models will be oriented more towards humans and their personal characteristics. we will have a greater opportunity, as individuals, to supply (personal) data that could be used to stimulate modelling. however, there remain deep, unresolved social and ethical issues around the ownership of data and the use of models derived from personal data. -models will help to train humans. simulators are already used to train jet pilots, formula one drivers and veterinary surgeons. high-fidelity models will soon be used more widely, in conjunction with virtual reality and 'gamification' in training for doctors, military personnel, police forces and school pupils, to name just a few. -models will become an important way to understand properties of many complex systems. we increasingly build systems so complex that their behaviours cannot be explored in any depth. the internet itself is an example of a complex, engineered system on which much of our developed world now depends, and which is continuously modelled and monitored in order to explore its behaviours and monitor its performance."
"the typical steps in chartr for estimating the parameters of a decision-making model from data are as follows: 1) model specification: specify models in the c programming language, and compile the c code to create the shared object, chartr-modelspec.so, that is dynamically loaded into the r workspace. future versions of chartr will use the rcpp framework and will not require the compilation and loading of shared objects (eddelbuettel and françois, 2011) . 2) formatting and loading data: convert raw data into an appropriate format (choice probabilities, quantiles of rt distributions for correct and error trials). save this data object for each unit of analysis (e.g., a participant, different experimental conditions for the same participant). load this data object into the r workspace. 3) parameter specification: choose the parameters of the desired model that need to be estimated along with lower and upper boundaries on those parameters (i.e., the minimum and maximum value that each parameter can feasibly take). 4) parameter estimation: pass the parameters, model and data to the optimization algorithm (differential evolution). the algorithm iteratively selects candidate parameter values and evaluates their goodness of fit to data. this process is repeated until the goodness of fit no longer improves (fig. 4) . 5) model selection: the parameter estimates from the search termination point (i.e., the point where goodness of fit no longer improves), the corresponding goodness of fit statistics and model predictions are saved for subsequent model selection and visualization."
"in chartr, we allow for variants of the ugm where the parameters of the urgency signal are not fixed. for instance, similar to the ddm with an urgency signal, we can test a ugm where the intercept (b) is freely estimated from data (bugm), and even an intercept that varies on a trial-by-trial basis (eq. (14))."
"informally, an argument is a communication presenting reasons for accepting a conclusion. unlike proofs that lead step-by-step from premises with logical justifications for a conclusion, arguments are non-monotonic and can be disproven. arguments may use various approaches including generalization, analogy, inference, and prediction. * this work was carried out during the tenure of an ercim \"alain bensoussan\" fellowship programme. the research leading to these results has received funding from the european union seventh framework programme (fp7/2007 (fp7/ -2013 under grant agreement n o 246016. the simplest possible argument connects two statements by means of an inference rule (figure 1) . inference rules are functions that input one or more statements (the premises) and return one or more statements (the conclusions)."
"the manual annotations described above, of argumentation schemes and of factors, suggest several possibilities for automation. scalable processes for analyzing messages are needed since wikipedia has roughly 500 debates each week about deleting borderline articles. argumentation mining could be the basis for several support tools, helping participants write more persuasive arguments, find weaknesses in others' arguments, and summarize the overall conclusions of the debate. first consider how we might give participants feedback about their arguments. from our research, we know which argumentation schemes are viewed as acceptable and persuasive within the community. if real-time algorithms could identify the argumentation schemes used in the main argument, authors could be given personalized feedback even before their message is posted to the discussion. when the argumentation scheme used in a draft message is not generally accepted, the author could be warned that their message might not be persuasive, and given personalized suggestions. thus debate participants might be nudged into writing more persuasive arguments."
"in business and manufacturing, models underpin a wide variety of activities, enabling innovative high-quality design and manufacturing, more efficient supply chains and greater productivity. modelling can also improve businesses' organizational efficiency, commercial productivity and profitability. in manufacturing, models tend to fall into three broad categories: complex models aimed at modelling physical reality with a high degree of accuracy, reduced physical models that capture behaviour at a specific scale and representative models (so-called 'black box') models that fit data and trends."
"in contrast, if a researcher believes that the true model is implemented in chartr or in the set of novel models they develop, then bic is likely to be the better tool. in this scenario, the goal of model selection is to address the question \"which of these models is correct?\". as more and more data are collected, the assumption under bic is that the correct model will be identified. bic is thus ideally suited to answer questions about identifying which model was most likely to have generated the data."
"the proposed architecture for the l 1 adaptive controller exploits the physical correlation between heading angle and turning rate to reduce the complexity (order of the system) of the state predictor and the adaptation law. moreover, since the ultimate objective is the embedded implementation of the station keeping controller in the real vehicle, the design of the l 1 augmented pd heading controller is undertaken in discrete time."
"from a structural point of view, class c gpcrs are characterized, in addition to the seven-helix transmembrane (7tm) domain, which is typical of all gpcrs, by a large extracellular domain (venus flytrap or vft) that in most cases is connected to the 7tm by a cysteine rich domain (crd) 13 . in contrast with rhodopsin gpcrs (also known as class a gpcrs), which bind their endogenous ligands within the 7tm domain, most class c gpcrs bind their respective endogenous ligand within the vft domain, thereby leaving the 7tm domain suitable for allosterism-based drug discovery [cit] . the vft is found only in group i, ii, and part of the group iii (gaba b subunits only). the absence of the vft in some of class c receptors has raised the hypotheses that either there is an endogenous ligand binding site at the 7tm domain for these receptors or they lack a ligand binding site and their function is related with allosteric effects through their potential heteromerization with other receptors 12 . also the crd is missing in group iii and iv class c receptors. in the case of heterodimeric gaba b, their two subunits (gaba b1 and gaba b2 ) have different functional roles. whereas the vft of gaba b1 is responsible for neurotransmitter binding, the tm domain of gaba b2 is responsible for g protein binding 17 . as examples of the relevance of class c gpcrs as drug targets, metabotropic glutamate and gaba b receptors are involved in various neurologic and psychiatric disorders amongst them parkinson's disease, schizophrenia and depression 18, 19 . pharmacological databases are fundamental for the analysis of the structure and function of biological signal transduction entities, that is, receptors and ion channels 20 . gpcrdb 21 is a web-accessible and publicly-available repository and information system containing data and web tools especially designed for gpcr research. [cit], it includes published information about the five major gpcr classes 11 . class c, investigated in the current study, in turn comprises several subtypes. [cit] was object of extensive analysis using machine learning methods in our previous research [cit] . these analyses revealed a possible receptor label noise problem 27 . here, label noise implies the possibility that the sequence subtype labels, taken to be the ground truth, were wrong due to the uncertainty of the own database sequence labeling procedure. the problem takes the form of primary sequences being clearly and consistently misclassified by the machine learning methods as belonging to a different subtype than that reflected by their database label. the obtained results were understood as the first foundations for the development of a tool to assist omics database experts in their curator tasks by shortlisting items (proteins, genes) with questionable labels."
"for the pwc there is no model readily available, and the identification of the system dynamics solely relies on full scale motion data always affected by induced oscillations in the wave frequency range. this results in significant parameter variation, which calls in for robust and adaptive control strategies."
"there is a further role for data when we are confident about the essential structure of the model, but do not know the bounds of some parameters. in this case, data are used to fine-tune parameters such as the duration or speed of an event. in all cases, care and expert judgement about interpreting validation results is required, especially when the model has been determined mainly by data with few structural assumptions imposed by the modeller, or if the data are sparse, or when it is not possible to experiment with the deployed system. for example, air traffic systems are so crucial to modern life that one cannot experiment with various parameters-such as frequency of landings or proximity of aircraft-to comprehensively check the system against the model."
"this paper aims to bring together knowledge about computational modelling across a wide range of domains, from public and economic policy to physical systems. a few examples and observations illustrate the current breadth and scope of modelling."
"models play crucial roles in finance and economics, from identifying and managing risk to forecasting how economies will evolve. yet major changes are afoot in economic modelling, triggered by the global economic crisis, the availability of huge datasets, and new abilities to model people's behaviour that overturn old certainties."
"finally, chartr currently allows the quality of the evidence signal (drift rate) to vary with an experimental factor (stimulus difficulty). in future versions of chartr, we will provide capabilities for different model parameters to vary with different experimental factors. there are a range of other experimental manipulations whose effect will likely appear in model parameters other than the drift rate; for example, emphasizing the speed or accuracy of decisions is most likely to affect the decision boundary, or the speed with which a boundary collapses. future versions of chartr will allow researchers to test and discriminate between these hypotheses."
"finally, we should consider the impact of the data transformations on the classification results. the interpretation of the corresponding comparative results bears similarities with that of the comparative of classification methods. [cit] version of the database, while the more complex acc performs best table 6 . analysis of misclassification of sequences h2u5u4_takru and t2mdm0_hydvu: for each sequence s the true class (tc), the predicted class (pc), the error rate (er s ), the voting ratio (r s ) and the cumulative decision value (cdv s ) are reported. for the meaning of these measures, see the methods section."
"the water jet vehicle is a modified sea-doo gtx 215 personal water craft (see fig. 1 ), whose physical specifications are listed in table a.1. a radio link between the vehicle and a ground station allows remote command of three servos, which control the throttle, the azimuthal angle of the water jet, and the elevator angle of a blade constraining the angle of attack of the pressurised water stream w.r.t. the horizontal."
"chartr currently provides only the most likely value for a parameter without any measure of its uncertainty, whereas the full posterior distribution provides uncertainty in the estimate for each parameter, thus reducing the likelihood of drawing over-confident conclusions. second, bayesian methods are advantageous when used in contexts where there are only modest numbers of trials per observer. hierarchical bayesian models in particular can enhance statistical power by providing opportunities for simultaneous estimation of the parameters of individual observers as well as the population-level distributions from which they are drawn."
"the transformed datasets were analyzed with svms 40, but also with nb 41 and rf 36 classifiers for comparison. all these classifiers are now standard in bioinformatics research and are different enough as to provide a well-informed comparative of results."
"we estimate the model parameters using differential evolution to optimize the goodness of fit [cit] . for the type of non-linear models considered in chartr, we have previously found that differential evolution more reliably recovers the true data generating model than particle swarm and simplex optimization algorithms [cit] ) . deoptim also allows easy parallelization and can be used readily in clusters and the cloud with large number of cores to speed the process of model estimation. however, we again provide flexibility in this respect; the user can change this default setting and specify their preferred optimization algorithm (s)."
dat is saved to disk as a new file. the dat file is loaded into the r workspace as required for the model estimation procedure.
"for our first case study, we assumed the data came from hypothetical observers who made decisions in a manner consistent with a ddm with variable drift rate (s v ) and variable start times (s t ). in chartr, this corresponds to simulating data from the model ddmsvs t, where an observer's rts exhibit variability due to both the decision-formation process and the non-decision components. we simulated 300 trials for each of 5 stimulus difficulties, for 5 hypothetical participants."
"common example combinations of techniques include stochastic partial differential equations and hybrid automata [cit] . the latter have discrete states and transitions between them, and each state is a set of differential equations that describes the continuous behaviour that applies during that state. a drawback of some combinations is that analysis can be difficult and may be poorly supported by automated tools."
"chartr provides a powerful framework for estimating and discriminating between candidate decision-making models. nevertheless, there is considerable scope for extending its capabilities. here, we outline a few future directions we believe would make chartr, and other toolboxes that come in its wake, even more useful for decisionmaking researchers."
"models have many technical aspects, such as data, mathematical expressions and equations, and algorithms, yet these are not sufficient for a model to be useful. to get the best out of a model, model users and commissioners must work closely with model developers throughout its creation and subsequent application."
"-has anything similar been done before? if so, what can be learned from it? -is there a schedule of reviews to ensure that the model remains up to date? -are sufficient skills and expertise available and, if not, how can this be managed? -what is the timescale for the work? -what resources (time and money, for example) are available? -is it necessary and affordable to build a model, or could some other approach be used that requires fewer resources? -what would be the consequences if the work is not carried out at all, or the start is delayed?"
"chartr provides two metrics for quantitative comparison between models. each metric is based on the maximized value of the qmp statistic, which is a goodness-of-fit term that approximates the continuous maximum likelihood of the data given the model."
"we argue that the ideal toolbox for developing and implementing cognitive process models of decision-making and evaluating them against choice and rt data should be simple to use, offer a plurality of cognitive models, provide model estimation and model selection procedures, provide simple simulation and visualization tools, and be easily extensible when new hypotheses are developed. such a view is broadly consistent with recent research that lays out the best practices for computational modeling of behavior [cit] . ready adoption is also facilitated when the toolbox is implemented in an open-source, free programming language obviating the need for expensive licenses. the added benefit of an open source toolbox is that researchers can look \"under the hood\", which has at least three benefits: (1) allow a deeper level of understanding of the models, (2) readily permit extension of the toolbox, and (3) catch errors in implementation. at the time of development of this toolbox and submission of this study, no existing toolbox has satisfied all of these criteria."
"over the five years elapsed between the earlier and later versions of the database analyzed in this study, gpcrdb has undergone major changes in the total numbers of proteins belonging to class c, but also in the ratio of the different subtypes to the total number of receptors and even in the sequences contained in each of those subtypes (see table 1 and fig. 1 for some summary figures)."
"the proposed l 1 [cit] . however, to the authors' knowledge this is the first attempt to tackle the design of the l 1 adaptive controller directly in discrete time. the proposed architecture results in"
"chartr is designed with the goal of being readily extensible, to allow the user to specify new models with minimal development time. this frees the user to focus on the models of scientific interest while chartr takes care of the model estimation and selection details behind the scenes. here, we provide an overview of the steps required to add new models to chartr."
"together, this case study highlights the power of chartr in discriminating between 37 albeit overlapping models of decision-making and ranking the most likely models. as we have emphasized, the models selected by aic and bic will differ slightly because of the different penalties assumed for the two methods which underlie their different philosophies. if we obtained this result in real data, our interpretation would be that for this population of subjects, the data are consistent with a model that involves a ddm and variable non-decision time and that there is also the possibility of variability in the drift rate parameter. we would also conclude that the most likely models are ddms without a dynamic component such as an urgency signal, since the ddms performed better than models with collapsing boundaries or urgency."
"the (simple) ddm assumes a level of constancy from one decision to the next in various components of the decision-formation process: it always commences with the same level of response bias (z), the drift rate takes a single value (v i, for trials in experimental condition i), and the non-decision time never varies (t er )."
"t s is the sampling time, and f (k) is an unknown nonlinear function that represents the unmodelled cross-couplings with sway and surge. the control signal u is designed as the sum of two contributions"
"the difference equation for the model variants implemented in chartr is specified in c code in the file \"chartr-modelspec.c\". an example algorithm for the ddm (section 2.1.1) is shown in algorithm 1."
"of parameters that can be freely estimated from data along with each parameter's lower and upper bound; we provide default suggestions for the lower and upper boundaries in chartr. model parameters can be generated by calling the function paramsandlims with two arguments: model name and the number of stimulus difficulty levels in the experiment. the number of stimulus difficulties is internally converted into drift rate parameters; for example, if there are n stimulus difficulties, then paramsandlims will estimate n independent drift rate parameters. there is also functionality in chartr to specify fixed (nonestimated) values of some parameters, such as a drift rate of 0 for conditions with non-informative sensory information (e.g., 0% coherence in a random dot motion experiment). paramsandlims returns a named list with the following fields: lowers, uppers, parnames, fitugm."
"3.4.3. case study 3: [cit] to demonstrate the utility of chartr in understanding experimental data, we model the freely available choice and rt data from two monkeys performing a random dot motion decision-making task [cit] . in this classic variant of the random-dot motion task, the monkeys were trained to report the direction of coherent motion with eye movements. the percentage of coherently moving dots was randomized from trial to trial across six levels (0%, 3.2%, 6.4%, 12.8%, 25.6% and 51.2%). monkey b completed 2614 trials and monkey n completed 3534 trials."
"figs. 3 and 4 provide flowcharts for chartr. fig. 3 provides an overview of the five main steps involved in the cognitive modeling process. fig. 4 provides a schematic overview of the steps involved in the parameter estimation component of the process, which uses the differential evolution optimization algorithm [cit] ."
"erroneous responses can differ, and participants' arousal may exhibit random fluctuations over time, possibly due to a level of irreducible neural noise. decades of research into decision-making models suggests that these effects, and others, are often well explained by combining systematic and random components in each of the starting state, drift rate, and non-decision time (fig. 1b) . in chartr, we provide variants of the ddm where all of these parameters can be randomly drawn from their typically assumed distributions over different trials,"
"some recent attempts have demystified the application of cognitive models of decision-making to behavioral data, providing a path for researchers to apply these methods to their own research questions. [cit] developed the diffusion modeling and analysis toolbox (dmat), [cit] developed the diffusion model toolbox (fast-dm; for an updated version see fast-dm-30, [cit] ) . other modern toolboxes have improved the parameter estimation algorithms and can leverage multiple observers to perform hierarchical bayesian inference [cit] . in hbayesdm [cit] and dynamic models of choice [cit] researchers can apply a range of models to behavior from a wide variety of decision-making paradigms ranging from choice tasks to reversal learning and inhibition tasks."
"one might not need to know anything about the mechanisms inside a very well established and understood model. however, for other models (especially newly developed models) it is useful to have some understanding of the basis of their construction. in this section, we give a brief summary of the main aspects and approaches used. stakeholders often have very different perspectives on the key abstractions and assumptions about the system being modelled. frames of reference [cit] are one way of articulating the variety of perspectives, and their context. clarity on frames allows different levels and type of concern to be balanced within model development and analysis, driving the selection of model type and techniques. some common frames are the following."
the functions take as input the various parameters that are to be optimized along with various constants such as the maximum number of time points to simulate as well as the time step.
"finally, environmental modelling, including climate change, plays an important role in guiding government policy as well as business decisions, in situations ranging from noise reduction to flood risk assessment and wherever there is an opportunity to enhance social resilience to severe natural hazards. open-access datasets are particularly useful in this domain."
"the ddm with collapsing boundaries generalizes the classic ddm by assuming that the sensory evidence required to commit to a decision is not constant as a function of elapsed decision time. instead, it assumes that the decision boundaries gradually decrease as the decision-formation process grows longer (e.g., [cit] . collapsing boundaries terminate trials with weak sensory signals (i.e., lower drift rates) at earlier time points than models with 'fixed' boundaries (i.e., simple ddm) and otherwise equivalent parameter settings. the net result of collapsing boundaries is a reduction in the positive skew (right tail) of the predicted rt distribution relative to the fixed boundaries ddm. this signature in the predicted rt distribution holds whether there is variability in parameters across trials (section 2.1.2) or not (section 2.1.1)."
"in public policy, models can enhance the quality of decision-making and policy design. they can offer cost-benefit analyses of various policy and delivery options, help manage risk and uncertainty or predict how economic and social factors might change in the future. there is still considerable untapped potential in this area but also obvious dangers."
-what must the model cover? -what can be excluded from the model? -what is the minimum viable scope that can be used as a starting point for the model?
"within-decision variability in the diffusion process. represents the standard deviation of a normal distribution. by convention, set to a fixed value to satisfy a scaling property of the model."
"computational models can help us translate observations into an anticipation of future events, act as a testbed for ideas, extract value from data and ask questions about behaviours. the answers are then used to understand, design, manage and predict the workings of complex systems and processes, from public policy to autonomous systems. models have spread far beyond the domains of engineering and science and are used widely in diverse areas from finance and economics, to business management, public policy and urban planning. increasing computing power and greater availability of data have enabled the development of new kinds of computational model that represent more of the details of the target systems. these allow us to do virtual what if? experiments-even changing the rules of how this detail operates-before we try things out for real."
"despite these benefits, we emphasize that it is far from straightforward to extend the models implemented in chartr to bayesian parameter estimation methods. the goal of chartr is simple and rapid implementation and testing of new models, which takes place via simulation-based techniques. bayesian methods require model likelihood functions, which can be challenging to derive and may not even exist for some of the models implemented in chartr, and as such the extension to bayesian methods is not trivial. in future work, we aim to extend the parameter estimation routines in chartr to make use of approximate bayesian techniques."
"similarly, when using posterior model probabilities, the most plausible model across the set of observers is the generative model bugmsv. again the next five best models are all conceptually related to the data generating model. for instance, the next best model was bugm which is an urgency gating model with an intercept and no drift rate variability. the third best model was uddm which is a ddm with an urgency signal but no gating."
"for the multi-class classifiers, at the global level, we report the accuracy (accu), which is the ratio of correctly classified sequence to their total number, but also the mcc and the f-measure as explained in 54, 55 . the reported measures are the mean values of the respective metrics over the five iterations of the 5-fold cv used to evaluate the classifiers."
"together these results serve as another reminder of the utility of chartr in the analysis of decision-making models, including the ability to quantitatively assess a large set of conceptually similar and dissimilar models. if we were to obtain results like the case study in a hypothetical experiment, we would reject a simple ddm as an explanation for our data and suggest that a model with an urgency signal containing an intercept is a more likely model to explain the data. we would also likely suggest the presence of a gating component in the data but qualify our conclusions by saying that additional subjects and larger number of trials per subject would be needed for more confidence in the result."
"given the effort it takes to make and check a good model, how might one decide whether this effort is worthwhile? for a given system, there are a number of answers to this question: -the complexity of the system means that the risks and consequences of any choice cannot be anticipated on the basis of common sense or experience. -there may be too many detailed interactions to keep track of, or the outcomes may be too complicated and interwoven to calculate easily. -it is infeasible or unethical to do experiments with the system. -one needs to integrate reliable knowledge from different sources into a more complex whole to understand the interactions between them. -there is a variety of views from stakeholders or experts about a complex system they are part of, which needs bridging in order to come to a coherent decision or find a compromise. -one needs to be prepared for possible future outcomes in a complex situation."
"in a second case study we simulated data from hypothetical observers whose decision-formation process was controlled by an urgency gating model [cit] with a variable drift rate and an intercept, termed bugmsv in chartr. we again assumed five hypothetical subjects, five stimulus difficulties and simulated 500 trials for each of them. we then fit the data with the redundant-estimation approach as in case study 1 and evaluated the results of the model selection analysis, all using routines contained in chartr. fig. 8a and b shows the aics and bics for the set of models considered for one hypothetical observer's data, again referenced to the ddm (i.e., as difference scores relative to the ddm). negative values suggest a more parsimonious account of the data than the ddm, and positive values suggest the opposite. fig. 8c shows the akaike weights and posterior model probabilities for the top six models. bugmsv provides the best account of the data for this hypothetical observer according to both aic and bic."
"particularly when considering very complex phenomena, one needs to understand why something occurs-in other words, we need to explain it. in this context, explanation means establishing a possible causal chain, from a set-up to its consequences, in terms of the mechanisms in a model. this degree of understanding is important for managing complex systems as well as understanding when predictive models might work. with many phenomena, explanation is generally much easier than predictionmodels that explain why things happen can be very useful, even if they cannot predict reliably the outcomes of particular choices. for example, a social network model may help explain the survival of diverse political attitudes but not predict this [cit] ."
"-who will be the users of the model? -who will have overall responsibility for the model, its development and its use? -who will provide the data and the knowledge required to build the model? -who will develop the model? -who are the stakeholders (in other words, who is interested in the issue, who could contribute, who can influence and who will be impacted)? -how will stakeholders be involved, and at what stage they can be most useful? -do the stakeholders all have the same concerns and questions about the issue? if not, what are their perspectives, and which frames of reference are to be considered? -who will provide quality assurance? -who will determine when the model is no longer useful?"
"second, the current instantiation of chartr assumes that observers are independent. recent efforts have proposed the use of hierarchical bayesian methods for the ddm and other decision-making models [cit] . bayesian estimation methods provide at least two advantages over the current framework provided in chartr. first, bayesian methods incorporate prior knowledge into the plausible distribution of parameter values and they provide full posterior distributions for all model parameters."
"these results were further confirmed from the viewpoint of visualization-oriented fully unsupervised machine learning methods (that is, methods that attempted sequence discrimination into subtypes without knowledge of sequence-to-subtype assignment). results clearly indicated that the subtypes shown to be worse discriminated by supervised classifiers were also those shown to heavily overlap in unsupervised visualization models from different unaligned sequence data transformations 31 . these results might be just considered as a typical case of heterogeneous levels of subtype separability, often observed in real biological datasets. closer inspection of the sequence misclassification behavior, though, revealed an intriguing and potentially more interesting pattern: different runs of the same, or even of different, classification algorithms, might be expected to yield different subtype predictions for the same sequences. that is, we might expect a given sequence to be misclassified only in part of the experiments and/or be misclassified to different subtypes. for instance, a receptor sequence might be misclassified in only a percentage of experiments, being perhaps sometimes predicted to be a cs receptor, while others predicted to be a gb receptor. some of the observed misclassifications conformed to this typical pattern, but many others were found to be far too consistent, in the sense that the sequence was almost always misclassified (by different classifiers and different implementations of the same classifier) as belonging to the same wrong subtype."
"one approach to distinguish between these different models is to systematically manipulate the stimulus statistics and/or the task structure and then test whether behavior is qualitatively consistent with one or another sequential sampling model [cit] . an alternative approach is to quantitatively analyze the choice and rt behavior with a large set of candidate models, and then carefully use model selection techniques to understand the candidate models that best describe the data (e.g., [cit] b; [cit] . the quantitative modeling and model selection approach allows the researcher to determine whether a particular model component (e.g., an urgency signal, or variability in the rate of information accumulation) is important for generating the observed behavioral data. it also provides a holistic method for testing model adequacy because the proposed model is judged on its ability to account for all available data (e.g., [cit] ), rather than focusing on a specific subset of the data."
"upper and lower response boundaries that vary as a function of t. t er, s t time required for stimulus encoding and motor preparation/execution (non-decision time), and decision-to-decision variability in non-decision time. s t is the range of a uniform distribution with mean (midpoint) t er . s"
"computational modelling is like any other technology: it is neither intrinsically good nor bad. models can inform or mislead. modelling can be applied well or misapplied. it is for this reason that a better understanding of the processes of computational modelling and a greater awareness of how and when models can be reliably used are important. this cannot be just left to the modellers but some of the understanding is also needed by commissioners and users of these models. making the right decisions when commissioning a model or when and how to use a model is as important as the more technical aspects of model development. a hammer may be perfectly designed by its engineers and fit its specification exactly, but be worse than useless for driving in screws."
"-what level of detail is needed for the model in each of its frames of reference? -what accuracy is required in the output? -what should the trade-off between accuracy, simplicity and robustness be? -what modelling techniques will be used, and why those? which alternatives were considered? -how do the chosen modelling techniques have an impact on the accountability of decisions?"
"almost all computational models 'predict' in the weak sense of being able to calculate an anticipated result from a given set of variables. a stronger form of prediction goes further than this, anticipating unknown (usually future) outcomes in the observed world (some describe this as 'forecasting'). this sort of prediction is notoriously difficult for complex systems, such as biological or social systems, and thus claiming to be able to forecast for these systems may be misleading. if we truly do not know what is going to happen, it is better to be honest about that, rather than be under a false impression that we have a workable prediction. fitting known data (e.g. 'out of sample data') is not prediction in this sense."
"to estimate the parameters of decision-making models in chartr, the data need to be organized in a separate comma separated values (csv) file for each participant in a simple three column format: \"condition, response, rt\". \"condition\" is typically a stimulus difficulty parameter, \"response\" is correct (1) or incorrect (0), and rt is the response time (or reaction time when response time and movement can be separated). for example, in a typical file, data for a single stimulus difficulty (e.g., one level of motion coherence in a random dot motion task) would look like listing 2."
"the likelihood of terminating the optimization algorithm in local minima, which can arise in simulation-based models like those implemented in chartr. variability occurs due to randomness in simulating predictions of the model at each iteration of the optimization algorithm, and randomness in the optimization algorithm itself [cit],b). we then select the best of the 5 independent parameter estimation procedures (or 'runs') for each model and participant (i.e., the 'run' with the highest value of the qmp statistic). if computational constraints are not an issue, then we encourage as many repetitions as possible of the parameter estimation procedure. fig. 6a and b shows the aics and bics for a set of models, obtained after using chartr to fit the choice and rt data from one of the hypothetical observers. both information criteria (ics) are reported with reference to the ddm (i.e., as difference scores relative to the ddm). thus, negative values suggest a more parsimonious account of the data than the ddm, and positive values suggest the opposite. fig. 6c shows the akaike weights and bic-based approximate posterior model probabilities (eq. (21)) for the top six models. the aic scores/weights suggest that ddmsvs t provides the best account of the data; by 'best account', we mean the model that provided the most appropriate tradeoff between model fit and model complexity among the specific set of models under consideration, according to aic. this suggests that chartr reliably recovers the generating model as one of the candidate modelsa necessary test for any parameter estimation and model selection analysis. we strongly recommend this form of model recovery analysis when developing and testing any proposed cognitive model; if a data-generating model cannot be successfully identified as a set of candidate models in simulated data, where the true model is known, it is not a useful measurement model for real data."
"the discrete time l 1 adaptive controller is shown in fig. 5 . the relevant transfer functions, namely from reference to heading and from disturbance to forced yaw acceleration, are calculated to be"
"as the power and use of modelling grows, there is increased risk that models could be poorly constructed, misused or misunderstood. we need to reinforce modelling as a discipline, so that misconstruction and misuse are less likely; we need to increase understanding of modelling across a wide range of domains, from social policy to life sciences and engineering, as well as encourage sharing of insights and best-practice across these domains; and we need to bring commissioners closer to modelling, so that results are more useful. as computational modelling develops and extends to new application areas, there is enormous potential for interdisciplinary and intersectoral developments. the cross-fertilization of ideas between industries, and academia, along with a mutual appreciation of different sectors' needs in modelling skills, represents an exciting future for computational modelling. computational modelling already has an increasing impact on how science is done, but this will now extend into other areas of our lives. thus it is imperative that this tool is used appropriately and carefully. we hope this paper will prompt all those involved to think about how models are used and when they can be relied upon. form the basis for an initial discussion between model commissioners and model developers, to clarify their understanding of what will be involved, and during model building and use. in addition, they can serve as a point of departure for model reviewers. purpose -what is the issue or issues under consideration? -if there is more than one issue, how are they related? -what is the context of the issue? -what are the specific questions that need to be answered and can modelling address them?"
"the indirect estimate of n δ is strongly affected by the inconsistency in the estimate of t n for values of β close to zero. hence the values of n δ are scattered, determining an uncertainty in the magnitude ofā c n . this uncertainty is modelled as a"
"this reliability across different hyperparameter settings suggests that when large computing resources are not available, one could perform an initial fast assessment using the hyperparameter settings that provide the fastest model selection analysis to identify a candidate set of models. after that first phase, a subset of the models that performed best could be re-estimated with more conservative fig. 10 . quantile probability (qp) plots showing data in blue (corrects) and yellow crosses (errors) [cit], along with the model predictions (gray dots). predictions from ddmsvs z s t are shown along with four other models uddmsvs b, bugmsvs b, uddmsvs t, bugmsvs b . numbers at the top of each plot show the log likelihood, the aic and the bic for the model under consideration. higher, that is, more positive values of log likelihood are better. aic and bic are reported assuming ddmsvs z s t as the base (reference) model. for both monkeys the model bugmsvs b is the best model for describing the data out of these candidate set of models. (for interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.) hyperparameter settings to refine and confirm the results of the initial assessment."
"it takes time and effort to develop good models, but once achieved they can repay this investment many times over. just as physical tools and machines extend our physical abilities, models extend our mental abilities, enabling us to understand and control systems beyond our direct intellectual reach. this is why they will have such a radical impact: not just improving efficiency and planning, but extending to completely new areas of our lives. computational models will change the ways we can interact with our world, perhaps allowing completely new ways of living and working to emerge."
"importantly, and despite the fact that the research reported in this paper has focused on class c gpcrs as a case study, the proposed method could be exported to any omics database in which biological entities are associated to a characterization label. this research also highlights the importance of documenting the reasons for changes between versions of publicly available biological databases."
the bandwidth of the filter c(z) sets a clear trade-off between the adaptation to unmodelled dynamics and the sensitivity of the control signal to wave disturbances. the robustness properties of the adaptive controller are investigated by a frequency analysis showing the system performance in response to changes in the nominal plant.
"biocuration in the omics sciences has become paramount, as research in these fields rapidly evolves towards increasingly data-dependent models. as a result, the management of web-accessible publiclyavailable databases becomes a central task in biological knowledge dissemination. one relevant challenge for biocurators is the unambiguous identification of biological entities. in this study, we illustrate the adequacy of machine learning methods as biocuration assistance tools using a publicly available protein database as an example. this database contains information on g protein-coupled receptors (gpcrs), which are part of eukaryotic cell membranes and relevant in cell communication as well as major drug targets in pharmacology. these receptors are characterized according to subtype labels. previous analysis of this database provided evidence that some of the receptor sequences could be affected by a case of label noise, as they appeared to be too consistently misclassified by machine learning methods. here, we extend our analysis to recent and quite substantially modified new versions of the database and reveal their now extremely accurate labeling using several machine learning models and different transformations of the unaligned sequences. these findings support the adequacy of our proposed method to identify problematic labeling cases as a tool for database biocuration."
"sometimes it is possible to be precise about what a model should contain, before the model is created. one can then write a specification and hand it over to a group of professional model developers. this situation can arise when dealing with a logistical or operational question, where there is a great deal of certainty about the system and clarity about what the model should output. much more often, however, the situation to be modelled is complex; the processes to be modelled are uncertain; and the questions to be answered are vague. in such cases, model commissioners need to stay very close to the modelling process, getting involved in an iterative process of deciding what should be included and how it is represented. such models will often produce a range of results and may identify possible tipping points. this is usually the best approach if one is concerned with strategic or policymaking questions; dealing with one-off issues; addressing uncertainty about the consequences of actions; or is unclear about appropriate ways of judging what a system does. in these cases, those involved in the process need to exercise their collective judgement when interpreting the results."
"model building is often out-sourced to consultancies or is the responsibility of specialized teams of inhouse developers. the downside of out-sourcing is that barriers to communication may arise, especially when the commissioner and the developer are in different organizations with different cultures and different priorities. regardless of the development approach and the location of the developers, it is essential that design decisions are logged and the development process is documented (not just the final modelling outcomes). this documentation will be an important input into the model's quality assurance review. it is important to establish, at the start, to whom the resulting model code belongs. some of this will be integrated into the code as comments, but there will also need to be separate documents intended for developers. -"
"for example, fig. 8d shows the top six models identified by chartr using aic and bic as providing the best account of another of the hypothetical observers' data. for this particular hypothetical dataset, many other models provided a better account than the generative model bugmsv. this result highlights two important points. first, some models under some circumstances can mimic each other (i.e., generate similar predictions), which makes their identification in data difficult. second, some models may not be mimicked, but they may require very many data points to reliably recover. we note that these points are not specific to chartrthey are properties of quantitative model selection in general and are an important reminder of the necessary careful steps needed when aiming to select between models [cit] . fig. 8e shows the akaike weights (left panel) and posterior model probabilities (right panel) for the different models averaged over all five observers considered. reassuringly, the most plausible model across the set of observers is the generative model bugmsv for both aic and bic. for aic, the next five best models are all conceptually related to the data generating model. for instance, the next best model was uddmsv which is a ddm with urgency but no gating. the third best model was bugmsvs b which is an urgency gating model with variable intercept."
"although a very simple model might be the work of one person, usually a team of people will be involved, and it is important to be clear about the individuals' roles. there will be at least an owner, or commissioner: the person whose responsibility it is to specify what the model is expected to do, provide the resources needed to get the model built, and sometimes monitor how the model is used. there will be model developers, whose job is to design, build and validate the model; and analysts who will generate results from the model. developers and analysts are often, but not always, the same people. there will also be the model's users: those who have the problem or question that the model is designed to answer. and it is good practice to have a reviewer or quality assurer, someone independent from the team whose task is to audit the model and the way it has been developed to ensure that it meets appropriate quality standards and is fit for purpose-standards will vary according to the importance and risk of the area. each of these roles may be carried out by several people-a large model might need a team of developers, and the review might be carried out by a group of peer reviewers, for example. in all but the most modest models, however, there should be at least one person for each role, because the skills required for each are different."
"finally, a key component of the parameter estimation routine is to simulate thousands of trials for a given set of parameters (i.e., one particle for one iteration of the differential evolution algorithm) and then assess whether the simulated data are in close agreement with the observed data. the speed-up obtained when we halved the number of simulated trials from our standard of 10,000 to 5000 was ∼50% ( fig. 11a-c) . for these hyperparameter settings -lut random number generation, 200 particles for the differential evolution optimizer, and 5000 trials for the simulation of the modelsa full model selection analysis was performed in approximately 16 h."
"the most promising approaches for advancing our understanding of fig. 12 . optimizing computational speed is not detrimental to model selection analysis in chartr. akaike weights averaged over five hypothetical subjects in a model selection analysis with different settings of the random number generator, number of particles, and number of simulated trials per particle for case study 1 (a) and case study 2 (b). for both case studies, chartr reliably identifies the correct data-generating model and in many cases agrees on the second best model for the data. we also note that the exact ranking of the models slightly differ across hyperparameter settings, but the set of identified models is consistent."
"in chartr, we estimate parameters for each model and participant independently, using quantile maximum products estimation (qmpe; [cit] . qmpe uses the qmp statistic, which is similar to χ 2 or multinomial maximum likelihood estimation, and produces estimates that are asymptotically unbiased and normally distributed with asymptotically correct standard errors [cit] . qmpe quantifies agreement between model predictions and data by comparing the observed and predicted proportions of data falling into each of a set of inter-quantile bins. these bins are calculated separately for the correct and error rt data. in all examples that follow, we use 9 quantiles calculated from the data (i.e., split the rt data into 10 bins), though the user can specify as many quantiles as they wish. generally speaking, we recommend no fewer than 5 quantiles, to prevent loss of distributional information, and no more than approximately 10 quantiles, to prevent noisy observations in observed data especially at the tails of the distribution potentially bearing undue influence on the parameter estimation routine."
"argumentation mining, a relatively new area of discourse analysis, involves automatically identifying and structuring arguments. following a basic introduction to argumentation, we describe online debates as a future application area for argumentation mining, describing how we have manually identified and structured argumentation, and how we envision argumentation mining being applied to support these debates in the future."
"sometimes one wants an illustration or visualization to communicate ideas and a model is a good way of doing this. such a model usually relates to a specific idea or situation, and clarity of the illustration is of over-riding importance-to help people see (possibly complex) interactions at work. crucially, an illustration cannot be relied upon for predicting or explaining. if an idea or situation is already represented as a model (designed for another purpose) then the illustrative model might well be a simplified version of this. for example, the dice model (dynamic integrated model of climate and the economy) is a 'simplified analytical and empirical model that represents the economics, policy, and scientific aspects of climate change' [cit] . this is a simpler version of the rice model [cit] that is used to teach about the links between the economy and climate change."
"this usually involves extensive testing and analysis to check behaviours and assumptions in a theory or design, especially which outcomes are produced under what conditions. outcomes can be used to help formulate a hypothesis; but they can also be used to refute a hypothesis, by exhibiting concrete counterexamples. it is important to note that although a model has to have some meaning for it to be a model, this does not necessarily imply the outcomes tell us anything about real systems. for example, many (but not all) economic models are theoretical. they might include assumptions that people behave in a perfectly rational way, for example, or that everybody has perfect access to all information. such models might be later developed into explanatory or predictive models but currently be only about theory."
"perceptual decision-making is the process of choosing and performing appropriate actions in response to sensory cues to achieve behavioral goals [cit] . a sophisticated research effort in multiple fields has led to the formulation of cognitive process models to describe decision-making behavior [cit] . the majority of these models are grounded in the \"sequential sampling\" framework, which posits that decision-making involves the gradual accumulation of noisy sensory evidence over time until a bound (or criterion/threshold) is reached [cit] . [cit] ) ."
"the only difference between aic and bic is the size of the penalty term correcting for model complexity. aic considers false negatives (\"type ii\" errors) worse than false positives and errs on the side of selecting more complex models, and thus can be perceived as favoring \"overfitting\" models. in contrast, bic is more conservative and considers false positives (\"type i\" errors) worse than false negatives and errs on the side of the selecting simpler models, and thus could be perceived as favoring \"underfitting\" models. both are valid perspectives and our opinion is that claiming one is better than the other is not a particularly fruitful endeavor."
"designing and building a model has some of the characteristics of software development and many of the same techniques and tools can be used. there are two basic approaches: either one can attempt to specify in detail what the model should do and then construct it to match that specification; or one can build the model in a much more iterative fashion, starting with a basic model at an early stage and incrementally improving it, meanwhile checking that it matches the users' requirements. these requirements may themselves change as the users improve their understanding of the problem."
"the mg receptors subtype grew by 33% and only 26% of sequences were kept unchanged (\"2011 ∩ [cit] \" column in table 1 ). the cs receptors subtype more than doubled, keeping only 10 sequences unchanged. finally, the taste 1 subtype grew threefold [cit] version it was characterized simply as taste), while the gb receptors subtype, on the contrary, decreased more than threefold."
"the objective of this study was to address this need and provide a straightforward framework to analyze a range of existing sequential sampling models of decision-making behavior. specifically, we aimed to provide an open-source and extensible framework that permits quantitative implementation and testing of novel candidate models of decision-making. the outcome of this study is chartr, a novel toolbox in the r programming environment that can be used to analyze choice and rt data of humans and animals performing two-alternative forced choice tasks that involve perceptual or other types of decision-making."
"visualization of choice and rt data is critical to understanding observed and predicted behavior. such visualization can prove challenging in studies of rapid decision-making because each cell of the experimental design (e.g., a particular stimulus difficulty) yields a joint distribution over the probability of a correct response (accuracy) and separate rt distributions for correct and error responses. since most decision-making tasks manipulate at least one experimental factor across multiple levels, such as stimulus difficulty, each data set is comprised of a family of joint distributions over choice probabilities and pairs of rt distributions (correct, error). [cit] ), we visualize these joint distributions with quantile probability (qp) plots. qp plots are a compact form to display choice probabilities and rt distributions across multiple conditions."
"one concern is that altering the hyperparameters of the estimation routine might be detrimental to model selection. this did not occur in either of our case studies involving simulated data: when using akaike weights, the data-generating model and the next most likely models were broadly consistent even when we used luts for the random number generation, combined luts with a smaller number of particles, or combined luts, smaller number of particles, and smaller number of simulated trials ( fig. 12a and b) . similar results were observed with posterior model probabilities (not shown)."
"last the l 1 adaptive heading controller is tested against wave disturbances, as shown in fig. 9 . as expected the controller limits the use of control authority to counteract the wave motion, while regulating the heading around the desired value."
"there are many ways in which uncertainty can arise. these include: errors in measuring or estimating; inherent chance events in the system being modelled; an underappreciation of the diversity of events in a system; ignorance about a key process, such as how people make decisions; chaotic interactions in the system such that even a small change can switch behaviours into another mode; and the complexity of the model's behaviour itself, which model developers may not fully understand. it is important to consider the uncertainties in the data that underpin a model, and the level of uncertainty that might be acceptable in the model's answers. in addition, there may be considerable uncertainty about the basic mechanisms that are being represented in the model and about whether alternative models using quite different mechanisms might be better. moreover, a complex model can sometimes act as an 'uncertainty amplifier', so that the uncertainty in the results is much greater than the uncertainty in the setup of the model and the data it uses. just as there are different kinds of uncertainty that affect a model, there are different kinds of uncertainty in model outcomes. the answers a model gives might be basically correct, but somewhat prone to a degree of error. in other cases, the outcomes might suddenly vary sharply when the inputs change, or shift from a smoothly changing continuum to an 'on/off' outcome. the kinds of uncertainty in model outcomes affect how it can be used reliably. consequently, it is vital that the uncertainty in a model's results is communicated together with the main results."
"algorithm 1. simulating a diffusion decision model (ddm): z is the starting state, v is the drift rate, a l and a u are lower and upper bounds for the simulation"
"r is an open source language that enjoys widespread use and is maintained by a large community of researchers. chartr can be used to analyze choice and rt behavior from the perspective of a (potentially large) range of decision-making models and can be readily extended when new models are developed. these new models can be incorporated into the toolbox with minimal effort and require only basic working knowledge of r and c programming; we explain the required skills in this manuscript. similarly, new optimization routines that are readily available as r packages can be implemented if desired."
"to illustrate the utility of the toolbox, we provide three case studies where we simulated data from decision-making models in chartr (case studies 1 and 2) or use chartr to model data collected from monkeys performing a decision-making task (case study 3). we use the case studies to demonstrate the typical model estimation and selection analyses. the case studies also provide a modest test of model and parameter recovery. that is, whether chartr reliably suggests that the true data-generating model is in the set of candidate models, and whether it reliably estimates the parameters of the true data-generating model."
"we also investigated factors that influenced computation time for the models. as might be expected, computation time increases as model complexity (number of parameters) increases, though this is not the sole driver of the time required for the model fitting analysis. our parameter estimation approach (qmpe) is based on quantiles of the rt data, meaning that the size of the data set does not influence run speed. this is different to alternative estimation schemes such as maximum likelihood estimation that scales directly with the size of the data set. however, three other factors that we loosely term \"hyperparameters\" increase computational time in chartr: the random number generators, the number of particles used in the differential evolution algorithm, and the number of monte carlo replicates per experimental condition (i.e., number of simulated trials). optimizing these hyperparameters increases the speed of the model fits. below we outline how changing these parameters improves the computational performance of chartr. fig. 9 . model selection outcomes from applying a range of cognitive models of decision-making to data from two monkeys [cit] . (a) and (b) show outcomes from monkeys b and n to compare models with various forms of urgency vs. simple diffusion decision models without urgency. for both monkeys, chartr suggests models with urgency are better candidates for describing the data than ddms without urgency. (c) and (d) show outcomes from monkeys b and n when comparing ugm vs. ddm models. for both monkeys, ugm based models substantially outperform the ddm based models."
"while the process of modelling can greatly increase one's understanding of a problem, the true value of a model only becomes apparent when it is communicated. the communication of model results is an important part of the modelling process: the user interface or visualization is the only contact those not directly working on it will have with a model. a visualization should encapsulate all that is important to know about the underlying model. it must somehow communicate the model's results and (ideally) its assumptions to the intended audience, who may base important decisions on their understanding of the visualization. consequently, even at the scoping stage it is crucial to consider who the user of a model will be, and how they will want to interact with it."
"making educated simplifications and assumptions is an inherent part of the modelling process, as is the presence of some uncertainty in model results. given the compelling nature of well-designed visualizations and user interfaces, it is vital that they do not misrepresent the reliability of the results they communicate, just as an executive summary should be representative of the conclusions and caveats of the underlying report."
"second, creating and using models well involves far more than raw data and technical skills. a close collaboration between model commissioners, developers, users and reviewers provides an essential framework for developing and using an effective model. we have offered a guide to that process, which is vital for building confidence in any model; the checklists in appendix a suggest some questions to aid those developing models and to aid communication between the different actors."
"in a recent study, evans (2019a) analyzed models similar to those in chartr and found that a relatively large amount of time is spent generating random numbers for simulating the models (in particular, sampling the diffusion noise on each time step of each simulated trial). in our c implementation, random number generation is performed using the norm_rand() function, and it is the most time consuming component of the simulation. to reduce the time required for random number generation, evans (2019a) recommended replacing the norm_rand() function of the random number generation process with lookup tables (luts). we implemented the recommended luts and compared them to the speed of our original implementation on the same compute nodes (bu scc, 28 core systems). using luts for random number generation decreased simulation time: for the 37 models we considered, the revised implementation was performed in ∼56 h (compare \"slow\" to \"fast\" in fig. 11a-c) . this is ∼36% faster than the standard implementation."
"we can also identify the misunderstandings that newcomers have about which factors are important, and about what kind of support is necessary to justify claims about whether a factor holds. when an article is unacceptable because it lacks reliable sources, it is not enough to counter that someone will publish about this website when it gets out of beta testing. 3 this newcomer's argument fails to convincingly establish that there are reliable sources (because for wikipedia, a reliable source should be published, independent, and subject to full editorial control), and may make things worse because it suggests that the sources are not independent. rather, a convincing counterargument would explicitly address how the most relevant criteria are met."
"chartr was guided by these pragmatic principles, and is our attempt to provide a practical toolbox that encompasses a range of cognitive models of decision-making. some of the models are grounded in classic random walk and diffusion models [cit] . others incorporate modern hypotheses that decision-making behavior might involve signals such as urgency [cit] ), collapsing boundaries [cit], and variable non-decision times [cit] . since all of the source code is freely available, the toolbox thus provides a framework where models that are proposed into the future can also be implemented and contrasted against existing models. we provide a suite of functions for estimating the parameters of decision-making models, methods to compare loglikelihoods, and calculating penalized information criteria from these different models. finally, the toolbox is developed in the r statistical environment, an open source language that is maintained by an active community of scientists and statisticians [cit] ."
"modelling is changing fast, due to rapid growth in computing power, an explosion in available data, and greater ability of models to tackle extremely complex systems. in the future, there will be a greater need for reliable, predictive models that are relevant to the large-scale, complex systems we want to understand or wish to construct. while larger and more sophisticated models will add to predictive capability, they will also allow us to get a better grasp on the limits to prediction, fundamental uncertainties, and the capacity for tipping points and lock in. some models will work closely with (perhaps be embedded in) operational systems and derive data from them, potentially in real time. these data may come from the many sensors and actuators that are now being added to systems, and we will see new forms of modelling emerge as a consequence. the following offers a glimpse of the changes, challenges and potential rewards over the coming decade."
"this paper presented a solution towards station keeping of an unmanned personal water craft, based on a discrete time l 1 adaptive heading controller. first, a steering model for the pwc in low-speed and reversing regions was identified based on full scale motion data. the identified model showed large parameter variations in response to similar operational conditions. a robust adaptive heading controller was then designed, which combines a baseline pd regulator with a discrete time l 1 adaptive controller. the proposed control architecture exploits the physical correlation between heading angle and turning rate to reduce complexity of the state predictor and the adaptation law. a robustness analysis was carried out, which showed the trade-off between adaptation and disturbance rejection properties. simulation results confirmed the validity of the proposed heading controller for station keeping purposes."
"just as there are many types and techniques, there are also different ways to ask questions and obtain answers from models. often the questions one can ask are fundamentally linked to the modelling technique. one of the most common types of analysis is simulation, usually over a time period, often called 'running' the model. if the model is deterministic, there is only one simulation result; the output of a static model depends entirely on the values assumed for any input parameters. but if the model is non-deterministic (i.e. has a random element) then there are many possible answers-each time you run it you will get a different answer that reflect random elements in the choices or in the environment. if you have such a model it will require many runs to achieve a representative picture of what happens."
"this behaviour suggested that we might be witnessing a case of the label noise problem 27 . this is, the possibility that the sequence subtype labels as appearing in the database, taken to be the ground truth, were actually wrong as table 1 . number of receptors in each subtype for the class c gpcr datasets from the different database versions, including percentages of sequences preserved from one version to the next. receptor acronyms as described in the main text. the last two columns reflect the intersection between different database versions."
-what kind of outputs or results might answer the questions raised? -what format should be used to present the results? -what controls are in place to make sure the model is not used incorrectly?
"thus, our position is that both metrics have utility when a researcher applies chartr to real data. practically, we recommend using both aic and bic for model comparison as a method for identifying a set of likely models. we take this approach in the case studies described below, which leads us to some nuanced conclusions. throughout this paper, and in other papers [cit], we argue that using model selection techniques such as aic and bic to identify a single best model might not be the best approach. rather, we suggest researchers use these metrics judiciously to guide their analyses and ultimately new experiments."
"we can use chartr to derive more insights into the behavior of the monkeys in this decision-making task, by examining whether urgency or the time constant of integration is a more important factor in explaining their behavior. fig. 10 shows quantile probability plots for five models: ddmsvs z s t, a model from the ddm class without urgency but elaborated with variability in various parameters (sv, s z, s t ), two models with urgency and variability in some parameters (uddmsvs t, uddmsvs b ), and two ugm models with variability in parameters (bugmsvs b, bugmsvs t ). as was shown in the model selection outcomes in fig. 9, the addition of urgency dramatically improved the ability of the models to account for the decision-making behavior of the two monkeys."
"the development of the l 1 adaptive control theory has allowed for design of controllers ensuring uniform closed loop transient response over a wide range of parametric uncertainties, while guaranteeing robustness and stability. [cit] took the first steps towards a fully autonomous pwc by designing and implementing two independent l 1 adaptive controllers for cruise and steering control at medium to high speed. this paper proposes a low speed adaptive heading controller suitable for station keeping. the architecture of the l 1 adaptive controller is modified in order to best fit the heading control problem, and the controller is completely and directly designed in discrete time to enable fast embedded implementation in the vehicle. robustness and performance of the proposed heading controller is analysed w.r.t. model uncertainties and disturbances."
"the models chartr ranked 3rd to 6th using both aic and bic were sensibly related to the data-generating model. these models all assumed that observed rts were influenced by factors other than sensory evidence (such as growing impatience), which might mimic the datagenerating model's rt variability that arose due to factors external to the decision-formation process (variable non-decision time). the results serve as an important reminder that model selection should not be used to argue for the \"best\" model in an absolute sense. rather, when considering the collection of the highest ranked models (e.g., models in green and orange in fig. 6a and b) it can be most constructive to rank useful hypotheses/explanations of the data that can then guide further study [cit], which is the approach we have used here. for instance, considering this set of highly-ranked models provides strong evidence that the true decision process involves perfect information integration (as opposed to low-pass filtering of sensory evidence, as in the ugm) and includes variability in non-decision time components, which were both components of the data-generating model. fig. 6d shows the estimated parameter values for the ddmsvs t model. the parameter estimates were very similar to the data-generating values, with some minor over-or under-estimation of the drift rate parameters. this suggests that chartr can reasonably recover the data-generating model and parameters. as above, we also strongly recommend this form of parameter recovery analysis when developing and testing any proposed cognitive model. fig. 6e shows the model selection outcomes from another hypothetical observer. when using aic, chartr again identifies the best fitting model as ddmsvs t and the next best model as ddms t . bic again prefers ddms t over ddmsvs t . a few other models also provided good accounts of the data. as was the case for observer 1, these models predict variability in rts due to mechanisms outside the decision-formation process."
"the ddm with an urgency signal assumes that the input evidenceconsisting of the sensory signal and noiseis modulated by an \"urgency signal\". this urgency-modulated sensory evidence is accumulated into the decision variable throughout the decision-formation process. as the process takes longer, the urgency signal grows in magnitude, implying that sensory evidence arriving later in the decision-formation process has a more profound impact on the decision-variable than information arriving earlier ( fig. 1d ). to make the distinction between an urgency signal and collapsing boundaries clear, the ddm with an urgency signal assumes a dynamically modulated input signal combined with boundaries that mirror those in the classic ddm; the ddm with collapsing boundaries assumes a decision variable that mirrors the classic ddm combined with dynamically modulated decision boundaries."
"the raw qmp statistic, as an approximation to the likelihood, can be used to calculate the akaike information criterion [cit] and the bayesian information criterion (bic; [cit] ) . we provide methods to compute aic and bic owing to the differing assumptions underlying the two information criteria [cit], and differing performance with respect to the modeling goal [cit] b) ."
"in this section we detail the experimental results of the analyses of the three different datasets. we report the classification results obtained using different supervised classifiers for the transformed primary sequences of the proteins applying 5-fold cross validation (cv). tables 2 and 3 [cit] database 24, for the obvious reason that none of the 11 [cit] databases (for a formal description of the misclassification consistency concept, we refer the readers to the methods section). [cit] database reveals that only the sequence h2u5u4_takru, labeled as gb, is misclassified for all 5 data transformations of the present study. nevertheless the prediction of class membership of this sequence is not completely uniform, as it is predicted to belong to ta in 4 cases and to mg in one case. this is, according to uniprot, an uncharacterized protein, i.e. inferred from homology. sequence t2mdm0_hydvu was also detected as frequently misclassified (for 4 out of 5 transformations). this sequence is labeled as mg, but the classifiers predict it to belong to cs. table 6 details the measures employed to analyze the consistency of misclassification of these two sequences."
"where e(t) denotes the momentary sensory evidence at time t; γ(t) denotes the magnitude of the urgency signal at time t. note that with increasing decision time the urgency signal magnifies the effect of the sensory signal (v t δ ) and the sensory noise ( s t δ (0, 1)). the first urgency signal implemented in chartr follows a 3 parameter logistic function with two scaling factors (s x, s y ) and a delay (d), originally proposed by ditterich (2006a, dddm"
"once models are specified, they can be used to generate simulated rts and discrimination accuracy for each condition. simulated data help refine quantitative hypotheses. they also provide much greater insight into the dynamics of different decision-making models and how different variables in these models modulate the predicted rt distributions for correct and error trials [cit] ."
"next consider how we could help participants find weaknesses in others' arguments. automatically listing critical questions might benefit the discussion. critical questions point out the possible weaknesses of an argument, based on the argumentation scheme pattern it uses. listing these questions in concrete and contextualized form (drawing on the premises, inference rules, and conclusions to instantiate and contextualize them) would encourage participants to consider the possible flaws in reasoning and might prompt participants to request answers within the debate. in the authoring process, supplying the critical questions associated with argumentation schemes might also help the author (who could consider elaborating before submitting a message)."
"this paper detailed how automated argumentation mining could be leveraged to support open online communities in making decisions through online debates about rationale. we first gave a basic overview of argumentation structures, describing arguments as consisting of statements, inference rules, and (possibly) attacks. then we described our own work on manual identification of argumentation schemes in wikipedia information quality debates. we envisioned three kinds support tools that could be developed from automated argumentation mining in the future, for authoring more persuasive arguments, finding weaknesses in others' arguments, and summarizing a debate's overall conclusions. open online communities are a wide area of application where argumentation mining could help participants reason collectively."
"-what data are available and how robust are they? -are there judgements about the quality of the data that will need to be made? -how accurate are the available data, and how does that match with the required accuracy of the outputs? -how will each of the assumptions be justified? -what alternative assumptions could be made?"
"evolution of the database. as mentioned in the introduction, the computational experiments reported in this paper concern gpcrs of class c. at the highest level of grouping, class c discriminates receptors as ion, amino acid, or sensory according to the type of ligand. this study covers the evolution of gpcrdb over three versions: [cit] and two recent and drastically changed versions: [cit] . at the second level of classification of the current database version, four subtypes are distinguished: metabotropic glutamate receptors (mg, amino acid), gaba b (gb, amino acid), calcium sensing (cs, ion) and taste 1 receptors (ta, sensory), covering sweet and umami tastes. [cit] version of the database also included three more sensory-related subtypes of the second level, namely vomeronasal (vn), pheromones (ph) and odorant (od) receptors."
"third, the framework in chartr is currently only amenable for analyzing behavior from decision-making tasks where the sensory stimulus provides constant evidence over time, albeit with noise, and varies along a single dimension. however, previous research suggests that a powerful way to dissociate between different models of decisionmaking is to use time-varying stimuli [cit] . in a related vein, there has been increased interest in combining frameworks that posit sensory stimuli are optimally combined and could drive multisensory decision-making models [cit] . future versions of chartr will provide opportunities for implementing and testing models in contexts where the sensory stimuli have temporal structure [cit], or involve multi-sensory integration [cit] ."
"analysis and explanation are just the starting point for the utility of models. they can help us to visualize, predict, optimize, regulate and control complex systems. in the built and engineered world, manufactured products can be simulated as part of the design process before they are physically created, saving time, money and resources. buildings, their infrastructure and their inhabitants can be modelled, and those models can be used not only to maximize the efficiency and effectiveness of the design and build processes, but also to analyse and manage buildings and their associated infrastructure throughout their whole working lifespan. in the public sector, policies can be explored before they are implemented, exposing potential unanticipated consequences and suggesting ways to prevent their occurrence."
"we next used chartr for a preliminary analysis of whether the gating component of the urgency gating model improves model predictions over and above urgency alone. in both monkeys, we found that the data are slightly more consistent with models such as bugmsvs b and bugmsvs t, models that involve urgency and gating with a 100 ms time constant of integration (fig. 10) . these observations provide hypotheses for further analyses of the neural data and further targeted model selection. together, our conclusions would be that urgency is the more important factor. however, there might be a modest role for imperfect integration as well (especially in monkey n)."
"in this final section we discuss computational requirements for a full chartr model selection analysis. fig. 11a -c shows that the average time to estimate the set of 37 chartr models for a single run for a single subject is approximately 88 h. this estimate is based on tests on a node of the boston university shared computing cluster (bu scc, two 14 core 2.4 ghz xeon e5-2680v4 processors, 256 gb ram) for an implementation with 400 particles in the differential evolution optimizer, 10,000 monte carlo replicates per experimental condition, and unoptimized random number generators. we consider this the baseline performance of chartr as it reflects the initial implementation of the code."
"many of the models considered in chartr have no closed-form analytic solution for their predicted distribution. to evaluate the predictions of each model, we typically simulate 10,000 monte carlo replicates per experimental condition during parameter estimation. once the parameter search has terminated, we use 50,000 replicates per experimental condition to precisely evaluate the model predictions and perform model selection. in chartr, the user can vary the number of replicates used for parameter estimation and model selection; in previous applications, we have found these default values provide an appropriate balance between precision of the model predictions and computational efficiency. to simulate the models, we use euler's method, which approximates the models' representation as stochastic differential equations. [cit] or others that use analytical techniques to calculate first passage times [cit], to generate exact distributions. we do not pursue those methods in chartr owing to the model-specific implementation required, which is inconsistent with chartr's core philosophy of allowing the user to rapidly implement a variety of model architectures."
"from these figures, we can directly observe that the self-attention scores focus on distinct events with each head. while the first head appears to focus on the first part of the time series, the second head observes features towards the later time series. also, the attention scores seem to re-distribute features over time. hence, even though hidden feature tensors throughout the network still maintain a temporal dimension, the temporal consistency with the input time series loses context. consistent with the previous experiment in section 8.2, we observed that the attention scores did not focus on the strong positive peaks in the time series which indicate a cloudy observation. hence, we can conclude that self-attention mechanisms are a key tool in suppressing the non-classification-relevant cloudy observations which explain the zero-gradients of the previous experiment."
"measuring the relationship between volume of diabetes care and quality: current approaches the level and volume of care delivered by clinicians plays a major role in the control of diabetes and resultantly in the patient's outcomes as measured by the prevalence and incidence of adverse diabetes-related medical open access outcomes. 7 numerous national and local efforts have been launched that seek to improve the care of patients with diabetes. 8 donabedian's paradigm is often used as the conceptual framework to determine the effectiveness of such interventions. 9 10 this paradigm, a model which serves as a dominant one in the area of healthcare management and administration, suggests that a causal pathway exists in which structural characteristics of the environment such as use of the chronic care model affect processes of care such as diabetes-related screening, which in turn affect patient outcomes."
"this research has a number of limitations. at the first level, while 55 practices were included in the study, these practices were selected based on a convenience sample. that is, a number of primary care practices were contacted and those who voluntarily decided to participate in the study were selected. greater precision could have been introduced into the study by using practice samples that were matched in terms of practice size, patient characteristics and provider characteristics which only differed in terms of diabetes processes of care measures. however, the process of selecting and matching in primary care practice in this way was cost prohibitive."
"indeed, a review of the literature revealed no studies that have sought to export findings from a chronicity such as diabetes care into a framework that examines the implications of the findings for the volume-to-value paradigm. moreover, only a small body of research considers both processes of care and intermediate outcomes as predictors of diabetes long-term value measures such as the rates of death, blindness, cardiovascular disease, amputation and patient's overall physical health per dollar spent. for example, kahn found that a change in a health-related quality-of-life score was significantly associated with the process of care composite (eg, exams, lab tests, diagnostic procedures). 30 this finding therefore alluded to a relationship between diabetes care and diabetes long-term outcomes. however, the inclusion of diabetes with other chronic diseases, non-diabetic medications and counselling in the definition of care introduces ambiguity into the results."
"methods data or study sample this study is based on an analysis of data collected for the evaluation of improving performance in practice, a colorado-based programme aimed at transforming the delivery of healthcare by giving doctors the tools, systems and support they need to provide consistently high-quality care to all patients. [cit] . practices electing to sign up were provided with access to a quality improvement coach, disease registry assistance, participation in a learning collaborative and healthcare administration consultation that was designed to elevate work flow, enhance the use of the chronic care model, increase open access scheduling, generate practice culture change and facilitate electronic medical record conversion. monthly reports of process of care and clinical outcomes data obtained from the registries of diabetic patients at the participating practices were submitted to the study team by the practice managers. however, evaluations of this programme have revealed significant growth over time in the use of process of care measures (volume), but no growth in patient's intermediate outcomes (quality). 34 thus, the study sought to disaggregate the larger pool of data so that the relationship between the volume of processes of care measures and value as measured by patient's intermediate outcomes could be ascertained."
"the label data for this study originates from a joint project with the bavarian state ministry of food, agriculture and forestry (stmelf) and the german remote sensing company gaf ag. this enabled us to obtain two sentinel 2 time series datasets from the same field parcels. one dataset with raw top-of-atmosphere sentinel 2 observations acquired with minimal effort from google earth engine, as described in section 7.2.1, and one preprocessed time series dataset provided by gaf ag which can be considered prototypical for the industry-standard, as outlined in section 7.2.2."
"the approach adopted by most studies that use donabedian's paradigm within the context of diabetes is one which uses structural change as the input, while treating both processes and outcomes as outputs. donabedian's model implies that structure entails process which, in turn, generates outcomes. thus, the assumption is that it necessarily follows that structure is also related to outcomes. moreover, many studies, based on this approach, report significant direct relationships between structures, processes and outcomes. [cit] as one analyses this approach, it is clear that this model also implies that the relationship between structure and outcome may be weaker than originally assumed. thus, it is less than surprising that another group of studies report significant relationships between changes in structure and changes in care. yet, empirical research does not confirm significant relationships between changes in structure and improved patient outcomes. [cit] this suggests that a structural shift from a volume-oriented clinical delivery system may not automatically generate adverse quality changes if decreases occur in the volume of processes of care as valuebased reimbursement approaches become dominant. accordingly, this study focuses on the direct relationship between changes in the volume of diabetes processes of care and quality changes as measured by diabetes intermediate outcomes. few studies have addressed such a relationship and even fewer have corroborated the relationship."
"in this section, we experimentally compare the transformer model, that is based on self-attention, to a recurrent neural network and two convolutional neural networks, as described in section 4. this experimental section is structured in three parts: first in section 8.1, we show quantitative results on all evaluated neural network architectures. table 1 : comparison of models on preprocessed (pre) and raw dataset on the 23-class land use and the 12-class land cover categorization. the values reported are the mean and standard deviation of three models with the best, second and third-best hyperparameter sets trained on the training and validation partitions and tested on the evaluation partition. here, we present results on preprocessed and raw sentinel 2 data described in section 7.2 and on two sets of categories. one 23-class categorization evaluates model performance on land use classification while the other 12-class categorization focuses on land cover. next in section 8.2, we analyze the ability of the models to suppress noise, e.g., induced by clouds, in raw time series data by a feature importance analysis based on gradient backpropagation. finally in sections 8.3 and 8.4, we focus specifically on the self-attention mechanism and analyze activation scores and internal states of the transformer model in detail."
"rather than using a combination of care and outcomes variable, harman used separate patient care and intermediate outcome variables to predict the individual health outcome survey physical and mental health scores of medicare plan enrollees. 31 the intermediate outcome measure was associated with changes in both physical and mental health while the process of care measure was only associated with a change in mental health. 31 this study also suggests a relationship between volume of care and quality as measured by outcomes. however, the non-significant association between care and physical health weakened this connection."
"for the rf model baseline, we augmented the raw reflectance input features by additional spectral indices since feature extraction is usually required for a good performance of this model. hence, we added the features normalized difference vegetation index (ndvi), normalized difference water index (ndwi), bridghtness index (bi), inverted red-edge chlorophyll index (ireci), and enhanced vegetation index (evi) to the time series data."
"in addition, the percentages are based on differentials in the number of data collection points per composite measure. this is because while based on a simple average the 55 practices participated in the programme for 13.9 months, the range for the period of participation spanned a period of 1-42 months. additionally, tremendous variability characterised adherence to the recommended processes of care across the 55 practices. stated in the language of the volume-to-value model, some of the individual practices in the sample primarily reflected a volume of diabetes services that was, on average, far below the levels for each measure that are traditionally associated with high quality in diabetes care."
"descriptive statistics (means, sd, maximums and minimums) were computed for practice characteristics. the date at which the practice diabetes registry was fully operational and stable was determined from the coaches and constituted time zero. a multilevel model with a time-varying covariate partitioned into a between-practice and within-practice effect 35 was estimated using sas proc mixed v.9.2. the equations used were as follows:"
"in summary, the feature extraction capabilities learned by this self-attention network is consistent with assumptions based on domain-knowledge of the particular classes. methodologically, we observed that features from deeper cascaded network layers get increasingly distinctive and field parcels of the same label are mapped in similar embedded regions. we would like to stress that this type of analysis is not unique to self-attention networks and can be reproduced for many deep learning architectures that extracts features in cascaded layers 6 ."
"interestingly, the random forest baseline achieved competitive results to the deep learning models on preprocessed time series data. it appears that preprocessing, as a form of feature extraction, helped the random forest classifier to improve its performance significantly. without data preprocessing on the raw dataset, the random forest baseline fell massively behind the deep learning models. the overall accuracy on pre-processed data was only 9 % worse which hints toward a generally accurate classification of the most frequent classes. the kappa metric-that ranged 0.15 worse in the 23-class land use variant and 0.1 to 0.2 worse in the 12-class land use variant-indicates that infrequent classes were classified less accurately by the random forest classifier. this got further confirmed by the poor f 1 score throughout both dataset variants where the random forest achieved worse performance on raw datasets compared to preprocessed datasets. overall, this stresses the necessity of feature extraction for shallow machine learning methods while demonstrating that-with sufficient manual effort in preprocessing and, thus, feature extraction-random forests can achieve competitive accuracies."
"conclusions despite the limitations of this study and the intensity of the need for additional research, this research, nevertheless, is important. it demonstrates the criticality of exporting the focus of healthcare administration into clinical practices. the need for such integration will be even greater"
background to develop a statistical tool that allows practitioners and/or their practice managers to easily select the relevant range in which volume and value are maximised. methods data for the study were based on 55 [cit] . we used two composite variables including the volume of processes of care variables listed in diabetes practice guidelines and value (quality) as measured by changes in the intermediate outcomes.
"as mentioned, 55 practices with an average of 13.9 months and a range of 1-42 months of data were included in the analysis. when aggregated, the data consisted of 775 data points per composite measure. the average number of diabetes patients (age 18-75 years) seen per month across all practices was 195.1. however, tremendous variability existed in the size of these practices. for example, the smallest practice saw only 4 patients per month while the largest practice treated 1206 patients per month. it is for this reason that the data described in table 1 are based on weighted means, sd, and minima and maxima of the practices' average individual and composite volumes of process of care, as well as patients' intermediate outcome variables weighted by the number of patients served. table 1 reveals the wide range of variation in processes of care and intermediate outcomes between the participating practices. for example, the practices had a weighted average of 49.2% on the intermediate outcome composite variable, but generated a range that extended from 21.5% to 69.4% relative to the patient outcome composite. the sheer variation in service volume and patient outcomes revealed by the descriptive data in table 1 suggests a need to assess the relationship between the volume of care delivered by these 55 practices and quality as measured by the patient's intermediate outcomes."
"yet, improvements in care do appear to affect outcomes. however, on average, two units of care in this study was the volume of care that was required to produce a singleunit change in intermediate outcomes. stated differently, diminishing returns in outcomes suggest that higher volumes of care may be required in order to sustain the same level of quality. this suggests that pricing strategies that are independent of volume or which seek to reduce the volume of healthcare services while improving value may be inconsistent with the health outcomes production function for chronic disease areas such as diabetes."
"the overall patient care mean, 57, was subtracted from the between-subject effect to provide a meaningful zero (a value in the range of data) to aid in the interpretation of the intercept. time was not included in the model as a fixed effect because previous analyses of these data have not shown a significant trend over time relative to intermediate outcomes. 21 consequently, the focus of this analysis was within practice fluctuations in intermediate outcomes rather than systematic change in intermediate outcomes. 36 the intercept and the effect of within practice patient care were allowed to vary randomly. an autoregressive-moving average (arma (1,1) ) structure was selected as the best fitting variance covariance structure in the unconditional model and served as the base model to which the process of care effects was added. 37 open access results as table 1 reveals, data from 55 practices were included in the study. as this table reveals, the average number of patients with diabetes seen per month varied from a low of 4 patients to a high of 1206. however, because the patient outcome composite represents weighted rather than simple means as well as the means of means, the accompanying sd, and minimum and maximum percentages across the 55 practices do not reflect traditional horizontal and/or vertical totals."
"the gradients ∂ ∂θ l are averaged over a batch of size n, while the learning rate µ determines the step size. this optimization scheme guarantees that the chosen parameters θ are optimal for the observed dataset and objective function."
"we focus on three spatially separate regions in bavaria, as shown in fig. 3 . these regions are hollfeld in upper franconia, krumbach in swabia, and a northeastern portion of the bavarian forest. these regions are located in approximately 100 kilometers distance from each other. while the climate is comparatively similar, different elevations and soil conditions favor differences in the distribution of cultivated crops, as can be observed in the class distribution histograms in figs. 3b and 3c."
"to get spatially separate partitions [cit] for training of model parameters using gradient descent, validation of hyperaparameter sets and final evaluation of the model, we divided the three regions further into rectangular blocks of 4500 by 4500 meters with a 500 meter margin between blocks, as shown in the train-test split in fig. 3a . these blocks were randomly assigned to training, validation, and evaluation partitions in a 4:1:1 ratio. we decided for such a block-wise spatial separation in order to enforce independence of the dataset partitions without implicit overfitting, as experimentally evaluated and observed in previous work [cit] and as recommended for geospatial data by further studies [cit] focusing the implicit bias of spatial auto-correlation."
"the findings of this study are relevant to many audiences. in a non-volume-to-value environment, the implications are quite simple. the methods of this study could be adopted by researchers and programme evaluators who compare health plan performance and present it to the public. consumers who use information of this type will benefit from knowing that providers with better care also have better outcomes. therefore, report cards that identify providers with better care will become more helpful to patients in selecting care providers. for practitioners, this statistical approach can quickly confirm that their efforts to improve care are effective. as a result, it may also provide evidence that can motivate them to make even better improvements in care in expectation of even greater benefits to patients in terms of outcomes."
"the hyperparameters of the rf models were determined through a random search on 300 runs with three-fold cross-validation. to reduce tuning time and the class imbalance in the data, we used only up to 500 samples per class."
"the purpose of this study was to determine how changes in the 'volume' of processes of care services, that is, asking about tobacco use, developing self-management goals and timely testing of feet, eyes, a1c, ldl and nephropathy, affect intermediate outcomes such as controlled blood pressure, a1c and ldl at the practice level in a diabetes patient population. it also explores the implications of the findings for the volume-to-value transition process. specifically, the study sought to answer the two following questions: ► do practices with a higher volume of processes of care, on average, have 'better' patient clinical out-open access comes, on average, using a chronicity such as diabetes as the case study? ► do practices exceeding or dropping below their average processes of care volume during a particular month also exceed or drop below their average 'quality' as measured by patient clinical outcomes in a month?"
"it may be concluded that the region-and domain-specific expert knowledge in data preprocessing helped all models to achieve better accuracies compared to the raw sentinel 2 time series dataset. all models achieved similar accuracies for preprocessed datasets. even the random forest classifier showed a competitive performance to the deep learning models with similar scores in overall accuracy, slightly worse kappa and f 1 scores. both, the lstm-rnn and transformer models, that rely on recurrence and self-attention, achieved better accuracies on raw time series data compared to the convolutional models ms-resnet and tempcnn. this effect was minor in the land-use categorization with 23-classes and more pronounced in the land-cover categorization, but overall consistent throughout this evaluation. we will investigate the difference of recurrence and self-attention compared to convolution further in the next section."
"conceptually, the between-practice effect can be explained as the effect of particular practices being generally higher or lower than other practices with regards to patient care. in comparison, the within-practice effect can be explained as the effect of a particular practice having a better or worse volume of patient care than usual in a particular month. in econometrics, these deviations from the average are referred to as shocks, innovations or errors. these terms are applicable in this context as well since the deviations may be the result of innovations such as quality improvement efforts or shocks such as loss of key personnel in the practice, and/or theoretically, efforts to reduce volume in order to prevent financial losses."
"as is known, the treatment of persons with chronicities is now a dominant proportion of the services offered by primary care physicians. thus, other studies are needed that test this statistical approach in other disease areas. in addition, studies are needed that assess the degree to which findings from the use of this volume-to-value apps would actually drive change in the behaviour of providers and their peer staffs. in addition, case study research is needed that focuses on the intricacies involved in generating volume/value trade-offs in a single medical practice. therefore, while this study was based on data from an actual diabetes intervention with the 55 medical practices, the focus of the intervention was not volume/value trade-offs. thus, more research is needed that explicitly addresses this matter. the future studies can also be stratified by age and other variables in order to determine subgroup differences (eg, healthcare disparities) in volume/value trade-offs."
"for this experiment, we estimated the influence of each input time step on the classification prediction for each of the evaluated networks, i.e., lstm-rnn (recurrence) and transformer (self-attention), as well as ms-resnet and tempcnn (convolution). figure 5 illustrates this by means of two separate examples of two corn-and summer barley parcels. the top figures each show the input time series x as a sequence of raw sentinel 2 [cit] . in the raw time series, we can identify atmospheric noise and clouds as positive peaks in the data. these are caused by the high reflectance values of clouds throughout all spectral bands. the presence of a cloud at a given point in time does not provide any additional information about the covered surface and should be, thus, considered irrelevant for the classification. the following rows display the gradients ∂y * ∂x for each of the respective models. these plots indicate the influence of the measurement at the particular input time in the top row on the classification prediction of the respective model."
"this formulation is prone to vanishing and exploding gradients through time [cit] that inhibited the extraction of features from long-term temporal contexts. while the effect of exploding gradients could be controlled through gradient clipping, vanishing gradients were addressed by the introduction of additional gates. this lead to long short-term memory (lstm) recurrent networks [cit] that introduced four internal gates"
"satellites observe the earth's surface in regular temporal intervals. often, data is provided free-of-charge to the public through initiatives like esa's copernicus program. for instance, the sentinel 2 multispectral satellites acquire data at up to 10m resolution in 13 spectral bands every two to five days. [cit], an enormous amount of 7.76 tib 1 sentinel 2 data was published on a daily average. still, only 7.6% 2 [cit] were actually downloaded [cit] . this means that 12 out of 13 published images remained unused. similar figures can be drawn for the sentinel 1, 3 and 5 missions. this is a clear sign that, despite claims in academia and industry, methods and principles of big data analytics are hardly applied to their full potential in the field of earth observation. the reasons for this low exploitation ratio are manifold. first, visual inspection of satellite images is still often the first step of data acquisition. intermediate results must often be visually interpretable to be controlled by domain experts. preprocessing steps, like atmospheric correction or manual or automatic cloud filtering, are ubiquitous in remote sensing. all of these processes require computational resources or visual inspection that scales poorly when applying methods at dense temporal or global spatial scales. this demonstrates the demand for methods that utilize the entire body of available satellite data and, thus, requires minimal supervision by experts with region-specific expert knowledge. in principle, deep learning mechanisms are well-suited to approximate preprocessing-like mechanisms by jointly learning feature extraction and classification within one neural network topology using gradient backpropagation. developing models that do not strictly require extensive data preprocessing are likely a key contribution to utilize all available published satellite data accordingly in the future. in the scope of this objective, we evaluate three deep learning mechanisms on four deep learning models on selectively available preprocessed and readily available raw satellite data."
"an alternative to lstms are gated recurrent units (grus) [cit] that follow the same principle of additional gates, but are parameterized with fewer gates and less weights. a empiric evaluation [cit] did not show a significant difference in performance between these parameterizations."
"but these findings are even more relevant to volumeto-value policy advocates. this study demonstrates that while the volume of processes of care are very mutable they have direct effects on relatively immutable patient outcomes. this suggests that as the volume-to-value movement progresses, analyses such as this one are needed in virtually every single chronic disease area so that the guesswork is removed from volume-to-value payment methodologies and any volume/quality trade-offs made will be quantitatively explicit. if constructed as a calculator app, this approach will allow practice managers to quantify the volume-to-value trade-offs that are being made at any single point in time."
"it is noteworthy that the overall accuracy measure, as presented in tables 1c and 1d, over-represents frequent classes. since the datasets used for this study show a heavily imbalanced class distribution, the accuracy of frequent classes dominates this metric. nevertheless, it is an intuitive measure and a good representation of the example-wise accuracy, representative of the visual impression from observing a spatial map classification. to account for the less frequent classes, we also report cohen's kappa metric [cit] in tables 1a and 1b . this is a correlation score that is frequently used in remote sensing and normalizes the classification scores by the probability of a random chance prediction based on emperical class frequencies. as a further measure of performance, we report the f 1 score, i.e., the harmonic mean of precision and recall, for each class and average over all classes in tables 1e and 1f . by doing so, all classes get equally weighted disregarding the the number of samples per category. the class-mean f 1 scores are generally lower than accuracy and kappa, as we chose a large set of classes where some classes semantically overlap or only have few examples which make it difficult for a data-driven neural network to learn feature extraction and decision function. overall, we aimed at comparing different properties of the classification models. hence, the overall accuracy reflects the classification accuracy per field parcel, while the f 1 score measures the accurate classification of all classes. table 1 reveals that data preprocessing had a positive effect on the accuracy, f 1 score, and kappa of all models. it seems that the manual supervision during preprocessing improved the classification performance. this better performance on pre-processed time series is, to a certain degree, expected, since this preprocessing makes a visual identification of classification relevant events possible, as could be seen in fig. 4b . comparing the visual examples at 4 suggests that the classification of the preprocessed datasets is an easier task than approximating preprocessing-like mechanism jointly together with classification in a holistic end-to-end model. overall, the region-and domain-specific expertise needed to design such preprocessing pipelines increases the classification performance compared to classification of the raw datasets were the deep learning models have to learn preprocessing-like mechanisms solely based on the provided examples without access to model knowledge. this holds especially true if the number of samples is limited, some examples are falsely labeled, or the semantic representation of classes do overlap. considering this, it is notable that the lstm-rnn and transformer models performed competitively well on raw data compared to preprocessed data. especially in the 12-class land cover categorization setting, the difference in performance was rather minor, speaking of 5 % in accuracy and 0.03 in kappa score. in terms of the f 1 score, the lstm-rnn variant even achieved better performance on raw data compared to preprocessed data, while is ranging behind the transformer. throughout both the 23-class dataset (cf. tables 1a, 1c and 1e) and 12-class dataset (cf. tables 1b, 1d and 1f) variants, all evaluated deep learning models performed similarly well on preprocessed data. interestingly, for the raw dataset partitions, the lstm-rnn and transformer models achieved slightly better accuracy, kappa, and f 1 scores values compared to the ms-resnet and tempcnn variants. in the 23-class setting, the difference is rather small with 1-2 % in overall accuracy and 0.01 in kappa score which seems not significant considering their reported variances. the 12-class case, however, confirms this observation with a more pronounced difference of 0.03-0.07 in kappa metric and 5 % and 11 % in accuracy. in the next section 8.2, we will investigate this further by a feature importance analysis monitoring the backpropagation process."
"the common agricultural policy of the european union subsidizes farmers based on the type of cultivated crop. each member country is required to gather geographical information of the geometry of the parcel and the type of crop. this information is provided by obligatory surveys as part of the subsidy application process directly from the the crop label categories provided for this study were provided by stmelf. they follow a long-tailed class distribution with over 269 distinct categories. here, the most common 15, 26, and 62 categories cover 90%, 95%, and 99% of the field parcels, respectively. in cooperation with stmelf and gaf, a set of land-use and land-cover categories was aggregated and selected with respect to the aims and objectives of the ministry. from this aggregation, we selected two labeled datasets: the first contains 23 classes, as shown in fig. 3b, which resemble the land-use of the parcels. these categories cover, for instance, multiple types of grassland. this dataset is a challenging to classify, since multiple categories (e.g., grassland for cattle-use and for machining) share similar surface reflectance features measured by the satellite. we also aggregated the categories further a second dataset which focuses on 12 land cover categories, as shown in fig. 3c . by evaluating models on two 23-class land-use categorization and 12-class land cover categorization, we aimed at reporting model accuracies from two differently difficult objectives."
"a second limitation is that the study does not address phase number issues. that is, given the developments of an app that includes a value-to-volume statistical calculations, how difficult could it be to train practice managers in its use? how much individualisation would be needed to allow the tool to be used on a patient-by-patient basis? thus, this article only addresses stage 1 of the process of introducing tools to support greater precision in marketing volume-to-value trade-offs."
"we utilized data from the optical sentinel 2 satellite constellation which consists of two satellites that orbit the earth on a sun-synchronous orbit on opposite tracks. these satellites observe the same spot on the earth's surface every 2 to 5 days, depending on the latitude. the data is gathered by a line-scanner at 13 spectral bands ranging from ultra-violet wavelengths, for capturing atmospheric water vapor, over optical and near-infrared wavelengths, sensitive to chlorophyll and photosynthesis, up to short-wave infrared wavelengths, which are sensitive to soil moisture. in regions where the sensor stripes overlap, we observed approximately 140 [cit], while on the stripe centers we only recorded 70 observations within the same time range."
"greater insight can be gained, of course, by disaggregating the process of care and intermediate outcomes data and assessing the degree of variability within the individual participating practices. open access the total data points equalled 775 across the samples for five of the eight processes of care variables, the data reveal that the provision of services involving self-management goals and influenza vaccinations were less likely to have been reported in any given month. while some practices fell below their own measures relative to a1c and ldl, testing influenza vaccinations and self-management goals were even more likely to exhibit variance from the mean. even more importantly, however, table 2 summarises the range of within-practice deviations from the practice mean in intermediate outcomes and processes of care. for instance, in a given month, an individual practice could be, on average, as much as 37.4 percentage points below their usual average or as much as 24.2 percentage points above their average on the intermediate outcomes composite. similarly, a practice could be as much as 47.7 percentage points below their process of care norm and as much as 22.4 percentage points above it. unravelling the rationale underlying such variations becomes highly critical in a volume-to-value world. do these variations represent patient-driven effects or are they provider-driven changes in service volume that occur as a response to changes in the shift in payment methodology? even more importantly, are these variations in the volume of process of care services reflected in the intermediate outcomes of the patients with diabetes served by each practice? stated differently, 'do volume of services decreases adversely affect quality as measured by intermediate outcomes in diabetes care?' table 3 reports these findings."
"for further evaluation, we had access to a second dataset which originates from the same publicly available topof-atmosphere data products. in contrast to the raw data set, this data was processed by gaf through their tested and operastional preprocessing engine. this process includes common preprocessing techniques, such as, e.g., atmospheric correction, temporal selection of cloud-free observations, a focus on observations of the vegetative period, and cloud masking. we show an example of the preprocessed dataset in fig. 4b that shows the identical parcel as fig. 4a . here, the cloudy observations have been identified and filtered by a separate cloud classification model. this makes this dataset easier to classify by shallow models, as distinctive phenological features, i.e., onset of growth and cutting of the meadows, can be visually distinguished. overall, this preprocessing pipeline can be considered prototypical for an industry standard but requires significant computational and design effort to generate."
"recently, the temporal convolutional network tempcnn [cit] has been proposed and evaluated specifically for crop type mapping. it is a comparatively light-weight architecture of three sequential 1d convolutional layers followed by batch normalization, a relu activation function, and dropout. the encoded features are flattened and passed to a final fully connected layer with batch normalization, relu activation function, and dropout, as shown in fig. 2d . these features are then projected to scores per class using a final fully connected layer and softmax."
"when comparing the f 1 scores in tables 1e and 1f with the accuracy scores in tables 1c and 1d on raw data, we see that the convolutional tempcnn model showed similar performance compared to the ms-resnet model. the f 1 scores, however, show a lower score for tempcnn. from these metrics, we can derive that the tempcnn did classify the majority of field parcels accurately but achieved lower accuracies on some of the more infrequent class categories compared to ms-resnet. this may be attributed to the shallower network topology."
"in the past decade, diabetes processes of care and intermediate outcomes improved in the usa. however, improvements in intermediate outcomes were weaker and were often not statistically significant. 38 the results of this study reveal an approximately half a percentage point increase in outcomes per unit increase in care. this finding implies that slower growth in intermediate outcomes relative to processes of care is to be expected."
"in the previous section, we experimentally observed that the transformer model was able to suppress the influence of an observation for classification-irrelevant observations, e.g., clouds. here, we concentrate on the self-attention mechanism (cf. section 3.4) that was implemented in the transformer model and analyse the attention scores on the example of the cornfield parcels from the previous experiment in shown in fig. 5b ."
"we attribute this difference in the influence of input observations to the respective mechanisms for feature extraction. recurrent networks utilize internal gates that can control the influence of the particular time instance to a hidden memory state. our previous work [cit], which visualized internal lstm states, supports this hypothesis for recurrence. similarly, self-attention enables a model to select specific observations by assigning a large attention score to them. in contrast, convolutions always extract features from a local neighborhood. hence, it seems to be more difficult for convolutional architectures to ignore sudden appearences of irrelevant observations within a temporal sequence, as indicated by the non-zero gradients at cloudy observations."
"level the intermediate outcome composite measure was treated as the response variable, and the process of care composite variable was treated as a time-varying covariate partitioned into a between-subject and within-subject effect. the between-subject effect (process of care composite. i ) was an individual practice's average across all time points on the process of care measures. the within-subject effect was the difference between a practice's average process of care and the practice's process of care measure in a particular month (patient process of care composite ti -patient process of care composite. i )."
"in this section, we introduce the notation throughout this work and provide background on convolution, recurrence and address self-attention before employing these mechanisms in four neural network topologies in section 4."
"we compared the performance of the deep learning models lstm-rnn [cit], transformer [cit], ms-resnet [cit], and tempcnn [cit], as well as a random forest (rf) classifier as shallow baseline. we determined the optimal hyper-parameters separately for preprocessed and raw sentinel 2 time series datasets, and for the 23 and 12 class categorizations, as described in section 6. for each experiment, we trained and evaluated three different models with the best, second-best, and third-best hyperparameter configuration and random seeds for parameter initialization and composition of the training batches. in table 1, we present the mean and standard deviation of the accuracy metrics from these three results. all models were trained and evaluated on block partitions for training and evaluation (cf. 3) in all three regions hollfeld, krumbach and bavarian forest. we evaluated these models on raw and preprocessed satellite time series on identical field parcels."
"for the raw sentinel 2 dataset, we utilized the top-of-atmosphere reflectances of the processing level l1c. this data was acquired from google earth engine [cit] and queried for each field parcel individually. pixels located within the boundaries of a field parcel were mean-aggregated into a single feature vector of 13 spectral bands at each time. we show examples of a meadow parcel of the raw sentinel 2 time series in fig. 4a . note that cloud coverage, visible as positive peaks in the reflectance profiles, dominates the signal and makes a visual interpretation of this time series difficult. this time series dataset is challenging to classify but can be acquired at minimal effort. hence, we benchmarked our models on this harder objective."
"for example, table 1 reveals that some of the participating practices reported that 0% of diabetes patients had received foot exams, a1c tests, ldl screening, influenza vaccinations or self-management goal-setting support during the period of the study. however, the aggregated data were not separated by period of participation in the programme. thus, these low-service volumes may reflect the administrative fact that the period of participation in the initiative was lower for some practices than for others. again, it is important to restate that the means, sd and ranges reported in the tables are not simple means. rather, they are averages that are weighted by the number of patients served by each practice. additionally, they are means that reflect the aggregate of monthly weighted means."
"the previous section provided an overview on neural network layers used for temporal feature extraction and introduced temporal convolution, recurrence, and self-attention. in this section, we describe four neural network topologies that each use one of these layer mechanisms and will be evaluated experimentally in the following 3 ."
"in the future, a robust classification of raw time series data without region-specific expert knowledge will be key to quantitatively exploit the satellite data that is published daily. we hope to have contributed a step towards this direction by this comprehensive study that evaluated model accuracies on preprocessed and raw data for a variety of mechanisms for time series classification."
"this finding is further reinforced by the within-practice analysis. the coefficient of the practices' mean process of care composite variable (β 10 ) answers the question, 'do practices exceeding or dropping below their average volume of processes of care during a particular month also exceed or go below their average \"quality\" of intermediate outcomes in that month?' the coefficient of this term reveals that in a particular month, for each unit that a practice exceeds their average 'service' volume as measured by the process of care score, their intermediate outcome composite score increases by 0.48. in other words, the volume of care offered by each of the participating practices directly improved their quality as measured by the intermediate outcome composite scores."
"to summarize, we saw that self-attention is a promising technique that allows neural networks to extract features from specific time instances with raw optical satellite time series. it suits-well in the canon of time series classification models that utilize recurrent or convolutional layers. we did not find any mechanism that systematically achieved better accuracies, but observed that self-attention and recurrence were, by design of the feature extraction, more robust to noise in the data and could better suppress cloudy observations in raw time series."
"we evaluated the mechanisms from section 3 that are implemented in the models of section 4 on the task of crop type identification. crop type identification is a field of land cover and land use classification where the model has to extract classification-relevant features and learn a discriminative decision function to separate the classes of vegetation. we analyze the extracted features later in section 8.4. vegetation life cycle events, known as phenology, provides a distinctive temporal signal to identify types of vegetation using a limited set of spectral channels. this makes the temporal signal a key source of relevant features when learning a discriminative model to differentiate various types of vegetation and thus well-suited to test the mechanisms and models of this work."
"in this work, we quantitatively and qualitatively analyzed self-attention for the application in multi-temporal earth observation. we performed a large-scale quantitative comparison in section 8.1 where we evaluated multiple model architectures that rely on self-attention, recurrence and convolution, and a random forest baseline. we compared these models from multiple angles by reporting their performance on preprocessed and raw sentinel 2 time series as well as land-use and a land-cover oriented set of classes. here, we observed that all models performed equally well on preprocessed data. even the random forest baseline achieved competitive overall accuracy. this leads to the conclusion that the choice of model architecture is less critical when extensive data preprocessing is utilized as a form of feature extraction aided by region-specific expert knowledge. for raw unprocessed sentinel 2 time series data, the transformer, and lstm-rnn architectures were able to achieve better accuracies compared to the convolutional models. we investigated this further by a feature importance analysis in section 8.2 using gradients where we observed that the mechanisms self-attention and recurrence helped to suppress non-classification-relevant observations in the time series. for the transformer, this was realized by learning weights in the attention mechanism which enables the model to specifically focus on some observations, as could be observed in fig. 1 in section 8.3. finally, we looked at the larger transformer network topology and observed how deeper neural network layers were able to learn increasingly separable representations of the classes in section 8.4. since we chose a challenging set of classes, class overlaps in the semantic representation of classes with common properties were present and followed common intuition. this is a typical challenge for land cover and land use classification with long-tailed class distributions."
"as described in the 'methods' section, a unique multilevel analytical approach was applied in order to more explicitly analyse the relationship between volume variations and quality outcomes as reflected in the intermediate outcomes. the model intercept of 49.48 (β 00 ) can be interpreted as the patient outcome score for an average practice during an average month or more specifically the expected value for a practice with a patient care mean of 57, during a month when they are performing at their average. the coefficient of the practice mean patient care variable (β 01 ) answers the question, 'do practices with better processes of care on average have better intermediate outcomes on average?'according to this model, on average, for every additional unit or higher volume of processes of care provided relative to other practices by each practice across time, the intermediate outcomes, on average, increased by 0.45. more specifically, for every unit above the overall practice average process of care composite score of 57, the intermediate outcome score is 0.45 units higher. accordingly, the degree of linearity between the volume of care and the intermediate outcomes in diabetes care suggests that volume to quality payment methodologies must be very carefully crafted in order to simultaneously achieve volume reductions without quality of care decreases. open access"
"as a result, the risks of financial loss that the volume-to-value paradigm poses for clinicians who treat persons with chronic health conditions can be reduced. real-world data on providers who treat patients with diabetes are used to demonstrate this approach."
"this raises the question whether a systematic and constructive approach could be established to exploit superradiance for the design of artificial quantum systems. such design capabilities could overcome the limited resources accessible in state-of-the-art experiments, and thereby enable more advanced level schemes required, e.g., for the exploration of non-linear and quantum effects at x-ray energies."
). this finding represents a unique feature as it is independent of the actual realisation and provides a signature suitable for a direct experimental test.
"is fulfilled. this criterion is a necessary condition for the emergence of superradiance and represents bounds on the allowed power laws of the coupling terms (exponent α in equation (2)) as a function of the lattice dimension d. for the remainder, we assume that eq. (3) is satisfied. the enhancement factor χ max is (see table 1 (2)). we show that the resulting collective eigenstates can be utilised for the implementation of an artificial transition with tunable decay rate and transition frequency."
"in conclusion, we have studied single-photon superradiance in extended media, and showed how superradiance can be engineered in such a way that an artificial optical transition with tunable decay rate and level shift is realised. this result provides the basic building block for a systematic approach towards engineering advanced artificial quantum systems via superradiance by design. a promising avenue for future studies is the extension of our work to coupled sub ensembles with the goal to design artificial multi-level atoms 13 ."
"here, we address this question by developing an analytical framework for superradiance in extended media encompassing different system dimensionalities, interatomic couplings, and environments. as our main result, we then derive expressions describing how collective decay rates and frequency shifts can be controlled in extended media, and show how they can be used for the design of an artificial optical transition."
"from a broader perspective, our results also enable us to understand how superradiant states from different realisations can be compared and categorised. this is important, e. g., if superradiant ensembles realised using different individual constituents are to be combined to an effective artificial quantum system. to this end, suppose that we can control the atom number and the volume such that"
"equations (6) and (7) also offer means to design an artificial optical transition with desired decay rate and frequency shift. in fact, the enhancement factor χ max represents a characteristic scale for both decay rates and frequency shifts. as expected, we find that the particle number n and/or the sample volume  can be used to control χ max . but additionally, eqs. (4) and (5) explain how the dimensionality d, the type of the inter-atomic coupling as described by α, as well as the coupling strength to the environment can be used to manipulate the enhancement factor. this is of particular relevance, since these parameters could also be tuned in situ 10, 30 . however, as mentioned previously, these quantities are not sufficient to change the ratio between decay rate and frequency shift. this only becomes possible by also controlling the wave number k (see fig. 2 ). experimentally, the wavenumber could be adjusted via the excitation angle of the probing light field."
". further, in eq. (2), we have not included exponential damping of the form exp(− k 0 r/), where  denotes a dimensionless absorption length that, for instance, empirically accounts for material imperfections. such a damping factor in the integral in eq. (14) would lead to a denotes the angle between the eigenstate's wavevector k and the x 3 axis."
"where k represents the number of hidden neurons and w j represents the connection weight from the j th hidden neuron to the output neuron. the real-valued mlp is adaptable for scalar data processing especially. the complex-valued mlp can treat twodimensional signal elements as a single entity. and the qmlp can handle three-channel signals in a whole manner, which preservers the interrelationship information. however, qmlp has a series of computational burden and large data redundancy, because of non-commutativity of the quaternion multiplication. the qmlp has been recently explored to naturally represent high-dimensional information, such as color and three-dimensional geometric signals, by a quaternionic neuron, rather than complex-valued or real-valued neurons."
"manufacturing of pharmaceutical and biopharmaceutical products is subject to standardized quality systems regulated by the good manufacturing practice (gmp) rules [cit] . mesenchymal stromal cells (msc) represent cell therapy products that under the european union regulation [cit] are classified as advanced therapy medicinal products (atmps). consequently, their production must take place according to gmp standards. the quality control department of a medicinal product manufacturing plant has the aim to guarantee the quality of the product that relies on the evidence of a clear relationship between accurate measurements and critical quality attributes of the product such as safety, identity, purity, and potency. these issues are well described in specific guidelines of european medicines agency (ema) [cit] . safety derives from the demonstration that the product does not contain adventitious agents: bacteria, fungi, and viruses as well as any other components that might represent a hazard for the patient who will receive it; the identity of the cellular components ensures the presence of the active substance and may consist of phenotypic and/or genotypic profile definition; purity demonstrates that the cell therapy product contains at high concentration the active substance and is free from other unwanted cell populations, as far it concerns the desired therapeutic effect. lastly, potency assay measures the required biological activity in the final cell product, in relationship with the mechanism of action in general or for any defined clinical purpose."
"it is much more challenging for an atmp quality control department to validate noncompendial analytical methods (those methods that are not included and described in the official ph. eur.), especially in terms of identity, purity, and potency. in addition to the limited availability of appropriate standards and reference material, the lack of specific monographs and guidelines makes the validation work even more difficult in this field."
"quality controls of atmps are a much jeopardized issue: there are few paragraphs of the pharmacopoeia dedicated to cellular products (e.g., the microbiological controls) while atmps are generally poorly represented in the official documents. that is why one of the most demanding and challenging operating field of the persons involved in quality control is to adapt compendial method to the atmp setting or to validate noncompendial methods."
"the instrument was set up for the acquisition protocol by manual calibration that was performed by preparing a working standard solution (wsts), consisting of 50,000 msc (stmsc) mixed to 50,000 k562 cells (stk662). seven tubes were prepared as follows: unstained wsts, wsts stained with anti-cd90 fitc (bd), wsts stained with anti-cd105 pe (bd), wsts stained with anti-cd90 pe-cy7, wsts stained with anti-cd105 percp-cy 5.5, wsts stained with anti-cd73 apc, and wsts stained with anti-cd45 apc-h7 (bd). the voltages and the compensation settings were verified with stmsc+stk562 stained with the combination of the chosen antibodies mixed together (cd90 pe-cy7, cd105 percp-cy 5.5, cd75 apc, and cd45 apc-h7). in this analysis, adjustments were made to ensure that no false staining occurs in the dual-color quadrant for any individual fluorochrome. after setting color compensation, the analysis protocol was defined with precise histograms and gating scheme. the samples for the validation step were analyzed in this protocol with no further adjustments."
"this validation was performed on three batches of cbmsc and on three batches of bmmsc with the purpose to verify if any component of the matrix in which the final product is resuspended has antibacterial activity and may therefore interfere with the results of the test. the matrix solution for cbmsc as a cryopreserved product is made of normal saline, hsa, and dmso at the concentration described above, while the bmmsc as fresh product was resuspended in hsa and normal saline (5% vol : vol)."
"new frontiers in the application of lal to atmps are represented by the development of alternative methods for complex tissue-engineered products or combined products (cells in combination with biomaterials). in this field, alternative methods have recently been proposed such as cell-based assays that are able to detect material-bound microbial contaminations not detectable with lal test [cit] or immunofluorescent staining assays to evaluate the endotoxininduced expression of e-selectin. the latter method could give information also regarding the localization of bacterial contamination sources in all steps of the manufacturing process of tissue-engineered product for human use [cit] ."
"the architectures of real-valued mlp, qmlp and the proposed rga-mlp models are mainly similar and composed of an input layer, a hidden layer, and an output layer, where qmlp model treats signals as quaternion numbers, and rga-mlp model treats signals as multivectors in rga space."
the results obtained (table 3) were satisfied for all the three dilutions of the product. we can conclude that the product was not interfering and the lal test was valid. it was decided to use the lowest dilution of 1 : 30 in the next phase.
"in our laboratory, we decided to validate an automated method that allows the count of global and viable total nucleated cells (tnc) in comparison to burker chamber in terms of accuracy, linearity, and precision (repeatability and intermediate precision)."
"so, the aim of this paper is to give a clear explanation of how we designed validation of compendial and noncompendial methods to determine safety (microbiological determination, bacterial endotoxins, and adventitious viruses), identity, and purity (cell count and immunophenotyping) for quality control of gmp msc, requested as release criteria for earlyphases clinical trial."
"the 3d geometrical shapes dataset is formed, in which 16000 samples are obtained in 4 class. patterns of the same style belong to the same class. each class contains different patterns which are generated by various geometric transformations, such as the rotation of the first pattern with different angles and translation with different vectors around the origin. figure 4 shows the example 3d geometrical shapes with 4 classes including 4 patterns for each of class, used to verify the performances of the rga-mlp model."
"in this section, the color images classification experiments with traditional real-valued mlp model, qmlp and rga-mlp models are carried out based on the cifar-10 dataset. in this experiment, 2 classes of color images in cifar-10 are used, 6000 [cit] color images of each class for testing. the learning rate of the both models are fixed at 0.04 and the three models are trained for 3600 iterations while the batch size is set to 50. since three color channels r,g,b are treated similarly as input, the labels with respect to three channels are set to the same,which ensures that the three channels (γ 1, γ 2, γ 12 ) are treated equally with the same priority. therefore, the interrelationship among the three color channels can be extracted more effectively using rga theory."
"in this paper, we have proposed an extended multilayer perceptron model for multi-dimensional signal processing and presented an error back-propagation algorithm for its learning scheme in rga space. taking advantage of rga theory, the multi-dimensional signals are represented as rga multivectors, multiple channels treated as a single unit instead of the separate components. and all elements and operators are extended into rga domain, exploits strong ability in capturing the inherent structures and significantly acts as an efficient and simplified network with formidable training ability. the experiments demonstrate that our proposed model can achieve higher classification accuracy, faster convergence rate and lower computational complexity. avenues for future work includes the combination of the rga-mlp model with other different network and more excellent networks designed for multi-dimensional signal processing."
"where x, y represent two vectors, x ·y and x ∧y represent inner product and outer product, respectively. and properties of the geometric product are given as follows"
"validation means in this context the successful demonstration of manufacturing and quality consistency, and it is the action of providing that any process, procedure, method, or activity actually and consistently fulfill specific requirements. in particular, according to international conference on harmonization q2 (ich q2 r1) guidelines [cit], validation of each analytical method is required with the purpose to demonstrate that the procedures and the test adopted from the quality control laboratory are suitable for the intended use, so they are appropriate to give results in terms of quality attributes, as described above. a validation activity is generally composed of four steps: (1) qualification of personnel and equipment used as prerequisite for all the operations; (2) description of the validation strategy in written and approved validation protocols; (3) performance of the validation experiments; and (4) collection of the results and considerations in a validation report [cit] . the validation protocol should clearly define the roles and the responsibilities of each person and element involved in the validation performance, such as equipment, supplies, reagents, reference materials and standards and, above all, the validation parameters and the acceptance criteria that guarantee the fulfillment of the validation specifications. the ich q2 (r1) guidelines define the following parameters that should be considered for validation: accuracy, precision (repeatability and intermediate precision), specificity, detection limit, quantitation limit, linearity, and range."
"regarding the decision to remove interfering factors by treating the product, we decided not to make any manipulation in order to test the most representative sample of the released final product with an acceptable final dilution. our results were consistently supporting this choice, but it is indeed possible in the case of sample interference, to adopt alternative approaches, that may consist, for example, in heating the sample or modifying the ph of the media. in this case, it may be possible to test also a lower dilution."
"for preliminary test on the interfering factors, the product was tested at different dilutions, in order to identify the most suitable noninterfering dilution. the chosen sample dilutions were 1 : 30; 1 : 90; and 1 : 180."
"the training loss curves for the three network with different number of hidden nodes are ploted in figure 5 and the results illustrate that the rga-mlp model outperforms the traditional real-valued mlp. it can be seen that multi-dimensional signals processed by real-valued mlp tends to be single channel independently, which are not rich enough to preserves more discriminative information among multiple channels. in contrast, the proposed rga-mlp model is capable of capturing the inter-relationship information between different channels, such as the scaling and the rotation of inputs in multi-dimensional space, this information is mort important for classification."
"(iv) detection limit (limit of detection, lod): the lowest amount of analyte in a sample which can be detected but not necessarily quantitated as an exact value."
"x li w ij (7) where f represents the activation function and is the connection weight from the i th input neuron to the j th hidden neuron. then, the final output value from the output neuron is computed as"
"the rest of the paper is organized as follows. section ii reviews the related work, including the basics of multilayer perceptron (mlp) and geometric algebra (ga). section iii describes the basics of rga. the structure of rga-mlp model and the rga-version of back propagation algorithm is introduced in section iv. in section v, the classification experiments are implemented to validate the superiority of the model. section vi concludes the paper."
"the operations of addition and subtraction in l n space are almost the same as l 4 in ga space, here we just deduce the multiplication operation in"
"the accuracy calculated as accuracy error (difference between the mean of values from two operators by burker chamber and mean of values of single nucleocassette by nucleocounter) was within the range of the acceptance criteria −5/+5 (0.07/0.36 as minimum and maximum e a for automated total cell count and −0.02/0.29 for automated viable cell count). in particular, we obtained the lowest discrepancy between the manual method and the automated method for viable cell count: mean accuracy error for viable count 0.11 versus 0.21 for total cell count (figure 4(a) in table) . these results demonstrate that the operators are well trained in performing the manual method according to our written sop and are able to distinguish the cells that are visually intact from those that show \"signs of death\" as damaged cell membrane."
"the theoretical results about rga have been obtained, it is clear that computational complexity is reduced based on rga, multiplication in l n space is divided into two multiplication in subspace l n 0 and l n 1 . for instance, in l 4 space, computational complexity is given as table 1, the computational complexity of rga is nearly half of ga. however, in practice, the computational complexity of the proposed rga-mlp model can be attributed to the parameter estimation of β 1 and β 2, which is over half than ga."
"more precisely, as shown in figure 7 and figure 8, the rga-mlp model outperforms the real-valued mlp and qmlp model with the faster rate of convergence under smaller number of iterations, that is to say, the proposed rga-mlp model has the formidable ability in approaching a stable value more rapidly. moreover, it can be clearly concluded that the proposed rga-mlp model achieves a more stable convergence trend and lower training losses while the real-valued mlp model and qmlp model fluctuate greatly."
"3.5. immunophenotyping validation. msc immunophenotypic characterization is fundamental for the identification of the cell product before clinical application. the lack of specific and distinct cell surface markers and the heterogeneity of the characterization studies led, more than ten years ago, the international society for cellular therapy (isct) to publish the minimal criteria for defining msc [cit] . in addition to plastic adherence and in vitro differentiation potential, it was defined that msc are characterized by the expression of cd105, cd73, and cd90 and lack of expression of hematopoietic and endothelial surface markers such as cd14, cd45, cd34, cd11b, hla-dr, and cd31. currently, these criteria are still used as accepted standards to define msc for clinical application. flow cytometry represents the most widely used method for immunophenotypic analysis also in gmp settings, as it allows a fast, multiparametric analysis of a cell suspension. nevertheless, as some authors have already underlined [cit], assessment of the analytical measurement as sensitivity and linearity for the validation of this method is affected by the lack of cellular reference materials and the difficulty in obtaining adequate controls (e.g., cell lines with varying levels of a given marker expression)."
"multi-dimensional signal processing is an important issue for artificial neural networks. there does not exist efficient models for multi-dimensional signal processing.for traditional real-valued mlp models [cit], a single neuron can take only one real value as its input, thus several neurons are used for handling multi-dimensional signals in a network, which is sometimes unnatural in practical applications to engineering problem, such as image processing. as for color image, it is processed in an independent manner, in which the color image has been represented just as three gray-scale images to apply the real-valued mlp model for three times, and, the correlation of the three color channels has been lost inevitably."
(v) linearity: the ability (within a given range) to obtain test results which are directly proportional to the concentration (amount) of analyte in the sample. we calculated it by considering the correlation coefficient r square (r 2 ) between 1 and 0.9.
"it is clear to see that the mathematical relationship of geometric product between the two vectors can be converted calculate two parts, one is a scalar part x · y, the other is a bivector part x ∧ y, which is neither fully anti-symmetric, nor fully symmetric. multivectors are viewed as the the basic elements in ga space, extending vectors to higher dimensions."
"for 3d geometrical shapes dataset,we split 3d geometrical shapes into 4 classes, where 3000 shapes and 1000 of each class are used to training and testing, respectively. the network for 3d geometrical shapes is a three layer network, which has one hidden layer. models are optimized with learning rate set at 0.04. the training ends at iteration 7200."
"it is important to notice that recently specific gmp guidelines for atmps have been published [cit], and for the first time, a distinction between investigational atmps (at least in the early experimental clinical phases) and authorized atmps (products that have reached the marketing authorization) is stated. as concerning the first class of products in this document is clearly declared that full validation of analytical procedures is not required, but demonstration of the methods' suitability may be sufficient, whereas validation is expected for clinical atmps in advanced experimental phases. in our experience [cit], risk assessment should always drive the atmp developer choices, and based on this approach, we chose to validate all the methods whose results are used to release investigational atmps. we are indeed convinced that only with an accurate, specific, and precise method it is possible to be confident of the results that can support the knowledge of our cellular products and so the way towards its authorization."
"(ii) accuracy: the closeness of agreement between the value which is accepted either as a conventional true value or an accepted reference value and the value found. in our noncompendial validation analysis, accuracy can be expressed as follows:"
"(vi) range: the interval between the upper and lower concentration (amounts) of analytes in the sample (including these concentrations) for which it has been demonstrated that the analytical procedure has a suitable level of precision, accuracy, and linearity."
"sterility has always been one of the major and most critical test for atmp release. in particular, the time to complete the analysis may be an issue for atmp product with a short shelf-life. in this regard, the paragraph 2.6.27 included in the previous edition of ph. eur. might have been incompatible with the need to release this kind of short-living atmp product, thus making this issue suitable for being considered as a parametric test [cit] . as we will more extensively comment below, the same paragraph 2.6.27 in the more recent edition of ph. eur. aims to facilitate the use of sterility analytical methods by including alternative approaches and reducing the incubation period, but time still remains an issue for those products that must be released immediately after the completion of the manufacturing process (fresh products). in our experience, some products, such as cbmsc, are cryopreserved before use and so the result of sterility is always available before release. other products, such as bmmsc, must be released as fresh products and requires alternative approaches for validation and testing. for cbmsc as well as other cryopreserved products, the validation study was designed with the aim to verify if any of the cryopreservation solution components (dmso, normal saline, and human albumin) could interfere with the detection of microorganisms. the method selected was the direct inoculation of the sample into test media as described in ph. eur. chapter 2.6.27 that fits specifically with cell products while the membrane filtration method described in ph. eur. chapter 2.6.1. may present difficulties when applied to cells. the main challenge of this validation was to define the most representative sample of the final product, in terms of volume and conditions (fresh versus cryopreserved). regarding the volume to be tested, we decided to assimilate our product to hematopoietic cell preparations for which the ph. eur. prescribes to inoculate 1% of the total volume for a final product volume greater than 10 ml. considering that the number of cells/volume of each cbmsc batch will vary in our manufacturing process (from 300 to 1000 ml), we decided to fix the volume as the maximum one that can be obtained in a standard manufacturing process (1000 ml). for that reason, for validation purposes, we tested 10 ml of final product for each microorganism strain. regarding the condition of the final product, since cbmsc are cryopreserved and must be thawed before clinical use, sterility testing was validated on a thawed retention sample contained in a cryopreservation bag as the final product. for routine quality controls, the retention sample must be thawed and tested for sterility within three weeks from the completion of manufacturing process and also in the validation study, the same time schedule was followed."
"in this section, the classification performance of the proposed rga-mlp model is evaluated on two datasets, compared with the traditional mlp model in the real and quaternion domain, quantitatively and visually."
"source and manufacturing. according to annex 13 of the gmp guidelines [cit], all the validation methods were performed with reference or retention samples of the final product that were represented by msc from cord blood (cb) and bone marrow (bm). briefly, the starting material, cb or bm, after quality control analysis was introduced in our class b-gmp facility and was seeded in alpha modified eagle medium (macopharma, mouvaux, france) supplemented, respectively, with gamma-irradiated foetal bovine serum (fbs) of australian origin (gibco, life technologies, carlsbad, ca, usa) or platelet lysate (institute für klinische transfusionsmedizin und immungenetik ulm gemeinnützige gmbh, ulm german), at the concentration of 50,000 total nucleated cells (tnc)/cm 2 in culture chamber system (corning, lowel, ma)."
"validation. lal evaluation is the most sensitive and specific test currently available to detect and measure bacterial endotoxins, defined as \"pyrogens\" as they induce fever and other adverse reactions caused by inflammatory mediators."
"the optimal number of hidden nodes is explored as follows. firstly, the qmlp and rga-mlp model is trained with 100 and 150 neurons, respectively. then, the real-valued mlp model followed with the same setting. we train the realvalued mlp model with a increasing number of hidden nodes until it reached the performance of the rga-mlp model. table 2 shows the test errors of the rga-mlp model could not be reached by the real-valued mlp using 100 hidden nodes, both of them are roughly set the same number of parameters. it is important to note that more neurons within the hidden layer lead to a better performance as well, which means more features will be achieved. altogether, the proposed rga-mlp outperforms than the other methods in terms of classification accuracy, which can better capture the geometrical structure of 3d geometrical shapes."
"for bmmsc that are released as fresh product, the tested sample for validation was composed by pure bmmsc in a solution made of normal saline and human albumin. the volume chosen for the inoculation with each microbial strain was 0.1 ml, 1% of the total volume that is 10 ml."
"for quantitatively comparison, test errors obtained by color images classification experiments performed by the realvalued mlp, qmlp and the proposed rga-mlp models are listed in table 3. the second column denotes different topologies in hidden layers ranging from a single layer net with 100 output neurons to a net with two hidden layers each with 100 and 50 neurons, respectively. expectedly, the proposed to be more precise, the proposed model treats color images as rga multivectors with the strong ability in capturing the relationship of color channels. furthermore, a more simplified network and a powerful learning algorithm are presented in the proposed rga-mlp model, thereby superior performance is achieved compared with the traditional real-valued mlp and qmlp models."
"despite being an important issue for the gmp production of atmps, in the literature, there are few papers regarding specific validation strategies [cit] with very different approaches."
"the recent revised ph. eur. chapter 2.6.27 entitled \"microbiological examination of cell-based preparations\" [cit] takes into account the characteristics and the limitation of these preparations, as their shelf-life, that if not cryopreserved, ranges from hours to few days [cit], as well as sample composition (in some cases, the cell-based preparation itself can inactivate contaminating microorganisms resulting in a false negative) and/or the sample size (the total volume of a batch could be less than 50 ml, thus limitating the sample size). in this regard, the main changes to previous version concern a greater flexibility for the incubation temperature(s), a change in the list of microorganisms to be tested (yersinia enterocolitica is replaced by micrococcus luteus that is more appropriate as it is a common contaminant of cellbased preparations), and information about the sensitivity to be achieved during validation has also been included. several authors have already been demonstrated that, for example, automated sterility testing is capable of rapidly detecting low-level contamination, with an average of 2.5 days [cit] and within 48 hours [cit] for different biopharmaceutical and transplantation products (e.g., pancreatic islets). we are also validating a rapid sterility testing with the aim to demonstrate that it is accurate, sensitive, and specific (results not shown)."
"for both cbmsc and bmmsc, the results obtained in the validation studies met the preestablished acceptability criteria and specifically (i) the growth of the inoculated microorganisms was observed in the presence and in the absence of the cell product (positive controls) for all the three validation runs, thus indicating that the product does not possess intrinsic antibacterial activity; (ii) no microbiological growth was observed in the negative controls both in the presence of the product and with culture media only (specificity); (iii) the limit of detection was 1-10 cfu, as requested, with and without the product. in particular, it was possible to detect the microorganism with the lowest quantity inoculation that was 2 cfu and the growth of microorganisms was observed by seven days of incubation; and (iv) the samples analyzed by two different operators on two different days had the same growth (intermediate precision)."
"the computational complexity of qmlp and rga-mlp models is evaluated in this section. the computation time for qmlp and rga-mlp models with two different datasets are listed in table 4. the elapsed time for the rga-mlp model is less than the elapsed time for qmlp model. thanks to the commutative multiplication rule of rga, the computational time of the rga-mlp model is nearly the twothirds of qmlp with lower computational complexity. the experiments have verified the effectiveness of rga-mlp model, where the correlation among channels in multidimensional signals can be preserved well and the network has been simplified using rga theory. moreover, this kind of computational workload reduction does not influence the classification results."
"algebra (ga) is introduced by william k. clifford, also known as clifford algebra, which gives geometric insight and effective representation for multidimensional signals. particularly, ga is regarded as a powerful framework for different geometric entities, offering a potential tool to solve many tasks connected to information science [cit] . technically speaking, ga subsume, for example, the real numbers, the complex numbers and the quaternions."
"in order to study the effects of different number of hidden layers on the performance of networks, we adopt different number of hidden layers of three networks in this experiment. figure 7, figure 8 and table 3 show a clear improvement in both the convergence rate and the test accuracy when increasing the number of hidden layers for each network."
"in order to confirm that the chosen dilution did not have any interference, the test was repeated by three different operators on three batches of product, and for all the experiment session, the acceptance criteria were met. finally, we calculated specific sensitivity of the test (pss) as follows:"
"the strategy and the acceptance criteria for the methods to detect microbial contamination in pharmaceutical products (microbiological examination, bacterial endotoxin, and mycoplasma) are described in the european pharmacopoeia (ph. eur.). the aim of their validation is to determine if a specific product contains substances that may interfere with the results of the analysis. since atmps for their nature are not inert products, appropriate considerations and adaptation strategies are required, in regard to their clinical application, to design an accurate validation study."
"the cifar-10 dataset consists of ten classes of objects with 6000 color images in each class. for each class, 5000 images are used for training and the rest 1000 images are kept for testing. and the size of all color image in this dataset is 32 * 32, which means 1024 pixels for each color image. each of pixel represents a color value (24 bits), describing the three components for color images including red, green and blue (denoted by 8 bits). more details of the cifar-10 dataset can be found at http://www.cs.toronto.edu/ kriz/cifar.html."
"(i) specificity: the ability to assess unequivocally the \"analyte\" in the presence of components which may be expected to be present. typically these might include impurities."
"multilayer perceptron (mlp) is an efficient feed-forward neural network, constituting one of the most common and popular classes of neural networks for image processing and pattern recognition [cit] . it consists of several subsequent layers which is of perceptron-type, including an input layer that simply obtains the external inputs, a set of hidden layers and one output layer. this model of neural network is known as a supervised network, which requires a desired output for learning. it is worth noting that mlp is applied to create a model that correctly maps the input to the output with historical data, therefore when the desired output is unknown, the model can be able to produce the output successfully. mlp is trained using back-propagation learning algorithm, which serves to minimize the squared error between the network outputs and the desired ones. then, this is used to adjust weights error, back-propagating minimum squared error to the neural network."
"when solving the equations in (22), the values of the individual components in (21) are correspondingly yielded, but not all the elements of l 4 are conjugate."
"samples spiked with hev were performed with or without proteinase k treatment, to exclude inhibition of the rna extraction due to treatment at 56°c (needed for proteinase k)."
"all the experiments in this paper are performed using matlab under the condition of intel(r) core(tm) i5-6500 3.20ghz cpu and 8 gb memory, windows 7. we choose sigmoid as the activation function, and the rga-version has been given in figure 2. the training error and the test errors are evaluated on the three networks for comparison experiments."
"the protocol was designed to validate an automated method for msc counting by \"nucleocounter®\" system (chemometec, allerod, denmark) in terms of accuracy, precision, and linearity in comparison to the manual cell count method by the hemocytometer (burker chamber). nucleocounter is a portable device based on integrated fluorescence microscope principle that allows to count total and viable cells stained with the propidium iodide (pi), immobilized inside the charger nucleocassette. reference samples of msc (three batches of cbmsc and three batches of bmmsc) were resuspended in a volume between 1 and 20 ml of pbs, in order to test different concentrations of cells. as shown in figure 1 for each cell suspension, two different samplings were counted in duplicate for total and dead cell. for the first, the cells were pretreated with a buffer of lysis (chemometec), in order to allow the pi to stain all the cell suspension. the cell stock solution was then serially diluted (1 : 2-1 : 4-1 : 8-1 : 16) and counted with the two methods."
"inspired by the recent progress of ga based models in various fields of multi-dimensional signal processing, and the advantages of rga theory, we present an extended multilayer perceptron model using reduced geometric algebra, which treats multi-dimensional signals as multivectors in rga space, and simplifies the computation of the networks. all elements of proposed rga-mlp model including inputs, outputs, activation function and operators are extended to the rga domain with commutative multiplication rules. we also present a rga-version of back propagation (bp) algorithm to train the network. thus, the proposed rga-mlp model is capable of achieving the state-of-art performance with lower computational complexity for multi-dimensional signal processing."
"where * is set as the conjugation and η is a constant denoting the learning coefficient. in the case of the update of the o-th neuron in the output layer, is given by (35) while of the h-th neuron in the hidden layer is given by"
"validation of instruments, supplies, and reagents and personnel qualification. according to gmp guidelines [cit], validation of instruments, supplies, and reagents have been performed as already described [cit] . briefly, the instruments were subjected to installation qualification (iq), in accordance with the manufacturer specifications and to operational qualification (oq). reagents upon receipt were properly checked against specifications and recorded. authorized gmp staff for quality control department follows a continuous training program. duties of the personnel involved in quality control procedures and quality control manager's responsibilities were clearly described in written analytical protocols and in job description, as requested by the gmp guidelines."
"to assess sensibility, accuracy, and precision, three different batches of cbmsc were charged with a viral load tenfold over the sensitivity cutoff declared by the manufacturer, 200 and 1000 copies for adv and hev, respectively, and 85 and 95 copies for cmv and ebv, respectively. for the recovery calculation of the genome equivalent (geq) copies in the sample, it was not possible to apply the formula indicated by the datasheet (built for plasma samples), so results were expressed as quantity of cmv and ebv target dna (gequ/reaction) that was present in the sample reaction and in positive control (consisting of viral nucleic acid alone). the quantity parameter was calculated by comparing ct values of each sample and the standard curve. for respiratory viruses, being the kit qualitative, it was not possible to demonstrate a quantitative recovery. two negative controls were performed: negative control of extraction, by processing water under the extraction conditions, and negative control for amplification, by putting water in the mix for amplification. precision was assessed within technical replicates and within the three batches of cbmsc."
"as an important prerequisite of our validation process, we first defined and applied written procedures to set up the instrument that included the titrations of the antibodies for msc staining and the creation of a specific acquisition protocol with fixed fluorescence settings and compensation (data not shown). in order to assess our ability to detect positive markers (to determine the purity of msc) and hematopoietic markers (to detect impurity), we chose to spike msc with different concentrations of a hematopoietic line (k562) that has similar dimension/scatter to msc. to simplify the analysis, we decided to take into consideration the two positive markers for purity (cd90 and cd105) and a negative marker (cd45) for impurity."
"for each method, the validation strategy was described in detail in the validation protocol that reported the chosen ich q2 (r1) parameters [cit], the type of analysis, the number of runs and replicates, the formulas used for calculation, the acceptance criteria, the instruments, the operators involved, and the time schedule for the completion of the validation study. all the results and the analysis were recorded in a report that is approved by the responsible of quality control (rqc) department. if the validation criteria were not met, the rqc managed this condition as a \"noncompliance,\" identified and corrected the causes for failure, and rescheduled the validation activities by issuing a new plan."
"is the connection weight between neurons in the two layers, θ denotes the bias of the neuron and is also represented in l 4 . the function g(·) denotes a nonlinear activation function for neurons, here we adopt the sigmoid function and give its rga version"
"considering both clinical protocols, here, we reported the validation of lal on cbmsc that represents the \"worst case\" as it has the lowest el and the most complex cell matrix (including dmso)."
"according to the specific gmp guidelines for atmps [cit], potency assays are expected to be validated prior to pivotal clinical trials. potency regards the relevant biologic cellular function, and it could be influenced by many variables as the donor variability and cellular population heterogeneity as immunogenicity, senescence, and resistance to cryopreservation that may affect their effectiveness in vivo [cit] . moreover, this variability, together with the uncertain mechanism of action and the lack of reference standards, makes the validation strategy difficult to develop. some groups have already addressed this important issue [cit], and their works are very precious to open an \"arena\" of discussion in order to improve the quality profile of atmps, thus fostering their reliability as effective and innovative therapeutic tools."
the associate editor coordinating the review of this manuscript and approving it for publication was guitao cao. the learning problem of multilayer perceptrons formulated as the problem of minimizing a smooth error function.
"as always, neurons are the atoms of neuron network, multidimensional input and output are interpreted as tuples by rga neuron with a manner of a left-side (right-side) weight association, which is generally inferior to spinor neuron in terms of two-sided weight association. however, spinor neuron is only meaningful for non-commutative algebras leading to more complicated computational complexity than that of the single-side one. it is fortunate that rga is commutative to exhibit the same great performance as spinor neuron, while the network is simplified with lower computational complexity."
"where n θ is defined in eq. (2) andn θ is the number of projections needed if using the multi-slice method with m sections. equivalently this can be thought as δθ m δθ, which indicates that the crowther criterion is reduced by a factor equal to the number of sections [cit] ."
"where p θ u is the fourier transform of p θ r and ru is the ramp filter juj. for an imaging system of transverse resolution or pixel size of δr and an object thickness of t, the required number of projections to achieve 3d isotropic resolution δr is given by"
"here we introduce a modified regridding method, gridrec-ms, that allows for direct tomographic reconstruction using the multi-slice depth information. an overview of the method is given in fig. 2 . a regridding technique [cit]"
"where denotes the convolution operation,ru x, u y is a 2d filter which serves a similar normalization purpose as the ramp filter for conventional tomography, and cr x, r y, the inverse fourier transform of cu, w, effectively compensates the effect of the kernel. equation (5) can also be written as"
"of this interpolation map are added as ones to a mask that will later be used for normalization. for every point ofp θ u, w such interpolation is performed. the process is repeated for all angles, resulting in a fourier space that is covered by all multislice projections as well as a mask for normalization. the fourier map qu x, u y is then divided by the mask, expressed as a multiplication byru x, u y in eq. (6), to normalize the contribution from each projection. subsequently, a 2d inverse"
"fourier transform is performed and a correction matrix in real space, cr x, r y, is applied to remove the effect of the interpolation kernel. the intensity of the correction function is shown in fig. 2(b) . for simplicity, these equations are given in continuous space while a discretized version was implemented."
"illustrations of conventional tomography with no depth information and multi-slice tomography are given in figs. 1(a) and 1(b), respectively. for conventional tomography in a parallel beam geometry, the problem can be simplified to mutually independent reconstructions of 2d disks perpendicular to the rotation axis. hence, from here on we focus on the reconstruction of these 2d disks from 1d projections. a 2d disk of an object is denoted as f r x, r y on the cartesian grid and its fourier transform as f u x, u y . for a given rotation of the object, θ, the object can be expressed as f r, z where z is the beam propagation direction, r r x cos θ r y sin θ and its fourier transform f u, w. these real-space and fourier-space coordinates are illustrated in fig. 1 . in conventional tomography, a projection measured at θ is expressed as p θ r r ∞ −∞ f r, zdz. using the fourier-slice theorem [cit], projections at multiple angles can yield a reconstruction, as illustrated in fig. 1(a) by"
"we have demonstrated gridrec-ms for direct multi-slice tomographic reconstruction, i.e., using depth-resolved projections. reconstructions using projections with m sections are comparable to those with conventional single-slice fine sampling, demonstrating that the crowther criterion [cit] can be relaxed by a factor of m compared to the regular sampling requirements. in other words, the number of sections, m, influences the angular sampling. we have also shown that even with sparse angular sampling, the reconstruction can still benefit significantly from the depth information. in practice the multislice sectioning, provided by the imaging method upstream to gridrec-ms, may not be perfect. if such imperfections are not considered, they can affect the quality of the reconstruction. for example, some methods show a dependence of the sectioning efficacy with respect to the lateral spatial frequency, in which low-spatial-frequency features leak to neighboring object sections, leading to a transfer function with a support in the shape of a bowtie [cit] . the method presented here can be adapted for this situation by a modification ofru x, u y, i.e., changing the fourier mask. in general, it can be advantageous to combine techniques that model the imaging system to extend the dof or obtain sectioning with tomographic methods. for imaging systems for which the depth information can be recovered, we expect that gridrec-ms can provide a robust and general reconstruction framework that is readily applicable for any contrast mechanism, including phase and absorption contrast, as well as any probing wave, including electrons, x-rays, and optical photons. the gridrec-ms code can be made available upon reasonable request. fig. 3 . the 1-bit threshold is shown in red. in both cases, multi-slice reconstructions (purple) show great improvement over the conventional method (blue) and slightly worse results compared to single-slice with fine angular sampling (yellow)."
"with specific experimental techniques, image processing, or physical modelling, the depth information can in some cases be extracted from a projection of the object to give virtual slices or virtual sections of the object, here referred to as a depthresolved projection, shown in fig. 1(b1) . multi-slice methods exist in the optical [8, 14, [cit] and x-ray regimes [9, 10 ] that obtain such depth sectioning. such a projection has a dimension of m by t, where m is the number of sections and t the object diameter, and is expressed aŝ"
"is the result of interpolating multi-slice projections,p θ u, w, onto the cartesian grid u x, u y through cu, w. the choice of the interpolation kernel is crucial as functions with a compact support both in real and fourier space provide higher accuracy [cit] and high computational efficiency. here we use a separable form of the kernel, which gives cu, w cucw, where cu is formed by prolate spheroidal wave functions (pswfs) of zeroth order [cit] . for efficient computation and storage, expansions of pswfs [cit] in terms of legendre polynomials are used here [cit], giving"
"the human operator performed the sweep task three times. by collecting data in the process of demonstration, the demonstration data are presented in figure 7a -c. clearly, the completion time of the sweep task was inconsistent for each demonstration. accordingly, the demonstrated trajectory curves were mismatched in the time scale for demonstrations. the dtw algorithm was employed to normalize the demonstration data in the same time scale. figure 9d, the value of the learned muscle stiffness in starting point is 0.1502."
"we also performed classification using a combined set of \"12 centrality measures\", as described in methods. classification using this feature set resulted in an auroc of 0.830 and a precision of 0.317 as can be seen from table 1 . further, we find that most of the centrality measures are significantly higher for essential proteins. to evaluate the significance of centrality measures, we compared the mean of each measure for essential genes with randomly (sub-) sampled non-essential genes and computed a p-value. this bootstrapped p-value is computed as the fraction of iterations in which the mean centrality measure for a random sub-sample of non-essential genes is greater than or equal to the mean centrality measure of essential genes."
"nevertheless, our proposed set of features can be derived for any organism containing both sequence and interactome information such as those in string [cit] . the extracted features can be used to predict essential genes in any organism lacking experimental information on essential genes."
"across-organism methods are particularly interesting since they help in utilising prior information from all the available essentiality studies conducted on different organisms; extract the universal set of features and transfer it to new organisms. essential genes are effectively transferred in closely-related organisms since they share a lot of orthologous genes. however, the number of organisms with experimental data on essential genes is very sparse. hence, this approach cannot be applied on a large scale. also, essential genes are transferred across distantly-related organisms [cit] . however, these approaches are effective in few pairwise transfers, but they need not generalise for all pairs of organisms since orthology accounts for an only small portion of the genome. in addition, genes show variations in gene regulations and functions across distantly-related organisms [cit] . scaled network-based features are potentially robust to these factors and are hence highly effective in predicting essential genes across organisms."
"string networks are obtained from genomic channels which inherently have some sequence information. however, adding sequence-based features yields further improvement suggesting that some residual information could be missing in these genomic channels that are explicitly captured in sequence-based features. also, hub proteins are found to evolve slowly and are mostly essential [cit] . while the evolutionary information of an organism can be obtained from the co-occurrence network that is one of the evidence channels used in constructing string networks, the role played by the protein is characterised by the underlying network-features that accounts for both the network position as well as the evolutionary context of that protein. so, these protein interaction networks offer a much better perspective incorporating the evolutionary and genomic information as well."
"in the teleoperation system, the human operator manipulates the robot remotely, and the slave movement follows that of the master synchronously. in general, the interaction information between the human operator and the remote environments can be reflected by variability of the muscle activation of the human operator in the process of manipulation."
"figures 10i-vi show the process of robot execution for the sweep task. by employing the proposed lwr algorithm with physiological interface, the sweep task was performed successfully within 0-10 s. in order to validate the performance of the learning model, we evaluate average value of robot in the process of robot execution in table 1 . it can be seen that the average error for learned trajectories in x-y-z directions and learned stiffness were 0.0130, 0.0018, 0.0346 and 0.0124, respectively."
"the set of \"14 network measures\" gave an auroc of 0.835 and a precision of 0.321. these approaches and \"12 centrality measures\" set discussed previously are highly scalable approaches to predict essential genes with a few sets of network properties. when we baselines. we can see that combined network properties, \"12 centrality measures\", \"14 network measures\" and \"refex feature set\" are effective in transferring essential genes across organisms, as compared to all the baseline methods. we can also see that adding sequence-based to network-based features yields more improvement in performance. note that all the improvements over the baseline are statistically significant, as we show in s5 combined all the 283 network properties, we achieved the best performance with auroc of 0.847 and precision of 0.320. this shows that a diverse set of network properties that capture the global, local and neighbourhood information can effectively predict essential genes across different networks. refex captures local and neighbourhood information recursively. centrality measures capture global information. local properties such as weighted degree and clustering coefficient capture the local properties of a node. a list of all 283 network properties along with their lasso coefficients is available in s3 table."
"in this study, the myo armband with 200 hz was used to collect the raw semg signal with eight channels [cit] . according to the sampled semg signal, the sum of semg signal can be defined as follows:"
"the collected demonstrated information contains the position of the slave and human muscle stiffness. in the process of demonstration, the demonstration data can be organized as follows:"
"uncovering the network aspects responsible for essentiality without using any complex biological information could help us unravel the significance of network structure and their importance in essentiality across various organisms. [cit] organisms (version 10 [cit] ). in addition, the database of essential genes (deg) has essential genes data for over 50 organisms (version 15.2 [cit] ), and online gene essentiality (ogee) database has essentiality data for 48 organisms [cit] . given the availability of data on essential genes, as well as network information from databases such as the string, there exists a need to develop effective methods for classification of essential genes, that make use of network-based features. extracting features based on network information and using them to predict gene essentiality can enable bridging the gap between organisms with known essential genes and interactome information."
the collected data contains task trajectories and human stiffness. the collected task trajectories and collected stiffness are ready for the demonstration and learning process. dtw unit is used to align the demonstration data in a united time scale. the robot learning model is mainly used to obtain a generative model according to the demonstrated information related to the collected task trajectories and collected muscle stiffness.
"figure 2b exhibits that the baxter robot (rethink robotics. made) was composed of two arms with seven dofs, a torso and a head with a radar. each robot arm was designed with eight links and seven rotational joints (the mechanical structure between baxter and touch x is different, therefore, a workspace matching method was developed in our previous studies [cit] ). figure 3 shows that the developed scheme consists of the following three modules: physiological interface module, demonstrations and robot learning module, and robot execution module."
"we employed feature selection to identify key features from the list of 283 features mentioned above. for this, we employed the widely used lasso (least absolute shrinkage selection operator) technique [cit] . lasso employes an l 1 regularisation to shrink the weights assigned to different features and make the set of weights sparse. this reduces the number of features with non-zero weights, and these features are subsequently selected for use in classification. we selected properties with non-zero weights for the best model using lasso by doing 10-fold cross-validation on the entire set of 87,159 genes. this gave rise to 100 features, comprising ten centrality measures, clique number, clustering coefficient and 88 refex features."
"through this process, we identified significant centrality measures in each network. we also conducted the wilcoxon rank-sum test [cit] to test for the significance of the centrality measures. in this way, we evaluated significant centrality properties across 27 networks and found commonly significant centrality measures across all the networks. table 2 lists all the measures used in our set of \"12 centrality measures\" and the number of organisms in which they were found to be significant. these centrality measures are found to be associated with lethality based on our study across 27 organisms and reaffirm our earlier observations on the \"centrality-lethality\" hypothesis that it holds true for a large number of organisms [cit] ."
"since refex has previously been shown to be useful in transferring labels across networks with a different number of nodes and edges, we employ refex on our organism ppi networks. overall, we compute 267 different refex features that fall under three different categories: (i) local features, such as degree, which are local to a node, (ii) egonet features, which refer to the node and the induced subgraph formed by a node and all of its neighbours, and (iii) regional features, which are a recursive combination of local and egonet features. recursive iterations of the means and sums of these local and egonet features are performed to capture the overall structural properties of the node. we used a total of 267 recursive features (for a detailed description, see s2 table) to construct the \"refex feature set\"."
"after the task is learned by using the lwr method, the slave robot was operated according to the learned task trajectories and learned human stiffness."
"where w m−j indicates the weight parameter and m is an empirical parameter in the experiment. according to literature [cit], the relationship between the semg signals and the indicator of the muscle activation can be presented as follows:"
"given the class imbalance in the dataset, accuracy is not a good measure to assess the performance of a classifier. in a class-imbalanced binary classification problem, a higher accuracy value is possible even if the classifier labels all items as belonging to the majority class. in order to tackle this problem, we used better metrics such as auroc (area under the receiver-operator characteristic curve), precision and recall, considering essential genes as the positive class. we evaluated our method by comparing auroc, precision, recall and auprc measures with the baseline methods. we performed statistical comparisons by means of a one-tailed ztest, to evaluate the significance of different metrics. this tests the mean of the 50 values obtained during 50 undersamplings of one method versus another."
"in the data collection stage, the demonstration data consist of several observations of the same task. after collecting demonstration data, a dtw method was proposed to synchronize the collected data."
"in order to verify the performance of the proposed algorithm, a practical sweep task was performed in this study. figure 5 demonstrates the construction of the experimental platform as follows:"
"proteins perform a plethora of different functions in every living cell. it is interesting to understand the role of different proteins and their importance, in terms of their essentiality for a cell to survive. several proteins orchestrate critical cellular functions. these proteins are consequently indispensable for the survival of a cell; the genes encoding these proteins are thus essential genes. essential genes have also been defined as those genes indispensable for reproductive success, either at a cellular or organismal level [cit] . the applications of essentiality are varied, ranging from finding the minimal genome required for sustenance to ranking drug targets [cit] . essential genes have been experimentally identified using transposon mutagenesis, anti-sense rna, rna interference and single gene deletion [cit] . however, experimental determination of essential genes is expensive, time-consuming and laborious [cit] . computational predictions of essential genes can give a prioritised shortlist for experimental validation."
"it is also clear from table 1 that 100 selected network features using lasso gave comparable performance to the entire set of combined 283 network features. also, the selected 300 combined network and sequence properties using lasso are equally effective as the entire set of 597 properties. lasso drives weights of 297 out of 597 features to zero; this is perhaps because the features do not contain any useful information or contain only information already captured in the selected 300. thus, the top features that are selected for classification perform nearly as well as the entire set of 597. a list of all 597 network properties along with their lasso coefficients is available in s4 table. top ranked set of features using lasso are equally effective"
"the remaining part of this study is organized as follows. section 2 presents equipment used for a myo armband, touch x and baxter robot. in section 3, the proposed algorithms including physiological interface design, task demonstrations, and learning method are described. results are presented in section 4. the conclusion and the future work are presented in section 5. the main notations are presented in table a1 in order to facilitate reading. figure 1a shows that the myo armband was used to detect raw semg signal, which consists of eight electrodes and nine-axis inertial measurement units (imu). the myo armband was produced by thalmic labs, canada and it can be easily adorned in upper limbs of human with a default frequency of 200 hz. figure 1b demonstrates the detection of the semg signals using the myo armband with eight channels."
"essentiality of a gene is conditionally dependent on various factors such as growth conditions, medium, developmental stage, and genetic context [cit] . gene essentiality is challenging to predict since it encompasses a number of factors. nevertheless, many studies have sought to predict essentiality using machine learning approaches. for this purpose, essentiality was defined as indispensability of a gene under rich media conditions [cit] . predicting essential genes and uncovering novel aspects responsible for essentiality could fundamentally improve our understanding of an organism's behaviour. experimentally, data on essential genes are available only for a few organisms. on the other hand, sequence and protein-protein interaction (ppi) network information are available for several organisms, making it feasible for large-scale in silico predictions."
"as with any approach to computationally predict gene essentiality, our study also has its limitations. firstly, we are limited by the quality of string ppi data. the ppi data is obtained from various evidence channels, yet they are prone to false positives and data incompleteness. the other bias could be that well-studied genes might have more interaction partners than poorly studied genes. the conclusions are based on balanced undersampled datasets across 27 diverse organisms. lastly, our conclusions need not be universal since we studied only 27 bacterial species based on the available essentiality data. however, as more experimental data on essentiality become available, it will be possible to further test our approach."
"lengths of the demonstration data differ in time frame, thus it is impossible to train a task model in teleoperation. therefore, the first step of data preprocessing was to align the demonstration data with different data lengths (if the demonstration data are not aligned in a united time scale, the task model could change both in temporal states and spatial states)."
"we also tried ranking the features using lasso coefficients and selected the top ones. we found that the top 200 selected sequence and network features gave similar performance to the network-based features to predict essential genes 300 lasso selected network and sequence features with non-zero weights. we also found that the top 100 selected sequence and network properties are equally effective as the 300 lasso selected network and sequence properties with non-zero weights. the performance did not deteriorate, suggesting that the top features are sufficient to perform better classification."
"where k is the moving window size.ǔ is the filtered semg signals through the moving average filter. then, an envelope of semg signals can be obtained as follows:"
we also evaluated our features using leave-one-species-out validation in which one species is left out as test set whereas all the other 26 species are kept as training set. we repeated this 27 times with different set of organisms as training and test set. this experiment was performed to check whether the features are effective to predict essential genes in a new unseen organism and are transferable across organisms. we used random forest classifier [cit] with 100 trees after undersampling equal number of non-essential genes since it was easily scalable for predicting essential genes in new organisms.
"the alignment of the demonstration data can be performed according to the entire obtained demonstration data in comparison with a reference demonstration data. in this study, the dtw method was employed to find the optimal alignment of the demonstration data in the time scale [cit] . the introduced dtw method could deal with the problem of the spatial distortion for the demonstration data. by employing the dtw method, the demonstration data can be presented as"
we also tried leave-one-species-out validation in which we found network-based features to be better than the sequence-based baselines. the results are in s8 table. we can clearly see from the results that the proposed set of features are significantly effective across all 27 organisms over the baseline methods. we can also find that the combined sequence and network properties are quite effective in a majority of the organisms.
"inspired by the robot learning algorithms and emg-based control method, in this study, a nonlinear regression model with physiological interface was constructed to relieve workload of the human operator to improve efficiency of the teleoperation system, and to capture the information details of the remote operation environments. first, the dtw method was employed to guarantee the demonstration data in the same time scale before proceeding with other steps in demonstration process (in our previous work [cit], we did not consider the influence of the length of demonstration data). secondly, a human-centric interaction method based on muscle activation was developed to collect the operation information and to actively capture the remote environment on-line. besides, in order to improve the efficiency of the teleoperation system and to reduce human operator's workload in a natural way, the lwr method was proposed to model the teleoperated prescribed task based on the collected task trajectories and the human muscle stiffness. finally, the feasibility and efficiency of the proposed method was verified by the experimental results."
"many previous studies have explored the correlation between centrality and essentiality (or lethality) in biological networks [cit] . the \"centrality-lethality\" hypothesis posits that nodes that are highly central in a network are much more likely to be lethal/essential [cit] . in network analysis, centrality measures identify the central nodes based on certain parameters. degree centrality, being the simplest of all the centrality measures, captures the number of immediate neighbours of a given node. betweenness and load centralities compute the significance of a node by calculating the fraction of all shortest paths that pass through a node. another set of centralities, eigenvector centrality and pagerank define the influence or the importance of a node in a network. overall, we used the following \"12 centrality measures\" in our analyses: closeness centrality, betweenness centrality, degree centrality, eigenvector centrality, subgraph centrality, information centrality, random walk betweenness centrality, load centrality, harmonic centrality, reaching centrality, edge clustering coefficient centrality and pagerank. detailed definitions of all these measures can be found elsewhere [cit] . we also combined clique number and clustering coefficient with the above-mentioned centrality measures and designated the set as \"14 network measures\". we combined all the above network properties and used them as features for essential genes prediction. the final number of network features that we used are 283, ignoring the repeated properties: 267 refex features, \"12 centrality measures\", clique number, clustering coefficient, biconnected components and weighted degree (for a detailed listing of these features, see s3 table) ."
"previous essentiality studies have not focused enough on aspects of the network organisation, and the role that 'network position' plays in essentiality. on the other hand, in the field of social networks, recent studies have illustrated the importance of network position and network properties in determining the structural roles played by different nodes in the network [cit] . regional features, as computed by refex [cit], recursively capture the properties of a neighbour and a neighbour's neighbour and so on. in the present study, we adapted these ideas to predict essential genes across diverse organisms. moreover, there is no extensive study so far that has focused on analysing different network-based properties across protein interaction networks of diverse organisms, for predicting essential genes."
"our key results are three-fold. first, we show that network-based properties are able to predict essential genes across organisms better. notably, they outperform sequence-based features by a distance. additionally, we show that a lasso-based feature selection that yields a reduced set of top features is able to perform better as well. second, we show that even a few network properties, such as those given by \"12 centrality measures\", are able to aid greatly in classification. finally, we show that augmenting the network-based features with sequence features further improves classification at the cost of an increased number of features, and is effective across organisms. importantly, obtaining sequence-based orthology features requires pairwise comparison of genomes that is computationally expensive. also, our reduced top ranked set of 100 sequence and network features could be highly useful to predict essential genes in a new organism."
"we compared our approaches with [cit], zupls [cit] and the conventional network feature set (\"naïve network baseline\") used in previous studies [cit] . these network features are degree centrality, closeness centrality, clustering coefficient and betweenness centrality. we focus only on these baselines since we are not using any expression data or function related information. we also didn't use many centrality measures proposed for the purpose of ranking based approaches since they were created using either biological domain information or expression-related information and usually ranked genes within an organism. we set out to verify features that are effective across a diverse set of organisms using plain sequence and network information."
"in this study, a robot learning method with physiological interface was proposed for the teleoperated sweep task. in order to capture the information details of the remote operation environments in hri, a novel method of human-centric interaction with human muscle stiffness was first developed. on the one hand, the semg signal of the human operator was used to capture the human's muscle stiffness. on the other hand, the collected muscle stiffness represented the human operator operational characteristic in the process of manipulation. integration of the dtw method and the lwr method enabled the robots to learn the task trajectories and human muscle stiffness in the same time scale after human demonstrations in teleoperation system. the remote robot can execute the task according to the learned task trajectories and learned stiffness. finally, the effectiveness of the proposed method was demonstrated by the experimental results."
"first of all, the proposed lwr method constructs a nonlinear model for a specific task. compared to traditional robot learning methods [cit], the proposed method combines the task trajectories with the muscle stiffness during the demonstration process."
"moreover, the human operator can regulate the muscle stiffness according to the remote environments in the hri. that is to say, the variability of muscle stiffness represents the operational characteristics of human operator. therefore, a model to describe the relationship between the prescribed task, task trajectories, and the muscle stiffness are very useful in the learning process. thus, the learned model can be used to improve the efficiency of the reproduction of the repetitive task. in the proposed method, we transmitted the learned model, i.e., learned trajectories and learned human stiffness, to the controller of the slave robot of the teleoperation system after robot learning. then, the slave robot can be executed according to the learned model. noteworthy, the experimental results in this work are preliminary and the experiments were conducted mainly to show the feasibility and effectiveness of the proposed method. the stiffness of the human operator varies from person to person. moreover, even for the same person, the stiffness property at different time could be different (time-varying). therefore, it is hard to find a baseline to compare the results under \"with stiffness\" and \"without stiffness\" conditions. different people may have different stiffness, and thus may show different motion model. therefore, it is difficult to define fair comparison criteria for different/multiple experiment subjects. there is no problem foreseen to generalize the proposed method to different subjects as the implementation procedures will not change and each step is not affected by the specific human subject. however, surely the obtained specific muscle activation/stiffness pattern(s) would be different from person to person."
"refex is a recursive feature extraction technique that has been previously shown to enable transfer of class labels across networks from various domains [cit] . each ppi network that we take into consideration has a certain structure. we hypothesise that essential genes across networks, i.e. across organisms, share \"structural\"/network features that are effective in transferring essential gene labels across networks. that is, these features capture the network structure of different organisms and may hence be useful in identifying essential genes in one organism, based on features/patterns learnt in another."
"the touch x serves as a master device to control remote slave device ( figure 2a ). the touch x with six degrees of freedom (dof) was manufactured by 3d systems, inc. and it could capture the position (three dofs) and its orientations (another three dofs) with force feedback. the touch x is widely applied in various areas in terms of simulation, virtual assembly, robot control, etc. (the kinematics information of the touch x with respect to its forward/inverse kinematics and jacobian matrix have been deeply studied [cit] ). in this study, touch x was controlled via the matlab/simulink [cit] ."
"in this section, we establish that our network-based features are highly informative and enable better classification of essential proteins compared to all previous methods. we further show that the addition of sequence-based features is able to further improve performance. we finally propose that the simplified features obtained using lasso is an effective feature set for performing predictions of essentiality in newer organisms. our results are discussed in detail in the following sections."
"the proposed teleoperation system employed a heterogeneous master-slave structure, therefore we utilized motion of the slave as the data source for demonstrations. owing to the workspace differences between the master and the slave, a workspace mapping between the master and the slave was carried out to ensure the accuracy of tracking."
"based on the learned model by using the lwr algorithm, the baxter robot can execute the sweep task automatically with the learned human muscle stiffness. the sweep task fully indicates the dexterity of human arms in the process of operation. in this experiment, human muscle stiffness was employed based on muscle activation to reflect the interaction information in demonstration phase."
"this work makes different contribution from our previous work [cit], wherein a hidden semi-markov model with gaussian mixture method was proposed to carry out the repetitive tasks with enhanced intelligence. in this work, a locally weighted regression method with dtw was proposed for a prescribed task. moreover, in our recent study, it was found that the length of demonstration data could significantly impact the performance of the teleoperation. to handle this problem, we employed the dtw method to deal with the issue related to different lengths. however, in our previous work [cit], the influence of the length of demonstration data was not taken into account."
"for classification, we used support vector machine (svm) [cit] with a radial basis function kernel, and a grid search was done to find the best parameters. all our codes were written in python and used the svm implementation from the scikit-learn python package [cit] . for leave-one-species-out validation, we used rfc implementation from scikit-learn python package [cit] since it was easily scalable across 27 organisms."
"through the above studies, we find that network properties outperform sequence-based properties. to examine whether combining sequence-based features with network-based improves performance, we combined sequence and network properties for predicting essential genes. this gave better auroc (0.857), precision (0.335) and recall (0.769) values than combined network-based features."
"undeniably, a lot more systematic explorations are demanded to introduce the vision information and rotation information to the perception system of the teleoperation system which will be pursued in future. besides, the relationship between shared control and the human stiffness in fatigue status should be comprehensively analyzed. moreover, the shared control strategy with respect to robot learning method is an interesting research topic in the teleoperation system. the level of autonomy between direct teleoperation and robot-learning-based automation is worth to exploit to explain the cooperative strategies for hri. moreover, the smoothness of task trajectories and safe trajectory generation with respect to the muscle stiffness are very important aspects for the complex task. in order to obtain the model with smoothness of task trajectories in-depth exploration of the intelligent segmentation algorithm to divide the complicated task into certain subtasks which can be approximated explicitly by locally smooth mathematical functions/polynomial will be carried out in future."
"we extracted refex and other network-based features for the 27 organism ppi networks that we obtained from the string database [cit] . all the network properties were scaled using the \"min-max scaler\" from the scikit learn library for python [cit] since the networks are of different sizes. the scaled features were then used for classification. as discussed previously, we used 87159 genes that had available network information, for classification and comparison. we extracted zupls features for the same 87159 genes based on the codes and supplementary information provided in the zupls study [cit] . for leave-one-species-out validation, we used the same set of features and considered only the 87159 genes belonging to 27 species that had available network information."
"the prediction of essential genes in an organism is a challenging machine learning problem. many previous studies have tackled this problem by engineering various types of features, from sequence to network [cit] . however, the network-based features employed in previous studies are somewhat simplistic and do not tend to capture the complexities of ppi structure. therefore, in this study, we set out to investigate various network features for their ability to enable discrimination of essential and non-essential genes across several organisms. using essentiality data from deg and ppi data from the string, we outline several interesting network-based features that are able to greatly enhance classification performance. overall, our approach statistically significantly outperforms the best reported results at matching the deg as a gold standard, by using features derived from local, neighbourhood and global network properties, and is also useful for predicting essential genes across organisms."
"it is also important to note that the experimental identification of essential genes itself remains a work in progress, and there remain major variations between multiple studies reporting essential genes on similar media, for identical strains. our notion of gene essentiality essentially pertains to the consolidated experimental data available via deg 11.1. however, as better and more reliable data accumulate from newer experiments, it is likely that we will be able to build better models and consequently, predict essential genes with higher accuracies."
"in this section, we first outline the datasets used in this study and then describe in detail the network-based features that we have employed, to predict essential genes across organisms."
"the demonstration data are high dimensional; therefore, it is difficult to find a function to describe the demonstration data globally. in this study, the locally weighted regression (lwr) algorithm was employed to explore the approximate function locally between the input and the output for the given aligned demonstration data."
"with significant progress in computer science, information science, automation, and artificial intelligence techniques, telerobots have been extensively applied in areas as diverse as telemedicine [cit], telerehabilitation [cit], minimally invasive surgery [cit], disaster rescue and relief operation [cit], maintenance and exploration in deep sea or out space [cit], surgeon training [cit], and telemanufacture [cit], etc. telerobots provide an alternative interactive way between the human operator and the teleoperation in order to enhance perception and motion ability of the human beings [cit] . it is the integration of human intelligence and the robot's advantages under the constraint of long distance [cit] . the performance of teleoperation largely depends on the perception of remote environmental conditions."
"while generating refex features, the algorithm terminated at a different number of iterations for networks of different organisms and consequently, yielded a different set of recursive features. we took the organism that had the smallest number of recursive features (36) and generated the 36 features for all the other organisms. this gave an auroc of 0.578, a precision of 0.162 and recall of 0.338. we also took the organism with the maximum number of recursive features (93) and generated the 93 features for all the other organisms. this gave better results (auroc: 0.817, precision: 0.322, recall: 0.718). when we combined all the unique recursive features across all the 27 organisms (267 features), the performance improved further (auroc: 0.838, precision: 0.321, recall: 0.754). we conclude that a diverse set of refex features, thus generated, are effective in predicting essential genes across organisms."
"the central contribution of this study is the engineering of several potent network-based features for predicting gene essentiality across organisms. notably, we have adapted algorithms such as refex to better predict gene essentiality based on local, global and neighbourhood properties. further, we find very small feature sets, such as the \"12 centrality measures\" and \"14 network measures\", which provide excellent discriminative power. adding sequencebased features to network-based features yields a further improvement and our selected set of 100 network and sequence features could be the most useful set for predicting essential genes in newer organisms. we also reported a leave-one-species-out validation, which demonstrates the proposed sets of features to be effective for performing predictions across organisms. notably, network-based features can probably point us towards uncovering the key roles played by the essential nodes in network structure."
"author contributions: j.l. conceived the method, performed the experiments and wrote the paper; c.y. conceived the method and helped to improve it; h.s. and c.l. further helped to improve it. all authors have read and approved the manuscript."
"the third step modified the content providers to recognize when they were being called by forensics tools. this takes into account the information from the separate programs on how to recognize usb debugging and behavior specific to the tools. the idea is to make the content providers exhibit anti-forensic behavior when they detect the presence of the forensics tools, but still be sufficiently close to the original behavior for the forensics tools to believe in the data they receive."
"the use of multimedia technology has become a necessity in order to develop and expand the field of education. accordingly, the development of interactive multimedia courseware for aural skills have been produced by a combination of several elements of multimedia such as text, graphics, audio and animation to assist the process of teaching and learning. addie instructional design model was used as a guide in this learning courseware development. this courseware applied the interactive multimedia scenario based learning approach and uses the constructivism theory to generate students' understanding and thinking. the courseware was developed using multimedia builder 4.9 as the authoring platform. [cit], adobe photoshop cs and sonar x3 used as the editing software for animation, video and audio. the research procedure as shown below: 2017, vol. 7, no. 9 issn: 2222 366 www.hrmars.com"
"presenting false data whenever the network connection is lost may be a valid anti-forensic strategy. the connection may be legitimately lost during everyday use, for example, by the user walking into an underground rail station. in this case, the phone functionality is unavailable to the user anyway, so the unavailability of phone-related data may not be a significant drawback."
"detecting a usb connection suffers from a lack of specificity, especially in earlier versions of android. version eight of the android api (corresponding to android 2.2 'froyo' [cit] ) is unable to use the improved usb support introduced in api version 12 (android 3.1 'honeycomb mr1') and would need to be back-ported to api 10 (android 2.3.4 'gingerbread mr1') [cit] ."
"recent research investigates the risk that mobile phones present to individual members of society [cit] and to the business world [cit] . it has also examined some of the challenges these devices present to forensic investigations [cit] . hence, it is only a matter of time before individuals, organizations and businesses implement solutions to mitigate these risks through anti-forensics activities."
"one of the most challenging obstacles to resolving problem situations is ambiguity in the problem situation that can cloud the issues and make it difficult to identify the causal roots of the problem. thus the first priority should be to find a way to establish shared sense making about the problem by overcoming the likely barriers such as the conflicting motives of each (sub) group, their sub-languages, metaphors, subjectivity and any myths and causal fantasies. this process of consensus solution seeking requires dialogue and methodologically-guided elicitation and analysis of the values and priorities of implicated stakeholders."
"the operating system was rebuilt to contain the module, uploaded to the phone and installed using the recovery image. this replaced the operating system already on the phone, but preserved user data such as contact lists and sms messages. each modified version of cyanogenmod was approximately 90 mb in size and took slightly less than one minute to install."
"specific for android is the separation between different applications enforced by the operating system. every application is run as its own linux user. standard linux file system permissions are used to ensure that no other application can read its files. this also applies to the applications uploaded to the phone by forensic analysis tools. this protection can only be bypassed if the phone is first rooted. if that is done, software can use the elevated privilege to read the entire file system."
"under the assumption that the content provider is always used, instrumentation code was developed and inserted into it to write information about its behavior to the system log. for each call to the content provider's main query function, the code output the name of the calling program, the query arguments and which part of the existing program logic handled the query."
"mobile phone anti-forensics is concerned, to a large degree, with overwriting or deleting information. while this potentially makes the data completely unavailable to the forensic analyst, it also makes it unavailable to the legitimate user if the phone is eventually returned. providing false data to the analyst presents the possibility of hiding the real data and reverting to it after the completed analysis."
"regression subnetwork. while the detectors alone provide good performance, they lack a strong relationship model that is required to improve (a) accuracy and (b) robustness particularly required in situations where specific parts are occluded. to this end, we propose an additional subnetwork that jointly regresses the location of all parts (both visible and occluded). the input of this subnetwork is a multi-channel representation produced by stacking the n heatmaps produced by the part detection subnetwork, along with the input image. (see fig. 1 ). this multichannel representation guides the network where to focus and encodes structural part relationships. additionally, it ensures that our network does not suffer from the problem of regressing occluded part appearances: because the part detection heatmaps for the occluded parts provide low confidence scores, they subsequently guide the regression part of our network to rely on contextual information (provided by the remaining parts) in order to predict the location of these parts."
"functions are related to 1. telling (receive and provide information); 2. enacting (discuss, deliberate, propose, vote); 3. making (share projects, co-design projects, collective problem solving, share goods)."
"cellebrite and xry were provided with contact lists containing one contact each, this being the technical support phone number for each tool. unknown tools will be provided with an empty contact list. cellebrite and xry were used, in turn, to extract the phone contact list. both tools displayed the same number of contacts as in the real contact list, but each displayed the name and number to technical support for the respective tool. the display of false data is demonstrated in figure 10 -cellebrite extraction report and in figure 11 -xry extraction report. finally to simulate the connection of an unknown forensics tool, the phone was connected to a pc still in usb debugging mode and the contact list inspected using the built-in application. in this case, the built-in application shows an empty contact list, as seen in figure 12 -empty contact list."
"cellebrite has a low tolerance for response delays. it accepts a delay of five seconds at each call to query(), but ten seconds is enough to make it abort the extraction and show the error message in figure 4 -cellebrite error message."
"the technological part of the project was devoted to the development of tools for promoting the combination of different kinds of services for online communities. we profited of the plakss (platform for knowledge and services sharing) framework, developed by cnr [cit] . such a platform was devoted to:"
"the experiments performed here have shown that it is possible to distinguish between normal use and forensic analysis by looking at whether usb debugging is enabled, and that it is possible to distinguish between different forensics tools by looking at the names of their applications."
"we proposed a cnn cascaded architecture for human pose estimation particularly suitable for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. key feature of our network is the joint regression of part detection heatmaps. the proposed architecture is very simple and can be trained end-to-end, achieving top performance on the mpii and lsp data sets. fig. 7 : examples of poses obtained using our method on mpii (first 3 rows), and lsp (4th and 5th row). observe that our method copes well with both occlusions and difficult poses. the last row shows some fail cases caused by combinations of extreme occlusion and rare poses."
"the anti-forensics solutions implemented for this research demonstrates that it is possible, from a proof of concept perspective, to deceive a device implementing a logical acquisition. the real information is left in its place, while false information is fed to forensics tools from other sources. forensics tools place a great deal of trust in the android software, but that software can easily be modified and replaced. when suitably modified and replaced, that software can feed false information to the tool. neither tool used in this research detects this subterfuge and presents the false information to the analyst as if it was real. the anti-forensics software modules are present on the phone and can be seen by the analyst should they do a logical extraction of the phone's file system. however, their presence and function is not obvious and, even if they are detected, reverse-engineering them potentially requires significant time and effort from the analyst. the results of this research empirically support the idea that examiners should not rely on a single tool or extraction method in their analysis of mobile devices."
conducting mobile phone forensics investigations is an increasingly challenging and complex undertaking. the interest in mobile device anti-forensics is increasing from within both academia and industrial environments. the complexity of the environment coupled with anti-forensics operating system modifications potentially inhibits mobile phone forensics investigation.
"anti-forensics in this case is broadly defined as \"any attempts to compromise the availability or usefulness of evidence to the forensics process. compromising evidence availability includes any attempts to prevent evidence from existing, hiding existing evidence or otherwise manipulating evidence to ensure that it is no longer within reach of the investigator\" [cit] . while several data extraction options exist for mobile devices, research has highlighted the fact that not all extraction solutions are equal nor do they necessarily provide the ability to validate results [cit] . this can be attributed to an array of factors that include numerous mobile phone hardware configurations and vast numbers of devices in the market. hence, the extractions that are most likely to be implemented with higher degrees of success are logical and manual extractions."
"the research demonstrates that it is possible to modify components to present false data. this task is aided by the fact that android is an open system, with specifications and source code that are freely available. several projects use that source code to build community distributions of android which can be installed on many different models of phones. the installation requires that the phone be rooted, which is possible to do on many phones and popular among technologically sophisticated consumers."
"eu-level policy supports engagement. the need for stakeholder engagement and transparent dialogue with citizens is clearly articulated in article 11 [cit] . being the primary customer for scicafe2.0 project dg connect has developed its own inclusive approach to the involvement of stakeholders into policies, programmers and services. scicafe2.0 project regards stakeholder engagement as a process that encompasses relationships built around one-way communication, basic consultation, in-depth dialogue and working partnerships. scicafe2.0 also developed a stakeholder outreach reference document to guide co-development of a content marketing plan, which helps to improve the engagement processes. in the document we distinguish four main stakeholder groups as amplifiers, brokers and the medium of the message to the majority:"
"due to this acceptance, forensic analysts rely heavily on the correct functioning of the phone's software when performing analyses. hence, altering functionality is a way of thwarting an analysis. smartphones running operating systems such as android and ios are designed to allow the installation of third-party applications. this has allowed for the development of applications with anti-forensic functionality [cit] . however, these applications have to work under the restrictions imposed by the operating system, such as application isolation and responsiveness demands. if anti-forensic modifications were to be made on a lower level, these restrictions would not apply in the same way, possibly making more advanced methods available. this idea promoted research into the hypothesis that it is possible to modify the android operating system to present false information to the forensics tools. several subsidiary research questions were identified in order to explore the hypothesis: 1. which components of the android operating system do the forensics tools trust? 2. is it possible to modify these components to present false information? 3. can the presence of a forensic analysis tool be detected? 4. is it possible to make the presentation of false information reversible, such that the phone will revert to presenting the real information after the forensic analysis? the research contribution is an initial empirical analysis of the viability of operating system modifications in an anti-forensics context along with providing the foundation for future research in this area. the paper is structured as follows: section 2 discusses relevant approaches to smartphone anti-forensics. section 3 presents the methodology, and the experimental design. section 4 discusses the implementation and results. section 5 draws conclusions from the research conducted and section 6 presents future work."
"they attempted to use the forensics tool paraben device seizure [cit], but found that this was incompatible with the phone they were using. instead, they used backup programs, which require the phone to be rooted and perform a logical acquisition of the phone memory. as expected, these programs were able to read the private directory where the data had been stored. had the phone not been rooted, the backup programs would not have worked [cit] ."
"as multimedia teaching technologies become more widely advocated and employed in higher education, researchers strive to understand the influence of such technologies on student learning. testing for aural interactive self learning courseware is made after the courseware developed. set of questionnaire was designed to calculate selected students' and expert's perception regarding the appropriateness of its use in the learning process. in particular, the test was conducted to verify whether the product congregate with the skills needed as a self learning tool among music students in the faculty of music and performing arts upsi."
"the fourth step created and entered a dataset into the phone. the phone contacts were entered using the standard on-board tools, and contained two entries, each with a name and a phone number. the contacts were not synchronized with any other service."
"one attribute common to the forensics tools used in this research is that they require the phone to be set to usb debug mode. this lets the tool control the phone and installed applications. setting the phone to debug mode requires going deep into the settings menu and acknowledging a warning that 'usb debugging is intended for development purposes only'. it also removes the possibility to use the phone as usb memory as well as transfer files to and from it using standard file management tools. the fact that activating debug mode removes these normal and desirable features suggests that most users will not have debug mode activated. if so, triggering anti-forensic behavior upon activation of usb debugging will have a lower false-positive rate than triggering it on all usb connections. a stand-alone application was built to be a receiver for the intent broadcast when the usb state of the phone is changed. using the extra information contained in the intent, the program determines whether the usb cable is connected or not, and whether the phone is in debugging mode."
"more recently methods based on convolutional neural networks have been shown to produce remarkable performance for a variety of difficult computer vision tasks including recognition [cit], detection [cit] and semantic segmentation [cit] outperforming prior work by a large margin. a key feature of these approaches is that they integrate non-linear hierarchical feature extraction with the classification or regression task in hand being also able to capitalize on very large data sets that are now readily available. in the context of human pose estimation, it is natural to formulate the problem as a regression one in which cnn features are regressed in order to provide joint prediction of the body parts [cit] . for the case of non-visible parts though, learning the complex mapping from occluded part appearances to part locations is hard and the network has to rely on contextual information (provided from the other visible parts) to infer the occluded parts' location. in this paper, we show how to circumvent this problem by proposing a detection-followed-by-regression cnn cascade for articulated human pose estimation."
"the plakss framework has been used for instantiating the scicafe2.0 platform. the scicafe2.0 platform is conceived as a participatory crowdsourcing platform that allows people and organizations to be active actors, playing both the roles of problem and solution providers. it acts as a multiplier of knowledge and innovation:"
"articulated human pose estimation from images is a computer vision problem of extraordinary difficulty. algorithms have to deal with the very large number of feasible human poses, large changes in human appearance (e.g. foreshortening, clothing), part occlusions (including self-occlusions) and the presence of multiple people within close proximity to each other. a key question for addressing these problems is how to extract strong low and mid-level appearance features capturing discriminative as well as relevant contextual information and how to model complex part relationships allowing for effective yet efficient pose inference. being capable of performing these tasks in an end-to-end fashion, convolutional neural networks (cnns) have been recently shown to feature remarkably robust figure) is a part detection network trained to detect the individual body parts using a per-pixel sigmoid loss. its output is a set of n part heatmaps. the second one is a regression subnetwork that jointly regresses the part heatmaps stacked along with the input image to confidence maps representing the location of the body parts. the first row shows the produced part detection heatmaps for both visible (neck, head, left knee) and occluded (ankle, wrist, right knee) parts (drawn with a dashed line). observe that the confidence for the occluded parts is much lower than that of the non-occluded parts but still higher than that of the background providing useful context about their rough location. the second row shows the output of our regression subnetwork. observe that the confidence for the visible parts is higher and more localized and clearly the network is able to provide high confidence for the correct location of the occluded parts. note: image taken from lsp test set."
"collective awareness platforms (caps) are applications based on internet or mobile communication, scaffolding on social networking for supporting communities by delivering new services, building innovative knowledge, promoting collective intelligence. the final goal of caps is the promotion of more sustainable lifestyles and inducing transformative social innovation [cit] . often such 'voluntary model' is conceptualized as a collaborative commons paradigm as it bypasses the capitalist markets and relies on zero marginal cost [cit] . many applications are devoted to real actions, beyond simple knowledge-sharing, for instance by promoting energy saving (e.g., carpooling, food sharing, buying groups) and essentially harness communication among people [cit] . with regard to the optimization and potentials of ict-enabled spontaneous, massive and collective citizen involvement the concept of crowdsourcing has been recently defined as a process of accumulating the ideas, thoughts or information from many independent participants, with aim to find the best solution for a given challenge [cit] . within the scicafe2.0 project we are setting up an observatory of crowdsourcing (european observatory for crowdsourcing) devoted to participative engagement, in the spirit of the science café movement [cit] . we are studying the information flow in a participative event, and what the psychological and social components beyond the individual participation are. we are also actively experimenting on such phenomena, through specific set-up and developing a platform for supporting participative actions. finally, we are supporting the science café movement, through specific actions and by means of experiments on mixed real-life and internet-based participative models. based on this, we present our approach to requirements elicitation that proved to be helpful co-creating robust, scalable and sustainable solutions; then we turn to social-cognitive patterns of social collaboration; and finally analyze patterns of online participative engagement and tools that help such collaboration."
"the results from this research demonstrate that it is possible to modify an android operating system to present false information to the forensics tools. the forensics tools trust the phone software to return the correct results. the means that the tools are trusting file system drivers, the installation package manger and lower level functions."
"performance and high part localization accuracy. yet, the accurate estimation of the locations of occluded body parts is still considered a difficult open problem. the main contribution of this paper is a cnn cascaded architecture specifically designed to alleviate this problem."
"within scicafe 2.0 project we have made a comparison of existing on-line participatory methodologies [cit], we have implemented and edited a handbook of online participatory methodologies [cit] in which some paradigms of on-line crowd-sourcing participatory methodologies are proposed, based on the analysis of online platforms."
"the emerging results contributed to the development of the scicafe 2.0 platform. the scicafe 2.0 platform integrating the citizens' say knowledge exchange tool provides for stimulating participation/cooperation. basing on the results of this preliminary work a delphi-based model of collective participation and knowledge building in the decision making processes was designed and implemented on the scicafe 2.0 platform. the aim of the model was to explore the potential of caps in participative policy-making, directly connecting with real social contexts and including relevant social actors. we implemented the participatory model to a real case study, in our case the \"science with and for society observatory\" of the second municipality of rome. the participatory process was designed from the beginning as a combination of online and offline activities and it is based on the collective participation of a multitude of different actors, including policy makers, experts, citizens. as a typical delphi model, our process is divided in different steps, and combines off-line and online activities this experiment was a success, more of three hundred people have participated at the first plenary meeting of the observatory, and many of them decided to continue the participatory process participating at the works of the different groups and at the online activities hosted by scicafe2.0 platform. after the implementation of the participatory process, we performed the validation by means of an online workshop in which we applied the delphi methodology within the re-aim framework. the delphi-based validation workshop involved 10 panelists from different categories (policy makers, researchers, science museums, schools and citizens) who discussed the effectiveness of online participatory decision making as well as the advantages and specifics of the different participative instruments and. we obtained a high level validation of the participatory model; in fact we received 8 recommendations regarding the implementation of a participatory model in order for the participatory model to be successful."
"to trigger the wipe, two methods have been used which include reading the system logs [cit] and detecting a usb connection. reading the system logs has the disadvantage of being slow, since it has to wait for the event to occur the log message to be generated, written and finally read back in before being able to take action."
"from the 1st to the 2nd [cit] the scicafe2.0 [cit] conference held in brussels. during the first day, 1st of july, a session was held by scicafe2.0 entitled citizens' say: have your say! this session was split into two spaces. one was used to present the scicafe2.0 project and citizens' say platform and the second one was a \"hands-on-session\". [cit] conference were attended by a smaller audience than previous workshops but there was an open plan setting and people just dropped in and out so the total audience was larger than that at any time. participants could explore and experiment with the platform on their computers in the room, assisted by the scicafe 2.0 consortium members who continued to engage with the audience."
"android is a young operating system, with the first commercial device, the htc dream, also known as the tmobile g1, [cit] . hence, it is expected that the android forensics and anti-forensics literature will not be as established as the ones for windows pcs. harris [cit] classifies anti-forensics into four groups: hiding, destruction, source elimination and counterfeiting. kessler [cit] also categorizes anti-forensics into similar groups which consist of data hiding, artifact wiping, trail obfuscation, and attacks against processes and tools."
"xry completed the extraction. however, a message indicates that an error had occurred and that the extraction was incomplete. the error log is displayed in figure 8 -xry extraction log. the log was not particularly clear on what went wrong and the extent of the consequences. the report was missing the sections for device/app usage, contacts and web/bookmarks."
"the phone was reinstalled using a standard cyanogenmod 7.2 operating system. using the built-in contact list application, two contacts were entered into the phone contact list (see figure 9 -contact list). extractions of the contact list were performed using cellebrite and xry, and both extractions successfully displayed the correct contact information for both contacts."
"a total of 60 respondents comprised of 58 students, 1 lecturer who specializes in the field of instructional technology and 1 lecturer who experts in teaching aural involved in the test and evaluation process. overall findings related to the use of assisted self-learning courseware for aural subject among students is overwhelming. this can be seen from the mean score being placed on the 'high' category based on indicator with regard to the suitability of the content interactive multimedia courseware (3.38), instructional design (3.34) and technical requirements (3.57). advances in technology enable pedagogical enhancements that some believe can revolutionize traditional methods of teaching and learning [cit] . the researchers found the experts and students agreed that the courseware produced an alternative to conventional aural lesson delivered in the face to face classroom. data also concluded that the users do not need a longer explanation to understand the topic or content, but just need to have additional emphasis to fix with the cognitive, psychomotor and affective domain. the result also indicated that the courseware can provided a good presentation throughout the use of graphics, fonts and visual. this gives an indication that the self-learning courseware be able to hold users' attention on each slide shown."
"the phone was reinstalled using a modified cyanogenmod 7.2 operating system. the contact list provider was modified to return results from alternate databases depending on whether the query comes from cellebrite, xry or the phone itself outside of usb debugging mode."
"for lsp, we fine-tuned the network for 10 epochs on the 1000 images of the training set. because lsp provides the ground truth for only 14 key points, during fine-tuning we experimented with two different strategies: (i) generating the points artificially and (ii) stopping the backpropagation for the missing points. the later approach produced better results overall. the training was done using the caffe [cit] bindings for torch7 [cit] ."
"in the scicafe2.0 project [cit] the main goal is to set-up an observatory of crowdsourcing devoted to participative engagement in the spirit of the science café movement. in particular, we aim at the promotion of science café networks through a supporting agency, the extraction of scenarios and best techniques, the use of this or similar methodologies (like the world cafés) beyond science, and the development of a web interface for supporting this type of communication. we are also interested in the cognitive basis of cooperation, participation and the emergence of collective intelligence."
"both images contain mostly raw information, so the processed information indicating usb connection and debugger state has been indicated with outlines. the debugger is indicated by its name \"adb\", which stands for \"android debug bridge\" [cit] ."
"after various anti-forensics options have been explored, investigations will take place into how to effectively detect operating system modifications and the most efficient way for law enforcement to confront these issues. this includes investigating additional activities to trigger similar behaviors that are not necessarily intentional antiforensics techniques. additional work by the authors will examine the creation of custom roms that will introduce additional forensics capabilities along with exploring crossover issues with bring your own device (byod) solutions in organizations."
"the fifth step performed forensic extractions using both cellebrite and xry. the first extractions were conducted with the phone running an unmodified cyanogenmod operating system and then with each anti-forensic modification in turn. the results of the extractions were inspected for the real data, the false data for that antiforensic case and any signs of the tools suspecting that something was wrong."
"as the relationship between the stakeholders, the problem situation and an emerging solution will evolve over time and the solution needs to be re-visited so as to remain dynamically responsive to evolving realities and relationships of the situation, it follows that there is a need for a dynamic usability evaluation and holistic impact assessment framework, e.g. the dynamic usability relationship-based evaluation (dure) method [cit] which takes account of the dynamic relationship that can develop between the stakeholders and the solution."
"finding no data in a well-used phone would be suspicious. arranging for the analyst to find plausible, but nonincriminating, data increases the chance of the analyst accepting the output and concluding that the phone holds no relevant data. while examining the behavior of the forensics tools and the operating system, sufficient data was obtained to determine the format for the extracted contact list. using that information, false data in the correct formats was constructed and inserted into the code for the contacts provider. this data is to be returned in response to queries from each tool. as a fallback, if a query comes from an unknown tool and usb debugging is on, no results will be returned."
"the tools in this experiment performed logical acquisitions of the devices. assuming the phone is supported, the tools in the experiment can be used to perform a physical acquisition of the phone's entire memory, thereby bypassing the high-level phone software and only trusting the phone's file system driver to return the correct files. however, using that mode of acquisition requires that the tool or the analyst perform data interpretation themselves, without the help of the phone software. both tools also use standard methods of installing software, thereby trusting the package manager to install that software correctly. these high-level software packages, in turn, trust the lower levels to function correctly. therefore, the forensics tools also trust, by extension, all lower levels of the android stack, including the dalvik virtual machine, the linux kernel and the hardware. any component of a system under forensic analysis that is trusted by one party is a point of attack for their opponent. since the content providers and package manager are trusted components, they are natural targets for anti-forensics activities."
for showing the current development of the platform we introduce how the platform implement the virtual world café meeting. this kind of virtual meeting is structured in discussion tables specified and configured by the organizer of the world cafe.
"step-by-step guides, available on the internet, describe how to root phones and install community distributions of android. these community distributions depend upon contributions of code from the general public. they, therefore, make it easy to modify their code and install the modified versions. while programming skills are a prerequisite, it is possible to modify and replace content providers and the package manager. in this research, the os components that were modified allowed for activity on the phone to be monitored and responses to be customized based on the state of the device."
"in our experience we can say that scicafe 2.0 and its participatory model is a suitable for change in the dynamics of social innovation especially those relating to participatory methodologies aimed to citizens involvement and bottom up actions, it facilitates and entices users to the implementation of participatory methodologies applicable to different fields and walks of life, becoming a powerful tool for public engagement within the broader dynamics of social innovation within the macro changes that new media favor, but also stand out in the social dynamics, economic policies, effectively making them a major catalyst for change."
"despite of all music education students be capable to sing, however in sight singing and aural skills is not as simple as expected. teachers who conduct classes with various ability levels know that much proficiency are required for students to successfully master the aural skill. often neglected are the skills that students must have to interrelate musically with their hearing and singing. this situation will have an impact on the effectiveness of their future career as music educator, especially involving singing activities. irregularities that occurred are likely due to several factors. approaches, methods and practices of teaching in the classroom that solely to use a combination of piano and lecturers, have to some extent affected the improvement of students' skills and thereby reducing their interest. the literature exposes a gap in the use of technology and sound instructional approach in current teaching and learning practices [cit] . based on this gap, the aural training lesson that is more emphasis on reading and singing through interactive multimedia technology should be carefully planned for ensuring future music teachers equipped with good singing skills. a number of researchers have written about the necessity for quality criterions to certify the academic reliability of technology based programs [cit] . [cit] mentioned that the accessibility of well-structured, effectively applied, and proficiently conveyed teaching material is crucial in order to fulfil the unique needs of current adult learners. furthermore, the importance of raised attentiveness on listening and aural skills that incorporated with interactive multimedia technology will be able to formulate creative development in other parts of music program, such as composition and music performance. 2017, vol. 7, no. 9 issn: 2222 this study is one step in serving to construct a comprehensive base of music theoretical knowledge learnt by hands-on experiences. this research aims to develop and test an interactive multimedia courseware for teaching and learning aural as a material for independent study at the faculty of music and performing arts, upsi. this study has also received the response and feedback from students and music education experts into consideration of the suitabality of the courseware as self learning tool. specifically, this study leads to answer to these questions:"
"the package manager will be modified to detect attempts by the forensics tools to install their applications on the phone, and reject the installation. in this case, the tools will be permitted to report errors to the forensic analyst, but no data should be returned."
"the program revealed that xry makes relatively few queries in total, retrieving an entire data module with each query. the modules retrieved are raw contacts and data. it is possible for xry to interpret this data using information found in the android api reference manual [cit] . the extracted contact list, seen in figure 1 -xry unmodified phone extraction, matched that entered and displayed in the phone's built-in contact list application."
the phone was reinstalled using a modified cyanogenmod 7.2 operating system. the only modification from default was the insertion of a delay into each query of the contact list provider. extractions of the contact list were performed using cellebrite and xry and the delay increased until the tools presented errors instead of performing successful extractions.
"the contact list database was copied from the phone to a pc. the phone was reinstalled using a modified cyanogenmod 7.2, the only difference from the standard operating system being the contact list provider, which contained hardcoded false data. three sets of false data were tested."
"furthermore, we observe that direct regression alone (case iii above) performs better than detection alone, but overall our detection-followed-by-regression cascade significantly outperforms the two-step regression approach. notably, we found that the proposed part heatmap regression is also considerably easier to train. not surprisingly, the gap between detection-followed-by-regression and two-step regression when both are implemented with vgg-fcn is much bigger. overall, these results clearly verify the importance of using (a) part detection heatmaps to guide the regression subnetwork and (b) a residual architecture."
"more than one specify the authorized audience for the registration. different levels of privacy are managed. in fact, a virtual world café meeting can be public (and followed by all people, connected to youtube), or the registration can be restricted to a small group as for example the scicafe2.0 group that represents the authorized audience."
-we compare the performance of our method with that of recently published methods illustrating that both versions of our cascade achieve top performance on both the mpii and lsp data sets.
"there is a very large amount of work on the problem of human pose estimation. prior to the advent of neural networks most prior work was primarily based on pictorial structures [cit] which model the human body as a collection of rigid templates and a set of pairwise potentials taking the form of a tree structure, thus allowing for efficient and exact inference at test time. recent work includes sophisticated extensions like mixture, hierarchical, multimodal and strong appearance models [cit], non-tree models [cit] as well as cascaded/sequential prediction models like pose machines [cit] ."
"we first observe that there is a large performance gap between residual part heatmap regression and the same cascade but implemented with a vgg-fcn. residual detection alone works well, but the regression subnetwork provides a large boost in performance, showing that using the stacked part heatmaps as input to residual regression is necessary for achieving high performance."
"the increasing integration of mobile smartphones, in today's digitally dependant, highly networked, communication based societies creates an environment that is conducive to encouraging anti-forensics activities. according to the international telecommunications union [cit] there were almost six billion mobile phone subscriptions for a world population of seven billion. [cit], 207.7 million smartphones were sold with android capturing over 50% of the operating system market [cit] . smartphones can be described as general-purpose computers with an attached phone. as such, many people use smartphones for their daily consumption, storage and communications tasks. this makes smartphones a great source of forensic evidence while, simultaneously, presenting interesting analysis challenges."
─ content-based: people collaborate sharing content. ─ group-based: people collaborate gathering around an idea or interest. ─ project-based: people work together on a common task or project such as a development project or a book.
"ii. what is the design of self-learned aural interactive multimedia courseware? the usability of the instructional model to guide both the design and evaluation of self-learned aural interactive multimedia courseware illustrates the dynamic intersections between theory and best practices. [cit] explains, \"[when] theoretical frameworks inform actions, and actions modify theories so that future actions grow out of what we have learned by experience and reflection, the entire system is energized\" (p. 25)."
"mobile smartphones are highly integrated devices that are built from non-standard components, running software which is often proprietary, undocumented and frequently changed. to perform a component-by-component analysis, an analyst would start by disassembling the phone and removing the surface mounted memory chips, which is a delicate and highly risky procedure. the memory chips can be read by standardized readers, but the interpretation of the data depends on the software running on the phone. a much easier method is to let the phone run, and access the data through the normal interfaces provided by the software. however, this presents a high risk of data being modified, both as a normal function of the phone and/or by specialized anti-forensic applications. the savings in time and effort gained by the utilization of normal interfaces are substantial enough that this technique is endorsed by the association of chief police officers (acpo) [cit] and the american national institute of standards and technology [cit] ."
"the anti-forensic behaviors will inject a delay before returning any data, returning no data, returning data hardcoded in the content provider and/or returning data from an alternate database. the full range of anti-forensics will be implemented for the phone's contact list. to prove that the techniques can be generalized, modifications will be introduced to return no data for queries for the sim contact list and sms messages. when anti-forensics is used, the real data should not be extracted, the intended false data should be extracted and no errors should be reported."
"the phone was reinstalled using a standard cyanogenmod 7.2 operating system. the monitoring application will be installed and started. with the monitoring application running, the usb cable will be plugged and unplugged with the phone in both usb mass storage and usb debugging modes. the messages printed by the application should match the state of the usb cable and settings. two screenshots from the running application are in figure 2 -usb debugging on and in figure 3 -usb debugging off."
"step 10 - [cit], vol. 7, no. sub-topics have been formulated based on the identified key topics. for each sub-topic has been outlined specific learning outcomes that to be achieved by the end of the designed self-directed interactive multimedia learning courseware. the following tables show the distribution of topics, subtopics and the learning outcomes. based on the details of the topics and learning outcomes outlined, subsequently, a form of aural self-learned multimedia courseware has been developed. this courseware incorporating interactive features to enable two-way communication between students and computer to create an active and meaningful learning environment. following the main menu, education process starts by defining the objectives that to be achieved by the user after each units of learning, afterward proceed with the interactive activities and exercises. 2017, vol. 7, no. 9 issn: 2222 369 www.hrmars. [cit], vol. 7, no. 9 issn: 2222 370 www.hrmars.com"
"when the contact list was entered manually, the phone software created an sqlite database file on the phone and stored it in /data/data/com.android. providers.contacts/databases/contacts2.db. this file was copied to a pc in two instances and changed to contain the technical support phone numbers for cellebrite and xry, respectively. these changed database files were uploaded to the phone to be used by the modified contact list provider. these databases were stored in /data/data/com.android.providers. contacts/databases/cellebrite.db and /data/data/com .android.providers.contacts/databases/xry.db."
"we report results for two sets of experiments on the two most challenging data sets for human pose estimation, namely lsp [cit] and mpii [cit] . a summary of our results is as follows:"
"this research aims to investigate the potential effectiveness of android operating system-level anti-forensics modifications. the idea focuses on the modification of the operating system to deceive automated tools. any tool could have been chosen for the examination of the hypothesis. as a matter of convenience, cellebrite (version: app: 1.1.9.4 ufed, full: 1.0.2.7, tiny: 1.0.2.1) [cit] and xry (version: 6.1.1) [cit] were used in this particular experiment. the phone that was used in the research was a htc desire running the cyanogenmod distribution of android. the following steps were executed in this experiment."
"in broad security terms, behavior that is repeated provides a foundation for identification. if that behavior is not the same as that produced by regular use, it presents an opportunity for anomaly detection. the tools in this experiment provided both. every time the tool behavior was observed, each utilized the same specific name for the uploaded application. the tools also queried the content providers in the same way. they both also require the phone to be in usb debugging mode, which is unlikely to be the case for a phone in regular use."
"we implemented two instances of part heatmap regression: in the first one, both subnetworks are based on vgg-fcn [cit] and in the second one, on residual learning [cit] . for both cases, the subnetworks and their training are described in detail in the following subsections. the following paragraphs outline important details about the training of the cascade, and are actually independent of the architecture used (vgg-fcn or residual)."
"the experiment was re-run and displayed the same results from the previous test. for both tools, the phone was disconnected from the usb cable and the contact list inspected using the built-in contact list application. this inspection displayed the number for the respective technical support. after thirty seconds were allowed to pass, the built-in contact list application was re-opened. at that point, it showed the two contacts in the real contact list."
"the scicafe2.0 consortium ran a session for practitioners and academics involved and interested in participatory engagement activities. it was held as part of a conference on innovative civil society organized in copenhagen by the living knowledge network of science shops over 9th to 11th [cit] . the session was entitled scientific citizenship: deepening and widening participation and raising the quality of debating and decision making. the objective of this session was to specify more requirements and features best valued by the potential adopters for our participative engagement tool. the workshop was based on metaplan methodology and aimed at eliciting enablers and barriers from the participants to take part in on-line discussions. the workshop generated a wide variety of insights regarding user requirements; the observation we would like to draw attention to was the repeated emphasis on the social dimensions and constraints: synchronicity, emotions, resonance, collaboration, attendance, reputation and reaching consensus."
"the combination of destructive data techniques with operating system modification should be explored as well. the improved hiding and triggering properties found by implementing anti-forensics in the operating system over using a standalone application would also be able to hide destructive anti-forensics routines. for example, the package manager could be extended to not only reject the installation of forensics tools, but use the installation attempt as a trigger to perform a complete wipe of the phone. this would free the anti-forensics routines from the timing constraints which apply when they run as a separate application."
"where p n ij denotes the ground truth map of the nth part at pixel location (i, j) (constructed as described above) andp n ij is the corresponding sigmoid output at the same location."
future mobile phone anti-forensics work will examine additional options for triggering anti-forensics behavior at both the operating system and the hardware levels. additional work should also investigate solutions at these levels that would counter additional extraction techniques like physical extraction or chip removal. more sophisticated triggers could include investigating single or multiple calls to areas of the phone such as raw_contacts. these calls could also be coupled with a series of other activities and/or states of the device. monitoring queries on a mobile device in conjunction with the state of the device could provide insight into device data traffic patterns.
"previous work has triggered anti-forensic behavior either on finding log-entries relating to the installation of forensics tools or on connection of a usb cable. reading logs requires waiting for the log messages to show up along with spending processing effort to read and interpret the log files. this is complicated by the fact that usb connections are frequent in everyday use which, potentially, generates a substantial amount of data to process."
"such a classification allows to group on-line participatory platforms basing on their primary functionality, identifying 10 paradigms or building blocks of on-line participation [cit] : inip -interactive information provider; ast -ask-tell; codi -collective discussion; direp -discussing for reaching power nodes; rep -reaching power nodes; cost -consulting stakeholders; shago -sharing goods; mapmapping; code -co-design; cops -collective problem-solving. these paradigms are considered as 'bricks' with which real participatory platforms are composed."
"the proposed cascade is very simple, can be trained end-to-end, and is flexible enough to readily allow the integration of various cnn architectures for both our detection and regression subnetworks. to this end, we illustrate two instances of our cascade, one based on the more traditional vgg converted to fully convolutional (fcn) [cit] and one based on residual learning [cit] . both architectures achieve top performance on both mpii [cit] and lsp [cit] data sets."
"the radius defining \"correct location\" was selected so that the targeted body part is fully included inside. empirically, we determined that a value of 10px to be optimal for a body size of 200px of an upright standing person."
"where m n (i, j) and m n (i, j) represent the predicted and the ground truth confidence maps at pixel location (i, j) for the nth part, respectively."
"the proposed architecture is a cnn cascade consisting of two components (see fig. 1 ): the first component (part detection network) is a deep network for part detection that produces detection heatmaps, one for each part of the human body. we train part detectors jointly using pixelwise sigmoid cross entropy loss function [cit] . the second component is a deep regression subnetwork that jointly regresses the location of all parts (both visible and occluded), trained via confidence map regression [cit] . besides the two subnetworks, the key feature of the proposed architecture is the input to the regression subnetwork: we propose to use a stacked representation comprising the part heatmaps produced by the detection network. the proposed representation guides the network where to focus and encodes structural part relationships. additionally, our cascade does not suffer from the problem of regressing occluded part appearances: because the part heatmaps for the occluded parts provide low confidence scores, they subsequently guide the regression part of our network to rely on contextual information (provided by the remaining parts) in order to predict the location of these parts. see fig. 2 for a graphical representation of our paper's main idea."
"artifact wiping is the act of overwriting data so that it is impossible to restore, even with un-deletion techniques. while the overwritten data will be irrevocably destroyed, kessler [cit] notes two weaknesses with this class of techniques. the first is that it may miss some data and, second, it may leave traces of wiping activity, most notably the presence of the wiping tool. a large portion of the existing android anti-forensic literature is concentrated on artifact wiping."
the proposed part heatmap regression is a cnn cascade illustrated in fig. 1 . our cascade consists of two connected subnetworks. the first subnetwork is a part detection network trained to detect the individual body parts using a per-pixel softmax loss. the output of this network is a set of n part detection heatmaps. the second subnetwork is a regression subnetwork that jointly regresses the part detection heatmaps stacked with the image/cnn features to confidence maps representing the location of the body parts.
"generally, pre-service music teachers are required to have skills and knowledge necessary for effectively carry out the music curriculum. based on needs analysis of the study, researchers have identified several topics which essential in aural skills such as identifying interval, melody, triad, chord, cadence and rhythm. in addition, inclusion of solfege sight singing also been given priority for the learning content to enhance the singing skills in various keys, meter and note values."
"the content provider interface is the only way for forensics tools to gain access to information such as the contact list on an un-rooted android phone. on a rooted phone, it would be possible for a forensics tool to bypass the content provider and read directly from the database, but this would require the tool to first find and interpret the database."
"regardless of the triggering mechanism being used, the anti-forensic application then has to delete data before the forensic analysis tool can extract it. all papers using this approach are concerned with this time window. they report measurements of time taken and how much can be overwritten in that time window prior to extraction."
it should be noted that this research is intended to be a proof of concept that instigates a focused examination of contact artifacts using both logical and manual extractions. all other artifacts and interactions with the mobile phones are considered out of scope for this research.
"the goal of our regression subnetwork is to predict the points' location via regression. however, direct regression of the points is a difficult and highly nonlinear problem caused mainly by the fact that only one single correct value needs to be predicted. we address this by following a simpler alternative route [cit], regressing a set of confidence maps located in the immediate vicinity of the correct location (instead of regressing a single value). the ground truth consists of a set of n layers, one for each part, in which the correct location of each part, be it visible or not is represented by gaussian with a standard deviation of 5px."
"since the main purpose of the scicafe2.0 project is to foster communities dialog and inquiry on specific topics, its members usually need to create a collaborative dialogue and to share knowledge and ideas. for this purpose, the scicafe2.0 platform implements the dialog:"
"the world café tool allows users to organize the space of the blackboard in different areas according to the different aspects or objectives that are discussed in the world café meeting and need to be modelled. all users can write a post-it putting it on the blackboard and when necessary moving it (according to established semantics) in the different areas of the blackboard shared among participants at the meeting. the blackboard and its content can be saved for the inclusion in the activity stream containing the world café meeting as one of the activities. after 20 minutes, participants move to another table and add to the content on that table's paper. at the end of the discussion in the table, the documentation (blackboard, forms…videos) of the world café meeting is automatically collected and recorded into the task having the same title of the world café. each virtual world café meeting is part of an activity in the scicafe2.0 platform and more than one virtual world café meeting is usually contained in the same activity. the virtual world café meeting allows users to participate to the stream related to world cafés directly from the activities."
"in this paper the authors have presented an account of their studies in the area of participatory engagement models specifically addressing the aspects of collective awareness platforms and digital social innovation mediating consensus seeking in problem situations. the paper has explored the various influences at play in societal problem situations including socio-psycho-cognitive, social engagement models, constructs and the situated cultural, and ambiguity challenges of the problem environment as well as the methodologically-guided means of reducing ambiguity, thus reducing and delimiting the contexts where there is disagreement and in doing so increasing agreement including about disagreements -towards consensus solution cocreation. the paper also briefly describes the ui-ref framework for problem situation disambiguation and requirements prioritization. the scicafe platform, including the citizens' say tool, is featured as an example of engagement platforms and the world café as an example of a participatory engagement model. the paper concludes with an account of the scicafe 2.0 user requirements elicitation and community engagements and the resulting insights shared."
"the first step was to investigate operating system modifications. in order to achieve this, the source code for the cyanogenmod 7.2 community distribution of android was downloaded, built and installed according to the cyanogenmod project's instructions [cit] . once the phone was running this version of cyanogenmod, modifications were introduced to trace the behavior of the forensics tools. content providers are applications that wrap databases on the phone, performing security checks and format conversions as required by the android specifications. on the assumption that both cellebrite and xry used content providers to access data on the phone, these modifications took the form of altering the content providers to generate logs of how they are called. the merits of this assumption are discussed further in section 4.1"
various methodologies have been proposed for usability requirements and evaluation and impact assessment. the ui-ref methodological framework is outlined here as one of the possible strategies to elicit and prioritise requirements and ensure maximum possible replicability potential for the resulting solution as well as optimal tradeoffs to re local /global and immediate/downstream impacts.
"to empower the stakeholders to achieve a more objective insight about the interplay of influences in the problem space we are reminded that things are most likely to remembered and defended as personal interests worth protecting only in the contexts that they are deemed significant by human beings according to their personal and/or social constructs. accordingly, to work towards a solution, the contexts of the most valued interests of each implicated sub-group have to be made explicit so as to identify both distinct and shared values and possible trade-offs in specific (sub)contexts. this will pave the way for areas of (inter)subjectivity and (dis)agreement to be delimited within specific (sub)-contexts so as to facilitate consensus solution building."
"overview of prior work. recently proposed methods for articulated human pose estimation using cnns can be classified as detection-based [cit] or regression-based [14-17, 27, 28] . detection-based methods are relying on powerful cnn-based part detectors which are then combined using a graphical model [cit] or refined using regression [cit] . regression-based methods try to learn a mapping from image and cnn features to part locations. a notable development has been the replacement of the standard l2 loss between the predicted and ground truth part locations with the so-called confidence map regression which defines an l2 loss between predicted and ground truth confidence maps encoded as 2d gaussians centered at the part locations [cit] (these regression confidence maps are not to be confused with the part detection heatmaps proposed in our work). as a mapping from cnn features to part locations might be difficult to learn in one shot, regression-based methods can be also applied sequentially (i.e. in a cascaded manner) [cit] . our cnn cascade is based on a two-step detection-followed-by-regression approach (see fig. 1 ) and as such is related to both detection-based [cit] and regression-based methods [cit] ."
"the extractions of the phones contact list were performed using cellebrite and xry. neither tool saw the two real contacts. both tools displayed a single number which is their own technical support phone numbers. the results are available in figure 13 -xry alternate database and in figure 14 -cellebrite alternate database. the phone was reinstalled using a modified cyanogenmod 7.2 operating system. the contact list provider was modified to return results from alternate databases, as in the previous test. in addition, the provider was also modified to continue returning the same false contact list for thirty seconds after the usb cable had been unplugged regardless of which tool was used for the extraction. the purpose of this delay is to address a scenario where the analyst performs a manual check after the tool has finished its automatic extraction."
"smartphones are complex, integrated devices, often necessitating the use of the entire original system in the analysis. this stands in contrast to personal computers, which consist of discrete components connected through standard interfaces which can be examined one by one, thereby bypassing some protection. for this reason, the published standards condone much more invasive examinations for mobile phones than for pcs [cit] . however, many of these investigations take time and effort, are specific to individual phone models, require individual testing and require potentially in-depth explanations in court environments."
"people collaborate for several evolutionary human biology reasons beyond the acting for themselves. firstly, the genetic component of collaboration implies that collaborating with others is beneficial: even if it is costly or detrimental for the collaborator. this is the main reason for the collaboration in social insects (and in some other animal) and for kin caring -an effective strategy in a small neolithic village that may lead to quite surprising effects on a highly connected society like ours. secondly, sexual selection drives the appearance of ornaments (like the peacock tail), which seems useless or even deleterious for survival, but are fundamental for finding a mate. thirdly, the origin of our intellectual capacities is based on alliances and power. the way this goal is implemented is through rather sophisticated mechanisms of understanding other's wishes (the theory of mind), which is lacking in social-impaired individuals (notably, autistic or suffering from asperger syndrome). fourthly, group selection and natural forms of loyalty to our in-group (accompanied with fierce hatred against out-groups) is limited by our cognitive capacities. we apply different heuristics when facing a chat group (4-5 people) or a small group (up to 10-12 people) or a crowd. all in all, we do not generally act following a deep reasoning, but rather applying \"rules of thumb\" (heuristics) that were successful in our recent (evolutionary speaking) past. how these heuristics determine our behavior in the internet world, the propensity towards collaboration, the importance we assign to privacy and reputation, are among the main subjects of our investigations. in particular, we are interested in how they modify when passing from the \"real life\" world made of physical contacts to the cyber-world, which is missing many of the non-verbal messages we most often rely on. we rely on a tentative classification based on four types and on three functions [cit] . as for types, we defined four categories as follows:"
"both of the tools used the content provider in all observed cases. cellebrite starts by making several queries to the raw contacts and settings modules, collecting general information such as the number of contacts and whether contacts are marked as deleted. it then goes through the raw contacts module, querying for information on each contact. for each contact, numerous queries are made for different kinds of information associated with it (name, phone number, email address, etc.). the extracted contact list matched the information entered and seen in the phone's built-in contact list application."
"the citizens' say knowledge exchange is used by the scicafe2.0 platform to provide the required access to external knowledge and additional functionalities such as recommendations, keyword extraction, named entity recognition, text enhancement (annotation) as well as a parametric description of the way citizens have responded to a participative engagement session -as required and envisaged within the scope of the scicafe2.0 project. thus the citizens' say knowledge exchange provides access to external repositories of information (e.g. dbpedia) and also makes recommendations to the scicafe2.0 users; suggesting activities/events depending on each user's specific interest (relevant profile) and activities description. this allows the scicafe2.0 tool to search for individuals, organizations or events that are present in the external knowledge repository. one other feature of the citizens' say knowledge exchange is the annotation tool, which provides enhanced text information or links, by linking important entities to wikipedia or dbpedia articles."
"another pattern of design for artifact wiping, adopted by several researchers [cit], uses an application on an unmodified (or rooted) android phone to detect the presence of a forensic analysis tool and start deleting data."
"data hiding on mobile devices will implement substantially similar approaches to that of personal computers like steganography, deleted files, and storing data in the cloud or in other users' storage space. the caveat with this approach on mobile devices is that recovery of deleted files depends on the file system. many mobile devices use a version of yet another flash file system (yaffs) [cit], which may be unsupported in commercial forensics tools."
"the vgg-fcn subnetwork used for body part detection. the blocks a1-a9 are defined in table 1 . regression subnetwork. we have chosen a very simple architecture for our regression sub-network, consisting of 7 convolutional layers. the network is shown in fig. 4 and table 2 . the first 4 of these layers use a large kernel size that varies from 7 to 15, in order to capture a sufficient local context and to increase the receptive field size which is crucial for learning long-term relationships. the last 3 layers have a kernel size equal to 1."
"1. tools are components used in online participatory activities; 2. toolkit is a collection of tools that are used in online participatory methodologies; 3. technique/application is a tool/toolkits put into action (implemented tool/toolkit); 4. method is a combination of tools, toolkits, techniques put together to address defined goals."
"the phone was reinstalled using a modified cyanogenmod 7.2 operating system. the only difference from the standard code was the modifications to the package manager for rejecting installation of applications named 'com.client.appa' or 'example.helloandroid' which previous experiments determined were the names used by the applications uploaded to the phone by cellebrite and xry, respectively. extractions of the contact list were performed using cellebrite and xry. the extraction results were compared to results from an unmodified cyanogenmod."
"noisemaker is simulation software for creating realistic, virtual high-throughput screens that can be used to evaluate hit identification methods and quality criteria. we establish its power by using it to clarify the utility of the b score under various screening conditions. this tool will be useful for broader comparisons of available hit identification methods, and is freely available for download and use by others interested in modeling screens in silico."
"in simulation 2, we tested whether the dual model has the same characteristics as animals reported by the previous studies. we analyzed the model with four experimental manipulations: the rate of reinforcement, the deprivation level, the schedule type, and the presence of extinction. the rate of reinforcement, the deprivation level and the presence of extinction affected the bout initiation rate and the bout length and adding the tandem vr schedule to the vt schedule affected only the bout length."
"their model has three free parameters: p, ω, and b, each of which correspond to different components in the bout-and-pause pattern. first, p denotes the mixture ratio of the two exponential distributions in the model and it corresponds to the length of a bout. the bout length is the number of responses contained in one bout. second, w denotes the rate parameter for the exponential distribution of within-bout irts and it corresponds to the within-bout response rate. finally, b denotes the rate parameter for the exponential distribution of between-bout irts and it corresponds to the bout initiation rate."
"hits may represent either an increase or a decrease from the default value. they are specified by their unique name, strength and frequency; the latter number can be either an absolute value (e.g. eight wells) or a percentage of the non-control wells (e.g. 8% of the wells). for each hit type, the noisemaker software will randomly select, without replacement, the appropriate number of non-control wells and assign them the value specified for that hit type. the hit type input can also be used to model random equipment or assay failures that could be mistaken for hits. the output of the 'true hit' generation is a plate map file that is annotated with the true values for every well and the type of value for that well (which is either 'default' or the well's assigned control or hit name). for convenience, this file's name is automatically copied to the input field of the 'noise' tab (fig. 1b) ."
"noise definitions are additive; e.g. if one is specified for the entire screen, one for plate 2 and one for row 5, then all values in row 5 of plate 2 will be permuted with the screen noise, the plate noise, and the row noise combined. this simulates the convergence of disparate systematic effects in real screens. all noise definitions model noise as a gaussian perturbance of the true values. they adjust the well values away from their initial values by approximately the amount assigned to the noise definition's mean change value, with the exact amount of adjustment being randomly chosen from a gaussian distribution centered on the mean change value and with the specified standard deviation (sd) value. this ensures that each noise definition produces realistically noisy adjustments even as it introduces the intended systematic effects. noise can also be limited to a specific range (such as that simulating an instrument's detection range) using optional floor and/or ceiling values. the output file contains the input true values and one column of noisy values for each simulated replicate."
"since the decay of any of the three model parameters p, ω, and b can cause extinction, we need to find which of these parameters actually decayed during the extinction simulation. we excluded ω because it was fixed during the simulation. to identify whether one or both of the p and/or b parameters decayed, we compared three models, that is, the pb-decay, p-decay, and b-decay models. we calculated waic (widely applicable information criterion) for each model. figure 4 shows the log-survivor plots of irts from each of the four simulations. in figure 4 (d), the total number of irts during the extinction phase was insufficient to reliably estimate the right limb. then, we analyzed a dynamic bi-exponential model fitted to the irts during extinction. table 3 shows the waic values for the three models. the smallest waic was attained by the pb-decay model."
"after data sets with appropriate noise were created by noisemaker, we calculated b scores for all wells and identified the wells whose scores were in the top 1% as positives. we found that the true positive rates of datasets in group a decrease and the false positive rates slightly increase as sd of the row and column noise increases (supplementary appendix ii). however, the true positive rates and false positive rates of datasets in group b remain steady regardless of the amount of mean change of the row and column noise, while the true positive rates and false positive rates of data sets in group c behave similarly to those in group a. these results suggest that b score is an appropriate choice for correction of systemic influences that primarily affect mean rather than variance. notably, makarenkov's work examined simulated data with varying sds, which is consistent with this finding."
"figure 2(a) shows event records of irts generated by each model and figure 3 shows the model schemes with transition probabilities between each state. the no choice model generated a dense repetition of only the operant behavior with a high rate without long pauses (top panel). the probability the agent stayed in the operant state was empirically 0.95, which is a good match with the theoretical value of p stay defined by equation 6. in the middle panel, the response rate under the no cost model was low and each response was separated by long pauses. the probability of the agent choose to transit to the operant state was empirically 0.06 and the agent returned to the choice state immediately after it responded. in the bottom panel, the agent with the dual model generated a repetitive pattern of responses with a high rate in a short period followed by a long pause. the agent in the choice state made a transition to the operant state with a .12 probability and it stayed in the operant state with a .71 probability."
parameter x denotes the number of responses that is emitted to obtain a reinforcer from a bout initiation. the other parameters are the same as equations 2 and 3.
"the difficulty in assessing the performance of hit identification methods can be avoided by moving to in silico-based strategies. in the computational environment, one can generate a virtual screen containing defined true hits at known locations, and then perturb these true values with varying degrees and types of noise (both * to whom correspondence should be addressed. systematically biased and random) to simulate the variation inherent in biological screens; statistical techniques can then be evaluated based on their ability to identify known true positives and true negatives. these evaluations will be valid to the extent that the in silico hit distributions and types of noise are congruent with those of the real system. this approach offers both speed and flexibility, providing the opportunity to profile a method's performance in many different realistic screening scenarios as well as the ability to simulate whole screens within minutes."
"animals engage in various activities in their daily lives. for humans, it may be working, studying, practicing sports, or playing video games. for rats, it may be grooming, foraging, or escaping from a predator. although specific activities are different between different species, common behavioral features are often observed."
figure 2(b) show log-survivor plots in to see whether they show a straight line or a broken stick. we used the irts from after the agent was presented 500 reinforcers to the end of the simulation. the log-survivor plots of the no choice model and the no cost model were described by one straight line whereas that of the dual model was described with a broken-stick shape. the no choice model has a steeper slope than the no cost model and is tangential to the curve of the dual model at the leftmost position. the no cost model slope was slightly steeper than that of the dual model at the right side.
we used only the dual model in simulation 2. we performed four experiments by manipulating only one of the four variables while keeping the other three variables the same as simulation 1. the procedure of simulation is also the same as simulation 1.
"currently noisemaker is limited to gaussian noise distributions, additive noise and linear positional effects. future development will address non-gaussian and multiplicative noise, as well as bowlshaped (non-linear) positional biases."
"on the tab for generation of true hits (fig. 1a), the user inputs a tab-delimited plate map file containing reagent identifiers represented by one row per well and a default 'true' value to be assigned to all reagents that are not treated as hits or controls. controls are specified by reagent identifier and assigned a name (such as 'up-regulating positive control') and a true value. the user may specify as many types of controls as desired as long as each has a unique name; e.g. an sirna-based screen might have lipid controls, negative controls for transfection and positive controls for both up-and down-regulation, all with different identifiers and different expected values. all instances of a control's reagent identifier in the plate map will be assigned the value specified for that control type."
"simulating bout-and-pause patterns with reinforcement learning 15 figure 5 shows the boxplots of q pref and q cost in the three simulations except for extinction, which are to be used for assessing how the changes in the bout components are mediated. we excluded the extinction simulation because we already know that q pref causes the change of the bout components since q cost is fixed during the extinction phase. the top panel show that q pref and q cost increased as the rate of reinforcement increased. the middle panel indicates that increasing the deprivation level moved q pref and q cost upward. from the low panel, we can see that adding tandem vr schedule increased q cost without affecting q pref ."
"noise can also be added to true values files not created with noisemaker, as long as they conform to the expected format; this can be useful for modeling the effect of noise on 'clumped' hit distributions such as those from non-randomly plated screening libraries. on the noise tab, the user specifies the plate dimensions and number of replicates and then describes the systematic noise to be applied. noise can be applied at the level of several different elements of the screen, including the entire screen, the edges of every plate, an individual plate in the screen, a particular row on every plate, a particular column on every plate and/or a particular well on every plate. this allows the user to simulate a wide variety of realistic outcomes, from evaporation of reagents in edge wells to a blocked dispensing tip at a single well position."
"the four experimental manipulations are applied independently to each of the four variables: 1) the rate of reinforcement, 2) the deprivation level, 3) the schedule type, and 4) the presence of extinction. 1) we manipulated the rate of reinforcement by when we analyzed the irts data from the extinction simulation, we used a dynamic bi-exponential model [cit], in which extinction causes the exponential decay of model parameters, p, ω, and b, according to the following equations."
"data analysis and hit identification are points of confusion for many screeners [cit] . those asking questions such as 'which method identifies the most \"true hits\" for my particular screen circumstances?' or 'what will the false positive rate of my chosen method be?' are frequently stymied, since answering these requires them to know the identity of the real hits. however, developing a list of the anticipated real biological hits for any given assay is extremely challenging and is likely to be both noisy and incomplete, especially for medium-to weak-strength effects."
"however, some results imply that these variables affect not only the bout initiation rate but also the bout length [cit] . schedule-type manipulations mainly change the bout length [cit] there are opposite results that adding a small vr schedule in tandem to vi schedule has no effect on the within-bout response rate [cit] . when a vi schedule is followed by a small vr (tandem vi vr), an animal stays in a bout longer and emits more responses in each bout. [cit] showed that if we add a small vr schedule in tandem to a vi schedule, the bout initiation rate decreased slightly. these previous findings are summarized in table 1 model. we designed a three-state markov process for modeling bout-and-pause patterns (figure 1(a) ). two of the three states are \"operant\" and \"others,\" in which the agent engages in an operant or other behaviors, respectively. in the third \"choice\" state, the agent makes a decision between the operant and other behaviors. by having the choice state in our model, we incorporate the fact that animals can choose their behavior from available options (e.g. grooming, exploration, and excretion) when they move freely during an experiment. the second fact that is observed in animal behavior is a cost required to make a transition from one behavior to another. animals must decide whether to keep doing the same behavior or to make a transition, because fast switching is not optimal if a transition incurs a cost. here is how the agent travels in the proposed model. the cost q cost is another function that defines a barrier in making a transition from the engaged behavior to the choice state. we assumed that the cost is independent from the preference and only depends on the number of responses that is emitted to obtain a reinforcer from a bout initiation. when a reinforcer is presented, the cost function q cost is updated according to:"
"bout-and-pause patterns are one of the behavioral features commonly observed in many species. activities engaged by an animal do not occur uniformly through time but often have short periods in which a burst of engaged responses is observed. for example, in an operant conditioning experiment, a rat presses a lever repeatedly in a short period and then it stops lever pressing. after a moment, the rat starts lever pressing again. the rat switches between the lever pressing state and the no lever pressing state repeatedly throughout the experiment. such a temporal structure comprising of short-period response bursts and long pauses is observed in various species and activities; for example, email and letter communication by humans [cit], foraging by cows [cit], and walking by drosophila [cit] showed that bout-and-pause patterns can be described with a broken-stick shape in the log-survivor plot of interresponse times (irts), which are characterized by a bi-exponential model. if irts follow a single exponential distribution, then the log-survivor plot shows a straight line. if irts follow a mixture exponential distribution called a bi-exponential model, the log-survior plot shows a broken-stick shape composed of two straight lines that have different slopes. [cit] shows that lever pressing by rats is well (1)"
"we posit both of the choice and cost mechanisms are necessary to organize responses into bout-and-pause patterns. the no choice model failed because it lacks the choice mechanism. without the choice mechanism, the agent almost always stayed in the operant state and responded with a high rate without pauses. the reason behind the failure of the no cost model was the knockout of the cost mechanism. when the cost of switching behaviors is zero, the agent easily return to the choice state, resulting in operant responses followed by long pauses. the choice and cost mechanisms contribute differently to generate bout-and-pause patterns; the choice mechanism generates pauses and the cost mechanism produces response bursts. since the dual model has the both mechanisms, it reproduced bout-and-pause patterns."
"the three model parameters, which define the overall response rate, are affected by motivational and schedule-type variables [cit] . motivational manipulations include the reinforcement rate, the response-reinforcement contingency, and the deprivation level, and they affect the bout initiation rate. an example of schedule-type manipulations is adding a small variable ratio (vr) schedule in tandem to a vi schedule and they affect the bout length and the within-bout response rate. the reinforcement rate [cit], the response-reinforcement contingency [cit], and the deprivation level [cit] affect the bout initiation rate. as the reinforcement rate and the deprivation level increase, the bout initiation rate increase. extinction decreases the bout initiation rate. [cit] showed that the overall response rate decreased through time under extinction and that the decrement is mainly caused by the decrease of the bout initiation rate."
"in the choice state, the agent chooses either of the operant or others states according to the probability distribution calculated from the preferences for the two behaviors. the transition probability is defined as follows:"
this simulation software offers two main features: (i) the ability to generate a random set of 'true hits' that conform to expected characteristics and (ii) the ability to apply user-specified noise to a list of true hits to model realistically messy screening results.
both the event records and log-survivor plots in figure 2 imply that only the dual model generates bout-and-pause patterns and the other two models failed to reproduce bout-and-pause patterns. the event records in figure 2 reproduced bout-and-pause patterns.
"the place model is used to compute the measurement likelihood in (5) . since the measurements are histograms of word counts, we model them using a multinomial distribution having dimensions equal to the dictionary size. further the prior over the multinomial parameter is the conjugate dirichlet to obtain a measurement z, which is a quantized feature histogram, we first sample from a dirichlet distribution with parameter α to obtain a multinomial vector θ . this multinomial distribution is, in turn, sampled to obtain the measurement histogram y. note that a different θ has to be sampled for each y. each α corresponds to a place or place category. distribution to aid in ease of computation. given a histogram measurement y, its likelihood according to (5) is"
"it has been proven that that selecting items randomly still provides certain guarantees of correctness. however, twister tries are still sensitive to unlikely hash outcomes. these unlikely, but possible, outcomes could affect the final dendrogram produced in a disastrous way. in practice, this would mean that a trie contains a pair of clusters which splits much lower than it should, which will affect the quality of the whole clustering negatively. it might also happen (although with a much lower probability) that a pair of clusters does not branch low enough in any of the tries, causing their clustering to happen too late."
"what we notice from the graphs is that the a posteriori trie removal is not only able to remove the bad trie from the forest; it also improves beyond the result of the standard twister tries algorithm. this means that the metaheuristic also removes other tries (which were not intentionally bad, but just created randomly) from the forest and improves the result further. as was noted by cochez and mou [cit], once the quality is at a certain level making a small improvement requires a lot more and higher tries, or in other words resources. hence, these improvements are significant."
"current memory systems generally seek to achieve high capacity and high throughput, which is effectively achieved by optimizing the memory interface for coarsegrained, sequential accesses. hence a coarse-grained memory system performs well on programs with high spatial locality, increasing peak memory bandwidth while amortizing control overheads. not all applications, however, can be re-factored to maximize memory bandwidth utilization because non-unit strides, indexed gather / scatter accesses, and other complex access patterns inherently exhibit very low spatial locality. previous studies have demonstrated that, for applications with low spatial locality, having fine-grained memory access capability can help in avoiding unnecessary data transfers, providing substantial improvements in terms of powerefficiency, memory bandwidth utilization, and overall system performance [cit] . the cache hierarchy and the memory subsystem of these adaptive granularity memory systems (agms) can effectively handle a mix of both coarse and fine granularity memory accesses. below we discuss some key architectural features that are necessary to enable fine-grained management and storage of data in the cache-memory system. we do not propose any innovation in these basic mechanisms and therefore keep the discussion short and refer the reader to prior publications for details [cit] ."
"a sample result from pliss on a sequence with 5 labels is shown in figure 1 . to test our claims, we present experiments on the large and difficult visual place categorization (vpc) dataset [cit] . we also provide comparisons using different types of features, as well as comparisons of the performance of pliss against the vpc system [cit] ."
"when a data is read out of the i-th subrank, the formula of equation 4 is again used to regenerate a check symbol p i, which is compared to the stored symbol read from the i chip. any single corrupted symbol will cause the check symbol to always be different and thus detected. errors in multiple symbols may lead to an undetected error because the limited domain of the galois field may result in a random match between the recomputed and stored symbols. with a good choice of weights the probability of such an undetected error is just 1 2 n, where n is the symbol length (0.39% probability in our code)."
"in this paper, we present a new place recognition method called pliss, for place labeling through image sequence segmentation, which tackles these problems. as its name suggests, pliss works with video or image streams, thus intrinsically considering the temporal component of the problem. we take the novel approach of using change-point detection to segment the image streams into portions corresponding to places. change-point detection is the problem of detecting abrupt changes to the parameters of a statistical model. the locations of these abrupt changes within the image stream are also the place boundaries, where a place is entered or exited."
"detection of an unknown place requires more involved calculation. in this case, we are required to show that the data does not arise from any of the models corresponding to known places. a systematic way of arriving at this conclusion involves statistical hypothesis testing. hence, at each timestep, we perform l hypothesis tests to determine if the data arises from a known place. if these tests are expensive, they may also be performed once every t timesteps."
"the ahc is expected to be below 1 for an approximate dendrogram. this is because a non-optimal merging decision might have been taken and hence the sum of the distances will be larger than in the optimal case. note, however, that a result larger than 1 is possible if the exact algorithm performs a clustering at a lower level forcing it to make a unfavorable clustering at a higher level. this might, for instance, happen if two clusters are at the same distance and the exact algorithm can choose among them. however, a benefit of this metric is that it is indifferent regarding insignificant re-orderings of the dendrogram. this means that if a re-ordering occurs which does not change the shape of the dendrogram, then the jdr will produce the same outcome."
"this section quantitatively shows the high reliability level of clean, which approaches that of a commercial chipkill implementation [cit] . we chose this particular commercial implementation because it matches our (b) sdc probability figure 11 : system level reliability over 5 years and varying memory capacity (using 16gib dimms): (a) the expected reliability impact of dues; and (b) the risk that the system has at least one sdc."
"this means that a hash function is proportionally sensitive if the probability that it hashes points p and q at distance d(p, q) to the same value is equal to 1 − kd(p, q), with k a predetermined constant. several examples of proportionally sensitive hash functions (i.e., for cosine, jaccard, and hamming distance) were shown in the original twister tries paper. for this work we will be using random hyperplane hashing (rhh) [cit] for the cosine distance and minhash [cit] for jaccard distance."
"thus, although optimization is not the focus of this paper, we have attempted an algorithmic design tailored to the problem. the binary nature of the problem, the active/inactive trie associated to a certain index, allows a natural binary encoding of the information. for example, the binary representation [cit] corresponds to the first trie being disabled, the second and third one active (see also fig. 2 ). in order to evaluate a candidate solution, the quality of the corresponding dendrogram is evaluated according to one of the procedures described in section iv."
"we established in the previous subsection that the jd is reasonably computable for moderate datasets. unfortunately, within the context of optimization algorithms, a quadratic growth of the complexity (o(n 2 )) can lead to very demanding tasks when large scale cases are taken into consideration. in the latter case the computational time to achieve an optimal solution can be unacceptable. therefore, we developed alternatives which are computationally less demanding but still preserve the properties required to find a satisfactory solution. these functions, which we will call surrogates or approximations, work well if they are able to assign a larger value to a dendrogram with a larger joining distance. or, in other words, the values produced by the surrogate should allow an ordering of the dendrograms which is the same as (or at least similar to) the order of the dendrograms by joining distance. the quality of the surrogates is measured as the fraction of dendrogram pairs the surrogate manages to order correctly. [cit] :"
"fix b for each merge step, we have a fixed budget b of distance calculations. meaning that to estimate the average distance we sample b pairs and calculate their average. sqrt instead of a fixed budget, we use sample √ number of pairs pairs to make an estimate. sub at each internal node, we create pairs by pairing all items from the smallest subtree with a randomly selected item from the largest subtree. the result is the average of their distances. w b at each merge step we select b pairs by traversing the dendrogram downwards and at each branching, continue to the left or right with equal probability. then, in the average distance computation we weigh the pairs. the weight of a pair is the product of the depths in the subdendrograms. the rationale behind this choice is that if an item is deeply nested in the dendrogram, then it is likely that a) there are many other ones which are close to this item and b) items in this dense cluster are unlikely to be chosen randomly."
"while the error coverage of the inner code is already high, the outer code can be used to further improve detection and curb the risk of sdc. when performing a coarse-grained access, if no error is reported by any of the inner-code checks, we can use the outer-code information to verify the parity of each bit location in the access. if any parities do not match, at least one chip has an error that was not detected by the inner code or the o chip has an error. in other words, if any single chip, other than the o chip has an error, coarse-grained detection is guaranteed to detect the error, no matter how many bits in the data chip are erroneous."
we propose several options for approximating quality and evaluate their performance. the main target is to limit the number of pairwise distances computed in the process by estimating the average distance for each merge by a lower number of individual distance calculations.
"the basic assumption underlying pliss is that places are sufficiently distinct to be identified visually. if this is not the case, i.e. the environment is severely perceptually aliased, performance will degrade as with any vision-based system. however, pliss can be easily extended to incorporate multiple sensors to overcome such scenarios."
"the largest benefits from adapting granularity are the increased energy efficiency of the dram subsystem because fewer chips are accessed and the performance improvements from better bandwidth utilization. we compare dram power consumption, system throughput, and power efficiency of three coarse granularity"
to get a deeper insight into the approximations of the joining distance and to evaluate our algorithm we performed two series of experiments. in the first series we investigate how useful the proposed approximations are by applying them on realistic dendrograms and comparing their outcome with the one obtained using the exact joining distance. we also discuss the execution times and select reasonable ones for the second for individual in pop do 4:
"we use an online bayesian change-point detection algorithm that computes the probability of a change-point occurring at each timestep. the algorithm keeps track of all possibilities and does not make an irrevocable decision at any step. the probability of a change-point at any given timestep is obtained by combining a prior on the occurrence of change-points with the likelihood of the current measurement given all the possible scenarios in which change-points could have occurred in the past. we demonstrate that this computation can be done exactly for certain classes of prior and likelihood functions. since the memory requirement for the exact algorithm increases linearly with the number of measurements processed thus far, we also provide an approximation using rao-blackwellized particle filters that is fast and requires only constant memory."
"given the high arithmetic performance of chip multiprocessor, the performance bottleneck is often the main memory system. this is particularly true for applications with poor spatial locality because memory systems are typically optimized for cache-line granularity access and squander performance and power efficiency when frequent finer-grained accesses are required. memory systems that can adapt to available spatial locality have been shown to alleviate the memory bottleneck and improve both performance and efficiency [cit] . these mechanisms enable accessing only a subset of the memory devices in each memory rank to reduce power consumption and allow the overlap of memory requests to different groups of devices in a rank (subranks) to improve performance."
"from the original jdr, we derive a metric which we will call the joining distance (jd). the jd is defined as it is easily seen that the jdr is obtained by dividing the jd of the exact dendrogram by the jd of the approximation. note that the jd measure does only make sense in the context of a specific clustering task. in other words, it does not make sense to compare the jd computed from a dendrogram for one clustering task and compare it to the jd obtained from another one. moreover, it is important that the jd is only computed for complete dendrograms. when comparing two jd values, the dendrogram which resulted in the lowest value is better than the one with the higher value."
"i. introduction place recognition is the task of consistently labeling a particular place every time it is visited, while place categorization is the corresponding problem for a category of places. such labels may range from \"place no. 1\" and \"kitchen on 2nd floor with microwave and coffee machine\", which are labels for particular places, to \"kitchen\" and \"corridor\", which are category labels. place recognition and categorization are essential in order for a robot or an intelligent agent to recognize places in a manner similar to that done by people. place recognition facilitates human-robot communication [cit] and is an integral part of semantic mapping procedures [cit] . for instance, the label of a place strongly affects, among other things, the types of objects found there [cit] ."
"we approach the problem by noting that the place label remains the same for the period of time when a robot is moving inside the particular type of place. it only changes sporadically when the robot travels into the next place. thus, the measurement stream can be segmented into pieces corresponding to places, i.e. measurements in each segment are assumed to have been generated by the corresponding place model. the start and end of segments where the generating model changes, hence called change-points, provide a very strong indication regarding the place label."
"it seems unfeasible to see directly from a trie that this type of issues exists, in a reasonable amount of time (i.e., less time than it would take to perform an exact clustering of the data). moreover, if we would discover a trie which looks bad in isolation, it might still contribute positively in the overall clustering because of the effects of other tries. what we will do instead of looking inside the tries, is treating each trie as a black box. then, we attempt to find the combination of tries which produces the dendrogram with the best quality. in the next section we will develop a way to decide the quality of a dendrogram. in the section after that we will propose a metaheuristic which allows us to find a good combination of tries within reasonable time."
"while the storage overhead of clean equals that of chipkill, the hardware implementation of the encoders and decoders is different. we estimate the expected hardware requirements for clean and compare them to those of chipkill by implementing its encoder in verilog and synthesizing and evaluating our designs with the cadence design vision (dve) targeting the freepdk45-gscl45nm [cit] ."
"almost all existing place recognition systems would be unusable if no training data were available. however, pliss is still able to segment the data into pieces corresponding to different places and learn their models online. hence, this experiment also tests the online learning capability of our system. output was obtained for six image sequences from the vpc dataset containing a total of 14,346 frames. the total number of changepoints in these sequences, taken to be frames where the place label changes, was 76. the changepoint detection performance using various features is shown in figure 5 (a). sift features perform best while texture features perform the worst. the system was declared to have found the change-point if it fired within 20 frames of the groundtruth. note that many of the change-points, especially involving \"transition\" regions, are hard to recognize even for people."
"clean achieves up to 21 − 40% reduction (32% on average) in terms of dram power consumption, mainly because the fine-grained accesses in benchmarks with low spatial locality reduces both the number of chips that are activated and those that communicate data ( figure 8 ). looking into the breakdown of power consumption, the reduction comes from both fewer activations (act) and fewer reads and writes (rw). background power is roughly proportional to the number of chips in a rank and is unchanged. clean provides a more modest reduction than dgms because clean requires 16b-wide data subranks as opposed to the 8b subranks used by dgms. the benefit of clean is in its much higher reliability."
"because agms grants both coarse and fine-grained accesses, the cache hierarchy must be capable of effectively handling both types of accesses. to amortize cache tag-array overhead, a sectored cache design [cit] or a more sophisticated derivative, can be used. in a sectored cache, each cache-line is partitioned into multiple sectors where; each sector has its own valid and dirty bits but all the sectors within the cache-line share a common address tag. thus, the cache maintains the information needed for fine-grained reads and writes with very small overhead."
"given these issues with most metrics, we investigated the use of the metric proposed by kull and vilo [cit], since it seems computable and can be adapted for the computation of the quality of a dendrogram in isolation (i.e., without an exact dendrogram available). in the next subsection we introduce this metric, analyze its computational properties, and propose computationally more attractive alternatives."
"an sdc occurs when the inner code fails to detect an error, which happens at most once out of 2 8 different data/error pairs on average. thus, multiple sdcs will only occur if unmodified data is read from a faulty location multiple times without either being overwritten or without other reads to different memory blocks that are also impacted by the same fault. in other words, the danger of multiple sdcs is from scenarios that are quite unlikely where software repeatedly performs fg reads from a single memory block with no intervening writes to that location and no accesses to other locations in the same dram row, bank, or chip (the faults that may lead to sdcs in the first place). any read of different data or a different memory location under the same fault has an independent chance of detecting the, thus the probability of an sdc decreases exponentially (a geometric distribution) with the number of such detection attempts."
"the second characteristic is that a partial writeback implies that there are invalid sectors in the cache block. thus, to avoid reading the entire rank or even just the ecc information from memory before a partial writeback, we store the original ecc information read when the cache was filled in an empty sector, as shown in figure 5b . when writing back a partial line, the first invalid sector following the last sector to be written holds the original ecc information, which is then updated as shown in equation 3 and written back to memory along with the data (figure 5c ). note that we read and cache, but do not use all the ecc information on a fg access. hence, data transfer errors that may occur on this unchecked ecc information may go undetected and in such a case a partial writeback may lead to data corruption. we have not seen fault models for transfers and do not evaluate the risk; however, we expect that it is very small."
"the server used in our experiments has two intel xeon e5-2670 processors. except for exact clustering (using python, numpy, and fastcluster) all evaluations are performed using an openjdk 8 64-bit server vm limited to use a maximum of 120 gb ram."
"to find out whether the proposed surrogates resemble the actual joining distance we performed two large experiments. the experiments differ in the dataset and the number of items used. in the first experiment, we used 20 dendrograms (190 pairs) representing the clustering of 10,000 images from the cif ar-10 dataset. while in the second experiment, we used 20 dendrograms for 30,000 newspaper articles. the moderate number of items used in these experiments is due to the fact that we need to compare the findings to exactly computed joining distances. the dendrograms created for these experiments are such that their joining distances are pairwise close to each other. (this is achieved by using the original twister tries algorithm with different seeding.) for each of the surrogates proposed above, and for several possible settings of their parameters, we measure the quality as defined in section iv-c. further, we measure the (average) time needed to evaluate the surrogate function. the results of these experiments are in tables i and ii. what we observe from these experiments is that overall the surrogates perform fairly well, while using much less time than the exact computation. further, the fixed and weighted computations use time linear in the number of samples performed and, with a few exceptions, the quality raises when more samples are taken. the exceptions are most likely due to random behavior. from a quality perspective, the sub and sampling the square root of the number of pairs samples surrogates perform poorly. they need more time than other surrogates which produce results with better quality."
"because clean accepts both coarse and fine-grained accesses, the cache hierarchy must be capable of handling both data granularities. clean utilizes a simple sectored cache to manage on-chip management and storage of fine-grained data. each 64b cache-line is divided into four 16b sectors. cache coherence is maintained at the granularity of a full cache line; sectors cannot be modified separately by multiple core."
"algorithm 2 describes, in greater details, the implementation of the proposed metaheuristic approach. an overview of the way the fitness function works can be found in fig. 2 . first the tries which have a zero bit in the vector are inactivated. then, the twister tries algorithm is used to generate an approximate dendrogram. finally, this dendrogram is evaluated using a dendrogram quality function, which produces a result which resembles the joining distance."
"if the o chip has an error and the inner-code does not detect an error, the implications are that either multiple chips faulted, or that all four independent innercode checks had an undetected error simultaneously despite having no errors -an extremely low probability ( 2 −8 4 ) that does not impact practical coverage."
"the consequence of this analysis is that the jdr is also computable in o(n 2 ) and somewhat scalable. therefore we will use it for analyzing the quality of the produced dendrograms of moderate sizes. however, if we want to use a metric for improving the quality of the clustering, we will need a faster alternative. the reason is that if o(n 2 ) operations are needed for the evaluation of one dendrogram, it is better to use this computational power to compute the exact dendrogram directly. therefore, we will propose several approximations for the jd in the next section. these approximations have a lower practical or theoretical complexity."
"because there is a gap between the sdc risk of fg and cg accesses, each application has a different sdc rate; a larger fraction of fg accesses results in greater sdc risk. figure 12 [cit] application based on their ratio of fg and cg accesses (profiled with a pintool [cit] ). applications with high spatial locality have low sdc whereas the sdc probability of applications that have lower spatial locality is dominated by the lower reliability of the inner code. applications that are not bandwidth intensive use clean-cg only."
"given a set of d images with features detected on them, the maximum likelihood value for α can be found by optimizing using gradient descent. it can be shown that this leads to the following fixed point update for the α parameter [cit]"
"in this section we describe the insights behind clean and the mechanisms for achieving access granularity- figure 2: data/ecc layout used by clean for spreading ecc locations to reduce subrank conflicts; for calculating the rotated positions, a modulo-9 residue generator using efficient parallel designs is available [cit] ."
"at each generation, parents are selected by means of proportionate fitness selection with a roulette wheel. this choice can be justified by the large size of the decision space (the total amount of possible combinations) and the time constraints due to the simulator (each fitness evaluation requires up till 10 seconds, or more for certain approximations applied on large dendrograms). in other words, in order to quickly achieve an improvement, a high selection pressure has been applied although this choice might result into a premature convergence."
"in this section we present the efficiency and performance benefits of clean and compare them to prior work. we do not dwell on these performance and efficiency results because the main contribution of the paper is in improving reliability. we then evaluate and discuss the reliability implications in detail, followed by an analysis of the hardware implementation requirements."
"system / dram power efficiency. figure 10a shows dram power efficiency, defined as the normalized throughput divided by dram power only. figure 10b shows system power efficiency, defined as the normalized throughput divided by the total system power, including cores, caches, and dram as estimated by mcpat. clean improves both system and dram power efficiency when compared to chipkill by up to 11% and 65% and 5.5% and 42% on average, respectively. these correspond to 67% and 74% (on average) of the benefits of dgms. because the cores consume most of system power as we estimate by mc-pat (42 − 46w out of 50w total), system power efficiency shows less improvement than dram power efficiency. note that some studies have reported a much larger proportion of power in the memory subsystem, in which case the benefit of clean will be closer to the improvement shown in figure 10a ."
"to allow comparison with the original twister tries paper we obtained the same datasets for our evaluation. the first dataset is the cif ar-10 dataset 1 . this dataset contains 60,000 32x32 pixel images, resulting in 60,000 vectors of 3,072 integers. the distance between images is defined by the cosine distance between their vectors. the second dataset is a collection of newspaper articles called trc2. this set consists of 1.8 million articles which we preprocessed by splitting on whitespace, removing punctuation, converting to lowercase, and applying porter 2 stemming. then, stop words, single characters, and numbers were removed. this procedure resulted in 1.68 million sets of words. the distance between two articles is defined as the jaccard distance between these sets."
"in our evaluation we compare the performance of the above approximations in practice, thus obtaining a fitness modified objective function which we indicate withf . in the next section we discuss a metaheurisic which uses the approximations developed in this subsection."
of this algorithm rises to o(n 2 log 2 n). in our experimental evaluation below we used the f astcluster implementation by müllner [cit] .
"in computer vision, place classification is also known as scene classification and image categorization. two broad methods can be discerned -those that model local features [cit] and distinctive parts of images [cit], and those that extract global representations of images and learn from them [cit] . the latter is closer to this work and includes the centristbased vpc system [cit], and spatial pyramid matching [cit] . these methods also use classifiers to learn place categories and cannot detect new categories online. further, they do not deal with image streams and videos except when filtering change-point detection: (a) univariate gaussian input data (blue/dashed) with 4 segments and inferred segment means (in black) (b) groundtruth for change-points shown as length of segments, which is the variable ct used in our inference. (c) output of change-point detection which is probability distribution on ct."
"traditional ways of assessing a hierarchical clustering algorithm work by comparing its outcome to a gold standard or to the dendrogram produced by a known (exact) algorithm. comparing to a gold standard (i.e., a natural clustering of the data made by a specialist) is not appropriate in what we try to achieve. doing this type of analysis would show that the clustering approach is suitable to express the semantic meaning of specific datasets. however, it would not show that the approach which we propose is able to closely resemble what ahc with average linkage offers. in practice, this would mean that if it is known that ahc with average linkage works for a given problem, there would be no guarantee whatsoever that our method can be used. for the comparison with the outcome produced by a known algorithm there are several options. a thorough approach is the measure developed by fowlkes and mallows [cit] . however, the interpretation of produced results is hard to be performed automatically. furthermore, the use of this measure for any larger evaluation is infeasible due to its computational complexity. the high computational cost is due to the calculation of an exact dendrogram, as well as due to the computation of the metric itself (whose complexity is o(n 3 )). the issue of scaling metrics for dendrogram comparison is discussed in depth in the original twister tries paper [cit] ."
"note that since the likelihood parameters ξ c in (5) are integrated out, this particle filter is rao-blackwellized [cit] and has lower variance than a standard particle filter. this also makes the convergence of the algorithm more efficient."
"chipkill is a popular ecc mechanism widely adopted in high reliability systems such as commercial server markets. a typical symbol-based chipkill protection scheme provides sscdsd (single symbol correction, double symbol detection) protection. this means that a chipkill -enabled system is able to correct all errors resulting from a fault to a single chip and detect all errors resulting from faults to two chips. in this way, severe faults to a single chip do not impact reliability and even faults to multiple chips do not result in silent data corruption."
"in this paper we explore the idea of solving this issue after the fact. we first perform the hashing step of the twister tries algorithm and then attempt to find out which hash outcomes to ignore in order to improve the quality of the dendrogram. note that it is important that this a posteriori correction is performed fast and with low space requirements. if the correction would be slow or consume much memory, it would be better not to use an approximation in the first place and tackle the whole clustering problem exactly."
"the agglomerative algorithm is know by many names such as globally closest pair (gcp) clustering [cit], sequential agglomerative hierarchical non-overlapping (sahn) clustering [cit], or the term which we will adopt agglomerative hierarchical clustering (ahc) [cit] . as mentioned, the algorithm works by repeatedly merging two clusters together into a larger one. the algorithm always merges the closest clusters in terms of a predefined linkage. this linkage defines the distance between clusters of items. examples include single linkage (the shortest distance between any of the items), complete linkage (the longest distance between any of the items), average linkage (the average distance between the items), etc. (see also [cit] ). in this paper average linkage is used as the measure for cluster similarity."
"sdc-minimization approach requires only a single very narrow saturating error counter per rank to identify the threshold of forcing all-cg access. if desired, the counter can be reset on every scrub interval to avoid transient faults forcing cg for long periods of time."
"we also compute a spatial pyramid using texture. texture is extracted using the 17-dimensional leung-malik filter bank [cit] . similar to the case of sift features, an image set is used on which texture is extracted and clustered using k-means to obtain a set of textons. histograms containing the proportions of each of the textons are computed at different resolutions as before to obtain the texture-based spatial pyramid."
"we use dcm distributions as place models in pliss. the α parameter for each place is learned from labeled images in an offline training phase, if training data is provided. during runtime, the distribution is used to compute the likelihood (5), and the α parameter is also updated after each measurement using the iterative rule (13) or the slightly faster gauss-newton updates. this facilitates online learning but if online learning is not required, the updated parameter can be discarded at the end of the segment."
"place labels for the sequences were also learnt online, and it was expected that the system would give the same place label when returning to a specific place but not when encountering another place with the same category label. the labeling was modified manually to reflect this for the six sequences and 39 distinct places were labeled. results for place labeling are given in figure 5 (b). considering that no training data has been used, the results are very promising when compared with the vpc system (figure 8) ."
procedures are added on top to get temporal consistency [cit] . other approaches use keypoint matching for location recognition based on image retrieval [cit] but cannot generalize.
"in terms of implementation, we augment the change-point algorithm of section iv so that the discrete distribution on places is stored with each segment c t . similarly, in the particle filter, each particle maintains a place distribution. the place distribution becomes increasingly confident as the length of the segment c t increases and is robust to noisy measurements and outliers. however, since it is also recomputed with each measurement, the algorithm does not make any irrevocable decision with regard to the place label and can \"change its mind\" given enough evidence. the cost of updating the place distribution is linear in the number of labels and hence, does not affect the runtime of the change-point algorithm."
"the process is to attempt multiple corrections (possibly in parallel), each time under the assumption that a different data chip in the accessed subrank is corrupted ( figure 6 ). after a correction attempt, the check symbol is regenerated and compared to the stored check symbol of the inner code. if the assumption that only a single data chip is corrupted is true, then only one of the correction attempts will succeed with high probability. if, on the other hand, more than one of the corrections appears to succeed or if no correction attempt succeeds, then more than one chip is corrupted concurrently, one of the correction attempts resulted in an undetected error when rechecked, or the check symbol has an error. in the first two cases a detected uncorrectable error should be reported. the third case, where the i chip is corrupted, can return corrected data and notify of the detected error in the i chip for diagnostics. to differentiate between the first two cases from the third, we again utilize the outer code for detection. specifically, if all correction attempts fail but the outer code detects no error in the original data, we assume the error is in the i chip only because the outer code does not include the i chip in its parity calculation. otherwise, a detected uncorrectable error is reported. similarly, we conclude that only o chip is faulty if only the outer code detects errors but none of the inner codes do and correction attempts using the outer code lead to inner-code detected errors (verification failure). with high-probability, we are also able to differentiate the case where only o is faulty from cases in which multiple chips, including the o chip, return erroneous data."
"the data likelihood from (2) can be calculated only if we know the distribution parameter to use. hence, we integrate over the parameter value using the parameter prior"
"we model images using a \"bag of words\" model wherein a histogram is used to represent the image. these histograms are also used as input measurements to the pliss algorithm. first, in an offline phase, sift features are computed on a dense grid on each of a set of images. the features are vector quantized using k-means to create a codebook of pre-specified size. note that it is not necessary in this step for the image set used to be similar to the test images, though better results are obtained if this is true."
"finally, an inner/outer coding scheme similar to clean has been patented [cit], but the patent is vague and we are not aware of any analysis and evaluation. to the best of our knowledge, no prior work has addressed the issue of guaranteeing strong reliability while simultaneously providing fine-grained memory accesses at reasonable overhead. our two-tier, inner/outer ecc coding mechanism effectively achieves the conflicting goals of chipkill-level reliability and the ability to provide finegrained accesses, while simultaneously incurring low ecc storage overheads (12.5%). we detail our clean ecc in the following section."
"clustering is used in a wide variety of domains and is often one of the first unsupervised learning tasks introduced in many data mining courses. the overall task is to place items into clusters such that items which are similar are placed in the same cluster, while items which have a low similarity are placed in different clusters. one way to perform this task is so-called hierarchical clustering. as opposed to others, the outcome will not just be a partition of the items, but a dendrogram (see fig. 1 ), showing a nested clustering."
"we evaluated the overall algorithm with several surrogates selected based on the experiments above. the selection consists of fix 1, fix 3, fix 50 and w 10. the reason to select these is that they span a broad range of quality and timing values. now, using these surrogates we run the metaheuristic optimization using the same portions of data as used in the previous experiment."
"we evaluate clean in terms of system performance and efficiency, reliability, and hardware overhead. to estimate the impact on system performance, we use gem5 [cit] integrated with drsim [cit], a detailed cycleaccurate dram simulator that supports subranked memory systems and fine-grained accesses (section 3). to quantify reliability, we run monte carlo simulations (of 1 billion iterations) using a dram fault/error evaluation framework to analyze both error coverage of different fault scenarios and the overall expected rates of faults that can lead to uncorrectable errors and faults that can results in sdcs. to gauge the impact on hardware, we implement clean and chipkill in verilog and synthesize the designs to estimate their area and delay."
"twister tries usually produce reasonable approximate dendrograms for average linkage. however, since it is a probabilistic algorithm, there is a possibility that the randomly chosen hash functions result into tries with unwanted properties. in effect, these tries will cause the overall algorithm to produce a dendrogram with low quality. we proposed a metaheuristic which is capable of identifying bad tries. in order to make this metaheuristic useful, we first proposed and evaluated several surrogates which give a fast, but rough estimation of the dendrogram quality. then we evaluated the metaheuristic and noticed (in a controlled experiment) that it is able to disable the trie and produce a better result than normal twister tries would. moreover, not only the metaheuristic could disable the artificially inserted trie, but also was able to further improve the performance of twister tries by disabling specific other (random) tries which had a slightly negative effect on the final dendrogram. in conclusion, we recommend to always use some form of a posteriori trie removal in order to improve the quality of the dendrogram. we noticed in our experiments that the use of a simple surrogate (which produces reasonable outcomes) is more beneficial than a slower one (which delivers even better accuracy). future work could attempt to combine multiple surrogates in the process by somehow determining that running a more expensive surrogate is worthwhile to make an important decision. another path for further exploration is that the current approach is in some sense an all or nothing approach. either a trie is enabled or disabled, even if this is only due to a couple of hash functions which cause the trie to be bad. it might be possible to save the good hash values if it would be possible to somehow find out which hash functions have a bad influence and then ignore these outcomes."
"while these adaptive-granularity architectures offer significant improvement for some applications (e.g., [cit] showed up to 31% speedup on average for a range of memory-intensive workloads), prior work did not sufficiently address the crucial aspect of memory reliability. recent studies have shown two important trends about memory errors [cit] . the first is that memory errors that affect a large number of bits in a single memory transfer are common, necessitating memory protection schemes that are stronger than those proposed previously for adaptive granularity. the second is that memory errors can be quite frequent. error rates are significant because the rate at which permanent faults occur is on par with transient fault rates, but once a permanent fault occurs many accesses to the faulty device result in memory errors. thus, there is a practical need for strong memory protection that can tolerate multi-bit errors as well as permanent faults."
"we propose a memory protection scheme specifically for adaptive-granularity systems. our chip-level reliable and access-granularity negotiable (clean) ecc uses a concatenated code with an inner code used only for error detection and an outer code for correction. this code design enables us to balance the contradicting demands of fine-grained access and strong ecc. we use the inner code for strong detection when performing fine-grained access on subranks and use the outer code for rare correction events, to ensure a low rate of undetected errors (that may lead to silent data corruption), and opportunistically when performing coarse-grained accesses. while this design is reminiscent of raid [cit], we make several new contributions to meet the constraints of memory system design and to adapt granularity and protection, briefly:"
"this section describes the mechanisms for implementing clean (chipkill-level reliable, access-granularity negotiable) ecc design using the baseline dynamicgranularity memory system described in section 3.1 as a concrete example. our goal is to provide reliability that is on par with that of chipkill . to achieve this goal of strong protection in concert with fine-grained memory access capability, we utilize a two-tier coding mechanism that relies on a concatenated error codean inner code is used for error detection and an outer code is used mostly for correction."
"our baseline system is configured as an out-of-order 8-core processor with a three-level cache hierarchy. table 4 summarizes our baseline system configuration, which is simulated using gem5 [cit] and drsim [cit] . 2"
"in this paper, we presented, for the first time, a highlyreliable memory protection scheme that is able to perform fine-grained subrank accesses in a dynamic fashion that is completely transparent to software. we conclude that prior work on adaptive granularity does not provide sufficiently strong memory protection because the data layout and codes it uses [cit] cannot account for chip faults or even single-pin faults. in contrast, clean can approach the reliability of chipkill when performing coarse-grained accesses, provides similar correction capability even with fine-grained accesses, and manages the rate of possible silent data corruption events. this is all achieved while maintaining the customary 12.5% storageoverhead, using standard ddr3 dram chips, and with no additional on-chip components compared to prior adaptive-granularity architectures."
"most existing place recognition systems assume a finite set of place labels, which are learned offline from supervised training data. this is done using classifiers, which then categorize places based on input measurements during runtime. classifier-based systems have the advantage of simplicity but also have many corresponding disadvantages 1) if a label category has large variation in measurements (for e.g., offices are vastly dissimilar in all aspects), a huge training set is necessary for adequate performance 2) the system is constrained to classify the input into the specified category set and cannot recognize the existence of a new label 3) temporal consistency in the output labeling has to be externally enforced since the classifier simply labels each measurement individually figure 1 . place labeling using pliss: output is for an image sequence of 1043 frames containing 5 labels. thumbnails of the images are shown on top, followed by groundtruth and maximum-likelihood output labeling. many, but not all, changes in labels correspond to visible changes in appearance of the image thumbnails. the change-point posterior on segment lengths is shown at the bottom with darker regions denoting higher probability."
"we use the visual place categorization (vpc) dataset [cit] for our experiments. the dataset consists of image sequences from six different homes, each containing multiple floors. the image set from each home consists of between 6000 and 10000 frames. in our experiments, we consider sequences from each floor to be a different image sequence. the dataset has been manually labeled into 5 categories to provide ground truth for the place categorization problem. in addition, a \"transition\" category is used to mark segments that do not correspond clearly to any class. the vpc dataset is significantly difficult since no effort has been made to keep all the images in the sequence informative. thus, a number of images contain only a wall, which is something that could also be expected when a robot is moving around."
"during the experiment, we use 20 tries of height 20 and 30 for the trc2 and cif ar-10 dataset, respectively. in addition to these (to observe the effect of the trie removal) we introduce a possible, but unlikely trie into the forest about which we know that it has unfavorable properties. the trie we introduced has the same hash function at each level and, as a result, many splitpoints will be lower than desired. we measure how the (real) quality of the best individual, as reported by the surrogate, evolves over time. further, we contrast this to the joining distance of the dendrogram produced by the standard twister tries and exact algorithm and the time needed for the exact algorithm. the result of these experiments can be found from figs. 3 and 4. note that the budget for the metaheuristic was a runtime of 15 minutes."
"we compute a spatial pyramid [cit] from the quantized sift features which is used as input to the pliss system. the spatial pyramid is obtained by computing histograms, at various spatial resolutions, of feature frequencies in each of the codebook clusters. following [cit], we obtain successive spatial resolutions by dividing the image into a grid as shown in figure 3 . note that only the histograms at the finest resolution need to be computed since the coarser resolution histograms can be obtained by simply adding the appropriate histograms at an immediately finer level. all the histograms from the different grids are then concatenated to yield the spatial pyramid representation. the two parameters for computing the spatial pyramid are thus, the number of levels in the pyramid v and the number of clusters computed in sift space (size of the dictionary) k. sift features only have local information about an image patch while an image histogram only has global information. by combining both of these at different scales, the spatial pyramid obtains more fine-grained discriminative power."
"of the dendrogram produced by twister tries twister tries were proposed as an algorithm which can overcome the scalability bottleneck of ahc with as a tradeoff the production of an approximate dendrogram. [cit] the algorithm works for average linkage and at least cosine, jaccard and hamming distance can be used. further, linear time and space guarantees are provided. in this section, we will not present every detail of the algorithm, instead we will focus on the parts related to this paper. readers interested in the details of the algorithm are referred to the original paper."
"once this structure is in place, the twister trie algorithm works somewhat similar to the naive ahc algorithm. the leafs of the tries represent the items inserted in the forest, these are now interpreted as one item clusters. then the following steps are repeated: a) the lowest point where any of the tries split is located, b) two clusters connected to this node are removed from the tries, c) these clusters are merged by taking their union (recorded in the dendrogram), and d) the merged cluster is re-inserted into the forest using the same hash functions, but applied on items randomly selected from the merged cluster."
"we envision significant performance improvements with more sophisticated place models. such models may even incorporate object and context information from places, which cannot be done easily with svm classifiers. currently, pliss does not distinguish between place labels and category labels and relies on the place models for making this distinction. it is future work to overcome this deficiency."
"the chosen algorithmic structure is that of an evolutionary algorithm (ea), see [cit] because the long and discontinuous binary structure requires a high diversity which can be achieved by using a population based system, see [cit] . the algorithmic features are listed in the following. at the beginning of the optimization process n binary strings are sampled at random. the length is the same as the number of tries. twenty tries are used on the basis of the experiments shown in the original twister trie article [cit] . the population size n has been set equal to 25. we have chosen this setting in order to have a fairly exploitative behavior without having an excessively high risk to converge prematurely (as it would be for a population size of very few individuals)."
"note that the closer the jd of the dendrograms in d to each other, the more difficult it will be to obtain a good quality. the reason is that that when the dendrograms are close, the surrogate becomes more likely to make mistakes in the ordering."
"the full detection and correction flow of clean is illustrated in figure 7 . clean first checks for errors in each accessed subrank with its corresponding inner codes. if no errors are detected and if the access was cg, clean double-checks for errors with the outer code. if an error is detected, clean attempts to correct all possibly erroneous chips (chips in subranks indicated by the inner code) using the outer code; each correction attempt is checked with the corresponding inner code and a successful correction increments a counter. after all chips are attempted, the outer code is used to once again verify the result. if the outer code confirms a correct value and only a single correction attempt succeeded, clean concludes that an error was detected and corrected. if no correction attempt succeeded, but the outer code did not detect an error, the most likely conclusion is that the i chip had an error, so no data error occurred. if more than one correction attempt succeeded or if the outer code could not verify the correction, an uncorrectable error is reported."
"as discussed in the previous section, we need a way to determine the quality of a dendrogram. to be more precise, for the execution of the proposed algorithm we need to be able to assess the quality of dendrograms without knowing a dendrogram produced by an exact algorithm, whereas to evaluate the performance, we will also need to compare the quality of the approximate dendrograms with an exact one. furthermore, since we are working with fairly large dendrograms, the methods used have to be scalable."
"conclusion: by the principle of strong induction, it follows that the hypothesis is true for all n. now, to compute the jd we need these distance computations and the addition of n − 1 terms. hence, altogether (n+2)(n−1) 2 constant time operations are needed, which implies that the joining distance is computed in o(n 2 )."
"system throughput. figure 9 shows that clean generally outperforms the baseline cg-only system, but that the gains in efficiency are more prominent. four of the seven benchmark mixes we evaluated showed noticeable gains in weighted speedup (up to 11.8% improvement), and three showed little gain or a minor performance loss (−1.5% at worse). on average clean provides a 3.3% improvement in system throughput across the seven benchmark mixes we evaluate. we attribute this minor performance degradation to the different memory scheduling policy required for supporting a mix of fg and cg accesses and again want to emphasize the significant energy efficiency benefits. clean does not support the critical-word first optimization technique, but finegrained access already provides some of those benefits; while our baseline ecc schemes utilize critical-word first in performance evaluation. clean is not as effective as dgms and only achieves 39 − 88% of the performance improvement of dgms. this is because of two reasons: the larger minimum access granularity and the data layout used for clean compared to dgms. because the finest access granularity of clean is 16b compared to 8b with dgms, the throughput improvement due to the decreased dram off-chip traffic is smaller than with dgms. moreover, the rotating position of the ecc chips is done with fewer effective \"banks\", which leads to more subrank conflicts and further diminishes performance improvements over cg-only systems."
"in order to mix access granularities it is necessary to first decide which accesses are coarse and which are fine. this can be done either statically per address or memory instruction [cit] or with a dynamic granularity predictor [cit] . in addition, the interaction of different granularity accesses may degrade memory scheduling effectiveness and granularity decision may also be based on such factors [cit] ."
"we tested the pliss system extensively on actual image data and describe the methodology and results here. the pliss system was implemented in matlab with no attempt made at any optimization. all parts of the algorithms, including feature detection, were performed in matlab."
while budget condition do 8: matingp ool ← select n individuals 9: newp op ← 3-parent cross over (matingpool) 10: biased mutate(newp op) 11: for i in newpop do 12:
"importantly, the sdc-minimization scheme that forces one out of every 10−100 fg accesses to be cg did not have a significant impact on performance or power; the impact is (less than 1% degradation the numbers reported above include this overhead."
"there are two major approaches to hierarchical clustering. first there are the divisive ones, where the items to be clustered are observed as a whole and split in two parts or sub-clusters. each of these parts is then recursively split in ever smaller sub-clusters until each item is inside its own cluster. the other group of approaches are called agglomerative. the reason is that these algorithms perform the clustering by first placing each item in its own cluster and then repeatedly merging (or agglomerating) two clusters together to form a new one. this process stops when only one cluster is left. in this paper we will be dealing with the later group of approaches (i.e., the agglomerative ones)."
"it is clear that the computation of the jd does only require a constant amount of memory. however, it is not immediately obvious what the computational complexity is. the jd is the sum of n − 1 terms, since the hierarchical clustering of n items will always have n − 1 clustering steps. however, the analysis gets complicated because the computation of each of these terms involves the items which are in the left and right subcluster of the mergers. in the next theorem, and accompanying proof, we show that the total number of distance calculations needed will always be n(n−1) 2 and how this leads to a time complexity of o(n 2 ) for both jd and jdr."
"clustering is a well-known, unsupervised learning task which has the objective of grouping similar items together. these groups are called clusters and they are to be discovered in the dataset. this is as opposed to classification where the classes into which the items have to be placed are predetermined. the clusters formed should be such that the similarities between the items in the same cluster are high, while the similarities between items in different clusters are relatively low. a specific type of clustering algorithms are the hierarchical ones. these do not just produce a partition of the items, but a dendrogram (see fig. 1 ). one interpretation of such dendrogram is that it shows partitions of sizes ranging from one until the number of items to be clustered. these partitions are obtained by slicing the dendrogram at a given height. this is useful in application were the number of clusters to be obtained is not known beforehand. some clustering approaches will determine the number of clusters automatically using a heuristic, but users might not trust these or it might be difficult to find a suitable one. hierarchical approaches further allow the user to browse trough the produced hierarchy, which might give them a deeper insight in the structure of the data."
"return best 18: end procedure the procedure has four parameters: forest which is the collection of tries containing the hash outcomes, b is the number of tries in the forest, f which is the function to be optimized (i.e., the approximation of the quality), and n the population size. series of experiments. in the second series of experiments we look at the overall performance of the proposed algorithm. we cluster various portions of data selected form two different datasets. then, we observe how the real joining distance of the best individual (according to the used surrogate) evolves."
"when looking at the performance of the different surrogates, we notice that fix 50 is slow, but always results in a monotonically decreasing joining distance. fix 1 on the other hand gives fast results, but seems slightly less reliable. this can be seen from the cif ar-10 figure around 300 seconds: the surrogate fig. 3 . evolution of the real quality of the best individual as reported by metaheuristic using the surrogate. results are for 10,000 images of the cif ar-10 dataset. for comparison: the computation of the distance matrix took about 14.3 hours (using python/numpy). from this matrix the exact dendrogram is computed in a couple of seconds, resulting in a joining distance of 4130.61. this exact computation only used one core. normal twister tries would produce a jd of 5991.87. when the bad trie is taken away manually the result becomes 5217.85. orders the individuals wrongly and the (real) quality of the best individual reported lowers. however, fix 1 is able to perform many more evaluations than fix 50 and overcomes the slightly lower performance in ordering results. broader experiments would be needed to give support for a general recommendation, but our experiments suggest that using a cheap, but reasonably good surrogate (e.g., fix 1) is more beneficial than using a slow but more accurate one (e.g., fix 50)."
"note that despite agms's superior memory bandwidth utilization, the use of fine-grained accesses significantly compromises ecc coverage. prior work on agms [cit] only provided secded (single error correction, double error detection) protection with very high storage overhead or a weak form of secded at conventional 12.5% storage overhead. unlike current secded ecc on full ranks, which can tolerate any single-bit or single-pin fault, the secded configuration of dgms [cit] can only tolerate single-bit faults [cit] . this difference results in markedly compromised reliability as shown in table 1 . in particular, dgms can detect only 74.6% of single chip errors while secded can detect almost 99% of single chip errors -this implies that dgms can result in many more silent data corruption events (sdcs) even with single faults (see also figure 11b) . the difference is even greater when compared to currently-standard chipkill-level techniques that can detect and correct all faults confined to a single dram."
"recent studies [cit] indicate that memory system error rates are rising and that the rate of permanent faults is on par with that of transient ones. with a high permanent-fault rate, the likelihood of multi-bit faults, overall error rate, and possibly even accumulated faults increases. as a result, the memory protection level provided with secded is no longer sufficient to guarantee reliable operation of the main memory system and stronger chipkill -level ecc is required [cit] ."
"the outer code requires a single coarse-grained read regardless of whether the error was detected on a finegrained access or not. this access is needed both for correction events, which are rare, and also for maintaining the correction information because it is encoded based on the data from the entire rank. updating the outer code when writing back only a subset of sectors from the llc is challenging."
"we need the locations of change-points and the exact algorithm above accomplishes this by maintaining the posterior over segment lengths c t for all t. we now approximate the posterior using n weighted particles, thereby obtaining a constant runtime algorithm."
"the main data structure used in twister tries is a forest consisting of tries (also called prefix trees). to create one such trie of height k max, one needs to select k max proportionally sensitive hash functions, at random. then, the data items are inserted into the trie, such that the labels encountered on the arcs when following the path from the root node to the corresponding leafs are the outcomes of the hash function evaluations."
"to minimize the likelihood that a fault affects multiple symbols, inner-code symbols are aligned with data pins (dqs); any single-bit or single-pin error is guaranteed to be detected, while other multi-bit errors are detected with high probability, which in practice is much better than the pessimistic bound of 0.39%. we evaluate the practical protection in section 5.2."
"the algorithm described so far is exact. after n measurements, the possible segment lengths range from 0 to n, and the posterior contains the probability of all these cases. further, if the optimal change-point locations are also needed, the posteriors from all timesteps have to be kept. hence, the runtime and memory costs per timestep are o(n) while the total memory cost is o(n 2 ). these requirements are incompatible with long term online operation. hence, we next provide a simple and intuitive particle filtering approximation that exhibits constant runtime and o(n) total memory cost."
"in order to understand the inner workings of the twister tries algorithm, we first need to introduce the notion of a proportionally sensitive hash function."
1.e-01 figure 12 : system level reliability over 5 years and varying memory capacity (using 16gib dimms) [cit] applications (profiled with a pintool [cit] ): the risk that the system has at least one sdc.
"fg clean suffers from a higher sdc risk than chipkill as discussed in section 3.5. this higher sdc rate is almost entirely the result of single-chip faults because these suffer from both having the highest probability going undetected by the inner code (0.26%) and are more frequent than double-chip faults. this manifests in a significant degradation of reliability with respect to sdcs compared to cg clean and chipkill . however, fg clean is still sufficiently strong for large systems with memory capacities of many tib (figure 11b ) and is orders of magnitude better than secded and dgms. in addition, some smaller systems that utilize ecc today can benefit from the performance and efficiency improvements of clean-ecc without effective impact on reliability."
"performing a coarse-grained read before every partial writeback is likely to have a very high memory traffic overhead. instead, we rely on two important characteristics of the clean architecture. first, the codes used in clean are linear and systematic. with such codes, the updated ecc information (o ) can be computed from only the modified data (d * i ) and the original ecc information (o) by subtracting the original fg data read (d * i ) from o. this procedure is defined in equation 2 and equation 3."
"we denote the place label at time t as x c t . the place label is indexed by the current segment since the whole segment has a single label. however, this label is updated with each measurement, and hence the time index t is also used. the probability distribution over x c t is taken to be a discrete distribution of size l, one for each of the place models. the case where the place label takes none of these values is detected using statistical hypothesis testing."
"negotiable strong protection. we first briefly review the baseline adaptive granularity memory system for which clean is designed (section 3.1). we then introduce the basic outer code and correction procedure including a description of how the ecc information needed for clean is accessed and updated with minimized memory traffic overhead (section 3.2). we then describe the inner code and detection mechanisms (section 3.3), followed by a discussion of how the two codes are combined to localize errors (section 3.4) and minimize the rate of undetected errors that can lead to silent data corruption (sdc) events (section 3.5)."
"the drawback of hierarchical clustering is that most algorithm scale poorly in the number of items clustered. hence, for the increasing data sizes of today's application domains, more scalable algorithms are needed. the scalable approach which we will enhance further in this paper is called twister tries, which was proposed by cochez and mou [cit] . this algorithm produces an approximate dendrogram and poses linear time and space requirements. in the core of twister tries there is a collection of tries. encoded in these tries are the outcomes of specific localitysensitive [cit] hash functions. after this hashing stage, there is the twisting stage in which the information in the tries is converted into a dendrogram, which is the final outcome of the algorithm. the locality-sensitive hash functions are chosen such that they will usually improve the quality of the final dendrogram. however, in a rare occasion it can happen that the outcomes of some of these functions has a devastating effect on the clustering."
"the main contributions of this paper are: 1) introduction and comparison of approximate ways to evaluate a dendrogram (see section iv-c and tables i and ii); 2) a better clustering of two datasets, demonstrating an improvement over the existing twister tries algorithm with reasonable time and space overhead (see section vi and in particular figs. 3 and 4) . a further minor point of interest is theorem 1 about the computational complexity of the joining distance ratio which was introduced by kull and vilo [cit] ."
"posed, for the most part, to reduce the memory access power of many-threaded cpus by improving row-buffer locality. such designs include hp's mc-dimm (multicore dual in-line memory module) [cit], rambus's threaded memory module [cit], the mini-rank memory system [cit], and convey's s/g dimm (scatter/gather dimm) [cit] .in a sub-ranked dimm, a register chip is used to direct memory command signals to a subset of the dram chips in a rank without changing the dram device structure itself. figure 1 shows the subranked memory configuration we assume in our work. the baseline a/dgms can perform 8b accesses, but to support strong ecc clean requires a minimum access granularity of 16b."
"we implemented an aggressively parallelized chipkill encoder that is hard-coded for a specific polynomial to reduce latency. this leads to a larger area than required for the well-known shift-register based encoder in which input symbols are sequentially computed, but results in far better latency which we believe is a better design point for the main memory system. the design of both the inner and outer encoders and decoders of clean is simple because they inherently rely on straightforward parity operations. we also note that a single layer of muxes and wires is needed for aligning the 32 inner and 32 outer-code parity bits with the encoders and decoders because of the modulo-9 raid-like data layout described in section 3.1; the data symbols are all symmetric and don't require long shifts. we implemented two versions of clean encoder: one that operates on all four subranks in parallel (clean par ) and a lower-area version that operates serially (clean ). table 6 summarizes our synthesis results and shows that clean provides lower latency decoding at a comparable area to chipkill 's encoder, which is the simplest part in chipkill . if correction is required, the clean localization is equivalent to four encoding operations."
"while change-point detection provides boundaries, the place label is assigned probabilistically based on the measurement, the most recent label, and the change-point distribution. statistical hypothesis testing is used to determine if the current measurement could have been generated by any of the prelearned place models. if this is not the case, the measurement is declared to have come from a previously unknown place. in this manner, pliss can systematically recognize a previously unknown place type and assign it a new label if required. further, pliss can learn and update place models online, which is helpful in cases where measurements from the same category show variation (albeit only if the variation is gradual). hence, pliss can operate with labeled data, which is used to learn place models at training time, or without it, when it recognizes new place categories as they arise and learns place models for them online. we model places using the multivariate polya distribution, also known as the dirichlet compound multinomial (dcm) model [cit] . the dcm is a bag-of-words model and captures burstiness of the data, i.e. it models the observation that if a word occurs once in a document, it usually occurs repeatedly. we use the dcm model to model histogram measurements obtained by quantizing dense features computed on the images."
definition 2 (joining distance ratio (jdr)). the joining distance ratio (jdr) is the proportion of the sum of the distance at each step of the standard ahc dendrogram and the sum of the distances of the approximate algorithm.
"a conventional cpu uses multiple dram chips organized into ranks to provide large coarse-grained accesses to memory (figure 1 (a) ). to provide finergrained access capability with low overhead, an agms leverages a sub-ranked memory module design (figure 1 (b) ). sub-ranked memory systems were originally pro-abus dbus (64bits data + 8bits redundancy) x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 (a) conventional rank module reg/demux abus x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 x4 i o"
"our evaluation (section 5.2) shows that the rate of undetected errors when using clean is dominated by errors experienced during fg reads from subranks with single-chip faults. in fact, because of the layout of the inner code, only faults that impact multiple dq pins across multiple bus beats in a single chip during a finegrained transfer or faults that affect multiple chips in the same transfer can lead to possible sdcs. furthermore, those faults that affect multiple chips are detected with higher probability with clean than when using the commercial chipkill code of recent amd processors [cit] . thus, to approach the sdc rate of chipkill, we focus on minimizing the number of potential sdcs caused by single-chip multi-dq faults."
"even though we did not observe problematic cases in the applications we evaluated, it is possible that repeated sdcs can occur in some software. clean avoids even this unlikely scenario of a possibly unbounded number of errors from a single fault by relying on the highlyeffective cg detection, which utilizes the outer code. if multiple errors are detected cg from a rank, clean forces all further accesses to the rank to be cg, avoiding the problematic case of insufficient coverage of the inner code. to ensure multiple errors are detected, even when in the problematic scenario, clean randomly overrides one out of every n fg accesses to be cg. thus, at most n sdcs are expected from even the most insidious faults and access patterns. in section 5.1 we show an occasional cg override has negligible overhead."
"we have presented pliss, a system for place recognition and categorization based on change-point detection. pliss has significant advantages over existing methods of being able to detect and learn previously unknown places and place categories, of being able to learn online, and of being able to operate without any training data. experiments on a difficult dataset show that, along with these advantages, pliss also matches the state of the art in performance."
"in addition to sift features, we also compute spatial pyramids using two other features, centrist [cit] and texture, which we now describe. centrist is based on the census transform [cit], which is a local feature computed densely for every pixel of an image, and encodes the value of a pixel's intensity relative to that of its neighbors. it was originally introduced for identifying correspondence between local patches. the census transform is computed by considering a patch centered at every pixel. the transform value is a positive integer that takes a range of values depending on the size of the patch. for instance, a patch size of 3, where there are 8 pixels in the patch apart from the central pixel, yields transform values between 0 and 255 (2^8 values). this is equivalent to having a dictionary size of 256 without the need for the clustering step. computing a histogram of these 256 values yields the centrist (census transform histogram) descriptor. additionally, the census transform itself is extremely efficient to compute. hence, a spatial pyramid can be calculated much faster using centrist than using sift features."
the correction capability of clean is very close to that of chipkill and is orders of magnitude better table 6 : synthesis results for overhead estimation.
the article is structured as follows: in the next sections we introduce agglomerative hierarchical clustering and the twister tries algorithm. then we propose approximate methods for measuring the quality of dendrograms. these will then be used in the metaheuristic proposed in section v to result in the main algorithm developed in this paper. in section vi the results of our experimentation are shown. we end the paper with a short conclusion and an outlook on future research.
"mhe adjusts model parameters to minimize the difference between measured variable values (x meas ) and model predictions (x), as shown in equation (5) . min u,x"
"gekko is presented as a fully-featured aml in python for lp, qp, nlp, milp, and minlp applications. features such as ad and automatic ode discretization using orthogonal collocation on finite elements and bundled large-scale solvers make gekko efficient for large problems. further, gekko's specialization in dynamic optimization problems is explored. special variable types, built-in tuning, pre-built objects, result visualization, and model-reduction techniques are addressed to highlight the unique strengths of gekko. a few examples are presented in python gekko syntax for comparison to other packages and to demonstrate the simplicity of gekko, the flexibility of gekko-created models, and the ease of accessing the built-in special variables types and tuning options."
"manipulated variables (mv) inherit fvs but are discretized throughout the horizon and have time-dependent attributes. in addition to absolute bounds, relative bounds such as movement (dmax), upper movement (dmax hi ), and lower movement (dmax lo ) guide the selection by the optimizer. hard constraints on movement of the value are sometimes replaced with a move suppression factor (dcost) to penalize movement of the mv. the move suppression factor is a soft constraint because it discourages movement with use of an objective function factor. cost is a penalty to minimize u (or maximize u with a negative sign). the mv object is given in equation (8) for an 1 -norm objective. the mv internal nodes for each horizon step are also calculated with supplementary equations based on whether it is a first-order or zero-order hold."
"the gekko gui uses vue.js and plotly to display optimization results quickly and easily. it also tracks past results for mhe and mpc problems, allowing time-dependent solutions to be displayed locally and in real-time as the iterative solution progresses. the gui itself is implemented through a python webserver that retrieves and stores optimization results and a vue.js client that queries the python webserver over http. polling between the client and webserver allows for live updating and seamless communication between the client and the webserver. the gui allows plots to be created and deleted on demand, supporting individual visualization for variables on different scales. model-and variable-specific details are displayed in tables to the left of the plots (see figure 2 )."
"our study is not necessarily the best or only way to integrate chipx and ped; however, to the best of our knowledge, this is the first systematic study of using ped to enhance chipx analyses in human and mouse. we hope that chip-ped will inspire new computational approaches that continue to maximize the value of chip-seq and chip-chip experiments."
"chip-ped relies on two large compendiums of gene expression profiles, consisting of 13 182 human gene expression samples generated from affymetrix human u133a (gpl96) and 9643 mouse samples generated from affymetrix mouse 430 2.0 (gpl1261) arrays [cit] . the gene expression profiles were downloaded from geo [cit], pre-processed and normalized consistently using frma [cit] . frma is designed to normalize large amount of heterogeneous microarray samples to reduce the effect of batch on gene expression estimates. for each probeset, we standardized the frma values across all microarray samples from the same array platform to have zero mean and unit standard deviation. the biological context of each sample was recorded and manually verified based on the sample descriptions in geo (see supplementary method 1.1 and supplementary fig. s1 )."
"for the six tfs analyzed, chip-ped made 178 tfþtgþ predictions listed in supplementary tables s2-s7 (oct4: 28, myc: 33, gata1: 37, stat1: 12, jarid2: 41 and esr1: 27). to systematically evaluate chip-ped prediction accuracy, we examined all predictions through a survey of existing literature. we found that 90 of 178 (50.6%) biological contexts predicted to be enriched with tfþtgþ activity were functionally validated in previous experiments (see section 2). for example in the oct4 analysis, 20 of the 28 predictions were functionally validated, even though 25 of the 28 predictions were known to express oct4 rna (i.e. þoct4 as described earlier). this is because functional experiments demonstrating changes in phenotype after perturbing oct4 or showing tf binding with associated transcriptional response of target genes could only be found for 20 predictions; therefore, we only counted those 20 predictions as functionally validated. the 50.6% accuracy rate is a conservative estimate, as the remaining predictions may not necessarily be false positives, but instead may represent unknown/novel functional relationships. altogether, these results demonstrate that given the target genes of a tf defined from chipx and tf perturbation data from one or a few biological contexts, chip-ped is capable of discovering tf-active contexts from a broad spectrum of ped samples."
"aml development history has moved from low-level models or text-based models to high-level implementations (e.g., pyomo, jump, and casadi) to facilitate rapid development. the next phase of accelerating the development process involves visual representation of the results. this is especially important in online control and estimation applications so the operator can easily visualize and track the application's progress and intentions. in some modeling languages, simply loading the optimization results into a scripting language for further processing and analysis can be difficult. gekko includes a built-in graphical interface to facilitate visualizing results."
"one of the primary limitations of chipx is it may be difficult for individual laboratories to study tf regulation in a wide variety of biological contexts, which we define as the cell or tissue types and associated treatments or disease conditions (see definition details in supplementary method 1.1). this is largely because of the prohibitively high labor and time costs to perform each experiment. to resolve this limitation, we investigate whether publicly available gene expression data (ped) in the gene expression omnibus (geo; [cit] ) can be used as a tool to increase the value of chipx experiments. currently, 4600 000 gene expression samples from a broad spectrum of biological contexts and species are deposited in the geo and arrayexpress [cit] . these data are freely available and contain rich information complementary to chipx, which may be extremely useful to help study tf regulation."
"equations are all solved together implicitly by the built-in optimizer. in dynamic modes, equations are discretized across the whole time horizon, and all time points are solved simultaneously. common unary operators are available with their respective automatic differentiation routines such as absolute value, exponentiation, logarithms, square root, trigonometric functions, hyperbolic functions, and error functions. the gekko operands are used in model construction instead of python math or numpy functions. the gekko operands are required so that first and second derivatives are calculated with automatic differentiation."
"two additional extensions in gekko modeling are the use of connections to link variables and object types (such as process flow streams). as an object-oriented modeling environment, there is a library of pre-built objects that individually consist of variables, equations, objective functions, or are collections of other objects."
"cubic splines are appropriate in cases where data points are available without a clear or simple mathematical relationship. when a high-fidelity simulator is too complex to be integrated into the model directly, a set of points from the simulator can act as an approximation of the simulator's relationships. when the user provides a set of input and output values, the gekko back-end builds a cubic spline interpolation function. subsequent evaluation of the output variable in the equations triggers a back-end routine to identify the associated cubic function and evaluate its value, first derivatives, and second derivatives. the cubic spline results in smooth, continuous functions suitable for gradient-based optimization."
"we have shown that chip-ped can improve the analysis of chipx data by integrating publicly available gene expression data. given a tf and its target genes, chip-ped examines the expression of the tf and the activity of its target genes across an assortment of diverse biological contexts to search for contexts with enriched regulatory activity of the tf. this process may lead to the discovery of novel functional connections between tf regulatory pathways and diseases, thus providing a cost effective way to expand knowledge from one chipx study to other research areas."
"the value of bias is updated from meas and the unbiased model prediction (model u ). the bias is added to each point in the horizon, and the controller objective function drives the biased model (model b ) to the requested set point range. this is shown in equation (16) ."
"gekko combines the model development, solution, and graphical interface for problems described by equation (2) . in this environment, differential equations with time derivatives are automatically discretized and transformed to algebraic form (see equation (6)) for solution by large-scale and sparse solvers."
"the following example demonstrates gekko's online mpc capabilities, including measurements, timeshifting, and mpc tuning. the mpc model is a generic first-order dynamic system, as shown in equation (17) . there exists plant-model mismatch (different parameters from the \"process_simulator\" function) and noisy measurements to more closely resemble a real system. the code is shown in listing 3 and the results are shown in figure 3, including the cv measurements and set points and the implemented mv moves."
"many tfs regulate a subset of their target genes through distal enhancers. recent tools, such as great [cit], have shown that by properly accounting for distal regulatory sites, (e) western blot analysis for myc protein, c-myc, in control and shmyc cells at 0 (pre) and 6 weeks (post). actin is provided as a loading control. blot displays decrease in myc protein levels on stable expression of shmyc in tc71 cells fig. 3 . series of chip-ped plots depicting the gradual decrease in stat1 expression and target gene (tg) activity when blood samples are successively drawn from hepatitis c-infected patients as they recover after treatment with interferon and ribavirin from day 1, 2, 7, 14 to 28 of recovery (gse7123). gray points are all samples in the gpl96 compendium, and colored points are the samples from the infected pbmcs in gse7123. the x-axis is stat1 expression (e tf ) and the y-axis is tg activity (a tg ). the mean e tf and a tg of each group of pbmcs are indicted at the top of each plot. normal pbmcs (bottom right in black) in gse7123 fall almost entirely out of the tfþtgþ cut-offs (the dashed lines), which suggests that only when infected with hepatitis c is stat1 functionally active in pbmcs one can improve the functional analysis of tf-binding sites. in our chip-ped analyses, we assigned peaks to genes if the peak overlapped with the à10-to þ5-kb region around each gene transcription start site, which may miss distal tf regulatory activity. this in turn may affect chip-ped prediction accuracy. to investigate, we generated chip-ped predictions for esr1 using target genes in estrogen-treated mcf7 cells derived from chromatin interaction analysis by paired-end tag sequencing (chia-pet), a method better able to link distal regulatory sites to tf-binding targets [cit], and compared them with predictions made using target genes defined by chip-seq using the à10-to þ5-kb window. we found that chia-pet-based predictions were similar to chip-seq-based predictions, and the former had slightly higher functional prediction accuracy of 43.5% compared with 40.7% (supplementary method 1.12 and supplementary table s7 ). we also analyzed all six tfs by using multiple annotation window sizes to annotate chipx peaks. different window sizes produced comparable prediction accuracies at the default significance cut-off. however, the à10-to þ5-kb window size produced the largest number (i.e. highest power) of functionally validated and/or indirectly supported predictions (supplementary method 1.12 and supplementary tables s9 and s10). thus, our results suggest that the à10-to þ5-kb window represents a reasonable choice as a default annotation region."
"sensitivity analysis is performed in one of two ways. the first method is to specify an option in gekko (sensitivity) to generate a local sensitivity at the solution. this is performed by inverting the sparse jacobian at the solution [cit] . the second method is to perform a finite difference evaluation of the solution after the initial optimization problem is complete. this involves resolving the optimization problem multiple times and calculating the resultant change in output with small perturbations in the inputs. for dynamic problems, the automatic time-shift is turned off for sensitivity calculation to prevent advancement of the initial conditions when the problem is solved repeatedly."
"after the initial chip-ped analysis, chip-ped can perform the following analyses to further explore each predicted context: (i) search for related contexts in the compendium based on userspecified keyword(s), (ii) extract the e tf and a tg values for the set of contexts found, (iii) calculate, sort and plot the mean and standard deviation of the e tf and a tg values for each context and (iv) perform t-tests between all pairwise combinations of the contexts for significant differences in mean e tf or a tg . see supplementary methods 1.6-1.7 for details and section 3.3 for an example analysis."
"the basic set of variable types includes constants, parameters, and variables. constants exist for programing style and consistency. there is no functional difference between using a gekko constant, a python variable, or a floating point number in equations. parameters serve as constant values, but unlike constants, they can be (and usually are) arrays. variables are calculated by the solver to meet constraints and minimize the objective. variables can be constrained to strict boundaries or required to be integer values. restricting variables to integer form then requires the use of a specialized solver (such as apopt) that iterates with a branch-and-bound method to find a solution."
"gekko fills the role of a typical aml, but extends its capabilities to specialize in dynamic optimization applications. as an aml, gekko provides a user-friendly, object-oriented python interface to develop models and optimization solutions. python is a free and open-source language that is flexible, popular, and powerful. ieee spectrum ranked python the #1 [cit] . being a python package allows gekko to easily interact with other popular scientific and numerical packages. further, this enables gekko to connect to any real system that can be accessed through python."
"we evaluated chip-ped by applying it to multiple tfs-oct4, gata1 and jarid2 in mice and myc, stat1 and esr1 in human-using the datasets listed in supplementary table s1 . the tf target genes were constructed by intersecting tfbound genes predicted from chipx data with differentially expressed genes [false discovery rate (fdr) 10%] in tf perturbation data. tf-bound genes were defined as genes with a significant peak (fdr 10%) overlapping with the à10-to þ5-kb region around the transcription start site of the gene. details are provided in supplementary method 1.2, and full target gene lists can be found in supplementary tables s2-s7. predictions were verified by a thorough search of existing literature to identify whether each prediction was functionally validated or suggested in previous experiments. 'functional' validations required previous experimental data from the predicted biological context demonstrating observable changes in phenotype when the expression of the tf is perturbed or tf binding coupled with transcriptional responses to tf binding of target genes. 'suggested' predictions must be supported by other lines of indirect evidence, such as experimentally observed high-tf protein levels in the predicted context. all supporting references are recorded in supplementary tables s2-s7 . we also experimentally validated a novel chip-ped functional connection between myc and ewing sarcoma (supplementary method 1.8)."
"fixed variables (fv) inherit parameters, but potentially add a degree of freedom and are always fixed throughout the horizon (i.e., they are not discretized in dynamic modes). estimated parameters, measured disturbances, unmeasured disturbances, and feed-forward variables are all examples of what would typically fit into the fv classification."
"since python is designed for readability and ease rather than speed, the python gekko model is converted to a low-level representation in the fortran back-end for speed in function calls. automatic differentiation provides the necessary gradients, accurate to machine precision, without extra work from the user. gekko then interacts with the built-in open-source, commercial, and custom large-scale solvers for linear, quadratic, nonlinear, and mixed integer programming (lp, qp, nlp, milp, and minlp) in the back-end. optimization results are loaded back to python for easy access and further analysis or manipulation."
"the objective function in equation (1) is minimized by adjusting the state variables x and inputs u. the inputs u may include variables such as measured disturbances, unmeasured disturbances, control actions, feed-forward values, and parameters that are determined by the solver to minimize the objective function j. the state variables x may be solved with differential or algebraic equations. equations include equality constraints ( f ) and inequality constraints (g)."
"we view chip-ped as an exploratory tool for fast and costeffective hypothesis generation and screening. in this respect, the default cut-offs that define high-or low-tf expression or tg activity should be primarily used for initial exploration or firstpass automatic hypothesis screening, rather than as strict optimal cut-offs that apply to all tfs. based on our real data analysis experience, we found it difficult to set a single consistent cut-off that was optimal across all tfs, as tfs can vary greatly in terms of regulatory behavior (fig. 2) . we, therefore, provide users with the flexibility to choose their own cut-offs, which can be adjusted to decrease or increase the number of predicted contexts (supplementary method 1.15) ."
"popular practical implementations of dynamic optimization include model predictive control (mpc) [cit] (along with its nonlinear variation nmpc [cit] and the economic objective alternative empc [cit] ), moving horizon estimation (mhe) [cit] and dynamic real-time optimization (drto) [cit] . each of these problems is a special case of equation (2) with a specific objective function. for example, in mpc, the objective is to minimize the difference between the controlled variable set point and model predictions, as shown in equation (3)."
"all types of gekko quantities may be included in the objective function expression, including constants, parameters, variables, intermediates, fvs, mvs, svs, and cvs. in some modes, gekko models automatically build objectives. mvs and cvs also contain objective function contributions that are added or removed with configuration options. for example, in mpc mode, a cv with a set point automatically receives an objective that minimizes error between model prediction and the set point trajectory of a given norm. there may be multiple objective function expressions within a single gekko model. this is often required to express multiple competing objectives in an estimation or control problem. although there are multiple objective expressions, all objectives terms are summed into a single optimization expression to produce an optimal solution."
"when studying the 88 functionally unverified predictions across the six tfs analyzed, we found that 51 of the 88 (58.0%) predictions were supported by other lines of indirect evidence in existing literature, such as experimentally observed high-tf protein level in the predicted context (supplementary tables s2-s7 ). thus, these predictions are likely to represent previously unknown functional predictions between each tf regulatory pathway and context, which further demonstrates that chip-ped can discover known and unknown tf-active biological contexts. in total, 141 of 178 (79.2%) tfþtgþ predictions for the six tfs analyzed were either directly supported by functional evidence (90 of 178) or indirectly supported in existing literature (51 of 178)."
"searching through ped only for tfþ samples or only for tgþ samples, rather than tfþ and tgþ samples, may result in substantially decreased chip-ped prediction accuracy and number of functionally validated predictions. for instance, when we modified chip-ped to search only for tfþ samples, we found that only 40.0% (62/155) of the predicted tfþ contexts were functionally validated in previous experiments compared with 50.6% (90/178) when using chip-ped to search for tfþ and tgþ samples (supplementary tables s2-s7) . conversely, searching for only tgþ samples resulted in only 34.0% (67/197) functionally validated tgþ predictions (supplementary tables s2-s7) . thus, it is useful to check both tf expression and target gene activity of each context to identify tfþ and tgþ samples when predicting tf-active biological contexts."
the time_shi f t parameter shifts all values through time with each subsequent resolve. this provides both accurate initial conditions for differential equations and efficient initialization of all variables (including values of derivatives and internal nodes) through the horizon by leaning on previous solutions.
"there are several modeling language and analysis capabilities enabled with gekko. some of the most substantial advancements are automatic (structural analysis) or manual (user specified) model reduction, and object support for a suite of commonly used modeling or optimization constructs. the object support, in particular, allows the gekko modeling language to facilitate new application areas as model libraries are developed."
"oct4 is a master regulator in mouse embryonic stem cells (escs). we obtained 519 activated and 337 repressed oct4 target genes by combining chip-seq data from mouse escs with gene expression data from escs in which oct4 was knocked down via sirna (supplementary tables s1 and s2) . using these target genes as input, oct4 target gene activity was plotted against oct4 expression after excluding the ped samples used to construct the target genes ( fig. 2a) . we found that undifferentiated escs clustered together with high-tf expression and high-tg activity. in contrast, differentiated escs or embryoid bodies (ebs) had lower tf expression and tg activity. this is consistent with the self-renewal and pluripotency role of oct4 in escs and its decrease in expression when escs differentiate [cit] ."
"differential algebraic equation (dae) systems are solved by discretizing the differential equations to a system of algebraic equations to achieve a numerical solution. some modeling languages are capable of natively handling daes by providing built-in discretization schemes. the daes are typically solved numerically and there are a number of available discretization approaches. historically, these problems were first solved with a direct shooting method [cit] . direct shooting methods are still used and are best suited for stable systems with few degrees of freedom. direct shooting methods eventually led to the development of multiple shooting, which provided benefits such as parallelization and stability [cit] . for very large problems with multiples degrees of freedom, \"direct transcription\" (also known as \"orthogonal collocation on finite elements\") is the state-of-the-art method [cit] . some fields have developed other unique approaches, such as pseudospectral optimal control methods [cit] ."
"controlled variables (cv) inherit svs but potentially add an objective. the cv object depends on the current mode of operation. in estimation problems, the cv object is constructed to reconcile measured and model-predicted values for steady-state or dynamic data. in control modes, the cv provides a setpoint that the optimizer will try to match with the model prediction values. cv model predictions are determined by equations, not as user inputs or solver degrees of freedom."
"the following three problems are examples of gekko used in solving classic dynamic optimization problems that are frequently used as benchmarks. the first example problem is a basic problem with a single differential equation, integral objective function, and specified initial condition, as shown in listing a2. the second example problem is an example of a dynamic optimization problem that uses an economic objective function, similar to empc but without the iterative refinement as time progresses, as shown in listing a3. the third example is a dynamic optimization problem that minimizes final time with fixed endpoint conditions, as shown in listing a4."
"gekko is an open-source python library with an mit license. the back-end fortran routines are not open-source, but are free to use for academic and commercial applications. it was developed by the prism lab at brigham young university and is in version 0.1 at the time of writing. documentation on the gekko python syntax is available in the online documentation, currently hosted on read the docs. the remainder of this text explores the paradigm of gekko and presents a few example problems, rather than explaining syntax."
"after measuring tf expression and tg activity, users can choose cut-offs c 1 $ c 4 to define (i) high-tf expression ('e tf 4c 1 '), (ii) low-tf expression ('e tf 5c 2 '), (iii) high-tg activity ('a tg 4c 3 ') and (iv) low-tg activity ('a tg 5c 4 '), denoted by tfþ, tfà, tgþ and tgà, respectively. by default, c 1 $ c 4 are chosen to be values corresponding to a one-sided p-value of 0.1 based on fitted normal distributions for e tf or a tg across all fig. 1 . chip-ped overview. gene expression profiles from tf perturbation experiments are intersected with chipx experiments to obtain a set of activated and repressed target genes. chip-ped then takes as input the tf and target genes and scans through a compendium of publicly available gene expression profiles to search for biological contexts in which the tf and target genes are enriched in activity. the final output is a ranked table of biological contexts enriched with a regulatory pattern of interest samples, with c 1 and c 3 taking values above the mean, and c 2 and c 4 taking values below the mean ( fig. 2a ). chip-ped can then search for biological contexts associated with four regulatory patterns: (i) tfþtgþ, (ii) tfþtgà, (iii) tfàtgþ and (iv) tfàtgà. the pattern tfþtgþ is of primary interest, as it focuses on discovering new contexts in which the tf is functionally active through its target genes (tf-active). this is because high-tf expression alone is not sufficient to imply the existence of functional tf protein because of possible post-transcriptional and translational regulation, but high-tg activity in addition to high-tf expression would strongly support the presence of active tf protein. other regulatory patterns are discussed in more detail in the supplementary method 1.4."
"computational power has increased dramatically in recent decades. in addition, there are new architectures for specialized tasks and distributed computing for parallelization. computational power and architectures have expanded the capabilities of technology to new levels of automation and intelligence with rapidly expanding artificial intelligence capabilities and computer-assisted decision processing. these advancements in technology have been accompanied by a growth in the types of mathematical problems that applications solve. lately, machine learning (ml) has become the must-have technology across all industries, largely inspired by the recent public successes of new artificial neural network (ann) applications. another valuable area that is useful in a variety of applications is dynamic optimization. applications include chemical production planning [cit], energy storage systems [cit], polymer grade transitions [cit], integrated scheduling and control for chemical manufacturing [cit], cryogenic air separation [cit], and dynamic process model parameter estimation in the chemical industry [cit] . with a broad and expanding pool of applications using dynamic optimization, the need for a simple and flexible interface to pose problems is increasingly valuable. gekko is not only an algebraic modeling language (aml) for posing optimization problems in simple object-oriented equation-based models to interface with powerful built-in optimization solvers but is also a package with the built-in ability to run model predictive control, dynamic parameter estimation, real-time optimization, and parameter update for dynamic models on real-time applications. the purpose of this article is to introduce the unique capabilities in gekko and to place this development in context of other packages."
"dynamic optimization is a unique subset of optimization algorithms that pertain to systems with time-based differential equations. dynamic optimization problems extend algebraic problems of the form in equation (1) to include the possible addition of the differentials dx dt in the objective function and constraints, as shown in equation (2) ."
"this section presents a set of example gekko models in complete python syntax to demonstrate the syntax and available features. solutions of each problem are also presented. additional example problems are shown in the back matter, with an example of an artificial neural network in appendix a and several dynamic optimization benchmark problems shown in appendix b. since the gekko fortran backend is the successor to apmonitor [cit], the many applications of apmonitor are also possible within this framework, including recent applications in combined scheduling and control [cit], industrial dynamic estimation [cit], drilling automation [cit], combined design and control [cit], hybrid energy storage [cit], batch distillation [cit], systems biology [cit], carbon capture [cit], flexible printed circuit boards [cit], and steam distillation of essential oils [cit] ."
"gekko has multiple high-end solvers pre-compiled and bundled with the executable program instead of split out as a separate programs. the bundling allows out-of-the-box optimization, without the need of compiling and linking solvers by the user. the integration provides efficient communication between the solver and model that gekko creates as a human readable text file. the model text file is then compiled to efficient byte code for tight integration between the solver and the model. interaction between the equation compiler and solver is used to monitor and modify the equations for initialization [cit], model reduction, and decomposition strategies. the efficient compiled byte-code model includes forward-mode automatic differentiation (ad) for sparse first and second derivatives of the objective function and equations."
"as a dynamic optimization package, gekko accommodates dae systems with built-in discretization schemes and facilitates popular applications with built-in modes of operation and tuning parameters. for differential and algebraic equation systems, both simultaneous and sequential methods are built in to gekko. modes of operation include data reconciliation, real-time optimization, dynamic simulation, moving horizon estimation, and nonlinear predictive control. the back-end compiles the model to an efficient low-level format and performs model reduction based on analysis of the sparsity structure (incidence of variables in equations or objective function) of the model. sequential methods separate the problem in equation (2) into the standard algebraic optimization routine equation (1) and a separate differential equation solver, where each problem is solved sequentially. this method is popular in fields where the solution of differential equations is extremely difficult. by separating the problems, the simulator can be fine-tuned, or wrapped in a \"black box\". since the sequential approach is less reliable in unstable or ill-conditioned problems, it is often adapted to a \"multiple-shooting\" approach to improve performance. one benefit of the sequential approach is a guaranteed feasible solution of the differential equations, even if the optimizer fails to find an optimum. since gekko does not allow connecting to black box simulators or the multiple-shooting approach, this feasibility of differential equation simulations is the main benefit of sequential approaches."
"the simultaneous approach, or direct transcription, minimizes the objective function and resolves all constraints (including the discretized differential equations) simultaneously. thus, if the solver terminates without reaching optimality, it is likely that the equations are not satisfied and the dynamics of the infeasible solution are incorrect-yielding a worthless rather than just suboptimal solution. however, since simultaneous approaches do not waste time accurately simulating dynamics that are thrown away in intermediary iterations, this approach tends to be faster for large problems with many degrees of freedom [cit] . a common discretization scheme for this approach, which gekko uses, is orthogonal collocation on finite elements. orthogonal collocation represents the state and control variables with polynomials inside each finite element. this is a form of implicit runga-kutta methods, and thus it inherits the benefits associated with these methods, such as stability. simultaneous methods require efficient large-scale nlp solvers and accurate problem information, such as exact second derivatives, to perform well. gekko is designed to provide such information and take advantage of the simultaneous approach's benefits in sparsity and decomposition opportunities. therefore, the simultaneous approach is usually recommended in gekko."
each gekko model is an object. multiple models can be built and optimized within the same python script by creating multiple instances of the gekko model class. each variable type is also an object with property and tuning attributes.
"if the model can be placed in this form, the open-loop stability of the model is determined by the sign of the eigenvalues of matrix a. stability analysis can also be performed with the use of a step response for nonlinear systems or with lyapunov stability criteria that can be implemented as gekko equations."
"in this article, we demonstrate that this is indeed the case by proposing and evaluating a new approach, chip-ped. given a tf regulatory pathway, i.e. a tf and the corresponding set of target genes defined using chipx and gene expression data in one or more biological contexts, chip-ped scans through a large collection of420 000 human and mouse gene expression samples generated by hundreds of different laboratories by quickly surveying the tf and target gene activities across 42000 biological contexts to identify potentially new connections between the tf regulatory pathway and various cell types, tissues or diseases (fig. 1 ). we will illustrate that the predictions from chip-ped are useful and can greatly expand the scope of discoveries one can make from chipx experiments. we also provide an r package for users to perform chip-ped analyses on their own chipx and tf perturbation data."
"gekko has additional options that are tailored to online control and estimation applications. these include meas, bias, and time_shi f t. the meas attribute facilitates loading in new measurements in the appropriate place in the time horizon, based on the application type."
the back-end fortran routines are only compiled for windows and linux at this time. the routines come bundled with the package for these operating systems to enable local solutions. macos and arm processors must use the remote solve options to offload their problems to the main server.
"chipx experiments, including chip-seq [cit] and chip-chip [cit], have become a powerful tool used by individual investigators, as well as consortium projects, such as the encode [cit] to study transcription factor-binding sites. each individual chipx experiment is non-trivial to perform-extensive time and effort must be spent to acquire effective antibodies and design efficient protocols to generate high-quality chipx data-thus, it is important to develop methodology to help investigators to maximize the value of each individual chipx experiment."
"chip-ped represents a novel conceptual approach to building computational tools for chipx data analysis. most existing tools for analyzing chipx data, including those for detecting protein-dna-binding sites [cit], discovering dna-binding motifs [cit], correlating chipx with gene expression data [cit] and so forth, focus on addressing analysis issues concerning a single or a few related chipx datasets. their discoveries are also typically restricted to the biological context in which the chipx experiments are performed, and none of them systematically integrates information from ped. ped has been shown to be invaluable in other applications [cit], but the possibility of using ped as a tool to boost the analysis of chipx data still remains largely unexplored. a number of methods do integrate large amounts of chipx and gene expression data to construct gene regulatory networks, but most are primarily used to study lower organisms (e.g. yeast; [cit] . the present study is different from those described works, as chip-ped focuses specifically on integrating chipx with large amounts of heterogeneous data in human and mouse to improve chipx analyses. instead of attempting to construct a comprehensive gene regulatory network, the primary goal of chip-ped is to produce simple testable hypotheses, such as 'tf a is functionally active in biological contexts x, y and z through target gene set s'."
"a differential term is expressed in an equation with x.dt(), where x is a variable. differential terms can be on either the right or the left side of the equations, with equality or inequality constraints, and in objective functions. some software packages require index-1 or index-0 differential and algebraic equation form for solution. there is no dae index limit in gekko or need for consistent initial conditions. built-in discretization is only available in one dimension (time). discretization of the the differential equations is set by the user using the gekko model attribute time. the time attribute is set as an array which defines the boundaries of the finite elements in the orthogonal collocation scheme. the number of nodes used to calculate the internal polynomial of each finite element is set with a global option. however, these internal nodes only serve to increase solution accuracy since only the values at the boundary time points are returned to the user."
"chip-ped first measures the tf expression and tg activity in each microarray sample in our ped compendiums. tf expression, e tf, is defined as a simple average of the normalized probeset intensities, p:"
"chip-ped would not be useful if the predicted biological contexts were always closely related to the context in which the experimental data were generated. our results indicate otherwise: among the 90 of 178 (50.6%) predictions supported by previous functional experiments, 40 (44.4%) are in contexts unrelated to the context(s) in which the experimental data used to construct the tf target genes were obtained (supplementary tables s2-s7) . furthermore, chip-ped can provide additional biological insights that otherwise could not be made using standard chipx analyses. for example, after the initial stat1 chip-ped analysis described in section 3.2, we found many hepatitis c-infected pbmcs predictions from experiment gse7123 (supplementary table s5 and supplementary method 1.11). to examine stat1 functional activity in hepatitis c-infected pbmcs in more detail, we searched for all contexts in gse7123 and also found healthy pbmcs along with the predicted hepatitis c-infected pbmcs. we then used chip-ped to compare tf expression and tg activity in each context and found that e tf and a tg values were significantly different between healthy and hepatitis c-infected pbmcs, with a gradual decrease in e tf and a tg values as patients recovered from infection (supplementary table s5, fig. 3 and supplementary fig. s6 ). when reviewing both of the original publications, the stat1 chipx study [cit] and the study that generated the gene expression profiles from hepatitis c-infected pbmcs [cit], we found that neither study had reported this finding. to verify whether this observation was correct, we searched through existing literature and found an entirely independent experiment that showed in a series of overexpression and sirna-mediated knock-down experiments of stat1 in hepatitis c virus-infected pbmcs that stat1 protein was indispensable for the control of hepatitis c virus expression [cit] ."
"in addition to those listed above, many other software libraries are available for modeling and optimization, including aimms [cit], cvx [cit], cvxopt [cit], yalmip [cit], pulp [cit], poams, openopt, nlpy, and pyipopt."
"although we have shown that chip-ped is able to capture pertinent biological information in ped, better statistical models are still needed to address technical biases and variations because of laboratory and batch effects. a natural extension of chip-ped would be to analyze multiple tfs and their tgs together to better connect cooperative tf regulatory pathways to cell types and diseases. similarly, more work is also needed to understand how homologous tfs or other tfs with similar regulatory functions impact the regulatory activity of the tf of interest in different contexts."
"where tf is the set of probesets that measure the expression of the tf, and n tf is the number of probesets for the tf. tg activity, a tg, is defined as:"
"we first investigated whether it was appropriate to compare gene expression across thousands of heterogeneous microarray samples generated by different laboratories. to this end, we asked whether laboratory and batch effects were a significant and detrimental source of variation [cit] . previous efforts have been made using our gene expression compendiums to demonstrate that similar tissue types do cluster together [cit], and that it is possible to accurately predict tissue types from a single gene expression profile in spite of the laboratory or batch effects [cit] . we reaffirmed these findings by observing that samples from the same tissues from different laboratories were more similar in expression compared with samples from different tissues from the same laboratory ( supplementary fig. s4 and supplementary method 1.9). we then examined the correlation between tf expression (e tf ) and tg activity (a tg ) for multiple tfs, including mouse oct4 and gata1 and human myc and stat1. we reasoned that if there were strong laboratory or batch effects that overwhelmed the biological signal, we would observe weak to zero correlation between e tf and a tg across the heterogeneous samples. instead, we found significant correlation between tf expression and tg activity; the pearson correlation coefficients between e tf and a tg for oct4, myc, gata1 and stat1 were 0.679, 0.418, 0.303 and 0.699, respectively (p50.02; fig. 2 ). as this observation holds for multiple mouse and human tfs from different microarray platforms (gpl1261 and gpl96), our results suggest that biological variability in the publicly available affymetrix microarray data is stronger than the laboratory or batch effects. [cit] ."
"in total, chip-ped predicted 28 biological contexts were enriched with tfþtgþ activity at a bonferroni-corrected p-value cut-off of 0.05 (supplementary table s2 ). among these, 89.3% (25/28) were different þoct4 contexts, and 10.7% (3/28) were -oct4 contexts related to differentiating escs and ebs. the 28 statistically enriched contexts covered 47.9% (230/480) of the tfþtgþ samples. these samples were from multiple laboratories (e.g. normal undifferentiated escs: 11 experiments), confirming that the observed enrichment in escs was unlikely to be caused by experimental artifacts or laboratory or batch effects. more importantly, chip-ped filtered out most -oct4 biological contexts: 30.8% (148/480) tfþtgþ samples were from -oct4 contexts, whereas only 10.7% (3/28) of the tfþtgþ-enriched contexts were from -oct4 contexts, and among the samples of the 28 tfþtgþ-enriched contexts, only 8.7% (20/230) were -oct4 samples. therefore, by integrating information from multiple samples, predictions made at the context level are more accurate than at the sample level."
"the main limitation of gekko is the requirement of fitting the problem within the modeling language framework. most notably, user-defined functions in external libraries or other such connections to \"black boxes\" are not currently enabled. logical conditions and discontinuous functions are not allowed but can be reformulated with binary variables or mathematical programming with complementarity constraints (mpccs) [cit] so they can be used in gekko. if-then statements are purposely not allowed in gekko to prevent discontinuities. set-based operations such as unions, exclusive or, and others are also not supported. regarding differential equations, only one discretization scheme is available and it only applies in one dimension (time). further discretizations must be performed by the user."
"dynamic optimization problems introduce an additional set of challenges. many of these challenges are consistent with those of other forms of ordinary differential equation (ode) and partial differential equation (pde) systems; only some challenges are unique to discretization in time. these challenges include handling stiff systems, unstable systems, numerical versus analytical solution mismatch, scaling issues (the problems get large very quickly with increased discretization), the number and location in the horizon of discretization points, and the optimal horizon length. some of these challenges, such as handling stiff systems, can be addressed with the appropriate discretization scheme. other challenges, such as the necessary precision of the solution and the location of discretizations of state variables, are better handled by a knowledgeable practitioner to avoid excessive computation."
"in mpc, the cvs have several options for the adjusting the performance such as speed of reaching a new set point, following a predetermined trajectory, maximization, minimization, or staying within a specified deadband. the cv equations and variables are configured for fast solution by gradient-based solvers, as shown in equations (12)- (14) . in these equations, tr hi and tr lo are the upper and lower reference trajectories, respectively. the wsp hi and wsp lo are the weighting factors on upper or lower errors and cost is a factor that either minimizes (−) or maximizes (+) within the set point deadband between the set point range sp hi an alternative to equation (12b-e) is to pose the reference trajectories as inequality constraints and the error expressions as equality constraints, as shown in equation (13). this is available in gekko to best handle systems with dead-time in the model without overly aggressive mv moves to meet a first-order trajectory. while the 1 -norm objective is default for control, there is also a squared error formulation, as shown in equation (14) . the squared error introduces additional quadratic terms but also eliminates the need for slack variables e hi and e lo as the objective is guided along a single trajectory (tr) to a set point (sp) with priority weight (wsp). it is important to avoid certain optimization formulations to preserve continuous first and second derivatives. gekko includes both mv and cv tuning with a wide range of options that are commonly used in advanced control packages. there are also novel options that improve controller and estimator responses for multi-objective optimization. one of these novel options is the ability to specify a tier for mvs and cvs. the tier option is a multi-level optimization where different combinations of mvs and cvs are progressively turned on. once a certain level of mv is optimized, it is turned off and fixed at the optimized values while the next rounds of mvs are optimized. this is particularly useful to decouple the multivariate problem where only certain mvs should be used to optimize certain cvs although there is a mathematical relationship between the decoupled variables. both mv and cv tuning can be employed to \"tune\" an application. a common trade-off for control is the speed of cv response to set point changes versus excessive mv movement. gekko offers a full suite of tuning options that are built into the cv object for control and estimation."
important cv options are f status for estimation and status for control. these options determine whether the cv objective terms contribute to the overall objective (1) or not (0).
"tf target genes can vary from one cell type to another. if two known tf-active contexts do not share any target genes, then chip-ped will not be able to predict either context using target genes constructed from the other context. to test whether chip-ped can still be effective when only a minority of the target genes are shared, we used chip-ped to analyze stat3 target genes constructed from mouse cd4þ t cells and th17 cells, which are both contexts in which stat3 plays an important regulatory role [cit] . we found that chip-ped was able to successfully recover both cd4þ t cells and th17 cells when analyzing target genes defined from the other context, even though 530% of the target genes were in common (supplementary method 1.10, supplementary table s8 and supplementary fig. s5 )."
"we also compared how well a median, rather than mean, target gene activity measure would perform and found the predictions and prediction accuracy to be almost the same; across all six tfs, 171 predictions were identical between the two measures accounting for 98.8% (171/173) of the median-based predictions and 96.1% (171/178) of the mean-based predictions (supplementary method 1.13 and supplementary fig. s9 ). in addition, we checked whether predicted biological contexts with more samples in the compendium were more or less accurate than predicted contexts with fewer samples. our results were unable to find a clear monotone relationship between sample count for a given biological context and prediction accuracy (supplementary method 1.14 and supplementary table s11)."
"all gekko variables (with the exception of fvs) and equations are discretized uniformly across the model time horizon. this approach simplifies the standard formulation of popular dynamic optimization problems. to add flexibility to this approach, gekko connections allow custom relationships between variables across time points and internal nodes. connections are processed after the parameters and variables are parsed but before the initialization of the values. connections are the merging of two variables or connecting specific nodes of a discretized variable or setting just one unique point fixed to a given value."
"after verifying that it is meaningful to compare e tf and a tg across heterogeneous samples, we asked whether the samples observed with high-tf expression and high-tg activity (tfþtgþ) and the biological contexts enriched with a tfþtgþ regulatory pattern were biologically meaningful. in this regard, we performed and evaluated chip-ped analyses of six tfs: mouse oct4, gata1 and jarid2 and human myc, stat1 and esr1."
"chip-ped acts primarily as a guide to highlight biological contexts that would be good leads for experimental investigation. as such, we do not expect all chip-ped predictions to be correct nor for chip-ped to recover all tf-active biological contexts. this, however, does not prevent chip-ped from being a useful and unique tool: our analyses have shown that it can predict many known and new tf-active contexts with reasonable accuracy, and there currently exists no other computational method for analyzing chipx data that performs a similar task."
"other modeling and optimization platforms focus on ultimate flexibility. while gekko is capable of flexibility, it is best suited for large-scale systems of differential and algebraic equations with continuous or mixed integer variables for dynamic optimization applications. gekko has a graphical user interface (gui) and several built-in objects that support rapid prototyping and deployment of advanced control applications for estimation and control. it is a full-featured platform with a core that has been successfully deployed on many industrial applications."
"gross error detection is critical for automation solutions that use data from physical sensors. sensors produce data that may be corrupted during collection or transmission, which can lead to drift, noise, or outliers. for fv, mv, sv, and cv classifications, measured values are validated with absolute validity limits and rate-of-change validity limits. if a validity limit is exceeded, there are several configurable options such as \"hold at the last good measured value\" and \"limit the rate of change toward the potentially bad measured value\". many industrial control systems also send a measurement status (pstatus) that can signal when a measured value is bad. bad measurements are ignored in gekko and either the last measured value is used or else no measurement is used and the application reverts to a model predicted value."
"besides verifying that chip-ped is able to correctly predict known tf-active biological contexts, we also experimentally investigated whether the predictions that were not functionally validated could possibly represent unknown tf-active biological contexts. as a proof-of-concept, we used our chip-ped analysis of human myc to illustrate the discovery of a novel myc-active context. among the enriched tfþtgþ predictions from the myc chip-ped analysis, 18 of 33 (54.5%) biological contexts were not supported by functional experiments that demonstrated myc functional activity (supplementary table s3 ). one of the non-functionally validated contexts was a673 cells (fig. 4a ), which were established from a patient with ewing sarcoma (martı´nez-ramı´ [cit] ) . although ewing tumor has been previously shown to exhibit high-myc expression [cit], the functional role of myc protein in ewing tumor currently remains uncharacterized. to verify the novel prediction that myc protein plays a functional role in ewing tumor, we assessed the phenotype changes of independent ewing sarcoma cell lines on myc knockdown. knocking down of myc using shmyc in tc71 and mhh-es ewing sarcoma cell lines resulted in a substantially slower proliferation rate and tumorigenicity when compared with control cells (fig. 4b and c and supplementary figs s7 and s8) . furthermore, xenograft of control and shmyc tc71 ewing sarcoma cells into immunodeficient mice (nod/scid/il-2 null) resulted in a significant decrease in volume and weight for the myc knockdown tumors after 6 weeks of growth (fig. 4d) . subsequent isolation of the tumors confirmed the decrease in myc protein by western blot analysis (fig. 4e) . these results strongly support the novel prediction that the myc protein plays a key functional role in ewing tumor."
"then given a compendium of n gene expression profiles, chip-ped searches among all biological contexts with at least three samples, for contexts that are associated with the regulatory pattern of interest (e.g. tfþtgþ). for each context c, it counts (i) k, the total number of samples in the compendium that exhibit the pattern, (ii) n c, the total number of samples in context c, and (iii) k c, the number of samples in context c that exhibit the pattern. fisher's exact test is then applied to the quadruplet (n c, n, k c and k) to test the association between c and the regulatory pattern of interest (i.e. whether k c is significantly larger than random expectation). to account for testing multiple contexts, the p-values are adjusted using the bonferroni correction. the final output of chip-ped is a ranked table of statistically significant biological contexts at a default bonferroni corrected p-value cut-off of 0.05 (supplementary method 1.5)."
"given a tf and its activated and repressed target genes defined using chipx and gene expression data in one or more biological contexts, chip-ped searches for other contexts in which the tf is likely to be functionally active. target genes (tg) are genes that are both tf-bound in the chipx experiments and differentially expressed in corresponding gene expression data in which the expression of the tf is perturbed. the latter are from tf perturbation experiments comparing wild-type with tf-knockout, control with tf-knockdown or control with tf-overexpression and so forth. users will need to provide and analyze their own chipx and tf perturbation experiments to define the input target genes. supplementary method 1.2 discusses methods for generating target gene lists. to define target genes in a particular biological context, ideally one would like to have chipx and tf perturbation data from the same biological context. however, such data may not always be available, and it is not uncommon to have chipx and tf perturbation data collected from two different contexts. in that case, one can still intersect the data from different experiments to obtain a putative target gene set assumed to contain the shared targets."
"where n is the number of nodes in each time interval, ∆u is the change of u, ∆u − is the negative change, ∆u + is the positive change and dudt is the slope. the mv object equations and objective are different for a squared error formulation as shown in equation (9). the additional linear inequality constraints for ∆u + and ∆u − are not needed and the penalty on ∆u is squared as a move suppression factor that is compatible in a trade-off with the squared controlled variable objective."
"algebraic modeling languages (aml) facilitate the interface between advanced solvers and human users. high-end, off-the-shelf gradient-based solvers require extensive information about the problem, including variable bounds, constraint functions and bounds, objective functions, and first and second derivatives of the functions, all in consistent array format. amls simplify the process by allowing the model to be written in a simple, intuitive format. the modeling language accepts a model (constraints) and objective to optimize. the aml handles bindings to the solver binary, maintains the required formatting of the solvers, and exposes the necessary functions. the necessary function calls include constraint residuals, objective function values, and derivatives. most modern modeling languages leverage automatic differentiation (ad) [cit] to facilitate exact gradients without explicit derivative definition by the user."
"partial differential equations (pdes) are allowed within the gekko environment through manual discretization in space with vectorized notation. for example, listing 1 shows the numerical solution to the wave equation shown in equation (7) through manual discretization in space and built-in discretization in time. the simulation results are shown in figure 1 ."
"gekko features special variable types that facilitate the tuning of common industrial dynamic optimization problems with numerically robust options that are efficient and easily accessible. these special variable types are designed to improve model efficiency and simplify configuration for common problem scenarios. in very large-scale problems, removing a portion of variables from the matrix math of implicit solutions can reduce matrix size, keeping problems within the efficient bounds of hardware limitations. this is especially the case with simultaneous dynamic optimization methods, where a set of model equations are multiplied over all of the collocation nodes. for each variable reduced in the base model, that variable is also eliminated from every collocation node. intermediates are formulated to be highly memory-efficient during function calls in the back-end with the use of sparse model reduction. intermediate variables essentially blend the benefits of sequential solver approaches into simultaneous methods."
"where f status is the feedback status that is 0 when the measurement is not used and 1 when the measurement reconciliation is included in the overall objective (intermediate values between 0 and 1 are allowed to weight the impact of measurements). a measurement may be discarded for a variety of reasons, including gross error detection and user specified filtering. for measurements from real systems, it is critical that bad measurements are blocked from influencing the solution. if bad measurements do enter, the 1 -norm solution has been shown to be less sensitive to outliers, noise, and drift [cit] . the cv object is different for a squared-error formulation, as shown in equation (11) . the desired norm is easily selected through a model option."
"where x is a state variable and x sp is the desired set point or target condition for that state. the objective is typically a 1-norm, 2-norm, or squared error. empc modifies mpc by maximizing profit rather than minimizing error to a set point, but uses the same dynamic process model, as shown in equation (4)."
"finally, it must be remembered that the actual optimization occurs in the bundled solvers. while these solvers are state-of-the-art, they are not infallible. gekko back-end adjustments, such as scaling, assist the solver, but it falls to the user to pose feasible problems and formulate them to promote convergence. knowledge of the solver algorithms allows users to pose better problems and get better results."
"there are many software packages and modeling languages currently available for optimization and optimal control. this section, while not a comprehensive comparison, attempts to summarize some of the distinguishing features of each package."
"therefore, the spca problem (6) is equivalent to the standard pca or probabilistic pca problems using the 'weighted' data matrixx [cit], where the weights are learned from the external information, i.e., the spatial, temporal or population structures of data, as:"
"the remainder of the paper is organized as follows. we begin by reviewing pca and its probabilistic model. next, we describe the details of skpcr, including skpca and adaptive regression (see fig. 1 for a graphical overview). we then conduct comprehensive simulations to compare the proposed skpcr approach with several state-of-the-art methods discussed above, such as the univariate approach, mdmr, aspu, in terms of their false-positive rate, power of detecting signals and computation time. finally, using four schizophrenia datasets, we evaluate and compare their between-sites reproducibility and the proportion of overlaps with existing schizophrenia meta-analysis findings. the code for our approach can be downloaded from https://github.com/ weikanggong/skpcr."
"we first evaluate whether skpcr could control the type i error rate using different kernels in different types of data. we simulated a casecontrol study in the absence of any group difference (see section 3.2), and skpcr was applied to detect signals. the results show that the proposed approach can control the false-positive rate appropriately using a wide range of kernels (linear, polynomial and gaussian) in both volumebased and surface-based fmri data (fig. 2), because the observed falsepositive rates are similar to their theoretical nominal level at 0.05. fig. 3 shows the results of comparing the power of different methods when the true signal is linear, i.e. some functional connectivities are linearly correlated with a simulated phenotype of interest, in volumebased fmri data (see section 3.3). similarly, fig. 4 shows the results of the same simulation using surface-based fmri data."
"in this paper, the controller is coupled with a novel observer to estimate f r, f f . the wheel dynamic model proposed is used to develop the observer, where the inputs of the observer are the torques acting on each wheel and the angular velocity of each wheel. the effects of rolling resistance is neglected since the dominant force here is the longitudinal tire force. the wheel dynamics can be expressed as:"
"additional areas can be refined. first, the method currently can only analyse binary and continuous phenotype variables. however, it could be extended to analyse categorical and multivariate phenotypes. second, a sparse version of skpca, which allows only a subset of functional connectivities related to a principal component, may further improve the performance of dimension reduction, just like sparse pca [cit] improves pca. third, with the larger size of the available datasets, such as hcp and the uk-biobank, an online version of skpca should be an important extension because it is not currently possible to fit thousands of high-resolution fmri data into memory. however, skpcr could be equipped to analyse big datasets by borrowing ideas from the online/group pca approach and other related variants (monti and hyv€ [cit] . fourth, it would also be very interesting to extend skpcr to infer causal relationships [cit] . finally, the current skpcr method is designed for single-site studies. however, combining the skpcr results from multiple imaging sites is an important extension for the future. possible methods include conventional meta-analysis methods and the model-based site-effect adjustment methods, such as combat [cit] . table 2 comparing the between-sites reproducibility of different approaches in 4 schizophrenia datasets using the dice coefficient (dc), with voxel-wise p-value threshold of 0.001 and permutation-based cluster-size fwer correction. in bold: the best performance among all methods."
"similar to the kernel principal component analysis (sch€ [cit] ), the optimization problem (8) can be solved by first performing a mean normalization of the kernel matrix k 2 ℝ nân, where"
"for both volume-based and surface-based data, it can be clearly seen that the proposed skpcr method with a linear kernel always has the highest power in different situations (different signal-to-noise ratios and proportions of non-null connectivities). the univariate approach and aspu have similar power in different situations. the performance of mdmr and spu(2) are similar to that of aspu and the univariate method when the number of non-null functional connectivities is large (e.g., 20% non-null). however, the power of mdmr and spu(2) decreases dramatically when only a few non-null connectivities exist (e.g., 1% nonnull). spu(1) performs the worst in these simulations. in addition, the power of skpcr displays a larger gap with other approaches in surfacebased fmri data compared to volume-based fmri data."
"the observer requires an initial value for the estimated longitudinal tractive force, that can be simply chosen as the initial torques divided by the effective tire radius."
"four resting-state fmri datasets were used here: taiwan, centers of biomedical research excellence (cobre http://fcon_1000.projects. nitrc.org/indi/retro/cobre.html), braingluschi (http://schizconnect. org/), and nmorphch (http://schizconnect.org/). all of them are datasets with schizophrenia patients and matched healthy controls. the demographic information is shown in table 1 . resting-state fmri data were preprocessed using the same pipeline as the swu dataset (code can be download from https://github.com/weikanggong/resting-state-fmripreprocessing). finally, 18757 voxels located in each subject's cerebral regions were extracted for the subsequent analysis."
"in electric vehicles, the driving torque is available via the electric motor drive. the angular velocity can be estimated by using an observer that its structure is similar to that of a proportional-integral-derivative (pid) controller, i.e.:"
"as illustrated in section 2.3, the proposed adaptive regression approach, which was used to detect association after skpca dimension reduction, was robust to the misspecification of number of principle components. to demonstrate this, we conducted a simulation study to compare it with the traditional general linear model approach."
"the super-twisting algorithm (sta) is a well-known second-order slidingmode control (2-smc) [cit] . the chattering phenomenon found in the conventional sliding-mode control (smc) algorithm prevents it from being extensively used in practice. second-order sliding mode control is a better solution to reduce the chattering effect without affecting the robustness of the system because it uses the second time derivative of the sliding variable instead of the first derivative. moreover, unlike second-order sliding mode control, which needs the first time derivative of the sliding variable, sta does not require the information of time derivatives of the sliding variable, which makes its implementation easier."
"again, when the true signals are nonlinear, we can see from figs. 6 and 7 that methods using a linear kernel usually have decreased power compared with the corresponding nonlinear kernel. this highlights the importance of specifying correct kernels in the analysis to achieve optimal power. however, this does not mean that our method is sensitive to the choice of kernels. in practice, we can run skpcr with different kernels, and for each kernel, we will get a voxel-wise p-value map which reflects the strength of different types of association signals. in addition, although the power of skpcr with a linear kernel decreases when the true signals are nonlinear, we can see that it is still higher than many other approaches, even with the correct nonlinear kernels. this may result from its effective modelling of the spatial structure of the data."
"a voxel/vertex identified by this approach can be interpreted as 'there may exist one or more functional connectivities which connect it that are associated with the phenotype of interest'. to know the associated connections, a subsequent seed-based analysis can be performed. that is, we can extract a seed time series by averaging the voxel/vertex's time series within a significant cluster and test the associations between the seed connectivity map and a phenotype of interest. however, no significant individual connections in the seed-based analysis may be found because our approach can detect more than simple linear association signals. for example, consider a scenario in which many of the connections only have small effect sizes. in addition, as our approach can produce a voxel-wise statistical map, but not a connectivity-wise result, it can be directly compared with results of other analyses, such as task-activation studies, voxel-based morphometry (vbm) analysis, and neurosynth metaanalysis results, even though our results do not reflect the direction of the association."
"where is the slip ratio. there are many factors affect the friction coefficient, which makes the behavior of the tractive forces complicated, such as road conditions, wheel slip, road surface, tire type, etc. the typical friction coefficient in dependency of the slip ratio is shown in figure (3) . it shows that some amount of slip is required to produce tractive force however; an excessive slip leads to a loss of the force."
"here, both the rows (e.g. features) and columns (e.g. subjects) of the noise matrix are assumed to be independent from each other. however, for neuroimaging data, noises among voxels/vertices are known to be smoothly correlated with each other. therefore, our spca model is a generalization of the probabilistic pca model, which allows two-way dependence between noise terms:"
"we use two resting-state fmri datasets to evaluate different methods, including 281 subjects from the southwest university (swu) dataset in the international data-sharing initiative (idni, http://fcon_1000. projects.nitrc.org/indi/retro/southwestuni_qiu_index.html), and 150 subjects from the human connectome project (hcp_rest1_lr, https:// www.humanconnectome.org/). all subjects were healthy adults with similar demographic information."
"we applied skpcr with a linear kernel to identify voxels with significantly altered connectivities in each schizophrenia dataset separately. at the same time, five other methods, including the univariate approach, mdmr, spu(1), spu(2) and aspu, were used for comparison. to compare the between-sites reproducibility of each method, they were applied to analyse the four schizophrenia datasets separately, and the dice coefficient (dc) between the resulting voxel-wise p-value maps of two sites was calculated. dc is defined as"
"finally, we compare the computation time of skpcr, mdmr and aspu in the above analyses. [cit] b using 20 cores on a linux workstation with intel xeon e5-2660 v3 (2.60 ghz) cpu and 128 gb memory. table 4 shows that our method is the most efficient."
"the super-twisting algorithm (sta) is one of the most promising second-order sliding-mode control (smc) [cit] . sta is considered as a solution for the chattering problem that appears in the conventional sliding mode controller while maintaining the same robustness and performance as that of smc. the chattering problem is harmful, because it leads to low control accuracy, high wear of moving mechanical parts and high heat losses in power circuits. [cit] ). consider the below dynamics system in (1), the sta can be expressed as in equations (2&3) [cit] ."
"traction control plays an important role in safety during acceleration on dry or slippery roads by preventing wheel slippage because it can directly enhance drive efficiency, safety, and stability. traction is the vehicular propulsive force produced by friction between tire and road [cit] . the amount of traction changes depending on a variety of factors such as the surface of the road, the conditions of the tire and the weight of the vehicle as well as the driver's behavior. hard braking, oversteering and severe acceleration each affect the traction of the wheel. one-third of the fatal crashes worldwide are due to loss of driver control over the vehicle [cit] ."
"in this paper, we propose a novel multivariate approach specifically designed to detect associations in the voxel-level connectome and overcome the above mentioned drawbacks of previous methods. this approach, termed 'structured kernel principal component regression' (skpcr), is specifically designed for the voxel-level connectome, and it can be applied to both volume-based and surface-based fmri data. the skpcr evaluates the simultaneous contribution of the whole-brain connectivities of each voxel to a phenotype of interest. we have designed this method to perform three steps: (1) extract important features from the data using a newly developed structured kernel principal component analysis (skpca) approach; (2) test the association between the low-dimensional features (principal components) and the phenotype of interest using an adaptive regression approach; and (3) control the voxel-wise family-wise error rate (fwer), using an efficient nonparametric permutation procedure. methodologically, we make three contributions to the science of voxel-level connectome analysis. first, we developed skpca as the first step of skpcr, which is an extension of the widely used principal component analysis (pca) [cit] and probabilistic pca methods [cit] . however, unlike the pca method, which assumes independent and identically distributed noise structure, skpca assumes a more realistic, spatially correlated noise structure among functional connectivities, leading to superior performance in dimension reduction. a nonlinear extension is also developed based on the idea of kernel principal component analysis (sch€ [cit] ) . second, we proposed a new adaptive linear regression approach as the second step of skpcr to test the association between a set of principal components and phenotypes of interest. the model can adaptively choose the optimal number of principal components, and its performance is robust, even if many noise components are wrongly included in the model. third, we developed a highly efficient permutation approach which can simultaneously estimate voxel-wise p-values and correct for multiple comparisons. other attractive features of skpcr include 1) applicability for both categorical (e.g., disease status) and continuous variables (e.g., iq, symptom scores), as well as 2) covariate effects (e.g., age, gender, motion)."
"in this paper, a nonlinear controller with observer is used for the traction control of the vehicle. the purpose of the controller is to control the vehicle to achieve a desired wheel slip ratio at different surfaces. the proposed control scheme in this paper is considered as an alternative smc-based scheme to the ones in the structure of this paper is as follows: section 2 presents some preliminaries. followed by the longitudinal dynamic model of vehicles during accelerating maneuvers in section 3. in section 4, control design is presented. finally simulation and results are resented in section 5."
"to evaluate whether skpcr is robust when the kernel was misspecified, we also used skpcr, kpcr and lskm with linear kernels for signal detection. the simulation procedures were exactly the same as those described in section 3.4, but we used the wrong kernels to detect signals."
"to get a more intuitive understanding of why skpcr had a better performance, we simulated a case-control study with some of the functional connectivities in one group having a higher mean than another group with the same strategies as the above simulation. we applied skpca with a linear kernel and pca to the simulated data and extracted the top 4 principal components. for each method, we plotted each pair of principal components in a 2d figure and used different colors to distinguish the two groups. as can be clearly observed from fig. 5, skpca (top row) has much better performance because the case and control groups are better separated than with pca dimension reduction (bottom row). fig. 6 shows the results of comparing the power of different methods when the true signal is nonlinear, i.e. a subset of functional connectivities are nonlinearly correlated with a simulated phenotype of interest in volume-based fmri data (see section 3.4). similarly, fig. 7 reports the results of the same simulation using surface-based fmri data."
"we propose a framework for structured pca (spca) in this section. it allows structured noise among features and subjects to be modelled. we first describe spca from a probabilistic perspective by modelling the noise as a multivariate gaussian distribution, and then provide an efficient algorithm for estimating the principal components."
"to test associations, we propose a novel adaptive regression approach which can estimate a single p-value per voxel to summarize the overall significance of the association. traditionally, we would manually select the top k principal components and then use a general linear model with f statistic for statistical testing. however, we found the pre-specification of k to be very difficult, and the top principal components may not always explain the phenotype of interest. therefore, we propose a new approach which is able to adaptively choose the optimal number of principal components to include in the model, one that is sufficiently robust to include noise components. the idea is similar to many other adaptive test approaches widely used in the neuroimaging [cit] ) ."
"where #fa ! ag denotes the number of times the elements in vector a is larger than a number a. after the above steps, we can get the p-values of the k scores s, denoting them as ðp 1 ;p 2 ;…;p k þ. the above steps can be computationally very efficient by using a simple matrix computation strategy. now, we define our test statistic as the smallest p-values in ðp 1 ;p 2 ;…; p k þ:"
"many methods have been developed to determine the number of principal components for conventional pca, such as the ratio estimator [cit], [cit], 2007), the distribution-based approach [cit] or just by the amount of variance explained (e.g., 90%). although these methods can be easily extended to the current skpca framework, we have found that none of them works optimally in the subsequent association tests, which is our main goal in this paper. therefore, we developed a novel adaptive regression approach in the next section, in order to address the problem of selecting the number of principal components in the association study."
"orthogonal projection of the data onto the latent space, enabling recovery of the standard pca model [cit] . from the probabilistic interpretation of pca, we can see that the noise is assumed to be independent between features (and subjects). however, the assumption may break down when analyzing voxel-based functional connectivity data because the noise terms among the connectivities are spatially correlated. in addition, pca can only perform linear dimension reduction and feature extraction, and many important non-linear factors may be missed by this method. therefore, we propose a structured kernel principal component analysis (skpca) approach in the next section, which can model the spatial structure of the noise and extract both linear and non-linear features."
"after getting the voxel-wise p-values, we can use a nonparametric permutation approach [cit] or false-discovery rate method [cit] to perform multiple comparison correction. for permutation-based approaches, we can still use the same set of permutations above to perform topological inference, including peak-level inference [cit], cluster-size inference [cit], cluster-mass inference [cit], and threshold-free cluster enhancement [cit] )."
"note also that this approach can provide an exact control of falsepositive rate. however, for the general linear model approach, the pvalue of a f-test or likelihood-ratio test may not provide a valid p-value when the number of components is comparable to the number of subjects [cit] ."
"the skpcr described in this paper is a powerful and efficient multivariate approach for voxel-level connectome-wide association studies. it can identify voxels, the overall connectivity pattern of which, as summarised by skpca as low-dimensional features, correlates with the phenotypes of interest. the idea behind skpcr simply involves reducing the dimensionality of the connectivity features and then performing association studies. however, we went further and carefully refined these two steps, aiming to extract more information from the fmri data. specifically, skpcr models the spatial noise structure in the dimension reduction step and automatically selects an optimal number of principal components in the association testing steps. in our simulation, we demonstrated that skpcr usually had the highest power in both volumebased and surface-based fmri data for detecting both linear and nonlinear signals. in real data analysis, we showed that skpcr usually had better between-sites reproducibility, larger overlap with existing findings, and faster computation speed."
"the habit of wearing animal furs is decreasing in western countries, and people are increasingly outraged and opposed to using pets in fur industry. [cit], the eu acted as a spokesperson for these feelings and issued a regulation that officially banned the use and import/export of dog and cat furs, hides or their derivatives in all member states. however, the enforcement of legislation is difficult for many reasons, and the regulation is actually disregarded in many eu countries, despite clear violations. as regards to laboratory testing, the obstacles are basically due to the use of degraded samples (furs and pelts), as well as to the difficulty of differentiating closely related species in canids and felids. mitochondrial markers were developed for amplification in degraded and poor quality dna samples, and were validated on a reference collection of tissues obtaining very solid results in terms of specificity, sensitivity, repeatability and reproducibility. moreover, sensitive and highly specific primer pairs proved effective in excluding human contamination from amplifications. the logic of our molecular steps is that a short segment in the mitochondrial nd1 gene is sequenced to provide a first identification of the species. afterwards, a set of diagnostic species-specific markers is used to confirm the results. the amplification of two markers to eventually identify the species adds confidence and reliability to species diagnosis and is highly recommended in forensics [cit] . additionally, the use of two markers enhances the chance for positive results in degraded samples: if one primer pair fails to bind for some reasons, a second chance of successful amplification with the second pair will still stand."
"dna amplifications produced positive results in 21 fur samples (table 3) . of these, some were very old, dating back to mid of last century, while others (fur-7, -12, -22, -23) were dyed, suggesting that age and chemical staining do not necessarily affect positive amplifications. despite multiple dna extractions and amplifications with all the available primer sets, no results were ever obtained from furs fur-3, -5, -15 and -25, presumably either because the dnas were too degraded to yield any pcr product or due to the absence of binding sites for primers in non-target species (e.g. l. europaeus and o. cuniculus, see table 2 ). the use of both identification (furnd1) and confirmation (lupcr, nyccr, vulcr, procr, felcytb) primer sets allowed us to assign the species to 18 fur samples. two felid furs could only be identified at the genus level, felis and lynx, respectively. less than 98% sequence homology was obtained between sequences from these fur samples and either our reference species or any sequence (both nd1 and cytb) on the online sources. the low percentage matches were best explained by the lack of our query species in the public databases, rather than by the presence of higher than expected intraspecific variation. among the non-canid and non-felid species, the use of furnd1 non-specific primer pair identified neovison vison (fur-16) and sciurus spp. (fur-9). primer pair vulcr was confirmed to anneal exclusively to the red fox v. vulpes dna . in fact, the phylogenetically close arctic fox alopex lagopus dna (fur-12, -24) did not yield any vulcr amplification product. five out of 21 samples belonged to c. lupus ssp. (fig. 1 ). among 10 furs for which the source species were declared, seven were confirmed (one fur yielded no dna). conversely, two samples, declared as wolf (fur-18) and siberian wolf (fur-17) furs were rather coyote c. latrans."
"species-specific primers were designed either to support the results from the furnd1 test, as in the case of c. lupus ssp., v. vulpes and n. procyonoides, or to definitively diagnose the species via positive amplification, as in the case of p. lotor, that did not amplify with the furnd1 primers ( table 2 ). the primer pair felcytb was used to confirm felid species via sequencing. primer sets were tested with target and non-target reference samples to verify their specificity. amplifications using lupcr, nyccr, vulcr and procr primers produced bands of the expected size in the target species, while no products were amplified in non-target species (table 2) . once entered in the blast algorithm, all sequenced amplicons matched the correct species with 99-100% homology. values of lod for specific bands ranged from 0.025 pg for nyccr primer pairs to 1.1 pg for procr. primers felcytb yielded a band of the expected size in f. catus, l. lynx and p. tigris, while no bands were observed in non-felid samples. lods for felcytb ranged from 0.108 pg in f. catus to 19.8 pg in p. tigris and 49.8 pg in l. lynx, with a binding efficiency of primers that was approximately 20-and 50-fold higher for cat dna than for tiger and lynx dna, respectively. lower affinity of felcytb primers for non felis sequences was likely due to mismatches (point mutations) occurring at the binding sites. same lods were obtained from three pcr replicates (100% repeatability) for all specific and nonspecific primer pairs. three sets of pcr replicates were carried out by different operators, and 100% reproducibility was obtained. bands from specific assays (at the determined lods) produced sequences that correctly identified the target species with 99-100% similarity when compared to the sequences in the online and in-house databases. mixed samples tested with both species-specific (lupcr, nyccr, vulcr, procr) and non speciesspecific (felcytb) primers yielded positive amplifications up to concentrations of human dna that were a hundred times higher than their lods. sequencing of pcr products from mixtures all yielded correct identifications of species."
"each pcr reaction contained 2.5 ml of 10â buffer gold (applied biosystems, foster city, ca, usa), 200 mm each dntp, 1.5 mm each primer, 2.5 mm mgcl 2, 2.5 ng bsa, 1 u of amplitaq gold polymerase (applied biosystems, foster city, ca, usa) and approximately 50 ng of dna (or 5 ml of eluted dna from questioned samples), in 25 ml total volume. thermal cycling was performed in a veriti system (applied biosystems, foster city, ca, usa), and consisted of an initial 5 min denaturation step at 95 c, 35 cycles of 30 s at 95 c, 30 s at the annealing temperature (table 1), 2 min extension step at 72 c, followed by 5 min at 72 c. conditions for amplification of questioned samples differed from those of reference samples only in the number of cycles, that were increased to 40. while furnd1 and felcytb assays require sequencing of the amplified products, species-specific singleplex pcrs (lupcr, nyccr, vulcr, procr) involve visualisation of bands under uv light following electrophoresis on 2% agarose gel and staining with gel red tm (biotium, inc., hayward, ca, usa). a 50-2,000 bp dna ladder (sigma-aldrich chemicals, milan, italy) was used to verify the molecular sizes of amplicons. for the purposes of test validation, target amplicons from specific pcrs were sequenced to verify species identity, even though no sequencing will be needed in the routine. pcr products were cleaned up with the qiaquick pcr purification kit (qiagen, hilden, germany) and sequenced bidirectionally using the amplification primers and the bigdye terminator kit v3.1 (applied biosystems, foster city, ca, usa) according to the manufacturer's protocol. unincorporated dyes and other contaminants were removed with the agencourt ò cleanseq solution (beckman coulter, beverly, ma, usa), then sequences were loaded onto a 3130 genetic analyzer (applied biosystems, foster city, ca, usa), and analysed using the sequencing analysis software v5.3.1 (applied biosystems, foster city, ca, usa). the contigexpress program in vector nti v9.1 was used to visualise, edit the resulting sequences, and to obtain contigs from forward and reverse sequences. questioned samples were processed using dedicated reagents, pipettes and thermal cycler. prior to their use, pipettes and all disposable equipment were placed under a uv hood for decontamination. amplifications of the three extracts from each fur sample were checked on the gel, and only the most intense product was purified and sequenced. two extraction controls and two pcr negative controls were included in each amplification round. no positive controls were used when amplifying questioned samples to prevent any contamination."
"in caseworks, we identified c. lupus ssp. in five samples, that were probably illicit furs, while the felid fur was identified only at the genus level felis spp., so that its (il)legality could not be ascertained. two samples that were declared as originating from wolf and siberian wolf, respectively, were actually both coyote furs. this suggests that commercial frauds are commonly committed through deliberate mislabelling and selling of aliud pro alio."
"one of the first steps to ensure that the ban is complied with and the violations are found out is the ability to discriminate dog and cat furs from products that are made with legal fur-bearing species. the eu regulation states that microscopy (morphological analysis of hairs), molecular testing (pcr-based dna analysis) and maldi-tof mass spectrometry (chemical analysis of hair keratin peptides) can be used to identify dog and cat furs. each member state is left free to adopt one of such techniques."
"a total of 25 casework samples consisting of furs, pelts and scraps ( fig. 1) were submitted to molecular analyses to test the protocol. samples were withheld at customs by the cites (convention on the international trade in endangered species of flora and fauna) personnel, or seized during police searches in stores and at hawkers. five furs were of old manufacturing, dating to mid of the last century, and six were artificially dyed. the species of origin was declared only for 10 items."
"dog and coyote are closely related canids that are both used in fur industry. in order to identify a sequence able to distinguish them, the complete mtdnas of five dogs and four coyotes available from genbank (http://www.ncbi.nlm.nih.gov/genbank/index.html) were downloaded and aligned (accession numbers in table s1 ). a primer pair (furnd1, table 1 ) was designed on conserved flanking blocks of a 187 base pair (bp) long segment in the nadh ubiquinone oxidoreductase subunit nd1 coding gene, the most informative sequence in the alignment. the segment showed 12 nucleotide sites differing between dog and coyote. in order to use the homologous nd1 regions as a diagnostic marker to identify all our reference canids and felids, and possibly the other noncanid/felid species, we verified in silico, through the alignment of online sequences (table s1 ), that interspecific variability was adequate to distinguish them unambiguously. currently, nd1 gene is commonly used as genetic marker in mammals, and more than 58,000 sequences were deposited in the genbank repository at the time of writing. finally, we tested in vitro for positive nd1 amplifications and sequencing. since the alignment for primer design was based on few sequences per species, intraspecific variability was not actually considered."
"microscopical identification of mammalian hairs requires extensive training and experience of the examiner, application of multiple analytical methods (cf. [cit] ), and a collection of reference specimens for comparison with questioned samples [cit] . maldi-tof technology has been rarely used to identify chemically processed hairs from animal fur samples, and very few studies have been published so far [cit] . the equipment is also economically demanding. both methods, however, identify hairs with a high degree of confidence only at the family level, in few cases down to the species level, but never in the case of closely related species of carnivores [cit] . conversely, dna-based technology overall has a higher diagnostic power, being at the same time a toolkit close-athand of every laboratory with basic molecular equipment."
"the present molecular procedure turned out to be a good starting point for enforcing the eu regulation against dog and cat fur trade. however, we are fully aware that both specificity and sensitivity of the method can be improved. dna-based distinction between wolf and dog, for example, can be made relatively easily at the local level [cit], but it represents a major challenge on a global geographic scale, when the source wolf population is unknown or when there are no population databases available. dog domestication dates back to 135,000-15,000 years ago [cit] : despite a wide range of estimates, this remains a short evolutionary time frame indeed, and the two taxa are phylogenetically very closely related. additionally, the wolf is legally hunted and potentially used as fur-bearing animal in some eu member states, as well as in most areas of its distribution worldwide. the same holds true for the domestic cat and its wild f. silvestris counterparts, that are hardly distinguishable both phenotypically and genetically [cit] . to face these issues at the molecular level, we are developing more sensitive and specific tests via real time pcr amplification of both mtdna markers and published nuclear single nucleotide polymorphisms [cit] . the ultimate aim is to discriminate efficiently closely related taxa in highly degraded samples, like furs and fur products, in forensic contexts where source attribution is essential to the assignment of responsibilities."
"in western countries, dogs and cats are the most popular and beloved companion pets. consequently, people find it unacceptable to farm these animals for their furs, nor do they want to inadvertently buy products containing such fur. on the contrary, dog and cat fur trade is a thriving market in many asian countries, china in particular, where the rearing of these domestic species for fur-pelt industry is practiced legally, and where laws on animal welfare are lacking. [cit], but killing cats to make furs at home is a lawful practice [cit] . [cit] . conversely, it still remains legal to import dog and cat furs into canada [cit] where, however, these products are often deliberately mislabelled in order not to hurt the sensitivity of potential buyers. following the autonomous initiative of some european countries, the eu officially banned the import to or the export from all member states of dog (canis lupus familiaris) and cat (both domestic and non-domestic felis silvestris) furs, and all products containing fur from these animals, with a regulation (n.1523) [cit] . the regulation allowed that the rules on type and severity of penalties to infringements of the provisions were laid down under each national law."
"about 55 million animals are assumed to die every year for their fur [cit] . china is the world's largest supplier and exporter of fur products (garments, clothing accessories, but also children's soft-toys), while the european union (eu) represents the major importer. nonetheless, the feeling of outrage against animal furs is currently widespread in europe, north america and asia, where anti-fur campaigns also contribute to raise public awareness through the support of many popular figures (people for the ethical treatment of animals, peta, http://www.peta.org)."
"dna from all specimens of the reference species were tested with primers furnd1. amplification reactions yielded products of the expected size in all canid and felid samples (table 2) . positive pcrs were also obtained for m. foina, l. lutra and s. vulgaris, while multiple bands of non-expected sizes amplified in o. cuniculus. no amplification occurred in p. lotor, l. europaeus and h. sapiens. amplicons were sequenced bidirectionally (raw data s1), and sequences were submitted to the genbank database for similarity search to verify their identity. dog and coyote sequences showed 100% similarity with the corresponding online entries. at the time of primer design, homologous sequences from c. aureus were not lodged in genbank and could not be compared with our results. however, our jackal sequence differed by seven and 16 diagnostic sites from those of dog and coyote, respectively. later, three complete mitochondrial sequences were published online. comparisons showed that our nd1 segment matched that of the eurasian jackal, and the diagnostic sites were confirmed (table s1 ). values of lod (limit of detection) dna in picograms (pg) are in parentheses. lods were obtained only in canid and felid species using furnd1 primer pair and in the target species using the other primer sets."
"all specimens of the reference species were used to validate both species-specific and non-specific primer pairs to ensure that cross reaction with non-target dna did not occur, and that human contamination was not detected. sensitivity and repeatability of all assays were tested (using one single individual per reference species) through three independent replicates of a 10-fold dna serial dilutions (from nearly 100 ng to 0.010 pg) to determine the lowest template limit of detection (lod) and variation in the performance of different replicates [cit] . in order to assess the reproducibility of results, the tests were repeated by three operators. for species-specific primers, amplification products of the expected size were sequenced and submitted as queries to the blast algorithm (https://blast.ncbi.nlm.nih.gov/blast.cgi) to check species identity. sequenced amplicons from questioned samples were compared with both online and in-house reference sequences. artificially mixed samples of the target species with human dna (ratios: 1:1, 1:10, 1:100) were analysed to verify the ability of primers to detect the target species at the determined values of lod (see results) without being affected by human contamination. mixtures were tested with species-specific primers using the relevant target dna. non species-specific primers were tested using separately dog and cat dnas with furnd1 pair, and cat dna with felcytb pair. amplicons from 1:100 mixtures (i.e. the ones with the highest amount of human dna) were sequenced to verify that only the target dna was amplified. to reach a conclusive identification of species, we arbitrarily used a threshold of 98% for the sequence match between questioned samples and the entries held on public databases or our in-house reference sequences [cit]"
"for over 10 years now, the eu has issued a regulation that prohibits the use and trade of dog and cat furs, but violations of law are recurrent in all member states. one of the reasons for that is the lack of an official, validated analytical protocol to exclude these species as source of furs. the regulation provides only vague indications on the methods to use, and data on their respective effectiveness in detecting dog and cat furs are completely lacking. among the methods suggested, we have explored a molecular dna approach as a best compromise between feasibility (dna laboratory with basic facilities and good laboratory practices) and diagnostic power (closely related species can be reliably distinguished). it is nonetheless true that ample room for improvement is still possible for this method, and we are working in this direction. the use of our non-specific primer pairs (furnd1 and felcytb) can also be extended to the identification of wild species that are victims of poaching and illegal harvest for trade, such as the endangered, protected and cites-listed species."
2. species-specific markers to confirm the results through an inexpensive end-point pcr or by sequencing; 3. short pcr amplicons for the analysis of degraded and poor quality dna sample;
"tissues from authenticated specimens of canidae (10 dogs canis lupus familiaris, five wolves c. lupus, two coyotes c. latrans, two jackals c. aureus, one raccoon dog nyctereutes procyonoides, 10 red foxes vulpes vulpes) and felidae (eight domestic cats felis silvestris catus, two wild cats felis silvestris silvestris, three eurasian lynxes lynx lynx and one tiger panthera tigris) were used as reference samples to develop and validate our tests. additional non-canid/felid species of mustelidae (two beech martens martes foina, one otter lutra lutra), leporidae (two rabbits oryctolagus cuniculus, two brown hares lepus europaeus), sciuridae (two red squirrels sciurus vulgaris) and procyonidae (one raccoon procyon lotor), that are commonly used in furriery, were also analysed to evaluate specificity of the tests. species identity of the reference specimens was assigned by expert veterinarians during necropsy, through morphological examination of animals accidentally died, legally hunted or poached. tissues of c. latrans and c. aureus were provided by zoologists. in no case were animals sacrificed for the purpose of the present study. all procedures for the handling of specimens and samples followed the criteria laid out by the istituto zooprofilattico sperimentale del lazio e della toscana. three individuals of homo sapiens (sampled by buccal swabs) were also included among the reference species to monitor cross-amplification."
"between 98% and 100% match with online sources was found for raccoon dog, fox, cat, lynx, tiger, beech marten, otter and red squirrel. tests for sensitivity of furnd1 primers were conducted only for canid and felid species, using one individual per species (as it was adopted in all subsequent validation tests). values of lod (table 2 ) ranged from 0.011 pg in c. latrans to 50 pg in l. lynx. results from three replicates yielded 100% repeatability of the tests. same results were obtained when analyses were conducted by three operators, thus yielding 100% reproducibility. mixed samples were artificially prepared separately with dog and cat dna at their lods (0.091 and 0.045 pg, respectively) plus human dna at increasing amounts according to the ratios 1:1, 1:10 and 1:100. they all yielded positive amplifications. sequencing of pcr products from 1:100 mixtures allowed correct species identifications in either dog/human or cat/human mixtures."
"in our workflow, amplification and sequencing with furnd1 non-specific primer pair is performed as a first test to identify the species; afterwards a confirmation test should follow. to confirm the nd1 results with such an additional test, we selected four mitochondrial segments in the highly variable cr that were specific to dog, raccoon dog, fox and raccoon, respectively. based on the online available entries, we aligned three or more sequences per species (table s1 ) and designed four primer pairs (lupcr, nyccr, vulcr, procr; table 1 ) on nucleotide sites showing interspecific variation to obtain species-specific amplification products of 149, 155, 169 and 173 bp in length, respectively. successful amplifications were verified in vitro for each target species. in felids, noncoding cr includes repeated interspersed sequences [cit] that can complicate setting up a cr-based diagnostic test for species authentication. therefore, in order to develop a confirmation test to identify cat furs after the nd1-based results, we selected a variable sequence in the cytb gene. since we could not identify binding sites on the aligned sequences that were specific to f. silvestris dna (i.e. different felis and non-felis species could amplify), alternatively we designed a nonspecific primer pair (felcytb, table 1 ) targeting a 145 bp long segment differing by nine nucleotides between domestic cat and eurasian lynx (lynx lynx was our reference felid species that was most closely related to f. silvestris). canis and homo were intentionally excluded from binding. amplicons from felcytb pcrs were directly sequenced to eventually identify the species. to obtain successful amplifications in fur samples with degraded and poor dna quality, all primers were designed to yield pcr products less than 200 bp in size. homologous sequences from h. sapiens were included in all alignments to exclude human dna as target for primer binding. [cit] . the multiple alignment program included in the package vector nti v9.1 (invitrogen, carlsbad, ca, usa) was used to align sequences under the default settings. the software primer3 (http://bioinfo.ut.ee/primer3-0.4.0/) was used for primer design. 18-24 h in 540 ml atl buffer (qiagen, hilden, germany), 60 ml proteinase k (20 mg/ml) and 40 ml 1 m dtt. the dna was eluted into 100 ml ae buffer (qiagen, hilden, germany). three independent extractions were performed for each fur, to increase the chance for at least one positive amplification. dnas from the reference and questioned samples were extracted in separated rooms and in different days to avoid cross contamination between high-dna-quantity and low-copy-number (lcn) dna samples. two mock tubes with reagents and no dna were included in each extraction session. quantification of dnas from the reference samples was obtained with the quantiflor ò dsdna system (promega, madison, wi, usa), whereas the amount of dna isolated from furs was not measured. quantification of whole dna in fur/pelt samples, containing lcn dna and, presumably, high levels of contamination from human dna, will overestimate the number of target mitochondrial genomes (cf. [cit] ), thus making quantification unreliable."
"for the same test sets, the crnn is significantly better than 1d-cnn and lstm. compared with 1d-cnn, crnn does not consume much time while obtaining higher accuracy. in addition, compared with lstm, crnn not only reduces the time-consuming amount by 10 times, but also improves the accuracy by 2%. this means that the high accuracy and"
"protein structure prediction is defined as the prediction of the tertiary structure of a protein by using its primary structure information [cit] . till now, it has become an important research topics in bioinformatics and it has important applications in medicine and other fields, such as drug design, prediction of diseases, and so on."
"as can be seen from the figure 8(a), the crnn method's testing loss curve remains at 0.08, and the diagnostic accuracy rate is 97.8%. in figure 8 (b), the testing loss curve of the 1d-cnn method also remains at 0.08, but a diagnostic accuracy of 95.2% is obtained. as displayed in figure 8( after integration of 1d-cnn and sru. therefore, crnn has better detection capabilities than single-structure methods such as 1d-cnn and lstm. comparison of accuracy and time-consuming situations is shown in table 6, from which it can be seen that crnn maintains an accuracy rate of 97.8% on the test sets after 800 iterations and takes 24m35s."
"in order to avoid close match, before the crossover operation, tsr uses the following way to select the individuals as the crossover individual couple into cross pool: it will first sort the individuals by their energy values in ascending order, and then the individuals with the furthest distance are selected for crossover operation. after the crossover offspring h n ' ' ' (,..., )   1 is obtained, its fitness value is compared with the desired level. the comparison is as follows: if the fitness value of the offspring is better than the desired value, the offspring will be set free, and accepted by next generation. if the fitness value of the offspring is worse than the desired value and is not in tabu list, the offspring will also be accepted. however, if the offspring is in the tabu list, tsr will select one parent with better fitness value to go into the next generation. in tsr, the use of tabu list keeps the diversity among individuals and avoids premature convergence."
"in this paper, the rnn block of the presented crnn is followed by a fully connected layer with a hyperbolic tangent activation function, which acts as the output layer of crnn mapping the hidden features ℎ + learned from the stack layers of cnn and rnn to the tag space ℎ + + of the sample as"
"in this paper, the proposed crnn is a combination method of 1d-cnn and sru, which inherits the advantages of two complementary methods. the method first extracts features from bogie signals through a plurality of convolution layers (having a one-dimensional small filter). then features extracted are passed to the stacked sru recurrent layers to obtain hidden features with time series correlation. the hidden features are sent to the fully connected layer to calculate the probability of signal classification. the experimental results in section 4 show that the deep learning method is more effective than the ensemble learning method for the fault diagnosis of hst bogie. more importantly, the recommended crnn method has significant performance improvements over 1d-cnn and sru. specifically, the crnn not only has a higher accuracy than the conventional model structure (i.e., 1d-cnn and lstm), but also can significantly reduce the time spent in training. this means crnn can simultaneously ensure the high efficiency and time saving of hst bogie fault diagnosis."
"for specific diagnostic objects, it is necessary to select the appropriate diagnostic method by analyzing the characteristics of the signals. for this reason, the bogie vibration signals need to be collected and analyzed before realizing fault diagnosis of the bogie. it will be at highly risk to install the bogie with faults on real operating hst. in order to simulate the characteristics of faulty bogie in actual operation, the multibody dynamic model of bogie is built by simpack software [cit] ."
"the results of the proposed crnn model with different layers are presented in table 3 . as can be seen from the results, the fifth crnn model that contains 2 1d-cnn layers and 4 sru layers achieves the best result. the first five experiments indicated that the fitting ability of the model will be gradually increased as the number of neural network layers expands. however, the experimental data contains noise. if the model overfits the data, the recognition accuracy of the model will be degraded, which confirms that the last experiment model is overfitting. therefore, the crnn structure that consists of 2 1d-cnn layers and 4 sru layers is adopted."
"(1) the convolutional layers of the presented crnn have the same properties as 1d-cnn, which can detect hidden features directly from the time series data effectively and does not depend on manual selection."
"where 휎, 휎, 휎, 푊, 푊, 푊, 푊, and 푏, 푏, 푏, 푏, respectively, represent the logistic sigmoid activation functions, weight matrices, and bias vectors of forget gates, input gates, output gates, and cell states. ⊙ means element-wise multiplication."
"gats was implemented by c++ in windows xp. the parameters in the algorithm were obtained by experiments and they were set as follows: self-adjustable population scale was set to be in the range of 100~500, the crossover rate was set to be 0.88, the mutation rate was set to be in the range of 0.012~0.025, self-adjustable tabu list length was set to be in the range of 7~14, neighbourhood set length was set to be in the range of 30~50, candidate set length was set to be in the range of 5~6. the minimal energy values ( e min gats ) obtained by gats on the three-dimensional off-lattice ab model are listed in table 1 . for comparison, we also list the minimal energy values obtained by the simulated annealing (sa) [cit], the energy landscape paving minimizer (elp) [cit], the conformational space annealing (csa) [cit], and the tabu search algorithm (ts) [cit] respectively. the corresponding bond angles and the torsional angles at the global minima are shown in table 2 . from table 1, we can find that the lowest energy value e min gats obtained by the proposed gats is smaller than those obtained by sa, elp and csa for all the four fibonacci sequences, and smaller than that obtained by ts for the sequences with lengths 13, 21, 55. although the lowest energy value obtained by gats is not as low as that obtained by ts for the sequence with length 34, it is smaller than that obtained by ts for the sequences with lengths 55, which shows that gats has better performance for long sequence."
"genetic algorithm and tabu search algorithm have their own advantages and disadvantages, thus the development of a scheme which keeps the advantages while overcomes the disadvantages of each algorithm can provide efficient search for protein structure prediction. gats, a hybrid algorithm, satisfies this requirement. for example, gats makes use of the advantages of multiple search points in ga and can overcome the poor hill-climbing capability by using the flexible memory functions of ts."
"cnn has the characteristics of sparse weights, which can detect small and meaningful features by using convolutional filters that are much smaller in size than the input. this means that cnn reduces the number of parameters that need to be stored and significantly improves the efficiency of feature extraction. the convolutional layer of cnn generally consists of two parts: (1) the first part performs the convolution operation to extract features; (2) the second part performs the pooling operation to adjust the output of convolutional layer."
"in off-lattice ab model, the shape of an n-mer is determined by the (n-2) bond angles θ 1,…,θ n-2, and the (n-3) torsional angles b 1,…,b n-3 . therefore, the prediction of 3d folding structure problem of n monomers chain is equivalent to finding the optimal (n-2) bond angles and (n-3) torsional angles which minimize the energy functional e defined in equation (1) ."
"the coordinates of the first few residues are (0,0,0), (0,1,0), and (cos(θ 1 ),sin(θ 1 ),0). latter residues' coordinates are all calculated on the base of the previous one's coordinate."
"in genetic algorithm, with the difference between individuals get smaller and smaller after several rounds of evolution, premature convergence to poor solution will generally happen. hence, the second strategy used in the proposed algorithm is to adopt variable population size. variable population size strategy adopted by genetic algorithm can prevent premature convergence by increasing or decreasing the population size when the optimal energy is very close to the average value of the population [cit] ."
"ranking selection strategy is used in the proposed algorithm to select individuals from the current population into next generation. the main benefit of ranking selection lies in that it can bring the individuals with lower energy values into the next generation, and let the best individual obtained so far not be crossed out or damaged by mutation, and thus guarantee the convergence of the algorithm [cit] .with this strategy, whenever the hypothesis population has been updated, individuals are rearranged from minimum to maximum based on the energy values e(h) (obtained by (1)), and then the individuals in the front of the population will be made to have higher probability to be selected. the adoption of this strategy can make the best individuals enter into the next generation directly and avoid being operated by crossover and mutation operators."
"a hybrid algorithm that combines genetic algorithm and tabu search algorithm is developed for 3-d protein structure prediction using off-lattice ab model. the proposed algorithm can deal with multi-extremum and multi-parameter problems. in the proposed algorithm, different strategies are adopted to make the proposed algorithm have different advantages. for examples, the variable population size strategy can keep the diversity of the population, and tsm strategy makes it possible to accept poor solution as the current solution and thus makes the algorithm have better hill-climbing capability and stronger local searching capability than many other mutation operators. in addition, tsr strategy can limit the frequency that the offsprings with the same fitness appear, and thus can also keep the diversity of the population and avoid premature convergence of the algorithm. compared with the previous algorithms, gats has stronger capability of global searching. in the future work, we will improve the algorithm and make it more effective for long protein sequence prediction using multi-core computing platforms [cit] . the pdb id is unique identifier of a protein in the database, representing its amino acid sequences author details"
"the experimental results we obtained in this work need to be further studied; i.e., it deserves applying more different methods based on deep learning to the fault diagnosis of hst. as a future direction of work, crnn and other deep learning methods can be used to solve the pattern recognition of the gradual deterioration of key components that occur during actual operation of hst. for instance, monitoring data can be utilized to estimate the degree of change in train performance and conduct safety assessments. it is even possible to detect early dangers of hst by mastering the deterioration law of fault states."
"rnn is the neural network for modelling sequential data. at the current time 푡 the network learns the lossy refinement ℎ from relevant information of the past sequences (푥, 푥 −1,..., 푥 1 ). in this way, rnn can adaptively model information captured from the past sequences. here, the rnn block of the crnn uses the sru cell, which has faster computational capability than the lstm. the structural characteristics of the sru and lstm are compared below."
"it can be seen from table 5 that crnn, 1d-cnn, and lstm all have higher precision rate than ensemble learning methods such as rf. however, crnn obtains the best performance of 98% in accuracy, and the accuracy of 1d-cnn and lstm also exceeds 95%. to further illustrate the superiority of the presented method, it needs comprehensive consideration of the time-consuming conditions and accuracy between crnn, 1d-cnn, and lstm. after each iteration of the training network, the accuracy and loss values of the test set are calculated once."
"in this work, the classification report function in sklearn library [cit] of python is used to display the main classification metrics. it can enumerate the precision rate, recall rate and f1-score of each category. for the effect evaluation of the fault diagnosis of the bogies, the classification results can be divided into true positive (tp), false positive (fp), and false negative (fn). these statistical indicators are calculated based on the real category of the sample and the category predicted by the machine. precision rate 푃 and recall rate 푅 are defined as"
"the last layer of the proposed crnn to the low-dimensional space, which is shown in figure 10 . in the low-dimensional space, the boundary between different types is very clear, and the three misclassified test samples are also distributed on the edge of the misclassified class. this verifies that crnn is very effective in feature extraction and fault recognition of hst bogies. in addition, crnn also has time-saving advantages over the other welcomed methods. therefore, crnn can meet the high-precision and low-time-consuming requirements for fault diagnosis of hst bogie."
"because of the complexity of the realistic protein structure, it is hard to determine the exact tri-dimensional structure from its sequence of amino acids [cit] . therefore, a lot of coarse structure models have been developed. the hp model is the most conventional one among them and has been widely used in protein structure prediction [cit] . different from the complex structure models, hp model only assumes two types of amino acids-hydrophobic (h) and hydrophilic (p) and the sequence of amino acids is assumed to be embedded in a lattice, which is used to discretize the space of conformations. for simplicity, the only interaction considered in hp model is the interaction between the nonadjacent but next-neighboured hydrophobic monomers [cit], which is used to force the formation of a compact hydrophobic core as observed in real proteins [cit] ."
"in crnn, recurrent layers with sru cell are used to learn the extracted features. after staking the feature maps output by convolutional layers, the output of staking layer is transmitted to sru as frames. hence, the rnn block of the presented crnn is capable of mining the context information ℎ, which is used as the inputs of the fully connected layers."
"at present, the most studied and applied models of deep learning systems are convolutional neural networks (cnns) and recurrent neural networks (rnns). the biggest advantage of cnn is feature extraction. since cnn can utilize the accumulated experience in the training process to select features, it avoids relying on artificial processes to extract features. furthermore, as the neuron weights on the same convolution kernel are identical, the network can improve training speed through parallel learning. therefore, convolutional networks have the advantages of faster training speed and higher feature extraction efficiency than traditional deep neural networks. 1d-cnn [cit], a cnn that uses one-dimensional filters to process time series, has achieved success in natural language processing tasks and voice recognition."
"bogie is one of the most important components in the structure of railway vehicles. since the structure of bogie is designed to facilitate the installation of springs and dampers, bogie exerts a good performance in vibration damping. train can be reliably supported on railway track by bogie that includes the wheelsets and suspension elements, as shown in figure 1 . to be more specific, the suspension elements of bogie consist of air springs, lateral dampers, antiyaw dampers, and so on. due to track irregularities, the train has to experience irregular vibration. the bogie can mitigate the interaction between vehicles and rails to effectively reduce vibration. however, the abnormal vibration caused by bogie failure may result in poor ride comfort and even side rollover."
"the 1d-cnn part of the presented crnn extracts the depth characteristics of the bogie signals. the stacked sru section learns the sequence information of the signal frame in each layer of forward delivery. therefore, the proposed method can quickly identify bogie sequence information to ensure the real-time and accuracy requirements of diagnosis. these advantages make the presented method more suitable for the fault diagnosis of bogie."
"the rest of this paper is organized as follows. section 2 analyzes bogie signals. the recommended crnn structure is explained in section 3. section 4 discusses experimental results, including evaluation indicators and comparisons with the other state-of-the-art methods. section 5 concludes the paper and summarizes the potential future works."
"where 휎, 휎 are the logistic sigmoid activation functions of forget gate 푓, reset gate 푟 . 푊, 푊, 푊, respectively, represent 8 complexity (3)- (8), it can be figured out that the output ℎ of lstm cell at the time step 푡 depends on the ℎ −1 from the previous step 푡 − 1. however, the main design principle of sru is that the gate calculation only depends on the current input 푥 . for input 푥 of sru cell, (9)- (11) can be calculated in parallel, so linear transformation 푥, forget gate 푓, and reset gate 푟 are amenable to parallelize calculation in the computer. moreover, as shown in (7)- (8), the element-wise multiplication is adopted to update the cell state 푐, which depends on the calculation of previous step. the matrix multiplications in sru take up less computing resources and time. due to the independent architecture of sru, sru can be trained as fast as cnn."
"to benefit from both cnn and rnn, the two approaches can be integrated into a combined network, which has several convolutional layers followed by multiple recurrent layers. ullah [cit] combined cnn and a deep bidirectional lstm network into a kind of crnn, which is adopted to process video data for action recognition. also, lopez [cit] put forward a crnn method, which incorporated twodimension convolutional neural network into lstm and was used to classify the network traffic. similar methods had been applied to the classification mask in audio signals [cit] and electrocardiography signals [cit] ."
"in the proposed algorithm, the mutation operator adopted is tabu search mutation operator. tabu search mutation operator is similar to the standard mutation operator except that tsm is a search process. with this strategy, the potential energy functional in equation (1) is used as the evaluation function to compute the offspring's energy values, and then these offspring and their energy values are combined with the tabu list to determine the output offspring. therefore, tsm can accept inferior solutions during the search process, and thus it has stronger hill-climbing capability than many other mutation operators [cit] . tsm is composed of several steps (see figure 1 ), which can be described as follows: firstly, disturbance mutation method is used to generate neighbor solutions of the current solutions. in this processing, two mutation operations are used. the first mutation operation is a two-point mutation operation and is used in the early stage; the second mutation operation is a single-point mutation which is adopted in the later stage to raise the convergence speed. disturbance mutation implementation is presented as follows. let the j th parameter selected be h j and the new parameter be h j ', then we have"
"chromosome encoding is the way the individuals are represented and is very important because it affects the performance of a genetic algorithm. in the proposed algorithm, cartesian coordinates are adopted to represent the individuals because of its simplicity. let h be an individual. for an n-residue long chain, h can be expressed as (θ 1,…,θ n-2, b 1,…,b n-3 ), which concatenates the (n-2) bond angles and the (n-3) torsional angles. cartesian coordinates of residue i in hypothesis h(θ 1,"
"for the sake of evaluating the proposed method, different methods are compared based on the same hst datasets and experimental environment. the experiments are all performed on tensorflow and a desktop machine with inter core i5-7400 processor. the advantages of the crnn structure are further explained by analyzing the recognition accuracy rate and time-consuming situation of different methods."
"in this section, we describe our experiments by using fibonacci sequences to test the efficiency of the proposed gats. a fibonacci sequence is defined recursively by"
"fft method could cause spectral aliasing, spectral leakage, and barrier effects. to overcome the deficiencies of fft method, a method combining eemd and autoregressive (ar) spectrum analysis is proposed to analyze bogie signals. the ar model is the widely used mathematical model in the time series analysis [cit], and it has the characteristics of accurate frequency location, which can reflect the peak information in the power spectrum. eemd decomposes the complex unsteady vibration signals into several single-component signals with a mean value of zero and a local symmetry with respect to the time axis. the eemd method is equivalent to the smoothing of the original signal. therefore, more effective analysis results could be achieved by applying this combined method."
"in softmax layer, the softmax function is adopted to turn ℎ + + into probabilities 푎 for each class. the process of training the neural network is the process to optimize the cost function. to minimize the value of cross-entropy cost function, the weights and biases of convolutional, recurrent, and fully connected layers are iteratively updated by backpropagation method. the computation of cross-entropy cost function in softmax layer is elaborated below."
"since the fault mechanism of bogie is complicated and the features of signal are not evident, the signal processing method cannot extract the signal features effectively and timely. hence, crnn was used as the model for the fault diagnosis of bogie. as depicted in figure 5, the framework contains five parts:"
"standard rnn architecture performs poor when using long-term information to process tasks. as the length between the relevant information increases, the ability of the rnn to concatenate related information becomes weaker. lstm is an advanced rnn architecture aiming to handle long-term dependencies. compared with standard rnn, lstm adds a new state referred to cell state, which is used to preserve long-term information. the lstm controls the cell state through the structure called \"gate\", which can store the information in the cell state. the architecture of lstm cell is displayed in figure 6 . the lstm cell outputs a hidden vector ℎ and a cell state vector 푐 at each time step. more specifically, the computation of ℎ and 푐 at the time step 푡 can be explained as follows:"
"after a structure model is adopted, an important issue in psp is to develop an optimization technology to find the best conformation of a protein sequence based on the assumed structure model. however, protein structure prediction (psp) is an np-hard problem even when the simplest models are assumed [cit] . in order to tackle this issue, many heuristic approaches have been developed. in the past decades, researchers have developed many algorithms to solve the global optimization problem in protein folding structure prediction (pfsp). genetic algorithm has been used for protein structure prediction for long time [cit] . the reason why gas are attractive is possibly due to their simplicity and efficiency in finding good solutions in large and complex search spaces [cit] . it is well known that the combination of ga with local search strategies is particularly effective in pfp [cit] ."
"genetic algorithms [cit] are adaptive heuristic search algorithms premised on the evolutionary ideas of natural selection and genetics, which select individuals by a fitness function. individuals with higher fitness values have higher opportunity to generate the successors. although genetic algorithms are widely used in optimization problems, they still need improvement for psp. ga has two main disadvantages which affect their performance for psp. one is the premature convergence and the other is the slow convergence rate. the premature convergence is mainly caused by the small variability in mutation strategy and the slow convergence rate results from heavy dependence on crossover strategy."
"to specifically analyze crnn's identification of each test sample, the true class and the predicted class assigned the highest probability are compared on the each sample. based on the comparison results for the each sample, the sklearn library gives a crnn confusion matrix as shown in figure 9 . of the 147 test samples, 144 are accurately identified. it can be seen that the network is very sensitive to the faults of hst bogie and can completely distinguish whether the bogie operation status is normal. moreover, the network also has a good ability to recognize the various faults of bogie. compared to the 144 correctly classified samples, the 3 misclassified samples are more interesting. the sample with condition label 3 is misclassified as the faults with the label 6. a sample with fault label 4 is considered as the fault with the label 1, and a sample with fault label 1 is identified as the fault with the label 4. these misclassifications correspond to the results of the analysis in section 2. the multifault signal features contain the feature components of single fault, which makes it difficult to distinguish between multiple faults and corresponding single faults. faults with condition labels of 3 and 6 all contain failure factor of lateral dampers, and those with the labels of 1 and 4 both contain failure factor of air spring, which will inevitably cause misclassifications. in order to visualize the classification result of the test set, the t-sne [cit] method is used to map the features of table 1 ."
" is used to assure large disturbance degree in the early search procedure (α tends to 0) to keep the diversity of the solutions, and small disturbance degree in the later search procedure (α tends to 1) to increase convergence rate and guarantee the algorithm to converge to a global optimum. f r ( ) and g j ( ) are defined as follows:"
"traditional pattern recognition only focuses on the classification stage. feature extraction is considered as an independent problem, which is mainly based on manual methods such as modern signal processing and expert knowledge. in contrast, feature extraction and classification are simultaneously trained in deep learning [cit] . in addition, deep learning is more suitable for big data analysis than modern signal processing methods. therefore, deep learning has become the technology of choice for fault diagnosis in recent years."
"as shown in table 5, crnn performs best on precision rate, recall rate, and f1-score compared to other methods. the experiment results show that all the neural networks based on deep learning framework have better performance in the above three statistical indicators than those methods based on the ensemble learning framework. it is worth pointing out that the neural networks with different kinds of hidden units have a stronger ability to learn nonlinear models than ensemble learning."
"is generated randomly, and equation (1) is used to obtain the energy values. after that, the individuals are sorted by the energy values from minimum to maximum and at the same time, the minimal solution and the minimal energy are saved as h min and e min respectively. during the search process, population p is handled by tsr and tsm by turns. when tsr handles the population, it will select r*n parents from the latter 90% locations to perform crossover operation with h min, and the preceding 10% locations are recognized as duplicated individuals. the offspring will be considered whether are accepted based on the current tabu list. when tsm handles the population, m*n mutation parents will be selected probabilistically, and each parent uses tsm operation to generate offspring. whenever the population p is updated, individuals will be rearranged to be from minimal to maximal by the energy values. finally, the hypothesis h min and minimum energy e min will be used as the optimal values at the end of algorithm."
"the acquisition of hst simulation data is described in section 2. in order to avoid the occasionality of the experiment, the experimental data is randomly sorted. the data is divided into 735 samples. each sample contains 2 seconds vibration signals. from the samples created, 80% are for training and 20% are adopted for testing. the recommended method is tested on test datasets."
"first, eemd is applied to deal with the vibration signals of bogie, and the 12 imf components (from imf0 to imf11) and a residual component can be sequentially obtained. the first 8 components after decomposition contain the most significant information of the original signals. accordingly, the ar spectrum analysis based on burg algorithm is performed on the first 8 imf components. the peak positions of eemd-ar power spectrum can be clearly observed in figure 4 . complexity under different operating conditions of hst bogie, the center frequency of each imf component gradually decreases. in addition, the components from imf3 to imf5 reflect the signal characteristics under different operating conditions. from the power spectrum of these components as shown in figure 4, the bogie generates resonance with the track spectrum mainly containing low-frequency orbit excitation of 5-25 hz, and the maximum amplitude appears in the corresponding frequency band. however, eemd-ar spectral method is a fault diagnosis method based on signal processing technology, and it depends on manual processes in feature selection and extraction. hence, it is difficult to automatically acquire and analyze the deep features of hst bogie based on the eemd-ar spectrum method."
"where 훽 and 푏 represent the weight and bias of max pooling; 푑표푤푛() means the max pooling function. after performing operation in 푙 layers, the output of cnn block is a tensor with deep features, which contains the most effective information in a small dimension."
"the lowest-energy ground configurations of fibonacci sequences listed in table 1 are presented in figure 3 . in figure 3, the solid dots indicate the hydrophobic monomers a while the empty dots indicate the hydrophilic monomers b. figure 3 shows that all the conformations form single compact hydrophobic cores surrounded by hydrophilic residues, which is observed in real proteins. the results verify that it is reasonable to use ab model with fibonacci sequences in three dimensions to mimic the real protein."
"another strategy used in the proposed algorithm is tabu search recombination. with this strategy, tsr records the fitness values of individuals in a tabu list and the fitness values of the offspring individual after crossover operation will be compared with some desired level and will be determined to be accepted by next generation or the tabu list. in this paper, the average fitness value of the population is considered as the desired level and the crossover strategy is random linear combination. let is the individual with minimum energy so far."
"this paper presents a joint neural network crnn that integrates 1d-cnn and sru. for recognizing the bogie vibration signals, the proposed crnn has the advantages of 1d-cnn and sru respectively:"
"where s sg means the rated power of the synchronous generator, and s mg means the rated microgrid power. recently, in microgrids, the synchronous generators have been replaced by the inverter/converter-based ress. hence, the response of system inertia (h ) and system damping (d), typically during 1-10s is significantly decreased. accordingly, the rate of change of frequency or derivative of frequency (df/dt) of the microgrids increases, leading to rapid frequency deviation, larger frequency drop/dip, system instability, and in the worst case; a rapidly cascading failure or power blackout. it is noted that during the short time interval of 1-10s, the primary and secondary control units are not effective enough to counteract the contingency (see fig. 1 ), especially under the severe situations of low system inertia and damping caused by ress penetration."
"in this paper, the novel enhanced modeling of derivative technique-based virtual inertia control has been proposed to imitate virtual damping and inertia simultaneously, improving system inertia, system damping, and frequency stability during the severe disturbances and contingencies. the derivative technique is applied to control the inverter-based ess in the microgrid for inertia emulation, by calculating the derivative of frequency. the study results are summarized as follows:"
where ace means the area control error. p w means the generated power from the wind system. p wind is the initial wind power change. p g is the generated power from the turbine system. p solar is the initial solar power change. p pv means the generated power from the solar system. p i is the commercial-industrial load power. p r is the residential load power. p vi is the virtual inertia power. note that p vi will be explained in the next section (see (15)).
"hence, the simulation results confirm that the proposed inertia control technique provides much better stability/performance in suppressing frequency deviation/transient excursion and improving system resiliency, specifically in settling/stabilizing time under the normal operating situation of the microgrid. the worst operating situation of the microgrid is when the system operates at the maximum ress power production (e.g. in this case, 90% of its installed capacities, see fig. 9a ) resulting in the extremely low system inertia (e.g. in this case, 80% reduction from its nominal value). to force more severe operating situation, the microgrid has the minimum load consumption (20% of its peak demand, see fig. 9b ) resulting in the extremely low system damping (e.g. in this case, 80% reduction from its nominal value). for this scenario, the related parameters have been listed in table 1 . this severe scenario is executed over a total time of 15 minutes. the necessity of applying the proposed inertia control for compensating the impacts of low system damping and inertia and achieving the stable frequency performance has been demonstrated in this scenario. fig. 8 reveals the system frequency stability under the critical scenario. with increasing penetration of ress in this scenario (specifically after 200 s), it is obvious that the maximum frequency deviation of the microgrid significantly increases. in case of no inertia control, the microgrid completely fails to regulate the frequency stability, leading to the power blackout. in case of conventional inertia control, strong frequency peak/transient and larger frequency deviation can be observed during the connection of the wind farms volume 7, 2019 figure 8. frequency response of the microgrid under the extreme operating situation of low system damping and inertia. from 200 s (i.e., the period of maximum ress power penetration). during no contingency event (from 201 to 699 s), the microgrid frequency in case of conventional inertia control severely oscillates over the frequency operating standard of ±1 hz [cit] twice due to the lack of system damping. this situation may lead to the instability and system collapse. to prevent such severe situation, the disconnection of solar power plants at 700 s is required in case of conventional inertia control. on the contrary, it is obvious that the proposed virtual inertia control gives much smaller frequency transient and derives frequency deviation close to zero with smooth changes for the whole operating situation. fig. 10 demonstrates the emulated virtual inertia power from the ess for scenario 4. clearly, the ess controlled by the proposed control technique is effectively charged/ discharged in response to the severe disturbances and contingencies."
"the remainder of this manuscript is arranged as; section ii explains a brief review of frequency regulation in regards to inertia control. the dynamic of the microgrid based on frequency regulation is discussed. section iii proposes the new modeling of derivative technique-based virtual inertia control. in section iv, the effects of virtual damping and virtual inertia are investigated. the microgrid frequency response is examined through the severe situations in normal and extremely low system inertia and damping. finally, the conclusions are offered in section v."
"1) eigenvalue investigation has been presented to evaluate the dynamic impacts of virtual damping and virtual inertia over the system performance. it is shown that the side effect of applying the high value of virtual inertia constant is that the system will require a longer time to settle. on the contrary, the side effect of applying the high value for virtual damping is that it can cause a large second overshoot, leading to the degraded system performance. thus, in applying virtual inertia and virtual damping simultaneously, it is better to keep a smaller value of virtual damping and give the main responsibility of suppressing the frequency deviation in case of the contingency to the virtual inertia part. 2) time-domain analysis and results report the higher stability/performance of the proposed control technique under different test scenarios including the severe disturbances and high uncertainty situations. obviously, in the severe situation of high ress penetration with low system damping and inertia, the cases of conventional virtual inertia control and no inertia control fail to stabilize frequency stability within the acceptable standard owing to the lack of virtual damping emulation, causing system instability and collapse. to handle this problem, the disconnection of ress unit is required in case of conventional inertia control to avoid the system collapse. on the other hand, the proposed control technique could effectively maintain the frequency within the satisfactory range under the applied disturbances. these results clearly validate the efficiency, resiliency, and robustness of the proposed control technique."
"the pertinent control parameters of the studied microgrid used in each scenario are listed in table 1 . he is currently pursuing the ph.d. degree in electrical and electronic engineering with the mitani-watanabe laboratory, kyushu institute of technology, fukuoka, japan. his research interests include power system stability, smart grid and clean energy, optimization in power systems, and application of synchrophasor in power systems. he is currently a professor and the director of the institute of electrical power engineering and energy systems, clausthal university of technology, clausthal-zellerfeld, germany. he has authored over 50 journal/conference papers. his research interests include power mechatronics, electrical energy conditioning, energy management systems, virtual synchronous machine, gas network simulation, and energy system technology. volume 7, 2019"
"to obtain the multi-source nature of microgrid, the studied microgrid in fig. 2a contains several kinds of generation system, with different types of load. the main thermal power station has 15 mw of installed capacity, representing a traditional synchronous generator-based generation. the generated electricity is consumed by 10 mw of commercialindustrial loads and 5 mw of residential loads. the system has 8.5 mw installed capacity of the wind farms and 7.5 mw installed capacity of the solar power plants. none of them equipped with inertia or damping emulation controller. hence, the renewable power generations will significantly change the microgrid operating point and reducing system inertia and damping property, affecting system stability and performance. to solve this problem, energy storage systems (ess) of 4.5 mw are installed in the microgrid. the system base is 15 mw. the communication network (dotted line) is applied for exchanging information and status. the frequency control network (dash line) is used for control instruction of inertia, primary, and secondary control units. the power network (solid line) is applied for delivering the electrical power to the loads in the microgrid."
"where f means the frequency of the system, f means the deviation of system frequency, p m means the generated power change from the synchronous generator, p l means the load power change, h means the system inertia, d means the system damping."
"one of the major problems is the lack of system inertia and damping owing to the replacement of traditional generations (i.e., synchronous generators) by ress-based generation. the main reason for system inertia reduction is because of the converter/inverter that is usually used to connect the ress to the microgrids. the inverter/converter does not possess any inertia or damping properties, leading to the degradation of system inertia and damping, larger frequency excursion, and system instability and collapse, see for an example [cit] . on the contrary, high system inertia and damping generated by the synchronous generators are required to attenuate the system dynamics and decrease frequency deviation of the system, so that the system instability, undesirable load or generationshedding, cascading outages, and power blackouts could be prevented. thus, dynamic stability in the presence of low system inertia and damping caused by ress penetration is becoming the main concern for today and future microgrid design, operation, and control, as its stability margin is reduced."
"in this part, the time-domain simulation and eigenvalue analysis of the studied system are presented to evaluate the dynamic impacts of virtual damping (d vi ) and virtual inertia (j vi ) based on the small-signal stability point of view. to illustrate the microgrid behaviour over a wide range of parameter changes, the complete eigenvalue trajectory is presented. this section is divided into two sub-scenarios as follows: (1) the impact of virtual inertia; (2) the impact of virtual damping and virtual inertia."
"3) the stability study confirms that the coordinated design of virtual inertia and virtual damping is suitable and effective to gain a new trade-off between frequency transient/deviation and settling time of the microgrid. it does not only reduce transient excursion/frequency deviation but also improves the damping performance and withstands the system disturbances without any harmful effect. volume 7, 2019 finally, the simultaneous emulation of virtual damping and virtual inertia in the derivative technique-based virtual inertia control for microgrid frequency regulation can be considered as an important outcome of this work. eventually, the proposed model of virtual inertia control will be useful and essential for further analysis and studies in frequency control respect to virtual inertia and damping emulation capabilities. besides, it could be implemented to any category of microgrids with different size/complexity."
"scenario 3: this scenario represents a normal operating situation of the microgrid with low penetration of wind and solar generations (see fig. 6a ) and high power consumption of residential and commercial-industrial loads (see fig. 6b ). both system inertia and damping are reduced 20% from their nominal values (see table 1 ). to replicate the typical microgrid operation, the power fluctuations due to wind velocity, solar irradiance, and electrical load consumption are simultaneously applied to the microgrid. how the proposed inertia control regulates the microgrid frequency stability under the impact of high load power consumption with low ress production is also examined in this scenario. for this scenario, the control parameters have been listed in table 1 . fig. 5 shows the microgrid frequency response under this scenario. obviously, due to the connection of wind power plants at 200 s, it results in the huge frequency rise of +0.37 hz in case of no virtual inertia control, +0.26 hz in case of conventional inertia control, and +0.22 hz (the lowest) in case of the proposed inertia control. moreover, the conventional virtual inertia control yields longer settling time, which is almost the same as the case of no virtual inertia control due to the lack of virtual damping emulation. on the contrary, when the proposed inertia control is applied, the frequency rise has the lowest value with less settling/stabilizing time. this improvement is due to the capability of the proposed virtual inertia control to emulate virtual inertia and damping, simultaneously. during no contingency event, for example from 460 to 490 s, the proposed control technique could effectively regulate frequency deviation with the lowest value. this indicates the efficiency in maintaining stable microgrid operation. following the disconnection of solar power plants at 700 s, it causes the frequency drop of −0.31 hz in case of no inertia control, −0.21 hz in case of conventional inertia control, and −0.17 hz (the lowest) in case of the proposed inertia control. similarly, both conventional inertia control and no inertia control give longer settling/stabilizing time due to the lack of damping emulation. by designing the virtual damping as in the proposed control technique, the stabilizing/settling time is significantly reduced, effectively enhancing frequency stability margin and resiliency of the microgrid. fig. 7 illustrates the emulated virtual inertia power from the ess for scenario 3. the positive value indicates the charging power while the negative value indicates the discharge power. it is obvious that the ess controlled by the proposed technique is greatly charged/discharged in response to the applied disturbances and contingencies."
"this section explains a novel concept, design, and modeling of derivative technique-based virtual inertia control for simultaneous emulation of virtual inertia and virtual damping, enhancing frequency performance and avoiding system instability/collapse owing to the lack of system damping and inertia. the main design objective is to simultaneously provide the virtual damping and virtual inertia to support the conventional synchronous generators in the situations of low system inertia and damping when the ress are highly penetrated to the microgrid. this concept can be established by the short-term ess, inverter, and proper inertia control technique. the proposed control technique is designed to operate independently of other control units, such as primary and secondary control. thus, the energy contained in ess is fully used to improve the system frequency stability in terms of steady-state and transient performance."
"considering the dynamic effects of generations and loads including inertia, primary, and secondary control (from fig. 2b), the system frequency deviation is obtained as:"
"recently, the transition in electricity from centralized generation to distributed/decentralized generation (dg) has made microgrids attractive and suitable for integrating renewable energy sources (ress). the microgrid infrastructure has proven to be an alternative strategy for solving the challenges of the energy crisis and environmental concerns, as it consists of dg/ress, energy storage systems (ess), and distributed loads [cit] . with the rising share of ress-based generation, it raises the new stability issues in regulating the microgrid."
"accordingly, applying the proposed virtual inertia control not only reduces transient excursion but also shortens the stabilizing/settling time and derives the frequency deviation close to zero, improving the stability and robustness of microgrid operation and control."
"is a vector of rbf basis functions with the form of gaussian function (11), and µ l, ϑ l are the center and width of the receptive field, respectively."
". the same adaptive technique and nn are designed to compensate for the unknown disturbance bound and unmodeled dynamics, respectively, and an adaptive control term is used to handle approximation errors. these compensation controllers can be written as"
"the work is organized as follows. problem formulation and preliminaries are introduced in section 2. section 3 is devoted to designing an adaptive course control strategy for usv. in section 4, a stability analysis is presented showing that the ultimate boundedness of all signals can be ensured. section 5 simulates the proposed control approach. finally, a brief conclusion is given in section 6."
"in general, the usv course control system can provide proper control under the condition of course tracking. fig. 1 demonstrates schematic of the usv course control system. first, when the actual course ψ deviates from the desired course ψ d, the rudder angle δ c is calculated by the course autopilot. then the rudder servo receives command to actuate the rudder, thus the course of usv is corrected."
", in order to reduce the influence of disturbance, an adaptive compensation controller is designed to handle the unknown disturbance bound, which is written as"
"in the recent years, there has been a growing interest in the development of unmanned surface vehicle (usv) due to its advantages of being fast, small volume and low cost. successful applications can be found in diverse areas, which include the use of oil exploration, oceanographic mapping and ocean surveillance [cit] . course control not only is the basis for these applications but also related to the development of usv motion control. therefore, the study of course control has attracted increasing attentions."
"from the characteristics of the model, it is mainly divided into linear models and nonlinear models. linear models adopt classical nomoto model describing usv motion characteristics. however, the linear model of usv may be inadequate"
"tlc technology is an effective method to solve nonlinear tracking and disturbance attenuation problems, which combines dynamic inversion and a linear time-varying (ltv) regulator in a novel way. the working principle of tlc is to transform the nonlinear tracking problem into an ltv stability problem, and thereby achieving robust stability and performance along the nominal trajectory. based on this advantage, tlc technology has been successfully applied in many fields, such as missiles, launch vehicle flight and helicopter [cit] . however, using this method to handle the course control problem is studied rarely in usv field. (2) a close-loop ltv feedback controllerũ is used to stabilize the system, which also makes the system have a certain response characteristics."
"this note has proposed an adaptive course control strategy for usv under the model uncertainties, external disturbances and rudder saturation. meanwhile, the rudder servo characteristic is considered in course control, which can better reflect the engineering reality. according to the enhanced tlc, nns, adaptive technique and auxiliary dynamic system, two loop course controllers are designed, and the enhanced tlc requires only two adjustment parameters, which is simple to compute and easy to implement. the neural network of the low frequency, adaptive technique and auxiliary design system are used to compensate for unmodeled dynamics, external disturbances and rudder saturation, respectively. the simulation results and the simulation comparisons demonstrate that the proposed scheme has robustness against unknown dynamics and time-varying disturbances. furthermore, the proposed scheme can also be used to solve the problems of path following and trajectory tracking."
"guofeng wang received the b.e., m.e., and ph.d. degrees from dalian maritime university, dalian, china, where he is currently a professor with the school of marine electrical engineering. he is engaged in the technical research of marine automation system and automation equipment, and has presided over and participated in a number of national and ministerial projects. his research interests include ship automation, advanced ship borne detection device, and advanced power transmission."
"in this paper, rbfnn w t s (x ) is adopted to approximate the uncertain system dynamics f (x) defined on a compact set x, which can be expressed as"
"in order to address the system uncertainties and rudder saturation, a robust distributed course control scheme is developed for usv in fig. 3, which consists of the steering system loop and rudder servo system loop. the usv steering system loop controller adjusts the actual rudder angle to follow the reference course by model reference technique. the rudder servo system loop controller regulates rudder rate to track the virtual command produced by the pseudo differentiator. for the solution of the unknown time-varying disturbances, rudder saturation and model uncertainties, similar structure is applied in each loop. both rudder saturation and unknown time-varying disturbances can be compensated by constructing auxiliary dynamic system and adaptive technique, respectively, and unmodeled dynamics are approximated and canceled out by employing nns. to further improve the applicability of the scheme, a low frequency learning method is used to handle more uncertainties caused by real actuator steerable variables."
"assumption 2: the external disturbances are bounded with the control objective is to develop a practical and concise course control law for usv with unknown system uncertainties, unmodeled dynamics and rudder saturation so that the actual course ψ tracks the desired course ψ d, and the course control law can ensure the boundedness of all signals in the whole system."
"(4) the uncertain system dynamics and the unknown bounded disturbance are estimated online using neural network and the adaptive technique, respectively. in addition, a low frequency learning method is introduced to improve the applicability of the strategy."
", in which θ 1, θ 2 are unmodeled degree coefficients. in practice, taking into account the physical characteristics of the rudder, the rudder saturation is written as"
"for the further quantitative comparison, the mean error (me) value and the mean integral absolute (mia) value are used to evaluate the proposed scheme, which are expressed as"
"(2) taking full account of the actual rudder, a secondorder underdamped model is used to replace the traditional first-order inertial model in this paper, which can better describe the characteristics of the actual rudder servo system."
"motivated by the existing results, considering the rudder servo system, a robust course control strategy is proposed to address the model uncertainties, unknown bounded disturbances and input saturation using trajectory linearization control (tlc), nns and adaptive technique. it is proved the proposed strategy makes usv follow and keep desired course with arbitrarily small error. the contributions of this study lie in the following aspects:"
"remark 3: the backstepping adaptive and pid are excellent control methods in the field of ship motion control, which are also the most commonly and widely used control algorithm in course control. therefore, it is very convincing to compare with the proposed strategy."
"where e 1 (ι) denotes the track error. the desired course is 60 degree, and the simulation results are depicted in figs. 4-11 . the me and mia indexes of the tracking error are reported in table 2 . from fig. 4, we can see that the proposed scheme has a faster convergence speed than the other two methods, and the course can also remain stable near the target value without overshoot. it implies that the proposed method has better performance in control quality such as keeping precision and robustness, especially in the presence of larger disturbances. control efforts of the controller are shown in fig. 5 . from which we can see that the input saturation problem is effectively compensated by auxiliary system. fig. 6 shows the change of rudder angles. due to considering the rudder servo characteristics, we can see that the variation of rudder angles are consistent with that of actual rudder angle. that is very meaningful and critical for the algorithm to be applied in practice. the comparisons on the performances of tracking errors are given in fig. 7 . it can be easily observed from figs. 7 that tracking error of the proposed scheme converge to a small neighborhood of the origin. however, the tracking errors of the other two methods are still fluctuating. figs. 8 and 9 give the comparison of approximation errors using the proposed scheme and the proposed scheme without low frequency (lf). from figs. 8 and 9, it is clearly noticed that the proposed scheme shows obviously better approximation performance than the proposed scheme without lf, and it also illustrates that lf learning method reduces the high frequency signal caused by external disturbance to a certain extent. figs. 10 and 11 depict that the unknown disturbance bounds can be estimated precisely by the designed adaptive laws with a small bounded region. moreover, the me and mia index of three control methods are given in table 2, it can be apparently noted that the proposed scheme is (0.028, 0.029), and it is only (1.48%, 1.26%) of backstepping adaptive control method and (1.45%, 1.18%) of pid. it is obviously observed that the proposed scheme performs better in control error accuracy than two other approaches. based on the above comparisons, the proposed scheme has better control performance, particularly in the convergence speed, keeping performance and control error accuracy."
"(3) an auxiliary dynamic system is introduced to mitigate the effect of rudder saturation constraint, where the smooth switching function is incorporated into auxiliary system to avoid the singularity induced by the system state approach to zero."
"according to the design principle of tlc, we definex 1, x 3 andȳ 1 as nominal state, nominal input and nominal output, the system (13) can be written as (without system uncertainties)"
"(1) a robust adaptive double loop course controller using the enhanced tlc technology is proposed for usv, in which tlc is studied rarely in the field of usv motion control. compared with the traditional tlc, only two parameters need to be tuned in enhanced tlc, which makes it extremely simple and practical to implement in real practice."
"where ς and ω δ are the positive constants, ψ r is the actual course, and ψ d is the output course of the reference model. as a second-order model, it can achieve the derivatives ofψ d andψ d and provide filtering. auxiliary dynamic system [cit] : to handle the effect of rudder saturation, an auxiliary dynamic system is constructed aṡ"
"the meandering nanochannel map technique allows for the characterization of large fragments of chromosomal dna from individual cells for characterization of disease associated rearrangements of the genome. they will also open up for identification of single microorganisms, which will be important for rapid pathogen diagnosis and metagenomic studies without the need for culturing the cells or dna amplification. here, a range of different labeling schemes can be used for mapping relevant information along the dna, including melt mapping as we show here, 17 competitive binding, 26 nick translation, 9, 12 fluorocode, 27 and labels for epigenetic patterns. 28 we have shown the visualization of whole microbial chromosomes, which will allow analysis of whole microbial genomes without the need to assemble smaller fragments. the fabrication of longer meander structures will open up the intriguing prospect of extending the approach to human-scale chromosomes and assembly-free genome analysis."
"prior to each new experiment, a buffer consisting of 5 mm nacl in 0.025x tbe, 3% (v/v) 2-mercaptoethanol and 50% formamide was flushed through the channels for at least 15 min to ensure correct buffer conditions before introducing the dna. after introduction of dna, the flow rate was kept as low as possible at all times to minimize shearing of the molecules. once the dna molecules reached the inlets to the nano-channels pressure was applied to both the reservoirs connected to the micro-channel containing the dna. in this way, the buffer was forced to flow through the nano-channels dragging the dna with it ( figure 1 ). after introduction into the nano-channels, the molecules were either imaged immediately to collect information about the relaxation behavior or heated for 10 min to create a barcode pattern before imaging."
"in this section, we investigate the performance of our proposed wideband spectrum sensing algorithm via numerical simulations. we first introduce the simulation setup and relevant performance evaluation indicators. then, we demonstrate and analyze the simulation results."
"where () h is hermitian transpose. since the noises of each channel is not correlated with other channels and the noises are not correlated with the pus signal, the correlation matrix can be denoted as"
"for business applications, the ability to connect and uplink a file may be of higher importance than the downlink capability. therefore, for this case application, the qec vs. qec comparison matrix is configured to reflect this assumption, as seen in table 8 . the qec vs. qec comparison matrix, normalized matrix and weight vector are presented in table 8 and the results presented in table 9 . as seen, when uplink is equally to moderately preferred to connectivity; connectivity moderately preferred to downlink; and uplink strongly preferred to downlink, the results of the perceived qos change to identify the umts network as the best performer, followed by cdma, and gsm. in similar fashion, the qec vs. qec parameters can be adjusted for applications in the personal class of mobile applications to appropriately evaluate qos."
"since the pus signals cannot be accurately estimated, there are some energy of pus signals in the residual. therefore, we set β 2 to decrease the false alarms. β 2 is a small value and the optimal value can be obtained through numerical simulation."
"analyzing genomic information by imaging long linear single dna molecules was first introduced by the groups of bensimon 1 (dynamic molecular combing) and schwartz 2 (optical mapping) by stretching the dna on surfaces. notably, optical mapping has been employed at the finishing stage of genome sequencing projects."
"the equivalent baseband signals of pus. therefore, the proposed algorithm can directly input the sample sequences, without taking the fft process, which can decrease the computation complexity."
"in this section, we first introduce the signal model of the wideband spectrum sensing. then, we introduce the multicoset sampling and analyze it in the frequency domain. the multi-coset sampling is a sub-nyquist sampling front end of our proposed wideband spectrum sensing scheme."
"while these methods stretch dna on a surface, subsequent fluidic approaches have analyzed dna in extensional flows. [cit] a recent variant of flow stretching is implemented by attaching a large bead at the end of the dna. 7 as the dna is pulled into a small channel, the bead blocks further motion of the dna and ensures that the dna can be fully stretched and kept in one place for monitoring of dynamic processes. dna molecules can also be tethered in both or a single end by chemical bonds to create dna curtains as pioneered by greene. 8 the technique is combined with a lipid bilayer for passivation to enable studies of protein-dna interactions without non-specific binding of the proteins."
"in table 1, we list the runtime of the simulations of figure 9, which can reflect the computation complexity of these three algorithms. as we can see in table 1, the runtime of omp and music is almost five times and six times that of the proposed scheme respectively. since the proposed scheme does not require to interpolate the sample sequences to the nyquist rate, the amount of data is 1/l times that of the conventional methods. therefore, the proposed scheme can greatly decrease the computation complexity and storage resource."
"the output from the theory is a base pair resolution probability map p ds (s). to handle the different flexibilities for single-stranded and double-stranded regions, we used the approach from ref. 17 together with a gaussian convolution with a width equal to the optical point spread function 200 nm to yield smoothed theoretical prediction. when comparing experiments and theory, both experiments and theory are scaled to be on average zero and to have variance one. 17, 22 experimental time averages were then compared to theory using subsequence dynamic time warp 23 with horizontal and vertical \"stretching\" costs set to 10 and the diagonal cost set to 1, thus heavily biasing against stretching, yet allowing for, for instance, channel defects causing a varying nanochannel width. the required stretching, determined by this process, was then applied to the full kymographs for visualization (see fig. 6 )."
"we have shown that meandering nano-channels substantially ease direct imaging of dna molecules in the mbp range, by demonstrating a 5.7 mbp dna molecule at 50% extension within a single field of view. in the current design, one of the meandering parts is almost long enough (1 mm) to hold the entire genome from saccharomyces cerevisiae (12.1 mbp). with an optimized denser design, we expect to be able to hold at least 20 mm of dna per field of view (at 50% stretching corresponding to 120 mbp; this length would be enough to contain the whole of most of the human chromosomes, while the longer chromosomes, could be covered by spooling the molecule through and taking a second image in a dumbbell arrangement of the dna, which may also be used for achieving an improved degree of stretching of the dna). this would allow us to image the entire diploid human genome (6.4 gbp) with a mere 50 frames. together with our algorithms for extracting and aligning barcodes from our meandering nano-channel, the novel channel design is a first step towards an automated tool for whole chromosome analysis."
"a setup similar to the one described in ref. 17 was used to mount the chip to the microscope, heating of the dna sample, and control of the flow within the micro-and nanochannels. the setup included a fused-silica chip containing the channel system (made as explained below), a chip holder (also referred to as a chuck), a heater system, and a homebuilt pressure-control system for flow manipulation."
"in the related works [cit], the sample sequences of the multi-coset sampling are up-sampled before processing, which results the quantity of the data is p times the quantity of the nyquist samples. for example, if the multicoset sampling has 20 cosets, the quantity of the up-sampled data is 20 times as much as that of nyquist rate, which is a significant consumption of computing and storage resources. for wideband spectrum sensing, the signal reconstruction is not required, so we can directly process the sub-nyquist sample sequences to estimate the active channel indexes based on (5) ."
"direct imaging of dna molecules in the mbp range was enabled by folding nano-channels into a long meandering pattern. each meandering channel could be imaged within a single field of view and was designed to hold up to 10 mbp of dna stretched to 30% extension (1 mm) (figures 1 and 4) . to simplify the analysis, the meandering part of the nano-channel was designed with 50 lm long straight parts parallel to each other and connected with soft bends. the bends were in turn designed with a 1 lm long straight part connected to two 90 turns to minimize stress on the dna molecules in the meander-channels."
"from the reviewed literature, it is evident that there is room for the development of a holistic, user-centric, and application-specific evaluation methodology that provides objective measurements of qos relative to all other networks available in the same region. this will allow network providers to better assess how well their services are being perceived relative to their competitors."
"the discrete fourier transform (dft) of the sampling sequence y i [n] links the known spectrum y i (e j2πflt s ) and the unknown spectrum x (f ),"
"assume that there are k active pus in the sensing frequency range, and ignore the false alarms. therefore, the proposed scheme would iterate k times before the detection is completed. formula (20) can be solved by least square method, which is denoted aŝ"
"the heating system consists of a temperature controller (336 cryogenic temperature controller, lake shore, usa), an aluminum cap, a cartridge heater, and a thermocouple. both the cartridge heater and thermocouple were positioned inside the aluminum cap which in turn was placed in contact with the backside of the chip through a hole spanning the center of the chuck. as a result, a reproducible thermal contact between the heater and the chip was achieved and a feedback loop created to ensure a stable temperature. to further increase the heat transfer thermally conductive grease (omegatherm, omega, usa) was applied at all interfaces in the heating system."
"as a proof of principle, an entire chromosome (5.7 mbp) from s. pombe was imaged in a 250 â 250 nm 2 meander channel (figures 3(a) and 3(b) ). figure 3(a) shows the dna molecule directly after introduction into the meander-channel. due to the shear experienced during introduction into the nano-channels, the molecule was highly extended. when the flow was stopped, the molecule started to relax towards its equilibrium state and after 10 min the molecule had relaxed considerably (3(b)). however, due to the slow relaxation for long molecules 25 had not reached its equilibrium state and was extended to 50%, compared to the expected 30% extension for a relaxed molecule."
"in order to sense multiple channels simultaneously, wideband spectrum sensing (wss) becomes a highly desirable feature in cr system. however, compact sus are energy limited, while wideband spectrum sensing limited by the nyquist sampling theorem requires very high sampling rate, which is of high energy consumption and computational complexity. therefore, efficient wideband spectrum sensing becomes one of the main challenges in cognitive radio systems. common wideband spectrum sensing methods are generally classified into two categories. one category is sequential detection [cit] . this method adopts a narrow band filter with adjustable center frequency, and uses the existing narrowband spectrum sensing technology to detect channels one by one. since only one channel is detected in each sensing, the full band sensing takes long time. thus, sequential detection causes large sensing delay. another category is parallel detection [cit] . this method uses filter bank to partition the whole frequency band into multiple sub-bands, and all sub-bands are simultaneously detected in parallel. compared with sequential detection, the sensing delay is greatly reduced. however, parallel detection requires a great number of radio frequency (rf) frontend, which is costly for compact sus."
"there are several important contributions from this research. first, the approach is simple and readily available for implementation using a simple spreadsheet. this can promote usage in practical scenarios, where highly complex methodologies for qos evaluation are impractical. second, the approach fuses unlimited qos quality evaluation criteria to provide a holistic view of the experienced qos. this allows the approach to be easily extended to include additional data (or voice) quality criteria not considered in this research. third, by using data from drivetesting, the approach provides objective measures of qos, which improves previous approaches using subjective data from surveys. finally, the approach provides a mechanism to evaluate qos based on application-specific scenarios. by modifying the parameters of the qec vs. qec comparison matrix, qos can be evaluated by taking consideration of prioritized services that are necessary for different classes of applications. overall, the approach presented in this research proved to be a feasible technique for efficiently evaluating the perceived qos in wireless cellular networks."
"in this paper, we consider the wideband spectrum sensing with sub-nyquist samples. when the snr is low, the conventional subspace decomposition based algorithms cannot distinguish the signal subspace and noise subspace, so these algorithms will loss efficacy. in order to perform wideband spectrum sensing at low snr, we propose a two thresholds iterative detection scheme, which can estimate the active channels support accurately at lower snr. furthermore, we analyze the effects of noises on the compressed sensing system, and the analytical method is suitable for all compressed sensing system, not only multi-coset sampling. based on the analysis, we introduce the theoretical performance of the proposed scheme. finally, we compare the performance of the proposed scheme with two subspace decomposition based algorithms, omp and music. the simulation results have shown that the proposed scheme has better noise robustness and low computation complexity. in addition, the proposed scheme needs less sampling resource at low snr."
"reservoirs in the chuck connected to the inlet holes on the chip enable easy loading and manipulation of the sample during experiments. for managing the flow inside the chip, nitrogen gas was used to create an overpressure at the appropriate reservoir. nitrogen gas was used to keep oxygen concentration at a minimum and thereby minimizing the photo damaging of the molecules."
"combining melting mapping with meandering nano-channels has led to direct visualization of a barcode pattern along dna molecules in the mbp range. as a proof of principle we show a melting mapping pattern along the 4.6 mbp s. pombe chromosome (figure 4 ). our meanderfinding algorithm correctly identified the meander in figure 4 (top, left) as is visualized in figure 4 (top, middle) . the time-trace in figure 4 middle was extracted using our tool by \"walking along\" the meander contour and successfully extracting the barcode pattern along the dna confined in the nano channel (figure 4, bottom) ."
"in this part, the computational complexity of the proposed wideband spectrum sensing scheme is analyzed and compared with the subspace decomposition based scheme. our proposed scheme directly use the sampling sequences to detect the active channels. the quantity of input date (i.e., the number of samples) is pn . in the calculation process, the main computational complexity is concentrated in the formula (20), (21) and (23) ."
"despite the use of thermally conductive grease, the measured temperature varied slightly from the actual temperature in the channels. also the temperature measured for dna denaturation to be initiated may potentially vary from day to day due to variations in buffer conditions and quality of formamide. during the melting experiment, the temperature was therefore slowly and step-wise increased from 29 c up to 45 c. several different molecules were studied at each temperature to find the optimal melting conditions in each case. to ensure a stable temperature during data collection, the system was equilibrated for 15 min in darkness at each new temperature before data collection was initiated."
"elongation due to confinement in nanochannels has been used to address questions in polymer physics and is advantageous for single molecule genomics. it can be performed by confining the dna in one dimension by using a nanoslit 9, 10 or in two dimensions by using a nanoscale channel. 11, 12 it does not require external forces to perform the stretching and after a short relaxation time a steady state is reached, at which the dna molecule remains elongated at equilibrium for an extended period. 13 in addition, the extension of the dna molecule scales linearly with the contour length of the dna molecule and thereby provides a correlation between spatial positions along the polymer and within the genome. this has led to launch of single molecule mapping in nanochannels commercially, although the current platform available is restricted to analyzing genomic sub-fragments in parallel rather than complete lengths of dna from whole chromosomes. similarly, even though modern sequencing methods have begun to use single molecule analysis, the kilobase length scales that are now amenable 14, 15 are a sub-fraction of the lengths of chromosomes (mbps). the problem with non-complete lengths is that de novo assembly of sub-sequences up to human chromosome length scales has been to-date computationally intractable and is confounded in diploid genomes by the sub-sequences originating from a mixture of homologous chromosomes, which means that haplotypes cannot be easily resolved."
"for personal applications, the ability to download information, such as web pages, music, and pictures are commonly expected among users of the personal class of applications. users in this class tend to accept the fact that minimal network flaws related to establishing connections is an inherently and unavoidable part of network services. therefore, users of the personal class of applications may put up with not being able to connect to the network at times; however, when a connection is made, they may mandate the ability to download data contents using the network. this can be reflected by configuring the qec vs qec matrix to reflect downlink having the higher priority, followed by connectivity and uplink. the qec vs. qec comparison matrix, normalized matrix and weight vector are presented in table 10 and the results presented in table 11 ."
"the fabrication of the nanofluidic chip comprises the following processing steps, based on photolithography (pl), electron-beam lithography (ebl), reactive ion etching (rie), sandblasting, fusion bonding, and dicing: (1) alignment marks for pl and ebl were etched 500 nm deep into a 4-in. fused silica wafer of 500 lm thickness (from hoya corporation, usa), using fluorine based (50 sccm ar, 50 sccm chf3, 30 m torr, 150 w rf-power) rie and a photo resist (s1813, shipley, usa) etch mask. subsequently, the wafer was cleaned in piranha acid (h 2 so 4 2:1 h 2 o 2 ) at 120 c for 10 min, rinsed in di-water and dried under nitrogen stream. (2) aligned to the marks fabricated during the first precessing step, meandering grooves of 250 nm width were etched 250 nm into the substrate with fluorine based rie (5 sccm ar, 50 sccm cf 4, 10 sccm chf 3, 8 mtorr, 100 w rf-power, 100 w icp-power), using an electron-beam resist (zep520a, nippon zeon co. ltd., japan) as an etch mask. after rie, the resist was stripped under the same conditions as before. (3) repeating the processing of the first step, 1 lm deep grooves were fabricated, overlapping with the ends of the meandering structures. after sealing the fluidic system, these grooves will form microfluidic channels that allow reagents to be brought to the nano-channels, which are formed by the sealed, meandering grooves etched during the previous step. (4) to fabricate inlet and outlet holes, nitto sw-tape (nitto denko, japan) was glued on both sides of the processed wafer. the tape served as a soft mask during sandblasting on the backside and as protection against contamination on the frontside. small openings of approximately 1 mm diameter were cut into the tape at the desired position of the in/outlets, and through-holes were sandblasted using fused aluminum oxide powder of 50 lm particle size. (5) after removing the protective tape, the processed wafer was cleaned again in piranha etch (see above) and, together with a second unprocessed wafer of 175 lm thickness (from university wafers, usa), immersed for 10 min first in rca ii (5:1:1 h 2 o: hcl: h 2 o 2 ) and then in rca i solution (5:1:1 h 2 o: nh 2 oh: h 2 o 2 ) at 120 c, rinsed in di and dried under nitrogen stream. the two wafers were then brought together and covalently fusion bonded by annealing them at 1100 c in nitrogen atmosphere for 6 h. (7) finally, nanofluidic chips of 25 â 25 mm 2 were diced from the bonded wafers, using a 200 lm wide resin-bonded blade with 40 lm-diamonds (dicing blade technology, usa)."
requirements for obtaining a sharp pattern structure in an image include a movement of the sample at slower rate than the exposure time during imaging. the long relaxation times for chromosomal length dna therefore enables imaging of fine structures on molecules that have not yet reached equilibrium and thereby gaining a higher resolution than would be possible for relaxed molecules in equal sized channels (figure 3(c) ).
"in this section, we first analyze the effects of noises on the proposed algorithm. then, based on the theoretical analysis, we propose the value of the threshold and analyze the performance of the proposed algorithm."
"conventional genomics methods, such as those that were used in the sequencing of the human reference genome handle and analyze molecules in bulk. methods such as cloning or pcr are needed to isolate and produce sufficient quantities of dna to be detected. however, the lengths of genomic dna that can be handled by these methods are limited and cannot extend to whole chromosomes."
"in the work reported here, we have drastically reduced the number of frames required to image large genomes by using serpentine or meandering nano-channels to systematically refold the dna along a meandering path, so that a larger fraction of the pixels of the ccd chip can be used to collect useful information. this design is used to image dna molecules in the mbp range within a single ccd image. we show that dna refolded in the meandering channel design can be used in combination with partial melting to create a high-resolution map 17 along an entire eukaryotic chromosome. we have developed new computational tools to extract single molecule optical maps from the images (supplementary material in ref. 18 )."
"the research presented in this paper develops an approach for evaluating application-specific and user-perceived qos in cellular networks based on data services. specifically, it presents a technology-agnostic methodology that uses ahp to create a unified measurement that represents how well cellular networks operate for a particular set of application classes and relative to other networks servicing the same area. through several case studies, the approach is proven successful in providing a way for analyzing user-centric qos for application-specific usage."
"with explosive increase of wireless devices and services, telecom operators require more spectrum resources to provide high date rate services for a large number of users. now the spectrum policies of almost all countries are predefined, i.e., allocating a fixed frequency band to some exclusive wireless services, which have caused the remaining spectrum resource very scarce for new services. spectrum scarcity has become a major bottleneck for the development of telecommunications industries. much measurement monitored by the national spectrum regulation agencies of many countries, however, shows that many frequency bands are underutilized [cit] ."
"even though nanochannel systems exhibit a large potential, they have so far only been used for visualization of relatively short dna strands when compared to the genome/chromosome sizes of most organisms. an interesting approach to handle long dna is the dumbbell approach where a long dna straddles a slit. 10 due to the excluded volume forces of the blobs at opposing sides of the slit, full stretching is achieved over a section of the dna. however, a major limitation of this and other current designs is that the dna is stretched in just one direction, which allows only limited segments of native genomic molecules to be observed in a single frame of the camera. these methods do not make efficient use of the real-estate on a ccd chip. in a recent study 16 where megabase lengths of genomic dna were stretched, up to 20 frames of a typical 512 â 512 chip needed to be stitched together to make a panoramic composite image. the vast majority of pixels in each image were devoid of useful information. more pertinent to the high-throughput needs of genomics, the longer the length of the genomic dna to be visualized the longer the time it takes."
"where k represents the k th element of vectors w and wa. once overall scores are computed for all networks, the highest score is identified as the network providing the best qos, followed by the second highest score, and so on. this prioritized list helps determine the best perceived qos for a particular class of applications among different network providers."
"in wireless communications, due to the effects of multipath fading, shadowing, and noises, the received signals are hardly maintain high snr. however, when the snr is low, as shown in figure 4, we cannot use the eigenvalues to separate signal subspace and noise subspace, which will result in high false dismissals or false alarms."
"image analysis is required for extracting time-traces from the meandering nano-channels. to this end, our algorithm first performed top-hat filtering using a linear structuring element to correct for uneven background illumination, then rotated the image to have the linear parts vertical. thereafter, a parameterized meander contour based on the known nanolithography mask was placed on top of the picture with a vertical scale-factor, a horizontal scale-factor, up-down"
"the landau's sampling theorem [cit] has shown that the minimum sampling rate allowing perfect reconstruction should be no less than the total bandwidth of occupied spectrum. however, this requires to know the frequency support of the sampled signal. in the blind scenario, the minimum sampling should be more than twice the landau rate [cit] . the average sampling rate of multi-coset sampling is"
"to show the effects that particular classes of applications have on overall qos evaluation, the qec vs. qec comparisons are made to properly reflect the relative importance of each qec in emergency applications (emts, police officers, etc). in this class of applications, users will come to rely heavily on location-based services, where connection and downlink data services are of most importance. the qec vs. qec comparison matrix, normalized matrix, and weight vector are presented in table 6 . finally, using (5), the results of tables 3 -6 are combined to provide the final qos measurement for data services based on the emergency class of applications. the final perceived qos measurement is presented in table 7 ."
"we have demonstrated that molecules in the meandering channels can yield useful bioanalytical information by creating melting barcodes along the elongated dna, which have been validated as a means for determining meaningful genomic information. 16 in order to validate the experimental barcodes, we compared experiments to a theoretical reference barcode, obtained using the poland-scheraga model with s. pombe dna sequences as input, and found good agreement. we could identify fragments of the genome corresponding to sequence positions 1.7 mbp-1.9 mbp, 2.8 mbp-3.1 mbp, and 3.5 mbp-4.4 mbp. what is particularly striking is that the matching patterns can easily be discerned by eye (see figure 6 ). furthermore, within the single field of view, we have detected and localized a region of repetitive dna in the single s. pombe chromosome, which cannot be seen in published sequences of s. pombe. the long-range view of genomic dna made possible by our approach allows us to count the number of repeated segments within the region. this finding points to the method's unique potential for detecting large structural variations in a single molecule."
"the chuck (designed to support the chip) was made of zeonor 1060r (sigolis ab, sweden), due to its high resistance to chemicals, low inherent fluorescence, and high transparency."
"is the correlation matrix of pus signal, s(f ) is the fourier transform of the pus signals, and i is an identity matrix. since the matrix a is a vandermonde matrix, if the matrix p is nonsingular, the rank of the matrix"
each chip has two 1 â 5 lm 2 channels for transporting the dna to the array of nanochannels spanning between the micro-channels. inlet holes at the ends of the micro-channels creates a connection to the macro world.
the higher resistance of longer channels together with the high risk of shearing the molecules during entry into the nano-channels at high pressures led to a design where the inlets of several meander-channels were kept in close proximity of each other. this increased the resulting bulk flow rate towards the nano-channels decreasing the time needed for the introduction of dna ensuring that the bulk flow rate exceeded the diffusional rate of the end of molecule that was to be introduced.
"melting of whole chromosomes can be used to determine the number of repetitive parts as well as their lateral placement, something that can be difficult when only smaller parts are analyzed. the left end of the time-trace in figure 5 fig. 6. comparison between convoluted theoretical dna melting prediction and the experiment shown in fig. 4 . to make this comparison, we split the experiment into three regions: (1) 20 lm-150 lm, (2) 240 lm-300 lm, and (3) 450 lm-680 lm. this was done because we believe cutting events occurred between these regions. each of these regions was then matched to the theoretical profile (chromosome 2) independently (i.e., without bias against overlapping) using subsequence dynamic time warping. all three regions align to the theoretical profile in the order seen from the experiment (fig. 4), supporting the hypothesis that cutting events occurred between them. bottom image). in order to check for expected repetitive regions in the barcodes of the three s. pombe chromosomes based on sequence data, we calculated theoretical barcodes. based on a simple repetitive region finding method, see supplementary material in ref. 18, we found no repetitive regions with of the order 40 repeats in these theory barcodes."
"drive-testing is performed by placing calls, either voice or data, to a cellular network and recording the data from these calls. the calls are automated by test equipment and the data are geographically referenced by using a gps unit. a typical setup for the drive testing equipment for ps data measurements is presented in figure 1 . as seen, the setup consists of the in-vehicle equipment and appropriate ps data server. the in-vehicle equipment controls data cards (or other data communicating devices), associated with the cellular networks under test. the ps data server is connected to the cellular networks through the internet cloud. the connectivity between the ps data server and the internet is accomplished through high bandwidth dedicated lines. this way the performance of the data service is primarily determined by the performance of the cellular side of the network. the in-vehicle equipment controls data devices and it is programmed to execute a sequence of actions. depending on what needs to be tested, the sequence may include one or multiple data services. each device in the vehicle loops through the same sequence of tasks and relevant measurements are collected, time stamped and geographically referenced. the types of tasks that are included in the test depend on what particular data service needs to be tested. in practice, one would at least test ping, ftp upload, ftp download and http based web browsing [cit] . the ps data server needs to be configured properly so that it is able to act as a termination point for all data calls."
"an important problem in melting mapping is the photo cutting of dna. in this case, it is not fully clear whether a dark area is locally melted dna or just discontinuous space in between two dna strands. for small fragments diffusion would reveal a cut, but for long dna it may be difficult to tell the difference. a simple edge detection algorithm along the aligned dna identified three different regions (20 lm-150 lm, 240 lm-300 lm, and 450 lm-680 lm) along the barcode shown in figure 4 . these three regions were then independently aligned to a theoretical profile for s. pombe chromosome 2, and they successfully aligned in the order seen in the experiment with no bias against overlapping. this suggests that the extended dark regions in figure 4 may have resulted from cutting events."
"in this section, we first introduce the wideband spectrum sensing method based on subspace decomposition. then, we propose a two thresholds iterative detection scheme for sub-nyquist wideband compressive spectrum sensing. finally, we compare the computational complexity of these two method."
"since the cross-correlation between different atoms is small, the index of the maximal z i is the most probable position of active channel. the index of the maximal z i is"
"this paper proposes a quantitative approach for evaluating the perceived qos of data services in wireless cellular networks. specifically, the approach uses the analytic hierarchy process (ahp) to evaluate data collected from drive-testing to characterize the perceived qos of data services from three different networks. this characterization can be used by network providers to evaluate how well their services are being perceived so that they can make corrections or improvements as needed. the development of this methodology provides a unique approach to characterize qos based on application-specific quality criteria and relative to other networks operating in the same geographical area. the remainder of the paper is organized as follows. section 2 provides a brief summary of user-centric qos evaluation methods. section 3 provides a brief summary of the solution approach, which includes drive-testing and the ahp. sections 4 and 5 provide detailed explanations of the drive-testing experimental environment and ahp technique. section 6 presents the results of various case studies. finally, section 7 provides summarized conclusions and highlights of the proposed approach."
"the experiments were performed with chromosomes from schizosaccharomyces pombe (s. pombe) (chef dna size marker, from bio-rad). the dna was delivered in agarose gel as an electrophoresis size standard comprising chromosomes of lengths 3.5 mbp, 4.6 mbp, and 5.7 mbp. to avoid degradation of the dna molecules, they were stored within the agarose gel at 8 c until use. prior to experiments dna was stained with yoyo-1 fluorescent dye (lifetechnologies, usa) at a ratio of 1 dye molecule per 6 base pairs. a simultaneous staining and gel-extraction process was performed in a buffer consisting of 10 mm nacl in 0.05 â tbe (4.5 mm tris, 4.5 mm boric acid, and 0.1 mm edta) and yoyo-1. in this process, small pieces of agarose gel, carefully cut with a sharp knife, containing dna were placed in a large volume staining buffer and left at room temperature (20 c) over night during which the dna diffused out from the gel. just prior to experiments the running buffer was created by adding 2-mercaptoethanol (sigma-aldrich, mo, usa) and formamide (sigma-aldrich, mo, usa) directly to the staining solution to final concentrations of 3% and 50%, respectively."
where a t is consisted of a part of columns of a whose indexes is corresponding to t . the energy of all estimated pus signals is
"once the meander contour had been extracted at each time frame, a kymograph was generated, with time on the vertical axis and distance along the contour on the horizontal axis. however, due to random diffusive processes, the dna had undergone thermally induced stretching and contracting at the local and global levels, resulting in a \"fuzzy\" barcode. 17 thus, before further analysis could be performed, the kymograph had to undergo an alignment procedure. for this purpose, we applied our novel alignment algorithm, wpalign. 19 our new approach brings down computational costs considerably compared to previous procedures. 17 importantly, for mbp-sized genomes as of interest here, the old approach is estimated to require more than a day to perform its task, whereas our new method aligns a barcode in less than a minute. 19 briefly, the algorithm identifies the most distinctive \"feature\" by treating the kymograph as an intensity landscape and finding the shortest vertical path through it. this feature is then aligned, and the process is called recursively on the portions of the kymograph to the left and to the right of the newly-aligned feature. the process eventually terminates when the left and right regions are smaller than a threshold width given by the width of a typical feature."
"in today's cellular markets one encounters multiple cellular networks servicing the same geographical area. for example, in mature us markets, there may be as many as eight cellular licenses servicing the area (2 in the 850mhz band and 6 [cit] mhz band). each license may host a different service provider. furthermore, with recent auctions of 700mhz spectrum and the rollout of broadband services in 2.5ghz and 3.65ghz bands, the number of wireless cellular networks is bound to increase even more. due to various competitive and historical reasons, deployed networks may be quite different. for example, the networks may have been deployed with different objectives in mind, they may utilize different access technologies or they may be at a different evolutional stage of a given access technology. furthermore, most of the networks support both circuit switched (cs) and packet switched (ps) communication services. while cs services are dominated by voice, ps services support a plurality of data centric applications. as a result, a comparison between qos levels offered by different networks becomes a non trivial task. as cellular technology continues to ingrain itself in all aspects of society, network providers must emphasize on delivering high qos to its end users. therefore, the assessment of qos based on user experiences becomes essential."
"the qec vs. qec pairwise comparison matrix is a n x n matrix where each location a ij represents how much more important the qec (e.g., connectivity, downlink, uplink, etc.) at row i is than the qec at column j. the importance of each qec is configured depending on the class of application (e.g., emergency, business, or personal) used in the evaluation process. the format of the qec vs. qec matrix is presented in (4), where w i is the weight given to qec i."
"a micro-post array spanning across half the micro-channel was initially placed at the inlets to the nano-channels for pre-stretching of the molecules, and thereby easing their introduction into the nano-channels (figure 2 ). 24 however, for the long chromosomes used in this study the micro-post array turned out to be more of an unneeded hindrance than an aid. the large dna molecules followed the flow lines around the micro-post array and thereby ended up further from the inlets than they would have in the absence of the array. when forced to travel trough the array, the dna molecules became entangled around the pillars which prevented further movement towards the nano-channels. dna molecules in the mbp range have a radius of gyration in the order of a micron and therefore a slight elongation due to confinement was apparent in the 1 lm deep micro-channel. furthermore, the parabolic flow profile inside the channel served to stretch the large dna molecules since different parts of the molecule experience different flow rates."
"in section iv, we analyze the effects of noises on compressed sensing system and derive the theoretical performance of our proposed scheme. section v conducts numerical simulation to evaluate the performance of the proposed algorithm and compare with other algorithms. conclusions are drawn in section vi."
"will have contributions to the correlation coefficients. when we estimate the pus signals, i.e., the step (3) in algorithm 1, the noises of other channels will mix in the reconstructed signal. the goal of (21) is to minimize the residual, so the noises components which are correspond to a t would be subtracted from y[k]. define α i,k as the correlation coefficient between a i and a k . assumeing the ith channel is the active channel, when we use a i to estimate the pu signal, the estimated pu signal can be denoted aŝ"
"the ahp is a multi-attribute decision-making method used to facilitate decisions that involve multiple competing goals [cit] . it provides a powerful tool that can be used to evaluate different network providers based on multiple qos evaluation criteria (qec). it starts by transforming the qos evaluation problem into a structured hierarchy where each qec is quantified and related to overall goals for evaluating alternative solutions. typical qec for data services could include network accessibility, success rates of ftp download, or success rate of ftp uploads. typical goals for evaluating alternative solutions could include maximizing (or minimizing) all qec identified. evaluation goals are customized based on specific application requirements. for example, in the emergency class of applications (emts, police officers, etc.), users will come to rely heavily on location-based services, where connection and downlink data services are of most importance. other examples include the business and personal class of applications. in all cases, ahp can be used to quantify goals, prioritize them, and include them in the evaluation methodology. a generic ahp hierarchy for the qos evaluation process is presented in figure 2 ."
"in the case of n regions represented via k level set functions, we can write the area-penalizing cost function as: evolution using the proposed area term, (e) initial configuration, (f, g) two stages of the evolution. note that using the traditional regularization the two contours shrink independently. on the other hand, using the proposed regularization, the parts of the contours that are close to each after few iterations snap onto each other where"
"comparison between the segmentations obtained using the traditional multiphase regularization approach (left column) and the proposed method (right column)- [cit] . in (b) and (c) we compare the 4 regions obtained using the two different regularization techniques. the areas within the green boxes are magnified in (d) and (e) to show that the spurious regions (highlighted within the green circles), present using the traditional regularization (d), are instead absent if the proposed method is applied (e) have not been addressed in the literature. this paper can be considered a contribution in this direction. specifically, while smoothing contours can be a good regularization model in the single phase case to avoid the presence of boundaries wrapping around noisy spurious pixels, this does not necessarily hold true in the multiphase case. this problem becomes crucial in segmenting complex natural images (see fig. 1 for example, where the segmentation is obtained using the 4 [cit] ), where many of these spurious regions arise, particularly in correspondence of objects boundaries (that is where two level set functions can change sign simultaneously). the reason why traditional regularization based on smoothing is not enough to get rid of these spurious regions is explained pictorially in fig. 2 . here two extremely smooth surfaces are running parallel and spatially close to each other. the multiphase segmentation model can produce a spurious region located between the two boundaries. the traditional length and area regularization terms, which act on each contour independently, would not get rid of this spurious region since the two contours are already fig. 2 (a sufficiently smooth. therefore there is a need to rethink the regularization terms in a multiphase perspective, introducing the possibility for each contour to \"sense\" the presence of the other contours and evolve accordingly."
"with the following lemma, we prove that the value of (16) represents the effective length of the contour (i.e. no parts are counted twice). as explained for the 4 regions case, this will ensure that property 1 is always satisfied."
"to guarantee the well-posedness of the pde (21), we need to show that the quantity within the square brackets is always greater than or equal to zero for any point in the domain. in order to accomplish this, we need the following lemma."
"in addition, expressing the cost in terms of the effective length of the contour (i.e. making sure that the contribution of each point belonging to at least one contour is always δ (0)) has the consequence of enforcing property 1. in fact, if two contours are overlapping at a particular location x, it is straightforward to see that the term within the square brackets in (13) vanishes, preventing the regularization effect from separating the contours. we visually demonstrate this property in fig. 4 ."
"as mentioned in the introduction, a second regularization term can be expressed in terms of the area of the region in-side of the contour (i.e. χ 1 ) [cit] . using the same notation as above, we can write this term as:"
"unfortunately the minimization of this term using calculus of variation leads to an extremely complicated expression, due to the coupling of the two level set functions φ's, not easily implementable. [cit] consists in a simplification of the problem, decoupling the dependency of the length term upon the two level set functions. the simplified term they introduced is the following:"
"in the following we show that this term takes value 1, if x belongs to a region characterized by an odd number of positive level set functions, and 0 elsewhere. in this way half of the partitions is penalized and the other half is favored (providing a principled extension of the two regions (i.e. one phase) case, in which the region where φ is positive is penalized and the complementary region is therefore favored). this will ensure that, as in the case of the 4 regions term, properties 1 and 2 are always satisfied."
"-formulation of a length regularization term e l, in which smoothing happens selectively, depending on the reciprocal position of the contours. -formulation of an area regularization term e a, in which shrinking/expanding is conditioned on the presence of other contours, providing a more principled regularization effect. -extensive experimental validation of the proposed regularization models."
"the main drawback of this model consists in the fact that some parts of the contours (i.e. the overlapping parts) are counted twice (see fig. 3 ). therefore nothing guarantees that two overlapping contours will not separate during the evolution process, since separating or merging contours does not change the regularization cost function in use. this can potentially violate property 1 stated at the beginning of the section. in order to always enforce this property, we need to redesign the length term, making sure that it represents the effective total length of the contour (i.e. without counting any (10), but with a more approachable formulation than the one in (9). the solution we propose consists in subtracting from (9) a term which compensates for those segments where two contours are overlapping."
"in this paper we presented effective regularization techniques for multiphase level set based image segmentation. in particular, we reformulated the traditional length term to penalize the effective length of the multiphase contour and the area term to penalize half of the existing partitions. mathematically this yields regularization equations in which the different level set functions involved in the segmentation process are coupled to each other, guaranteeing that the evolution of one of them affects the evolution of the others. in turn, this guarantees that properties 1 and 2 are enforced. this is in contrast with traditional approaches, where this coupling is granted only by the data term (i.e. the term driving the segmentation), not in the regularization term. we qualitatively and quantitatively demonstrated that the segmentation results obtained using the proposed regularization technique outperforms the ones obtained via the traditional regularization."
"open access this article is distributed under the terms of the creative commons attribution noncommercial license which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited."
"for the 4 regions case, similarly to what has been done for the length in (10), a naive version of the area regularization term can be written as:"
"as mentioned in the introduction, the length regularization term is expressed as a term proportional to the length of the contour [cit] . translating this concept using a formal notation we can write:"
"in this section, we provide both a qualitative and a quantitative comparison of the performance of the proposed regularization techniques with traditional multiphase regularization techniques. to achieve this, we compare segmentation results obtained using the same data term (i.e. e d in equation (1)) but changing the two regularization terms (e l and e a ). [cit], 2002), which aims to minimize the variance of the partitions."
"for a more formal explanation regarding the choice of this normalization coefficient γ we refer the reader to appendix b. minimizing (11) w.r.t. φ 1 (or equivalently φ 2 ) using calculus of variations and parameterizing the descent via a virtual time variable t, we obtain the following pde, which evolves φ 1 in the direction of steepest descent of the cost function:"
"in the following we design novel multiphase regularization terms, such that, during the regularization process, the contours could sense the presence of the other nearby contours and evolve accordingly. in particular we require the regularization term (i.e. combination of area and length regularization) to satisfy the following two properties: this section is composed of two parts: sect. 3.1 concerning the length term and sect. 3.2 concerning the area term. in each part, for the sake of clarity and readability, we will start discussing about the 4 regions case (i.e. two level set functions φ 1 and φ 2 ) and then we will generalize our model to the n regions case."
"summarizing this section, we have introduced two new formulations for the length and area regularization terms. we have demonstrated that their combined effect satisfies the two desirable properties listed at the beginning of the section. in fig. 7 we pictorially show the effect of the combination of these two regularization terms on the evolution of two interacting contours."
"this paper is organized as follows. in sect. 2 we briefly review regularization techniques in presence of a single level set function, while sect. 3 extends the concepts to the multiphase scenario. in particular, sect. 3.1 deals with length regularization, while sect. 3.2 is concerned with area regularization. after presenting experimental results in sect. 4, we briefly conclude in sect. 5."
"where γ is a constant coefficient. if the point x 0 belongs to the zero level set of only one of the two level set functions φ i 's (let us assume w.l.o.g. that is on the zero level set of φ 1 ), we have that:"
"t denoting e i the k-dimensional indicator vector, which has all zeros except a one at index i, we can write the general expression for the regularization cost function in differential form as:"
"minimizing this term leads to a shrinking force that acts independently on each level set function φ i . the lack of a term which couples the two level set functions prevents them from sensing each other's position during the evolution. reasoning along the lines of what we have done for the length term, we modify (28) by considering that it does not faithfully represent the area of the partitions. in fact the area term as defined in (28) penalizes three partitions out of four (i.e. all the partitions for which at least one level set function is positive). in addition, the partition where both φ 1 and φ 2 are positive is penalized twice as much as the others. the idea we apply to modify this term is to penalize only half of the partitions. in particular, we introduce a coupling term such that only partitions with only one level set function being positive are penalized:"
"in order to define l(χ e 1, χ e 2 ), we introduce ∂e 1 as the perimeter of set e 1 and π(∂e 1 ) as the measure of the perimeter. we can then write our proposed length term as"
"the use of the flows in (8) and (6), along with the one that minimizes the data term, are perfectly suitable in the case of binary segmentation (i.e. background/foreground segmentation using only one level set function). on the other hand, in the following section we will demonstrate that, for the multiregion segmentation case, a reformulation of the traditional regularization term is needed in order to achieve accurate and reliable results."
"similarly, in order to define a(χ e 1, χ e 2 ) we introduce λ(e 1 ) as the measure of the area of set e 1 . the proposed area term becomes then"
"proof in this case, in fact, the vector ψ (x 0 ) has n entries equal to δ (0) and the rest of the entries are approximately zero. we can therefore re-write (15) as:"
"have been added to new conference information data format. furthermore to lessen the load, all conference servers share the processing of conference information data which should be transferred periodically to all participants. the suggested load control event package makes each server can get current load status of the overall servers. when load increases in one server sip client requests are distributed by selecting a server which has the lowest load value, or new server is created to share the load. the performance of the proposed system has been evaluated by experiments. they shows 21.6% increase in average delay time, and 29.2% increase in average sip message processing time."
"navigating back to country x, the user can verify that a country cannot anymore be an instance of another class. however, looking at possible roles, she discovers that a country can not only be the country of origin of something (adjunct ∃hascountryoforigin − . ), which is fine, but can also have a country of origin (adjunct ∃hascountryoforigin. ), and a spiciness. those two undesirable possibilities can be ruled out by selecting an unexpected adjunct, and asserting a negative axiom with the \"exclude\" button, and by repeating this sequence on the other unexpected adjunct. note that selecting the two adjuncts simultaneously, and then asserting an axiom would not be equivalent because this would only exclude countries that have both a country of origin and a spiciness. yet, it is possible to use only one axiom provided a class union is used between the two unexpected adjuncts. at this stage, the user can see no more undesirable adjuncts for countries, and move to other named classes."
"in this paper, the kepler k1 gpgpu of nvidia shield tablet is used for the implementation of deep classifiers. the used target device has 192 cores and a quad-core cpu of 2.2 ghz. in this section, the performance analysis is presented first, before moving to power efficiency considerations."
"in this section, we provide an additional example to show that our approach does not only apply to simple interactions between named classes and roles."
"although the proposed methodology is suitable for any sufficiently expressive logical formalism, we focus our consideration on the owl web ontology language [cit] as the currently most prominent expressive ontology language. table 1 . syntax and semantics of role and class constructors in sroiq. thereby a denotes an individual name, r an arbitrary role name and s a simple role name. c and d denote class expressions."
"example 5. given kb from above, we find that the (not fully balanced) class expression eucitizen ∃livesin.(eucountry ∃colonyof.eucountry) and the fully balanced class expression eucitizen ∃livesin.(eucountry) are equivalent w.r.t. kb ."
"from a cognitive viewpoint, the particular twist of this methodology is that it facilitates the specification of negative constraints by staying on a positive, scenario-like level, by exploring what is (logically) possible, pushing the boundaries of what is conceivable, and trying to cross them by constructing \"nonsensical\" descriptions. arguably, this is much easier and more intuitive than the task of directly coming up with negative constraints."
"from the viewpoint of formal semantics, the axioms of an owl ontology can be seen as conditions or constraints which a possible world has to satisfy for being in line with what the ontology modeler has specified to be \"true\" in the considered domain. thereby, one can distinguish between positive constraints, which specify what must be necessarily true, and negative constraints declaring what is impossible. 3 it has often been noted that positive constraints -such as"
"all remaining functions (batch normalization, relu, tanh and threshold) are also executed using the gpu's three-dimensional grid of concurrent blocks of threads for accelerating their operations. the batch normalization layer requires training of the mean, variance, gamma and beta parameters, as well, which are imported from the torch model of the deep classifier to normalize the image using the cuda-based accelerated function."
"in order to better understand the possible interactions between classes and properties, the user decides to navigate to each named class to discover what an instance of that class can be. for example, by selecting the adjunct country, the pointed class expression becomes country x (see figure 2) . the instance box says there are 5 known countries, namely america, england, france, germany, and italy. surprisingly, the adjunct box says that a country can be some food (possible adjunct food), or not (possible adjunct ¬food). this implies we can further select the adjunct food to reach the satisfiable class expression country food x. obviously, such an individual should not be possible, and we exclude this possibility by pushing the \"exclude\" button, which has the effect of adding the axiom country food ⊥ to the ontology. this illustrates the claimed fact that even very basic negative constraints such as disjointness hal-00779938, version 1 -22 [cit] axioms are often missing in ontologies. on the contrary, we found no missing positive axiom like subclass axioms."
"torch is an open source scientific computing framework that comes with machine learning libraries. using these open source libraries, deep convolutional networks can be created and trained easily in the torch environment on a desktop computer. torch relies on cuda for efficient operations on nvidia gpus. cuda also enables clustering of gpus to accelerate the process of training a network. torch relies however on a number of third-party libraries for performing ancillary operations on the cpu. however, porting such libraries to mobile cpus is not always possible because of architecture differences and the memory footprint thereof. in addition, gpu clustering is not applicable to mobile platforms as mobile platforms typically include just one gpu. finally, there exist a torch library for mobile terminals; however, this library is not mature in terms of dependencies."
"we now define a subclass of owl class expressions which we deem particularly intuitive to deal with from a cognitive perspective as they essentially represent (alternatives of) existing structures, while negations are only used at an elementary level."
"from figure 1, we can conclude a number of things about the pizza ontology 12 . from the class box, we conclude that a pizza may have no topping. from the emptiness of the instance box, we conclude that there is no known individual pizza without topping. from the adjunct box, we further conclude that such a pizza must be some food, that it must have some base as an ingredient, but that it may also be a country, and that it may have no country of origin."
"these models are comprised of multiple layers with different pooling types and activation functions, as listed in table 1 . resnet-34 has an additional layer of batch normalization in its architecture. all of these required functions are implemented in the cuda computing framework to accelerate and replicate the trained network on the mobile device. trained parameters are imported in the internal memory of the mobile device to complete the task of classification. the size of trained parameters is also an important factor to be considered for the successful implementation. these trained parameters include the weight and biases for the convolutional layers or values of gamma, beta, mean and variance for the batch normalization functions. the sizes of these parameters depend on the size of the filters and the dimensions of the feature maps of each layer. the sizes of these trained parameters of all three architectures are listed in table 2 ."
"convolutional layers serve the purpose of extracting robust features from the input images, and recent deep classifiers include many convolutional layers [cit] . convolution is a computationally-intensive task that can be accelerated using the concurrent processors [cit] . mathematically, it can be expressed as the sum of the products of mask/filter coefficients with the input function/image. the convolution operation can be extended to two or three dimensions according to the required solution."
"as shown in figure 1, the integrated camera of the mobile phone is used to capture the image for the real-time classification by the trained network. therefore, there is no need for an additional camera module as required by a single-board computer or desktop workstation. this captured image is then passed through the same pre-processing steps (cropping/scaling and normalization) as performed at the time of training. because the trained network is for a specific input dimension and the captured image can be of different aspect ratios depending on the used mobile phone, so it is necessary to perform the scaling or cropping of the image to fit the input dimension. similarly, normalization of the input image is also essential to minimize the bias for one feature over another and to restrict the range of input/output values. after these two preprocessing steps, the captured image is fed to the cuda-based replicated neural model to perform the task of classification using imported trained parameters. the final classification result is displayed using the display screen of the mobile phone."
"as a starting point, we assume that an owl ontology has been created by stipulating the used vocabulary and possibly arranging classes and properties in taxonomies. it is not essential that the ontology contains individuals, our method works equally well for non-populated ontologies. also, the ontology may or may not already contain axioms beyond taxonomic relationships."
"convolutional neural networks are becoming an all-encompassing package for a number of computer vision and machine learning tasks. image classification is an important computer vision task that has numerous practical applications. over the last few years, image classification has been revolutionized by the advent of deep learning-based architectures. deep classifiers have shattered existing performance records in several image classification contests, enabling a large leap over previous so-called shallow classifiers. a typical deep network for image classification tasks is mainly composed of two stages. the first stage accounts for the extraction of features, also called kernels or filters, that are robust to changes in illumination, size and translations from the input image. a detailed description of such a feature extraction stage is provided in section 4. next, one or more fully-connected layers in the second and final stage performs the feature classification on a highly dimensional space, providing the final output. recent results show that the depth of a neural network (number of stacked layers) plays an important role in achieving better classification accuracy [cit] . deep convolutional networks have showed recognition accuracy comparable to humans in many visual analysis tasks [cit] . larger datasets and deeper models lead to better accuracy, but also increase the training and classification time [cit] . heterogeneous systems play a pivotal role in the field of visual analysis where the computational capabilities of cpus and gpus can be utilized altogether to maximize the performance of deep classifiers in terms of training and classification time. in the following section, the available frameworks for implementing image classification via deep neural architectures are discussed."
"there are also other algorithms for computing the convolution, like winograd or the lookup table-based approach, which can be used to accelerate the state-of-the-art deep classifiers having small filter and batch sizes [cit] ."
definition 3. the pruning of a pointed ci class expression is obtained by applying the recursive function prune (we tacitly exploit commutativity of and to reduce cases):
"to exclude such unwanted cases, we introduce the notion of balancedness (definition 4) as a desired property of class expressions. intuitively, a class expression is balanced, if all alternatives described by unions can possibly occur. toward the formal definition, we first have to introduce the notion of prunings (definition 3). by pruning a pointed class expression, we specialize it by removing disjunctive side branches, thus enforcing that the disjunctive branch in which x is located must be \"realized\"."
"there are different approaches to perform the convolution operation. in this paper, the multi-channel convolution operation is performed using two approaches for the adaptation of the torch-based trained network on the mobile platform."
"for the evaluation of deep classifiers on a mobile platform and the performance comparison of classifiers for both convolution approaches, three different architectures are implemented on the mobile device using the cuda computing platform: alexnet for the cifar-10 (canadian institute for advanced research 10) dataset; overfeat and resnet-34 for imagenet [cit] . these architectures need the following number of layers and functions to be replicated on the mobile device to perform the classification:"
"there are also some important activation functions, like the tangent hyperbolic unit (tanh), rectifier linear unit (relu) and the thresholding unit, which are essential components to construct the architecture of trained classifiers. the purpose of these functions in neural architectures is to improve the training and eliminate the problems like the vanishing gradient [cit] ."
"in this section, we describe an example scenario of exploration and completion of the pizza ontology. this ontology has the advantage of being well-known, covering a large subset of owl constructs, and being representative for owl ontologies. while the pizza ontology is often referred to and was subject to a number of refinements, we found in our exploration a number of unexpected possible worlds, and hence missing axioms. the following scenario describes a non-exhaustive exploration, and illustrates various situations that may arise."
"as a future direction, the presented approach will be extended in order to exploit the most recent advances in neural networks and embedded architectures. in order to speed up computationally-intensive convolutional tasks, techniques like winograd's minimal filtering technique can be adopted to reduce the arithmetic complexity of the convolution operation over small tiles. winograd reduces the number of multiplications compared to traditional convolutions, and it is computed by an element-wise multiplication instead of a matrix multiplication. further, the proposed scheme can also be optimized using the unified memory architecture of recent gpus where extra memory transfers can be avoided by defining a more memory-efficient scheme. by using this scheme, the variable of the activation results and filters can be created in such a way that they are accessible both by the cpu and gpu to optimize the memory storage and avoid extra transfers."
"ontologies -logical descriptions of a domain of interest -are at the core of semantic technologies. expressive ontology languages like owl allow for very precise specifications of semantic interdependencies between the notions describing a domain of interest. while it has been argued that \"a little semantics goes a long way\" and lightweight formalisms provide for better scalability properties, it is also widely accepted that expressive formalisms are superior in terms of modeling power and the capability of deriving implicit knowledge, thereby allowing for a more intelligent way of handling information."
"once the training is completed, the trained parameters of neural models are ready to be imported for the classification purposes. in this work, the cuda computing framework is used for the realization of identical neural architectures on an embedded device to exploit the already trained network. required layers (convolution, pooling, batch normalization) and activation functions (tangent hyperbolic unit, rectifier linear unit and thresholding unit) are implemented and accelerated using gpgpu. convolution is the core building block of the convolutional neural network (cnn). two versions of convolution are implemented to support the different trained models of torch. format conversion of imported parameters according to the used convolution method is also an important step to match the results with the desktop-based trained model. after replicating the required architectures using cuda and importing the trained parameters, accelerated neural classifiers are executed on the mobile device as a part of the android application. implementation of just the classification phase of the convolutional neural network lowered the barrier of implementing the deep classifier on the heterogeneous mobile platform."
"definition 4. let kb be a knowledge base and let c be a ci class expression in which a union d 1 d 2 occurs as a subexpression. let c (x) be obtained from c by replacing this occurrence with x, such that by definition, full balancedness of a class can be checked by a twofold class satisfiability test for each disjunctive subexpression, thus the number of necessary class satisfiability checks is linearly bounded by the size of the class expression."
"matrix multiplication-based convolutional layer (convmm) is also implemented using the cuda computing language. this approach to compute convolution is accelerated using heterogeneous resources of mobile device where the computational powers of both the cpu and gpu are exploited. convolution matrix multiplication (convmm) is computed by partitioning the suitable computations between the cpu and gpgpu of the mobile device. the transformation of the image and filters into the matrices, which is a sequential task, is performed using the powerful cpu, and matrix multiplication of these transformed data is computed using gpu. this cpu-based transformation step cannot be performed concurrently because values from multi-dimensional maps have to be placed in a certain order so as to achieve the equivalent two-dimensional representation of data for multiplication that can be much slower if performed by a gpu that is not suited for sequential operations. furthermore, input data are exchanged several times between the device (gpu) and host (cpu) memory to realize this matrix multiplication-based convolution. figure 5 visualizes the scheduling of computations between the cpu and gpu for convmm and the full convolutional layer."
"a number of frameworks for implementing deep neural networks are currently available. such frameworks leverage the parallel computational capabilities of modern gpus, allowing to practically train and deploy deep neural networks in a reasonable time. however, most of these frameworks are tailored to desktop and server platforms; therefore, they do not take into account the unique peculiarities of mobile devices. mobile devices' peculiarities include, among others, reduced memory and limited energy stored in the battery: operations are optimized just for execution speed, without considering the need to extend battery life. in the following, major frameworks that are currently available for implementing deep neural networks are discussed. the shortcomings of these are discussed when it comes to mobile applications, and the need for efficient schemes designed ad hoc for mobile platforms is highlighted and motivated."
"batch normalization is also a significant part of deep classifiers, which yields substantial acceleration in the training. it preserves the representation ability of deep classifiers and ends the need for any further regularization [cit] . the batch normalization function can be implemented using the following formula:"
"example 2. considering the knowledge base kb introduced above, the pointed class expression eucitizen ∃livesin.(eucountry ∃colonyof.x) would allow for the class eucountry as possible adjunct, as the class eucitizen ∃livesin.(eucountry ∃colonyof.eucountry) is still satisfiable thanks to either of the disjuncts eucitizen and eucountry."
"deep convolutional networks have recently shown top performance in important machine learning tasks, such as image classification [cit] . image classification via deep neural networks involves two stages: an offline learning stage (training) followed by proper image classification afterwards (testing). during the learning stage, a network is trained over a set of labeled input images where the network parameters (weights and biases) are iteratively adjusted to predict the output values corresponding to the input labels. once the network training is complete, the learned set of parameters can be used for the classification of new sample images."
"in words, ci class expressions allow for the description of situations: existing objects, their interrelations and their properties (in terms of being (non)-members of atomic classes, (not) participating in a relationship, or (not) being identical to a named individual). thereby, the structure of the axioms enforces that only tree-like relationships can be described. moreover, the use of disjunction allows for specifying alternatives for parts of the situation descriptions."
"our methodology can be seen as an exploration of the possible worlds admitted by an ontology. thereby, a domain expert starts to describe an individual of one of these possible worlds by specifying its class memberships and relationships to other individuals. this specification process is supported by an interface in the spirit of faceted browsing, which naturally constrains the specification process in a way that no descriptions can be constructed which would contradict the ontology. in other words, the navigation-like stepwise refinement of the description of a possible individual ensures that an individual matching this description indeed exists in at least one of the models of the ontology. in this sense, the proposed methodology can indeed be seen as an \"exploration of possible worlds\"."
"training and testing a neural network is largely a parallel problem, so heterogeneous gpu-cpu architectures are commonly employed to speedup such processes. cpus are well-suited for sequential tasks due to higher operational frequencies, whereas gpus can execute concurrent tasks efficiently thanks to their single instruction multiple threads (simt) architecture. by proper scheduling of the computing resources of heterogeneous cpu-gpu architectures, the training and testing time of neural networks can be largely reduced [cit] ."
"definition 2. given a knowledge base kb and a pointed ci class expression c(x), we call c(x) satisfiable w.r.t. kb, if c( ) is satisfiable w.r.t. kb . we further define the possible adjuncts of c(x) (denoted by poss kb (c(x)) as all simple class expressions d for which c(d) is satisfiable w.r.t. kb . moreover, we define the necessary adjuncts of c(x) (denoted by nec kb (c(x)) as the set of all simple class expressions d for which c(¬d) is unsatisfiable w.r.t. kb . example 1. let kb be a knowledge base containing just the following two axioms: (a1) ∃colonyof − . eucountry ⊥ stating that eu countries must not have colonies and the axiom (a2) ∃colonyof − ."
"we have proposed an intuitive methodology for adding negative constraints to owl ontologies in an exploratory way. to this end, we devised and implemented an interaction paradigm for constructing intuitive satisfiable class expressions in an interactive way reminiscent of faceted browsing, which-if found to be absurdcan be turned unsatisfiable by adding a corresponding negative constraint to the underlying ontology. future work on this subject clearly includes scalability and usability investigations and improvements. for seamless navigation and editing, the underlying reasoning steps must be performed in near-realtime which poses some restriction on the computational intricacy of the considered ontology. in order to enlarge the scope of applicability of our method, we will optimize pew in terms of minimizing owl api calls."
"on the usability side, next to thorough user studies, we will further enrich the tool with further functionality beyond mere exclusion of unwanted class expressions. ultimately, we plan to provide pew as a protégé plugin."
"finally, looking at spiciness (degrees), the user excludes the following possibilities: a spiciness that has a country of origin, a spiciness that is the country of origin of something, and a spiciness that has a spiciness (degree). after those exclusions, a spiciness can only be the spiciness of something. the class spiciness has three subclasses: hot, medium, and mild. selecting any of those classes shows that no other class is possible simultaneously, which means that disjointness axioms have already been asserted between them."
"when exploring roles, we investigate for each role what can be at their range and domain. class exploration has covered axiom schemas a b ⊥, a ∃r. ⊥ and a ¬∃r."
"concluding, the presented work differs from and improves upon the above-mentioned references addressing most of the above-listed issues. first of all, the proposed scheme has the benefits of jointly exploiting the computational capabilities of both the cpu and gpu, enabling true heterogeneous computations. moreover, this approach has the merit of supporting nearly all layer types of neural networks found in a modern image classification networks and is suitable for deploying very complex topologies. in addition, the accuracy of the presented approach is similar to the existing desktop and server platforms, whereas it does not require an intermediate framework for actual image classification. the proposed scheme can be easily integrated into an android application and provides compatibility for models trained with other desktop/server frameworks. figure 1 illustrates the proposed flow to realize a deep convolutional classifier on a mobile device. first of all, state-of-the-art neural classifiers are trained in the torch framework. a powerful gpu server is used to train the required neural architectures. a few preprocessing techniques are also used before training the classifiers. neural networks need a fixed size input, while training data may be collected from various sources and can be of different sizes/dimensions. therefore, it is necessary to crop or scale the images of different sizes and dimensions to fit the defined architecture. there are several approaches to perform the cropping and scaling. data normalization is also an important preprocessing step and useful when the inputs are generally on a widely-spaced scale. it is performed here by removing the mean value of each feature and then dividing the non-constant features by their standard deviation. since the classifiers require being trained on a vast amount of training data, data augmentation is used to improve the classification accuracy. flipping, random cropping, reflection, color jittering and other different data augmenting techniques are commonly combined to augment the dataset, improving performance and accelerating training. finally, the trained neural network can be deployed in the field for the actual classification."
"in classical convolutional networks, the output or activations of the convolutional layer are passed to the pooling layer. the purpose of this layer is to achieve the spatial invariance by aggregating the information within a small local region. it basically reduces the size of feature maps. two conventional options to perform the pooling are max and average. these pooling layers do not require any trainable parameter. there are also other pooling strategies that can be combined with some regularization techniques to improve the training of deep classifiers [cit] ."
the set ci[x] of pointed ci class expressions denotes ci class expressions with the symbol x occurring exactly once in the place of an unnegated class name.
"the current implementation is rather naive, and optimization is left for future work. for information, we here give the nature and number of performed reasoning tasks (owl api calls to reasoner methods). first, the hierarchy of simple class expressions has to be computed, which amounts to 1 call to getinstances for the top class, 1 call to getsubclasses for each named class, and 1 call to getsubobjectproperties for each named role. then, at each navigation step, the known instances of the class expression c(x) are computed with 1 call to getinstances, and the possible adjuncts by checking the possibility and necessity of c(d), for each positive simple class expression d. checking possibility amounts to 1 + 2d call to issatisfiable, where d is the number of unions in the class expression; and checking necessity amounts to 1 call to issatisfiable."
"it is rather easy to see that by restricting to fully balanced class expressions, we do not lose anything in terms of expressivity, since for any satisfiable class we find an equivalent one which is fully balanced by pruning away the fake disjuncts."
"among the downsides of deep neural networks, one of the downsides is their computational complexity, which is an issue especially with embedded devices. the complexity of a deep neural network for image classification stems largely from the convolutional layers. convolutional layers serve the key purpose of extracting features robust to changes in the image scale and illumination."
"summing up, our navigation paradigm is tuned in a way that favors construction of \"meaningful\" (in terms of satisfiability and balancedness) class descriptions but does not restrict expressivity otherwise."
the value of mean and variance can be calculated over the training data; while gamma and beta are the scaling and shifting parameters to be used by the normalized output.
"pointed class expressions are used to put a focus on a subexpression of a class expression. this focus will serve as a marker to indicate a point in the expression where new subexpressions can be attached. consequently, given a pointed ci class expression c(x) and a ci class expression d, we write c(d) for the class expression c(x)[d/x] obtained by replacing the occurrence of x in c(x) by d. the following proposition is an easy consequence of the observation that by construction, x occurs in a position with positive polarity. proposition 1. let kb be a knowledge base, let d and d be arbitrary class expressions and let c(x) be a pointed ci class expression."
"country expressing that only countries may have colonies. then, considering the pointed class expression country ∃colonyof.x has country as a necessary adjunct since (a2) would render country ∃colonyof.¬country unsatisfiable. on the other hand, eucountry is not a possible adjunct, since country ∃colonyof.eucountry is not satisfiable. clearly, the sets of possible and necessary adjuncts of a pointed class expression provide useful information on how the expression can be reasonably extended and what extending adjuncts would be implied anyway, both taking the provided knowledge base into account. still, in specific cases, with disjunctive information being involved, poss kb (c(x)) might not quite capture the needed information, as illustrated by the following example."
"definition 5. given a knowledge base kb and a pointed ci class expression c(x), we define the nice possible adjuncts of c(x) (denoted by poss kb (c("
both functions of the pooling layer (average and max) are implemented to realize the architecture of different classifiers on the mobile platform. the cuda computing language is used to perform these operations concurrently and to exploit the computational power of the gpgpu to outperform the sequential versions of the same layer. the cuda kernel for padding is also implemented using parallel resources of the gpgpu to support the same options provided by the pooling layers in torch.
"according to the model-theoretic semantics, an ontology can be seen as a set of constraints characterizing possible worlds (the models of the ontology). adding axioms to an ontology results in strengthening these constraints and thereby \"ruling out\" models."
"the traditional fully-convolutional layer is implemented using the cuda language. this layer is accelerated using the concurrent three-dimensional grid of threads and blocks of the gpgpu. the complete workload is offloaded into the gpu for the acceleration of the convolution operation. one of the important tasks to realize the torch-based replicated architecture is the formatting of the captured/imported input image and trained parameters. this formatting is required to achieve the same results in the cuda computing environment as provided by the torch model. the multi-dimensional input image and trained parameters are arranged and accessed in row-major order because the access of the contiguous array elements is faster. the padding function is also implemented in cuda using the parallel resources of the gpgpu to support the same options provided by the fully-convolutional layer of torch. the structure of the implemented convolutional and pooling layers is different from the other layers because both of these layers are comprised of two kernels: one kernel to perform the padding of input data and the second kernel to perform the selected operation of convolution or pooling on this padded data. while the other layers, like the rectifier linear unit and tangent hyperbolic layers, are computed using the single kernel where the function of padding is not required. the flow of a padding-based convolutional and pooling layer is illustrated in figure 4 . input"
"navigation from one (pointed) class expression to another is entirely performed in an interactive way. double-clicking an adjunct extends the class expression by inserting it at the focus. alternatively, adjuncts can be found by auto-completion in the text fields (one at the focus in the class box, and another above the adjunct box). the focus can be moved simply by clicking on various parts of the class expression. the contextual menu of the class box (c) provides the additional navigation steps: inserting a disjunction, and deleting the subexpression at focus. the toolbar (t) provides navigation in the history of class expressions, as well as the update of the ontology. the button \"exclude\" adds the axiom c( ) ⊥ to the ontology, in order to rule out models in which the current class expression has instances. figure 1 displays a situation where this operation would make sense. to provide feedback about the update, the focus background color switches from green (satisfiable class) to red (unsatisfiable class). in the case where the update would make the ontology inconsistent, the button \"exclude\" triggers an error message. finally, the button \"save\" saves the updated ontology."
"these findings justify our suggestion to restrict the possible adjuncts for a pointed class expression to those which would not just maintain its satisfiability, but also its balancedness."
"x) with focus at the domain, it appears that those things may not be food (adjunct ¬food). this can be excluded by asserting the axiom ∃hasspiciness. ¬food ⊥, which is equivalent to ∃hasspiciness."
"the actual task of the domain expert is now to construct descriptions which are possible according to the ontology but absurd given the experts domain knowledge. that is, the domain expert is supposed to assume the role of the \"devils advocate\" by actively trying to construct situations which are impossible according to his/her knowledge of the domain, thereby showing that the given ontology is underconstrained. once such a problematic description has been constructed, it can be converted into an axiom which exactly prohibits this situation. by adding this axiom to the ontology, the just constructed absurd description is made impossible and every model featuring such a situation is excluded from the possible worlds."
"this section provides first the relevant background on deep convolutional neural networks and their application to the problem of image classification. next, the relevant literature concerning deep neural networks on mobile devices motivating the present research is discussed."
"in this work, artificial neural network (ann) is proposed as the envelope fitting algorithm. ann is well-known for mapping any nonlinear function with theoretically any given accuracy and has significant dominance in robustness compared to aforementioned approaches."
"numbers of neurons in the hidden layer ranging from 5 to 15 are then examined, and the results shows no significant changes in the shape of the curves when the number varies. but they sometimes changes a lot when we repeat calculating the envelopes with fixed neuron numbers. this is not allowed in the blood pressure measurement. and the value of training goal is then lowered."
"multilayer perceptrons (mlps), back propagation (bp) networks, radial basis function (rbf) networks, hopfield networks and elman networks are widely implemented architectures. mlps are often used as a method of speech recognition, image recognition and machine translation. bp networks can avoid converging to a local minimum and their convergence speed is relatively faster than mlps. rbf networks excel at exact interpolation but perform poorly with noisy data. hopfield networks and elman networks are applied to simulate associative memory more often compared to other applications. for these reasons, bp network is chosen as the ann architecture for this curve fitting. generally, bp networks include three layers: input layer, hidden layer and output layer [cit] . as is shown in figure 3, the number of neurons in the input/output layer equals to the number of input/output parameters. however, there is still no universal method to figure out what the optimum number of neurons in the hidden layer is [cit] . it will be determined through experimentation in this research."
"the method consists of four sequential steps: digital filtering, shannon energy envelope extraction, peak-finding logic and true r-peak locator. in the main step, peak-finding logic, hilbert transform and zero crossing are combined so as to get a better performance [cit] ."
"the proposed method is tested after some requisite preparations, such as data acquisition and peak detection. data acquisition in our work is mainly conducted in labview environment with auxiliary devices. peak detection and curve fitting algorithms are conducted by the matlab script node in labview."
"since warren mcculloch and walter pitts introduced the first \"threshold logic\", ann has made much progress both on theory and its application. as a result, over 40 different architectures of ann are available at present. whether the structure and learning algorithm are properly selected will affect the performance of the ann to a great extent [cit] ."
"after numerous experiments, the ann with 10 neurons in the hidden layer and training goal of 0.002 is found to be the much better. it has a good repeatability of the shape and a relatively fast learning speed. figure 5 shows the training performance of the anns with 9, 10, and 11 neurons in the hidden layer. ann with 11 neurons in the hidden layer performs slightly better than the one with 10 neurons, but the latter is preferred since it is less computationally intensive. the ann with 9 neurons in the hidden layer is not so stable, and sometimes it takes much more time to reach the training goal as figure 5 (a) shows."
"a diagram of the set up for blood pressure data acquisition is illustrated in figure 2 . the cuff and the cuff inflation/deflation system, which is chiefly composed of an air pump as well as controlling circuit, are connected by a hose. controlling signal that tell the air pump when to start working or halt with a given rotating speed is sent by an ni usb-6008 daq device. ni usb-6008 is also used for acquiring data from the embedded pressure sensors and communicate with pc. a pc running labview program is exploited here for sending commands to the cuff inflation/deflation system, receiving data and continuing further data processing."
"where n is the sample size in the calibration set, n is the number of output parameters, y i (k) is the ith desired value in the nth training pattern, and ˆ( ) i y k is the corresponding output of the trained ann. the results in figure 4 demonstrate that the curves tend to be more oscillometric and more peaks tend to be passed through when the number of neurons in the hidden layer increases. this is because the ann's ability to describe the relationship between the inputs and outputs becomes stronger, which is a disadvantage in this circumstance for it overfits the peaks contaminated by noise. so it is obvious that the optimum number is around 10."
"oscillometric method for blood pressure measurement is widely used in electronic tonometers because of its low cost, rapidness and convenience [cit] . principle of this method is based on the fact that oscillations of the arterial wall can be detected by sensors fixed on the occluding cuff when the pulsatile blood flows through the blood vessels. vol. 8, no. 2; after the oscillation waveform is acquired, the mean arterial pressure (map) can be directly measured by searching for the maximal oscillometric pulsation during deflation (or inflation) of the cuff. typically, the values of systolic blood pressure (sbp) and diastolic blood pressure (dbp) are determined by maximum amplitude algorithm (maa) (s. [cit] ). as figure 1 shows [cit], oscillation amplitude of the pulses will be divided over map to get a ratio. if the ratio that equals to a predefined value appears before map, then the corresponding cuff pressure will be regarded as sbp. otherwise, if the ratio that equals to another predefined value appears after map, then the corresponding cuff pressure will be regarded as dbp."
"figure 2. block diagram of the data acquisition system 48 subjects ranging in age from 18 to 60 years is involved in this research. the 12-cm-wide and 23-cm-long cuff embedded with piezoelectric sensors is bound on the left arm at heat level. to check whether the cuff will move on the arm or not, it is inflated and then deflated before data acquisition. vol. 8, no. 2; are acquired when the cuff is deflated at the rate of approximately 3 mmhg per heartbeat."
"for the purpose of fitting the curve as fast as possible, levenberg-marquardt algorithm will be chosen as the learning algorithm [cit] . besides that, training goal is another parameter that should be carefully considered. if it is too small, overfitting will occur and the relationship between inputs and outputs won't be described in a right way. otherwise, if it is too big, the shape of the trained curve will have a poor repeatability."
"it is intuitive in figure 6 that oblate \"bell curves\" of asymmetric gaussian/lorentzian functions are almost the same with anns. but their mses are much higher than anns. architecture of the ann implemented in this research is relatively simple while its computational intensity is slightly higher than the other two approaches that also apply levenberg-marquardt as the optimization algorithm. in most cases, both of the three algorithms can complete their calculation in less than a second (3.3 ghz of cpu and 4gb of ram). however, sometimes it takes ann for over 10 seconds to finish the process. a simple solution to this problem is lower the max epoch to about 50. as a result, the training goal can't be reached sometimes, but we find that they still have much lower mses than the other two methods in all of our experiments. ann and the other two methods are then implemented to measure blood pressure for further comparison. the 48 subjects are divided into two groups according to their health condition. group 1 contains 36 healthy subjects, and the other 12 subjects are people with cardiovascular disease. the cuff pressure value corresponding to 70% of the amplitude of the map pulse when the cuff pressure is above map is considered as sbp, and dbp is acquired when 50% of the map amplitude appears. reference blood pressure values are measured by auscultatory method. the oscillometric and auscultatory method are performed simultaneously by a skilled nurse with the same cuff. that is, the cuff is connected to both the oscillometric device and a mercury manometer. sbp and dbp by auscultation method is defined as the appearance of korotkoff sounds and the disappearance of the sounds during deflation. table 1 shows the standard deviation (sd) of the three methods when the subjects are divided in to two groups. it can be implied from the results that all the three algorithms performs well, and none of them performs significantly better than others when the subjects are all healthy. but when they deal with oscillometric waveforms that interfered by cardiovascular disease, ann is more precise than the other two methods that are www.ccsenet.org/cis computer and information science vol. 8, no. 2; relatively easy to be affected by the shape of the oscillometric waveforms."
"curve fitting for envelope is crucial in blood pressure measurement. after careful consideration, ann is implemented for curve fitting instead of asymmetric gaussian/lorentzian functions, and the selection of its topology is conducted. through abundant tests, it is found that 11 neurons in the hidden layer and 0.0025 as training goal are in a sense optimum parameters for the bp network in the curve fitting. this architecture is then compared with the widely used methods. the results indicate that it outperforms the other two algorithms in terms of mse and has an acceptable computing time. by properly reducing the max epoch, the problem that sometimes ann consumes too much time to reach the training goal is solved while its mse still maintains at a lower level."
"each of the 48 subjects are tested with the three algorithms. criteria for evaluating the performance of the algorithms are mse and the time they consume. it should be point out that we use bp network with fixed architecture: 11 neurons in the hidden layer, 0.0025 as training goal and levenberg-marquardt learning algorithm. figure 6 shows some representative results of the comparison."
"multicoded segmentations. usually, e.g., with mevislab, each segmentation mask, representing one structure, is stored in a single file. this is inefficient with respect to memory and performance. storing all segmentation masks in one image stack can overcome this problem. however, one voxel of an image may belong to more than one anatomic structure, when structures overlap each other. for example, a voxel in the liver tissue may belong to a tumor and the liver tissue itself. thus, we cannot assign one label to each structure for the resulting segmentation mask of all structures. a straightforward approach is to assign each structure to one bit of an 8 byte voxel value. but this approach is limited by the number of bits, e.g., only 64 labels could be stored in an 8 byte value. since in real data only a small subset of all possible combinations of overlapping structures occurs, we developed a more efficient solution and refer to it as multicoded segmentation masks (mcsm)."
"since the metk is an extension of mevislab, all metk applications can be built up by creating networks of modules in a visual programming environment. the metk modules can be arbitrarily mixed up with other mevislab modules. due to the high-level functions provided by the metk, a developer can focus on application logic. this quickly yields applications that a surgeon can use (see fig. 9 ), which often elicits essential feedback. thus, this stage should be achieved as fast as possible, and the development cycles of new applications need to be accelerated. the application's logic is defined in python scripts. we support application building by integrating many ready-to-use python scripts in the metk. the design of the graphical user interfaces is scripted as well, using the module definition language of mevislab. we extend the basic set of widgets such as buttons and sliders by more complex widgets that can be integrated in an application with minimum scripting effort."
"all functions are conceptually organized in three layers: the data management and communication layer, the visualization layer, and the exploration layer (see fig. 2 ). the lowest layer imports the case data and provides data management functions. the visualization layer comprises viewer classes and special rendering modules. basic viewer classes and the volume rendering are reused from mevislab. all high-end interaction and exploration techniques that are necessary to create powerful surgical applications, are available within the exploration layer. the layers and their provided functions will be described in the next sections."
"we compare the optimal serialization methods provided by big data framework with other high performance methods to optimize the existing framework's method. compared with the other serialization methods integrated by hadoop and spark, protobuf performs better and better in data processing as the number of serialized objects increases. compared with java defaults, protobuf takes only 7% of the time to process data. at same time, our data platform has many data analysis functions including machine-learning, finite element analysis, optimization and knowledge mapping."
"to support the correlation between structures in 3d scenes and 2d slices, structures can be visualized in 2d slice data as colored and semitransparent overlays, so the underlying gray values are still visible. if more than one structure should be displayed at the same voxel position, the combined color can be calculated in different ways:"
"both sqoop and flume are the main framework for data acquisition modules [cit] . the sqoop is used for data interaction between hadoop and relational database, and imports relational database data into hadoop distributed file storage system (hdfs) [cit] . as a data acquisition framework, the flume supports the development of various data senders in the log file system to acquire their data, which simply processes the data and then stores the processed data in hdfs, hive, or turns it into a producer of kafka."
"all objects hit by the pick ray are determined and sorted by the depth distance of their intersection point. only objects that are visible by at least 10 percent at the intersection point are taken into account. as a next step, the size of the projected bounding box on the viewport of all objects and the transparency degree of the single objects are determined. the impact of transparency and the projected bounding box size is adjustable to consider different types of scenes (e.g., scenes with objects of rather equal size). the object with the highest rating is selected at the end. thus, for example, opaque structures behind structures with a strong transparency are selected."
"in table 1, for four cases from the scenarios described in section 2, the number of segmented structures and the number of labels needed in an mcsm are opposed. for the 81 segmented structures of the living liver donor transplantation, 11 bits can be used instead of 81 bits."
"camera paths. to produce appealing movements of the camera from one viewpoint to another, we developed and integrated a set of path algorithms in the metk. to preserve the orientation on long distance, movements of the camera, we first zoom out to a global view on the scene and zoom into the target structure at the end of the flight. we also make camera movements more appealing by slow acceleration at the beginning and at the end instead of abrupt speed changes."
"with the metk it took less than 1 minute to create the application network consisting of only six metk modules. additional 5 minutes were used to write the script for the application layout, and it took another 8 minutes to import the data into the application."
"defaultcodec is the default compression method for java. at present, lzo is the method integrated into hadoop. in figure 6 (a), five data compression methods are tested with data size of 13210 kb. the size of data compressed by lzo is obviously larger than that compressed by the other four methods. specifically, the size of data compressed by lz4 is 96% of that compressed by lzo. moreover, the size of data compressed by the other three methods are 91.5% of that compressed by lzo. the size of data compressed by lz4 is slightly bigger than that compressed by deflatecodec, gzipcodec and defaultcodec."
"illustrative visualizations. it became apparent that applying transparency is not sufficient to visualize complex structures. in particular, if the density of anatomic structures is high, illustrative techniques are employed to better convey object shapes and relations. for that reason, illustrative visualization techniques were developed. illustrative visualization was found to be useful for selected therapy planning tasks, e.g., hatching lines convey surface shape better compared to conventional shading for radiation treatment planning [cit] . the application of silhouettes to strongly transparent structures increases the recognizability. the use of local transparency was also promising (e.g., cut aways or ghosting [cit] ). these illustrative techniques are provided in the metk and can be flexibly combined with surface and volume rendering. silhouette rendering is the default style in the necksurgeryplanner [cit], which is used in the clinical routine and appreciated by medical doctors for providing adequate support. however, so far there is no evidence for an advantage of stippling and hatching in surgical planning."
"graphical application interface. for a fast and efficient application development, predefined widgets for common and recurrent tasks are provided, e.g., lists to select structures or to change their visibility. we provide panels to change the visualization parameters of structures, such as color, transparency, or silhouette width, efficiently."
"automatic object selection. we provide several new techniques to select objects in 3d scenes with many objects of different transparencies. in such scenes, the selection is ambiguous, if there is more than one object in the picking ray. the simplest approach to disambiguate the selection is always to select the first object in the pick ray. however, if this object has a strong transparency, the user probably intends to select another object behind. in complex medical scenes, some objects are completely enclosed by others, so they are never the first object in the pick ray. for example, in liver surgery, the liver tissue always encloses nearly all intrahepatic structures, e.g., vessels and tumors (see fig. 6a )."
"the design of the metk is guided by experiences in several fields of surgical application development. thus, we present conceptual considerations and selected application scenarios, and derive requirements for surgical applications."
"for all scenarios, the exploration of the data set must be as fast as possible in the clinical routine. presentations are essential for collaborative intervention planning such as tumor board discussions, where a complex case is presented by one medical doctor to initiate an interdisciplinary discussion to finally come to treatment decisions, e.g., a combination of radiation treatment planning and chemotherapy or a chemotherapy to downsize a tumor preoperatively."
"object manipulation. even if users adjust the appearance of a visualization globally by selecting a preset, they might want to adapt the appearance of single structures individually. we provide some gui widgets that can be integrated in a panel or window to adjust all visualization parameters such as color, transparency, or silhouette width. in addition, we provide an exploration technique, where the user can easily adjust the most important parameters directly in the viewer (see fig. 7b ). the provided list of parameters can be adapted and extended for individual application requirements."
"scenario 2. in abdominal surgery, the resection of tumors in the liver, kidney, or pancreas is rather similar with respect to the demands of software support. here, a tumor or several metastases need to be resected with a specific safety margin. in difficult cases (e.g., a tumor in a central or dorsal position, where frontal access is severely limited), this intervention requires in-depth computer-based operation planning. the tumor and especially the surrounding vessels must be carefully inspected in 2d as well as in 3d. the remaining liver volume must be calculated with respect to vessel supply and drainage. the results need to be adequately visualized in 3d as well as in 2d [cit] ."
recent developments with user interface devices used in surgical applications [cit] necessitate the integration of a wider variety of input devices in the metk. those extensions require the integration of device drivers in the system as well as a refinement of interaction and exploration techniques. fig. 11 . liversurgerytrainer. the liversurgerytrainer is an application to teach abdominal surgeons the planning workflow for liver resection and living liver donor transplantations. the application layout contains only a few widgets.
"key states. for presentation purposes, interdisciplinary discussions, or patient consultation, several views and visualizations of the explored data need to be saved. instead of only saving screenshots, we employ key states, which store all information about a scene and its visualization. this includes camera parameters as well as visualization properties. thus, a complete state of a visualization can later be restored for further explorations or demonstrations. since key states are stored in the case data, they can be transferred from one application (e.g., a surgical planning software) to another (e.g., an application for patient consultation). naturally, key states can also be exported as screenshots for usage in documents or presentations. usually, a surgeon creates a couple of key states during a planning process (see fig. 5 ). in combination with animation facilities, videos can be created automatically from a set of key states, where smooth transitions between the key states are computed. these videos, for example, are used to teach other surgeons. key states can also be used to define presets. applying a once-defined key state to a new case with similar structures, these structures are visualized with the same properties."
"-fifth, hive and hbase are integrated into the built server logical file system. setting table and columnfamily. serializer used the optimal protobuf method. sink is hbasesink. by some of the above operations, flume and hbase are integrated."
"scenario 3. in spine surgery, small changes of the spine's anatomy can evoke symptomatic disorders for the patient. hence, the spine surgeon must inspect the spatial relations between nerval and spinal structures as well as the relation of the spine to surrounding muscles, vessels, and glands. the surgeons need to place virtual needles and implants in the spine region to plan different strategies for the access route in the later intervention. dedicated 3d visualizations can help the planning surgeons to locate such access routes without injuring important structures [cit] ."
"the data acquisition method of sqoop relational database is shown in figure 3 . after receiving the shell command or java api command from the client, the sqoop converts the command to the corresponding mapreduce task by task translator in sqoop, and then transfers the data from relational database to hadoop to complete the copy of the data. the sqoop operates on relational databases by four ways. first of all, the sqoop gets data from all mysql databases. then sqoop sends these data to hdfs. next, sqoop imports data from mysql to hive. last, sqoop exports data from hive to mysql."
"generally, big data platform include three modules, which are data acquisition module, data storage module and data computation module [cit] . the data acquisition module provides a data source for data analysis of the big data platform, and the data storage module provides data source and storage space of the data computation module. it provides data calculation, machine learning, graph analysis, data query, which is the core component of data analysis. the computation module computes massive amounts of data, mines useful data value information, and provides decision-making grounds for industrial decision makers."
"interaction support of object selection. the algorithm reveals its limitations when an opaque structure lies behind a semitransparent structure, while their projections are almost equal (see fig. 6b ). in such cases, at all points where the user picks the transparent structure, the opaque one in the background is selected. therefore, we provide two interaction techniques in the metk. the first allows the user to scroll between all structures in the pick ray, using the mouse wheel or the cursor keys. starting with the structure that the automatic algorithm would choose, the user can scroll back and forth between objects adjacent in depth. the cso will be clearly emphasized, using a thick silhouette and an opaque color. the second selection technique offers a list of all structures in the pick ray directly at the cursor position in a small panel, so the user does not need to de-focus from the scene (see fig. 7a ). we extend the list of textual structure names by pictorial representations of the structures. however, this technique aims at experienced users who know all structures by name."
"konrad mü [cit] from the university of magdeburg, germany, and is currently a phd student. he is working as a researcher and software developer at the department of simulation and graphics at the university of magdeburg. his research interests include medical animations, user interface design, educational software, and software engineering methods to facilitate a fast development of clinically useful applications. he is a student member of the ieee and the ieee computer society."
"communication. besides the data management, the metk provides a communication structure between all modules. events can be sent between specific modules for a direct intermodule communication and be broadcasted to reach all modules. therefore, all changes of underlying data and parameters are communicated to all modules \"listening\" to those parameters, so they can adjust their own parameters, data, and visualizations. this leads to identical visual properties of all structures in all viewers and widgets, and thus to a consistent view of all data."
"in general, surgeons are medical experts, usually with only modest computer experience. they benefit from faithful spatial renditions of the patient's individual anatomy, but they usually have no special abilities to explore and handle 3d data. from our experience with surgical applications, we derive basic requirements for such applications:"
"feedback from surgeons clearly revealed that a rather low level of flexibility is needed and guidance is considered essential. surgeons prefer clear and easy to understand interfaces [cit] instead of interfaces with many parameter sliders and value inputs. they want to get a good visualization for the current task or medical question automatically, or at the utmost selecting a well-defined preset from a small list of choices. the metk supports this, e.g., with key states and the animation facilities. furthermore, ready designed graphical interfaces for surgical applications are provided as templates. the interfaces were gathered by many interviews with our medical partners and approved by evaluation [cit] ."
"the data management and communication layer contains functions for case data management, interapplication communication, and generation of 3d polygonal surfaces. since dicom is widely used and standardized, the metk is focused on processing dicom data. additionally, segmentation masks can be loaded to automatically generate 3d polygonal surfaces. this operation must only be performed once, since the surfaces are stored for further loadings of a case. depending on the type of structures, different algorithms for generating the polygonal surfaces are used depending on the metadata, acquired during the segmentation. in most cases, marching cubes in combination with a surface smoothing is applied. for vascular structures we use a model-based surface reconstruction that respects the thin and branching structure of vessel trees [cit] . besides segmentation masks, polygonal meshes of structures and secondary objects (e.g., medical probes) can be imported."
"botha and post [cit] recently introduced a development environment for fast visualization prototyping, the devide system, which provides substantial capabilities in accessing and changing code and underlying data-flow networks during runtime. devide supports the development of new visualization and segmentation techniques well."
"synchronization. moreover, the currently selected object (cso) is automatically communicated in the metk. hence, a synchronized view in different viewers can be provided. if the user selects a structure in a 3d viewer, all 2d viewers can display the suitable slice for this structure and vice versa. if the user picks a structure in a 2d slice, it can be emphasized in all 3d viewers, moving the camera automatically to a good viewpoint on this structure (see section 4.3)."
"for volume rendering, the metk employs the mevislab gigavoxelrenderer [cit] . it enables the tagged volume rendering of segmented structures. thus, different structures can be visualized with local transfer functions. for the sake of consistency, the colors of structures are the same as for their surface visualization. to visualize unsegmented tissue, a global transfer function can be applied and the volume rendering can be combined with the polygonal surface visualizations."
"to extend the metk or to supplement existing functions, developers can refine existing metk modules or create new modules. depending on the complexity of extensions, there are basically two options: developers can implement simple functions, e.g., a patient data management or widget panels in modules, written in python. advanced and especially performance critical issues can be implemented in c++ libraries. since this is only necessary for special visualization techniques not incorporated in the metk so far, such as dti visualization, this does not contradict the supposed low programming skills that are needed to build up readyto-use applications with the metk. in fig. 8, we illustrate the steps that are necessary to build a new metk module that visualizes the cso with its name and anatomical affiliation. this module can, for example, be integrated as part of a larger application."
"since with the basic version of mevislab for each case a network must be created manually, it took 51 minutes to add all 148 modules for the sample case and parameterize them individually. writing the script took 8 minutes. this process of about 1 hour needs to be repeated for every new case. the required time primarily depends on the number of segmented structures."
"the data storage module mainly uses the hdfs [cit] . hive [cit], shark [cit] and spark sql [cit] support data operations by structured query language (sql), but the efficiency of the three operations is increasing in steps. spark sql is about two orders of magnitude more efficient than hive."
"s oftware assistants for intervention planning, e.g., for surgery, interventional radiology, or radiation treatment planning, are a relatively recent development. surgical applications have special demands on visualization and interaction. it is not sufficient to display and analyze slice data and to create volume-rendered images. instead, an in-depth analysis of the image data needs to be supported with appropriate 3d interaction techniques and advanced visualization techniques. with the medi-calexplorationtoolkit (metk) we present a widely applicable library for application development that closes the gap between image analysis, processing, and basic visualizations on the one hand, and the surgical needs concerning visualization and interaction on the other hand. the metk is based on the image processing and development environment mevislab [cit] ."
"there are many forms of industrial data distribution, so the data acquisition module of this platform adopts sqoop and flume. the sqoop acquires a lot of data stored in relational databases in the industry, and the real-time requirement of these data is not high. and the flume acquires real-time data acquired by the intelligent equipments. in the industry, dynamic data, like that of safety monitoring and that of equipment operation status, is acquired by flume. and then these data is transmitted to the message queue of kafka. the kafka becomes the provider of real-time consumption of dynamic data for spark streaming and structured streaming. some managers of the plant need to query historical data, so the data acquired from flume needs to be stored in hbase or hdfs. yet another resource negotiator (yarn) is adopted by the resource scheduling framework, it supports all computing frameworks and facilitates integration of spark and flink. the computing framework uses spark sql for batch processing, spark streaming for micro-batch processing, structured streaming and flink for stream processing. the results are encapsulated by web services in the service layer, and then the encapsulated data is transmitted to the display layer which integrates echart, h5 and hue by django. then the user can get the data analysis results by the display interface. in addition, zookeeper completes the collaborative management and high availability patterns of some distributed big data frameworks of the whole industrial data platform."
"in this paper, we start with the process of industrial data processing and the requirement of industrial data analysis. high availability file stored system is built to avoid single point failure. in the process of building file stored system, the performance of lzo provided by the framework is inferior to lz4 in the time and the size, and the time spent by lz4 is only 36% of that spent by lzo. the size of data compressed by lz4 is 96% of that compressed by lzo."
"a challenging problem, if not the most challenging problem in computer-aided surgery, is a precise understanding of requirements and needs, priorities and relations between them. it turns out that fast prototyping and discussions of initial suggestions are essential for this process. surgical departments often are too small to include people with substantial it background. furthermore, specific features of new applications are hard to describe verbally. medical doctors are often not aware of the available options. if application prototypes are available in an early stage, the discussions with the users amount to substantial input for the development process. this input is crucial in application development, since the wishes, aims, and experiences are rarely apparent after a first specification of the application. therefore, we follow a process where we offer different solutions to the medical doctors and ask them to comment on these solutions. to come up very quickly with such prototypes was one of our central requirements during the development of the metk. a second major goal was to enable that once developed visualization and interaction techniques are easily reused in a larger department with many projects. providing a broad common base of techniques, in particular management of image data and related information to a particular case, enables researchers to focus on new aspects and current applications. this involves a large investment in software design but pays off later. from the first project on, the metk significantly enhanced the reuse of newly developed techniques. two major developers from different projects guided and drove this process. the application of the metk by a broader user group lead to a refinement of the requirements and provided input for technical challenges."
"for a toolkit for application development it is crucial to provide a modular character, where every application can just use the modules it needs. extendibility is a second major aspect, since the toolkit may be used for a large variety of applications with specific requirements. to guarantee the extendibility, it is important to design an open and extendable structure for communication and data management, where new modules are able to instantly communicate with all other existing modules."
"1. the metk provides advanced visualization techniques that fit especially the requirements of surgical planning, such as safety margins, vessel visualization, or extensions for 2d viewers. 2. animation facilities are provided that enable the usage of animations as prerendered videos or as a support for interactive exploration. 3. the metk provides a case management that enables applications to load and save collections of many segmented structures, including the image data and further information about the patient."
"we developed a procedure to automatically select an object after the user has clicked on the scene. it is assumed that the user points the mouse consciously. that means, when the mouse cursor is placed over a very large and a very small object, the user placed it deliberately over the small object. furthermore, it is assumed that the perception of strongly transparent objects appears less prominent than the perception of more opaque objects. to identify the desired object, the algorithm proceeds as follows (algorithm 1)."
"the structure of this paper is as follows. in section i, we introduced the motivation of this research. in section ii, we introduce the relevant background. we build the data platform for industrial data analysis based on big data in section iii. then, we show some results of the acquisition and processing system in section iv. in section v, we discuss the conclusions of this research."
"case and cache management. to reduce the memory consumption and to speed up the whole process of loading and exploring a case, we integrated an efficient case management in the metk. each structure and each image stack is only loaded once and distributed virtually in the application network. even if the structures are visualized with different techniques in different viewers (see fig. 10 ), it will only be maintained once in the application cache."
"based on the surfaces stored in the cache, a material is assigned to each structure to achieve appealing surface visualizations. important structures are visualized with a high opacity. for structures which serve as anatomic context, e.g., organs or large skeletal structures, we provide silhouette rendering. thus, they are still visible but do not hide the view onto other important structures."
"computing system needs to support multiple types of data analysis and data visualization services, so the data computing module of the industrial big data platform needs to support a variety of data processing methods. therefore, we evaluate four high-performance data processing frameworks. as we know, the spark is the third-generation computing framework, whose performance is high and reliable. the flink is called the fourth-generation computing framework and has high performance. considering the functionality and the data processing performance of different computing frameworks, we finally chose spark and flink. in table 2, we can see that the spark and the flink support more functions of data analysis than the other two frameworks. computing model of big data computing framework includes stateful and stateless data computation. stateless computation is used for real-time environmental monitoring. intelligent plant often need to accumulate statistics, such as energy consumption, equipment costs, staff working hours, etc, during the process of industrial production. the general computation method can only compute the current data, which is stateless computation. in this case, we need to use big data window mechanism to implement stateful computation."
"in recent years, spark and flink have been well studied and applied on the internet. the mapreduce is a computing framework used by most industrial enterprises at present. iterative computing is involved in many aspects of industrial data analysis, which is used to seek optimal control and management solutions. however, the mapreduce framework is ineffective in iterative computing and the performance of the spark framework has some obvious advantages over the mapreduce [cit] . there have been many hot issues and difficult problems in the research of industrial big data platforms: reducing the data processing time and the size of space for big data acquisition and storage, optimizing the performance of computing framework, providing a wealth of function, improving system stability."
"there are a large number of data analysis requirements in the plant. the data platform needs to support multiple types of data analysis and data visualization services, such as finite element analysis [cit], optimization, deep learning [cit], knowledge mapping [cit], digital twins [cit] ."
"to assess the effectiveness of the metk in comparison to current application development, we implemented a small reference application in both the standard version of mevislab and mevislab extended with the metk. the application should load a given ct image and multiple segmentation masks, and visualize them as 3d surfaces and as 2d images. in 3d, the structures should be visualized with their standardized style and their visibility (on/off) should be changed individually. after the description of the particular development process, we will compare both solutions by means of development time, resulting application, and usability. for the solution with the basic mevislab version, all segmentation masks must be loaded manually. afterwards, modules to create the surface for each single structure must be added and parameterized individually. to achieve a correct visualization style, the structures' nodes in the network must be categorized (e.g., vessel or muscle) and a material (color and transparency) must be attached to each group. the scene is visualized in a simple 3d viewer. for the 2d visualization, the image is added to the network and visualized in a 2d viewer. the viewers are put together in one application interface by writing a script that defines position and extension of each viewer. to switch the visibility of each structure, each visibility parameter of every structure must be added manually to the script. this process must be performed again for every case, since only the network can be saved and no case data management is available."
"qilin liu received the b.s. [cit] . he is currently pursuing the m.s. degree in control engineering with the chongqing university of posts and telecommunications, chongqing, china. his research interests include semantic web, the internet of things technology, and big data."
"recently, there are a number of studies on data acquisition and optimization of computing framework methods, and most of these methods provided good performance in the field of internet. we consider the characteristic of real-time and stability in industrial production in our research. therefore, data platform optimization includes data acquisition, data compression, data serialization, and system stability."
"a c++ toolkit specializing in intraoperative support is the image-guided surgery toolkit (igstk) [cit] . it supports the development of applications for interventional radiology procedures and image-guided surgery, where external tracking devices are applied."
"-third, the data structure of intelligent equipment is analyzed. setting topic and docker. equipment data acquisition location is specified. flume and kakfa are integrated. -fourth, hadoop is integrated into the server cluster by hdfs and yarn of the configuration files, and adding packages optimized of java to library files to configure to use."
"in order to make the test data more intuitive, we draw a broken line graph and draw a broken line graph every three orders of magnitude. the broken line graph of the data in table 3 is shown in figure 7 . kryo can serialize faster with fewer objects in figure 7 (a). kryo, protobuf and protostuff changed significantly when the processing object was about 10000. as the number of objects processed increases, processing time increases sharply. however, protobuf and protostuff show a downward trend with the increase of processing object data in figure 7 (a). when 10 million objects are processed, kryo's performance is very poor and protostuff's and protobuf's have very similar performance in figure 7 (b). protobuf performs better than protostuff when the amount of data processed is more than 100 million in figure 7 (c). by evaluating above five data serialization methods, we finally choose protobuf."
"both the window mechanism of spark and that of flink support common protocol mechanism and incremental protocol mechanism. all data blocks in the window are computed by the general protocol mechanism, even if the data has been computed. incremental protocol only needs to consider the data entering and leaving windows. the general protocol is shown in figure 5 (a) and the incremental protocol is shown in figure 5 (b). we can see that the size of window is n and the size of sliding step is m. n must be bigger than m in the normal operation of the system. the sampling interval is usually small in industrial data monitoring, such as one second or less. according to different industrial production requirements, the period of data acquisition includes minute, hour, month and year. n is the number of data per partition. so n -1 is the computation times in each partition. the number of sliding times of a conventional protocol is as follows: (n -1) * n + n. the number of sliding times of incremental protocol is as follows: (n -1) * 2 * m + 2 * m + 1. therefore, when m is less than half of n, the state computation adopts the common protocol. in contrast, the state computation adopts the incremental protocol. when m is equal to half of n, we can choose either of them."
"another framework for medical image analysis is cavass [cit] . it provides simple volume rendering and surface visualizations, but no dedicated support for application development, since cavass is a fixed system that is only extendable by its open source interface."
"between all modules of an application. thus, changes in one part of an application may directly affect other parts. 5. the metk provides special facilities to save different states of a visualization for later reuse. 6. to support fast application development, many user interface widgets are available in the metk, e.g., structure lists and interface templates. 7. to use the metk and to build up own ready-to-use applications, no extended programming skills are needed. almost all tasks can be solved using script languages like python."
"to bridge the gap between preoperative planning software and intraoperative usage of the planning results is a challenge for future work. the adaption of techniques such as the automatic viewpoint selection for an intraoperative use would be a useful extension, since more guidance in 3d exploration is needed there due to the particular surroundings."
"the system transmits data by three channels. firstly, data flow is a timeless computing module consisting of flume, kafka, structure streaming and flink. secondly, data flow is a data query module consisting of flume, hbase, spark sql and table. thirdly, data flow is a visual analysis module consisting of flume, hbase, spark streaming and flink."
"our experiences with automatic techniques such as the object and viewpoint selection showed that more semantic information about the importance of structures and their relations into the visualizations needs to be integrated. therewith, the presented context to structures of interest or the intended user interactions can be adjusted in a more appropriate manner."
"as the core of the new generation of information technology and industrial development, industrial big data is deeply affecting all aspects of the whole industry chain, such as r&d, manufacturing, operation management and sales service of china's manufacturing industry [cit] . in the future, it will promote the transformation and promotion of traditional manufacturing. [cit] ''."
"advanced 2d visualizations. the basic problem of the slice-based visualization, namely, the lack of an overview in cross-sectional images, has been tackled with a 2.5d approach to provide the essential information, the so-called liftchart [cit] . using this technique, the range of slices that a specific structure spans over can be quickly seen. a narrow frame attached next to the cross-sectional image represents the overall extent of slices in the volume data set. the top and bottom boundary of the frame correspond to the top and bottom slice of the data set (see fig. 4 ). each segmented structure is displayed as a bar at the equivalent vertical position inside this frame. upper bars correspond to higher structures in the body. different arrangements of the bars are possible, e.g., condensing all structures of the same type into one column. the currently displayed slice of the volume data set is depicted by a horizontal line in the liftchart widget (see fig. 4 )."
"as we know, the data compressed time and size of the space compressed should be considered during the selection of compression methods. large disc capacity means high cost of disk, compared with real-time analysis of industrial data, however, timeliness is more important than the cost of disk. volume 7, 2019 furthermore, the timeliness is one of the most important indicators in industrial production, such as intelligent plant safety monitoring and equipment failure diagnosis. therefore, we give priority to the data compressed time."
"the data serialized method of the acquisition framework is upgraded. the specific acquisition scheme is as follows. the module for flume data acquisition is shown in figure 2 . the information configured of flume requires three parts: source, channel, and sink. the source receives the event from the server or log file and puts it into the channel in bulk. data transmission of flume is divided into two data transmission channels, so you need to set kafkachannel, hbase-channel, kafkasink and hbasesink. the information configured of sources requires to configure type, channel, internet protocol address (ip), port and selector type. the specific configuration information is shown in table 1 ."
"nearly all surgical applications need an efficient case management, advanced 3d visualizations of segmented structures, and guidance for their exploration. measurement facilities are less important for patient's consultation, while resection techniques are primarily necessary for abdominal surgery planning. thus, a modular approach is necessary where only required features are integrated in an application."
"-sixth, spark and flink are integrated into the built server logical file system. adding java packages optimized to library files to configure to use. -seventh, hue is integrated, at the same time, industrial data visualization front-end is designed by django frame-work."
"five methods of data compression take significantly different time to process the same amount of data as shown in figure 6 (b). lz4 takes far less time than the other four methods. specifically, the time spent by lz4 is only 36.1% of that spent by lzo. furthermore, lz4 takes less than 10% of the time that deflatecodec, gzipcodec and defaultcodec do."
"visual computing in surgical applications has to provide comprehensive patient-related information, including visualizations of the relevant anatomical and pathological structures, and enabling a faithful representation of the area around the pathologies. moreover, measurements, annotations, resection lines, and other information may be important to directly support preinterventional decisions. on the one hand, flexible visualization and interaction are needed to cope with the peculiarities of individual cases, but on the other hand, strong guidance is desirable to avoid that surgeons are overwhelmed by these facilities. most of this information, e.g., measurements of the structure's extent, can only be derived after the segmentation of relevant structures. further analyses such as infiltration of and distances between structures as well as safety margins and volumes of structures can only be performed after segmentation."
"there are massive multi-source heterogeneous data inside and outside the plant [cit] . the data platform needs to connect multiple types of data in equipments, production lines, products and industrial software. at present, industrial data lack effective interoperability standards or low-cost acquisition schemes. this leads to high costs of data collection and integration of equipments and systems. thus, it is difficult to support functions of industrial data analysis and application development."
"using the metk, we first add a manager module that enables an application to load and save cases, and provides the communication functions for the whole application. a second module provides the import facilities for the slices and segmentation masks. for the 3d visualization, the module for surfaces visualization and an metk 3d viewer are added. for 2d slice visualization, an metk module for image loading and an metk 2d viewer are added. afterwards, the script to arrange the widgets in the application window is written, whereas a special metk list widget is integrated for fast visibility changes of single structures. after executing the application, a new case can be created by importing an image and segmentation masks located in the same directory. for each structure, its type and anatomical affiliation can be entered. this is only necessary if the case was not segmented with an metk compatible application as mentioned in section 1. for later fig. 9 . sample metk application networks. (a) a network to present segmented data sets in a 3d viewer with a gui widget to change visibility of structures. a spine surgery data set is displayed. (b) a network of an application to synchronously explore 3d and 2d data, extended with silhouettes and a colored distance transformation in 2d and 3d. a neck surgery data set is displayed. fig. 10 . necksurgeryplanner. the necksurgeryplanner supports the operation planning for neck dissections. to provide deep insight in the original 2d data as well as in the segmented 3d structures, 3d and 2d views are used synchronized. on the left, a browser for enabling and disabling structures and key state previews are provided. reuse, the case can be saved, so this procedure must be performed only once."
"there are many types of computing frameworks in the big data ecosystem, including mapreduce, spark, storm, flink and so on [cit] . the mapreduce is almost eliminated because of requiring constant serialization, which seriously wastes computing system resources. moreover, many platforms for industrial big data are developed based on mapreduce computing framework. obviously, the performance of these data platform is inefficient. the spark, however, is the most active framework for the big data ecosystem. in addition, the spark is based on memory, and it can automatically adjust the memory usage. when the memory is insufficient, it will automatically transfer the location of data computation from memory to disk. the speed at disk computing is also nearly 10 times faster than mapreduce [cit] . furthermore, marhout's r&d team is no longer maintaining and perfecting for mapreduce, but the machine-learning module needs to be updated constantly in the field of industrial data analysis. therefore, in this study, the spark computing framework is used instead of the hadoop computing one."
"scenario 1. neck dissections are carried out for patients with malignant tumors in the neck or head region to remove lymph node metastases. depending on the broadening of enlarged lymph nodes, only a few of them or a large part of the neck including muscles must be resected. a large number of structures have to be taken into account (e.g., vessels, muscles, and up to 60 lymph nodes). the surgeons must explore the distances of all larger lymph nodes to vital structures of risk in order to judge if there is enough space to safely resect them individually [cit] ."
"2. measurement capabilities must be provided to support, e.g., distance, volume, and angle measurements, since these measurements are often closely related to surgical decisions. 3. due to the importance of 2d slice data, 2d and 3d views of data should be coherent and synchronized, while the exploration of 3d data must be supported in particular. 4. important anatomic structures need to be emphasized, preserving the context. 5. dedicated techniques for special surgical fields should be provided (e.g., resection techniques for abdominal surgery, dti visualization for neurosurgery, and multimodal data visualization for cardiac surgery), while many techniques are usable for several fields. 6. in many areas, such as web applications and learning environments, it is essential that systems are not only easy to use, but also are perceived as motivating and appealing which is often summarized as providing a convenient user experience. surgeons nowadays use highly appealing interfaces designed by educated visual designers, thus expecting an appealing user interface [cit], even from research prototypes. case management. one further requirement is an efficient case management. we define a case as a single data set (e.g., an mri scan), or a collection of data sets, relating to one patient, with additional data such as segmentation information, information about the type of structures, and information about the patient. this information is necessary to provide sophisticated and adapted visualizations. for every application, different standard and default visualization styles may be available. these styles must be identified in many interviews with potential users and medical experts [cit] ."
"data compression and serialization perform on a windows 7 64-bits pc equipped with an intel (r) core (tm) i5-8400 cpu @2.8ghz 2.81ghz processor, and 4gb-ram. three same computers are used in the overall test of the data platform."
"a weighted mixture of all colors of the overlapping structures is calculated. 3. application-dependent overlapping regions can be emphasized separately in dependency of involved structures, e.g., the infiltration of lymph nodes in a muscle can be colored red with a silhouette, even if this visualization style does not appear in one of the two structures. the calculation of the overlays is performed based on our multicoded segmentation masks."
"with the development of intelligent plant, the data acquisition volume of plants increases rapidly. flume can meet the needs of data acquisition in plants. in the research of the method of data stored, we compress data by lz4 and serialize data by protobuf. compared with the original method of hadoop integration, the time of processing data is greatly reduced and the processing effect is better. both spark and flink are used in computing framework. according to the size of sliding window, we adopt appropriate window mechanism to compute the data for certain time periods. furthermore, the selection method is given."
"from the beginning until the ready visualization of the data it took 14 minutes with the metk and 59 minutes with the basic version of mevislab. for new cases (that only consist of images and segmentation masks and come without any metk compatible metadata), with the metk it took only the time of importing the data, whereas with the free mevislab version the whole network would have to be recreated."
"medical viewers. the visualization layer also provides several viewers that consist of wrapped and extended mevislab viewers. the extended viewers are able to communicate their parameters (e.g., camera position and orientation) to other metk modules and can receive commands, e.g., to control the viewers remotely by the animation system. 2d viewers can display slices in many ways: singular or in a multislice view, where axial, sagittal, and coronal as well as free multiplanar reformations can be shown. 3d and 2d viewers can be freely combined in an arbitrary number and arrangements."
"measurement tools [cit] are also integrated, e.g., for distance measurements and its appropriate visualization by means of arrows. the proposed measurement tools are extended by automatic measurement facilities for computing minimal distances between two structures and by calculating a structure's volume."
"an mcsm contains all segmentation masks of all structures of a case. each combination of labels of a voxel that appears in the data is encoded with a distinct voxel value (see fig. 3 ). for example, all voxels simultaneously belonging to the liver tissue and the hepatic vein (and to no other structure) are assigned one unique voxel value. the mapping of voxel values to structure lists is stored separately in the case data. an mcsm is created by sequentially adding one segmentation mask after another. if a new combination of voxel labels occurs at a voxel position, a new number is assigned to this combination. after all single segmentation masks were added to the mcsm, it can be used, e.g., as an efficient base for colored overlays. the upper bound of 2 64 labels will never be reached with medical data sets, since even the theoretical case that in a data set of 512 3 voxels each voxel represents another combination of structures is covered by the mcsm."
"undo. one important feature, especially for surgeons who are inexperienced in 3d exploration, is an undo function for 3d scene manipulations. in the metk, after every performed action (e.g., a camera movement or a visualization change) the whole scene is stored in a key state. changes performed in a very narrow time range (e.g., automatic changes of the visualization) are combined in one key state. the user can return to arbitrary steps."
"in the medicalexplorationtoolkit, each function is encapsulated in a module. using mevislab's visual programming environment, modules can be freely combined in a data-flow network to build up applications with an individual feature profile. this allows the developer to design applications that support the specific workflow of different surgical intervention planning processes in an efficient and fast manner."
"there are more than 20 kinds of methods for the evaluation of serialization. among them, several methods, such as protobuf, kryo, protostuff, fasterxml, jackson and java default method, are selected to evaluate the serialization. in this paper, we mainly evaluate the timeliness of the data serialized method. except for the java default method, other methods have no significant difference in the size of the space including the memory space and the storage space. because of the poor performance of jdk, it is just as a simple reference."
"according to industrial data acquisition and processing requirements, this paper designs an industrial big data platform. the overall framework of the industrial data platform is shown in figure 1 . the data platform includes six layers in terms of data flow. these six layers are device layer, acquisition layer, storage layer, computing layer, service layer and display layer, which correspond in turn to data acquisition, data storage, data analysis, service package and front end of industrial data. this study focuses on the acquisition layer, the storage layer and the computing layer."
"visualizations based on presegmented structures are mandatory for operation planning in many fields, due to a high density of soft tissue structures with overlapping image intensity values. thus, operation planning in the abdominal region (e.g., liver, pancreas, or kidney), the neck region, and the orthopedic interventions is preferably performed using segmented surfaces in combination with the original 2d slices, while in neuro surgery or in emergency cases, volume rendering of the original image data is preferred. the metk development focuses on segmentation-based visualizations but does not support the segmentation process itself, since this process is supported in the underlying mevislab. furthermore, several applications (e.g., hepavision for liver surgery [cit] or neckvision for neck surgery [cit] ) and service providers [cit] are available to perform this task. the metk can import dicom segmentation masks as well as polygonal meshes of structures (e.g., in open inventor or stl format)."
"the open source and well-documented data interface enable developers to easily extend the metk by new modules. all these techniques and facilities are not available in mevislab. in comparison to other toolkits, only a very few of these techniques are available in different systems, e.g., illustrative techniques in volumeshop [cit] ."
"larger numbers of labels for liver cases result from a more frequent overlapping of structures. 10 mm) turned out to be appropriate. the distances may be displayed in 2d as well as in 3d. in the 2d view, silhouette lines visualize the important distances (see fig. 4a ). in 3d, unicolored surfaces are drawn on structures visualizing the range of distance to critical structures, e.g., lymph nodes to vessels and muscles (see fig. 4b )."
"hadoop is adopted by the data storage framework. both file storage and inter-cluster communication on hdfs require data compression. the former can increase the amount of industrial data stored and reduce the cost. and the latter can reduce the bandwidth consumption of data transmission between networks and reduce data transmission time. as we know, industrial big data platform needs high stability. therefore, all the distributed frameworks integrated by the data platform are always adopted high availability mode. in the paper, the high availability data stored system of hdfs and hbase is built by zookeeper. figure 4 shows the high availability framework of hdfs. the high availability mode of hbase is similar to that of hdfs. besides, yarn in the later computing framework also uses the same way layout. in figure 4, we can see two namenode (nn) and three datanode (dn) in the data platform because all versions of hadoop2 only support two high-availability modes for nn. the client obtains meta information through the nn when the data is written to the dn specified by the metadata. we select lz4 as the data compression method by evaluating various methods."
"multimodal data. as the underlying mevislab, the metk has the abilities to load multiple modalities like scans from ct, mri, or pet. the visualization techniques are able to combine different registered data volumes, e.g., an ct and pet scan. to perform the necessary registration between the different data sets, mevislab provides a wide range of capabilities in its free version. thus, they are no integral part of the metk. however, the metk provides an interface to use the multimodal capabilities of mevislab in the context of the metk techniques."
"the data platform includes three parts: data acquisition module, data computation module and data presentation module. it takes eight steps to set up a data acquisition, storage, computation and visualization of the platform. the display interface of this platform is shown in figure 8 ."
"-eighth, the data platform is tested as a whole. figure 8 (a) is a real-time monitoring interface. monitoring information includes plant temperature, humidity and pm2.5. three kinds of information are acquired per second. the monitoring interface displays information for a time span of 20s to prevent monitoring personnel from missing data. figure 8 (b) is visualization of energy consumption, equipments performance, equipment status and production efficiency after data computation and data mining."
"good viewpoints are employed in the metk to generate standardized views for documentation (in combination with other standardized visualization parameters). if the user picks a structure from a list or from the viewer, the camera can be automatically moved to the best viewpoint of the structure."
"processing a large quantity of data that cannot be processed by ordinary infrastructures. industrial infrastruct-ure faces a large number of problems, including challenges such as defining different efficient architecture settings for different applications and defining specific models for industrial analysis."
"for the fast generation of visualizations, [cit] presented vistrails. vistrails is a publicly available pipelinebased environment, where visualizations for many fields of use, e.g., time-varying or diffusion tensor data, can be created. the system also provides the ability to reuse several parts of previous visualization pipelines using visualization by analogy [cit] . vistrails focuses on the generation and exploration of data sets and not on application building."
"outline. in section 2, we present conceptual considerations as well as different surgical application scenarios and derive requirements concerning visualization, interaction, case management, and interface design. in section 3, we review related toolkits and discuss the differences to the metk. in section 4, we give an overview of the key concepts and present the techniques that we integrated in the metk. furthermore, details with respect to new techniques for 2d overlay visualizations, to advanced facilities for selecting objects, to reuse once-defined visualization parameters are presented. in section 5, we describe how application building is achieved using the metk and present some applications where the metk has been successfully applied. in section 6, we present and compare the evaluation of implementing a reference application in mevislab with and without the metk. in sections 7 and 8, we close with a discussion of the toolkit and the lessons learned, a summary, and an outlook on future developments."
"mevislab has many similarities to amira, scirun, and a broad overlapping of functionality (see [cit] for a comparison of mevislab, amira, scirun, and mitk), since all offer visual programming, and amira and mevislab use open inventor. however, mevislab focuses on medical image data and quantitative image analysis, and provides facilities for application development, like a higher definition language to design user interfaces. however, there is no support for capabilities to handle whole cases as described in section 2. even if all techniques of the metk can be developed using mevislab (as the complete metk was), mevislab itself does not provide those high-level building blocks. mevislab focuses on algorithmic functionality and offers the ability to construct reusable building blocks. using the metk, a developer of medical applications can design a running prototype more efficiently."
"big data analysis of industry is considered as a necessary aspect for further improvement in order to improve the profit margin of industrial production and operation, and represents the next frontier of innovation, competition and productivity [cit] . nowadays, industrial data platform is the core component of industrial data storage, computation and analysis for the management of intelligent plant. with the increasing number of intelligent equipments used in intelligent plant, however, intelligent plant can acquire a large quantity of data of radio frequency identification (rfid) and intelligent equipments, thus providing rich data sets for manufacturing industry [cit] . the current trend in industrial systems is to use different big data engines as a means of"
"moreover, different 3d viewers can be automatically synchronized in the metk by connecting its camera parameters (position, orientation, etc.). this can be used to explore different data sets from the same viewpoints, to compare different intervention strategies for one patient, or to compare pre-and postoperative data."
"in essence, there are many toolkits and frameworks for medical image analysis and visualization. but some of them are focused on the creation of singular impressive visualizations (e.g., volumeshop [cit] ), some of them focus on medical image analysis (e.g., mitk [cit], 3dslicer [cit] ), and only a few support application building (e.g., scirun [cit], mevislab [cit] ). to the best of our knowledge, there is no toolkit or framework to create efficient medical applications with high-end visualizations, adequate interaction techniques, and user interface guidance."
"several full-fledged applications were designed and developed with our toolkit. using the metk, a training system for liver surgeons was developed, the liversur-gerytrainer [cit] (see fig. 11 ). the feedback from the surgeons within the evaluation of the liversurgerytrai-ner [cit] inspired several refinements of the metk. one inspiration was the large importance of 2d slice-based visualizations in contrast to a pure focus on 3d visualizations. the necksurgeryplanner supports the decisionmaking process for neck dissections. here, the target group is experienced surgeons [cit] (see fig. 10 )."
"there are many development environments for scientific visualization available. some of them support the developing process by sophisticated techniques, e.g., graphical network programming (e.g., scirun [cit], mevislab, and devide [cit] ). even though scirun and mevislab provide basic facilities for application development, it is still very complex to efficiently create applications that use new visualization techniques and can be used by \"real users\" independently. in a comparison study of four visualization frameworks (thereunder scirun and mevislab), mevislab was determined as the \"best framework for creating applications\" [cit] . hence, we extended mevislab with a toolkit that especially supports the application building process for surgical planning. even if there is some knowledge of python and the layout language mdl of mevislab necessary to build up complete applications, it is much easier and faster to come up with fully functional prototypes in an early development stage, since many of the most used visualization and exploration techniques are already implemented in the metk. the application networks are also easier to maintain and an reduced cognitive effort is necessary to integrate changes or extend the application."
"safety margins around tumors and metastases are essential for intervention planning and intraoperative navigation. therefore, for all structures at risk, a 3d euclidean distance transform is performed. depicting important distance thresholds (e.g., yellow and red representing 5 and"
"we presented an extensive toolkit for surgical application development-the medicalexplorationtoolkit (http://www.metk.net). using the metk, applications that fulfill surgical requirements of exploration support and visualization techniques, can be built up quickly. introducing the multicoded segmentation masks, we provide an efficient way to store multiple overlapping segmentation masks in one mask, supporting colored overlays in 2d. advanced 3d selection techniques and key states for storing of visualization state are also dedicated to surgical planning, but useful for other application areas. with animation facilities, the viewpoint selection as well as new support for object selection, a substantial guidance for the exploration of 3d scenes is provided. although the metk is a good basis for solving many intervention planning problems, special applications will yield new requirements."
"open source and freely available toolkits are widespread in the research community. using toolkits, application prototypes can be built up quickly, reverting to ready-touse basic functions. in the medical domain, toolkits and libraries for image analysis and volume rendering are widely available [cit], e.g., the mitk [cit], 3dslicer [cit], or volumeshop [cit] . however, they are difficult to extend for professionals without a substantial background in computer science, since substantial c++ knowledge is required. the metk supports an easier application building process for surgical applications where no extended programming skills are needed. instead, graphical programming in combination with script-based interface design is employed. the metk is a turn-key environment because all described functions are fully implemented and basic applications in form of example networks and data sets are provided with the metk. the metk extends the underlying mevislab development environment in the following ways:"
"our motivation for this study is to look for an efficient plan for acquiring and processing industrial data. moreover, we need to find compression and serialization method which have good performance in the time of data processing time and the space of data storing. furthermore, we aim at designing an industrial data platform with higher real-time performance and higher compression ratio. based on the above considerations, we design and implement an industrial data platform, which integrated both lz4 and protobuf method for data processing. the lz4 is used for data compression, and the protobuf is used for data serialization. based flume and sqoop, we build up a data acquisition module in the industrial data platform. meanwhile, the hadoop is adopted to store data acquired by intelligent equipments in intelligent plant. in order to enhance the data analysis capabilities of the industrial data platform, both the spark and the flink are integrated into the computing system. in addition, the industrial data platform adopt django as front-end framework for the sake of system stability and system maintenance management."
"two papers by luk and park [cit] established equivalence between numerous strategies, showing that if one of them is convergent, all the strategies are convergent. in the same year shroff and schreiber [cit] showed convergence for a family of strategies called the wavefront ordering, and discussed the parallel orderings weakly equivalent to the wavefront ordering, and thus convergent."
"3) classification of alerts: alerts generated by the rule checkers are classified as (un-)actionable as follows: considering an alert appearing in an artifact (class, method, or package) in revision i. if, in revision i+n, the artifact still exists and the alert is removed (the artifact no longer contains it) then it is an actionable alert. an alert is un-actionable if the artifact is removed before the alert is corrected. finally, if the alert and the artifact still exist in the latest revision of the benchmark, we can not determine whether the alert is actionable or not, and it is removed from the benchmark. table ii summarizes the number of alerts for each of the three rule checkers."
"2) external validity: we tried to be as generic as possible by selecting a range of systems from different domains and two programing languages. the systems are real-world and non-trivial application, with a consolidated number of users. data are collected for a large number of versions over an extended time frame (several years)."
"additionally, we explain the staged shape of the zranking fault detection rate curves by the grouping of alerts (see definition of zranking). because of the defective grouping operator used, when a group comes in, its actionable alerts raises the curve steeply. then when the other (un-actionable) alerts of the same group are considered, the curves remain stable (no improvement) until the next group comes in. this shows the consequence of grouping alerts together."
"the full block variant of phases (2a) and (2b) usually has 30 sweeps limit for both the inner blocking and the pointwise, shared-memory jacobi level. the block variant has both limits set to 1. between them many hybrid variants may be interpolated."
"we compare the alert ranking algorithms between themselves using these curves. for this we compare the curves two by two with a chi-squared test of homogeneity (χ 2 ). this test allows us to determine if two curves follow a different distribution. when the p-value of the test is less than a given threshold (we use 5% and 1 ) we can conclude that the two curves do not follow a similar distribution. note that the comparison is not symmetric, but in practice, comparing a to b rarely leads to a different result than comparing b to a."
"we first selected three datasets from the faultbench 0.3 benchmark 2 [cit] . it contains three real, java programs for comparison and evaluation of alert ranking methods. the revision history of each subject covers a period of approximately seven years, from this, a number of revisions have been sampled by the authors of this benchmark. for each sample revision, faultbench provides the alerts generated by findbugs and pmd and also a set of metrics on the subjects."
"finally, when all the chunks are processed, h pq is written into r pq, and v ′ pq is set to i 32 . note that data in the gpu ram are accessed only once. no symmetrization is needed for h pq, since only its lower triangle is taken as input for the cholesky factorization. for details of this subphase see alg. 4.1."
"the jacobi method is an easy and elegant way to find the eigenvalues and eigenvectors of a symmetric matrix. [cit] hestenes [cit] developed the one-sided jacobi svd method, an implicit orthogonalization of a factor of a symmetric positive definite matrix. [cit] /62 by francis and kublanovskaya, the jacobi algorithm seemed to have no future, at least in the sequential processing world, due to its perceived slowness [cit] . however, a new hope for the algorithm has been found in its amenability to parallelization, in its proven high relative accuracy [cit], and finally in the emergence of the fast jacobi svd implementation in lapack, due to drmač and veselić [cit] ."
"however, this still does not mean that no serious issues exist regarding convergence of b. fig. 6 .1 indicates that a further investigation into the causes of the extremely slow convergence (approaching 30 block sweeps) of b may be justified."
"see http://www.ieee.org/publications_standards/publications/rights/index.html for more information. inc, ca) system. we validate the new algorithm and model using benchmark data and a newly labelled dataset that we will make available."
"the respective reverses, r m and c m, operate in the same block-recursive fashion (preserved by alg. 3.2) of the mantharam-eberlein br strategy [cit], i.e., processing first the off-diagonal block, and then simultaneously the diagonal blocks of a matrix. it follows that all three strategies are step-equivalent. thus, r n and c n can be regarded as the generalizations of the br strategy to an arbitrary even order n, albeit lacking a simple communication pattern. conversely, for the power-of-two orders, r m and c m might be replaced by the br strategy with a hypercube-based communication."
"in this section we describe the two-level blocked jacobi (h)svd algorithm for a single gpu. the algorithm is designed and implemented with nvidia cuda [cit] technology, but is also applicable to opencl, andat least conceptually-to the other massively parallel accelerator platforms."
"the columns of a block pivot pair should be relatively orthogonal after completion of a pstep call, but they may have departed from orthogonality, since (see [cit] ) 1. the accumulated rotations are not perfectly (j-)orthogonal, and 2. the postmultiplication introduces rounding errors. independently from that, when subject again to factorize (and its rounding errors), even the numerically orthogonal columns may result in the shortened ones that fail the relative orthogonality criterion."
"the first purpose of this study is to set up a framework to evaluate different alert ranking algorithms and find the \"best\" one. the notion of best algorithm is too vague, and needs to be refined. on one hand, rule checkers are typically noisy, they generate many false positive (un-actionable alerts). on the other hand, it would be illusory to expect these checkers to identify all bugs in a system [cit] . therefore, we chose to give more importance to the precision of the results than to their completeness. thus, we prefer filtering methods that accept only actionable alerts over the ones that recognize all actionable alerts. this is measured by an effort metric that counts the average number of filtered alerts to (manually) inspect to find an actionable one. the effort metric is further detailed in section iii-d."
"the results are given in table iv : effort for the first 20%, 50%, or 80% of all alerts. figure 1 also presents all the fault detection rate curves, and table v homogeneity tests between the curves. in table v results of the χ 2 tests are marked with (*) when the difference between the curves is statistically significant at the 5% level, and marked with (**) when the difference is statistically significant at the 1 level."
"improve the surgeons' ability to perform tasks and precisely target and manipulate the anatomy, it is crucial to monitor the relationship between the surgical site and the instruments within it to facilitate computer assisted interventions (cai)."
"all ranking methods do not require the same information. for instance, the rpm algorithm requires lot of data about the alerts and obtaining such information is difficult and expensive."
"one of the first attempts of a parallel svd on a gpu is a hybrid one, by lahabar and narayanan [cit] . it is based on the golub-reinsch algorithm, with bidiagonalization and updating of the singular vectors performed on a gpu, while the rest of the bidiagonal qr algorithm is computed on a cpu. in magma 1, a gpu library of the lapack-style routines, dgesvd algorithm is also hybrid, with bidiagonalization (dgebrd) parallelized on a gpu [cit], while for the bidiagonal qr, lapack routine dbdsqr is used. we are unaware of any multi-gpu svd implementations."
"in this letter, we proposed a 3d fcnn architecture for surgical-instrument joint and joint-connection detection in mis videos. our results, achieved by testing existing datasets and new contribution datasets, suggest that spatio-temporal features can be successfully exploited to increase segmentation performance with respect to 2d models based on single-frame information for surgical-tool joint and connection detection. this moves us towards a better framework for surgical scene understanding and can lead to applications of cai in both robotic systems and in surgical data science."
"aware: heckman and williams [cit] rank individual alerts using their rule and their location in the source code. an assumption of the model is that alerts from the same rule and/or from the same source code artifact are similarly actionable or un-actionable. aware ranks alerts on a scale from -1 to 1, where alerts close to -1 are more likely to be un-actionable and alerts close to 1 are more likely to be actionable. alerts are ranked by considering the developer's feedback, via past actionable and un-actionable alerts, to generate a measure of the set of alerts sharing either the same rule or code location."
"endovis.a was probably the less challenging video in terms of background complexity and both the proposed and network showed similar results [cit] . when instead the endovis.b test video was considered, the previous model [cit] was barely able to properly recognize and separate tip joints and connections from the background, achieving poor dsc values and overestimating joint/connection detection. this result is visible in fig. 8, where multiple tip-points are erroneously detected for ltp and rtp and double connections for the related joint pairs."
"we present two approaches for factorize. the cholesky factorization of the gram matrix h pq, as in (2.7), is described in subsection 4.1 in two subphases, and the qr factorization (2.8) is discussed in subsection 4.2."
"at first glance a choice of the parallel strategy might seem as a technical detail, but our tests at the outermost level have shown that the modified modulus strategy can be two times faster than the round-robin strategy. that motivated us to explore if and how even faster, yet still accurate strategies could be constructed. we present here a class of parallel strategies designed around a conceptually simple but computationally difficult notion of a metric on a set of strategies of the same order. these new strategies can be regarded as generalizations of the mantharam-eberlein br strategy to all even matrix orders, outperforming the brent and luk and modified modulus strategies in our gpu algorithm."
"3. parallel pivot strategies. in each step of the classical, two-sided jacobi (eigenvalue) algorithm, the pivot strategy seeks and annihilates an off-diagonal element h pq with the largest magnitude. in the sequential one-sided algorithms this approach has a prohibitive overhead, since the scalar products of all columns have to be recomputed. in the parallel case there is an additional problem of finding ⌊n/2⌋ independent (their indices pairwise disjoint) off-diagonal elements with large magnitudes. therefore, a cyclic pivot strategy-a repetitive, fixed order of annihilation of all off-diagonal elements of h-is more appropriate for the one-sided algorithms."
"if an orthogonality failure results from the first two causes, the ensuing rotations might be justified. however, if the failure is caused only by the rounding errors of the factorization process, the spurious rotations needlessly spoil the overall convergence."
we also generated three datasets from systems in smalltalk for which we had access to the required data. alerts were generated with the smalllint rule checker (see below) and we extracted the information required to run the different alert ranking methods tested.
"to overcome this problem, one solution is to use more complex rules and tools to increase the precision of the rule checkers [cit] . but even with more sophisticated analyses, the rate of un-actionable alerts can still be high [cit] . another solution is to add a filtering step by computing the probability of an alert being actionable in a given context. the filtering may also be performed on individual alerts or on rules, to identify the ones that, in the given context, are more likely to produce actionable alerts. thus, by selecting the alerts or rules with the higher probability, the rate of un-actionable alerts can be reduced."
"in conclusion, overall one should probably opt for ranking algorithms that work on individual alerts. however, on a practical point of view, it would be interesting to check whether the training on one system can easily be ported to another one. because of the fine grained training at the level of individual alerts, on new systems without history, it might among the two best ranking algorithms (aware and feedbackrank), one is ad hoc and the other is based on a statistical algorithm. similarly, among the two worst ranking algorithms (zranking and alertlifetime), one is ad hoc and the other is based on a statistical algorithm. therefore it seems difficult to clearly decide between the two approaches based on these data. from our experience, we can only say that the adhoc algorithms were easier to implement and usually required less information. we therefore suggest to go with one of these (typically aware) if one has a choice."
"other publications are more related to the pertinence of the domain. for example, [cit] are following the gqm 1 paradigm to determine whether rule checkers can help an organization to improve the economic quality of software products. their results indicate that rule checkers are an economic complement to other verification and validation techniques."
"in this paper we show that such gpu-centric design is possible and that the jacobi-type algorithms for a single and the multiple gpus compare favorably to the present state-of-the-art in the gpu assisted computation of the (h)svd. since all computational work is offloaded to a gpu, we need no significant cpu ↔ gpu communication nor complex synchronization of their tasks. this facilitates scaling to a large number of gpus, while keeping their load in balance and communication simple and predictive. while many questions remain open, we believe that the algorithms presented here are the valuable choice to consider when computing the (h)svd on the gpus."
"to realize this experiment, we must select appropriate subjects. to compare the ranking alert methods we need a set of alerts already classified as actionable/un-actionable. we must also run the ranking methods on the subjects, which implies having access to all the required information, or being able to collect it."
"the six alert ranking algorithms are implemented in smalltalk. to compute the binomial regressions and bayesian network necessary for the algorithms [cit], we use the r statistical language/system. the datasets are split in two: a training set and a test set. each set contains 50% of the benchmark alerts. then, each algorithm is trained on the training set and used on the test set. the results are compared which the correct results computed as described in section iii-b3. this process (training set generation, algorithm training, result testing) is repeated 100 times for each ranking algorithm to avoid bias. the results presented in this article are the average of the 100 executions. additionally, we also take care that training set and test set have the same percentage of (un-)actionable alerts as the whole dataset. this helps us comparing the different alert ranking methods by ensuring that they all learn from training datasets with similar characteristics and then all run on test datasets with similar characteristics."
"we will also use the fault detection rate curve. for a given algorithm, the curve is formed by the percentage of all actionable alerts found within the first y alerts of the alert ranking algorithm. for an optimal alert ranking algorithm, when y is less than the total number of actionable alerts, the curve raises linearly from 0% to 100% as all extracted alerts are actionable (in figure 1, this is materialized by the upper dotted line). when y is equal or greater than the total number of actionable alerts, the curve is constant with value 100%. in the figures, we also materialize a random alert ranking algorithm (lower dotted line) that achieves 100% only when all the alerts are considered."
"a reasonable hybrid variant might try to keep the running times balanced. a cpu thread that first completes the full block variant of (2a) or (2b) informs immediately other threads, before proceeding to phase (4) . the other threads then stop their inner block sweeps loops in (2a) or (2b) when the running iteration is finished."
"on the other hand, the results obtained by the 3d network suggest that the temporal information was exploited to improve the network generalization capability on unseen backgrounds, obtaining dsc scores of 77.6% and 76.4% for ltp and rtp, respectively, as shown in table iv ."
"we consider two specific robotic surgical tools in this letter, endowristlarge needle driver and endowristmonopolar curved scissors, however, the methodology can be adapted to any articulated instrument system."
"as shown in table iii, the two best ranking algorithms (feedbackrank and aware) use exactly the same type of data: the artifact which contains the alert, the rule of the alert, and the actionable and un-actionable alerts. in addition, the best ranking algorithm working at rule-level (efindbugs) also uses the actionable and un-actionable alerts as well as the rule of the alert. in such ranking algorithms, the history of past alerts is used to determine the ratio of actionable alert/unactionable alert for the system code artifact and the rules of the rule checker. one might hypothesize first that not all data sources have the same relevance in ranking alerts or rules, and second that history of past alerts and the location of the alerts (artifact containing the alert) are probably good sources."
our implementation of the zranking algorithm is rough. the grouping operator implemented is overly simple and the successful/failed check behavior could only be approximated as the rule checking tools used do not output successful checks. the results of this ranking algorithm might be different from the original implementation.
"a second issue is related to the 2d nature of the estimated joint position. it would be interesting to include da vinci(intuitive surgical inc, ca) kinematic data in the joint/connection position estimation. such information may be useful to provide a more robust solution for occluded joints. this is realistic and feasible using dvrk information but requires careful calibration and data management. while dvrk encoders are able to provide kinematic data for end-effector 3d position and angles between robotic-joint axes, this requires a projection on the image plane to be suitable for 2d tracking, with errors associated to encoders' precision."
"to rank alerts, some methods use statistical tool such as bayesian networks or linear regression. implementing such tools may be complex and their results might be difficult to explain or understand. if \"simpler\" more intuitive methods are as successful, it could facilitate their implementation and/or adoption."
"first, we observe that the aware algorithm has consistently the lowest effort (see table iv ). its effort is typically close to 1. even when one inspects almost all (80%) of the alerts ranked by the algorithm the effort is still only about 1.33 which means that 3 out of 4 inspected alerts are actionable."
"when considering the ucl dvrk testing video, the proposed fcnn substantially outperformed the state of the art on ep and sp-ep, achieving δdsc differences of +15.7% and +5.0%. a sample of the performed segmentation for the two models is shown in fig. 7 for ep and sp-ep for illustration purposes."
"the paper is organized as follows. in section 2 a brief summary of the onesided jacobi-type (h)svd block algorithms is given. in section 3 new parallel jacobi strategies-nearest to row-cyclic and to column-cyclic are developed. the main part of the paper is section 4, where a detailed implementation of a single-gpu jacobi (h)svd algorithm is described. in section 5, a proof-of-concept implementation on multiple gpus is presented. in section 6, results of the numerical testing are commented. two appendices complete the paper with a parallel, numerically stable procedure for computing 2-norm of a vector, and some considerations about the jacobi rotation formulas."
"the results we obtained on endovis.a and ucl dvrk may be explained considering that the backgrounds in the videos are very similar to the ones of the videos of the training set, meanwhile endovis.b's background is completely missing in the training data domain. the low dsc score achieved by step 1 model on endovis.b, coupled with the high scores on the other two datasets, showed that, with high probability, the model overfitted. such a conclusion may be expected: despite the large amount of data, the high correlation between datasets, due to the use of a temporal step w s of only one frame, led the sliding window algorithm to produce a dataset with too little variability for training a model over a good domain."
"pmd 4 is an open-source tool that supports a rich set of rules for detecting potential bugs and checking coding style in java. for example, pmd supports rules for detecting empty try/catch/finally/switch statements or dead code like unused local variables, parameters and private methods. there are also rules for dealing with particular frameworks, such as java beans, jsp, junit, android etc. pmd requires the source code of the target program, because constraint rules are defined over the ast of the programs. finally, as in findbugs, rules have a priority and a category."
"alertlifetime: kim and ernst [cit] prioritize rules by the average lifetime of alerts in this rule. the idea is that alerts fixed quickly are more important to developers. in our case, the lifetime of an alert is measured at the file level for java and, in smalltalk, from the revision of this rule's first alert until closure of its last alert. alerts still existing in the last revision studied 5 are given a penalty of 365 days added to their lifetime."
"1 specifically, we used 8 videos for training and 2 (endovis.a and endovis.b) for testing and validation. it is worth noticing that endovis.b has a completely different background with respect to the 8 training endovis videos, differently from endovis.a that has a similar background."
"cai promises to provide surgical support through advanced functionality, robotic automation, safety zone preservation and image guided navigation. however, many challenges in algorithm robustness are hampering the translation of cai methods relying on computer vision to the clinical practice. these include classification and segmentation of organs in the camera field of view (fov) [cit], definition of virtual-fixture algorithms to impose a safe distance between surgical tools and sensitive tissues [cit], and surgical instrument detection, segmentation and articulated pose estimation [cit] ."
"in summary, for two test bed (smalllint and pmd), the aware and feedbackrank algorithms are significantly better than the other alert ranking methods while the zranking and alertlifetime algorithms are significantly worse."
", else do nothing (see fig. 4.1(c) ). after each stage, a thread-block-wide synchronization (__syncthreads) is necessary. column norm computations should be carried out carefully, as detailed in appendix a."
"however, not all alerts are corrected by the developers. some may be ignored because it would be too difficult to fix them or because the developers do not agree that they represent an instance of bad code. following [cit] terminology, when an alert is actually fixed by developers, we say it is an actionable (\"true\") alert, conversely, an unactionable (\"false\") alert is one that does not require to be fixed, whatever the reason. on real systems, a rule checker may commonly raises thousands of alerts, many of which unactionable. in fact, between 35% to 91% [cit] of reported alerts are un-actionable. in this case, the high rate of un-actionable alerts adversely impacts the review of the alerts and the identification of the actionable ones. additionally, this suggests that the rules are wrong or do not make sense and may discourage developers to use these tools."
"and (at least) one of the updated block-columns of g (and v ) is replaced by a blockcolumn from another task. a completion of this exchange is a necessary synchronization point for the otherwise concurrent tasks. the same blocking principle may be applied recursively, acting on r pq . the recurrence terminates at the pointwise (non-blocked) jacobi algorithm. in that way a hierarchical (or multi-level) blocking algorithm is created, with each blocking level corresponding to a distinct communication/memory domain (see [cit] )."
"with (b.1) and a correctly rounded-to-nearest rsqrt prototype cuda implementation 3 there was a further improvement of orthogonality of v (−t ) . although (b.1) has only one iterative operation (rsqrt) instead of two (rcp and √ x), and thus has a potential to be faster than (4.6), we omitted (b.1) from the testing due to a slowdown of about 1% that we expect to vanish with the subsequent implementations of rsqrt. it is still far from conclusive which formulas from (4.6) or (b.1), and for which ranges of ct 2ϕ, should be used. however, cs 2 ϕ or cs ′ 1 ϕ formulas might be an alternative to the established cs 1 ϕ ones. a deeper analysis is left for future work."
"where t p is the number of pixels correctly detected as joint/connection and background, while f p and f n are the number of pixels misclassified as joint/connection neighbors and background, respectively. multiple comparison one-way anova was performed to detect significant differences between results achieved when investigating e1 and e2, always considering a significance level (α) equal to 0.01."
"synchronize (4, 6) 6. numerical testing. in this section we define the testing data, describe the hardware, and present the speed and accuracy results for both the single-gpu and the multi-gpu implementations. by these results we also confirm our jacobi p-strategies ( r and c ) from section 3 as a fast and reliable choice for the jacobi algorithms on the various parallel architectures."
"observe that phase (4) forces all gpus to wait for the slowest one, in terms of execution of phase (2a) or (2b). the full block variant exhibits the largest differences in running times between gpus, depending on how orthogonal the block-columns are. although, by itself, is the fastest choice for a single-gpu algorithm (see section 6), the full block variant may be up to 35% slower in a multi-gpu algorithm than the block-oriented one, which has a predictable, balanced running time on all gpus."
"exactly t independent pivots can be simultaneously processed in each of the s parallel steps (p-steps). as the p-strategies for an even n admit more parallelism within a p-step than those for n − 1, with the same number of p-steps, we assume n to be even. besides the standard terminology of equivalent, shift-equivalent and weakly equivalent strategies [cit], here we add a definition of a p-strategy closest to a given sequential strategy. the motivation was to explore whether a heuristic based on such a notion could prove valuable in producing fast p-strategies from the well-known rowand column-cyclic sequential strategies. the numerical testing (see section 6) strongly supports an affirmative answer."
"to analyze these results with more details, we plot the cumulative percentage of actionable alerts in the rules in figures 2, 3, and 4. the x axis represents rules with a given percentage of actionable alerts. thus, the left side of the x axis shows the \"bad rules\" with low percentage of actionable alerts, and the right side of the x axis shows the \"good rules\", with high percentage of actionable alerts."
"in the case of a multi-gpu system, we identify access to the global memory (ram) of a gpu as slow compared to the shared memory and register access, and data exchange with another gpu as slow compared to access to the local ram. this suggests the two-level blocking for a single-gpu algorithm, and the three-level for a multi-gpu one. moreover, the latencies on every memory level are much higher than the speed of computation, so we adopt the full block variant at each of the levels."
"a first answer comes from the fact that from the three ranking algorithms working on individual alerts (table iii), two (aware and feedbackrank) were found to be better than the other methods. moreover, from the three algorithms working on rules, two (zranking and alertlifetime) were found to be worse."
"the wall execution times of such an approach may be even lower than the blockoriented variant, but on the average are 10% higher. moreover, both the full block and its hybrid variant induce larger relative errors in σ than the block-oriented variant. such effect may be partially explained, as in section 6, by the same reasons valid for the single-gpu case (more rotations applied), but the larger differences in the multi-gpu case require further attention. we therefore focused on the block-oriented multi-gpu variant for the numerical tests."
"this induces i o, a one-to-one mapping from the set of all cyclic jacobi strategies on matrices of order n to the symmetric group sym(τ ), as"
"our instrument model poses each tool as a set of connected joints as shown in following previous work, to develop our fcnn model we perform multiple binary segmentation operations (one per joint and per connection) to solve possible ambiguities of multiple joints and connections that may cover the same image portion (e.g., in case of instrument self-occlusion) [cit] . for each laparoscopic video frame, we generated 9 separate ground-truth binary detection maps: 5 for the joints and 4 for the joint pairs (instead of generating a single mask with 9 different annotations which has been shown to perform less reliably)."
"natural extensions of the proposed work would be to include the instrument articulation estimation within other scene understanding algorithms, e.g., computational stereo or semantic slam, in order help with algorithms coping with the boundary regions between instruments and tissue."
"in findbugs and smalllint test beds (figures 2 and 3 ), the bar plot raises steadily, whereas for the pmd test bed (figure 4), it raises very fast at the beginning. this means for pmd that 80% of all actionable alerts are raised by \"bad rules\" (with less than 35% of their alert being actionable). for findbugs, this means that less than 50% of all actionable alerts come from reasonably \"good rules\" (up to 85% of their alerts are actionable). therefore, alert ranking algorithms working on rules give poor results for pmd, while for findbugs and smalllint the results are better."
"unlike magma, the jacobi gpu algorithms are perfectly scalable to an arbitrary number of gpus, when the matrix order is a growing function of the number of assigned gpus. that makes the jacobi-type algorithms readily applicable on the contemporary large-scale parallel computing machinery, which needs to leverage the potential of a substantial amount of numerical accelerators."
"also, because we use findbugs, pmd, and smalllint rule checkers (see section iii-b) that don't output successful/failed checks, we approximate this result. considering a rule that applies on methods, for example to check whether a call to a given primitive is correctly performed. all alerts reported by our rule checkers are failed checks; all other methods are considered successful checks which obviously might not be true."
"such results for the zranking algorithm may be explained by the simple grouping operator used in our implementation (see section iii-c) and because of the approximation of the successful/failed check behavior. figure 1 and table v allow one to statistically test the differences between the alert ranking methods. one must note that results depend on the used rule checker. for pmd, all alert ranking algorithms are different two by two, except for the zranking and alertlifetime algorithms. for findbugs and smalllint, we can not draw conclusions taking into account their pairs."
"proof. s (1) n is an admissible p-step; e.g., brent and luk strategy starts with it, and we show that the same is true for any p-strategy closer to r n or c n ."
"an obvious limitation of this study is the limited number of testing videos, which is due to the lack of available annotated data. nonetheless, this number is comparable to that of similar work in the literature [cit] and we will release the data we collected for further use in the community. as future work, it would be interesting to assess the model performance when varying w d . moreover, a larger test set, which will possibly encode challenges such as smoke and occlusions, should be collected, annotated and analyzed, too."
"the one-sided approach is better suited for parallelization than the two-sided one, since it can simultaneously process disjoint pairs of columns. this is still not enough to make a respectful parallel algorithm. in the presence of a memory hierarchy, the columns of g and v should be grouped together into block-columns,"
"magma's dgesvd routine was tested with a sequential (seq.) and a parallel (par.) ( slower, and magma (par.) is up to 2 times faster. on the other hand, magma (par.) is, on average, 30%, and for the larger matrix sizes, more than 45% slower than the block-oriented 4-gpu fermi (solve) implementation. the (acc.) implementation is about 35% slower than (solve) (see fig. 6 .3), and only marginally more accurate (see fig. 6 .4). for the matrix orders at least 4096, the fastest jacobi implementation on 4 gpus is about 2.7 times faster than the fastest one on 1 gpu. magma's accuracy is comparable to a single-gpu algorithm for the well-conditioned test matrices, and better than a multi-gpu algorithm, but in the (separately tested) case of matrices with badly scaled columns (κ 2 ≈ 10 12 ), the relative errors of magma could be more than 20 times worse than the jacobi ones."
"for every joint mask, we consider a region of interest consisting of all pixels that lie in the circle of a given radius (r d ) centered at the joint center [cit] . a similar approach was used to generate the ground truth for the joint connections. in this case, fig. 2 . ground-truth example for shaft point (circle) and shaft-end point connection (rectangle). we used the same pixel number (r d ) for both circle radius and rectangle thickness, highlighted in green. the ground truth is the rectangular region with thickness r d and centrally aligned with the joint-connection line. an example for sp and sp-ep link is shown in fig. 2 ."
"a thread block behavior is uniquely determined by the block indices p and q, since the thread blocks in every pstep invocation are mutually independent. computation in a thread block proceeds in the three major phases:"
"where n is the total number of considered masks, p k x and p k x are the ground truth value and the corresponding network output at pixel location x in the clip domain ω of the k th probability map."
"the model trained on step 4 dataset was not able to achieve competitive results in any of the test videos with respect to the other models. since the proposed architecture has a very large number of parameters (∼ 80000), it needs a huge amount of data in order to be properly trained. for this reason, the model achieves lower quality predictions."
"ideally, such an algorithm should access the gpu ram as few times as possible, and be comparable in speed to the cholesky factorization approach. we show that the algorithm can be made to access g p g q exactly once, but the latter remains difficult to accomplish."
"on a fermi gpu (matrix order 6144), ddrjac is only 14% slower than a simple procedure we discuss in the sequel. however, protection from the input columns of too large (or too small) norm that ddrjac offers has to be complemented by the qr factorization at all blocking levels, which is extremely expensive."
"for the comparison with the state of the art, we chose the model proposed [cit], which is the most similar with respect to ours. we compared it with the model that showed the best performances according to e1."
"there are no differences between ranking methods effort. h 1 a some ranking methods have a lower effort than other. assuming we have an answer to this research question, we may want to get into more details to better understand the differences between the different alert ranking methods."
"the strategies just described progress from the diagonal of a matrix outwards. however, if the magnitudes of the off-diagonal elements in the final sweeps of the twosided jacobi method are depicted, a typical picture [14, page 1349] shows that the magnitudes rise towards the ridge on the diagonal. that motivates us to annihilate the near-diagonal elements last, which is easily done by reverting r n and c n . a reverse of the strategy o is a strategy o, given by"
"the input to our 3d fcnn is a temporal clip (i.e., set of temporally consecutive video frames) obtained with a slidingwindow controlled by the window temporal length (w d ) and step (w s ). a visual representation of the sliding-window is shown in fig. 3 . starting from the first video frame, the first w d images are collected and used to generate a 4d data volume of dimensions frame height x frame width x w d x 3, where 3 refers to the spectral rgb channels. the window then moves w s frames along the temporal direction and a new temporal clip is generated resulting in a collection of m 4d clips."
we did not test the statistical validity of the difference in the results of the effort metric. for this reason we tried to be very conservative in drawing conclusions from these results and back them up with the χ 2 test comparing the fault detection rate curves.
the letter is organized as follows: sec. ii presents the structure of the considered instruments and the architecture of the proposed fcnn. in sec. iii we describe the experimental protocol for validation. the obtained results are presented in sec. iv and discussed in sec. v with concluding discussion in sec. vi.
"(i) 8-byte wide shared memory banks on kepler vs. 4-byte wide on fermi-the profiler reports 99.8% shared memory efficiency on kepler vs. 49.8% on fermi, (ii) warp shuffle reductions, i.e., without shared memory, on kepler, and (iii) no register spillage on kepler, due to the larger register file. the other profiler metrics are also encouraging: the global memory loads and stores are more than 99% efficient, and the warp execution efficiency on fermi is about 96.5%, which confirms that the presented algorithms are almost perfectly parallel. even though the instruction and thread block execution partial orders may vary across the hardware architectures, the presented algorithms are observably deterministic. combined with a strong ieee floating-point standard adherence of both fermi and kepler, that ensures the numerical results on one architecture are bitwise identical to the results on the other. this numerical reproducibility property should likewise be preserved on any future, standards-compliant hardware."
"feedbackranking: [cit] developed an adaptive prioritization algorithm based on the intuition that alerts sharing an artifact location (method, class and package) tend to be either all actionable or all un-actionable (similarly to aware). each inspection of an alert by a developer adjusts the ranking of un-inspected alerts. after each inspection, the set of inspected alerts is used to build a bayesian network, which models the probabilities that groups of alerts sharing a location are actionable or un-actionable. in our case, the training set (inspected alerts) is static and we do not used feedback to improve the accuracy of this method."
"the block-oriented variant has more block sweeps and, while slightly faster for the smaller matrices, is about 7% slower for the larger matrices than the full block variant. it may be more accurate in certain cases (see fig. 6 .2), due to considerably smaller total number of rotations performed, as shown in table 6 .3. strategies m and b are far less accurate than the new p-strategies."
"rule checkers [cit] are tools that check whether some source code violates some good programming rules. for example, rules can specify an upper bound for method size [cit], perform flow analysis to verify that a stream is properly closed [cit], or check the correct use of an array to avoid out of bound accesses. these rules model good programming standards or frequent programming mistakes. rule checkers thus raise alerts (or warnings) each time one of their rules is violated. these alerts must then be manually inspected by developers, to be eventually fixed. correcting alerts improves the quality of the target system [cit] and may prevent future bugs [cit] . the systematic use of these tools facilitate and improves software maintenance [cit] ."
"as alerts are ranked in decreasing order of probability of being actionable, one expects that, as x% increases, the effort will also increase because their will be more and more unactionable alerts in the results."
"one might hypothesize that all rule checkers do not behave equally. this was expected from the discussion at the end of section ii. more experiments could be undertook to understand why and how these differences occur, whether due to the tool themselves or to the rules they contain."
"surgical-tool joint detection in particular has been investigated in recent literature for different surgical fields, such as retinal microsurgery [cit] and abdominal mis [cit] . information provided by algorithms can be used to provide analytical reports, as well as, as a component within cai frameworks. early approaches relied on markers on the surgical tools [cit] or active fiducials like laser pointers [cit] . while practical, such approaches require hardware modifications and hence are more complex to translate clinically but also they inherently still suffer from vanishing markers or from occlusions. more recent approaches relying on data driven machine learning such as multiclass boosting classifiers [cit], random forests [cit] or probabilistic trackers [cit] have been proposed. with the increasing availability of large datasets and explosion in deep learning advances, the most recent work utilizes fully convolutional neural networks (fcnns) [cit] . despite the promising results using fcnns, a limitation is that temporal information has never been taken into account, despite the potential for temporal continuity as well as articulation features to increase the fcnn generalization capability and also capture range."
"however, a parallel strategy alone is not sufficent to achieve decent gpu performance. the standard routines that constitute a block jacobi algorithm, like the gram matrix formation, the cholesky (or the qr) factorization, and the pointwise one-sided jacobi algorithm itself had to be mapped to the fast, but in many ways limited shared memory of a gpu, and to the peculiar way the computational threads are grouped and synchronized. even the primitives that are usually taken for granted, like the numerically robust calculation of a vector's 2-norm, present a challenge on a simt architecture. combined with the problems immanent to the block jacobi algorithms, whether sequential or parallel, like the reliable convergence criterion, a successful design of the jacobi-type gpu (h)svd is far from trivial."
"the diagonal pivoting in the cholesky, or analogously, the column pivoting in the qr factorization should be employed, if possible (see [cit] for further discussion, involving also the hyperbolic svd case). either way, a new, square pivot factor r pq is obtained. note that a unitary matrix q pq in the qr factorization does not have to be formed. a variant of the jacobi algorithm is then applied to r pq . the following variants are advisable: block-oriented (see [cit] ), when the communication (or memory access) overhead between the tasks is negligible, and full block (see [cit] ) otherwise."
"appendix b. a choice of the rotation formulas. in the block jacobi algorithms, it is vital to preserve (j-)orthogonality of the accumulated v (−t ) . in the hyperbolic case, perturbation of the hyperbolic singular values also depends on the condition number of v [19, proposition 4.4] . a simple attempt would be to try to compute each rotation as (j-)orthogonal as possible, without sacrificing performance."
"on the other end of the spectrum, we have the zranking and alertlifetime algorithms with efforts going up to 4.44 (only 1 out of 4 inspected alerts is actionable). the two alert ranking algorithms present the interesting characteristic that the effort may decrease when one inspects more alerts. this means that the actionable alerts are not ranked first, but after the 20% or 50% first. one can see in figures 1(b) and 1(c) that zranking can even perform worse than a random classifier (i.e., its fault detection rate curve is under the lower dotted line)."
"thus, r n and c n progress inwards, ending with s (1) n reversed. we tentatively denote the reverses of both r n and r n (resp. c n and c n ) by the same symbol."
"first, ranking methods can be split into two categories, those working on rules (all the alerts of a rule are considered equally good), which are generally faster; and those working on alerts: q2"
"in two of our previous papers [cit] we discussed the parallel one-sided jacobi algorithms for the hyperbolic svd with two and three levels of blocking, respectively. the outermost level is mapped to a ring of cpus which communicate according to a slightly modified modulus strategy, while the inner two (in the three-level case) are sequential and correspond to the \"fast\" (l1) and \"slow\" (l2 and higher) cache levels."
"looking again at the results in table iii, one may notice that smalllint has results consistently better than the two others, or that findbugs lead to consistently better results than pmd for ranking algorithms working on rules (see also section iv-b)."
"in this paper, we propose a framework for comparing six alert ranking algorithms and identify the best conditions to separate actionable from un-actionable alerts. we selected six alert ranking algorithms described in the literature and identified some of their characteristics that could be meaningful in this comparison:"
"we now present each of the six treatments. some of them work on individual alerts, other work on rules, considering that all alerts from a rule are equally actionable or not. table iii summarizes some information on the alert ranking algorithms."
"besides reducing the curse of dimensionality, irafnet is expected to increase accuracy and coverage of grn inference through integrating diverse information using its weighting scheme. of course, this may not always be the case, as we have seen that when datasets of different quality and different origin are combined, the overall performance may actually reduce. in this work, we only illustrate the integration of limited types of genomic data. however, irafnet can be utilized to integrate almost any data type as long as its information can be transformed into prior knowledge regarding potential regulatory relationships."
with weights proportional to the corresponding similarity measures. the similarity measure derived in step b3 is based on the assumption that when two genes are functionally related they are more likely to be affected by a similar set of genes [cit] .
"the kolmogorov-smirnov statistics reflects the degree to which a gene ontology (go) category is overrepresented at the top of the ranked list of importance scores. table 3 shows the number of go terms with significant enrichment. as shown, irafnet results in more enriched go categories than the original algorithm which relies on a single data type. supplementary table s1 in the supplementary material shows the list of go categories and corresponding p-values under each method."
a proportional-integral-derivative (pid) controller is introduced for the active suspension system. pid controller is implemented into the active system to reduce the error between the actual and desired reference values. the transfer function of the pid controller is written as:
"forest. in the original random forest algorithm, at each node, sampling is done by randomly selecting n potential regulators and, among them, the predictor maximizing the decrease in node impurity is chosen for the splitting rule. this strategy may be less effective when the number of informative predictors is small compared with the total number of genes; in such case, informative predictors have small chance of being chosen as candidates for the splitting rules. this will cause the importance scores of informative predictors to be lower compared to the cases when they have larger chance of being chosen as potential regulators."
"we denote x wt j the expression of gene g j in wild-type condition, and x ko k!j the expression of gene g j after knocking out gene g k . similarly to time-series data, weights w"
"step a1, sampling weights are derived from fs d k!j g which can be any score measuring how likely regulatory relationships fg k ! g j g are based on the dth genomic data. in particular, when scores fs"
"kp, ki and kd represent the proportional, integral and derivative gain parameters respectively. in this paper, pid controller is designed to guarantee that the output bounce and pitch displacements can be controlled and minimized as the disturbance occurs. therefore, appropriate pid parameters must be chosen in order to ensure good performance for the system. there are several methods for tuning pid parameters [cit] . in this study, the values of the three parameters of pid controller are tuned using heuristic (trial and error) method. the half vehicle model with an active suspension system using pid controller is implemented in simulink as depicted in figure ("
"however, several online tools, such as idseq (2, 3) and genome detective (4), have recently been made available for research involving pathogen discovery and identification. the cloud-based nature of these tools removes the requirement for users to have high-specification computers for data processing, and automated identification of microbial sequences reduces the need for any significant background in bioinformatics. hts data sets, with identifying information removed, are simply uploaded, and annotated sequence matches to potential pathogens are delivered within hours, in a format that can be easily interpreted by those with relevant clinical or academic skills. while idseq automatically discards any human genomic reads, the submission of data sets containing patient sequences, although anonymized, to third-party platforms necessitates ethical consideration and permission."
"one open question in the estimation of grn is whether the best performance is achieved by single models or by community methods which derive a consensus network combining results from different models [cit] . [cit] claimed that community methods perform better than single methods. despite their predictive performance, community methods remain computationally demanding algorithms which require the estimation of many different models followed by the estimation of the consensus network. for the dream5 challenge, our algorithm was compared to community, which integrated the predictions of 35 teams participating in the challenge [cit] . as we showed in the manuscript, irafnet is comparable to if not better than the community and, therefore, represents an efficient alternative (for the datasets used in this work, irafnet can finish in less than an hour on a cluster running in parallel)."
"overall, irafnet results in better predictive performance than genie3. the best predictive performance is achieved when sampling weights were obtained from knockout data alone. this result is not completely surprising since knockout data is considered one of the most informative data for inferring regulatory relationships [cit] . the slightly less optimal performance resulting from integrating all data types may be due to the inconsistency among different datasets (e.g. some datasets could have less optimal quality). this result suggests that a careful selection of input data is very important regardless the underlying algorithms."
"in this paper, pid controller has been designed and used for controlling the active suspension system of the modal of a half vehicle. the dynamic behavior of the vehicle's model is investigated under different road profile inputs. the results proved that the implementation of pid controller in an active suspension system is efficient in controlling the vehicle's vibrations for all road profiles considered. this results in great comfort and better stability of the vehicle. a future work is to apply the active suspension system for a full vehicle model and test the performance using real practical system."
"the design and analysis of the suspension system is based on the mathematical modeling of the vehicle. a quarter vehicle model (one of the four wheels) is simple and widely used for dynamic analysis in many studies [cit] . however, the disadvantage of the quarter model is its failure to capture the realistic dynamic response of the vehicle."
"in our implementation of irafnet, we treated time-series and steady-state gene expression data separately, one for deriving prior information regarding potential regulators, while the other as main input for random forest construction. alternatively, they could be combined as a single dataset and be used as input for a single source-based random forest algorithm. although combining the two datasets would increase the sample size and generally provide greater power in detecting regulations; problems may arise when the sample sizes of the two data sets are imbalanced. in such situation, the construction of tree ensemble may be largely driven by the dataset with more samples while the signals embedded in the smaller dataset may be concealed. this would be less an issue when the two datasets are used separately to train different models. for this reason, we decided to integrate time-series and steady-state gene expression in two stages. a rigorous test should be considered to evaluate and compare the performance of either combining datasets or treating them separately in a two-stage learning procedure."
"our results show that both platforms can accurately identify viral genomes in hts data sets, with little or no prior knowledge of bioinformatic approaches. idseq has the additional capability to detect bacterial genomes as well as viral genomes. while not as sensitive as some of the other methodologies tested, idseq and genome detective were able to identify all of the infectious agents included in the proficiency data set, in a fraction of the time reported for the other pipelines, and required very little local computational power. idseq, genome detective, and similar free cloud-based online tools will significantly reduce the barrier to entry for exploiting hts, without the hardware and background required for traditional bioinformatics approaches."
"(1) fig. 1 . irafnet schematics. for each gene g j 2 f1; :::; pg, we determine a ranked list of potential regulators via irafnet. based on each data d 2 f1; :::; dg, we derive weights fw d k!j g measuring the prior belief of regulatory relationships fg k ! g j g. using expression data, we run random forest to find genes regulating g j . at each node, instead of sampling a random subset of genes from the entire set of genes; we randomly choose an integer i 2 f1; :::; dg and we sample genes according to weights fw i k!j g. the final network is derived by ranking potential regulators based on the random forest importance score irafnet for grn inference i199"
"to demonstrate the advantage of integrating multiple data in the construction of grn, we consider synthetic data from the dream 4 [cit] and the dream 5 [cit] challenges, which have been used as gold test data sets for objectively comparing the performance of various grn inference models. we show that irafnet performs better than previous models in most considerations. as a real data application, we apply irafnet to the inference of yeast grn by integrating multiple public data sets. we show that our new approach has an improved performance in predicting transcription factor (tf) regulations and it also provides additional functional insights to the predicted gene regulations."
"when compared with other integrative models like bayesian network, the advantage of irafnet relies on its computational efficiency and the robust predictive performance resulting from its nonparametric nature. in fact, a limitation of many existing methods such as bayesian networks are the linearity and normality assumptions often made to reduce the computational complexity of the algorithm. although non-parametric bayesian networks have been proposed in literature [cit], they are computationally intensive and generally require very large sample size. the computational complexity of bayesian networks is amplified by the difficulties encountered in parallelizing the algorithm. in contrast to bayesian networks, irafnet can flexibly model non-linearity and higher-order interactions while being efficient on large-scale applications as it can be easily executed with parallelization. moreover, the performance of irafnet is in general robust to the number of trees in the random forest model upon our investigation."
"we compared networks resulting from genie3 and irafnet based on go terms enrichment. we focused on 58 go slim terms obtained from the saccharomyces genome database [cit] containing from 20 to 200 genes. for each model, importance scores for all regulatory relationships were derived and we focused on the 200 000 highest scored regulatory relationships (in the dream5 challenge, only the first 100 000 predicted regulations were considered for the competition, we relax the cutoff so that true predictions are less likely to be excluded due to this parameter setting). for each go term, the enrichment score was computed via a one-sided kolmogorov-smirnov test [cit] . specifically, for each go term, we considered every undirected edge between all pairs of genes contained in the go category and calculated the kolmogorov-smirnov statistics based on importance scores of undirected edges resulting from each method."
"in this article, we develop irafnet, a unified framework based on random forest which constructs grns by integrating information from multiple data types. specifically, information from different data sources is used to derive a series of weights, which, then, are utilized for sampling potential regulators during the tree construction. this weighting scheme provides multiple benefits compared with the sampling procedure adopted by the standard random for both genie3 and irafnet, we consider the set of 200 000 highest scored directed edges, referred to as d. as shown, the number of unique undirected edges a à b was 156 359 and 163 886 for genie3 and irafnet, respectively. for each method, we show the number of go categories with significant enrichment for different p-value thresholds (0.05 and 0.01). ko 0.657 (0.645, 0.673) 0.567 (0.562, 0.574) expression and ts 0.543 (0.528, 0.557) 0.536 (0.530, 0.541) expression and ppi 0.574 (0.562, 0.591) 0.557 (0.551, 0.561) for each model, the auc and the aupr and corresponding 95% confidence intervals are reported."
the importance score of each undirected edge (g s à g k ) was defined as the mean between importance scores of the two directed edges
"nowadays, different types of controller are utilized to control the active suspension system such as fuzzy logic [cit], h-infinity [cit], lqr controller [cit], pid controller [cit] and neural network [cit] ."
"step a1. for the dth supporting data, with d 2 f1; :::; dg, we derive scores fs d k!j g which measure the likelihood of regulatory events fg k ! g j g based on the dth genomic data. then, scores fs d k!j g are transformed into sampling weights fw d k!j g, which are utilized in the next step for data integration;"
"the aim of this paper is to propose the application of pid controller (proportional, integral and derivative controller) for active suspension system in a half vehicle model (one front and one rear wheel) using matlab / simulink. comparison between the performance of passive and active suspension systems are conducted under different road profiles inputs."
"upon applying laplace transform to the differential equations (1) and (2), the transfer functions x(s)/y1(s), θ(s)/y1(s), x(s)/y2(s), θ(s)/y2(s) of bounce and pitch motions due to each input y1 and y2 can be obtained. a matlab code has been written to provide these transfer functions. the input parameters for the model of half vehicle are presented in table (1). the half vehicle model with passive suspension system is implemented in simulink as depicted in figure ( 2)."
"in real world applications, only a small subset of genes is generally knocked-out and only some regulatory relationships could be inferred by this approach. to overcome this problem, we propose a method that imputes causal relationships by borrowing information from other knocked-out genes. let r be the set of knocked-out genes; then, missing causal relationships are inferred based on the following steps:"
"in this section, we evaluate the ability of our model to predict tf regulations. [cit] which used chromatin immuno-precipitation techniques to detect tf-gene interactions and provided p-values of regulations between 72 tfs and 3644 genes. based on these p-values, we derive the 'true' network. specifically, an edge between tf g k and gene g j ðg k ! g j þ is considered true if the corresponding p-value is smaller than 0.01; while the edge of the opposite direction ðg j ! g k þ is used as negative control. table 4 shows the auc and aupr for genie3 and irafnet. specifically, for irafnet, we used different set of weights derived from either knockout, time-series or proteinprotein interactions data, as well as used all these weights simultaneously."
"in contrast to protein-protein interactions, time series data can provide information on the directionality of regulatory relationships. according to the definition of granger causality, a gene g k is causal for gene g j if past values of g k are predictive for future values of g j [cit] ). for a pair of genes (g j, g k ), the expression value of gene g j at future time (t þ 1) is modeled as a linear function of the expression value of gene g k at current time (t) and the significance of regulation g k ! g j is tested via a standard t-test. the resulting p-values fp ts k!j g are, then, utilized to derive sampling weights as follows w"
three different road profiles are presented in this study to excite the vehicle's suspension system. the first profile input is a single bump with 10 cm height. the second profile input is a sine wave with amplitude 10cm and frequency 3 hz. the last profile input is a random road profile with maximum amplitude 10 cm. these three road profile inputs are illustrated in figure (4) .
"simply increasing the number of trees may not resolve the aforementioned problem. in order to better illustrate this point, we applied genie3 to infer network 1 from the dream 5 challenge using different tree numbers. as shown in table 6, the predictive performance is relatively unchanged when we increase the tree number from 500 to 5000. this result is not surprising since a larger tree number is not going to significantly change the average score of a feature across all trees when t is already large. our approach of weighted sampling allows potential regulators identified as informative by other genomic data to be more favorably selected. as a result, the corresponding importance score will be more favorably measured compared with other regulators with no prior support. the importance of appropriate prioritizing relevant features in highdimensional learning has been recognized by multiple works. for example, a recent work showed that using a bootstrap ranking to derive a robust prioritization of snps could significantly increase the performance of disease risk prediction [cit] . our work provides another demonstration on this concept and points out a direction to further improve the prediction performance under random forest framework."
"(1) ̈+ ( 2 2 − 1 1 )̇+ ( 2 (2) where m and i are mass and mass moment of inertia for the vehicle, respectively. k represents the spring stiffness and c represents the damping coefficient, l represents the distance from the center of mass to the front and rear wheels. index 1 and index 2 refer to front and rear suspension system, respectively."
the present work is to study the dynamic behavior of a half vehicle model using matlab/simulink. pid controller has been proposed for controlling an active suspension system of the modal of a half vehicle. time domain simulation has been performed to test the dynamic response of the vehicle with passive and active suspension systems. the numerical simulation has been carried out under different types of road profiles. from the results obtained it is noticed that the maximum bounce and pitch for bump road input are larger than that for sinusoidal and random inputs. it can also be seen that in the steady state response the maximum bounce and pitch for random road input are larger as compared with sinusoidal road input.
"as shown in table 1, our algorithm achieves better predictive performance compared with genie3 in terms of both auc and aupr for all the five networks involved in the dream 4 challenge. furthermore, as shown in table 1, irafnet performs similarly to the best performer in the dream4 in-silico size 100 challenge, which inferred grn from knock-out data alone [cit] ."
"the procedure utilized to sample potential regulators in step a2 [cit] . as we described in section 2.1, the importance score for a given predictor is derived by averaging the decrease in node impurities across all trees. under the standard random forest algorithm, at each node, n potential regulators are proposed as candidates for the splitting rule via random sampling. when the number of potential regulators is large, relevant variables will less likely be sampled as candidates for establishing splitting rules. consequently, for each tree, the total decrease in node impurity of relevant variables will be reduced. irafnet overcomes this problem by sampling potential regulators according to prior information so that variables supported as relevant by other data will be more frequently sampled as candidates for the splitting procedure."
"perturbation experiments, are equally weighted. this characteristic is appealing when different source of data provide equally important information about regulatory relationships. however, in real world applications, some experiments may be less informative about the network structure and an equal weighting procedure may penalize the overall performance. to overcome this problem, as future work, we consider to design a new model where the contribution of each data source is estimated and appropriately weighted within the unified random forest framework."
"it is also worth noting that prior weights may be computed using alternative methods. for example, in section 2.3.3, instead of using the jaccard index, other methods may be utilized to measure the similarity between genes. because the 'best' methods for prior weights calculation may depend on the data inputs, users are encouraged to explore different options when calculating the prior information."
"in this article, we propose irafnet--a new algorithm in which different data types are integrated under a unified random forest framework. the key idea of irafnet is to introduce a weighted sampling scheme within random forest to incorporate information from other source of data. specifically, the model considers the expression of each gene as a function of the expression of other genes. for each node in the tree ensemble, instead of randomly sampling n genes from the entire gene set as done by genie3 [cit], irafnet samples genes (the potential regulators) according to the information provided by other data such as protein-protein interactions or expression data from perturbation experiments, so that genes supported by other data as potential regulators will be favorably sampled. by doing so, information embedded in other datasets is integrated into the network construction, while the effective search space of potential regulators is significantly reduced."
"one key step of irafnet is to transform information embedded in supporting data into indicators of potential gene regulations. in this section, we focus on some commonly used data types which include steady-state gene expression, time-series gene expression, proteinprotein interactions and gene expression from knockout experiments. for each data type, we provide detailed information on how weights are derived."
"processing hts data sets is computationally intensive, may require significant investment, and often necessitates a comprehensive technical background to fully analyze the results. currently, these requirements can limit the use of hts, preventing clinicians and researchers with minimal funding or expertise in bioinformatics from exploring and exploiting this powerful technology."
"in this article, we introduce a weighted sampling scheme under the framework of random forest to allow the integration of heterogeneous data types. as shown in figure 1, first, irafnet processes supporting data to derive the prior belief of regulatory relationships among genes, then, it integrates such prior information to the main dataset via random forest to construct the final grn. we consider different genomic data including gene expression data from steadystate experiments, time-series experiments, knockout experiments and other biological data such as protein-protein interactions. as shown in figure 1, one data source is considered as main input data for random forest inference while other d datasets (supporting data) are utilized to derive prior information. irafnet can be summarized in the following major steps, and detailed information regarding each step is provided in later sections:"
"step b1. for any gene g k with g k 2 r, we derive p-values fp ko k!j g and we consider the regulatory event g k ! g j true if p ko k!j is smaller than 0.01;"
"a vehicle suspension system is a mechanism that isolates the vehicle's structure from the road irregularities. the basic function of the suspension system is to reduce the vibration transmitted to passengers in order to improve ride comfort and safety as the vehicle is passing over a bump or a hole. a passive suspension system consists of springs and dampers which have fixed coefficients. there is no mechanism for feedback control to adjust these coefficients depending on the road conditions. in contrast, an active suspension system, which represents a closed loop control system, has the ability to respond to the road irregularities. hence, the vibration transmitted to the vehicle is reduced."
"in this work, a half model of vehicle with two degrees of freedom subjected to irregular road excitation is considered as shown in figure ( 1) . a half vehicle model is used to capture the bounce motion (upward and downward) and pitch motion (rotation about center of gravity) of the vehicle's body. the governing differential equations of motion can be directly derived by lagrange's equations. the coordinates x (t) and θ (t) are used as generalized coordinates to represent the bounce and pitch motions of the vehicle respectively [cit] ."
"although only a car-like mobile robot is used in this paper, the cooperative analytical path planning method is also suitable for mobile robots with other kinematic models."
"at the second instant, the obstacles' states change and thus, robot 1 plans its path again to avoid colliding with obstacle 1. at the third instant, there may be a conflict between robot 1 and robot 2. according to equation (23), the priority value of robot 1 is greater than that of robot 2; therefore, during path planning, robot 1 can ignore robot 2, but robot 2 needs to change its path to avoid colliding with robot 1."
"the potential field method is a classic and useful method for path planning. in reference [cit], a potential field is utilized to represent the influence of complex-shaped obstacles. the influence of all obstacles is calculated by a weighted average of the circular obstacle potential fields. the potential field method is extremely fast and also striking, because of its mathematical elegance and simplicity. nevertheless, a shortcoming of this method is the existence of local minima in the potential function, which cause the robot to become trapped. in addition, the optimality of this method is not assured [cit] . the grid-based method combined with the a * algorithm is also considered a good choice for path planning [cit] . this method uses a grid-based representation of the environment. the a * algorithm can then be used to solve grid-based path planning problems [cit] . the algorithm uses a heuristic concept based on the euclidean distance to search for the shortest paths. alternatively, the breadth-first search is a graph search algorithm that is also suitable for grid-based path planning. this algorithm begins at the root node and explores all neighbouring nodes, then explores all neighbouring nodes of each nearest node. grid-based methods can effectively represent the environment; however, a shortcoming of grid-based path planning is that higher precision requires more computation, making this method unsuitable for complex environments [cit] ."
"there are also many heuristic methods that can be used for path planning, such as rapidly exploring random trees [cit], neural networks [cit], genetic algorithms [cit], simulated annealing [cit], ant colony optimization [cit], particle swarm optimizer and fuzzy logic [cit] . these methods can yield feasible schemes; however, their optimality cannot be assured with any of the above-mentioned methods."
"mobile robots are usually non-holonomic systems; on the other hand, most path planning methods are merely developed to address geometrical constraints, while neglecting kinematic ones. however, reference [cit] presents a feasible algorithm for addressing kinematic constraints."
"the generalized coordinates of the car-like robot model is shown in figure 1, where the rear wheels are driving wheels and the front wheels are steering wheels. the radius of the wheels is ρ . the car-like robot is round; o is the centre, where the cartesian coordinate of o is (x, y) and the radius of the car-like robot is ro. point w is the midpoint of the rear wheel axis and point v is the midpoint of the front wheel axis. l is the distance between m and v. θ measures the orientation of the car body with respect to the x -axis. ϕ is the steering angle. let 1 u be the angular velocity of the driving wheels and 2 u be the steering rate of the front wheels; thus, the kinematic model for the car-like robot can be obtained by equation (1). to reveal the potential linear characteristic of the kinematic model, the model can be translated into the chained form as follows:"
"path coordination methods first plan independent paths for robots separately; then, they seek to plan the robots' velocities to avoid collisions along these paths [cit] . in path coordination, the area the robots travel across is treated as a shared resource. the decomposition of path planning and velocity planning incorporates an additional time dimension into calculations."
"the rest of the paper is organized as follows. section 2 describes the kinematic model of the car-like non-holonomic mobile robot. section 3 describes the analytical path planning algorithm for one non-holonomic mobile robot. the formulation of path planning for multiple mobile robot systems is described in section 4. then, the prioritized path planning method for mmrs is presented in section 5. the experimental results are presented in section 6. section 7 concludes the paper and outlines future research."
"in this work, we presented a mapping algorithm to reduce soft errors for fpgas. our solution offered excellent soft error reduction while guaranteeing optimal mapping depth under the unit delay model. in addition, we consider power optimization to reduce the power overhead. experimental results showed that, compared to svmap and emap respectively, our algorithm setmap produced 40.6% and 48.0% more soft error rate reduction with 2.22% and 2.18% power penalty. the future work would include studying the electrical and latching-window masking effects of the fpga routing interconnects. layout-driven technology mapping will also be studied to further improve circuit reliability against soft errors."
"s v is the smallest propagated soft error cost up to node v under the constraint of an optimal mapping depth. we use equations (4) and (5) to calculate the soft error costs of the cuts and nodes iteratively and go through all the nodes from pis to pos. note equation (4) will be enhanced to include power cost and duplication cost in section 4.3. next, we present our cost estimation method of soft error for a cut itself."
"soft error can occur in the memory cell or logic circuit. traditionally, soft errors in memories have a greater impact than in logic circuits because memories have smaller cell size and a bit flip resulting in seus (single event upsets) becomes permanent before reprogramming takes place. now, soft errors in logic have become a major concern as well. previous works attempting to reduce soft errors thus have focused on these two areas: enhancing memory cells and modifying logic circuits. for example, ibm and nasa [cit] presented several sram architectures to resist seus. in terms of logic, one of the famous structures is triple modular redundancy (tmr) [cit], but the area penalty (200%) is large for this approach. moreover, tmr architecture needs a majority voting circuit to output the correct data, thus the depth of a circuit will increase. work [cit] tries to minimize area and reduce error rate at the same time. the authors follow a tmr method but only replicate the most susceptible gates for soft error protection. however, its area overhead is still very high (more than 100%)."
"this algorithm is completely distributed and each robot plans its path by utilizing local information. to realize cooperative path planning, a distributed prioritized scheme is introduced. first, each robot calculates a priority value according to its situation at each instant and then the priority value is sent to the other robots. the robot's priority is then determined to realize cooperative path planning. higher-priority robots can ignore lower-priority robots, whereas lower-priority robots should regard higher-priority robots as obstacles. after a confirmation of each robot's priority, the robots plan their paths using the basic analytical path planning method, while taking into consideration the path length constraint. to minimize the path length for mmrs, the priority value is also calculated by a path cost function that takes the path length into consideration."
"based on the cut-enumeration framework, we first present our solutions in terms of soft error cost propagation (section 4.1), cost function for a cut (section 4.2), power cost and cost adjustment (section 4.3), and cut selection (section 4.4). then, we present the overall algorithm in section 4.5."
step 1: utilize the initial boundary condition and the goal boundary condition to represent the feasible paths in terms of the adjustable parameter 6 k a .
"to verify the effectiveness of the proposed algorithm, we performed a number of simulations. the simulations were carried out on an hp computer with 4g ems memory and an intel core i2-2120 cpu."
"theorem 1: if a multiple mobile robot system has complete communication, the cooperative analytical path planning algorithm can produce paths that can prevent collisions with moving obstacles and other robots according to the path length constraints."
"based on equation (29), we can observe that only centralized planning can resolve this optimization problem. because centralized planning can create heavy communication and computation loads, the constraint for minimizing the path length of multiple mobile robots can be divided into n sub-constraints:"
"therefore, during the path planning of each robot, both the path length constraint and the criterion for avoiding dynamic obstacles and other robots should be taken into consideration."
"in this paper, the kinematic model of a car-like mobile robot is used. the main feature of the kinematic model of the car-like mobile robot is its non-holonomic constraints, because a rolling-without-slipping condition exists between the wheels and the ground."
"the second class is referred to as decoupled approaches [cit] . these approaches can find good solutions quickly, but at the cost of losing optimality and completeness. decoupled approaches are typically further sub-divided into two broad categories: path coordination and prioritized planning."
"in this paper, we reconstruct an analytical path planning method to accommodate multiple mobile robot systems with the assistance of the prioritized planning scheme. a practical and cooperative analytical path planning algorithm is presented for mmrs."
"the final condition is assume that there are three obstacles moving within the environment. all of the obstacles are assumed to be circular and their radii are all set to 0.5, and each obstacle moves at different velocities that can change dynamically. the initial position of obstacle 1 is (5, 0). the initial position of obstacle 2 is (9, 4) . the initial position of obstacle 3 is (19, 10) . the velocity of each obstacle in the simulation is shown in table 1 ."
"before we explain how p flip-m is computed, we define distance-1 vectors and distance-2 vectors. in the truth table, if two outputs are immediate neighbors (distance-1 neighbors), then the vectors to generate these two outputs are distance-1 vectors to each other. meanwhile, the vectors that generate outputs that are distance-2 neighbors in the truth table are called distance-2 vectors. in the truth table for cut c r1, output 0 is a distance-1 neighbor of four other outputs highlighted in the dark background and is a distance-2 neighbor of six other outputs highlighted in the gray background. the physical meaning for distance-1 (distance-2) vectors is that if a single-bit (double-bit) soft error occurs, one of those distance-1 (distance-2) vectors would be the original vector that experiences a bit flip (two bit flips or a double-bit soft error) to produce the current vector. to compute p flip-m, we need to examine the truth table to find out the neighboring outputs with different logic values. then, we can retrieve the corresponding vectors for these outputs. among these input vectors, each vector has a probability to generate an incorrect output logic value when one or more of the inputs in the vector had a bit flip (i.e., soft error propagated) because bit flip(s) in the input(s) would make this vector change into one of the distance-1 or distance-2 vectors which could produce a different output value. therefore, we call these types of vectors error-propagation vectors. it is obvious that some vectors are not error-propagation vectors. in figure 3(a), the vectors corresponding to the white cells with logic \"1\" will not propagate a soft error when single-bit or double-bit flips occur in their inputs. on the other hand, the error-propagation vectors are (0000) and the vectors for the dark and gray cells. to evaluate the probability of error propagation for a cut, we need to evaluate how many error-propagation vectors are there for this cut. an errorpropagation vector has at most cut_size number of distance-1 vectors and at most max_nfcut_size number of distance-2 vectors. the max_nfcut_size for a cutsize between 2 to 6 can be easily computed, which are 1, 3, 6, 10, and 15, respectively. we compute the probability to propagate a soft error for a cut due to the m-th input vector as:"
"setmap is implemented in c and merged with sis [cit] system. we show the detailed comparison results between setmap, svmap, and emap in terms of the power consumption and the soft error rate using both mcnc and iscas'89 benchmarks."
"in the cooperative analytical path planning algorithm, each robot uses the same planning scheme at each instant. first, the initial and goal conditions should be verified; then, the path from the current time position to the goal position is determined and each robot will move along its planned path at this time. at the next time interval, the boundary condition is modified and a new trajectory that can also satisfy the obstacle-avoidance criterion is planned. the flow of the cooperative analytical path planning algorithm for mmrs can be illustrated by the i th robot path planning at time"
"where c represents every cut generated for v through cut enumeration. here, the arrival time of c is max(arr i ) + 1, where arr i is the minimum arrival time on input signal i of c. all the cuts that can provide the minimum arrival time arr v form a set ma v . thus, the minimum arrival time for each node in the network is propagated from the pis through cuts and iteratively calculated until all the pos are reached by a topological order. the longest minimum arrival time of the pos is the minimum arrival time of the circuit, i.e., the optimal mapping depth of the circuit. similarly, the mapping cost can be propagated along the process of cut enumeration. the cost for a cut c can be calculated as follows [cit] :"
"as shown in figure 4, it is not immediately clear that there are no conflicts between robots and obstacles. to demonstrate the effectiveness of the algorithm, the final paths of the robots and obstacles are depicted as three-dimensional figures. in this paper, a practical and cooperative path planning method for mmrs that can produce sub-optimal or optimal obstacle-avoidance paths in real time within a dynamic environment is presented. this algorithm appropriately combines the analytical path planning method with a priority scheme. the priority scheme used in this paper does not require significant computation; thus, it can accommodate the dynamic environment."
"the cooperative path planning algorithm requires accurate robot self-localization and target localization. the paths generated by inaccurate localization information may create conflicts. however, for most real robots, whose navigation systems usually rely on vision systems or laser sensors, accurate self-localization and target localization are hard to accomplish. therefore, in future studies, we will explore how to modify the algorithm proposed in this paper to tolerate certain localization errors in a bid to expand application of the algorithm."
"overall our algorithm can be summarized in figure 4 . it mainly includes three stages. it first estimates p one values and switching activities for each node in the original network. it then uses cutenumeration to compute all possible cuts of every node and the functionality of every cut. during the cut-enumeration, delay and cost values are propagated. the compute_soft_error_cost function calculates the soft error cost for every cut following equation (9) . meanwhile, the compute_power_cost function and power cost adjustment (section 4.3) can be carried out for every cut. after cut-enumeration, an optimal mapping depth is obtained for the circuit, and the cost for each cut is also available. based on this framework, we can then pick the best cut for a node driven by the timing constraint to generate the final mapping solution."
"for the cooperative analytical path planning algorithm for mmrs, the priority has a serious effect on the quality of the planning path; thus, the priority should be determined appropriately. in this paper, a priority function is proposed to calculate the priority value. this priority value is transmitted to all robots and priority is then determined through the distributed, prioritized planning scheme."
"in this paper, we present a new soft error tolerant mapping algorithm, setmap, for fpga designs with low power. we adopt a cut-enumeration-based method that consists of cut generation and cut selection. our essential goal is to reduce soft error rate. to achieve that, we design a novel approach to effectively masking out soft errors during the mapping process. meanwhile, to make the mapper power aware, we consider switch activity in the cost function. experimental results show that our algorithm produces significant error rate reduction over previous low-power mapping algorithms, svmap [cit] and emap [cit], across a series of mcnc and icsas'89 benchmarks. to the best of our knowledge, this is the first technology mapping algorithm that targets both soft error reduction and low power for fpgas."
"briefly, at each time instant, the algorithm proposed in this paper is utilized to plan a new path from the current position to the goal position, and the path planned at the last instant is the true path along which the robots move."
the difference between single robot path planning and mmrs path planning is that various planning approaches are incorporated in the latter to handle conflict between autonomous robots. the path planning approaches for mmrs can mainly be categorized into two classes [cit] .
"we accumulate all the switching activity values on the input nodes of a cut and use this sum (cost power ) to penalize cuts that incur larger switching power. the smaller this sum is, the bigger the chance that the cut can be picked. this naturally selects cuts that hide highly switching nodes in luts to reduce power. simply adding the power cost in our cost function, however, is not accurate because of node duplication. in this paper, we carried out duplication cost adjustment, considering the specific characteristics of power cost. we use figure 1 to illustrate our solution. when cut c r1 is formed by combining subcut c 1 and c 2, node f needs to be duplicated in the mapping solution due to an extra fanout going out of c r1 . the duplicated node f has its own cost and should be added to the cost of cut c r1 . the propagated soft error cost for a cut is refined from equation (4) as follows:"
"in prioritized planning approaches, different priorities are assigned to each robot [cit], either randomly or determined by motion constraints; for example, more highly constrained robots are given higher priority. the sequence of path planning in mmrs is then determined by the priority. this method can prevent collisions between robots. the advantage of prioritized planning approaches is that they reduce a single planning problem in a very high-dimensional space into a sequence of planning problems in a much lower-dimensional space. in this approach the paths must be computed in a sequence and the sequence is determined by the priority. the computation will last until the robot with the lowest priority plans its path. thus, it can be observed that the disadvantage of the prioritized planning approach is time consumption."
"cut selection needs to select the best cuts to cover the entire circuits to complete the mapping. to map a critical node v, only the cut that provides ma v (refer to section 3.1) is picked to implement the lut to guarantee the optimal mapping depth. for the nodes that are on the non-critical paths, we can use a cut that has smaller soft error cost s c as long as the cut can still fulfil the timing requirement on the node to guarantee the optimal mapping depth."
"it is worth mentioning that although each robot plans its path according to the path length constraints, the path length of the mmrs is not guaranteed to be the shortest. the path length constraint can only ensure that each robot can produce the shortest path when it treats other robots with higher priority as obstacles. thus, the algorithm presented in this paper is a sub-optimal algorithm."
"where i is the switching activity on input i of the lut. c in is the input capacitance on an lut (a constant). o is the switching activity at the lut output. c net is the estimated output capacitance of wires and buffers contained in the net driven by the lut, which can be estimated using a wire-load model with good accuracy [cit] . we do not specifically model the static power but we try to reduce the total number of luts in our mapping solution and with a smaller area, the static power would be reduced as well. to obtain an accurate power evaluation, the gatelevel fpga power estimator fpgaeva_lp2 [cit] will be used in this study to obtain post layout power analysis. in fpgaeva_lp2, the capacitances of devices, interconnects and programmable switches are extracted after routing to calculate dynamic power during signal transition. the static power is estimated based on macro-modeling using spice simulation. fpgaeva-lp2 achieved high fidelity compared to spice simulation, and the absolute error is merely 8% on average [cit] ."
where i b is the set of all obstacles and robots in the i th robot world model that need to be considered during path planning.
"in the cooperative analytical path planning algorithm, the analytical path planning method is combined with the distributed priority scheme to obtain a new multiple mobile robot path planning method that can plan sub-optimal obstacle-avoidance paths in real time."
"where cost c is the soft error cost contributed by the cut c itself. s i is the estimated soft error cost of the fanin cone rooted on signal i. we propagate the soft error cost with the propagation process of the minimum arrival time to guarantee the optimal mapping depth. after we calculate the soft error cost for every possible cut rooted on the node v, the lowest propagated soft error cost s v in the fanin cone f v is below:"
"to illustrate this characteristic, the planned paths of two robots at six instants are shown in figure 4 . at the first instant, robot 1 and robot 2 plan their paths according to the information obtained at time 0. the planned paths can satisfy both the current and goal conditions without colliding with the three obstacles or the other mobile robots."
"analytical path planning takes into consideration both the boundaries and the obstacle-avoidance criterion and then describes the feasible paths by sixth-order piecewise-constant polynomials. thus, the path planning problem is transformed into a problem of calculating the parameters of the polynomials."
"a priority function is proposed to calculate the priority value of the mobile robots. when robot i r is located at point p, the priority value can be calculated by equation (23):"
"the mapping problem for soft error tolerance on sram-based fpgas is to cover a given l-bounded boolean network with kfeasible cones so that soft error tolerance after mapping is maximized while the optimal mapping depth is guaranteed under the unit delay model. we also strive to minimize the area and power overhead during such a mapping process. our initial networks are all 2-bounded and k is 5 and 6 in this study because most of the state-of-the-art fpga devices use these k values currently. therefore, our final mapping solution is a dag in which each node is a 5-lut or 6-lut and the edge (lut u, lut v ) exists if lut u is in input(lut v )."
"the goal of path planning is to find a continuous trajectory for a robot from the initial state to the goal state without colliding with obstacles, while maintaining robot-specific constraints [cit] . as a result of its importance, path planning has attracted much attention in the field of robotics."
"a traditional priority scheme is time consuming; computation will last until the robot with the lowest priority has planned its path. the priority scheme used in this paper overcomes the time-consumption issue. that is because once the priority has been ensured, the path will be planned in parallel, thereby accelerating the algorithm's execution."
"the cooperative analytical path planning algorithm is a decoupled strategy. given a team of robots, each robot can be assigned a priority value that can determine its priority. when planning the path, a robot with high priority can neglect robots with lower priority, whereas a robot with low priority should avoid robots with higher priority. this scheme can coordinate the paths of robots and prevent collisions between robots."
"in reference [cit], a new analytical solution to mobile robot path planning is proposed. this method can provide a real-time collision-free path for a car-like mobile robot moving in a dynamically changing environment. based on the explicit consideration of the robot's kinematic model and the collision avoidance condition, a family of feasible trajectories and their corresponding steering controls are derived in terms of one adjustable parameter. this method exhibits excellent performance with respect to precision and real-time ability for mobile robot path planning."
"the first class is referred to as coupled centralized approaches [cit], which apply global information and plans directly. these approaches treat an entire team of robots as a single composite system, to which the classical single-robot path planning algorithms are applied. these approaches pursue both optimality and completeness; thus, they are too computationally intensive to use in practice."
"each robot utilizes local information to calculate a priority value by (23) . then, each robot has to verify the order of the priority. the order will determine the way each robot coordinates path planning with other robots."
"all of the abovementioned methods concern path planning for single robots, which in turn form the basis of path planning for mmrs. the mmrs path planning problem can be defined as follows: given a set of m robots in a k-dimensional workspace, each robot has an initial starting configuration (e.g., position and orientation) and a desired goal configuration; thus, the path should be planned for each robot to reach its goal and avoid collisions with obstacles, as well as other robots in a given workspace [cit] ."
"soft errors have received much attention in the research community in recent years. a soft error occurs when a cosmic particle, such as a neutron, strikes a portion of the circuit causing the state of a node to change from 1 0 or 0 1. soft errors are becoming a serious problem in circuit design due to shrinking process dimensions. the smaller dimensions create a situation where the capacitance at each node in the circuit is lower, consequently requiring a smaller amount of charge to cause a glitch. this glitch can propagate through a logic network provided: 1) the glitch occurs on a sensitized path, i.e., there is no logical masking, 2) the glitch propagates un-attenuated or even amplified, i.e., there is no electrical masking, and 3) the glitch arrives at the data input of a storage element during the latching window, i.e., there is no latching-window masking."
"and store the truth table of each cut. then we use the iterative procedure mentioned in section 3 to estimate the soft error cost for each cut and each node in the network. a cut has a higher soft error cost if it has a bigger probability to propagate a soft error (bit flip) occurring at one of its inputs. the cost is smaller if a cut has a bigger chance to mask such a soft error to propagate from its inputs. once the soft error cost for the cut itself can be estimated, the soft error for a fanin cone and the propagated cost for a cut c can be estimated using a similar idea shown in equation (2) . however, soft error cost should not be divided by the fanout number. the error can propagate through all the fanouts of a node and it is equivalent that the cost is duplicated by the amount equal to the fanout number. therefore, the propagated soft error cost for a cut becomes:"
"without taking the obstacles into consideration, the feasible path only has to satisfy the initial boundary condition according to (2) and the boundary conditions, the boundary states"
"it is assumed that in mmrs, there are many robots moving in a two-dimensional environment. all of the robots have to avoid colliding with both moving obstacles and other robots. each robot has limited sensing and communication abilities; thus, not all of the information about the environment can be obtained. usually, information about obstacles and other robots is essential for any robot to plan its path; therefore, the world model of the mobile robots mainly maintains information about the obstacles in the environment and the robots."
"at each instant, a new path will be planned for the robot according to the initial position, the goal position and the current state of the environment. it can be seen in figure 4 that the initial position at each time instant changes, so a new path will be planned for each robot according to the current situation."
"monte carlo simulation is used to evaluate the soft error occurrence and propagation; and fpgaeva-lp2 [cit] has been applied to obtain accurate post layout power measurement. 20,000 random input vectors have been generated first. the monte carlo simulation randomly flips one bit or two bits in the circuit and then evaluates these 20,000 input vectors to obtain the output data. for double-bit flip simulation, we consider fpga architecture to table 2 . results of 5-luts in a nutshell flip two bits simultaneously among the luts in the same logic cluster guided by its occurrence probability. for each benchmark, we carried out 500 separate runs with random bit flips within each run (each simulation is driven by different 20,000 input vectors to get more stable results). the output vectors at the pos are compared with correct output data (the golden model) and calculate the total number of propagated errors for the benchmark. table 1 shows the final results. \"err rate\" columns show the error rate, which indicates the percentage where a soft error propagates all the way to a po. power columns show the power consumption reported by fpgaeva-lp2. we compare to two previously published low-power technology mappers, svmap [cit] and emap [cit] . both guarantee optimal mapping depth. comparing to svmap (the \"vs svmap\" column) and emap (the \"vs emap\" column), setmap shows 40.6% and 48.0% improvement respectively for soft error reduction with 2.22% and 2.18% power penalty on average using 6-luts. the error rate reduction is calculated as (err(setmap)-err(map2))/err(map2). power overhead is calculated as (power(setmap)-power(map2)) /power(map2). we also did a comparison on area (lut number after mapping). on average, setmap uses 5.0% more luts than svmap and 2.0% less luts than emap."
"the probabilistic roadmap method is also a popular path planning scheme for mobile robots. this method uses a sampling technique to discover a sparse representation of obstacles in a configuration space. reference [cit] makes use of a probabilistic roadmap to avoid occlusions of the target and any obstacles. the probabilistic roadmap method is easily implemented and can be applied to complex environments. however, this method may fail when the environment does not have a sufficient number of free points with which to construct a probabilistic map."
"the x, y, and z coordinates, air superficial velocity, and pressure gradient are some of the parameters yielded by the cfd method. due to the considerable computational time required for the cfd method, artificial intelligence (ai) can be highly helpful to obtain fluid characteristics at different points and dramatically reduce the computational time. this study intends to investigate the information obtained by the cfd method using ai algorithms (anfis method)."
the overall interfacial force that is active between the two stages is on the basis of the turbulent dispersion force and interphase drag force. these can be restated as relation (3):
"anfis is defined as fuzzification and defuzzification framework for providing an exact prediction of figure 1 . in the current work, (the x, y, and z coordinates, air superficial velocity) are used for obtaining (pressure gradient) as output. in the first layer, the inputs are classified in different function characteristics. as an example, the signal incoming i th rule function is stated as follows,"
"in order to stimulate the liquid and gas interactions, the two-stage model was applied, which was based on the eulerian-eulerian approach. the stages are treated like a continuum in the domain being considered in this approach. the basis of the eulerian modeling framework is the momentum transport and ensemble-averaged mass equations."
"for this study, the neural network is used to learn the data, while the fuzzy interface structure is implemented to predict the process. this process of learning and prediction within the framework of anfis structure is better than another individual model such as a single neural network or single fuzzy interface methods. single core programming is used to learn and predict data. during the learning process, 70% of data is used for learning and the rest of the data is used for the evaluation process. this process of evaluation is applied in the computing code. the evaluation process is used for the validation of our code implementation. for the prediction process, we use the meshless method. in this case, the ai predicts the non-existing data in the cfd domain. based on previous work, the number of inputs and percentage of training data have a large effect on the accuracy of training and testing."
"the best value obtained for r 2 was 0.66, indicating an insignificant improvement compared with the condition that two inputs were used under the same number of mfs (see figs. 8a and 8b) . therefore, the number of inputs was decided to increase from three to four. accordingly, air superficial velocity was added to the system as the fourth input in tandem with other three inputs (viz., x, y, z coordinates), and the pressure gradient was assumed to be the output. then, the training and testing processes with two mfs were separately accomplished for all types of mfs (see figs. 10a and 10b ). an improvement in the r 2 value (0.76) reflects the noticeably positive effect of the increased number of inputs (from 2 and 3 to 4) on the increased level of system intelligence by 10%. increasing the number of mfs to four can lead to the complete intelligence of the anfis method, such that the r 2 value for training and testing processes was obtained to be 0.97 and 0.95, respectively (figs. 11a and 11b ). this r 2 value reflects the considerably high intelligence of the anfis method and a significantly appropriate agreement between the outputs of the anfis method and that of the cfd method (see fig. 12 (a, b, c, d, e and f)). using the air superficial velocity as the input yielded considerably good results associated with intelligence, enabling different parts of bcr can also be predicted owing to the resulting anfis intelligence. fig. 13 depicts the parts of the bcr that were involved in the learning process ."
"this study uses the new framework of the anfis plus cfd method to predict the pressure gradient as an output of the anfis with combined input and output hydrodynamics parameters of the cylindrical bubble column reactor (such as mesh positions and gas speed) as the anfis input parameters. for this study, different function characteristics, the number of function and input parameters are used to achieve the accurate anfis method for the prediction of reactor. the results show that the input parameters and the number of rules significantly affect the accuracy of the intelligent algorithm. due to the high density of neural nodes for high input parameters or the high number of rules the algorithms can mimic the pressure gradient in the bubble column reactor. the membership function has a minimal effect on the intelligent of the combination of the anfis and cfd. furthermore, we suggest that to improve the accuracy of the method after using the maximum input parameters and number of rules, the tuning parameters should be considered. additionally, the results show that the output parameters such as the speed of gas in the column can be used during the learning process, and for the better understanding of this complex hydrodynamics behavior in the column, deep learning methods such as long short-term memory (lstm) [cit] ) can be considered."
"in the third layer, the relative value of each rule's firing strength is measured(j.-s. [cit] . this value is equal to each layer's weight over the overall amount of firing strengths of all rules:"
"where ̅̅̅ denotes the normalized firing strengths. in the fourth level integration algorithm, the calculation of a consequence if-then rule is applied that was suggested by previous study [cit] ."
"where wi denotes the signal out-coming from the node of the second layer and μai, μbi and μci denote the signals incoming from the mfs implemented on inputs, x coordination (x), y coordination (y), z coordination (z) and air superficial velocity (vas), to the node of the second layer."
"to commence the study using the anfis method, a part of the cfd output data was considered as the input, and the other part was used as the output. accordingly, four inputs (viz., x coordinate, y coordinate, z coordinate, and air superficial velocity as input 1, 2, 3, and 4, respectively) and one output (viz., pressure gradient) were considered for the investigations. the number of iterations, the total size of data, and p-value (i.e., a percentage of the total data used in the training process) were considered 700, 6000, and 70%, respectively. in this study, 70% of the data was used in the training process, and 30% of the remaining data accompanied with 70% of the data related to the training process were scrutinized in the testing process."
"the anfis intelligence can help prognosticate the parts failing to be involved in the learning process, which indicates the significantly high prediction power of the ai (see fig. 14 (a, b, c, d, and e)). integrating ai (anfis method) with the cfd method reduces the computational time required by the cfd method and also avoids solving complex equations by the cfd. taking advantage of the anfis intelligence can provide further information about considerably more points."
"enumerating all possible interactions based on initial actions of both players and conditions on their values of a and g, we can enumerate all possible outcomes. this is given in table 9 table 9"
"we implemented the selective reactivation method in the baseline clh and p-clh. figures 8 and 9 illustrate the performance results of our implementations with the microbenchmarks and the execution time breakdowns, respectively. the results show that compared with the baseline, our selective reactivation method is effective in lowering the communication latency in multithreaded communication cases (figure 8(a) ) and in improving the bandwidth significantly with a larger number of threads (figure 8(b) ). specifically, the latency and bandwidth are improved by 3 times and 5 times with 36 threads, respectively. these improvements come mainly from the reduction of the time spent in empty cs, as shown in figures 9(a) and 9(b) ."
"the msa is very useful for preliminarily extraction of region of interest. however, the msa for different weight of node and found that the algorithm offered some erroneous results for exudates pixel detection. hence, the results obtained from the msa with node weights will be assigned to the fine segmentation using mathematical morphology method. morphological by dilation and closing operators, introduced by zaharescu [cit], will be used to extract the most contrasted structures and the corresponding size of the exudates regions. both operators are controlled by a disc structuring element in order to find the region of interest (roi) in the retinal images. in order to identify the roi by morphological dilation method, the number of pixels is indicated as a, and a structuring element is indicated as b. the origin of b is reflected by (b) s, followed by a shift by s. the morphological dilation was explained by using eq. (11) . assign b value as the center of the original image. then, the center of the original image is dilated to be subset of avalue. the morphological dilation of image and the structuring element of s are represented asf ⊕ s, where sis the structuring element of the original image at position g(x, y) and the value of new pixels is denoted by using eq. (12) ."
"the environments themselves have an effect on the strategies of agents, these effects are introduced by the distribution of agent interactions. these effects can been seen throughout our experiments, as it has affected the success of different strategies and how cooperation has evolved. we have noted that these effects between the environment structures are still valid even when floor space is taken into account, however the adjustment of floor space also has an effect and in this work we have distinguished between differences in floor space and the environment shape. we concluded in this work that there are environment effects on agents regardless of their strategy and we have highlighted the effects in the four environments that we have tested. we have shown that the individual effects the environments had were due to how the environment shape affects the range of agents they interact with. where more open environments support a large range and less open environments support a smaller range."
"mood is represented as a number between 0 and 100, with the grouping as follows: a mood of below 10 is characterised as extremely low, below 30 as low, higher than 70 as high and above 90 as extremely high, and between 30 and 70 as neutral. definition 1 shows how the agent chooses an action based on our mood model with the simulated emotions."
"the present paper discusses a novel algorithm to detect exudates by employing composite features based on morphology mean shift algorithm. in this place, two algorithms are integrated for exudates detection and are believed to perform better than mean-shift-only algorithms in terms of detection accuracy in all experiments. this integration increases the average accuracy values which are 98.35% for exudates detection. the results of the current study have proved prominent success in detecting exudates. therefore, the algorithms can be used to evaluate color retinal images for exudates detection without requirements of experts. however, in case of detecting small exudates regions, experts are still to be consulted."
the empty environment is similar to the small world environment but with an increase of interactions between each individual agents. there is more benefit from reacting to cooperation quickly as this cooperation is likely to continue to be reciprocated with the same agent since they are more likely to interact with each other. this diminishes the advantage distrustful has in the empty environment and allows responsive to be the most effective.
"in this work, we tackled the problem of thread arbitration and synchronization in the context of mpi, and we proposed thread synchronization techniques to improve the communication performance in multithreaded communication scenarios using mpi_thread_multiple. our techniques reduce the wasted time in the critical section while preserving data locality. our method adopts a synchronization counter-based selective wakeup mechanism to reactivate waiting threads. it relies on electing and assigning at most one waiting thread to drive a communication context for improved data locality. furthermore, active threads are prioritized and synchronized by using a locality-preserving lock that is hierarchical and exploits unbounded bias for high throughput. our method does not count on an additional dedicated communication server but is incorporated into the implementation in a decentralized manner, which produces a scalable runtime systems. we implemented our techniques in a production mpi implementation, mpich. experimental results on multicore clusters show significant improvement in synthetic microbenchmarks and two mpi+openmp applications."
"the average payoff is obtained directly from the opponent, since we study how effective these agents are in an ideal situation we force all agents to be truthful. similarly the agent will not lie when communicating the emotional characteristics it is currently inhabiting. exploring how lying can affect these emotional agents is an interesting topic but it is out of scope of this paper since we are most interested in isolating the effects of movement on a mixed group of emotional agents."
"the latency benchmark is similar to the multithreaded osu latency benchmark (osu_latency_mt [cit] it assumes a global critical section (protected by g_lock) and a callbackbased request completion. poll_network is a hardware network call that progresses all outstanding operations. in particular, during this call, userprovided completion callback functions are executed when the corresponding operations have completed. complete_request simply marks the request as complete. lock ownership passing is ensured through the acquire, acquire_low (for low priority), and release calls."
"human decision making does not only use a systematic logical approach; emotions and mood both inform the decision that is made [cit] . the distinction that psychology makes between emotions and mood is that emotions are short-term feelings that are directed towards a particular object or person [cit] . in contrast mood is a long term feeling without a focus on a particular individual or object [cit] . we recognise that emotions and mood both have a psychological and physiological effect on humans [cit], however we will be focusing on the functional aspect that mood and emotion play in the decision making process."
"we use a simulated environment with our agents being modelled as e-pucks, which are small disc shaped robots. they are simulated within the player/stage application [cit] . we have selected a simulation rather than mathematical models of graph-based interactions as this naturally allows us to emulate a number of interesting properties such as asynchronous interactions, dynamic neighbourhoods, and differing rates of interaction between agents."
"since a request object is associated with a scount_t object, we can tie the waiting thread to the request by making the waiting thread wait on the associated scount_t object. in addition, because the one-to-one mapping between a thread and a scount_t object is maintained, one scount_t object can be regarded as representing one waiting thread. if the thread cannot complete the request and is required to wait, it will not enter the cs using the progress loop path but instead will be blocked inside the synchronization object if a server already exists, as shown at line 33. when the request is finished, the signal operation is performed by the server at the callback function (line 15). the scount_t objects are linked in a globally shared doubly linked list (line 9), so that the server can find a target waiter when needed. with the doubly linked list, we can efficiently remove a waiter from the list when it is signaled, that is, when the request is finished by the server."
"the random environment has responsive as its most successful character, by reacting quickly to both defection and cooperation it can protect itself from the pure defectors while keeping its payoff high through cooperation. this stops the character from being taken over by the pure defectors. as the environment splits the agents into groups, there is a chance that the agents may not encounter any pure defectors. this allows different agents to be dominant in different groups within the environment which is shown by how close each characteristic is in terms of number of games dominant."
"the increase in cache misses happens mainly in the issuing part of the benchmark because of the fifo ownership passing exercised in the clh lock used in clh and p-clh. even though issuers initiate mpi operations and make progress, they take turns in entering the cs, thus causing the data movement and resulting in the increase in cache misses. the decreasing message rate with a larger number of threads signifies the problem of thread synchronization, especially lock ownership passing, between active threads."
"this experiment explores how cooperation evolves and whether it is affected by differing initial mood levels. the initial level of mood will be categorised into three types, low, medium and high where low has a mood level of 30, medium is 50 and high is 70. there will be seven scenarios each with a different distribution of these levels among the agents which can be seen in table 5 . we have kept the scenarios the same as from previous work. each of these scenarios will be run against a number of sub-scenarios. the sub-scenarios define how many agents will be in the environment, with a range from 27 to 108 agents. the details of the scenarios can be seen in table 4 . again, we have changed the number of agents from previous work and as such we can also keep the number of agents consistent between the mood and emotion experiments. each scenario will also contain an equal distribution of each emotional characteristic, with the initial actions distributed equally among them. we keep to previous work by having the admiration threshold for each agent set to 3(high). we predict that there will be little difference in the level of cooperation from the previous work. we also predict that the mood will stop individual characteristics becoming dominant as the mood evens out the differences in average payoffs."
"the benchmark suite also provides three implementations: mpi-only, mpi+openmp, and openmp. the mpi+openmp implementation, however, is a mpi_thread_single application where openmp is used only for parallel loops. our strategy for the hybrid implementation is to further subdivide the matrix into smaller domains and assign those smaller domains to each thread. thus, each thread has to perform both communication and computation. communication between threads in a node is done via shared memory for collectives and shared states with appropriate synchronization."
"first, we introduce a \"locality-preserving locking with unbounded bias\" method based on clh, which we refer to as clhub (figure 12 ). the lock is designed to provide the necessary property for our purpose-the lock is biased toward the high-priority thread that most recently released the lock. it is implemented by combining a simple spin lock (i.e., a biased lock) to exploit the lock monopolization with two clh locks to handle high and low priorities while taking advantage of the fifo lock (e.g., reducing thread contention on the lock by queuing threads in the lock structure). in the figure, the data structure for the lock includes the following fields (lines 1-6): bias -a spin lock that has a biased behavior; fifoh and fifol -clh locks to block threads in the high-priority and low-priority paths, respectively; and filter -a flag to switch between two priorities."
"(a) latency (the lower, the better) (b) bandwidth (the higher, the better) (c) message rate (the higher, the better) fig. 9 . execution time breakdown of the p-clh and p-clh-usc results in figure 8 . we omit the breakdown of the clh-usc results because it shows a pattern similar to that of the p-clh-usc results. issue, poll, empty cs, and sync represent time spent in issuing operations, time spent in making progress, time wasted in cs without doing useful work, and time spent in synchronizations, respectively. the timing overhead is about 8%, 1%, and 7% on average (harmonic mean) of the total execution time for the latency, bandwidth, and message rate benchmark respectively. implementing the selective reactivation requires two important changes to the runtime: (1) defining a synchronization counter implementation and (2) associating a counter per blocking operation and storing its reference in the corresponding request objects. in figure 6, we assume a generic synchronization counter object referred to by the abstract type scount_t. this object supports two types of operation: a signal operation is translated to scount_signal(), and a wait operation is translated to scount_wait(). each request contains a scount_t object (line 5) representing our synchronization structure. for an mpi_waitall operation, a reference to the counter object will be initialized to the number of pending operations and stored in all of them to consume signal events."
"2) to deal with the performance challenges, two different database with local dataset and a publicly available dataset named diaretdb1 is dedicated to this study. finally, the performance of all previous algorithms in the literature is used to compare the accuracy performance with the newly method in terms of both image-based basis and pixel-based classification. the algorithms used for comparisons are nb, svm, fcm, nn, mlp, pca and k-mean clustering."
"with the empty environment allowing all agents to meet other agents briefly, there is a benefit to an agent taking a more cautious approach and reacting to defection quickly to protect itself from the defectors. additionally reacting to cooperation slowly, allows the agent to take an advantage as the defectors do. this then ensures that cooperation is likely to continue when the agent does eventually choose cooperation. so, an agent should react quickly to defection and slowly to cooperation, such as in the distrustful character, which is reflected in the results."
"we have looked for any notable differences in environments for the mood experiment, as shown in figure 7 . while there is little difference in the regular, small world, and empty environments, the random environment however achieves high levels of cooperation more quickly. we have noted how the random environment separates the agents into smaller groups which can not interact with each other. we have also noted how there are dips in cooperation in the mood scenarios as agents with high moods meet agents with low moods, and the cooperation continues as the low moods rise more quickly than the high moods reduce. when we combine these two things we can conclude that when there are few agents with a high chance of meeting every agent in that group, the low level moods will meet the high level moods more quickly than in situations with more agents in a more open environment. this will cause the low level moods to rise more quickly in the smaller environment. in conclusion we can say that emotions enable a stable level of cooperation, and with the addition of mood can allow cooperation to flourish."
"rely on the fifo property of the lock without the monopolization through acquire_low(). since active threads always advance the system, in addition to raising their priority over waiting threads, we synchronize their concurrent accesses with a locality-preserving high-throughput lock. the locality preservation is achieved through a competitive ownership passing, which results in core-level unbounded lock monopolization. the monopolization achieves locality preservation of the lock and critical section data but does not cause starvation for waiting threads in practice since active threads are guaranteed to complete their operations in a bounded number of steps. figure 13 illustrates an example usage of our localitypreserving lock of figure 12 . in the figure, threads t0, t1, and t2 perform acquire(), and t0 succeeds initially. in acquire(), trying to acquire the biased lock (bias) first (line 9 in figure 12 ) allows the same thread, here t0, to execute the cs in a loop without interfering with other threads because of the lock monopolization behavior. both t1 and t2 fail to acquire bias, and thus they attempt to acquire a fifo lock, fifoh (line 10). only t1 will succeed and become the candidate for entering the cs after t0; t2 is queued in fifoh. the candidate t1 waits on the biased lock for its turn (line 12) but is able to succeed only if t0 releases and does not immediately reacquire the lock. when that happens, t1 becomes the owner of bias, and it elects t2 waiting on fifoh as the candidate by releasing fifoh (line 14). on the other fig. 13 . illustration of five threads (t0,..,t4) using the locality-preserving lock: (a) t0 performs acquire() and succeeds, taking bias. right after t0, t1 and t2 perform acquire() while t3 and t4 perform acquire_low(). as a result, t1, t2, t3, and t4 spin at bias (i.e., t1 becomes candidate), fifoh, filter, and fifol, respectively. (b) t0 releases bias. (c) if t0 calls acquire() immediately again, it has a higher chance of taking bias than does t1 because of locality. (d) if t0 finishes and moves on, t1 can succeed, acquiring bias. (e) only when there is no high-priority thread is filter released and low-priority threads can acquire bias."
"hpccg is a miniapp from the mantevo benchmark suite [cit] . the miniapp represents a close approximation to a finitevolume application. the communication pattern is irregular, mainly due to several sparse matrix-vector multiplication steps generated by the application. the problem size is determined by the size of the matrix, in this case generated by the number of rows per process. the communication is performed prior to the local computation by using mainly point-to-point mpi calls. (a) results of graph500 using non-blocking calls (b) results of graph500 using blocking calls fig. 16 . graph500 results (harmonic mean over 16 runs) using a weak-scaling experiment with problem size scale 24 per node (i.e., 32 at 256 nodes) and 16 threads per node, in terms of traversed edges per second (teps -the higher, the better)."
"the proposed methods have been constructed a better body of knowledge to solve the problem of exudates detection using machine learning and clustering method, which is presented and evaluated on two different databases. the msa is proposed for coarse segmentation of retinal images. it can separate exudates lesions and background parts of the image with great information. exudates in retinal images generally have a clear relative in intensity, exudates are brighter than background. finally, the efficiency of coarse segmentation is used to fine segmentation stage by using mma. the schematic diagram of the proposed method is illustrated in figure 1 ."
"we propose in this work to build on our prioritization method a synchronization model that incorporates mpi and hardware information to achieve an o(1) reactivation of waiting threads and locality preservation. the key components of this method are as follows. first, at most one thread among waiters (none if there are no waiters), called the server, is elected to drive a communication context. restricting access to a single waiter improves locality of the communication context data structures. second, the reactivation of waiting threads is driven by mpi knowledge; waiting threads use private synchronization counters to wait, track their pending mpi operations, and receive wakeup signals from the server on their completion. third, in addition to raising the priority of active threads, we synchronize their concurrent accesses with a locality-preserving high-throughput lock achieved by a combination of unbounded lock monopolization and numaawareness. results show significant improvement in synthetic microbenchmarks and two mpi+openmp applications."
we have changed the number of robots from previous work as we predict that in the random environment the very low densities of the previous work of 9 agents will struggle to interact at all. we have also lowered the very high densities as we have made some of the environments smaller to account for the differing floor space; we want to ensure that all robots are able to fit into the arena and have the chance of movement.
"message passing interface (mpi) applications are moving toward interoperating with mpi through multiple threads. the primary driving factors are ease of programmability for emerging fine-grained threading models and the desire to efficiently utilize modern network fabrics, which require multiple communicating cores to fully exploit their capabilities. in order to meet such expectations, thread safety is a prerequisite, and its corresponding overheads should be minimal."
"the intricacies of the mpi standard render designing and implementing correct and efficient support for threading nontrivial. as a result, most production implementations satisfy the core of the thread compliance through locks, since using exclusively lock-free or wait-free objects is complex to implement and to maintain. mpi implementations were shown to suffer significantly from scalability issues due to lock contention. thus, several works explored different directions for potential improvements. three orthogonal aspects contribute to the costs of locking: lock granularity, ownership passing latency, and ownership arbitration. lock granularity in mpi implementations were explored in prior works [cit], and reducing lock ownership passing latency has been extensively studied [cit] . in our prior work [cit], we identified arbitration as an important factor that was missing from the literature. here, we build on our prior findings and provide further insight and improvements that combine more advanced arbitration and locality-preserving methods."
"regardless of the state in which a thread is, however, the arbitration of the concurrent accesses is dictated solely by the lock arbitration. that is, lock ownership passing defines the order in which threads execute the critical section. in order to promote progress of the system, our prior work exploited this distinction between the threads in the locking implementation [cit] . we considered active threads as having a higher priority; thus we extended traditional lock acquisition interfaces with a low-priority interface. the goal was to have waiting threads acquire the lock with lower priority than that of active threads (line 33). by combining this method with fifo locks (ticket [cit] and clh [cit] ), we achieved better communication progress by promoting operation injection into the network and reducing waste. this solution, however, relies on a blind fifo arbitration among waiting threads. that is, if only one among n threads has its operation completed, o(n ) lock-passing operations are needed in order to reactivate it."
"the exudates detection experiments with two different databases is developed. the proposed algorithms show performance in the following three aspects: first, the current study consistently outperforms all other approaches in all the se, sp and ac experiments."
"to the best of our knowledge, no other methods can perform the best consistently in all experiments. second, the proposed morphology mean shift algorithm achieves the best performance in the publicly available diaretdb1 database and local database without tuning its parameters, significantly better performance of the proposed algorithms comparing to other algorithms in the literature. finally, the exudates segmentation results significantly outperform all other algorithms for a large number of the color retinal images. this demonstrates that the presented algorithms are very successful in detecting exudates. as the original retinal images generally contain an amount of noise and low contrast, therefore, the difference pre-processing methods for removing noise and increasing contrasts of the original images to reveal the uneven contrast is used. afterward, the mmsa to segment the retinal images into exudates and non-exudates regions is proposed. in the first place, an msa with node weighted is a very good method for segmenting the exudates and non-exudates regions. in this stage, the optimal weight is well suited for exudates segmentation. however, the results still have certain flaws in detecting tiny exudates regions. to increase the performance of exudates segmentation, a mathematical morphology is assigned to classify pixels to the closest cluster in the exudates or non-exudates."
"tit-for-tat initially cooperates, then mimics the opponent's last move joss tit-for-tat with a 10% chance of defection tester initially defect. if the opponent retaliates with defection play tit-for-tat until the end of the game, otherwise play cooperate twice then repeat."
"in the more open environments we can see that the games are closer with the characteristics that achieve cooperation quickly while protecting themselves from being taken advantage of become dominant. this is due to the low number of times that agents meet with the same agent. this requires the agent to protect its payoff quickly as it is unlikely to be able to punish this behaviour or force the cooperation to happen. later in the experiment when the mood effects take place and cooperation is enforced, the difference comes at the beginning, where agents that protected their payoff do better, whether they took advantage of cooperators or cooperated to raise their payoffs. this can be seen by the dominant characteristics of responsive and distrustful."
"negative moods lead to a more logical outcome as people tend to think more thoroughly about the action they will take [cit] . in our experiments we use low moods to lead to defection, as this is the nash equilibrium and can be considered the more rational decision. very low mood levels will lead to defection regardless of the emotional state of the agents."
"the major arbitration aspect of locking in mpi implementations is the relation between threads waiting for communication progress (waiting threads), which occurs in routines with blocking semantics, and the others (active threads). when threads in either of these states compete for the same resource (e.g., a critical section that protects a message queue), ownership passing to a waiting thread does not guarantee communication to advance. consequently, wasted resource acquisitions unnecessarily degrade the progress of active threads. we showed in our prior work that such a scenario often occurs when an unfair lock is monopolized by waiting threads [cit] . we also demonstrated that prioritizing active threads and adopting first-in-first-out (fifo) ownership passing within each class of threads can significantly improve communication progress. this method, however, scales poorly. first, it suffers from residual wasted acquisitions caused by a blind o(n ) lookup complexity to find a thread capable of making progress among n waiters. second, it is data locality oblivious."
"as the high moods are being taken advantage of the most, we expect that the payoffs for the defectors should be the highest when faced with the highest mood. the average scores of the defectors are also shown in table 16 and clearly show that the defectors do the best when faced with high moods, meaning that they will replicate the fastest in the high mood scenarios. the medium and low moods do not collapse as they adapt to the newly replicated defectors through the use of their directed emotion strategy. the high moods do not do this as when the mood is very high they act as pure cooperators. we then looked at the increase in defectors for each mood level, expecting low moods to have the smallest increase and high moods to have the highest. the results from experiment, as shown in table 17, show an unexpected outcome: the highest increase in defectors is in the low mood levels, while medium and high mood showed expected results. we can explain why low mood levels do both the best and the worst as the standard deviation is much higher than the other mood levels. low mood levels act closer to pure defectors which enables them to keep the payoffs of pure defectors low as they always defect so both agents will attain an average of 1. the difference comes when the low moods attempt a cooperative action; if the low mood agent attempts a cooperate action with a pure defector it raises the pure defector's average higher than the majority of the low agents. when the replication happens the pure defectors will always replicate, causing the high increase in pure defectors. however if the low mood agents attempts a cooperative action with the other emotional agents such that the emotional agents start cooperating, their high average prevents pure defectors from replicating as they can not get the advantage from any of the other agents. leading to the high average increase and high standard deviation."
"there are a number of emotional characters which have differing thresholds for these emotions. the full set of characters is shown in table 2, and are intended to show a range of characteristics that could reflect a simple simulation of personality differences. an agent's anger increases by one when its opponent defects; gratitude increases when the opponent cooperates. for example take the two characteristics responsive and active. if responsive chooses to cooperate. active's gratitude increases to one, if active chose to defect then responsive's anger increases to one. responsive's anger level is at the anger threshold, so in the next game with that agent, responsive will choose to defect and the anger level will return to 0."
"in this paper, we leverage knowledge from the mpi runtime to reactivate waiting threads in o(1) complexity. furthermore, prior works ignored the memory hierarchy and incurred heavy lock-passing costs. we address this issue as well by promoting locality-preserving practices. we use the o(n ) localityoblivious prior work implemented on top of the fifo-based clh lock [cit] and the production mpich [cit] as the baseline for comparison. we choose mpich for its thread-compliance maturity and its relevance as one of the most widely adopted mpi libraries, used directly or indirectly through third-party platform-specific tuned derivatives. we do believe, however, that the findings in this paper can be beneficial to other mpi implementations as well as other communication runtimes."
"our representation of positive mood values comes from psychology literature showing how people take riskier behaviour to achieve a more ideal outcome [cit] . however if the mood is too positive, as it is when a person has mania, then the behaviour becomes extremely likely to hurt that person [cit] show that negative moods can be more likely to lead people to make a more logical and thought out choice. research into human patients with depression shows that these people are more likely to choose defection in a prisoner's dilemma game. the research also showed that depressed patients were more critical of themselves [cit] . this provides us with grounding for our choice of defection as part of our implementation of the mood model in the prisoner's dilemma, and validates how the mood values are more greatly affected when the mood is low."
"to justify our claim that the speed at which cooperation is achieved is proportional to the starting level of mood, we have plotted the average mood values against the number of (coop,coop) actions, as can be seen in figure 6 . we have shown this against scenario 1 as this is where the effect is most pronounced; we can see that when the cooperation between agents falls, the average mood level still rises. as cooperation rises the standard deviation of mood levels gets wider but as the standard deviation gets smaller the cooperation still rises, showing us that the low moods are rising more quickly than high moods are lowering. this shows us that the mood reflects the level of cooperation, and the higher the starting level of mood the faster cooperation is achieved."
"the waste in the latency and bandwidth results is caused primarily by the increased number of network polls when more threads are involved in the communication. figure 4 shows figure 2 . issue, poll, empty cs, and sync represent time spent in issuing operations, time spent in making progress, time wasted in the cs without doing useful work, and time spent in synchronizations, respectively. the timing overhead is about 8%, 1% and 6% on average (harmonic mean) of the total execution time for the latency, bandwidth and message rate benchmark respectively. the average number of network polls performed per message in the latency benchmark (we omit the result of the bandwidth benchmark because it showed a similar pattern). although the number of messages is constant across all experiments, the number of network polls per message increases as the number of threads increases. recall that any waiting threads executing a blocking operation can enter the cs through the progress loop. if more waiting threads enter the cs and perform unnecessary work (e.g., network polling), it can delay active threads from proceeding and consequently slow the overall performance. by prioritizing the main path, p-clh enables more work to be injected into the runtime compared with clh; hence it improves the performance because of the presence of more active threads. however, p-clh still suffers from o(n ) fifo ownership passing between n waiting threads, which reduces its performance with increased numbers of threads."
"the regular environment shows the largest difference from the other environments, due to the way agents can move around more freely than in the small world. an agent in the regular environment takes longer to get to other agents on the other side of the environment. this allows agents who take advantage to increase their payoffs as they will be meeting the same agents more frequently. as the there is more freedom of movement in the regular arena than in the small world environment, this allows cooperation to be attained provided the agent can be assured that this cooperation will be reciprocated. by taking a balanced approach and protecting its payoff as well as taking advantage consistently allows an agent to be successful in this environment, as shown by the success of the impartial characteristic. we can conclude from the results that characteristics are affected by differences in environment, and that pure defectors affect which strategies are successful."
"supporting mpi_thread_multiple-allowing multiple threads to concurrently call mpi-is nontrivial. first, the mpi standard imposes certain ordering 1 and progress 2 guarantees on thread-compliant implementations, which restrict the thread safety design space. second, application threads are allowed to share mpi objects, such as communicators and requests, and may suffer contention because of the resulting sharing of mpi internal data structures (e.g., message queues). combined with the large set of mpi routines with disparate thread safety requirements, these intricacies make correct and efficient support for threading arduous. as a result, most production mpi runtimes implement the core of the thread compliance through locking, since lock-free or wait-free designs are complex to implement and to maintain. moreover, the majority of mpi implementations ensure thread safety through a single global lock (often referred to as global granularity). such a conservative approach is prone to high contention, but its simplicity makes correct threading support less error prone than with fine-grained designs. in this study, we also focus on the global granularity. other related work on the topic of thread granularity is reviewed in section vi."
"for a lock-based mpi implementation, arbitration of the concurrent accesses is an important factor. suppose that one thread is performing a blocking operation (e.g., mpi_recv or mpi_wait) and successfully acquires the global lock of the mpi library. if the operation cannot be satisfied immediately, however, the thread must release the lock. failure to do so prevents other threads from entering the critical section, violates mpi progress requirements, and may lead to a deadlock. thus, arbitrating lock ownership has correctness implications. in the remainder of paper, all our thread-safe methods guarantee arbitration correctness, and the focus will be their performance implications."
"to reduce such difficulties, a robust msa with node weights for each node is proposed. before applying mean shift procedure, a node weight is used to find the neighbors of data points. these procedures are now explained in detail below. 1. suppose w i is the weights of a node i and it is calculated as eq. (9)."
"the simulated emotions that will be implemented in our agents are based on the ortony, clore and collins model of emotions, known as the occ model [cit] . the model was developed through psychology research and has been used throughout the ai community [cit] b ). the occ model takes a functional view of emotions, in which emotions influence changes in behaviour. the action taken is a result of the emotional makeup of the person, which is a result of all the previous outcomes. this functional view lends itself to being a good platform for implementing emotions as the descriptions are of the outward effects of the emotions rather than how emotions are processed internally. of the 22 emotions defined in the occ model we will be modelling anger, gratitude and admiration, so we can compare to previous work [cit] b; [cit],b,c) . moreover, anger and gratitude intuitively make sense in the context of defection and cooperation."
"in the random environment, the agents are more limited in the range of characters they can interact with. this closed off environment allows the active characteristic to become the most dominant by a wide margin. the advantage that can be achieved from defecting in this environment is reduced as the agent is likely to be punished since the chance that the agent meets the same agent again is heightened, however a small advantage can be taken provided that the agent protects the payoff quickly by reacting to this punishment. this is seen by the success of responsive and active. we have compared the differences in payoffs based on the distance moved for the mood experiment (table 12 ) and the emotion experiment (table 11 ). we again see that the distance moved has the same effect in the mood experiment as it does in the emotion experiment. the differences being that the payoffs in the mood experiment are higher due to the simulated mood raising every agent's payoffs."
"after that, the od detected is superimposed on the original retinal image (see figure 3(d) ). however, the method maybe detect the wrong od regions. therefore, a binary dilation with a disc structure element with size 6 is used to reconstructed binary image to obtain a better od detected image and then used a thesholding method with 0.68 to obtain a binary image again. the difference between the reconstructed image and a binary image gives the large od candidates image (figure 3 (e) and (f), for an illustration). this method is extremely efficient. consider in figure 3 (f), the od localization gives significant intensity about the region of the od. similarly, it has been observed that the od regions has a similar intensity of the exudates in an image. thus, by thresholding the candidates contrast image, any regions has compactness more than 2,500 pixels (fixed for all of retinal images) will be assumed are the od area. for each connected component of the structures, the od areas are extracted and empirical selection is done by eq. 5."
"the empty environment is constructed to have no obstacles. the random environment is different for each run of the experiment, its shape is constructed from the regular environment. the inner obstacles are split into 20 equal sized blocks which are then placed randomly within the environment while ensuring that they do not overlap."
"to construct training datasets of exudates segmentation, three expert ophthalmologist manually segmented a pixel of representative the exudates and non-exudates. an almost balanced training data sets was established to segment any possible bias toward either of the exudates and non-exudates pixels. the training data sets of 72,214 exudates pixels from 28 segmented abnormal images and 77,422 non-exudates pixels from 28 segmented normal images. however, the od regions were excluded from all retinal images. several examples of common non-exudates and exudates are shown in figure 4 . the difficulty of small dot (see figure 4 (b)) detection comes from their extreme variability in shape and contrast, which makes them difficult to detect, and easily causes false positives. the exudates (figure 4(d) ) are a common type of exudates, which are very difficult to detect, because they are similar to cotton wool sport and microaneurysms regions. (a) image after pre-processing, (b) gray scale closed background and vessel information, (c) approximate optic disc pixels by using the optimal threshold, (d) overlaid on the preprocessed image, (e) dilated image using morphological, (f) candidate optic disc regions using the optimal threshold with values 0.68, (g) optic disc maker specifically in the compactness, (h) watershed segmentation of the imposed image, (i) inverted binary image with maker specifically in the compactness, (j) edge detection using sobel method, (k) overlaid of the internal markers on the original image, (l) final result, after optic disc boundaries localized."
"drive a communication context while making all other waiting threads wait outside the cs until their request is completed. at most one thread, called the server, is elected among all waiting threads, and only the server is allowed to enter the cs and poll the network for communication progression. in contrast to a previously studied approach that utilizes a dedicated communication server [cit], our method is a decentralized one that can be easily integrated in existing mpi implementations, which do not assume a centralized entity for making progress or thread arbitration. restricting access to a single waiter also increases the likelihood of residency in cache of the communication context data structures and avoids contention for the cs."
"when run on a computer workstation with core i3 3.30 ghz cpu and 4 [cit], the proposed algorithm, on an average, takes 1.24 minutes per image for exudates detection (46 seconds for preprocessing stage and 31 seconds for candidate exudates segmentation by using msa and plus 7 seconds for final exudates detection by using mma). however, the time required to localize the od and to segment the exudates is relatively high which makes the time requirement a bit higher and needs further improvement."
"having an equal distribution of emotional characters initially makes sure that we test character strength without being affected by characteristics having an initially higher representation. we will run each combination of scenario and sub-scenario 10 times. each run will last for 10 minutes during which the agents move around and interact, which allows sufficient interactions and replication to take place. we record data for each interaction including: agents involved, actions chosen, current number of games, current average, time initiated, and distance travelled. we also record the number of each characteristic at the end of the run, as well as the final averages for each agent. this provides us a good dataset to perform a deep analysis on our agents. we are expecting the active agent to be most dominant in our emotion experiment, as in previous work. we expect some variation in rankings due to the random nature of the interactions, if the active agent continues to be dominant in all environments then we can say that some strategies are more successful despite differences in environment or floor space."
"where p are the pixel in the initial image, min and max are the minimum and maximum intensity value in the moving window and u w is the expected output minimum and maximum intensity value and defined as eq. (2)."
"notation actions are c (cooperate) and d (defect); the anger threshold is a, superscript denotes a player, e.g. a c is the anger threshold of the cooperating player; and similarly the gratitude threshold is g."
"where a is its od area and perimeter is obtained by counting the corresponding number of od pixels (see [cit] ). the result is illustrated in figure 3 (g). the background is very clean, and the od regions are kept. after that, the marker-controlled watershed method of gradient magnitude is used to segment the image the external marker is obtained manually by drawing a circle enclosing od of region interest (see figure 3 (h)) and the internal-marker is determined by a morphological operation with structure element was empirically set to 29 pixel size and automatically by combining methods including, thresholding operation and sobel edge detector. in figure 3 (i), the white pixels belong to the background area and the dark pixels belong to the od area and superimposed on the retinal images is shown in figure 3 (j). the final edge detection result of od is illustrated in figure 3 (k) and superimposed on the original images is shown in figure 3(l) ."
"remaining or new waiting threads become waiters and wait until the server completes their request. a waiter indicates its waiting intent through a synchronization counter whose initial value equals the number of pending operations that it is waiting for. when the server completes a request, it enables the waiter associated with the request to continue the execution with a signal (i.e., work-driven, selective reactivation). to avoid starvation, when the server finishes its own request, it elects another waiter, if there is one, to hand over its server role; otherwise, the waiters will be waiting forever, since none is making progress. note that the number of servers depends on the network hardware and communication volume. for our purpose, a single server is sufficient; but the selective reactivation method can easily be extended for multiple servers. figure 6 depicts our selective reactivation method with a single server. fig. 8 . performance results of clh and p-clh with selective reactivation. the message size is 64 bytes for the latency and message rate benchmarks and is 1 mb for the bandwidth benchmark. clh-usc and p-clh-usc represent the results using selective reactivation with a user-level synchronization counter implementation along with clh and p-clh, respectively."
"some early work on supporting the interoperability between threads and mpi, such as mimpi [cit] and mpich-mt [cit], focused only on thread safety issues, since multicore machines did not exist at that time. another approach is to implement mpi processes as threads [cit] . while this approach can bring performance benefits for on-node communication by exploiting efficient data sharing between threads, it requires completely new implementations of both the mpi runtime and shared-memory programming model runtime. it also needs compiler support to privatize global variables for each thread because mpi processes, which are implemented as threads, have to own separate memory space for global variables."
"an example implementation of the synchronization counter is illustrated in figure 7 . it assumes generic lock and condition variable implementations underneath. a scount_wait operation publishes the intent to wait for n events. scount_signal events decrement the counter and wake up the thread when reaching zero (i.e., all events are satisfied). the force argument forces a thread wakeup regardless of the counter value. this is essential for electing the next server when the current one is leaving the runtime. in the case of waiting for the completion of multiple requests, more than one request object may share one synchronization counter object. the server issues signals whenever it completes one of the requests, and a thread wakeup is triggered when the counter has reached 0 (see line 15 in figure 6 and the routine scount_signal in figure 7 ). this strategy prevents the waiter thread from being woken up multiple times, many of those just to figure out that not all its requests are finished, and thus adding meaningless (a) latency (the lower, the better) (b) bandwidth (the higher, the better) (c) message rate (the higher, the better) fig. 10 . performance results of the vanilla mpich-3.2 (mtx) and our modification using selective reactivation with the pthreads mutex and kernel-level synchronization counter (mtx-ksc). the message size is 64 bytes for the latency and message rate and is 1 mb for the bandwidth benchmark. fig. 11 . ping-pong latency between a thread pinned to core 0 and another thread pinned to a different core indicated by the thread id using (1) pthreads mutex and kernel-level synchronization counter (mtx-ksc) and (2) the clh lock and user-level synchronization counter (clh-usc)."
"we plan to explore more communication kernels and applications with our runtime. when more and more clusters with a larger number of cores are deployed, we expect even greater performance improvements on those systems using our thread synchronization techniques."
positive moods tend towards an ideal outcome even if that affects the person negatively [cit] . in our experiment the riskiest behaviour is cooperation as it can lead to the worst outcome for the individual agent. cooperation is the most ideal outcome as it gives the highest payoff for the group as a whole.
"the amount the agent cares is represented by applying the mood to our α value, such that higher moods give a lower α. this results in mood changes being larger when the mood is low. if the mood is low then the agent \"thinks\" that it is doing poorly in the environment when compared to other agents. we do this to represent the property that humans care more about equality when doing poorly in society [cit] ."
"we illustrate the relationship between thread safety and mpi communication in figure 1 . it describes a simplified implementation of mpi_isend and mpi_wait. these routines are examples of a nonblocking mpi call, to send a message, and a blocking mpi call, to wait for its completion. in the example we distinguish two major code paths with distinct progress properties. the first, which we refer to as the main path, is taken by both routines between the first lock acquisition (lines 18 and 40) and the last lock release operation (lines 22 and 44). this code path is similar to most mpi routines and often advances the system. the other path, which we refer to as the progress loop, concerns only blocking calls. it is characterized by a tight loop (line 28) waiting for the completion of a target operation. this path does not guarantee progress and is characterized by high-frequency lock acquire/release operations. thus, by our prior definition, threads executing the progress loop are waiting threads and the others are active threads. we consider lock acquisitions by waiters as wasted (or simply as causing waste) when they yield no progress while active threads are waiting for the lock."
"1) the main objective of this propose method is develop a study for exudates detection in color retinal images by using morphology mean shift algorithm by a combining the msa and mma concepts to improve the exudates detection performance. through this application, experts will be able to screen exudates faster from unseen images. this will benefit the dr patients in general, as their treatment time will be allocated more adequately and functionally."
"previous work has shown that simulating emotions within agents (we refer to these agents as emotional agents throughout the paper) can influence the evolution of cooperation within the prisoner's dilemma game [cit],b), with initial work on showing how adding mobility can effect which strategies are the most successful [cit],b) . a simulated model of mood has been proposed, which was developed with a grounding in psychology, and which has been shown to increase the level of cooperation in the prisoner's dilemma game when added to simulated emotions [cit] c) . [cit] have shown that with agents without emotions the environment type influences the evolution of cooperation in a social dilemma situation."
"the selective reactivation method can also be applied to mtx by using a kernel-level synchronization counter, which can be implemented with a pthreads condition variable and a counter. however, since the kernel-level synchronization counter has higher latency due to its kernel-specific implementation, it should be used only when oversubscription is required. the performance improvement when applying the selective reactivation technique to mtx is shown in figure 10 . the selective reactivation (denoted as mtx-ksc) performs significantly better than the baseline in the latency and bandwidth benchmarks. however, the absolute latency of mtx-ksc is still far worse than that of clh and p-clh using selective reactivation (p-clh-usc and p-clh-usc in figure 8(a) ) because of the higher latency of both the kernel-level lock and synchronization counter implementations. figure 11 shows up to sixfold differences in latency when using a kernel-level and a user-level lock and synchronization counter."
"when the environments become more closed, the payoffs achieved given by taking advantage early become more important, especially if the agents are able to get to other agents more quickly. we see this in the small world environment where the dominant characteristics take the most advantage of other characteristics with distrustful being the most dominant as it protects its payoff the most. we have seen that the regular and small world environments are similar, however in our experiment the regular environment acts more open due to its larger corridors. this makes the successful characteristics closer in the number of games dominant, as in the empty environment. dominant characteristics in the regular environment react quickly to defection as previous noted, however this environment also allows consistent interactions with the same agent. this allows agents which react to defection slowly to also become dominant provided they also take an advantage from the agents. this is seen by the success of the passive, stubborn, and responsive characteristics."
"the prisoner's dilemma is a social dilemma where two players are given the choice of cooperation or defection. this choice is made simultaneously with no communication prior to the decision made. each player then will get a payoff according to the choices made by both players. the payoffs for the game are 3 for each agent when they both cooperate, 1 for each agent when they both defecting and 5 for the agent which defects in a non-mutual outcome and 0 for the cooperative agent. the game matrix is shown in table 1, with player one choosing a row, player two choosing a column, and both players receiving the payoff indicated in each cell."
"when looking at the prisoner's dilemma outcomes, it seems in the best interest of both players to both play cooperatively since this would lead to the largest total payoff for the group as a whole. however, there is a temptation to defect as this can lead to a higher individual payoff. when both players reason this way, this then leads to the nash equilibrium of (defect, defect ), which gives the worst outcome for the group as a whole, highlighting the dilemma of the game. investigating methods by which self-interested agents can be incentivised to cooperate in the prisoner's dilemma has been an active area of research in the past decades, with a particular focus on the evolution of cooperation within groups of agents [cit] . it is for this reason that we adopt this model of interaction in the current work as well."
"we use latency, bandwidth, and message rate as metrics to evaluate the performance of our baseline clh and p-clh. we note that the performance of the baseline is superior to that of mtx for mpi_thread_multiple when there is no oversubscription [cit] ."
"the medium moods do the best with the smallest increase in pure defectors as they are quick to adapt to the pure defectors as these agents are using the emotional aspect of their decision making. this is also an advantage in getting cooperation between other emotional agents since by using their emotional aspected of decision making are more responsive to cooperation. this allows the medium mood agents to increase their payoff between each other, which the pure defectors can not do and since the medium moods have adapted to the pure defectors they do not replicate as often. the high moods act similarly to pure cooperators allowing pure defectors to take advantage quickly as mentioned, leading to the higher increase in defectors. figure 13 dominant characteristics in the resilience experiment by environment, excluding pure defectors figure 13 shows which characters are successful in the resilience experiment. in contrast to the mood experiment, the number of runs with a dominant characteristic is much higher. the defectors have an effect on the other agents' mood, causing the agents to act more unpredictably. this allows advantages to be taken within each run by the relevant characteristics."
"the images are taken in different patient and different types of cameras. the images show important color, size and quality diversities. moreover, the intensity within the retinal image and between images can be significantly different. this variation makes the image processing methods development more challenging. therefore, pre-processing is necessary for original retinal images. the illumination correction is required to normalize of the image, contrast enhancement, noise removal, and the localization of the od before starting coarse segmentation stage. different image processing algorithms are adopted to make the retinal images more clear and enhanced so that accurate exudates detection can be performed. at the volume 7, 2019 figure 1. overall procedure of exudates detection using the mmsa."
"we start by giving the background to this work including an introduction to the prisoner's dilemma game. we then explain the implementation of simulated emotions along with the background of previous work that have used this implementation. following on from this we reintroduce the implementation of the simulated model of mood and the justification of this implementation. we explain our experiments that we will be conducting along with where we will be making our comparisons for analysis. we then discuss our main contribution, which is a deeper analysis of the mood model showing that the emotional characteristics do not make a large difference against identical strategies. however we show that they do make a difference when faced with pure defectors. we also give a deeper analysis of the differences between different environments, showing that the shape of the environment does have an effect. then we conclude this work describing the contributions in more detail."
"we have expanded the analysis of the emotional agents, and the agents with mood, by taking this deeper analysis of the agents we have seen how the two different types of agents differ and how this can affect both the cooperation of the group and what characters are successful. we have pinpointed how group size can affect these agents, as well as multiple effects that the environment can have. the deeper analysis of the resilience experiment with agents that use simulated mood have shown that low starting moods provide a strong yet brittle form of resilience to pure defectors. we have also highlighted how the characters in the resilience experiment have a more significant effect on which character is successful when compared to an experiment where all agents are using the simulated mood for their decision making."
"we aim in this paper to gain a deeper understanding of how different environments affect the evolution of cooperation within emotional agents and emotional agents with mood. we consider four types of environment: regular, small-world, random and an empty environment. we have also scaled the environments so that they all have the same amount of floor space for the agents to move around in. the construction of the environments will be discussed later in the paper. we continue to explore the developed mood model in practise to further understand how cooperation flourishes within a society of agents. the resilience of cooperation growth achieved is tested by the addition of defectors, indicating the stability of the cooperation strategy that uses our model. in this work, we combine previous efforts by giving simulated emotional agents the opportunity to move around in the environment, and therefore allowing them to interact with many other agents over time. we examine whether the environment structure has the same effect on emotional agents as it does on non-emotional agents. by giving our agents mobility we aim to give a more accurate description of the evolution of cooperation in a multi-agent setting."
our idea for solving the latency and bandwidth problems described in section iii-c is to allow only one waiting thread to the routine scount_wait allows waiting for n events and scount_signal decrements the number of pending events (cur_count) for c and wakes up the corresponding thread when there are no more pending events. we also allow forced wakeup with the boolean force.
"in the current study, the morphological dilation function with the value of structuring element of 7 is used to segment the exudates pixels (see figure 8) . therefore, the proposed method are used on other retinal images with the same structuring element size. afterward, the otsu thresholding method [cit] with 0.68 value is performed for a binary image to candidate the exudates pixels (see the result in figure 8(c) ). two classes, the exudates and backgrounds, are assigned to candidate from the expert annotation. then, all exudates pixels in figure 8 (c) will be overlaid on the original retinal image for locating the exudates pixels (see figure 8(d) ). finally, a sobel edge operator is applied to detect the edge of exudates regions, and the result is shown in figure 8(d) ."
in this work we will be exploring cooperation in the prisoner's dilemma using emotional agents with and without mood. we are also investigating how the environment shape can affect which strategies are successful. to achieve this we have constructed a number of experiments to be conducted. these experiments will take place in four different environments as shown in figure 1 . [cit] . the graph and the environment equivalents are shown in figure 2 .
"the random environment had the effect of separating the agents into groups, limiting the range of agents that could be played against. this causes the number of games with a particular agent to go up when compared to the other environments. this changes the dynamic of the game as agents are able to boost their scores with mutual cooperation and prevent losses with mutual defection, unlike more open environments where this dynamic is reversed. this leads to the agent which is most able to get into mutual outcomes quickly to be the most effective, which is the responsive character."
"overall, applying a classification method and a machine learning methods in color retinal images still offers the following challenges. firstly, machine learning methods require high computing power for training processing time to candidate the exudates and non-exudates regions. secondly, classification methods have a disadvantage regarding low accuracy in detecting exudates."
"where ne(i) denotes the neighbors of node iand k(i, l) is the length between a node i and a node j. therefore, the msa vector with node weights at positionx in feature space is explained as eq. (10)."
"to reveal where most of the execution times in figure 2 are spent, we divided the execution time into four parts as shown in figure 3 . the breakdown timing is measured and reported at the sender side (similar results can be obtained at the receiver side). in the figure, as the number of threads increases, the execution time of each portion increases. most notably, in the latency and bandwidth cases, we see a large increase in empty cs, that is, wasted time in the critical section (cs) without doing useful work."
"to support other blocking operations without requests such as mpi_win_flush, we can tie the waiting thread to associated pending objects, for example, mpi_window, in a similar way. we leave for future work the task of extending our selective reactivation method to support these kinds of operations."
"the mood model will only affect the decision making process when an agent has no emotional attachment to the opponent, that is the agent has not interacted with the agent previously. the mood levels will only override the current emotional decision when they are either extremely high or low. we have done this to represent that mood levels in humans do not necessarily reflect cooperation within the group, but affects the choice an individual makes [cit] ."
"in the preceding subsection, we showed that our selective reactivation technique can eliminate wasted executions and ensure that any thread entering the cs has a high chance of performing useful work. this method, however, does not improve the performance of the message rate case (figure 9(c) ). the reason is that message rates were bound by the injection rate of nonblocking calls to the runtime that suffers from intranode data movement, as was analyzed in section iii-d. based on that analysis, we propose a new locking strategy that is designed to reduce cache misses."
"additionally, the machine learning algorithms is take a long time in training process. it is important to note here that the efficiency of exudates identification can be obtained only when using a combination of modified mean shift with node weight integrated with mathematical morphology method. however, an efficient methods and implementation of mathematical morphology method make the computing time compatible with application."
