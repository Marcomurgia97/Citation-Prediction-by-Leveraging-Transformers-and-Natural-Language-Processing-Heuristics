text
"the third step is to define the order n of the n-gram language model. then, the model parameters can be estimated using the defined lexicon and the tokenized training corpus. high values of n are required if one wants to ensure modeling long term dependencies."
"to conclude, we compare our multilingual recognition system with state-of-the art systems (table 6 ). in our system, the optical model has a simpler architecture compared to state-of-the-art systems while these systems rest on language models based on words or hybrid models of words and characters. multilingual systems based on multigrams reach performance closed to most of the state-of-the-art systems, while the system complexity is reduced. the specialized system presented in table 6 refer to a selective approach where the right specialized model is always selected (an ideal case). we show that, whatever the test set (fr or en), our unified system makes profit of the combination of the languages to outperform the selective approach. system wer (%) on rimes wer (%) on iam our unified system 9.8 (2-multigrams) 11.2 (5-multigrams) our specialized system 10.8 (2-multigrams) 13.5 (5-multigrams) [cit] 9.6 9.3 [cit] 11.8 11.9 [cit] 7.9 10.5"
"such approaches should have the advantage to train a unique system whatever the languages that are considered. this should avoid to face the issues highlighted in the selective approaches, when the system is composed of specialized models."
one can first notice that the unified optical model (us and uu) significantly improves the specialized framework whatever the test set (fr and en) and the configuration of the system (specialized or unified). the optical model takes advantage of language similarities to be more robust and more efficient.
"for the two-dimensional (2d) case, wavelet denoising algorithm transforms the image to time-frequency domain under dwt processing, then we could keep only some large coefficients and throw away the rest using a properly threshold-level, too. the result is that a small number of largest coefficients which has key information are saved while most noise coefficients that are small will be discarded, and the so-called image ewc is emerged. if we reconstruct the image from this ewc, the noise would be reduced effectively [cit] . despite the many advantages, there is also difficult to research the optimal threshold level to form the image ewc [cit] ."
"the cs theory which was put forward by donoho, candes, tao and others, has offered a framework for the detection of sparse signals with a reduced number of samples [cit] ."
"designing a unified optical model can be done easily by combining the character sets. combining the character sets may have many advantages when the languages are of the same origin, i.e. where some characters are shared between the languages. on the one hand, the system complexity is reduced as the shared characters are modeled only once. in addition, the number of occurrences of the shared characters is larger in the training set and the optical model can take benefit from this. moreover, we also expect the optical model to generalize better as the shared characters come from various languages, so they appear in different contexts in the text."
"in this paper, we proposes an end-to-end unified multilingual recognition system for handwriting recognition, where both the optical model and the language model combine various languages. both models rest on state-of-the-art methods. on the one hand, we make use of an optical model based on deep recurrent neural networks. on the other hand, our language model uses sequences of sub-lexical units, called multigrams. dealing with multigrams allows to reduce the lexicon size from each language and thus to build a unified language model of reasonable size. we evaluate our approach on languages of the same origin (french and english), as the similarities between the languages may impact the capacity of the system. in our experiments, we show that a) combining languages of the same origin in a unified framework allows to strengthen the capacity of the optical model; b) combining the languages does not significantly affect the robustness of the language model. this allows to build a language model estimated on all the training corpus, without the need to separate the languages; c) build a unified multilingual system seems better than dealing with specific-language systems, where one of them must be selected to provide a prediction."
"(dc) were removed and a hemisection contralateral to the recording side was performed in th11. intracellular recording was made with glass microelectrodes filled with 2 m potassium acetate (tip diameter 1.0-2.0 µm, impedance 2-5 m ). lrn neurones were identified via antidromic activation from the ipsilateral cerebellar white matter dorsal to the interpositus nucleus at a depth of 6 mm below the cerebellar surface (insertion point of the electrode: 1-2 mm caudal to the primary fissure at a laterality of 4 mm). the arrival of the incoming volley to the lrn neurons was recorded by silver ball electrode at the surface near the patch used for the intracellular recordings. the stimulating and recording sites were verified histologically. the position of nr was also verified by recording the antidromic response followed by stimulation of the rubrospinal axons in th13."
the beam search parameter has been fixed to 0.8 in all the experimentations while language model scale and word insertion penalty are optimized for each scenario (see below).
"in all of the investigated lrn neurons there was a broad convergence between the various spino-lrn systems as shown in table 2 . of particular interest in this study is the fact that lrn neurons with input from presumed c3-c4 pns evoked from the pyramid and nr, also exhibited converging excitation and/or inhibition from the pyramid, ift and bvfrt systems. this holds true also for lrn neurons with input from cervical or lumbar bvfrt systems."
"the fluorescence response of microarray is so weak that the noises would affect the image quality seriously. moreover, the wavelet soft-threshold denoising algorithm has poor effective on this low psnr image. considering that the cs theory provides a more effective method for reconstruction of perfect sparse coefficients despite the noise pollution in wavelet domain, it not only avoids the problem of threshold estimation, but also recovers the completely sparse representation of wavelet coefficients. especially, the ndoa-optimized wavelet denoising method based on cs has achieved good results. when the compressed sampling rate is 0.875 for microarray image, compared with the traditional wavelet soft-threshold denoising algorithm, the rmse is reduced significantly and the psnr is increased about 9db. the results show that the ndoa-optimized denoising method improves the effectiveness in reducing the noise of microarray images."
"image denoising algorithm has received considerable attention in various fields. most of the conventional filtering techniques, such as mean filter, gaussian filter and minimum mean squared error filter, cannot always guarantee the acceptable quality of denoised image with the high peak signal noise ratio (psnr) and so on [cit] . in recent decades, the discrete wavelet transform (dwt) has been applied to dispose the problem of noise reduction, and it has been shown to be outperformed to traditional filters in terms of root mean squared error (rmse), psnr and other evaluation indicators [cit] ."
"according to equation 1, the composition of the tokens, lexicon and language model transducers is performed after successive application of the minimization and determination processes on the lexicon and language model transducers:"
"in order to further delineate the lrn effects of c3-c4 pn activation, the dlf was transected in c2 and c5. the isolated c2 to c5 strip of dlf was then stimulated electrically using a monopolar tungsten electrode. figure 4 shows that stimulation of cortico-and rubro-spinal fibers in the c3 dlf evoked disynaptic epsps and ipsps in lrn neurones with latencies within the expected disynaptic range of 1.6-2.4 ms. these data further indicate that the disynaptic lrn effects of cortico-and rubro-spinal excitation are mediated by pns located in the c3 to c4 segments."
"the search graph can be decoded using dynamic programming such as the well-known viterbi algorithm. here the search graph is a weighted finite state transducer (wfst). it is build by the composition of three sub-transducers representing the tokens (characters), the lexicon and the language model (grammar) respectively. the token transducer (t) represents all possible characters that can be produced by the unified optical model, from the input frames. the lexicon transducer (l) represents the possible kmultigrams of the languages of interest that can be produced from the character set. the language model transducer (g) represents the n-gram language model of k-multigrams which is trained from the multilingual training text dataset."
"the cs flitting system is composed of sparse transform, compression sampling and signal reconstruction, which is shown in figure 1 [cit] . generally, the mallat algorithm is often used to form the sparse transform. and that the wavelet coefficients of noiseless signals obtained by mallat have sparsity. if there is noisy, the sparsity of wavelet coefficients can be greatly reduced. based on the above foundation of cs theory and sparse features, we can reconstruct the sparsity of wavelet coefficients and achieve the denoising signals with highquality."
"obviously, there is an underdetermined system. since the x is k-sparse, the freedom of x is only k + 1 degree. thus the accurate recovery of x could be obtained by some nonlinear algorithms when the number of observations is no less than k + 1. the cs theory has pointed out that the signal x would be reconstructed with an observation matrix by minimizing the following type of l 0 -norm."
"then we could obtain the denoised image by the inverse wavelet transform and draw the fitting performance at the same time. coefficients reconstruction images of wavelet denoising based on cs processed by omp, swomp and ndoa respectively, are shown in figure 8 ."
"it is clear that the spino-lrn-cerebellar route provides information from many spinal pre-motoneuronal centers (cf. [cit] ) and as pointed out in a companion perspective article [cit] in this research topic, the ift, bvfrt and c3-c4 pn systems may reflect a phylogenetic development in need of increased control of dexterous forelimb movements. we first discuss the input from the c3-c4 pn system alone, then the convergence of effects from all three systems and finally functional implications of the convergence patterns."
"the c3-c4 propriospinal system is of special interest because these neurones project not only to the lrn, but also to forelimb motoneurones (mns), [cit] . the function of the c3-c4 pns has been investigated in behavioral experiments, revealing a role in mediating the voluntary command for visually guided forelimb reaching in the cat [cit] b; alstermark and kümmel, 1990) and additionally for precision grip in the macaque monkey [cit] . the c3-c4 pns are characterized by monosynaptic excitation from cortico-, rubro-, tecto-and reticulospinal fibers as well as from cutaneous and muscle afferents in the forelimb nerves . in addition, all of these converging descending and sensory inputs can mediate disynaptic inhibition of c3-c4 pns via feed-forward and feed-back inhibitory interneurones [cit] c) . these excitatory and inhibitory inputs are integrated by c3-c4 pns, which then excite or inhibit forelimb mns [cit],b; [cit] ) ."
"if we deal with high-frequency lh, lv and ld coefficients with soft-thresholds according to the noise features independently, the wavelet soft-threshold denoising reconstruction could be obtained from ewc of lh, lv and ld. and the finally reconstruction is indicated in figure 4 ."
"designing a unified language model is attractive for taking into account similarities between languages of the same origin that exist at the lexical, morphological or syntactic levels [cit] ). in this respect, language models based on sub-lexical units have been recently proposed for handwritten recognition [cit] . dealing with sub-lexical units has a number of advantages: on the one hand, it allows to significantly reduce the lexicon size and, on the other hand, to improve the recognition rate, as it partly tackles the out-of-vocabulary words problem."
"our experiments on english and french languages with small or large lexicons, highlighted that the optical model benefits from the language unification and provides significant improvements compared to specialized systems. a major contribution is to show that unifying languages that have some similarities does not affect the language models which provide similar results than the specialized language models. in addition, dealing with multigrams allows to improve the traditional language models based on words or characters. finally, our system reaches state-of-the art performance with a unique and less complex system."
"in addition to targeting mns, c3-c4 pn output bifurcates and ascends to the lrn, potentially providing the cerebellum with efference copy, now often referred to as internal feedback. such information about the ongoing reaching movement may allow the cerebellum to quickly modify the descending motor command via the rubro-and reticulospinal systems [cit] ) . this idea was recently supported by a combined electrophysiological, optogenetic and behavioral study in the mouse, in which the ascending branch from v2a pns to the lrn could be selectively activated [cit] . these authors, found that photoactivation of v2a pn terminals evoked strong activation of lrn neurons that caused errors in reaching, but not grasping."
"we now report experimentation results on the large en and fr datasets described in section 4.1. figure 4 presents the word error rate (wer) obtained on the fr and en test datasets respectively. similarly to small lexicons, we observe that unifying the optical models (us and uu) improves the recognition rate. unifying the language models does not impact the recognition performance, even if the unified lexicons are highly imbalanced (the unified word lexicon has 116k words with only 29k from the french language). moreover, dealing with multigrams language models generally reduces the wer, compared to systems based on traditional language models of words or characters."
"as well known, the image under wavelet transform (wt) has sparse properties, even though the sparsity of wavelet coefficients will be greatly reduced when the signal is noisy. according to cs theory, the original or clean signal can be reconstructed from a small amount of the projection coefficients by an appropriate optimization algorithm [cit] . therefore, we can rely on cs to reconstruct the sparsity of wavelet coefficients, which not only avoids optimizing threshold in wavelet threshold filter, but also recovers the wavelet coefficients with most sparse representation and without losing important information. that is, a new denoising method for sparse signal based on cs could be applied for achieving the high quality image."
"similar to the traditional n-gram language model of words, the n-gram language model of sub-lexical units (made of multigrams) is estimated with back-off coefficients. the idea is to replace words in the traditional language model by their corresponding multigrams sub-lexical units."
"moreover, languages models based on multigrams often outperform traditional language models based on words or characters and there is always a system based on multigrams which is better than systems based on traditional models. another advantage of dealing with multigrams is related to the lexicon size, which is highly reduced compared to word lexicons. for instance, there is 116k words in the large unified en + fr lexicon while there is 71.6k 5-multigrams, 16.9k 3-multigrams and only 3.1k 2-multigrams."
"during test, we are interested in quantifying the contribution of each stage (i.e. optical model and language model) to the performance of the unified system. in this respect, we conducted four experiments (scenarios). the first one (uu) consists in having a whole unified system (both the optical model and the language model are unified). the second experiment (us) consists in combining a unified optical model with a specialized language model. the third experiment (su) combines a specialized optical model with a unified language model. finally, the last one (ss) is composed of specialized models (both the optical model and the language model). in the following section we report and analyze the performance of these four configuration of the system."
"finally one can notice that the performance obtained on the iam dataset is rather low for any of the language model type (word, character or k-multigrams), and system unification type (ss, su, us, uu), when a small vocabulary is used for training the language models. this is a particular difficulty encountered on the iam dataset because of the low lexicon coverage rate of the training dataset lexicon on the test dataset. this is the reason why most studies have come to use additional linguistic resources to get better performance."
"although the ndoa-optimized denoising method has good performance for low psnr image, we still have the challenge of computational complexity of cs reconstruction based on ndoa. fortunately, the parallel computation by hardware or software would be an effective way to solve this problem, so the hardware implementation and the software parallel operation of wavelet denoising based on cs optimized by ndoa will need to be further studied to improve the real-time performance."
"we now evaluate our approach on the small and large en and fr vocabularies. we first present observations we obtained for the four experiments described above in sections 4.3.1 and 4.3.2 and then discussed the results in section 4.4. finally, we compare our system with state-of-the-art methods."
"in a previous work [cit] ), we introduced a unified bilingual recognition system based on syllables. the proposed system is composed of a unified optical model based on hmms and a bilingual n-gram language model of syllables. in this work, french and english linguistic ressources were used to decompose words into syllables. besides, no out-of-vocabulary (oov) words were considered by the language models. this simplifies the recognition task but this is a serious limitation. this preliminary work showed that a unified language model based on sub-lexical units (syllables) is feasible and can be envisaged as an alternative to word-based language models. a major issue concerns the linguistic expertise which is required to get word decompositions into syllables, and syllables databases are only available for a few number of languages."
"the ift-system is characterized by strong activation from forelimb nerves [cit] b; [cit] b) . in order to restrict activation to ift neurones in the forelimb segments (c6-th1), the dorsal column (dc) was transected in c5."
"equation 3 once the hsmm has been trained, the decoding step consists in assigning a sequence of multigrams to any sequence of characters of the training corpus. in other words, the goal is to find the most probable sequence of multigrams q *, given any observation sequence o 1:t of the training corpus, which is defined as follow:"
"training a multigram-based n-gram language model involves three main steps (as for training any n-gram statistical model): the training corpus tokenization; the lexicon size and type determination; and the language model parameter estimation with a fixed n-gram order. the tokenisation step replaces any word of the training corpus by its corresponding k-multigram decomposition, where k is the multigram order (see section 3.1). table 1 gives an example of word decomposition into k-multigrams, for various values of k. the training corpus that contains text from two or more languages can be considered as a multilingual language model training corpus. the second step is to fix the size of the language model lexicon. due to the compact size of the lexicon of k-multigrams, all words in the training corpus can be considered. the multigram order k defines the type of k-multigram lexicon."
"this theory can realize the perfect reconstruction of the original signal at a rate far below nyquist, and it provides a strong support for the accurate recovery of sparse signals."
"designing a unified language model is less straightforward than designing a unified optical model. this is due to the word lexicons sizes which are often large (to model as much as possible each language). thus, combining word lexicons from various languages strongly increases the model complexity and may become intractable. one solution is to consider a lexicon of characters but language models based on characters perform often poorly (plötz [cit] )."
"offline handwriting recognition is a challenging task due to the high variability of data, as writing styles, the quality of the input image and the lack of contextual information. the recognition is commonly done in two steps. first, an optical character recognition (ocr) model is optimized for recognizing sequences of characters from input images of handwritten texts (plötz [cit] . second, a language model is used to model language constraints [cit] . at decoding step, both models are combined to get a prediction. recent improvements in deep learning techniques [cit] have enhanced the capacity of optical models. in contrast, only few works have been dedicated to language models for handwriting recognition. they are generally trained to model sequences of words and sequences or characters )."
"as one of the great advances in modern technology, a dna microarray was designed to detect specific gene sequences, which were developed as probes fixed at specified positions to perform a large number of different hybridization experiments simultaneously on a single glass substrate. it's wellsuited to quantitatively compare the expression levels of many genes, as well as easily used for qualitative detection to study the dna sequences [cit] ."
"the selective approaches are based on specialized recognition systems, i.e. each system is dedicated to one language, and the output of one of them is selected at the time of processing one specific sample [cit] ). there are two kinds of selective approaches. the first one consists in applying specialized recognition systems in parallel to get several transcripts. the transcript with the highest confidence score is then selected. a major issue is to compare the confident scores given by each specialized system, as they span over different scales [cit] )."
"the basic idea of cs denoising algorithm is introduced below. if the noise ω is taken into account, the mixed signal of f +ω will no longer be completely sparse in wavelet domain. but if we still make the signal f + ω compress by ϕ, the (4) would be obtained as"
"the location of lrn neurones receiving monosynaptic pyramidal and rubral excitation and disynaptic excitation and inhibition mediated by c3-c4 pns from the pyramid and nr, is shown in figures 7 and 8, respectively. the lrn was divided into 1 mm thick segments, the lower being the most caudal section. recordings were made mainly from the caudal and middle part of the nucleus, which receives most of the input from the spinal cord. in figures 7 and 8 are shown: left column, the location of cells with monosynaptic excitation; middle column, the location of cells with disynaptic excitation and right column, the location of cells with disynaptic inhibition (filled circles). empty circles indicate recorded cells lacking synaptic input from these systems."
"after combined c2 and c5 dlf lesions, it was common to find a mixture of excitation and inhibition in many of the lrn neurones following cortico-and rubro-spinal fiber stimulation. disynaptic epsps evoked by a train of stimuli in the c3 dlf are shown in figure 4a . in another cell illustrated in figure 4b, a mixture of epsps and ipsps were evoked at the same latency when the membrane potential remained virtually unchanged, shown in figures 4c,d . it can be more easily observed in the expanded sweeps (lower records). these results strongly suggest that a subpopulation of lrn neurones receives mixed input from excitatory and inhibitory c3-c4 pns."
"the wavelet denoising algorithm has been well acknowledged as an important method. in mathematics, the essence of wavelet denoising is a function approximation problem. in other words, that is how to find the best approximation of the original signal in the wavelet space developed by the scaling and translation of the wavelet generating function, according to the proposed criteria, so as to achieve the complete distinction between the original signal and the noise signal. compared with noise feature, the larger amplitude in wavelet domain is the coefficients with important signal characteristics, while the amplitude of noise coefficients is smaller [cit] . therefore, the wavelet coefficients with larger absolute value can be retained or contracted only by setting an appropriate threshold and the estimated wavelet coefficients (ewc) has been obtained."
"our unified optical model is optimized to recognize a unified character set. in case of languages of the same origin, unifying the character set reduces the system complexity and increases the number of training examples per character classes that are shared between the languages. while traditional language models are based on words, which can become intractable in case of unified lexicons, we proposed to build a language model based on sub-lexical units, called multigrams. dealing with multigrams has many advantages: the multigrams are obtained using a data-driven process without the need of linguistic expertise; it reduces the model complexity compared to words; finally, it allows a better modeling of long dependencies than with characters."
"microarray contains a series of probes which have lots of dna fragments on a small piece of glass. in order to eliminate the brightness interference caused by imperceptible flaws, a morphological filter with circular structural element and multiple filtering strategy has been firstly designed as when we take the p [cit] sub-array from the fluorescence image in figure 2, called p [cit] sub-image, we could apply mallat algorithm to decompose this sub-image into four sectors, denoted as ll, lh, lv and ld, under the sym8 wt, as shown in figure 3 ."
"the goal of reaching is to approach and then grasp an object, and the timing between these two motor components is critical information about grasping, reaching and posture can be mediated via the ift, c3-c4 pn and bvfrt systems to the lrn. in the lrn, a comparison can be made about the activity level of each system in isolation, but also their combined effects. the overview can then be further analyzed in the deep cerebellar nuclei and cerebellar cortex."
"in the process of microarray scanning, the excitation spectra of probes induced from the fluorescence dye are usually stored as 16-bit tiff by the scanner [cit] . however, due to the weak fluorescence response, complex biochemical reaction, imperfections in glass slide and photoelectric sensor conversion distortion, etc, the signal of fluorescence probe is inevitably degraded, which leads to serious noise interference in the microarray image. this contamination will directly affect the accuracy of quantitative analysis [cit] ."
"this cs filtering system includes three parts: sparse transform, compression sampling and signal reconstruction. among them, cs reconstruction for sparse signal is the most important step [cit] . in cs theory, the approximate solution is usually obtained by solving the l 1 -norm [cit] . and there are many ways to take the optimal solutions for cs reconstruction, including greedy algorithm, convex optimization algorithm and so on [cit] . as a global optimum approximation, ndoa introduces energy function to solve the convex optimization problem by transforming the energy function into the corresponding global convergent differential equations, which has the advantages of parallel computation and suitable for software and hardware implementation. furthermore, the effectiveness of ndoa-optimized reconstruction of cs gets better work than the orthogonal matching pursuit (omp) and its improved algorithms, such as regularized orthogonal matching pursuit (romp), stagewise weak omp (swomp) and subspace pursuit (sp), etc [cit] . it has obvious application advantages in many image processing fields."
"we train one hsmm per language. each one is optimized to produce the most frequent multigrams which appear in the training texts of a given language. finally, language specific multigram lexicons are combined. after the tokenization step of the whole training corpus into multigrams, the unified language model is trained as described in section 3.2.2. at test time, the unified language model is applied on a text whatever its language, using the decoding step described in section 3.2.3."
"compared to the specialized language model, the contribution of a unified language model appears rather moderate, as results are generally equivalent whatever the lexicon size. in fact, there is no reason to believe that some improvements should be gain by combining the english and french languages. indeed, the language modeling task should become more complex due to some language similarities. the key point is that unifying both languages into the language model does not affect the system performance, which allows to design a system with only one language model whatever the languages."
"for a successful movement. the role of the c3-c4 pns is to mediate the command for reaching as has been demonstrated in the cat, monkey and mouse [cit] . importantly, in the mouse it was shown that the ascending branch from pns to the lrn involves a cerebellar loop that can affect motoneurons and reaching behavior [cit] . behavioral studies in the cat have shown that grasping is primarily controlled by interneurons within the forelimb segments (c6-th1; [cit] b; alstermark and kümmel, 1990) . it therefore seems likely that the cerebellum, would need to receive grasping information from these forelimb segmental interneurons in conjunction with information regarding reaching mediated by c3-c4 pns. last-order segmental interneurons within the forelimb segments have been identified that mediate disynaptic corticospinal excitation to forelimb motoneurones [cit] . one possibility is that at least a subset of the segmental interneurons involved in the control of grasping are included in the ift system which sends information from forelimb segments to the lrn. as shown in table 2, excitatory and inhibitory ift convergence was commonly found in lrn neurones with input from presumed c3-c4 pns."
"after c5 dlf transection of cortico-and rubrospinal fibers, leaving descending connections with the lrn neurons and c3-c4 pns, but not with interneurons in the forelimb segments, monosynaptic epsps and disynaptic and late epsps and ipsps could be evoked in the lrn from the contralateral pyramid and nr. the expected minimal disynaptic linkage of pyramidal and rubral epsps and ipsps to lrn via c3-c4 pns is 2.1 ms, and the maximal linkage is 2.9 ms (based on a conduction velocity of 60 m/s for the corticospinal fibers and 26 m/s for the ascending branch of the c3-c4 pns; [cit] ) . disynaptic pyramidal epsps were found in 28% (19/67 neurones) and ipsps in 55% (36/66 neurones). disynaptic rubral epsps were observed in 16% (11/69 neurones) and ipsps in 46% (32/69 neurones). the higher frequencies of the ipsps most likely reflect the fact that the membrane potential decreased after electrode impalement of the lrn cells, making it easier to record ipsps than epsps. these findings strongly suggest that disynaptic pyramidal and rubral excitation and inhibition in lrn neurones can be mediated by excitatory, respectively inhibitory c3-c4 pns. the disynaptic pyramidal ipsps that remain after the c2 dlf transection are presumably mediated via reticulospinal neurones. it is worth noting that all rubral ipsps were mediated via spinal neurones located caudal to c2 (cf . figures 2 and 3) ."
"at the same time, the classical runge-kutta method is a suitable approach to solve these differential equations [cit] . and the optimal sparse solution also can be produced from"
"besides, multigram-based language models often outperform traditional language models based on words or characters. especially, the 2-multigrams language models are always better than the traditional ones. this confirms our hypothesis that multigrams are a good trade-off between words and characters for a language modeling task. compared to the specialized frameworks, the unified scheme generally provides similar results except for the fr dataset where a slight improvement is observed when the optical model is unified."
all procedures were approved by the local ethical committees (at the university of göteborg and university of lund) and were in accordance with swedish regulations on animal experimentation.
"the rest of the paper is organized as follows: section 2 introduces the related works on multilingual handwriting recognition systems; in section 3, we present the framework of the unified multilingual handwriting recognition system that we propose here; then, we show and discuss experimental results where the english and french languages are combined (section 4) before concluding."
"there is only one case where unifying the language model improves the performance of the specialized system. this is for the uu setting on the fr test dataset with a small lexicon. in this case, the system benefits from both a better cer with a unified optical model and a better coverage of the unified language model. this improvement may be explained by the fact that the oov rate, computed at a word-level, is reduced in the unified cases (compared to the specialized ones) and that the effective coverage rate, computed at a sub-lexical-level, is high. for instance, the oov rate on the fr set for the 3-multigrams is of 1.4% in the specialized case and of 0.7% in the unified one, while the effective coverage rate for the unified dataset is equal to 98.5%. in contrast, the oov rates on the en set are very small (0.06% and 0.03% for the specialized and unified case respectively) and the effective coverage rate stay low enough even if the lexicon has been unified (89.9%). besides, when lexicons are large, unifying the languages does not significantly reduce the oov rate, which does not impact the recognition performance. for example, the unification of languages for the 3-multigrams only reduce the oov rate from 0.5% to 0.4% on the fr dataset and from 2.2% to 1.6% on the en dataset."
"decoding consists in applying the viterbi algorithm on the combined transducer, so as to find the best (or the n best) sequence(s) of multigrams corresponding to the observation (the input observation feature sequence). two hyper-parameters are used to guide the decoding process: the language model scale parameter γ and the word insertion penalty parameter β that controls the insertion of frequent short words. these two parameters need to be optimized to find an optimum coupling of the optical model with the multigram language model, because these two models are estimated independently from each other during training."
"the bvfrt neurones are monosynaptically activated from the lvst and have large, often bilateral, receptive fields [cit] c; [cit] ) . effects in the lrn mediated via the subcomponents of the bvfrt-system were restricted by transection of the contralateral lvst in th13 and the primary afferents in the dorsal column in c5. cervical bvfrt neurons were activated by stimulation of the lvst in the cvq in c4 or c6. lumbar bvfrt neurons were activated by stimulation of the contralateral lvst in l2 caudal to transection in th13."
"we presented an end-to-end unified multilingual system for handwriting recognition where both the optical model and the language model are trained on datasets composed of examples from several languages. our proposal allows, on the one hand, to optimize a unique system, whatever the languages that are in training and, on the other hand, to avoid the use of a decision process for selecting one specialized system trained on a specific language."
"another requirement for successful reaching and grasping is concomitant postural control, especially in the contralateral forelimb that supports much of the body weight [cit] . previous experiments using activitydependent transneuronal uptake of wga-hrp into last-order interneurones to identify spinal circuits involved in reaching and grasping revealed not only c3-c4 pns and segmental interneurones on the ipsilateral side of injection, but also commissural interneurones in the forelimb segments on the contralateral side (alstermark and kümmel, 1990) . given their location, we propose that some of these contralateral neurones belong to the cervical and lumbar bvfrt systems, providing ascending information about the coordination of the limbs. convergence from these systems was often found in lrn neurons with input from presumed c3-c4 pns ( table 2) . taken together, as shown schematically in figure 11, we propose that the lrn may provide an overview of reaching, grasping and posture to cerebellum that could compare the activity to make fast updating and corrections by the use of the internal feedback from the various spino-lrn-cerebellar pathways."
"for instance, latin languages share at least 21 characters [cit] ) while 14 arabic and persian languages share at least 28 characters (märgner [cit] )."
"comparison of convergence in single lrn neurones revealed two major differences. first, among lrn neuron subtypes, it was common to receive monosynaptic pyramidal excitation among ift, bvfrt ( table 1 ) and c3-c4 pns (table 2) in a range of 35-60%. in contrast, monosynaptic pyramidal excitation was rarely observed in lrn neurons with monosynaptic rubral input or from lumbar bvfrt lrn neurons (table 1) . interestingly, in each lrn neuron with monosynaptic pyramidal excitation, there was no overlap of excitation and inhibition for each of the converging systems shown in table 1 . thus, motor cortex can select differentially among lrn neurons with excitatory or inhibitory inputs for a given spino-lrn system. the frequency is given both in percentage and the number of tested cells within parenthesis."
"three types of unified recognition systems have been presented in the literature: there are systems where 1) only the optical model is trained on multiple languages, i.e. in a unified manner, and then specialized language models are used; 2) the optical model is partially-unified as an encoding part is done on multiple languages and then decoding parts are defined for each language; 3) both the optical model and the language model are trained in a unified framework."
"in order to independently characterize the influence of each ascending system on the lrn, we took advantage of their differences in neuronal input and spinal cord location. the c3-c4 pn system is activated by stimulation of corticoand rubrospinal fibers. it has been demonstrated that the vast majority (84%) of the c3-c4 pns with projection to motoneurons have ascending projection to the lrn [cit] ) . we identified c3-c4 pn effects in the lrn via transection of cortico-and rubrospinal fibers in the dorsolateral funiculus (dlf) in c5 (sparing the input to the c3-c4 pns, while interrupting input to the more caudal (ift system). in three experiments the dlf was transected in c2 to interrupt the input both to c3-c4 pns and more caudal systems. in addition, when both c5 and c2 dlf lesions were performed, the c3-c4 pns could be synaptically activated in isolation via stimulation of cortico-and rubrospinal fibers in the dlf of the c3 segment. the isolated strip of dlf was stimulated using a monopolar tungsten electrode. threshold for evoking the dlf volley was 10 µa. the threshold for current spread to the nearby dorsal column was checked by recording from the sr and dr nerves and was usually approximately 500 µa."
"in the next step, we could divide the sparse signal x into two parts, namely, the positive elements u and the negative elements v. and define [cit] . therefore, refer to (7), there is"
"similarly to standard handwriting recognition system, our proposed system is composed of two components: 1) an optical model which is trained to recognize a sequence of characters from a sequence of observations; 2) a language model which is trained to model language constraints in sequences of words ( figure 1 )."
"we have seen that unifying english and french optical models allows to significantly improve the recognition performance of the system. this may be explained by two facts: on the one hand, there is a lot of shared characters between the two languages, which increases the number of training samples for theses character classes; on the other hand, the shared characters appear in a different context (i.e. into different words), which gives more variability in the data as well. the model can benefit of it for increasing its generalization capacities. the unified optical model achieves a character error rate (cer) of 10.0% alone (i.e."
"the ndoa provides a powerful tool to solve the constrained optimization of linear programming (lp) and quadratic programming (qp) problems [cit] . when we are concerned with these problems like (9), there is"
"in this paper, we propose a unified language model based on sub-lexical units called multigrams. we consider multigrams as sequences of characters of variable length. the interest of using such decomposition have been highlighted in subsection 3.1. here, we focus on the description of the data-driven statistical model that allows to learn a set of sub-lexical units from textual data."
"in order to examine monosynaptic excitatory effects of descending cortico-and rubro-spinal tracts on neurons in the lrn, and disynaptic excitation and inhibition mediated via c3-c4 pns, recording was assessed following dlf transections of descending tracts in c2 or c5. the rostral c2 dlf transection eliminates the inputs from cortico-and rubrospinal fibers to the c3-c4 pn, whereas the c5 dlf transection spares this input, but eliminates it to more caudal spinal levels. note, that after the c2 dlf transection both the input from pyramid and nr to the lrn is intact. figure 2 shows recordings from two lrn neurones after a c2 dlf transection. the contralateral pyramid was stimulated by a train of three volleys at different strengths (figures 2a-c) . small monosynaptic excitatory postsynaptic potentials (epsps) could be evoked with a threshold below 40 µa (figure 2a) . the amplitude increased when the current stimulus intensity was raised to 80 and 200 µa (figures 2b,c) . the longer epsp duration with 200 µa is likely due to activation slower conducting corticoreticular fibers. monosynaptic pyramidal epsps were observed in 25% of the neurones. monosynaptic rubral epsps after c2 dlf transection was observed in 10% of the neurones (not illustrated). lack of disynaptic cortico-or rubral epsps were observed in all of the 40 cells tested. the segmental latencies are shown in figures 2d,e . in contrast, disynaptic pyramidal inhibitory postsynaptic potentials (ipsps) could be elicited after c2 dlf transection, but at lower frequency (7/19 neurones) compared to before the lesion (33/66 neurones). one example of pyramidal disynaptic ipsps is shown in figure 2f . rubral ipsps were lacking following c2 dlf transection (0/21 neurones) and one example is shown in figure 2g, which is taken from the same lrn cell as in (figure 2f ). these data reveal monosynaptic excitation of lrn neurons by cortico-and rubro-reticular fibers, and disynaptic pyramidal inhibition, but not from nr."
"our results corroborate earlier findings by ekerot and colleagues that there is a parallel input from excitatory and inhibitory ift and bvfrt neurons to subpopulations of lrn neurons, and in addition show a similar organization for the c3-c4 pn system as illustrated in figure 10a ."
"this study aims to investigate convergence of the ift, bvfrt and c3-c4 propriospinal systems onto individual lrn neurones in the cat in order to clarify how ascending information from functionally different circuits is processed in the lrn. we find a minority of lrn neurones with input from only individual ascending systems, and a majority of neurones with convergent input from two or all three systems, suggesting that subpopulations of lrn neurones integrate ascending premotor information from the forelimb to enable cerebellar modulation of ongoing movement. a preliminary report has been presented [cit] ."
"zhenhua gan received the ph.d. [cit] . his research interest includes cross-subject field between electrician theoretic and biomedicine, such as biological systems modeling and simulation, biomedical signal detection and analysis, biomedical imaging and image processing, and related medical equipment development."
"the lrn cells with monosynaptic pyramidal and rubral excitation were located in the dorso-medial part of the nucleus. in contrast, lrn cells with disynaptic epsps, mediated via the c3-c4 pns, were located both in the dorso-medial part, central and ventro-medial parts of the nucleus. together these data indicate that the direct pyramidal and rubral modulation of lrn neurones is restricted to the dorso-medial part of the lrn, which was shown to project mainly to the ipsilateral pars intermedia and paramedian lobule v in the cerebellum with input from the ift [cit] c; [cit] ) . furthermore, the disynaptic effects mediated via the c3-c4 pns reached not only this region of the lrn, but also more ventral parts of the nucleus, which was shown to project bilaterally in lobules iv and v of the anterior lobe and in the vermis of lobule viii with input from the bvfrt [cit] b) . these findings corroborate earlier observations on the termination of c3-c4 pns in the lrn [cit] ) ."
"where x 0 denoted as l 0 -norm. unfortunately, solving the l 0 -norm is known as np-hard. in order to keep away from this problem, it is usually converted (2) into the minimizing l 1 -norm with optimization constraints. as long as the observation matrix a, such as gaussian observation matrix, satisfies the restricted isometric property (rip), the (1) would agree with the following constraints [cit]"
"we apply our multilingual system on the english iam (en) and the french rimes (fr) handwriting databases. the rimes database [cit] ) contains handwritten mails of different writers. the iam database [cit] ) is inspired from the lob corpus [cit] ) and it is composed of texts from different writers. these databases are divided into three parts: the training, the validation and the test datasets. our unified system is trained on the combination of the fr and en training datasets. the hyper-parameters γ and β used for decoding (see section 3.2.3) are optimized using the fr+en validation dataset. the system is evaluated on the fr and en specialized test datasets independently. table 2 shows the number of text line images per dataset. this highlights a significant reduction of the lexicon size when using k-multigrams sub-lexical units instead of words."
"three major ascending systems from the spinal cord to the lrn have been described in the cat: the bilateral ventral flexor reflex tract (bvfrt; [cit] b; [cit] b), the ipsilateral forelimb tract (ift; [cit] c; [cit] ) and the c3-c4 propriospinal neurones (c3-c4 pns; [cit] ) . however, while lrn input from the ift and bvfrt have been previously investigated [cit],b,c; [cit],b), the role of c3-c4 pn input, and how signals from all three ascending systems are integrated in the lrn, remains unknown."
"in higher mammals, like the cat and monkey, it is not known if the ift and cervical bvfrt systems have direct projections also to motoneurons as the c3-c4 pns, but recent findings in the mouse show that this is the case [cit] . interestingly, whereas the vsct neurons mainly receive input from inhibitory interneurons [cit], the lrn neurons receive input from both excitatory and inhibitory interneurons. another difference is that a majority of lrn neurons receive convergence, excitatory and inhibitory, from at least two of the ascending systems investigated in this study as shown in figure 10b . a smaller fraction receives convergence from all three systems (ift, bvfrt and c3-c4 pn). apparently, there is a possibility for the cerebellum to compare the excitability level of each system alone as well as in combination. thus, although these indirect spino-cerebellar pathways share several common properties with the direct spino-cerebellar tracts, the difference may be related to the increased demands required to control dexterous forelimb movements [cit] ."
"our work employs a novel wavelet filtering technique based on cs optimized by the ndoa, whose results show that the designed denoising method is superior and effective. the rest of the paper is organized as follows: the related knowledge about cs theory, wavelet filtering technique and ndoa model are introduced in section ii. and then, experimental results and analyses are given in section iii. finally, conclusion and discussion are drawn in section iv."
"without language model) on the fr test set, while the specialized model has a cer of 12.3%. similarly, we obtain a cer of 15.3% on the en test set with the unified optical model while the cer of the specialized model is 18.9%."
"meanwhile, in order to investigate the effect resulting from compression sampling rates on wavelet deniosing algorithm, we try to reduce the noise at sampling rates of 0.875, 0.50 and 0.125 for the same image. we notify that the relatively consistent denoising trend of omp, swomp and ndoa at different compression sampling rate. simply, table 2 only demonstrates the psnr and rmse corresponding to the swomp algorithm to illustrate the trend under different compression sampling rates."
"2.2.3. [cit] introduced a bilingual recognition system consisting of a unified blstm optical model and a unified 3-gram language model. the training corpus contains documents written in two languages where each paragraph is related to a specific language. four unified optical models are trained on various data representations (binarizations, segmentations), which allows to get 4 recognition hypotheses from a single test sequence. then, a sliding window process is applied to match every 3 consecutive words from a recognition hypothesis with 3-grams estimated by the language model. a cumulative score computed on the entire hypothesis sequence allows to select the most probable hypothesis. this process can be seen as a verification algorithm, which limits the recognition only to the words belonging to the training corpus."
"finally, we analyze the system complexity according to the search graph size (number of states and transitions in the fst automaton s defined in equation 1), the search graph volume on disk and the decoding time. besides, the memory and the processing time during the decoding process depend on the search graph size [cit] ). note that complexities are computed from the unified fr+en lexicon and that the character language model is a 9-gram model (in contrast to previous results based on a 10-gram model) to provide comparable complexities. as illustrated in table 3, the number of states and transitions is highly reduced using multigram language models compared to the word-based language models. for instance, the 2-multigrams model reduces the sum of states and transitions of the search graph by 31%. the volume on the disk is also divided by 1.45. similar comments can be made on a large lexicon with more significant results (table 4) . dealing with 2-multigrams allows to reduce the sum of states and transitions of the search graph by 63% compared to word model. the volume on disk is also reduced by a factor of 2.7. whatever the lexicon size, the character-based model is lighter than the other models but it never produces the best results in our experiments and it can perform sometimes poorly (as shown on the en dataset in figure 4 ). table 5 shows the decoding time on the fr validation set, which relates to a measure computed on 1.7 millions of frames. the decoding time is reduced using multigram language models compared to traditional language models based on words. for instance, using a 5-multigram language model reduces the time by 18% and by 24% using 2-multigrams on the small lexicon while there is a reduction of 22% using 2-multigrams on the large lexicon."
"the large number of inputs to the lrn (monosynaptic input from the pyramid and nr, c3-c4 pns, ift, and cervical and lumbar bvfrt) suggest an elaborate process of descending and ascending synaptic integration in lrn neurons before producing mossy fiber output to the cerebellum. this integration has an additional layer of complexity since all spinal afferent systems consist of both excitatory and inhibitory neurones, while the monosynaptic input from the pyramid and nr is only excitatory. to investigate the process of integration, we identified lrn neuron subtypes receiving monosynaptic epsps from the contralateral pyramidal (table 1) . only 5 out of 61 tested lrn neurons received monosynaptic rubral epsps and therefore it was not meaningful to make a table of these cells. disynaptic pyramidal and rubral epsps and ipsps mediated via presumed c3-c4 pns were commonly observed in the different subtypes of lrn neurons ( table 2) ."
"the observations given in input of our system are gray-scales images of text-lines. the input images are normalized to a fixed height of 100 pixels, preserving the aspect ratio. the gray scale is normalized to zero mean and unity variance (standardization)."
"according to the table 2, it shows that the decrease of compression sampling rates, the more information lost in the sampling process of wavelet coefficients, and thus a larger deviation is produced in signal reconstruction. that is to say, volume 7, 2019 if the compression sampling rate is small, the performance of wavelet filtering algorithm based on cs is worse. on the contrary, in order to ensure the better filtering effect, a high compression sampling rate needs to be selected."
"in this paper, we propose a unified language model based on sub-lexical units, called multigrams. here, we consider multigrams as sequences of characters of variable length. some examples of word decomposition into multigrams are illustrated in table 1 and the proposed system architecture is illustrated in figure 1 . working with a language model based on multigrams has many advantages compared to language model based on words. first, the lexicon size is strongly reduced, as we only consider subparts of words i.e. short sequences of characters. this allows to build a unified language model based on a lexicon of reasonable size. second, sub-lexical units can generate an open-vocabulary (i.e. words that do not belong to the training lexicon can be generated during decoding if the transitions between their multigrams have non zero probabilities). this allows to reduce the outof-vocabulary words rate and increases the effective coverage rate. third, compared to other sub-lexical units such as syllables, multigrams do not obey any linguistic rule but statistical rules. therefore defining the decomposition of a word into multigrams is a statistical data-driven process which does not require any linguistic expertise, and can be apply to any language. finally, multigrams can be seen as a good trade-off between words and characters to improve the capacity of language models on specific languages [cit] ). the many advantages of language models based on multigrams led us to design a multilingual language model based on a unified lexicon of multigrams. to our knowledge, this is the first work that proposes an end-to-end unified multilingual system where the language model is based on sub-lexical units that can be obtained using a data-driven process for any language and which is applied in an open-vocabulary recognition task."
"our third hypothesis was that automatlab users will be more confident in their understanding of the system's computational and quantitative aspects than users who used opm without automatlab. the results received for this hypothesis were not conclusive. no clear differences were found for the different groups or datasets. since the experimental group students had limited level of experience with matlab, perhaps some of their lack of confidence was due to the tool being used and not due to lack of understanding of the model. this may be a subject of a future research. the fourth hypothesis was that automatlab users will understand the system's computational and quantitative aspects better and with less difficulty than those who used opm without automatlab. the results indeed show that the students who did not use automatlab rated their difficulty as higher. analysis of the students' explanations suggests that the difficulty in the control group is associated mainly with the challenge in representing the customer behavior model in simple tools like excel or handwritten calculations: \"it took a long time to get the calculations since i didn't know the appropriate action in excel.\", \"i had to go over every buyer and every product which is a lot of intersections!!!\". explanations of experimental group students for q2-q4 repeatedly mentioned using the previously created code as a reason for low difficulty, supporting our hypothesis: \"no changes were needed from previous…\", \"from the way i implemented the solution of the answer in the first part… no more changes had to be made.\" from both the statistical and qualitative analyses of the evaluation we have seen that the automatlab method does indeed benefit the user in several ways. automatlab may improve the accuracy of understanding a system's quantitative aspects and decrease the difficulty of reaching such understanding. results regarding the time needed to understand the quantitative aspects and user's confidence regarding her or his understanding are not significant, probably due to the lack of participants' experience with the"
"we define computational simplification as the simplification of a conceptual model by elimination or reduction of its computational aspects. this abstraction gives rise to the computational simplification problem-the problem of lack of complete model comprehension arising from a conceptual model computational simplification. conceptual systems modeled in some of the modeling languages or methods presented in the following sections, may suffer from this problem. other methods that enable modeling lower levels, which include computational aspects, lack high-level abstraction abilities, which are vital in conceptual models. the computational simplification problem poses the challenge of equipping system architects, designers, and domain experts with the ability to incorporate computational modules into the conceptual model in order to make it more complete. in this paper we tackle this challenge by presenting and evaluating possible solutions for this problem that expand object process methodology (opm) with the capabilities of matlab and simulink."
"object process methodology (opm), the iso/ [cit] 0, titled \"automation systems and integration -object-process methodology\", is a conceptual modeling approach and language that uses a single unifying bimodal graphical and textual model. this single model captures the functional, structural, and behavioral aspects of a system [cit] ). an opm model consists of elements that are things and relationships: stateful objects and processes that transform them are the things, which are related by procedural and structural relationships. objects are the stateful components the system is made of, while processes are things that transform objects by creating or consuming them, or by changing their state. an opm model is represented in two complementary modalities: graphical and textual. a set of interconnected object process diagrams (opds) constitutes the graphical representation of the model, while a corresponding set of object process language (opl) paragraphs is the parallel textual representation of the model. opl is a subset of natural english, which anyone with basic knowledge of english can readily understand. both representations are completely interchangeable and convey the same information about the model, appealing to \"both sides of the brain\" [cit] this approach provides a structural view of a system, but it does not capture the dynamics of the system, nor does it represent relationships between the system and the environment any more than a code-based simulation."
"a key stage in the early stages of architecting and design of a new system or in understanding an existing one is its conceptual modeling: creating a primarily-qualitative model, which clearly specifies the system's function (why it is designed, what value is it expected to provide its beneficiaries with-the utility aspect), its structure (what is the system made of, how is the whole related to its parts-the structural aspect), and its behavior (how the system operates and changes over time and how objects in it are transformed to achieve its function-the behavioral aspect). similarly, when aiming to research and fully understand existing systems, a conceptual model of the system under study can be highly valuable [cit] . different modeling methodologies and languages, such as opm [cit] and sysml [cit], enable one to conceptually model a system and simulate its behavior. holistic understanding is achieved by simplifying certain aspects of the real system, such as its level of detail [cit] . one aspect that is often simplified in conceptual modeling is the computational aspect of a system-the mathematical entities that may govern the actions and reactions of a system, the exact output of some actions, and representation of random effects that determine the dynamics of the system. while this simplified, qualitative-only view of the system helps in initial, overall comprehension of the system, it often lacks important information, especially when the model is simulated, making it necessary to explicitly express the dynamic, time-varying aspect of the system. moreover, while making progress in the design or study of a system, simulation of the system's structure and behavior often becomes mandatory to ascertain that the system meets the requirements it is expected to fulfil. this is especially true for systems that exhibit complex behavior, which might require representation of non-deterministic aspects and advanced numerical calculations that drive different actions, as well as sophisticated quantitative decision-making processes. advancing from the conceptual model to an elaborate simulation is therefore often critical for testing and validating the system under design, or confirming theories regarding systems under study. in some cases, due to the human in the loop, the transition from the modeling stage to the simulation stage can result in errors or inaccuracies. creating a simulation of the system can be done by studying the model or the original system directly, aiming to understand it, and building the simulation accordingly. as long as the model-to-simulation transition process involves human intervention, it is prone to mistakes and inaccuracies. moreover, such manual transitions can"
"searching is complex and unintuitive due to opm lack of ability to represent formulae in a simple form. to visualize this unnecessarily complicated view of the opm model when it comes to relatively simple equations, in error! reference source not found. we show the opm model of searching that implements the radar equation, where the definitions for addition, subtraction, multiplication, common log and the control structure 'if then…else' are taken from table 1 through table 4 ."
"we present a case study that demonstrated the use of opm/cs for a search and tracking radar system. the opm model of this system is presented in figure 9 . the searching process creates a detection output, which can be either 0 (no detection) or 1 (positive detection in the in-zoomed searching process in figure 10, we see that it consist of two subprocesses: when running the opm simulation of the model shown in figure 9, searching creates detection in either the 1 state or the 0 state randomly, regardless of the target and radar attribute values. the received power of a target is described by the radar equation 1 [cit] ."
"mldesigner [cit] ) is an open environment for unix or linux systems used for designing and testing of system architectures and their functions. an mldesigner model consists of a block diagram containing a variant of c++ code that represents the functionality of each block. blocks can be modified by editing their size, position, color, etc. many predefined code blocks are available, simplifying the design and execution of a model. mldesigner can partially serve as a computational layer of a conceptual modeling language, similar to one of the solutions suggested in this work. [cit], where an opm-to-mldesigner translation was propose to add simulation capabilities to an opm model. the main advantage of the opm-mldesigner approach is the simulation abilities that are built into the mldesigner case tool, which, for simple simulations, can be more convenient than matlab. the opm-mldesigner approach has some downsides: mldesigner is not as widespread as matlab and it is not windows-compatible. the opm-mldesigner approach aims to generate an mldesigner model from the opm model by using opl-the textual modality of the opm model-to generate an mldesigner model, referred to as mml. mml is not linked to the opm model and opcat simulation tool, and it does not affect the original opm model. in other words, the opm-mldesigner approach does not improve opm's computational capabilities, but rather generates the opm model in a different language. as mldesigner focuses on processes, it does not have an entity compatible with the opm concept of object, making it necessary to define dummy processes that play the role of objects. this can substantially complicate the model."
"this research has tackled the problem of merging computational aspects and capabilities into conceptual models of systems, which are primarily qualitative in nature. due to the level of abstraction of conceptual models, their computational capabilities are weak or missing altogether. some modeling and simulation methods that do provide the computational aspects generally lack the high level of abstraction required from a conceptual modeling language. for example, arena, a widely used discrete simulation software tool [cit] discussed above, or modelcenter, which integrates simulation tools from various vendors, can be used for this purpose, making it unnecessary to merge the computational aspects into the system. however, the abstraction power of opm and its complexity management mechanisms make it highly suitable for early-stage conceptual modeling, so incorporating matlab's computational capabilities into opm for seamless transition from conceptual to detailed design is highly desirable. the computational simplification problem defined in this work relates to the difficulty of incorporating quantitative aspects into conceptual models, which focus primarily on key qualitative aspects of the system. we presented two possible solutions for this problem, based on expanding opm to combine matlab and simulink. in the first, automatlab approach, we solve the computational simplification problem by adding a parallel matlab-based representation of the entire opm model, which can be augmented with any desired computational aspects in the matlab representation. a major advantage of the automatlab approach is that the opm model becomes an integral part of the augmented matlab model, enabling it to evolve and serve for increasingly more quantitative-oriented simulations in downstream lifecycle stages of the system. in the second, opm/cs approach, we aim to solve the computational simplification problem by augmenting the capabilities of the opm in-zooming mechanism. the additional capability provides for the content of an in-zoomed process to be replaced by a matlab function or a simulink model containing the necessary computational aspects. the main advantages of the opm/cs approach are (1) the simplicity of the enhancement that uses an intuitive extension of the opm in-zooming mechanism, and (2) the preservation of the original opm conceptual model with its semantics. the two approaches were demonstrated via examples and case studies. an evaluation of the automatlab approach was conducted with human subjects, showing benefits of this approach in terms of better system understanding. results regarding user confidence in system understanding and time required to achieve such understanding were not conclusive, perhaps due to the small sample. the statistical results were supported by qualitative content analysis of the subjects' responses to questions."
analysis of the explanations provided by the students when submitting their answers suggests that the students in the experimental group attempted to create a more accurate simulation of the system behavior. one of the subjects noted: this is assumed to be the cause for the difference in accuracy between their answers.
"modelica [cit] ) is an object-oriented equation-based modeling methodology. the modelica language defines and describes the system, its components, structure, and behavior by a set of mathematical equations. the modelica methodology includes case tools for designing modelica models. these tools allow the user to draw or import a scheme of the system, connecting the model equations to the appropriate component on the scheme. using modelica, one can decompose the model hierarchically, simplifying the model and making it more understandable. another feature of modelica is a large selection of model libraries, offering predefined subsections of systems, sorted into different fields (electrical, mechanical, aerospace, etc.), which can be easily implemented as part of a model. other predefined libraries enable integration of numerical solutions and stochastic behavior modules. modelica supports quantitative and stochastic aspects due to its equation-based representation, allowing direct modeling of the computational aspects of the system. describing the architecture of the modeled system using modelica without the equation-based representation is also possible. however, describing the behavior of the system does require using the mathematical representation, demanding a greater level of user effort to comprehend the model in comparison to a simpler representation of a system flow. another somewhat limiting aspect is that modelica definitions are object-oriented, confining the users to think in terms of the oo paradigm, which emphasizes objects and system structure at the expense of suppressing behavioral aspects and processes (\"methods\") as secondary to and owned by objects."
"the control group was allowed to answer the questions using whatever tool they desire. the experimental group students received the automatically-generated matlab code from the automatlab approach shown in appendix b, figure 23 15 through figure 29, and were instructed to use it in order to answer the four questions. the matlab code was intended to help them gain better understanding of the model and to serve as a basis for a simulation of the system, including computational and stochastic aspects required to answer the four questions. the students from the experimental group who received the automatlab generated matlab code submitted their code when answering the questionnaire. an example of the shopping list creating enhanced code is presented in appendix b, figure 29 ."
"for each answer, the students were then asked to explain how they had deduced their answer, how accurate they thought their answer was and why, how difficult it was to complete their answer and why, and how long it took them to complete their answer. since our sample was not sufficient for accurate statistical analysis, we combined qualitative analysis of the evaluation with the statistical analysis. the students were given data sets containing a list of items sold by the web based grocery shopping system, monthly fee for premium users, a list of customers, and potential shopping lists for customers. identical questionnaires and data sets were given to both the experimental and control groups. the questionnaire included four main questions, such as \"what are the three most profitable products for the ibuy owner?\" each one of the four questions was graded according to its correctness (correct/incorrect). in addition, students were asked to rate on a 1-5 likert scale how accurate their answer is and how difficult for them was it to obtain this answer. finally, students were asked to report about the time it took them to answer this question."
click here to view linked references 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 overlook insights gained during early stage of the conceptual model.
"a basic requirement of a modeling and simulation language is expressiveness that provides for modeling the phenomena encountered in the design of a system, such as non-linear, multi-disciplinary, continuous or discrete flows. the models must also be easy to create and reuse [cit] ). simplification of the computational aspect of a system, which provides an accurate, quantitative representation of the system's behavior, might conceal some important information about the system. certain opm models might be adversely affected by this computational simplification problem. many of the modeling languages and methods presented in the previous section provide conceptual models of the system under development or study and enable some kind of simulation. however, in most cases, either the level of the quantitative aspect of the simulation is insufficient, or it is achieved at the expense of sacrificing simplicity and generality of the method and the resulting model. matlab is a convenient tool for simulating complex systems, but it does not have advanced abstract conceptual modeling capabilities. adding a numerical computational layer to the conceptual modeling power of opm provides for simulating its behavior both qualitatively and quantitatively. while opm enables exporting a model as an xml file, java code and more, it does not fully integrate into common development tools such as matlab. exporting a matlabbased representation of the opm model allows one to express the model so that a domain expert who uses matlab can understand the system without knowledge of opm. a conceptual model must not be overly complex. in other words, \"the conceptual modeling mantra is one of developing the simplest model possible to meet the objectives of the simulation study\" [cit] . figure 1 illustrates how excessive levels of detail hamper model accuracy. at a certain point, the increasing level of detail reduces the model accuracy due to lack of knowledge on how to model such details. it is thus clear that we would want a model to be as simple as possible while still allowing sufficient accuracy and detail required for the model objective. figure 2 (the graphical modality -object-process diagram, opd) and in figure 3 (the corresponding auto-generated textual modalityobject-process language, opl, paragraph), consists of three main processes: in order to generate the automatlab layer efficiently and accurately during the matlab code generating stage, we define the syntax and semantics of the various opm constructs and their matlab translations. this enables the modeler to add to the original opm model the computational aspects expressed in opm. as a first step, we have mapped the main basic built-in matlab functions in the matlab documentation [cit] to their opm model equivalents. to distinguish these built-in functions from usercreated functions that may have different meanings, the process names listed in table 1 through table 4 are added to the list of opm reserved words."
(for bidirectional) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 21 appendix b. matlab code generated automatically from the opm processes of the ibuy system figure 23 : matlab code generated from the process shopping list creating figure 24 : matlab code generated from the process item choosing 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65
"as the design or study of a system progresses, elaborate simulations are often created for examining the system in operation. while in general a human in the loop may introduce errors during the transition from the conceptual model to a simulation, our approach provides for a matlab-based simulation, which is created directly from the evolving opm model, avoiding the likely introduction of new errors, which, given the early stage of their introduction, are often very costly to correct."
"evidently, this opm model for representing the relatively simple radar equation is complicated. the in-zoomed content of searching can be replaced by the simpler matlab code in figure 11 or the corresponding simulink diagram in figure 12, although each of them is also more complicated than the radar equation (1), which is probably the most compact expression, as it uses the mathematical conventions of multiplication (lack of any symbol), exponentiation (as superscript), parentheses, etc. applying opm/cs, the opcat simulation arrives at the searching process. then, the matlab code (figure 11 ) or the simulink diagram (figure 12 ) is called. it reads the data files containing the inputs to the diagram, calculates the outcome, and returns the appropriate output using the data files."
"the comparing uml and sysml on one hand with opm on the other hand, a major difference that sticks out is the holism of the model. uml and sysml require using at least a subset of the 14 (in uml) or nine (sysml) diagram kinds to represent the system. conversely, opm represents the system with an interconnected hierarchical set of diagrams of a single kindobject-process diagram (opd). this graphical representation is accompanied by an equivalent set of object-process language (opl) paragraphs that specifies the system in a subset of natural english. following this minimalism principle, we aim to extend the computational power of opm while minimizing additional diagram kinds."
"the different factors we analyzed referred mainly to the shopping list creating process in the opm model shown in figure 21 . the students were requested to evaluate the answer to four different questions, and answer a few questions regarding their evaluation, confidence, difficulty, and the time it took them to answer. the analysis was performed on two data sets with different levels of difficulty. the first data set was simpler in the sense that there were fewer customers, and the behavioral model was simpler. for example, in the simple data set, a customer starting as a regular user would either purchase his/her entire shopping list as a regular user, or pay a fee and become a premium user, whichever is cheaper for that purchase. in the advanced data set, when a regular buyer has the option to pay less if she or he becomes a premium user (buyer), the buyer's behavior is stochastic: s/he might choose to stay a regular user, pay a fee and become a premium user, or cancel the entire purchase. the students were asked to answer the following questions:"
"both approaches have been designed and implemented with forward compatibility to the future online opm case tool, and partial compatibility to the development version of the current opcat. in a future opm modeling tool, a large scale test and comparison of automatlab and opm/cs should be performed . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 a is the whole, b and c are parts."
our approach of integrating the qualitative high-level conceptual stage with the quantitative one that follows it can be considered a step in solving this problem.
"a total of 96 answers from 24 questionnaires were graded according to their accuracy, and student explanations regarding difficulty, confidence in the outcome accuracy, and the time required to complete the assignment were analyzed qualitatively. the number of questionnaires submitted from each group is presented in table 5 . we analyze the data from the evaluation for the three variables: group (experimental or control), level ('jerusalem' -level-1 -the easy data set, or 'tel-aviv' -level-2 -the difficult data set) and question (q1, q2, q2, or q4) using multi-way repeated measures tests with two within-subjects independent variables (level, question) and between-subjects independent variable (group). the dependent variable, namely grade, time, confidence in answer accuracy, and difficulty was changed in each hypothesis test. independent t-test and oneway anova with a bonferroni correction served as our posthoc tests, where it was needed."
"our second hypothesis was that automatlab users will understand the system's computational and quantitative aspects quicker than users who used opm without automatlab. the results did not show a conclusive difference between the experimental and control groups. we have seen that the time needed to answer q1 was significantly longer than the time needed for q2 through q4. for the experimental group, we assume that the longer time required to answer the first question is due to the need to enhance the automatically generated matlab code when solving the first question, and this code was later used to solve the rest of the questions, as one student commented: \"calculations were very similar to previous questions…\", \"the code was already prepared -only one function needed to be changed\"."
"second, cloud services help to improve business focus by keeping a leaner business model. specifically, cloud computing frees up employees from maintenance and non-core activities; thus, they could focus on core skills and competencies [cit] ."
"twelfth, because it plays the role of a strategic necessity [cit], firms prefer to minimise spending on it applications; this can be achieved in cloud environments where it resources can be utilised at significantly lower costs. for example, numerous off-the-shelf/erp solutions with minimum customisations are available in the cloud [cit] ."
"technology has significantly changed how business is conducted. while technology governance in the 20th century mainly focused on standards and centralised management, the information technology and management science ________________________________________________________________________________________________ 2019/22 44 21st century has been experiencing a transition to federated and then to participatory governance models. as an alternative technology-delivery model, cloud computing has been evolving since the past decade to increasingly respond to cost cutting and business demand."
"thirteenth, the standardisation of it functions will lead to simplification and reduction of efforts by the management in handling the problematic and complex it function [cit] . cloud users are able to concentrate on aspects of innovation without concerns on constant server updates and other computing issues. the ability to deploy new applications quickly shortens the time to market for it application development [cit] . it follows that cloud services allow organisations to cope with the rapid changes in business process as well as to shorten it system life cycle."
"cloud computing has emerged as an alternative technologydelivery model that has altered the rules around technology acquisition, deployment, and support [cit] . unlike the traditional organisational governance model utilised for buying/installing or leasing software and/or hardware, cloud computing calls for a different governance model that requires a core competency in vendor and service-level agreement management. further, many executives predicted that because of cloud computing potential to free businesses from corporate it (such as building and maintaining huge server farms) the cloud could be an engine for small businesses and job creation [cit] ."
"third, the \"pay-as-you-use\" cloud services transform a large amount of fixed costs into variable costs [cit] . in other words, organisations can convert capital expenditures (capex) to operating expenses (opex) and pay-per-use basis [cit], increasing their operational flexibility."
"a web-based survey was conducted. it professionals who had agreements with qualtrics to participate in online surveys and polls in return for compensation were contacted via emails. an opt-out option was also provided to all subjects. non-responses were minimised through only using close-ended questions. participants were first asked to indicate the industry they were employed in; this information was used to ensure adequate representation of industry type. the target quotas consisted of 80 service sector responses (with 40 respondents in the computer software industry sector and 40 respondents in other service industry sectors) and 70 manufacturing sector responses. once a quota was reached, qualtrics deactivated the links given to participants in that sector. respondents who began a survey before the link was deactivated were allowed to finish the survey. one hundred and forty-eight usable responses were received from the initial sample of 153, resulting in a 97 % response rate. tables ii and iii show a fair representation of the intended population in the sample. seventy-nine percent of the respondents were from it departments, as intended. the remaining respondents were directly associated with the it department. the senior level management was represented to a higher degree than mid-level managers were. in most of the respondents' organisations, the full-time it employees were 100 or higher and having an it department budget of more than $10 million."
"the third construct, named \"perceived risks,\" was measured using three items: \"higher resistance from employeesjob losses, lower employee morale, potential for poor quality\"; \"potential breach of security\"; and \"risks during transition\". this construct echoes many aspects of risks and the salience of security, privacy, and availability mentioned in our literature review. the organisational risks represent that executives are concerned about the risks that are presented by the dependence on it. if they perceive that there is less risk associated with using cloud computing instead of their owned resources, they are more likely to switch to cloud-based systems. finally, the fourth construct, named \"resource constraint\" was measured by two items: \"lack of internal resources\"; and \"cost of failure.\" these items taken together would seem to indicate the degree to which executives saw cloud computing as an inexpensive and supplementary solution to acquiring and managing it resources."
"a delphi-like technique was used to finalise the phrasing of measured items. the delphi technique is designed to solve problems in domains that are not suitable for more structured models or in areas of limited research where a generally accepted standard does not exist [cit] . in the delphi method, experts are consulted across multiple rounds to anonymously answer the question under consideration. after each round, the results are presented and the experts perform the analysis again based on the new information gained. the process continues across multiple rounds until either consensus is achieved or no additional perspectives are obtained. for this study, the classification of the questions into variables was conducted using a panel of seven industry experts from various backgrounds using the above approach. the face validity of the twenty-nine items was established by revising questions until consensus was reached."
"international data corporation (idc) has predicted that total annual spending on public cloud services and infrastructure will grow 23.8 [cit] to reach $210 billion based on their study of 20 industries across 49 countries [cit], expected to reach $124.6 billion [cit] . relatively speaking, cloud computing was ranked as the fourth largest information technology (it) investment for organisations [cit] sim it issues and trends study also showed that cloud computing was ranked third in the \"get-more-investment\" and fifth in the \"most-worrisome\" lists [cit] . these rankings reflect not only the potential of cloud computing to affect the it delivery practices but the need to obtain a better understanding of drivers influencing the adoption and usage of cloud services in organisations."
"given the challenges associated with fulfilling the potential organisational benefits of cloud computing, this study addresses the following research question: \"what factors do decision makers use when deciding whether or not they should move an it operation to the cloud?\" to address this question, a review of related literature is used to develop a set of survey questions to measure possible contributors to cloud computing adoption. next, data collection and data analysis are performed; results obtained are explained. the paper concludes with practical implications and suggestions for future research."
"although there are many kinds of risks involved when adopting a new technology, concerns about security, privacy, and availability seem to be critical for cloud computing [cit] . in particular, previous research has extensively examined information technology and management science ________________________________________________________________________________________________ 2019/22 39 security issues [cit] . neumann [cit] provided many real cases of risky exposure such as dropbox's sharing services, and amazon web services. these incidents call for the need to consider several newly appeared security and privacy risks in cloud environment [cit] . juels and oprea [cit], for instance, proposed an auditing framework to increase security as well as cloud-service operation visibility to enterprise adopters. another framework [cit] to assess risk was suggested based on two dimensions: the criticality of the business process being supported by the cloud computing solution, and the sensitivity of the data that will be stored in the cloud; the overall risk level should be equated with the highest risk realised for either dimension. by realising their risk exposure, companies could effectively manage and substantially reduce risks associated with cloud computing [cit] ."
"ranking of attributes: responses to the questionnaire were analysed using the r programming language. respondents ranked the contribution of each item to their overall decision from 1 (none) to 7 (very high). the mean and standard deviation of each item was calculated. since higher numbers are associated with a stronger preference; we ranked items based on means and counting occurrences of data. table iv shows the mean and standard deviation of each item sorted in descending order of preference. column 3 of table iv shows a very high perceived importance for all items; in particular, the mean varies from 4.51 to 5.06. surprisingly, the median of all items was 5. on the one hand, this result confirms the validity of stage 1 that these items were indeed considered contributors to the adoption decision; on the other hand, their low variability in terms of central tendency prevents us from focusing on a smaller set of items (say, the top 10 contributors). interestingly, items describing benefits of cloud services tended to have higher rankings than obstacles. to some extent, this result confirms the recent growth of cloud computing adoption. further examination of item description did not provide unambiguous groupings of closely ranked items. as a result, we focus more on the item correlation via exploratory factor analysis to better classify items."
"sixth, with on-demand services and streamlined systems/processes provided in the cloud environment, organisations have seen increased efficiency in their operations and stronger customer relationships [cit] ."
"ninth, cloud computing is often a by-product of business process reengineering; it allows an organisation to immediately realise the benefit of accelerated reengineering by having a world-class outsider, i.e., the cloud computing vendor taking over the process [cit] ."
"a list of items found in the literature was generated for usage in the survey. to establish face validity (i.e., a subjective assessment of whether an item or question measures what it claims to measure), a three-step approach was used. first, a pool of measured items was initiated by the authors with expertise in the management, mis, and it disciplines; a preliminary consensus was reached among them on the question-phrasings that evoked acceptable face-validity. next, each judge independently again assessed the face validity of each item; these independent assessments were then compared with each other for inter-rater concordance and the questions were rephrased as required. finally, four it executives from different firms were asked to comment on the question phrasing and the time required to respond."
"in an effort to understand the distribution of the responses, the authors counted occurrences of data; items were ranked by a process that consisted of summarising the count of each importance response for each item. responses of 5, 6, and 7 were considered to be favourable responses so the counts of those responses were summed at the item level for a favourability count. for items that were tied, the sum of the counts for items 7 and 6 was used as the next sorting item. if there were still ties, the count for item 7 was used to break the tie. at that point there were no more ties. the results are shown in descending order of preference and can be found in a bar chart, see fig. 1 . the bar chart shows the cumulative percentage for each item's responses along the x-axis. the count for each response type is contained within the item's bar and is out of 128 total responses."
"seventh, to reduce training costs and improve user satisfaction, organisations demand an easy-to-use computing environment. previous study [cit] has shown a relatively high perceived ease of use among users of cloud services. for example, a simpler user interface is among major factors leading many commercial off-the-shelf (cots) erp systems in cloud environment to be usable with little or no training."
"being a new and growing technology-delivery model, cloud computing offers companies several benefits but exposes them to various kinds of risk in such an environment. organisations that deploy cloud computing may be exposed to four categories of risk: organisational, operational, technical, and legal risks [cit] . organisational risks cover those related to potential changes in areas such as it governance, compliances to individual regulations, in-house it specialists, business continuity and resilience, and is risk planning and management. operational risks relate to those influencing daily business and it operation such as service level agreement, financial issues, data and application movabilityinteroperability, system users, and service reliability. technical risks are realised through the lack of it expertise in companies that adopt cloud computing; potential issues include data quality and maintenance, system performance, system integration, and data security. finally, legal risks pertain to data privacy, intellectual property, and contracts."
"the second construct, named \"intrinsic motivation,\" was measured by three items: \"saving energy and reducing carbon footprint (greener it)\"; \"improving mobility, collaboration, and productivity\"; and \"firms to become more flexible, dynamic, adaptable, agile, and having scalability\". these items reflect that executives were more likely to move to cloud computing when they perceived that it would strengthen the company, assisting them to gain a competitive advantage in implementing their business strategy."
"exploratory factor analysis was used to classify the items into constructs (or factors) and to eliminate items with low factor loadings. this allowed us to explore new relationships among ranked items. a confirmatory factor analysis (cfa) was later conducted to evaluate the identified constructs; we used cfa for classification purposes only, not to extensively validate the scales for the construct or to develop a survey instrument [cit] ."
"cloud computing is a model of using a networked it environment and/or the internet to provide it services to a client location [cit] . more specifically, the definition from the national institute of standards and technology (nist) considers cloud computing to be a three-part model of service provisioning composed of essential characteristics, service models, and deployment models. a brief summary is provided here (see [cit] for a detailed explanation)."
"eighth, the high backlog of projects dampened the performance of the it department, causing management to replace in-house development with cots/outsourcing/offshoring and now with cloud computing [cit] . this shifting is considered necessary to organisational survival in an increasingly competitive environment. in fact, extant research demonstrated that through cloud computing, organisations are able to access advanced technology and skilled labour that could provide them with new capabilities through numerous value-added services [cit], which in turn further enhance their competitive advantage [cit] ."
"there are several opportunities for future research from this study. first, further refinement of the measuring items is warranted; while the current survey items were able to help advance this exploratory research, the number of items that are cross-loaded is a concern. last but not least, a longitudinal study (if possible) would improve the robustness of the result."
"fourth, a cloud environment also allows easy change in it solution whenever there are changes in the business environment. the increased agility/adaptability [cit] and flexibility/scalability [cit] further create an illusion of availability of infinite resources in the users' mind [cit] while creating a greener computing environment [cit] . this flexibility also improves business continuity, mobility, and productivity [cit] ."
"this review seeks to examine previous research to identify possible drivers of cloud computing adoption. a brief overview of cloud computing is provided, followed by a discussion of numerous previously identified benefits as well as obstacles that could influence decision makers in choosing whether to migrate toward cloud solutions."
"43 method with varimax rotation, we first considered the number of factors that would exceed sixty percent of variance explained and dropped items whose loadings were less than 0.6 [cit] . the more factors were considered, the higher number of items was dropped, except the four-factor and five-factor models where the twelve dropped items were almost identical (only one item was different). with this outcome, we run subsequent factor analyses on the remaining items with both four-factor and fivefactor models. the five-factor model was abandoned because of the high cross-factor loadings and a single-item factor. hence, we selected the four-factor model; the initial model accounted for 73.4 % of the total variation among items. table v shows the factor loadings of 13 measured items. the four-factor model indicates that there are four main constructs that are most salient to executives considering cloud computing; the first two contain items that have higher rankings in table i, while the last two consist of items with lower rankings. the first construct, named \"extrinsic motivation,\" was measured using five items: \"simplifying overall it environment and data centre or it consolidation\"; \"rapid change in business process cycle\"; \"shrinkage in system life cycle\"; \"access to world-class capabilities, including 24/7 services\"; and \"maturity and standardisation of technology\". these items reflect the fact that executives were more likely to move to cloud-based services when they believed that doing so would allow them to better cope with a rapidly changing business environment."
"following [cit], the analysis was performed in three stages. first, based on the literature and expert feedback, we generated a pool of 29 items (or attributes) that decision makers perceive to be important for the adoption of cloud computing (see table i ). in the second stage, we further ranked these items based on their average scores. finally, we used exploratory factor analysis to classify different groups of measured items that contribute to the adoption decision."
"eleventh, cloud services are available 24/7 with worldwide access to it application developers; more importantly, high redundancies [cit] provide developers with fast and easy resources for testing and development [cit] . easy access also increases mobility, allowing employees to obtain information at every moment from any location [cit] ."
"as with any research study, the present research has limitations that need to be discussed. first, there were several items on the questionnaire that were eliminated using an exploratory factor analysis. although the literature was thoroughly reviewed and additional perspectives were obtained from is academicians and managers, the description of these items should be improved to prevent ambiguity and high crossloadings. second, the sample was obtained from the manufacturing sector (automobile, computer hardware, pharmaceutical, telecommunicationhardware, and other) and the service sector (banking, retail, hotels, computer software, construction, government, healthcare, insurance, technology, transportation, utilities, and other). other types of organisations such as airlines manufacturing, railway, chemicals, airlines operations, etc. were not included in the sample. hence, any inferences based on the results might be restricted to the companies listed in the directory."
"in this section, we conduct extensive experiments on image denoising and image compression artifacts reduction to verify the effectiveness of the proposed rrc model. to evaluate the [cit], lssc [cit], epll [cit], plow [cit], ncsr [cit], gid [cit], pgpd [cit], linc [cit], agmm [cit], oglr [cit], wnnm [cit] and rrc for image denoising quality of the recovered images, both psnr and structural similarity (ssim) [cit] metrics are used. the source codes of all competing methods are obtained from the original authors and we used the default parameter settings. for color images, we only focus on the restoration of luminance channel (in ycrcb space). due to limit space, please enlarge the tables and figures on the screen for better comparison. we choose the following stop criterion for the proposed rrc based image restoration algorithms,"
"now, we incorporate the quantization noise model in eq. (20) and two image priors (i.e., the proposed rrc in eq. (16) and qc in eq. (25)) into eq. (17), and achieve the joint image compression artifacts reduction as follows:"
"additionally, the erroneous variant pairs manually filtered out can be used as counter examples of variants, and further used to train variant classifiers (see, for instance, [cit] )."
"it is worth noting that the significant difference between the proposed rrc and the existing low-rank based methods (e.g., nnm and wnnm) is that we analyze the rank minimization problem from a different perspective. to be concrete, traditional low-rank based methods estimated the low-rank matrix directly from the corrupted observations. by contrast, in our rrc model, we analyze the rank minimization problem from the point of approximation theory [cit], namely, minimizing the rank residual; the singular values of the recovered matrix progressively approaches the singular values of the reference matrix. note that the reference matrix and the recovered matrix in our rrc model are both updated gradually and jointly in each iteration. moreover, we provide an analytical investigation on the feasibility of the proposed rrc model from the perspective of group-based sparse representation [cit] ."
"where x i, j andx i,k represent the j -th and k-th patch of x i andx i, respectively. m is the total number of similar patches and w i,k is the weight, which is inversely proportional to the distance between patchesx i andx i,k, i.e.,"
"we have proposed a novel rank minimization model, dubbed rank residual constraint (rrc), to reinterpret the rank minimization problem from the perspective of matrix approximation. different from existing low-rank based methods, which estimated the underlying low-rank matrix directly from the corrupted observations, we progressively approximate the underlying low-rank matrix by minimizing the rank residual. based on the group-based sparse representation model, an analytical investigation on the feasibility of the rrc model has been provided. we have developed the high performance low-rank matrix estimation based image restoration algorithms via minimizing the rank residual. specifically, by exploiting the image nss prior, we have applied the proposed rrc model to image restoration tasks, including image denoising and image compression artifacts reduction. experimental results have demonstrated that the proposed rrc not only leads to visible quantitative improvements over many state-ofthe-art methods, but also preserves the image local structures and suppresses undesirable artifacts."
"in the following, we develop an alternating minimizing strategy to solve the large scale non-convex optimization problem in eq. (26) . specifically, the minimization of eq. (26) is divided into two sub-problems, i.e., x and z k, and we will show that there is an efficient solution to each of them."
"the newly created resource contains 876 pairs of variants, from which 400 were identified in the training corpus, and 476 in the lookup lexicon. the size of the created resource depends on the size of the lookup corpora and lexicon, and on the number of rules. the application of the method to any unpreviously seen text may increase the number of variant pairs. a subset of 60 pairs of these automatically generated variant pairs were submitted to an alsatian teacher, familiar with both the dialectal and spelling variants. the pairs were presented in the context of their sentence. the expertise of the teacher was used to measure the precision of the pairs, not the recall."
"the size of this resource does not allow us to perform direct lookup for any out-of-vocabulary word me might encounter. however, we can use the aligned variants to identify substitution patterns, and extract sets of rules we apply to any oov word as described in the following section."
"in fact, even the expertise necessary for the validation of the variant pairs is about to be transferred to the participants of the crowdsourcing platform."
"according to eq. (14), one can observe that z k and s p are unknown in our proposed rrc model for image compression artifacts reduction, and thus we need to estimate them. similar to subsection iii-b, we exploit the image nss prior to estimate z k from the recovered imageẑ in each iteration, i.e.,"
"in the following experiments, we chose to train melt, which enables us to take advantage of available lexicons existing for alsatian. the differential in performance is more interesting to us than the performance per se, which is why we chose not to focus on testing our methodology on other taggers."
"the only information we possess about these participants is the languages they speak and their place of origin, when they fill it in in their profile. based on the information provided by 8 of them, we can assume that 3 to 4 dialectal areas are covered by the towns of origin of the participants. no assumption can be made regarding their proficiency in alsatian."
"where ϕ i denotes the estimated standard variance of γ i, and c, are the constants. the complete description of the proposed rrc based image denoising approach to solve the problem in eq. (6) is presented in algorithm 1."
"theorem 3: under the condition of the adaptive dictionary d i shown in eq. (43), the proposed rrc model in eq. (6) is equivalent to the gsrc model in eq. (41) ."
"to embrace this diversity, we propose a methodology based on crowdsourcing alternative spellings from which variation rules are automatically extracted. the rules are further used to match outof-vocabulary words with one of their spelling variants. this virtuous process enables the unsupervised augmentation of multi-variant lexicons without requiring manual rule definition by experts. we apply this multilingual methodology on alsatian, a french regional language and provide (i) an intrinsic evaluation of the correctness of the obtained variants pairs, (ii) an extrinsic evaluation on a downstream task: part-of-speech tagging."
"the domain-specific terminal nodes are listed in two tables: table ii shows nodes describing characteristics that have to do with the board in its entirety, and table iii shows nodes describing characteristics of a certain square on the board."
"where h is a predefined constant and w is a normalization factor. it is worth noting that eq. (7) is based on nonlocal means filtering [cit] . again, the recovered matrixx i and the reference matrix x i are updated gradually and jointly in each iteration."
"using the vantage point of the gene-centered view of evolution it is easier to see the logic of crossover in our system. in a gene-centered world we look at genes as competing with each other, the more effective ones out-reproducing the rest. this, of course, should already happen in a framework using the generic two-way crossover alone. using selective crossover, as we do, just strengthens this trend. when selective crossover applies one-way crossover, the donor individual pushes a copy of one of its genes into the receiver's genome at the expense of one of the receiver's own genes. the individuals with high fitness that are more likely to get chosen as donors in one-way crossover are also more likely to contain more good genes than the less-fit individuals that get chosen as receivers. the selective crossover operator thus causes an increase in the frequency of the genes that lead to better fitness."
"we apply tree-based genetic programming (gp) to evolving players for reversi. our guide in developing our algorithm parameters, aside from previous research into games and gp, is nature itself. evolution by natural selection is first and foremost nature's algorithm, and as such will serve as a source for ideas. though it is by no means assured that an idea that works in the natural world will work in our synthetic environment, we see it as evidence that it is more likely to. we are mindful of evolutionary theory, particularly as pertaining to the genecentered view of evolution. this view, presented by williams [cit] and expanded by dawkins [cit], focuses on the gene as the unit of selection. it is from this point of view that we consider how to adapt the ideas borrowed from nature into our synthetic gp environment."
"initiatives such as the orthal guidelines (crévenat [cit] ) have been developed to unify the alsatian spelling while being respectful of its variations. yet, these keep the variability (kìrisch and kìch are the orthal version of kerisch and kîch, northern and southern possible versions for the word \"church\"), and are still unknown by a majority of alsatian internet users as shown by a recent survey [cit] ."
"our system also incorporates explicitly defined introns (edis) that appear under each nullj and notg. introns in gp are comprised of code that has no effect on overall fitness. edis are introns that have been designed to be introns, and therefore can be safely ignored when compiling the program, thus saving runtime. luke [cit] discusses introns in some detail. for more discussion of introns in our system see benbassat and sipper [cit] ."
"where each b i is the group sparse coefficient for x i and d i represents the dictionary, which is usually learned from each group [cit] . the 1 -norm is initially imposed on each column of b i, and here extended to be the 1 -norm on matrix."
"alsatian is a french regional language counting 550,000 [cit] . this continuum of alemannic dialects is an example of language in which the dialectal variants are not erased in the written form by any spelling system."
"in our system, the runtime of a single turn of an evomcts player is similar to that of the standard mcts player using the same number of playouts. this stems from the fact that evomcts players spend the majority of computation time on tasks other than board-state evaluation, which the standard mcts players also perform. ultimately, this behavior depends upon implementation. in our case we focused on flexibility and ease of transfer between games. it may be that in highly specialized code the overhead of evomcts board evaluation will have a more significant cost in computation time relative to the standard mcts player."
"as table xi shows, evomcts dodgem players outperform both the mcts player that uses the same number of playouts, and the much stronger mcts player that uses twice as many playouts, by a wide margin."
"in this section, we first analyze the weakness of the traditional nnm model and then propose the rrc model to improve the performance of rank estimation."
the mcts algorithm can be seen as comprised of 3 steps that are repeated over and over again as many times as the time constraints allow: 1) descend down the game tree using statistics recorded in the tree from previous playouts until an unvisited node n is encountered and added to the tree. 2) evaluate node n by preforming a quick simulation (or playout) and recording the result. 3) update the statistics of n and all of its ancestors in the tree in accordance with the result.
"we developed a slightly gamified crowdsourcing platform, recettes de grammaire 1 which allows us to collect (i) raw corpora in the shape of cooking recipes, (ii) part-of-speech annotations on the recipes, and (iii) alternative spellings. the platform is language independent and its source code is freely available on github 2 under the ce-cill v2.1 license. 3 we do not differentiate variants due to a variation in dialects, in spelling or in an accumulation of these two factors during collection."
"the interface allows the participants to provide an alternative spelling for either a single word or a sequence of words. although the latter facilitates the task for the participants, it sometimes leads to alternative spellings which number of words did not match the original version, hence could not be immediately aligned. in such cases and when possible, the alternative spellings were manually aligned with the original version."
"if the length of the oov word is less or equal to four characters (ˆand $ excluded), only the l+r rules are applied: it has been observed in preliminary tests that shorter words were more likely to lead to erroneous matching such as das (determiner) /dass (subordinating conjunction) or dien (auxiliary) /dene (determiner). additionally, we force the variant candidates to have the same letter case as the oov word."
"these features enable the participants to modify the content they read and further annotate in a manner that suits their writing habits. in fact, feedback we received on previous experiments led on crowdsourcing part-of-speech annotations for alsatian [cit] ) highlighted the fact that some participants felt unrepresented by the texts on the platform, and that annotating dialectal or spelling variants they are not familiar with was an obstacle hard to overcome."
"after variant pairs have been generated, the oov words are replaced by their variant candidate, and the pre-trained model is applied on the transposed evaluation corpus. after the corpus has been tagged, the transposed words are replaced by they original form."
"however, it closely relates to other experiments that involve at least a standard spelling and sometimes an expert linguist supervision. one such example is vard 2, [cit], 2009) . another one concerns the basque language [cit] and proposes a solution to map the variations of the language to the standard form using an existing morphological analyzer and a parallel corpus. obviously, a lot of more or less recent publications concern the design and use of morphophonological rules, in particular in various flavors of fsts, but most of them require the intervention of a highly-skilled linguist."
"although this \"brute-force\" method generates a great quantity of noise, the filtering operated by v lookup leads to the matching of oov word with existing variant candidates."
"whenever a node returning a floating-point value was chosen for mutation, a decision had to be made on whether to activate the traditional tree-building mutation operator, or the local factor mutation operator. toward this end we designated a run parameter that determined the probability of opting for the local mutation operator."
"in a coevolution round, each member of the population in turn played black in a number of games equal to the parameter cop layn um against cop layn um random opponents from the population playing white. the opponents were chosen in a way that ensured that each individual also played exactly cop layn um games as white. this was done to make sure that no individuals received a disproportionately high fitness value by being chosen as opponents more times than others. when playing a game, each player in the population received 1 point added to its fitness for every win, and 0.5 points for every draw."
"from the 367 variant pairs collected for alsatian, we extracted 213 unique rules using the left and right contexts, 227 rules using the left context only, 186 rules using the right context only."
"it is important to understand that this process is independent from the tagger, and occurs after it has been trained. the extraction of pairs is performed at the time of annotation on a previously unseen corpus."
"we show the equivalence of the rrc and gsrc models by showing that the problem eq. (6) and eq. (41) are equivalent, provided that the group-wise dictionaries are constructed using eq. (43) . we first prove the lemma 2."
"it is difficult to define an effective local mutation operator for tree-based gp. any change, especially in a function node that is not part of an intron, is likely to radically change the individual's fitness. in order to afford local mutation with limited effect, we changed the gp setup. to each node returning a floating-point value we added a floating-point variable (initialized to 1) that served as a factor. the return value of the node was the normal return value multiplied by this factor. a local mutation would then be a small change in the node's factor value."
"in this section, we provide an analytical investigation on the connection between the proposed rrc model and the popular group-based sparse representation (gsr) model [cit] . more specifically, we show the equivalence of the proposed rrc and the gsr using a specific method to construct the group-wise dictionaries, i.e., the group sparsity residual constraint (gsrc) model [cit] ."
"where z k, j andẑ k,i represent the j -th and i -th patch of z k and z k, respectively. note that the initialization ofẑ is the jpeg compressed image."
"experiments in pos tagging alsatian include our previous work [cit] b), which uses melt [cit], a freely available sequence labeller achieving at best 84% accuracy when the variants in the training and the evaluation corpus are carefully controlled. experiments using word embeddings have been also been carried on alsatian by, using a raw corpus of 200 000 tokens and reaching 91% accuracy."
two pos-tagged corpora are available for alsatian. both are made of texts produced in an uncontrolled environment (such as wikipedia 5 ) and contain multiple variants of the language:
"we performed several evolutionary runs, experimenting with various parameters, in order to find a suitable parameter setup. table vi shows the results from some of our best reversi runs. the table clearly demonstrates that our players not only beat the standard mcts player that uses the same number of playouts, but also hold their own against a much stronger mcts player that uses twice as many playouts."
"it is well-known that image denoising is an ideal test bench to measure different statistical image models. obviously, these experiments have verified that the proposed rrc is a promising image model."
"where the first term is the data fidelity and the second term depends on the employed image priors. in the following, we will introduce how to design each term of eq. (17). 1) quantization noise model: in our task, the observed jpeg-based compression image is usually modeled as the corrupted image with quantization noise,"
"in addition to being game nonspecific, our method is also to a great degree algorithm nonspecific within the mcts algorithm family. we used our method in conjunction with the standard uct algorithm (with an added enhancement of gametree memory) for which we hand-tuned some parameters. this method can, however, be used together with a more specialized game-specific version of mcts that navigates the game tree in any other way. as our method focuses on evolving playout behavior it is indifferent to changes in the particulars of the mcts implementation."
"the proposed rrc is a traditional model-based optimization algorithm. here we evaluate the average running time of the proposed rrc model for image denoising by using 16 widely used images with different noise levels. the proposed rrc requires about 5∼6 minutes for an image on an intel (r) core (tm) i3-4150 with 3.56hz cpu and 4gb [cit] b environment. the running time of the proposed rrc for image denoising is faster than lssc, ncsr and gid methods. 1 [cit], pc-lrm [cit], ance [cit], dictv [cit], bm3d [cit], wnnm [cit], concolor [cit], ssr-qc [cit], lerag [cit] and rrc on the bsd 100 [cit], classic5 and live1 [cit] dataset"
"these results show that the generated variant pairs should be hand-checked, a task that can itself be crowdsourced, provided that we have access to the context of appearance of both elements."
"in this paper we shall refer to mcts players evolved using our system as evomcts players. the evomcts players use the same mcts parameters as the handcrafted player. instead of using random playouts, the players use evolved board evaluation functions in the following fashion: an additional parameter dubbed playoutbranchingfactor is used in the evomcts players. before each simulated move in the playout, the players evaluate playoutbranchingfactor randomly chosen legal moves and select the move evaluated as best by the evolved evaluation function. algorithm 4 describes how evomcts players' playouts work in our system. in order to allow even the moves evaluated as bad a chance to be selected, playoutbranchingfactor is a maximum value of moves to be considered. with a low probability the algorithm can choose the same move more than once, thus allowing even the move evaluated as worst a chance to be chosen. we did this because of the inherent limitation of even the best fast evaluation functions that sometimes fail to correctly asses the value of a board state."
"in the context of oov words reduction in a given corpus, step 4 is followed by a transposition of those for which a variant has been identified in r lookup . especially, in the context of supervised machine learning, one cannot expect to find all existing variants in a training corpus. by replacing an oov word by one of its already known spelling variants, we make the most of the annotations we have at our disposal (see section 4)."
"in much of the work on games the focus is on a single game, the goal being to reach a high level of play, using techniques such as opening books [cit] and endgame databases [cit] . conversely, our focus has been on multigame generality [cit] . using our game system, which we have demonstrated to be flexible and easily applicable to multiple games, we choose to avoid using specialized techniques and expert domain knowledge in favor of generic, easily transferable evolutionary tecniques. it is with this view in mind that we present evomcts, a new method for enhancing monte-carlo tree search (mcts) game players."
"this work opens several avenues for future research. firstly, the evomcts approach presented here can be applied to high branching-factor games for which mcts-based methods have proven especially effective (e.g., go or hex). evolving players for these \"heavier\" games may require more computational resources, but we believe this goal is within reach even with current available hardware. another possible avenue would be to expand evomcts and have it evolve algorithm behavior within the search tree itself. this may help in improving on standard methods like uct in domains where a fine-tuned domain-specific implementation does not exist."
"we show that in a low-resource scenario, collecting spelling variants for only 145 words can lead to (i) the generation of 876 additional variant pairs, (ii) a diminution of out-of-vocabulary words improving the tagging performance by 1 to 4%."
"for this reason, the dialectal variations (6 to 8 variants emerge from the continuum) combine with the variety of spelling habits, which might depend, for instance, on the linguistic backgrounds of the speakers."
the rest of this paper is organized as follows. section ii describes the rrc model based on the rank minimization scenario. section iii presents how to use the proposed rrc model for image denoising. section iv develops the algorithm for image compression artifacts reduction exploiting the proposed rrc model. section v provides an analysis investigation for the proposed rrc model in terms of group-based sparse representation. extensive results for image restoration are presented in section vi and section vii concludes the paper.
"we first obtain the solution of the unconstrained quadratic minimization of eq. (31), and later project the solution to . specifically, without considering the constraint of, eq. (31) can be rewritten as"
"we plan to extend this work to other nonstandardized languages. we have started working on adapting the platform to mauritian, a frenchbased creole, the morphology of which is very different from that of alsatian."
"with the rapid development of social network and mobile internet, billions of image and video resources have been spread through miscellaneous ways on the internet everyday. to save the limited bandwidth and storage space, lossy compression scheme (e.g., jpeg [cit], webp [cit] and hevc-msp [cit] ) has been widely used to compress images and videos. however, these lossy compression techniques often give rise to visually annoying compression artifacts, i.e., sacrificing the image quality to satisfy the bit-budget, which severely degrade the user experience. furthermore, the performance of many other computer vision tasks (e.g., image recognition [cit] and object detection [cit] ) largely depends on the quality of input images, and therefore it is desired to recover visually pleasing artifact-free images from these compressed images. it is well-known that jpeg is the most popular lossy compression method [cit], and therefore in this work we focus on jpeg compression artifacts reduction."
"where η is a constant and the image z (t −1) is reconstructed by all the z (t −1) k at the (t − 1) th iteration. after solving the two sub-problems, we summarize the complete algorithm to solve eq. (26) in algorithm 2."
the code of both the gamified crowdsourcing platform and the variants generation is freely available on github 7 . the created multi-variant lexicon is also available under a cc license.
"every individual in the population plays coplaynum games as black against coplaynum random opponents in the population and as a result also plays coplaynum games as white. assign 1 point per every game won by the individual, and 0.5 points per drawn game"
"developing players for board games has been part of ai research for decades. board games have precise, easily formalized rules that render them easy to model in a programming environment. in this work we will focus on perfectinformation, deterministic, zero-sum board games."
"for each experiment, we extract from the training corpus the vocabulary v t lookup and from the external lexicon, the vocabulary v l lookup. we use the set of rules presented in section 3.1."
"one-way crossover, as opposed to the typical two-way version, does not consist of two individuals swapping parts of their genomes, but rather of one individual inserting a copy of part of its genome into another individual, without receiving any genetic information in return. this can be seen as akin to an act of \"aggression\", where one individual pushes its genes upon another, as opposed to the generic two-way crossover operators that are more cooperative in nature. in our case, the one-way crossover is done by randomly selecting a subtree in both participating individuals, and then inserting a copy of the selected subtree from the first individual in place of the selected subtree from the second individual."
"to meet this goal, we developed a crowdsourcing platform that collects two types of resources: (1) raw texts and (2) spelling variants on these texts. these resources are used to seed the unsupervised augmentation of the multi-variant lexicon following a process that we detail in figure 1."
"as the results show, using gp to evolve heuristic board evaluation functions for playouts has proved useful in improving mcts players in two very different board games. the scalability of results means that although time constraints render our evolutionary approach limited to producing only very fast players, we can later improve those evomcts players offline by increasing the number of playouts employed by the mcts algorithm."
"l ow-rank matrix estimation has attracted increasing attention due to its wide applications. in particular, it has been successfully applied in various machine learning and computer vision tasks [cit] . for instance, the netflix customer data matrix is treated as a low-rank one for the reason that the customers' choices are largely dependent on a few common factors [cit] . the foreground and background in a video are also modeled as being sparse and low-rank, respectively [cit] . as the matrix formed by nonlocal similar patches in a natural image is of low-rank, various low-rank models for image completion problems have been proposed, such as collaborative filtering [cit], image alignment [cit], image/video denoising [cit], shadow removal [cit] and reconstruction of occluded/corrupted face images [cit] ."
"reversi, also known as othello, is a popular game with a rich research history [cit] . the most popular reversi variant is a board game played on an 8x8 board. reversi is a piece-placing game, meaning that moves are made by placing a new piece on the board rather than by moving existing pieces around as in games such as chess and checkers. the players place their pieces on the board in turns, attempting to capture and convert opponent pieces by locking them between friendly pieces. in reversi, the number of pieces on the board increases during play, rather than decrease as it does in chess and checkers. this fact makes endgame databases all but useless for reversi. on the other hand, the number of moves (not counting the rare pass moves) in reversi is limited by the board's size, making it a short game. there is also 10x10 variant of reversi, which is quite popular. in this paper we focus on the 8x8 version."
"the change in population from one generation to the next was divided into two stages: a selection stage and a procreation stage. in the selection stage we used tournament selection to select the parents of the next generation from the population according to their fitness. in the procreation stage, genetic operators were applied to the parents in order to create the next generation."
"the efficiency of the methodology largely depends on: (i) the respective and relative sizes of the training and evaluation corpora, (ii) the variation in variants existing between them."
both basic types of crossover used have their roots in nature. two-way crossover is often seen as analogous to sexual reproduction. one-way crossover also has an analog in nature in the form of lateral gene transfer that exists in bacteria.
"given an existing linguistic resource (corpus, lexicon, or both) r lookup and a set of out-ofvocabulary words v oc oov, the process consists of four steps: these steps are detailed in sections 2 and 3 and illustrated with their application on alsatian."
"in what follows, we first present our approach to generate spelling variant pairs based on an initial set of crowdsourced spelling variant pairs. this method is language independent and relies on resources that do not require expert knowledge, hence can easily be crowdsourced."
"in this context, we use our methodology to match oov words from the evaluation corpus with their potential spelling variant appearing in the training corpus."
"our first model is trained with c oncat c80 (17,136 words) and evaluated on c oncat c20 (4,374 words) before and after its transposition. after the application of the three sets of rules, using both the vocabularies extracted from c oncat c80 and m ultiv ar l for the lookup, 56 variant pairs were discovered and the same number of words were transposed. before transp. after transp. overall 0.859 0.864 oov words 24% 22% table 2 : accuracy of the model trained on multivariant corpora, before and after the corpus transposition."
"as demonstrated in fig. 2, due to the influence of noise, it is difficult to estimate the matrix rank precisely using nnm. more specifically, in fig. 2(d), the singular values of the observed matrix are seriously deviated from the singular values of the original matrix. however, in low-rank matrix estimation, we expect that the singular values of the recovered matrix x and the singular values of the original matrix x are as close as possible. explicitly, we define the rank residual by"
"selection was done by the following simple method: of several individuals chosen at random, copies of a subset of fitter individuals was selected as parents for the procreation stage. the pseudocode for the selection process is given in algorithm 3. two more parameters are crossover and mutation probabilities, denoted p xo and p m, respectively. every individual was chosen for crossover (with a previously unchosen individual) with probability p xo and self-replicated with probability 1 − p xo . the implementation and choice of specific crossover operator was as in algorithm 1. after crossover every individual underwent mutation with probability p m (another parameter, p lm, denotes the probability of the algorithm choosing to perform local mutation). there is a slight break with traditional gp structure, where an individual goes through either mutation or crossover but not both. however our system is in line with the ga tradition where crossover and mutation act independently of each other."
"this type of crossover operator is uni-directional, with a donor and a receiver of genetic material. this directionality can be used to make one-way crossover more than a random operator. in this work, the individual with higher fitness was always chosen to act as the donor in one-way crossover. this sort of nonrandom genetic operator favors the fitter individuals as they have a better chance of surviving it. algorithm 1 shows the pseudocode representing how crossover is handled in our system. as can be seen, one-way crossover is expected to be chosen at least half the time, giving the fitter individuals a survival advantage, but the fitter individuals can still change due to the standard two-way crossover. the algorithm can be seen as describing a new genetic operator, which we dub selective crossover, since it exerts selective pressure because less-fit individuals are more likely to receive genetic information from fitter ones than vice versa."
"fitness calculation was carried out in the fashion described in algorithm 2. though our system supports various methods of fitness evaluation, in the evolutionary runs described in this paper fitness is decided by having evolving players face their own cohorts in the population. this method of evaluation is known as coevolution [cit], and is referred to below as the coevolution round."
in all evolutionary dodgem runs above we used a a personal computer with an asus sabertooth 990fx board with an amd phenom ii x6 1100t @ 3400mhz 6-core processor with 6mb l3 cache and 16gb ram. runs took 1-2 days.
"then, based on the adaptive dictionary d i in eq. (43) and theorem 2, we have proved that eq. (51) is equivalent to eq. (11) . note that we assume the pca space of x i and x i are equivalent here. we have thus that rrc is equivalent to gsrc, i.e.,"
"by \"controlled\", we mean that training and evaluation each contain a specific variant of alsatian selected in a multi-variant corpus. in the following, we compare homogeneous and heterogeneous setups, in which the training and evaluation corpora either contain the same or distinct variants of alsatian."
"since the identification of potential variant pairs depends on the initial conditions of the experiment, i.e. the corpus, and optionally, the lexica used to train the model beforehand, we present two experiments in which these parameters vary."
"this experiment shows that the performance of a tool trained on a given corpus can be improved by modifying the corpus it is applied on to match the vocabulary it was trained with. table 3 : accuracy of the model trained on mono-variant corpora, before and after the corpus transposition."
section ii contains some basic information on reversi. section iii is a short presentation of mcts and the uct algorithm. section iv presents previous work related to ours. section v explains the genetic programming system we use and how we apply it to games. in section vi we present the results of our evolutionary runs and in section vii we demonstrate their scalability. in section viii we demonstrate the flexibility and generality of our method by showing how the whole process can be repeated for the very different board game of dodgem. section ix contains conclusions and discussion of results.
"second, we exemplify the use of such a method to reduce the proportion of unknown words that undermines supervised algorithms in the context of non-standardized languages."
"where w is a constant which is typically set to be not greater than 0.5 [cit] . the frequency-domain coefficients of the image should satisfy the bound (i.e., qc) [cit] :"
"to highlight the effect of our methodology in an heterogeneous context, met when no corpus of each possible variant is available, we manually split c oncat c in two sub-corpora n orth c (4,880 words) and s outh c (7,690 words) based on the frequencies of the -e and -a noun endings, which are specific of the northern and southern variants respectively."
"in practice, one would like to approximate the oracle group sparse coefficient b i using the group sparse coefficient a i based on the corrupted measurement y i . the quality of the approximate heavily depends on the the difference between b i and a i, i.e., the group sparsity residual, which we define as the following,"
"given a vocabulary of known words v lookup, the identification of potential variants of an oov word includes (i) optional preliminary filtering, (ii) application of rules, and (iii) lookup: 3. lookup: the sequence of rules apply until the produced form is matched with a word present in v lookup ."
"theˆand $ characters, respectively representing the beginning and the end of a word, are in- since the result we seek is not to normalize the spelling, each rule can be used in both directions which are considered equally frequent."
"in our basic system the individuals in the population act as board-evaluation functions, to be combined with a standard game-search algorithm-in our case mcts. the value an individual returns for a given board state is seen as an indication of how good that board state is for the player whose turn it is to play. in this work the evaluation functions are used to choose between possible moves in the playouts that mcts performs."
"the originality of this methodology is that once the rules have been extracted, the process feeds from previously unseen texts. this is particularly useful in a less-resourced scenario where a raw corpus is being collected from various sources."
"a man-count terminal returns the number of pieces (or \"men\") the respective player has, or a difference between the two players' man counts. the mobility terminal node allows the square-specific nodes all return boolean values. they are very basic, and encapsulate no expert human knowledge about the game. in general, one could say that the domainspecific nodes use little human knowledge about the game of reversi. this goes against what has traditionally been done when gp is applied to board games [cit] . this is partly due to the difficulty in finding useful board attributes for evaluating game states in some games (benbassat and sipper [cit] deals with a game that is a perfect example of this)-but there is another, more fundamental, reason. not introducing game-specific expert knowledge into the domain-specific nodes means the gp algorithm defined is itself not game specific, and thus more flexible."
"the functions implemented include logic functions, basic arithmetic functions, one relational function, and one conditional statement. the conditional expression renders natural control flow possible and allows us to compare values and return a value accordingly. in figure 2 we see an example"
"our evolutionary system evolves gp players that use the mcts algorithm. we implemented a uct variant of mcts. the parameters we can tune control the number of playouts used before each move, the initial value of unexplored nodes in the game tree (in the standard mcts this value is 0, leading to unexplored game states always being favored), the c constant from the uct formula (equation 1), and a parameter used to enhance search by having players remember the search tree from previous turns (this way mcts gains some of the playouts from its previous turns \"for free\"). we decided on values for these parameters empirically in order to get better players. based on this we can define handcrafted mcts players to be used as yardsticks to test evolved players against."
"the proportion of oov words was diminished by around 2% resulting in an improvement of the tagging performance of 0.5 points (see table 2 ). this minimal impact is expected since the performance on \"known words\" is around 10 points higher than on oov words in this setup. in fact, considering the sizes of our corpora, lowering the number of oov words of 100 is expected to improve the overall results of 0.2 points."
"the corpus resulting from the concatenation of the two corpora, c oncat c, was used for the following experiments. we performed a cross validation on 4 subdivisions (80% used for training, c oncat c80, 20% for the evaluation, c oncat c20)."
using mcts with 100 playouts is fine if what one wants is a fast player with basic game proficiency. but to obtain strong players more playouts are needed. just as the standard mcts players can be tuned and improved by increasing the number of playouts (see table v ) so can our evolved players. tables vii and viii show how two top evolved players maintain their advantage when the number of playouts is scaled up.
"of a gp tree containing a conditional expression. the subtree depicted in the figure returns 0 if the friendly corners count is less than double the number of enemy men on the board, and the number of enemy men plus 3.4 otherwise."
"producing linguistic resources, be it lexica, raw or annotated corpora, represents a cost that cannot be afforded for languages missing resources in the broad sense, including funding and experts. crowdsourcing has proven to be a viable option to produce quality resources at a reduced cost [cit] . applying crowdsourcing to less-resourced non-standardized lan- guages presents additional difficulties such as accessibility to the speakers, or representativity of contents [cit] b ). yet, when a community of speakers can be found on-line, it seems necessary to empower them to produce raw corpora and to document variability. in fact, the speakers appear to be, collectively, the only experts of the mechanisms at stake."
"we now develop an efficient algorithm to solve the optimization in eq. (6) . in order to do so, we first introduce the following lemma and theorem."
"the method does not require manual rules definition by experts and is language independent. the resources needed to perform variant pair detection can be easily produced by the speakers, who hold the knowledge of the of the variation mechanisms. the crowdsourcing of variants, unlike that of pos tags, requires no prior training."
"similar to the proposed rrc model, in real applications, x i is inaccessible and we thus employ an estimate of it, denoted by x i . given x i and the dictionary d i, the group sparse coefficient b i for each group x i is solved byb"
"the addition of a new spelling variant can be performed: (i) by adding a variant to any word that is present on the platform by clicking on a word cloud on the main page (see figure 2 ), (ii) by dynamically editing the written contents on the website thanks to a feature called \"personally, i would have said it like that!\", illustrated on figure 3 ."
"non-standardized languages present a great productivity of spelling variants for a given word. the absence of standardized spelling points up the geographical and demographic variations that might exist and are otherwise smoothed down. this variability results in the coexistence of alternative written forms, hence in a large proportion of outof-vocabulary words in the context of supervised machine learning."
"dealing with non-standardized, less-resourced languages, takes us to the limits of nlp: first, we have no standard to rely on and not enough expert linguists to help us, and second, very few language resources are available for us to work with, even raw corpora. these two constraints are rarely met in the literature and, to our knowledge, the solution we propose has never been used before."
"unsurprisingly, the best results are obtained when training and evaluation corpora are of the same variant. yet, we can observe that in this setup, the effect of transposition to identified variants has a higher impact on the proportion of oov words and the tagging performances."
"monte carlo tree search (mcts) is a general method for making decisions in a given domain, initially proposed and developed by multiple research groups [cit] . the idea of mcts is to gradually build the domain search-tree by way of performing successive random playouts. in games, a gametree is built one game-state at a time, with the next state to be expanded and added to the game tree chosen according to the results of past random playouts (i.e., the partial gametree is biased towards moves that yielded better results). this approach has proved useful in generating effective board game players and is responsible for the great improvement in level of play seen in games with a high branching factor such as go [cit] and hex [cit] . mcts is also the leading approach in the field of general game playing [cit] ."
"in order to test the quality of evolved players we need to test them against some sort of benchmark opponent-in this work we used standard mcts players that used the uct formula. before beginning the evolutionary experiments, we first evaluated our mcts benchmark players by testing them against each other in matches of 10,000 games (with players alternating between playing either side). table v shows the relative strengths of the different reversi players. as expected, in all evolutionary reversi runs that follow we used 16 cores of 3 ibm x3550 m3 servers with 2 quad core xeon e5620 2.4ghz smt processors with 12mb l3 cache and 24gb ram. runs took 3-5 days."
"the question of variation in non-standardized languages naturally arises starting when one begins the process of corpus building (or collection). when dialectal and spelling variants overlap, inter-and intra-dialectal variations can be hard, not to say impossible, to untangle. in the following, we will design as \"spelling variant\" any variant due to either dialectal variation, spelling convention variation, or an accumulation of both. although one might chose to work on corpora produced in a controlled environment, in which the spelling conventions and writers are carefully chosen, this setup is unlikely to produce satisfying results on real-life data."
"since part of the dialectal and spelling variation mechanisms may be similar to some of the language morphological rules (such as gender, number, conjugation or declension), the generated variant pairs should be manually checked in context."
"service robot is an emerging technology in robot vision, and demand from household and industry will be increased significantly in the future. general vision-based service robot should recognizes people and obstacles in dynamic environment and accomplishes a specific task given by a user. the ability to face recognition and natural interaction with a user are the important factors for developing service robots. since tracking of a human face and face recognition are an essential function for a service robot, many researcher have developed face-tracking mechanism for the robot (yang m., 2002) and face recognition system for service robot ( budiharto, w., 2010) . the objective of this chapter is to propose an improved face recognition system using pca(principal component analysis) and implemented to a service robot in dynamic environment using stereo vision. the variation in illumination is one of the main challenging problem for face recognition. it has been proven that in face recognition, differences caused by illumination variations are more significant than differences between individuals [cit] . recognizing face reliably across changes in pose and illumination using pca has proved to be a much harder problem because eigenfaces method comparing the intensity of the pixel. to solve this problem, we have improved the training images by generate random value for varying the intensity of the face images. we proposed an architecture of service robot and database for face recognition system. a navigation system for this service robot and depth estimation using stereo vision for measuring distance of moving obstacles are introduced. the obstacle avoidance problem is formulated using decision theory, prior and posterior distribution and loss function to determine an optimal response based on inaccurate sensor data. based on experiments, by using 3 images per person with 3 poses (frontal, left and right) and giving training images with varying illumination, it improves the success rate for recognition. our proposed method very fast and successfully implemented to service robot called srikandi iii in our laboratory. this chapter is organized as follows. improved method and a framework for face recognition system is introduced in section 2. in section 3, the system for face detection and depth estimation for distance measurement of moving obstacles are introduced. section 4, a detailed implementation of improved face recognition for service robot using stereo vision is presented. finally, discussions and future work are drawn in section 5."
"although the catastrophic bit errors caused by cs can be mitigated by pilot symbols [cit], their occurrence cannot be avoided and performance degradation will be experienced in the presence of high laser phase noise. this cs degradation caused by practical limitations of an explicit cpr is exacerbated by laser frequency instabilities introduced by mechanical vibrations including power supply noise [cit] . these frequency fluctuations are modeled as sinusoidal frequency modulation of large amplitude (e.g., $500 mhz) and low frequency (e.g., 635 khz)."
"furthermore, from fig. 2 notice that the two polarizations at the output of the dsp block are treated as two independent coded channels. consequently, in this work a single polarization is considered for the channel modeling and the theoretical formulation of the jidd algorithm."
"the ccr block estimates the carrier frequency offset (e.g., based on the spectral shift of the received signal) and compensates it in order to achieve a capture range of several ghz. 1 the chromatic dispersion is compensated typically by a frequency domain equalizer (fde). the fiber length is automatically identified during the startup and the response of the filter is programmed accordingly. a t/2 multiple-input multiple-output (mimo) feedforward equalizer (ffe) performs the polarization demultiplexing and the compensation of pmd and polarization-dependent loss (pdl). an adaptation algorithm is essential in optical channels since the receiver must track nonstationary effects (pmd, pdl, changes in the state of polarization of the tx or lo lasers, etc.). towards this end, decision-directed lms and/or the constant modulus algorithm (cma) are typically used. a low latency phase-locked loop (pll) is included to get tentative decisions required to implement the ffe adaptation algorithm [cit] . since most part of the carrier frequency offset has already been compensated by the ccr, the pll is required mainly to track short-term frequency instabilities of the lasers as well as part of the phase noise [cit] . to avoid the cycle slips introduced by the pll in transmission with non-differential modulation, the signal not demodulated by the pll is fed to the cpr. therefore the cpr block must be able to track high-frequency laser phase noise, nonlinear phase noise as well as laser frequency fluctuations. finally, the samples are processed by the soft decision demapper (sdd) which provides the soft information used by the iterative ldpc decoder to estimate the transmit bit."
"t where d j is a distance (in pixels) from the centre of the screen which is positive for target 2 (right), negative for target 1 (left) and the magnitude is one of 10, 25, 40 or 55 as determined by the condition (difficulty) of trial j."
"to implement the maximum-a posteriori (map) detector, the a posteriori probability (app) pðc k jrþ must be evaluated. towards this end, jidd uses the sum-product algorithm (spa) on a factor graph (fg) to evaluate the joint app distribution function 2 the proper value of p r shall depend on the amount of phase noise power in the system. 3 note that the effective bit rate is $200 gb/s. of the edge l k (not depicted for purposes of clarity) are the pdfs of l k given the past and the future, respectively, i.e.,"
"this chapter presents an improved face recognition system using pca and implemented to a service robot in dynamic environment using stereo vision. by varying illumination in training images, it will increase the success rate in face recognition. the success rate using our proposed method using its face database is 95.5 %, higher than att face database 95.4%. the simple face database system propsed can be used for the vision-based service robot. experimental results with various situations have shown that the proposed methods and algorithms working well and robot reaches the goal points while avoiding moving obstacle. estimation of distance of moving obstacle obtained by stereo vision. bayesian decision rule implemented for state estimation makes this method more robust because the optimal solution for avoiding obstacles is obtained by trading between maneuver and stop action. in future work, we will implementing this system and develop a vision-based humanoid service robot for serving customers at cafe/restaurants."
"the shape of the collapsing bound (fig. 9 ) determines when the bound drops strongest throughout the trial. we have found that difficult conditions exhibit an early drop, whereas in easier conditions there is a more gradual decrease. although this does not seem to comply with the intuition that bounds should collapse towards the end of the trial, it is consistent with previous findings 21, 22 . clearly, more research is needed to investigate the causes of these particular shapes of collapse. overall, we suggest that the exam is in principle sensitive enough to find evidence for a collapsing bound based on experiments, but the actual implementation of a collapsing bound by participants may depend on several factors, for example on the precise experimental design, the level of training, and instructions to the participants 25, 49 ."
"models. ddm-equivalent model vs. exact input model. we used a recently developed bayesian model 16 that consists of: (1) stimulus input to the model, (2) generative model(s) of the stimuli, (3) bayesian inference as evidence accumulation and (4) a decision criterion. depending on how we modelled the input, we obtained two different model instances, i.e., the ddm-equivalent model and the exam. both models are exactly the same except for the input to each model: in the ddm-equivalent model, we obtained the equivalent to the pure ddm by modelling the input as the constant, correct target position (yellow dot) with random noise 16 ( fig. 2a) . in the exam, we used as input the exact white dot positions (the bee locations) plus some noise caused by sensory processing (fig. 2b) . this models the time-varying sensory input seen by the participant during each trial. the bayesian modelling technique used here computes after each update of the white dot position what the participant should compute as evidence for a decision. this is the so-called posterior beliefs over the two decision alternatives (left/ right) fig. 2c,d . the decision is made when the posterior belief for one of the alternatives exceeds a bound (for details on the models, see material & methods)."
"a recent auditory decision making study has modelled the precise timing of a sequence of click sounds to better resolve specific decision making parameters, as compared to standard analysis procedures 34 . the authors found that the main source of variability was based in the sensory input, and the accumulation was without leak. in addition to the same single-trial approach in this study, we also analysed reaction times: by only modelling participants' choices, one may be limited in fully resolving the underlying parameters of decision making. in particular, for our design, the reaction time of a single trial determines how many dots have been seen by the participant and is therefore clearly relevant for modelling the observed choices. furthermore, one implicit assumption of the exam is that participants weight single pieces of evidence (here, a single dot shown for 93 ms) by their relevance for deciding between the two alternatives [cit] . for example, the more a dot's location is to the left, the higher the likelihood for the left target. we were able to show that this weighting of evidence actually holds by finding that the exam is a better model than the ddm-equivalent model."
"input stimuli and evidence accumulation. how is this noise reduction achieved by the exam? in fig. 2a,b the sensory input for the ddm-equivalent model and exam is shown for an illustrative single trial. for the exam, the dot jumps around the right target but then (due to the random movement of the dot) jumps to the left side for several time points between 400 ms and 900 ms (red box in fig. 2b) . figure 2c,d show the evolution of the posterior beliefs for both models. the ddm-equivalent model mostly predicts a decision for the right, correct target (fig. 2c) . in contrast, the exam gives more specific predictions reflecting the dynamics in the actual sensory input over time (fig. 2d) . in this example trial, the participant's (correct) response lies within the rt distribution predicted by the exam (fig. 2e,f, asterisks below histograms) . interestingly, the exam also predicted that the participant may have responded earlier with the correct response, followed by a time period where the incorrect response would have been likely. this dynamic aspect of the exam predictions makes the model diverge from the predictions of the ddm-equivalent model, which (always) predicts a uni-modal rt distribution (fig. 2e) . this enables the exam to explain more variability in actual decision behaviour. while this is an exemplary single-trial result, this difference between models is the basis for the following results."
"in fig. 8a -c, we show the three collapsing bound parameters (see material & methods), for both the ddm-equivalent model and the exam. these three parameters, the initial bound, the stretch (how far the bound falls), and shape (how early the bound falls) determine the overall effect of the collapsing bound. these parameters and their changes over conditions reveal how the collapsing bound may operate in our experiment: the initial bound (fig. 8a) increases as the conditions get easier from d1 to d4 (fig. 8b) for both ddm-equivalent model and exam. this effect is more prominent for the exam. for both the ddm-equivalent model and the exam, the stretch decreases as the conditions get easier. this means the overall effect of the collapsing bound (which also depends on the initial bound) is smaller for the easier trials. again, this effect is more pronounced for the exam for all difficulty levels except for the easiest condition d1. for the shape parameter, we found that the bound drops earlier for the two most difficult conditions than for the two easiest conditions (see material & methods). see fig. 9 for a visualisation of the fitted collapsing bounds. figure 8d shows the absolute amount of collapse for each difficulty level in terms of bound. importantly, the amount of collapse closely resembles both the overall ppl pattern and the model frequency for the exam (see fig. 6c,d and 7, number of trials under dotted vertical line). this indicates that both the ppl differences and model frequencies indeed capture the amount of collapse expected for different conditions. figure 10 shows an example of the evolution of the decision-making process under exam, with (fig. 10b,d,f) and without (fig. 10a,c,e ) the collapsing bound from the same trial. due to the first dot starting on the right side (fig. 10a), early decisions tend to be the right alternative (fig. 10c,e), with later decisions choosing the correct left decisions. however, with the collapsing bound, the initial bound is higher than the standard accumulation, thereby delaying the earlier decisions, which makes the erroneous right decision less frequent. this therefore increases the probability for the left decision, while reducing the probability for timed-out trials altogether (fig. 10d,f) . in other words, the collapsing bound shifts the probability mass from both early and late time points to more intermediate rts."
"to apply bayesian decision theory for obstacle avoidance, we consider the appearance of an unexpected obstacle to be a random event, and optimal solution for avoiding obstacles is"
"where is the intensity value after brightness operation applied, is the intensity value before brightness operation and is a brightness level. the effect of brightness level shown at histogram below: we have developed a framework of face recognition system for vision-based service robot. this framework very usefull as a information for robot to identify a customer and what items ordered by a customer. first, to storing training faces of customers, we have proposed a database for face recognition that consists of a table faces, products and order. an application interface for this database shown below : fig. 3 . we have proposed face databases using 1 table, 3 images used for each person (frontal, left and right poses)."
"figure shown below is the result of 2 moving obstacle identification using stereo vision, distance of obstacle obtained using depth estimation based on eq. 4. state estimation is used for handling inaccurate vision sensor, we adopted it using bayesian approach for probability of obstacle denoted as p(obstacle) and probability of direction p(direction) with the value between 0-1."
"we computed point estimates of choice and rt for each trial and each participant from the predicted response distributions. specifically, in a given trial we chose the alternative (left, right) which was most frequent among 15000 simulations of that trial from the model with the fitted parameters. further, we chose the corresponding rt as the most frequent time-point for the chosen alternative. then, we did a simple match test between the real behavioural data and model predicted data (fig. 5) . for the rts, we plotted the observed rts against the predicted rts as an indicator of a good match (fig. s1 )."
"to estimate the parameters of the two models for each participant, we used an advanced inference scheme called expectation propagation -approximate bayesian computation (ep-abc) 26 . we used seven free parameters that red box indicates dots leading to an early left decision. (c) posterior beliefs as in accumulated evidence, using the parameter estimates for participant 20, d2 under the ddm-equivalent model over 100 iterations. the model mostly predicts \"right\" (red) decisions at early rts. in the experiment, the participant decided for the \"right\" alternative. the grey background shading is an interval defined by participant rt minus 95% quantile interval of the ndt. black horizontal line is the bound. here, the process is in the decision time frame (without ndt). are typically used in modelling perceptual decision making 12, 16 . (1) noise standard deviation (sd): the amount of noise in the perceptual process in the brain, (2) the mean and (3) standard deviation of the gaussian which is transformed to the log-normal distribution for the non-decision time (ndt), i.e., the portion of the rt which is irrelevant to the decision process (e.g., perceptual encoding, motor preparation) (see material & methods), (4) the bound: the amount of evidence required to commit to a decision, (5) prior: the bias of a participant for a specific alternative, (6) lapse probability: the fraction of trials with a random response, and (7) timed-out (to) lapse probability: the fraction of timed-out trials within the lapse trials (for a complete list of parameters, see material & methods). we estimated the parameters from the behavioural data (reaction time and decision for each single trial) for each participant and each difficulty level (for details see material & methods)."
"this paper is organized as follows. section 2 investigates the impact of the laser frequency fluctuations in optical coherent receivers with ldpc codes and existing carrier phase recovery algorithms. the jidd algorithm is investigated in sections 3 and 4. the performance of the proposed receiver in the presence of nonlinear effects is analyzed in section 5. conclusions are drawn in section 6. to facilitate the reading of this paper, abbreviations most frequently used are listed in table 1 ."
"next, to compare the two models at the single trial level, we calculated the ppl for each trial and each participant. the ppl quantifies how likely a participant's response is under a given model after fitting the parameters. as we have shown the same 800 trials to each participant we can average the trial-specific ppl, over participants. figure 4 shows that a larger number of trials, for each difficulty level, has a higher ppl for the exam than for the ddm-equivalent model. this is most obvious for the most difficult condition (d1), for which the stimuli display the highest levels of random movements. this further substantiates that behavioural responses can be explained better if one informs the model about the exact noise pattern of the stimulus."
"inference over models given participant responses. we used a second level of bayesian inference to infer model parameters from behavioural data of participants. standard inference methods require knowledge of the likelihood of the model for given data and model parameters. for the bayesian response model, however, we chose a likelihood-free inference method. this approach has the advantage that we can easily modify the model without having to derive the potentially very complex likelihood function for the modified model. yet, inference works as for the original model. likelihood-free inference methods are also known under the name of \"approximate bayesian computation\" abc, see 51 for a recent review. computation of likelihood values is skipped in these methods by simulating data points from the model and comparing simulated data points to the recorded real data on the basis of a summary statistic of the complete data set. as data points are simulated using a particular sample of parameter values (a sampled parameter set), abc then rejects parameter sets incompatible with the data and approximates posterior distributions of model parameters based on the accepted samples. the main drawback of abc methods is that inference is computationally very demanding, especially, when the statistic summarising the data set is high-dimensional."
"evidence accumulation variants. we investigated two additional decision models which implemented two additional mechanisms previously considered in perceptual decision making: leaky accumulation 31, 34, 54 and collapsing bounds 21, 22, 46, 47, 55 . leaky accumulation departs from the optimal accumulation of evidence as defined by the bayesian decision model by continuously degrading the influence of past evidence on the current beliefs."
"the collapsing bound has been suggested as a mechanism to cope with the time limit to execute a decision 21, 46, 47 . therefore, intuitively, harder trials should benefit more from the collapsing bound. however, our results indicate that in the most difficult condition the drop in bound is rather low (fig. 8d) . this may be due to an interaction between collapsing bound and the speed-accuracy trade-off chosen by participants across difficulty levels. figure 8a shows that the initial values for the bound decrease as the task becomes more difficult. we also observed this effect in the models without collapsing bound (table 1) where the bound is thought to implement the speed-accuracy trade-off chosen by the participant, see also 4, 48 . this finding suggests that participants adapt their bound, i.e., the speed-accuracy trade-off, to the difficulty level of a trial which is indicated before the stimulus comes on (in our task, yellow dots are shown before white dot, fig. 1a) . especially, participants may choose low bounds in difficult trials to counter small momentary evidences and an otherwise slow accumulation. since the bound is already low at the start of a difficult trial, the effect of a collapsing bound in the hardest condition d1 is limited. in the easiest condition d4, the amount of collapse is the lowest (fig. 8d) . this result is fairly intuitive -the participants can make fast decisions without having to fear the upcoming end of the trial. if we used a paradigm which does not indicate the difficulty level at the beginning, we expect to see a higher initial bound at the most difficult d1 condition, and therefore more measureable collapsing bound effect, under the exam."
"in our present study we employed a particularly simple stimulus to maximally control the spatiotemporal information available for decision making. the proposed bayesian account of evidence integration and computation can be easily adapted to different, more standard or more natural stimuli as long as there is high certainty about what features or parts of the stimulus participants is used to make decisions. for example, for the rdm stimulus motion energy 36 could be used as spatiotemporal stimulus feature in our model. the described approach thus opens up a wide range of new experimental-computational studies with a large variety of stimuli to enable improved insights into the underlying mechanism of perceptual decision making."
where h k is the total phase noise and z k represents the ase noise sample. the latter is modeled as a white complex gaussian random variable with power 2r 2 . let dm be the total laser linewidth parameter. the received phase h k can be expressed as
ecpr-1 is the blind phase search carrier recovery algorithm [cit] with differential modulation (see fig. 3 ). the bps block estimates the phase noise as follows:
"fiber optic impairments include only linear effects (i.e., cd and pmd). the impact of the nonlinear effects experienced in fiber optic transmission systems shall be analyzed later in section 5. cd and pmd are perfectly compensated by the fde and the mimo-ffe, respectively. most carrier frequency offset is compensated at the receiver input by the ccr stage (i.e., the residual carrier frequency offset at the output of the dsp is low and easily compensated by traditional cpr blocks such as bps)."
"been introduced recently to evaluate performance of iterative decoding schemes in dwdm systems [cit] . in the following, we investigate the performance of s-jidd in dwdm systems with pdm transmissions based on two models for nli proposed in previous works."
"with / k being the laser frequency fluctuation given by (3); laser phase noise and nli effects are modeled by the set fw k g, which is assumed i.i.d white real gaussian random variables with variance"
"our findings point towards a re-conceptualization of what constitutes an error: in the exam approach, there is no such label as correct/incorrect; rather we quantify how well such an apparently erroneous decision can be explained given the concrete sensory input seen by the participant (fig. 2) . this probabilistic way of modelling binary choices 34 in combination with rt histograms may be a better basis for understanding the underlying mechanisms of decision making, than analysing average performance rates using only two classes of correct/incorrect decisions in conjunction with rt histograms reduced to five quantiles as typically done in standard analyses 12, 13 . many experimenters in the perceptual decision making field use the 'random-dot motion (rdm)' task rather than a single-dot tracking task as used here. the rdm task gained popularity after it had been used in perceptual decision making experiments with monkeys 10, 35 . the original motivation was to employ a visual stimulus which is optimal for activating extrastriate areas, especially the motion-sensitive area mt, for neuronal recording 10 . in the present study, this was not the primary concern, but the aim was to design a stimulus that is most certainly perceived by the participant over all time points. this is important because with many dots as in the rdm task it is uncertain which dot movements the participants perceived and have used in their internal decision making. this uncertainty makes predicting behaviour at a single trial level challenging. by using single-dot stimuli, we minimized this uncertainty to showcase a proof of concept that precise modelling of the sensory input is useful for addressing specific questions about the underlying decision making mechanism. in our stimulus, the single dot location captures all variability whereas in standard rdm stimuli variability is distributed across many dots in the display. thus, it is possible that we only observe a strongly predictable effect of the stimulus (tested with the exam), because participants attend more to the particular variability present in the single dot stimulus. this would also suggest that our paradigm is effective in isolating the mechanisms of decision making by reducing the uncertainty effects of lower level sensory processing, i.e., stimulus feature extraction. note, however, that there is evidence that low-level features of rdm stimuli affect choices. for example, it has been found that the transient motion energy content of a rdm stimulus influences choices even in zero coherence trials, especially early in long trials 36, or when only the moment-to-moment fluctuations not associated with the motion strength nor direction is considered 37 . further, bair and koch 38 have shown that the firing of mt neurons is temporally highly consistent across repeated presentations of exactly the same rdm stimulus. because these mt neurons are thought to provide the evidence for decision making in rdm tasks 4, this finding suggests that the particular, dynamically changing features of the rdm stimuli affect perceptual decisions. indeed, when performing an analysis of the responses recorded in an rdm experiment (online database used in 11, http://www.neuralsignal. [cit] .1), we found that responses to rdm stimuli with \"frozen noise\", i.e., with exactly the same sequence of dot movements, were markedly less random than responses to rdm stimuli in which the particular dot movements varied across trials (see fig. s2 ). stimulus details, therefore, also matter in rdm tasks, but it remains to be shown how much, especially, because it is still unclear exactly which spatiotemporal features the brain uses to judge the direction of rdm stimuli. in our study, we aimed to show that these results showing the correlation between noise and behaviour can be modeled and predicted by accounting for the exact sensory input. although we did not collect data with 'frozen noise', we used the exact same stimuli for all 24 participants. in fig. s3-6, the predicted rts pooled over all participants are shown for all difficulty levels. these figures identify those trials (by the trial-specific ppl difference) for which most of the participants chose the incorrect targets. this further supports the notion that details in the input stimuli were inducing similar behavior across participants."
"in conclusion, modelling random spatiotemporal dynamics in the sensory input provides better fits of participant behaviour and enhances the predictive power of perceptual decision making models. further, the increased modelling precision gained through incorporating the spatiotemporal dynamics of input stimuli into the model has allowed us to detect subtle effects of different decision making strategies in the behaviour."
"perceptual decision making is a core aspect in everyday cognition. in difficult perceptual situations, for instance when driving a car through heavy rain and trying to read a traffic sign, perceptual decision making can be error prone. standard models assume that these errors are either due to a high noise level in the sensory input (raindrops on the windshield), or to internal brain processes (neuronal noise) or to a mixture of both [cit] . in the laboratory, perceptual decision making is usually investigated by linking the perception of incoming sensory information to making a choice among several alternatives [cit], often under time pressure. for example, in the widely-used random dot motion task (rdm), participants are required to report the net direction of a cloud of moving dots within a certain time frame 9 . the difficulty of such a decision depends on the amount of noise in the stimuli, i.e. the percentage of coherently moving dots in the cloud of dots 10, 11 . the two current standard approaches to modelling behavioural data of such perceptual decision making experiments are the drift diffusion model (ddm) 12, 13 and the nonlinear attractor model 14, 15 . these models analyse average measures of the behavioural data (e.g. performance expressed as percentage of correct decisions and averages of the reaction times across trials). the implicit assumption of these analyses is that behaviour can be understood by the average effect of sensory noise on decision making performance 12 . what standard models cannot explain however, is how each single decision depends precisely on the specific noise pattern in that particular input stimulus. by only looking at the average performance over many trials, one cannot tell what caused a particular incorrect decision: in the example above, the raindrops might be in one instance covering more informative parts of the traffic sign than in others. following this intuitive notion, we hypothesized that errors in perceptual decisions can be explained better based on the precise pattern of sensory input noise, as compared to using the average noise level. such a finding would mean that one can analyse perceptual decision making data in high detail, and thereby enable more precise inference about the underlying mechanisms of decision making."
"the target i for which the posterior belief reached the bound is the choice of the model. the time t at which the bound was reached first is the decision time δ t which is a discrete variable counting the number of observed dot locations. as described in the main text, the total reaction time of the model is the decision time plus a variable non-decision time t n see, e.g. 12, 50, i.e.,"
input to the decision model. the bayesian response model makes decisions based on observations which provide evidence for or against an alternative. we considered two kinds of observations reflecting different amounts of information about the true stimulus constituting the ddm-equivalent model and the exact input model (exam).
"recently, a variant of abc has been proposed which circumvents the use of summary statistics of the data 26 . the method extends an efficient approximate inference method (expectation propagation: ep) to the likelihood-free setting and is called ep-abc. instead of comparing whole data sets, ep-abc compares sampled against measured data points individually while speeding up likelihood-free inference by orders of magnitude."
"from previous results we can see that powerful carrier phase estimation and decoding techniques shall be needed to combat the effects of both the laser phase noise and the frequency fluctuations. to compensate these impairments, in next section we propose the use of a joint iterative detection and decoding technique. this technique has been proposed in the past to compensate timevarying phase noise and constant frequency offset experienced in satellite communication systems [cit] . so far, its application to optical coherent receivers in the presence of laser frequency fluctuations has not been investigated in the literature."
"we also evaluate the result of our proposed face recognition system and compared with att and indian face database using face recognition evaluator developed by matlab. each of face database consists of 10 sets of people's face. each set of its face database consists of 3 poses (front, left, right ) and varied with illumination. att face database consists of 9 7 differential facial expression and small occlusion (by glass) without variation of illumination . the indian face database consists of eleven pose orientation without variation of illumination and the size of each image is too small than its and att face database. the success rate comparison between 3 face databases shown below: fig. 4 . success rate comparison of face recognition between 3 faces databases, each using 10 sets face. it shown clearly that its database have highest success rate than att and indian face database when the illumination of testing images is varied. the success rate using pca in our proposed method and its face database is 95.5 %, higher than att face database 95.4%."
"to further show the difference between the two models, we calculated point-predictions for matches between model and participant responses (fig. 5) . we calculated the most probable decision for each trial (see material & methods) based on the ppls, and compared the model choice to actual participant choice for both ddm-equivalent model and exam. figure 5 shows that for each of the four difficulty levels, the exam predicted the participants' decisions better than the ddm-equivalent model. similarly, the exam provides for more accurate rt point estimates (fig. s1 ). again, as with the ppl results, the difference between the two models was more prominent in the most difficult conditions. critically, we also found that the exam explained significantly more errors made by participants, as compared to the ddm-equivalent model (table 2 )."
"we expected two key findings. first, the exam will be able to explain more precisely and on a single-trial basis, how human participants make their perceptual decisions, as compared to the ddm-equivalent model. second, this increase in modelling precision can be used to answer more detailed questions about the underlying mechanism of perceptual decision making. to show this, we addressed the question whether participants use a specific mechanism to respond within a specific time window. one standing hypothesis is that participants dynamically lower their internal criterion of how much information is needed to commit to a decision by reducing their decision boundaries 21, 22 . neurophysiological experiments have found evidence that monkeys do employ this so-called collapsing bound model (or, similarly, the urgency signal model) [cit] . however, in humans, there is less clear evidence presumably because the effect of time-pressure on behavioural data is small 24, 25 . we hypothesized that a more precise model, as proposed here, should be able to detect whether human participants are indeed using a collapsing bound when making perceptual decisions."
"based on fig, 10, is the linear velocity, is the angular velocity, and are radial and angular coordinate of the robot [cit] . the kinematics equations of motion for the robot given by :"
"where and are the i eigenspace and the k value of the i eigenvector. then, we can determining which face class provides the best description of an input face images to find the face class k by using the euclidian distance between the new face projection ω, the class projection ω and threshold using formula :"
"we define a loss function l(a, θ) which gives a measure of the loss incurred in taking action a when the state is θ. the robot should chooses an action a from the set a of possible actions based on the observation z of the current state of the path θ. this gives the posterior distribution of θ as:"
"ep-abc cycles through all data points in a data set where in our application each data point corresponds to the recorded behaviour of a single trial (choice and reaction time). after reaching a minimum number of accepted parameter samples for the given data point (set to 300 in our analyses), it updates the posterior distribution over model parameters and moves to the next data point. the updating scheme implements expectation propagation (ep) 52, 53 in which the true, potentially complex posterior distribution is approximated with a product of feasible distributions. in our application of ep-abc each factor of this product represents one data point (trial) and is a gaussian distribution. updates of the posterior adjust the mean and covariance of one gaussian factor such that a hybrid distribution and the full product of gaussian factors become maximally similar. the corresponding hybrid distribution for one data point is obtained by replacing the gaussian factor under consideration with the distribution estimated from the accepted parameter samples. consequently, the main operation in the update of the posterior simply consists of estimating the mean and covariance of the accepted parameter values. in our analysis we used the implementation of this updating scheme provided by the first author of 26 at https://sites.google.com/ site/simonbarthelme/software/abcep_release.zip."
"next we tested our hypothesis that the exam outperforms the ddm-equivalent model in providing evidence for a collapsing bound decision-making mechanism. to do this, we modelled and compared, for both models, three different mechanisms that have been suggested for perceptual decision making: the standard evidence accumulation employed above, the so-called leaky accumulation 30, 31, and the collapsing bound model 21, 22, 32 . (a) protected exceedance probability (ep) (belief of one model being more likely than any other model) between the ddm-equivalent model (yellow bars) and exact input model (green bars). the cut-off probability for the exceedance probability is typically set at 0.95 (red line). (b) posterior model probability (expected probability of obtaining the model when randomly selecting a participant). the red line is the null frequency profile over models, i.e., chance frequency level 28 . error bars are standard deviation over participants. no evidence for the collapsing bound model; the standard evidence accumulation was the best model across all four difficulty levels and there was no evidence for leaky integration either (fig. 6a,b) . in contrast, with the exam we found a different picture: the collapsing bound model was frequent among the participants (fig. 6d) . this was the case for the intermediate difficulty levels. for the easiest and hardest difficulty levels there was strong evidence for the standard accumulation (fig. 6c,d ). these results are congruent with the ppl results, where we found for the exam that the collapsing bound model provides for each condition a closer fit to the behavioural data than the standard accumulation model (fig. 7) . furthermore, for the exam there was no evidence of leaky integration (fig. 6c,d )."
"stimuli were composed of one white moving dot, and two yellow stationary target position dots on a black screen (fig. 1a) . two yellow target position dots were located mirror-symmetrically from the vertical midline of the screen. the distance of the two dots from the centre determined the difficulty of the trial (fig. 1b) . there were four difficulty levels where the two targets' horizontal positions had a distance of 55 (easy), 40, 25 and 10 pixels (hard) symmetrically from the centre of the screen (corresponding visual angles: 3°, 2.2°, 1.4°, 0.6°, respectively). each target position pair (e.g., −10, 10) was presented simultaneously on the screen."
"all had normal or corrected-to-normal vision, and reported no history of neurological or psychiatric diseases. written informed consent was obtained from each participant. the experimental procedure was approved and carried out in accordance with the guidelines by the ethics committee of the university of leipzig. one participant's data were excluded due to the participant not being able to follow the instructions."
"as we shall show in section 5, the generic model given by eq. (1) can also be used to include the nonlinear interference caused by the kerr effect in the fiber [cit] ."
"fig. 2 depicts the simplified model of the pdm coherent optical receiver, where a carrier phase recovery stage is used to demodulate the received signal. cpr is a key function of coherent optical receivers [cit] . in particular, feedforward phase estimation schemes such as the viterbi-viterbi (vv) [cit] or bps [cit] algorithms have been proposed for these receivers, as a result of their good laser linewidth tolerance and feasibility for parallel implementation. we consider two existing carrier phase recovery techniques denoted here as ecpr-1 and ecpr-2."
"the white moving dot was presented for 93.2 ms (7 frames) at a specific location (fig. 1a), and jumped to a new position every 93.2 ms. each position of the white dot was drawn from a gaussian distribution with a mean equal to the target positions (left and right) and a standard deviation of 70 pixels. each difficulty level had 200 trials (100 left + 100 right), resulting in a total of 800 trials per participant."
"is a normalization variable required to avoid numerical overflow when finite precision arithmetic is used. compared to (18), eq. (22) not only reduces the computational complexity of the algorithm, but also provides an improved numerical stability for implementing with finite resolution arithmetic. fig. 8 depicts a possible architecture of the proposed jidd-based receiver. notice that the jidd replaces the following blocks: cpr, sdd, and ldpc decoder. we highlight that the carrier frequency offset is compensated before by the ccr, as explained in fig. 2 . the low latency pll is kept as part of the ffe equalizer adaptation loop. as in the traditional architecture based on a cpr followed by ldpc decoding, each polarization is processed independently after the polarization demultiplexing achieved by the mimo-ffe [cit] . this architecture has shown to provide an excellent performance in real applications. we realize that some authors have proposed to combine information of the two carrier recovery blocks of the two polarizations in order to improve the receiver performance [cit] . although this approach could also be used in the context of the jidd, in this work we focus on the architecture with independent jidds for each polarization, which has shown to achieve a good performance in ultra-high speed coherent receivers [cit] . fig. 9 shows a simplified block diagram of the jidd. notice that existing architectures of ldpc decoders can be used to implement jidd receivers (e.g., see [cit] for more details). development of practical architectures for the symbol node (sn) blocks will be required to implement jidd in commercial optical transceivers."
"we have identified the effect varying illumination to the accuracy of recognition for our database called its face database as shown in table 1 . testing images without and with varying illumination. results shows that by giving enough training images with variation of illumination generated randomly, the success rate of face recognition will be improved."
"camera become important sensor if we want to identify specific object such as face, small object, shape etc) that could not identified by other sensor such as ultrasonic sensors. camera as a vision sensor have limitation in angle area for capturing object. we defined as a maximum angle that moving obstacle can be detected by camera used in this research. based on the fig. 1, we defined angle between moving obstacle and robot as: where and are the position of the obstacle in pixel and s is the scaling factor in cm/pixel. we proposed mechanism for predicting collision using time t needed for robot to collides the moving obstacle that move with orientation as shown in fig, 1 and should be greater than threshold t for robot to allowing moving forward, can be calculated by formula:"
"we found two main differences between parameters fitted by the ddm-equivalent model and the exam ( table 1) : as compared to the ddm-equivalent model, the exam has for all difficulty levels (i) significantly lower noise parameter estimates (~48 pixels lower over conditions) and (ii) significantly shorter non-decision times (~40 ms shorter over conditions) (ndt mode, see material & methods). these differences in parameters indicate that with the exam the variability in the participants' behavioural data was explained by using less neuronal noise and a shorter unspecific decision delay, as compared to the ddm-equivalent model. this means that the exam is able to explain more variability of the data by the actual decision making mechanism, as opposed to unspecific noise and processing delays."
"in this work we investigate the performance of a joint iterative detection and decoding (jidd) algorithm in optical systems based on pilot symbols and powerful fec codes such as low density parity check (ldpc). jidd uses the soft-output information on the coded symbols provided by the decoder and performs forwardbackward recursions, taking into account carrier phase information. although performance evaluation of jidd has been addressed in the past (e.g., see [cit] and references therein), its behavior in high-speed transmissions over optical channels with laser frequency fluctuations and phase noise has not been reported so far."
"analysis of behavioural data. all analyses were performed in matlab ® (version 7 or above, mathworks, ma). the reaction time and response (left, right, timed-out) for each trial were recorded. importantly, since the input stimuli were generated before the experiment, each participant saw the exact same 800 trials of white dot movements but in randomized trial order. therefore, for each participant and trial, the precise visual input was available for subsequent analysis using the bayesian model. the models were fit to the data five times and evidences showed little variation across ep-abc repetitions. the models were fit to the actual response (left, right, timed-out) of the participants rather than the correct/incorrectness of the responses. parameter estimates are shown in table 1 . the runs that gave the best fit to the data (largest model evidences) were used for further analyses."
"given the generative models and observed dot positions the decision model accumulates evidence for a target using bayesian inference. starting from a prior (bias) ( ) p t i, the decision model recursively computes the posterior beliefs ( ) p t x i t 1: that the observed dot positions were generated from (one of) the two targets:"
"since most of the m-qam schemes considered for practical applications have rotational symmetry, errors in the carrier phase estimation may cause cycle slips (cs). after a cs occurs, all detected symbols are erroneous and they cannot be corrected by forward error correction (fec) codes [cit] . to counter this catastrophic effect, differential modulation is typically used [cit] . in this modulation technique, the information is transmitted as the phase difference between two consecutive symbols. therefore, the effects of a cs do not translate into catastrophic bit error bursts. while this option provides a solution to the cs problem, its sensitivity in terms of signal-to-noise ratio (snr) is worse than that achieved by non-differential schemes. for instance, a penalty of 1.2 db has been reported for differential qpsk modulation [cit] . to avoid the penalty of differential modulation formats, the use of pilot symbols has been proposed in previous literature [cit] to prevent error propagation in non-differential modulation."
"camera as vision sensor sometimes have distortion, so bayesian decision theory used to state estimation and determine the optimal response for the robot based on inaccurate sensor data. bayesian decision rule probabilistically estimate a dynamic system state from noisy observations. examples of measurement data include camera images and range scan. if x is a quantity that we would like to infer from y, the probability p(x) will be referred to as prior probability distribution. the bayesian update formula is applied to determine the new posterior p(x, y) whenever a new observation is obtained:"
in both models we added noise to the mean (stimulus driven) observations x t such that the actual observations which were input to the decision model were
"we found that the exam with a collapsing bound better predicted participant behaviour than the other two model variants (fig. 7) . in addition, we found trends towards a collapsing bound model using bayesian model selection for the medium difficulty levels (fig. 6c,d ). we only found evidence for a collapsing bound with the exam, when the exact sensory input entered the analysis."
"experimental setup. the experiment consisted of three phases: 440 training trials with feedback followed by 40 rehearsal trials (without feedback) and finally the main phase with 800 trials (200 trials x 4 blocks, with breaks). in order to promote attentiveness and motivation, participants received a bonus for good performance (fast and accurate)."
"due to the dispersion of ut links, the properties of the signal propagation are altered drastically with respect to dmt. it has been shown that the propagated signal appears to take on statisticallyindependent zero-mean gaussian distribution [cit] . this result has also been extended for the signal after the dsp block used at the receiver to compensate, e.g., cd and pmd of the ut link. 8 consequently, the nonlinear interference is modeled as awgn. this is called the gaussian noise (gn) model [cit] . the sample at the input of the cpr also reduces to 7 that is, a dwdm system with 100 gb/s pdm-qpsk channels spaced at 50 ghz [cit] ! dm þ dm nl $ 10 mhz. 8 dsp functions in the receiver include linear equalization only of the fiber dispersion (i.e., nonlinear compensation algorithms such as back-propagation are not considered in this work)."
"we have developed a vision-based service robot called srikandi iii with the ability to face recognition and avoid people as moving obstacles, this wheeled robot is next generation from srikandi ii (budiharto, w. 2010) . a mobile robot involving two actuator wheels is considered as a system subject to nonholonomic constraints. consider an autonomous wheeled mobile robot and position in the cartesian frame of coordinates shown in fig. 10, where and are the two coordinates of the origin p of the moving frame and is the robot orientation angle with respect to the positive x-axis. the rotation angle of the right and left wheel denoted as and and radius of the wheel by r thus the configuration of the mobile robot c r can be described by five generalized coordinates such as :"
"we have developed a system for face detection using haar cascade classifier and depth estimation for measuring distance of peoples as moving obstacles using stereo vision. the camera used is a minoru 3d stereo camera. the reason for using stereo camera in order robot able to estimate distance to obstacle without additional sensor(only 1 ultrasonic sensor infront of robot for emergency), so the cost for development can be reduced. let's start froma basic conce where a point q captured by camera, the point in the front image frame i p( i p x, i p y ) is the projection of the point in camera frame c p( c p x, c p y, c p z ) onto the front image frame. here, f denotes the focal length of the lens. fig. 6 shown is the projection of a point on the front image frame. in the stereo imaging model, the tree-dimensional points in stereo camera frame are projected in the left and the right image frame. on the contrary, using the projection of the points onto the left and right image frame, the three-dimensional points positions in stereo camera frame can be located. fig. 7 shows the stereo imaging model using the left front image frame lf and right front image frame rf (purwanto, d., 2001) . by using stereo vision, we can obtain the position of each moving obstacle in the images, then we can calculate and estimate the distance of the moving obstacle. the threedimensional point in stereo camera frame can be reconstructed using the two-dimensional projection of point in left front image frame and in right front image frame using formula:"
"the figure below shows the proposed model of maneuvering on the service robot, pl which is the probability of moving obstacle leads to the left, and pr the probability of moving obstacle leads to the right. by estimating the direction of motion of the obstacle, then the most appropriate action to avoid to the right / left side can be determined, to minimize collisions with these obstacles. if there are more than 1 moving obstacle, then robot should identified the nearest moving obstacle to avoid it, and the direction of maneuver should be opposite with the direction of moving obstacle. result of simulation using improved face recognition system and implemented to a service robot to identify a customer shown in figure 14 . in this scheme, robot will track the face of a customer until the robot heading exactly to a customer, after that, robot will run to customer. if there are moving obstacles, robot will maneuver to avoid the obstacle."
"i j indicates the true target i chosen in trial j). this is equivalent to how input is treated by previous perceptual decision making models, such as the drift diffusion model."
"with increasing number of data points the posterior approximated by ep-abc converges. to ensure convergence we made four passes through all data points of a data set. additional to the posterior distribution over model parameters, ep-abc also provides an estimate of the model evidence for the used model. the model evidence is returned as the log-marginal likelihood of the model given the whole data set."
"the service robot should navigates from start to goal position and go back to home savely. we assumed the when robot running, people as moving obstacle may collides with the robot. so we proposed a method for obstacles avoidance for service robot in general as shown in fig. 12 . the model of experiment for customer identification is using stereo camera, the advantage is we can estimate the distance of customer/obstacles and direction's movement of obstacles. there is no map or line tracking to direct a robot to an identified customer. image captured by stereo camera used as testing images to be processed by haar classifier to detect how many people in the images, and face recognition by pca. we fig. 11 . prototype of service robot srikandi iii using stereo camera implementing visual tracking to heading a robot to a customer. robot continuously measures the distance of obstacle and send the data to laptop. the next step is multiple moving obstacle detection and tracking. if there is no moving obstacle, robot run from start to goal position in normal speed. if moving obstacle appeared and collision will occurred, robot will maneuver to avoids obstacle. figure shown below is a flowchart that describes general mechanism for our method for detecting multiple moving obstacle and maneuver to avoids collision with the obstacles. to implement the flowchart above for service robot that should recognize the customer and have the ability for multiple moving obstacle avoidance, we have developed algorithm and programs consist of 3 main modules such as a framework for face recognition system, multiple moving obstacle detection and kalman filtering as state estimator for distance measurement using stereo camera."
"in the present study, we have shown that modelling the detailed sensory input observed by the participants enables predicting response time and choice at the single trial level for a simple visual perceptual decision making task. we found that with the detailed sensory input model, a collapsing bound and its shape can be identified, providing a deeper understanding of the underlying decision making mechanism in humans. the single trial analysis may be especially useful when applied to neural measurements, because the model predicts trial-specific variations of internal, decision-related variables. this can be used for investigating the representations of these variables in the brain 33 . when we compared the exact input model (exam) to the ddm-equivalent model, there were three main findings: firstly, we found that the behavioural data, both choices and reaction times were better explained by the exam, both at the individual, single-trial level and on average. secondly, the fitted noise parameter and non-decision time were significantly smaller for the exam compared to the ddm-equivalent model. this indicates that the ddm-equivalent model explained more of the behavioural data by unspecific noise processes, as opposed to the exam, which attributed more variability of the behavioural data to the actual decision making mechanism. thirdly, the exam explained significantly more errors made by the participants than the ddm-equivalent model."
"before a trial started, a fixation cross was shown (duration: 300~500 ms). next, two yellow target dot positions appeared for 700 ms, in addition to the fixation cross. then the fixation cross disappeared (the yellow target position dots remained) and the moving white dot appeared (fig. 1a) . the task was to figure out around which target the white dot was moving. the stimulus was displayed until the participant reported the decision by pressing one of two buttons (left or right) on a 2-key button box, but maximally for 25 jumps of the white dot (≈ 2330 ms). if the participant failed to respond during this period, the trial timed-out and the next trial started."
"tell a story through several stages in which the main character is a foreigner visiting mexico city with whom students can identify and feel empathy towards. the inciting element of the story is the search for the coatlicue, an aztec monolith, in different locations of mexico city. the story must include elements of mystery and plots to maintain the interest of the user and incite him/her to pursue. a set of virtual resources, which young people are familiar with, will be offered for transmitting the information, such as a mobile phone to communicate with family and experts, a camera, a gps to move around the city, a music player, interactive books, and a facebook link to communicate with other students. the credibility of the information is established by the creation of a realistic context and characters."
"thus, using kernel functions, the feature space does not need to be computed explicitly, only inner products in the kernel feature space are taken into account. gaussian radial basis function, polynomial, sigmoidal, and inverse multiquadrics function are used in a role of kernel functions. every linear algorithm that uses scalar products only can implicitly be executed in high-dimensional feature space by using kernels. nonlinear versions of linear algorithms can be constructed in this way [cit] ).  is the standard deviation), the relationships between the vectors are then defined as:"
"this study proposes a methodology for miis specification based on: 1) a communicational analysis that allows the definition of a creative concept of the system to be built; 2) a metamodel of miis to be used for designing the system; 3) new characteristics that enrich the model of quality in use iso/iec 25010, in order to analyze miis communicational aspects; and 4) a prototyping-centered methodology for designing a miis. this paper is organized as followed: section 2 summarizes existing methodologies which served as background of our work; section 3 presents multimedia interactive informative systems (miis) as a metamodel; section 4 defines the expected qualities for miis; the proposed methodology is developed in section 5 and a case study is detailed in section 6; section 7 is the conclusion."
"one of the most mature of these works is by friedmann, who proposes a process of analysis and scriptwriting for different means of communication, called visual media, which include traditional media but also websites, educational and training software applications, information kiosks, and videogames [cit] . friedmann establishes an analytical and creative process whose goal is to yield a creative concept of visual media on the basis of which the script is written."
"in the national university center for foreigners in mex-ico, students must follow classes on spanish and mexican culture. if the students are captive in the study of the language, they are not necessarily captive in the study of mexican culture since their priority is generally to learn spanish and not in learning mexican culture. besides, teachers on cultural issues are not always familiar with teaching foreigners and are having problem in communicating with them."
"an original learning algorithm from original space is used in the feature space. highdimensional space increases complexity of a problem; fortunately, it can be solved. computation of a scalar product between two feature space vectors can be done using kernel function k"
this study will use friedmann's framework to analyze the domain of miis' problem and to propose a creative concept to be translated into specifications of system.
"in this paper, the modeling of the quality of miiss is proposed, as a necessary step towards a systematic design of these systems involving the knowledge of the expert on the domain. the main goal of this work is to present the miis quality modeling by adapting the iso/iec 25010 quality model to specify the communicative quality of miiss. the results can be used to improve the requirements elicitation discipline in the development process of miis."
"each image capturing generates digital or analog noise of diverse intensity. the noise is also generated while transmitting and copying analog images. noise generation is a natural property for image scanning systems. diverse types of noises exist. herein we use three different types: gaussian [cit], salt & pepper [cit], and speckle [cit] noises. fig. 9 . graph of maximum recognition rates for methods pca+mahcosine, pca+svm, lda+ldasoft, lda+svm, svm (left to right) with \"bigface\" preprocessing"
"the second line comes from the design and development of conventional software. it focuses on requirement engineering and on the resolution of problems linked to the handling of multiple media, but leaves aside the communicational problem."
"the above observations explain why it is necessary to explore new methodologies aiming to establish the specifications of a miis, and to define new quality requirements more appropriate to evaluate the quality of the transmission of knowledge to non-captive users."
"the definition of target audience tries to sharpen a public, even if it is potentially wide, diverse and dispersed. there is always a target public which allows focusing on a specific age, gender, cultural level, nationality, socioeconomic level, etc."
"one possible solution can be based exactly on results of machine face recognition. this approach is illustrated in fig. 22 and in corresponding table 4 . fig. 22 shows images of the subject corrupted by different types of noises. the noise parameters are chosen in such manner that all studied methods (pca-mahcosine, pca-svm, lda-ldasoft, lda-svm) result in recognition accuracy near 85 %. table 4 specifies each noise type and its corresponding parameter. pca-mahcosine and lda-svm methods are included in fig. 22, since pca-svm and lda-ldasoft methods are visually similar to lda-svm. fig. 22 table 4 . types and intensity of noise resulting in recognition rate about 85 % (for training set 4img./subj.). * included in fig. 22"
"in the second step, each window is modeled as an instantiation of miis metamodel, deciding which media compose his scene and content. this model can be represented through a table instead of diagram. in table 3, first location (zócalo) is represented as an example. zócalo location is composed, as all windows, by scene and content, which are composed by different media. this model allows graphic designer to propose a first sketch (figure 2) . in order to apply a graphical style to the sketch, non functional requirements specifications are needed."
"having used it in all along our real case study, we have experienced using the miis' metamodel, very useful to describe the system and analyze its quality looking at each of the windows instances which are all composed of content and scene. communicability attributes enable us to characterize exactly our required quality in use properties and then allow use to test that the system actually meets users' expectations. this work opens new possibilities in taking communication aspects into consideration while designing miis. in particular, prototype evaluation used in this work was useful in order to decide if the windows and media defined in the system took actually correctly in charge each communicability attributes and suggesting the way to improve them. this type of evaluations could be performed many times during the iterative development process, assessing how each specific window was responding to communicability attributes, and this, before the end of the process."
"4. the recognition rate is most significantly affected by setting of the co parameter -for pca+mahcosine and lda+ldasoft it is better to truncate vectors from the end of the transform matrix leaving only 20% -60% of the vectors. methods pca+svm and lda+svm perform better when leaving more (60% -100%) vectors of the transform matrix. 5. results of lda+ldasoft are more influenced by setting the co parameter compared to pca+mahcosine -especially with only 2 images per subject in the training set, where the worst recognition rate is around 30% (see table 2 and table 3 ). 6. using svm for classification (methods pca+svm and lda+svm) makes the recognition rates more stable and less influenced by setting the co and dpf parameters (see table 2 and table 3 ) and these methods perform better compared to simple pca+mahcosine and lda+ldasoft -see fig. 8 and fig. 9 . fig. 8 . graph of maximum recognition rates for methods pca+mahcosine, pca+svm, lda+ldasoft, lda+svm, svm (left to right) with \"face\" preprocessing"
"the strategy selected is the use of narrative. telling a story has the virtue of motivating the user to know what is going to happen next, and therefore maintains his or her interest in the application [cit] ."
"diary window is composed by images and texts where pierre explains his quest. surprisingly for us, time spent by students was over our expectation, mostly because texts were very simple and allowed students to understand quickly all information about story. photos window did not interest students: they were too small and subjects were not original. we decided to enlarge them and use more original images."
"salt & pepper noise is perceived as a random occurrence of black and white pixels in a digital image. it can be caused by incorrect data transmission or by a damage of already received data. in ccd and cmos sensors or lcd displays, the salt & pepper noise can be caused by permanently turned-on or turned-off pixels. remaining pixels are unchanged. usually, the intensity (frequency of the occurrence) of this noise is quantified as a percentage of incorrect pixels [cit] . the median filtering (as a specific case of order-statistic filtering) is used as an effective method for elimination of salt & pepper noise from digital images [cit] . noise parameter settings for our simulations vary from 4% of noise intensity (0.04) up to the 30% of damaged pixels. the label sp0.04 means, that the salt & pepper noise of intensity 4% was applied on the image. examples of images corrupted by salt & pepper noise are shown in fig. 11 ."
"the analysis of those elements should lead to define a strategy: on friedmann's opinion [cit], \"to write a successful script that solve the communication problem, we need to figure out how to achieve the objective, reach the target audience, and suggest the content that leads to effective communication\". this strategy could be one, already used in other successful systems with similar problem domain, or can be an original one."
"in order to traduce the concept into specifications, it is necessary to decide in first place, which windows would assume communicational attributes. in figure 3, those attributes are associated to each window. as an example, in table 4, we established specifications requirements for window zócalo, the first location pierre visits."
"methods utilizing pca or lda (fig. 6b -fig. 6e ) were tested using three training sets with 2, 3 and 4 images per subject. for each method, we tested 25 different parameters dpf and co setups on three different training sets, what gives total 75 tests per each method and per each type of preprocessing (600 tests in total). results of these tests are shown in table 2 and table 3 . the maximal recognition rates are summarized in fig. 8 and fig. 9 ."
"in this chapter, we consider biometric recognition based on human face. biometrics became frequently used in automated systems for identification of people [cit] . along with well-known methods such as fingerprint or dna recognition, face image already opened new possibilities. face recognition has been put into real life by many companies. it is already implemented in image organizing software (e.g. google's picasa: http://www.deondesigns.ca/blog/picasa-3-5-adds-face-recognition/), web applications (e.g. web photo albums http://picasa.google.com/intl/en_us/features-nametags.html) and even in commercial compact cameras (e.g. panasonic lumix). [cit] . in the area of face recognition, a class represents all images of the same subject (person). the goal is to implement an automated machine supported system that recognizes well the identity of a person in the images that were not used in a training phase (an initialization and training by representative sample of images precede an evaluation phase). various applications are possible, e.g. automated person identification, recognition of race, gender, emotion, age etc. the area of face recognition is well described at present, e.g. starting by conventional approaches (pca, lda) [cit], and continuing at present by kernel methods [cit] . advances in face recognition are summarized also in books [cit] and book chapters [cit] . our aim is to present complex view to biometric face recognition including methodology, settings of parameters of selected methods (both conventional and kernel methods), detailed recognition results, comparison and discussion of obtained results using large face database. the rest of this chapter is organized as follows: in section 2, we present theoretical background of methods used for face recognition purposes -pca (principal component"
"in table 4, we synthesized the prototype that we built, composed by windows and media that should allow carrying on the evaluation. criteria of evaluation proposed in table 1, are then associated to each window and applied to questionnaires in order to evaluate communicability attributes ( table 5) ."
"the quality of a system is defined as the degree to which the system satisfies the stated and implied needs of its various stakeholders, and thus provides value [cit] . quality models represent these needs; they categorize the product quality into characteristics, which can be further subdivided into sub-characteristics; these can be refined until the quality attributes or measurable elements are obtained. the iso/iec 25010 standard offers different quality models for the software product; in this context, we are interested in quality in use model."
"traditionally, the specifications of a computer system are guided by functional requirements expressed by clients and final users. in the miis' case, where the objective is to transmit information, the users cannot easily decide on the content of the application and therefore there is a need of intervention from experts for deciding what information should be contained in the system [cit] ."
"we examined different scenarios of face recognition experiments. they contain both singlestage and two-stage recognition systems. single-stage face recognition uses svm for classification directly. two-stage recognition systems include pca with mahcosine metric, lda with ldasoft metric and also methods utilizing both pca and lda for feature extraction followed by svm for classification. all methods are significantly influenced by different settings of parameters that are related to the algorithm used (i.e. pca, lda or svm). this is the reason we presented serious analysis and proposal of parameter settings for the best performance of discussed methods. for methods working in ideal conditions, the conclusions are as follows: when comparing non-svm based methods, higher maximum recognition rate is generally achieved by method lda+ldasoft compared to pca+mahcosine; on the other hand lda+ldasoft is more sensitive to method settings. using svm in classification stage (pca+svm and lda+svm) produced better maximum recognition rate than standard pca and lda methods. experiments with single-stage svm show that this method is very efficient for face recognition even without previous feature extraction. with 4 images per subject in training set, we reached 96.7% recognition rate. the experiments were made with complex image set selected from feret database containing 665 images. such number of face images entitles us to speak about general behavior of presented methods. altogether more than 600 tests were made and maximum recognition rates near 100% were achieved. it is important to mention that the experiments were made with \"closed\" image set, so we did not have to deal with issues like detecting people who are not in the training set. on the other hand, we worked with real-world face images; our database contains images of the same subjects that often differ in face expressions (smiling, bored, …), with different hairstyles, with or without beard, or wearing glasses and that were taken in different session after longer time period (i.e. we did not work with identity card-like images)."
"therefore, the main target is foreign students at national university and their teachers. since the system will be on line, it can be used by everybody interested in learning spanish and mexican culture."
"window metaclass is an abstract class composed by scene and content metaclasses. both of them are composed by several media, represented by the media metaclass. this metaclass is associated with interac-tion saying that media are to be interactive. finally, the metaclass data contains users and system data."
"traditionally, requirements of a computer system are established based on an analysis of the needs expressed by clients and/or final users. in the case of miis, where the objective is to transmit information, the clients, for example academics, teachers or art institute managers, cannot easily decide on the content of the application and its treatment. this task therefore requires the participation of experts in communication and information content, since they must decide what information should be contained in the system [cit] ."
"the second step, defines scene and content for each window: scenes represent all the audiovisual components and content contain the information to be transmitted. in order to insure that quality characteristics are considered by the miis model, it is necessary to decide which component will take in charge each one of them."
"attractiveness questions are used to assess student's interest to go forward and imagine the rest of the story. if the story is too predictable, there is less interest to go forward."
"in order to carry on the evaluation, teachers used the system in their classroom and after, the students were asked to freely use the system, as they were home. all the session was recorded through \"camtasia\" and, then students responded to the questionnaire. they were asked for each window to give a mark between 1 and 5 (5 is the highest) and a comment, and the system recorded the time spent on each of them. table 6 shows the results of these questionnaires."
"another work which considers new types of requirements is offered by davide callele [cit], who studies video games in particular and has introduced the concept of emotional requirements which analyze and document the emotions to be transmitted to the users/players. on the basis of these emotional requirements, he establishes functional and non-functional requirements. the concepts presented in these works will allow us to analyze the requirements of miis through qualitative requirements such as attractiveness, interestingness, informativeness and credibility."
"in general, the works coming from the field of communication studies are prepared as scholarly manuals and they base their conceptual and development methodology on other media, such as cinema or television."
"where i is the original human face image and n is the uniform distribution of the noise with zero mean value and variance 2  . for our simulations, variance varied from 0.03 to 0.7. the label s0.03 means that the speckle noise of variance 0.03 was applied on the image. presence of speckle noise in the face image is illustrated in fig. 12 ."
gaussian noise is the most common noise occurring in everyday life. the gaussian noise can be detected in free radio waves or in television receivers. gaussian noise is produced in analog images that are stored for a long time. we studied face recognition with different gaussian noise intensity. gaussian noise was generated with gaussian normal distribution function which can be written as:
"the first line derives from the field of communication, where the authors concentrate on the creation of a new communication medium. these works are based on a development process strongly influenced by film and video making."
"among these studies, a combination of production inspired by movie realization and software engineering is used in video game industry [cit] . the development process of a video game is composed by two phases: preproduction and production. the first one results in a game design document (gdd), which is a creative work written by game designers. the second one is a traditional development process. transition between both phases is particularly difficult since it is necessary to translate the gdd into a specification: \"despite the recognized need, we have discovered no evidence that a process for managing the transition from preproduction to production has been proposed (recognizing that such a process may exist within an organization but remain unreported in the literature) [cit] .\""
"questionnaires allow to measure subjective attributes as \"attractiveness, interestingness, informativeness\" or credibility through qualitative criteria. some quantitative criteria may be defined. they include: time spent by users, number of windows visited and number of media displayed or information retained by users."
"for simulation of methods in presence of noise, we use the best parameter settings we obtained running 600 tests in section 5, i.e. when the methods worked in ideal conditions. in order to mimic real-world conditions, we use images not distorted by noise for training purposes whilst noisy images are used for testing. such scenario simulates real-world face recognition conditions. we concentrate on \"bigface\" preprocessed images only, since this preprocessing gives better results compared to \"face\" preprocessing (this can be seen when comparing tables 2 and 3 ). parameters for settings of the algorithms (co and dpf) were empirically obtained from table 3 . we selected and used only those parameters for which the recognition experiments were most successful (they are marked by red in table 3 ). this was necessary in order to reduce the number of experiments. using all possible settings from simulations in ideal conditions and combining them with three types of noises with all selected parameters would lead to total 13500 results. selecting best parameters only lead us to total 540 results. obtained results are shown in fig. 13 -21 along with brief comments."
"in order to model pierre y la coatlicue miis, the first step consists in selecting the different windows of application: registration, different locations visited by pierre, mobile phone composed by calls, camera, gps and music player, spanish and history books and a field diary."
"this granular noise occurs in ultrasound, radar and x-ray images and images obtained from the magnetic resonance [cit] . the multiplicative signal dependent noise is generated by constructive and destructive interference of detected signals. the wave interference is a reason of multiplicative noise occurrence in the scanned image. the speckle noise is image dependent. therefore it is very hard (if possible) to find a mathematical model that describes the removal of this noise, especially if we expect the randomness of the input data [cit] ."
"based on this definition, we propose a metamodel to describe a miis. the metamodel is composed by four main metaclasses, window, content, scene and data (figure 1) ."
"calls window is composed by videos which represent dialogues between pierre and other characters of the story. time spent on playing videos was under our expectation, mostly because they were too long and not enough interesting. basic information was clearly understood, meaning that the mode of transmission was right."
"the objective is closely associated to the communication problem: it states what problem the miis will help. generally, the objective is easy to see. the difficulty is how to reach it."
"zócalo window is composed by the introduction animation and the main scene with pierre. attractiveness evaluation showed that students were interested about story. however, the ending was too easily predictable: they all imagine the same continuation and ending. we proposed then to complicate the story with sub-stories for maintaining student's interest. pierre character was not credible enough, so we presented to the public hem many other proposals. one of them got 85% of acceptation."
"the definition of communication problem tries to focus on what is the need to produce a miis: \"our axiom states that every program is a response to a communication problem. if there were no need to show, tell, explain, attract, entertain, seduce, delight, or distract an audience, there would be no reason to make a program and, therefore, no need to write a script\" [cit] ."
"there are a variety of techniques to elicit requirements [cit], which generally involve the system's end-users and stakeholders. but there are very few studies that consider requirements not elicited by users or stakeholders, as needed in the case of miis."
"multimedia interactive informative systems (miis) are software systems that result from the convergence of multiple technologies, for example the combination of computational, audiovisual and communication technologies. the purpose of miis is to diffuse information to a large, dispersed, diverse, and non-captive public. an example of a miis would be a website whose goal is to transmit information, such as an online newspaper or a cultural website. a kiosk in a museum, cultural cd-roms, and many applications for mobile phones and tablets can also be considered as miis."
"existing models of quality barely consider these characteristics. software applications traditionally help captive users to solve concrete problems, and there is no need for retaining their interest. the miis's user, on the other hand, is generally non-captive since its acquisition of information is a voluntary decision and not bound by the necessity to use the system."
"we use different methods in our single-stage and two-stage face recognition systems: pca (principal component analysis), lda (linear discriminant analysis) and svm (support vector machines). the role of pca and lda falls into feature extraction. we use different classifiers that are in the form of both simple metrics and more complex svms."
among the models of quality that are most often used to evaluate software is the standard iso 25010 which substitutes the iso 9126 standard for the models of quality of software [cit] and iso 25012 [cit] for the quality of data.
"the purpose of the creative concept is to \"solve the communication problem, reach the target audience, achieve the objective, embody the strategy, provide the content of the application, and show how it will work\" [cit] . it also should respond to the quality characteristics stated in the extended quality model, specifying how the system will be attractive, interesting, informative and credible to end users."
"it is then necessary to decide which window will assume one or more attributes and specify how media assume the corresponding attribute, establishing specifications for each of them. in this way, it is possible to trace problems through evaluations during the process of production. as a result of choosing a strategy, it is applied to a specific problem domain, and inventing a creative concept."
"this concept specifies the manner in which the system will respond to questions such as: how to attract and maintain interest, and how to inform and involve the user."
"gps window is composed by the image representing city map. time spent by students was over our expectation, mostly because they tried to click over all places. however, map was not interactive so it seemed necessary to make it interactive, allowing students to visit virtually the city."
"in this paper, we introduced a methodology for the design of a miis, taking into account its communicational aspects. in order to guide the analyst during the specification of the system, we defined a miis metamodel that can be used as a domain specific language (dsl). to be able to precisely characterize the miis, we complemented the iso/iec 25010 model of quality in use with four new quality sub-characteristics of communicability: attractiveness, interestingness, informativeness and credibility. in order to elicit the requirements of a miis, without the benefit of the users' and stakeholders' input, we suggested the realization of a communicational analysis which derives into the creative concept of miis. to deduce the system specification from the creative concept, this has been expressed as a model, instance of the proposed miis metamodel and then analyzed it by means of our communication attributes. we have shown with a case study that it is possible to establish the system's specifications quality by analyzing each instance of the class window of the model through its quality attributes."
"in order to illustrate the above, we will apply the methodology to a case study: \"pierre y la coatlicue\", a learning system for spanish language and mexican culture. the first step is then to carry out a communicational analysis, as suggested by friedmann [cit] ."
"as with other communication media, such as cinema and television, whether or not the miis fulfill its goals depends largely on quality characteristics related to the communication between the transmitter and the receiver. in this case, the qualities of communication between the miis and its final users are essential, and the success of the miis depends on the user finding these systems attractive, interesting, informative, and credible. however, these quality requirements are not included among the attributes of quality models used for software such as the iso/iec norm 25010 [cit] and its predecessor iso/iec 9126."
"characteristic the evaluation of the characteristic of communication of a miis is generally used to assess the impact of its attributes on the user before a complete (and expensive) development of the application. typically, this evaluation is done through a prototype which includes the most important windows responsible of assuming the communicational attributes."
"the formalization of a methodology to design a miis is becoming more and more indispensable since it offers a multidisciplinary set of communication tools for the use of professionals from very different fields, in one hand, and allows the effective drafting of documents guiding the developers in the other."
"in the field of computing, there are few studies which consider the transmission of information as the main purpose of the system. among the most original studies are those from bolchini [cit], who proposed the term \"infosuasive\" to define web applications that are both informative and persuasive, such as those applications that \"aim at supporting knowledge needs and have also the (declared or not declared) goal of influencing user's opinions, attitudes and behaviors.\" bolchini based his work on the objective-centered design cooper's methodology [cit], centering his analysis on the need to meet the objectives of different users and introducing communication objectives as elements of analysis for the design of web applications. one of his contributions was to identify the need to carry out and to document a communication analysis to build website requirements, such as content, layout, architecture, and navigation."
"the content, as part of the communication analysis, should include the main themes to be treated. their investigation and documentation is part of the following process."
"communicational metrics measure the extent to which users feel if the product is attractive, interesting, and credible and transmit information efficiently. metrics are used to evaluate each attribute. a balanced project should meet the four attributes in the same proportion. in table 2, we define some metrics that will be used in the case study. to achieve the required quality of communication, the components of the system must be designed with that intention in mind. in other words, they must be the product of a requirements analysis that includes the application of communicational purpose."
"as mentioned before, we will follow the development process used in video game industry, which has two different phases: preproduction and production. the first one delivers miis' concept that has to be transformed into specification which therefore allows, a multidisciplinary team to develop it. the second is a traditional software process, generally iterative and incremental, resulting in a miis. in this work, we are interested in the transition from preproduction to production."
"this standard statistical method can be used for feature extraction. principal component analysis pca [cit] . in pca, the projection opt w maximizes the determinant of the total scatter matrix of the projected samples"
"1. more images in the training stage cause better performance of all methods. 2. lda+ldasoft performs better than pca+mahcosine, but pca+svm is slightly better than lda+svm. 3. the best performing setups of parameters co and dpf differ using different preprocessing and number of images per subject in training set. generally pca+mahcosine and lda+ldasoft perform better for truncating 0-4 first vectors and leaving 20%-60% of the vectors in transform matrix."
"these types of non-functional requirements are difficult to translate into specifications. but if they are not satisfied, the user can discard the application with the same freedom that she or he can change the channel of the television or leave a cinema when the film is disappointing, rendering the system essentially useless."
"respectively, where i n is the number of samples in class i x and i  is the mean image of class i x . the transform matrix opt w maximizes the ratio of the determinant of the betweenclass scatter matrix of the projected samples to the determinant of the within class scatter matrix of the projected samples:"
miis' concept is the product of a communicational analysis and a creative process. this creative concept should be translated or specified in terms of an instance of the miis metamodel.
5.1 single-stage recognition svm was directly used for recognizing faces without previous feature extraction from the images (see fig. 6a) ). input images were of size 65x75 pixels.
"nonetheless, even in the case of interactive systems, the screenplay is written in a style strongly influenced by the cinematographic industry, given that: \"books, movies, television, theatre-all imply the creation of specific documents that establish formats that the interactive industry does not have\" [cit] . in particular, friedmann does not specify how the script can be converted to the specifications of a system that must be understood, interpreted, and developed by a multidisciplinary team of designers, communication specialists, and programmers, among others."
"creative concept should be translated in terms of media and interactive media. the role of miis metamodel, just as dsl (domain specific language), is to guide this process:"
"in this work, we are interested about the second group, which consists of test methods used with end-users to obtain feedback on whether the system meets the predefined quality attributes or not. the most common form of user testing is the use of checklists or questionnaires."
the area of iterative compilation contains many projects that use different machine learning techniques to optimize lower level compiler optimizations [cit] . these projects change both the order that compiler passes are applied and the types of passes that are applied.
"the configuration manipulator provides a layer of abstraction between the search techniques and the raw configuration structure. it is primarily responsible for managing a list of parameter objects, each of which can be used by search techniques to read and write parts of the underlying configuration."
"benchmark possible configurations we validated opentuner by using it to implemented autotuners for seven distinct projects. this section describes these seven projects, the autotuners we implemented, and presents results comparing to prior practices in each project. figure 5 lists, for each benchmark, the number of distinct configurations that can be generated by opentuner. this measure is not perfect because some configurations may be semantically equivalent and the search space depends on the representation chosen in opentuner. it does, however, provide a sense of the relative size of each search space, which is useful as a first approximation of tuning difficulty. for many of these benchmarks, the size of the search space makes exhaustive search intractable. thus, we compare to the best performance obtained by the benchmark authors when optimality is impossible to quantify."
"while implementing these seven autotuners in opentuner, the biggest lesson we learned reinforced a core message of this paper of the need for domain-specific representations and domain-specific search techniques in autotuning. as an example, the initial version of the petabricks autotuner we implemented just used a point in high dimensional space as the configuration representation. this generic mapping of the search space did not work at all. it produced final configurations an order of magnitude slower than the results presented from our autotuner that uses selector parameter types. similarly, halide's search space strongly motivates domain specific techniques that make large coordinated jumps, for example, swapping scheduling operators on x and y across all functions in the program. we were able to add domain-specific representations and techniques to opentuner at a fraction of the time and effort of building a fully custom system for that project. opentuner was able to seamlessly integrate the techniques with its ensemble approach."
"for example, the best hand-tuned schedule (against which we compare our autotuner) for a two-stage separable blur pipeline is written as follows: blur_y.split(y, y, yi, 8) .parallel(y) .vectorize(x, 8); blur_x.store_at(blur_y, y)"
"the method run (line 40) implements the measurement function for configurations. first, the configuration is realized as a specific command line to g++. next, this g++ command line is run to produce an executable, tmp.bin, which is then run using call_program. call_program is a convenience function which runs and measures the execution time of the given program. finally, a result is constructed and returned, which is a database record type containing many other optional fields such as time, accuracy, and energy. by default opentuner minimizes the time field, but this can be customized."
"we obtained the raw execution data, courtesy of the authors, and use opentuner instead of exhaustive search on the data from a nehalem-class 2.66 ghz intel xeon machine, running linux 2.6. we compare against the optimal performance obtained by the original autotuning system through exhaustive search. the search space for this problem involves searching for parameters for the parallel decomposition, cache and thread blocking, and loop unrolling for each kernel; to limit the impact of control flow and cache misalignment, these parameters depend on one another (for example, the loop unrolling will be a small integer divisor of the thread blocking). we encode these parameters as poweroftwoparameters but ensure that invalid combinations are discarded. figure 11 shows the results of using opentuner for the laplacian and divergence kernel benchmarks, showing the median performance obtained over 30 trials as a function of the number of tests (results for the gradient kernel are similar, so we omit them for brevity). opentuner is able to obtain peak performance on laplacian after less than 35 tests of candidate implementations and 25 implementations for divergence; thus, using opentuner, less than 2% of the search space needs to be explored to reach optimal performance. these results show that even for problems where exhaustive search is tractable (though it may take days), opentuner can drastically improve convergence to the optimal performance with little programmer effort."
"the built in parameter types boolean, switch, and enum could theoretically also be represented as primitive parameters, since they each can be translated directly to a small integer representation. however, in the context of search techniques they make more sense as complex parameters. this is because, for primitive parameters, search techniques will attempt to follow gradients. these parameter types are unordered collections of values for which no gradients exist. thus, the complex parameter abstraction is a more efficient representation to search over."
"the area under the curve credit assignment mechanism, based on [cit], draws a curve by looking at the history for a specific technique and looking only at if a technique yielded a new global best or not. if the technique yielded a new global best, a upward line is drawn, otherwise a flat line is drawn. the area under this curve (scaled to a maximum value of 1) is the total credit attributed to the technique. this credit assignment mechanism can be described more precisely by the formula:"
"the third challenge is the landscape of the configuration space. if the configuration space is a monotonic function, a search technique biased towards this type of search space (such as a hill climber) will be able to find the optimal configuration. if the search space is discontinuous and haphazard an evolution algorithm may perform better. however, in practice search spaces are much more complex, with discontinuities, high dimensionality, plateaus, hills with some of the configuration parameters strongly coupled and some others independent from each other. a search technique that is optimal in one type of configuration space may fail to locate an adequate configuration in another. it is difficult to provide a robust system that performs well in a variety of situations. additionally, many application domains will have domain-specific search techniques (such as scheduling or blocking heuristics) which may be critical to finding an optimal solution efficiently. this has caused most prior autotuners to use customized search techniques tailored to their specific problem. this requires machine learning expertise in addition to the individual domain expertise to build an autotuner for a system. we believe that this is one of the main reasons that, while autotuners are recognized as critical for performance optimization, they have not seen commodity adoption."
"in addition to these primary primitive and complex abstractions for parameter types, there are a number of derived ways that search techniques will interact with parameters in order to more precisely convey intent. these are additional methods on parameter which contain default implementations for both primitive and complex parameter types. these methods can optionally be overridden for specific parameters types to improve search techniques. parameter types will work without these methods being overridden, but implementing them can improve results."
"a core concept in opentuner is the use of ensembles of search techniques. many search techniques (both built in and user-defined) are run at the same time, each testing candidate configurations. techniques which perform well by finding better configurations are allocated larger budgets of tests to run, while techniques which perform poorly are allocated fewer tests or disabled entirely. techniques are able to share results using a common results database to constructively help each other in finding an optimal solution. algorithms add results from other techniques as new members of their population. to allocate tests between techniques we use an optimal solution to the multi-armed bandit problem using area under the curve credit assignment. ensembles of techniques solve the large and complex search space problem by providing both a robust solutions to many types of large search spaces and a way to seamlessly incorporate domain specific search techniques."
"to demonstrate opentuner's versatility, we used the system to learn to play super mario bros., a nintendo entertainment system game in which the player, as mario, runs to the right, jumping over pits and onto enemies' heads to reach the flagpole at the end of each level. we use opentuner to search for a sequence of button presses that completes the game's first level. the button presses are written to a file and played back in the nes emulator fceux. the emulator is configured to evaluate candidate input sequences faster than real time. opentuner evaluates 96 input sequences in parallel in different instances of the emulator. a lua script running in the emulator detects when mario dies or reaches the flagpole by monitoring a location in the emulated memory. at that time it reports the number of pixels mario has moved to the right, which is the objective function opentuner tries to maximize. the game is deterministic, so only one trial is necessary to evaluate this function. opentuner does not interpret the game's graphics or read the emulated memory; the objective function's value is the only information opentuner receives. horizontal movement is controlled by many enumparameters choosing to run or walk left or right or to not move, each paired with an intparameter specifying the duration of the movement in frames. as the goal is to move right, the search space is given a 3 to 1 right-moving bias. jumping is controlled by intparameters specifying on which frames to jump and how long to hold the jump button. figure 13 shows the performance of the mario example. opentuner rapidly reaches 1500 pixels because many initial (mostly random) configurations will make it that far in the level. the bottlenecks evident in the figure correspond to pits which mario must jump over, which requires coordinating all four kinds of parameter: mario must jump on the correct frame for a long enough duration, having built up speed from walking or running right for a long enough duration, and then must not return left into the pit. performance is aided by a \"ratcheting\" effect: due to the nes's memory limits, portions of the level that have scrolled off the screen are quickly overwritten with new portions, so the game only allows mario to move left one screen (up to 256 pixels) from his furthest-right position. the final sequences complete the level after between 3500 and 5000 input actions."
"in this paper we present opentuner, a new framework for building domain-specific program autotuners. opentuner features an extensible configuration and technique representation able to support complex and user-defined data types and custom search heuristics. it contains a library of predefined data types and search techniques to make it easy to setup a new project. thus, opentuner solves the custom configuration problem by providing not only a library of data types that will be sufficient for most projects, but also extensible data types that can be used to support more complex domain specific representations when needed."
"the gcc/g++ flags autotuner is described in detail in section 3.1. there are a number features that were omitted from the earlier example code for simplicity, which are included in the full version of the autotuner."
"the default meta technique, used in results in this paper and meant to be robust, uses an auc bandit meta technique to combine greedy mutation, differential evolution, and two hill climber instances."
"program autotuning is increasingly being used in domains such as high performance computing and graphics to optimize programs. program autotuning augments traditional human-guided optimization by offloading some or all of the search for an optimal program implementation to an automated search technique. rather than optimizing a program directly, the programmer expresses a search space of possible implementations and optimizations. autotuning permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than acm must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. can often make the optimization process more efficient as autotuners are able to search larger spaces than is possible by hand. autotuning also provides performance portability, as the autotuning process can easily be re-run on new machines which require different sets of optimizations. finally, multi-objective autotuning can be used to trade off between performance and accuracy, or other criteria such as energy consumption and memory usage, and provide programs which meet given performance or quality of service targets."
".compute_at(blur_y, yi) .vectorize(x, 8); blur_y(x, y) and blur_y(x, y) are the halide functions which make up the pipeline. the scheduling operators available to the autotuner are the following:"
"ensembles of techniques are created by instantiating a meta technique, which is a technique made up of a collection of other techniques. the opentuner search driver interacts with a single root technique, which is typically a meta technique. when the meta technique gets allocated tests, it incrementally decides how to divide theses tests among its sub-techniques. opentuner contains an extensible class hierarchy of techniques and meta techniques, which can be combined together and used in autotuners."
complex parameters present a more opaque view to search techniques. complex parameters have a variable set of manipulation operators (manipulators) which make stochastic changes to the underlying parameter. these manipulators are arbitrary functions defined on the parameter which can make high level type dependent changes. complex parameters are meant to be easily extended to add domain specific structures to the search space. new complex parameters define custom manipulation operators for use by the configuration manipulators.
"the permutation parameter type assigns an order to a given list of values and has manipulators which make various types of random changes to the permutation. a schedule parameter is a permutation with a set of dependencies that limit the legal order. schedules are implemented as a permutation that gets topologically sorted after each change. finally, a selector parameter is a special type of tree which is used to define a mapping from an integer input to an enumerated value type."
"halide separates the expression of the kernels and pipeline itself from the pipeline's schedule, which defines the order of execution and placement of data by which it is mapped to machine execution. the schedule dictates how the halide compiler synthesizes code for a given pipeline. this allows expert programmers to easily define and explore the space of complex schedules which result in high performance."
"to implement an autotuner with opentuner, first, the user must define the search space by creating a configuration manipulator. this configuration manipulator includes a set of parameter objects which opentuner will search over. second, the user must define a run function which evaluates the fitness of a given configuration in the search space to produce a result. these must be implemented in a small python program in order to interface with the opentuner api. figure 3 shows an example of using opentuner to search over the space of gcc compiler flags in order to minimize execution time of the resulting program. in section 4, we present results on an expanded version of this example which obtains up to 2.8x speedup over -o3."
opentuner includes implementations of the techniques: differential evolution; many variants of nelder-mead search and torczon hillclimbers; a number of evolutionary mutation techniques; pattern search; particle swarm optimization; and random search. opentuner also includes a bandit mutation technique which uses the same auc bandit method to decide which manipulator function across all parameters to call on the best known configuration. these techniques span a range of strategies and are each biased to perform best in different types of search spaces. they also each contain many settings which can be configured to change their behavior. each technique has been modified so that with some probability it will use information found by other techniques if other techniques have discovered a better configuration.
"our terminology reflects that the autotuning problem is cast as a search problem. the search space is made up of configurations, which are concrete assignments of a set of parameters. parameters can be primitive such as an integer or complex such as a permutation of a list. when the performance, output accuracy, or other metrics of a configuration are measured (typically by running it in a domain-specific way), we call this measurement a result. search techniques are methods for exploring the search space and make requests for measurement called desired results. search techniques can change configurations using a user-defined configuration manipulator, which also includes parameters corresponding directly the parameters in the configuration."
the default implementation of the configuration manipulator uses a fixed list of parameters and stores the configuration as a dictionary from parameter name to parameter-dependent data type.
"the primary components in the search space for petabricks programs are algorithmic selectors which are used to synthesize instances of poly-algorithms from algorithmic choices in the petabricks language. a selector is used to map input sizes to algorithmic choices, and is represented by a list of cutoffs and choices."
the configuration manipulator can be extended by the user either to change the underlying data structure used for configurations or to support a dynamic list of parameters that is dependent on the configuration instance.
"opentuner supports multiple user-defined objectives. result records have fields for time, accuracy, energy, size, confidence, and user defined data. the default objective is to minimize time. many other objectives are supported, such as: maximize accuracy; threshold accuracy while minimizing time; and maximize accuracy then minimize size. the user can easily define their own objective by defining comparison operators and display methods on a subclass of objective."
"opentuner has a hierarchy of built-in parameter types. each parameter type is responsible for interfacing between the raw representation of a parameter in the configuration and the standardized view of that parameter presented to the search techniques. parameter types can be extended both to change the underlying representation, and to change the abstraction provided to search techniques to cause a parameter to be search in different ways."
"while the practice of autotuning has increased in popularity, autotuners themselves often remain relatively simple and project specific. there are three main challenges which make the development of autotuning frameworks difficult."
"the built in parameter types float and logfloat (and similarly integer and loginteger) both have identical representations in the configuration, but present a different view of the underlying value to the search techniques. float is presented directly to to search techniques, while logfloat presents a log scaled view of the underlying parameter to search techniques. to a search technique, halving and doubling a log scaled parameter are changes of equal magnitude. log scaled variants of parameters are often better for parameters such as block sizes where fixed changes in values have diminishing effects the larger the parameter becomes. poweroftwo is a commonly used special case, similar to loginteger, where the legal values of the parameter are restricted to powers of two."
"additionally, there were a number of smaller features such as: time limits to abort slow tests which will not be optimal; use of loginteger parameter types for some values; a save_final_config method to output the final flags; and many command line options to control autotuner behavior."
"for each benchmark, we keep the environment as constant as possible, but do not use the same environment for all benchmarks, due to particular requirements for each benchmark, and because one (the stencil benchmark) is based on data from an environment outside our control."
"opentuner is free and open source 4 and as the community adds more techniques and representations to this flexible framework, there will be less of a need to create a new representation or techniques for each project and we hope that the system will work out-of-the-box for most creators of autotuners. opentuner pushes the state of the art forward in program autotuning in a way that can easily be adopted by other projects. we hope that opentuner will be an enabling technology that will allow the expanded use of program autotuning both to more domains and by expanding the role of program autotuning in existing domains."
"for most parameters, we utilize enumparameter or switchparameter, as they generally represent discrete choices in the algorithm used. the major parameter that controls performance is the blocksize of the matrix; this we represent as an integerparameter to give as much freedom as possible for the autotuner for searching. another major parameter controls the distribution of the matrix onto the processors; we represent this by enumerating all 2d decompositions possible for the number of processors on the machine. figure 9 shows the results of 30 tuning runs using opentuner, compared with the vendor-provided performance. the median performance across runs, after 1200 seconds of autotuning, exceeds the performance of intel's optimized parameters. overall, opentuner obtains a best performance of 86.5% of theoretical peak performance on this machine, while exploring a miniscule amount of the overall search space. furthermore, the blocksize chosen is not a power of two, and is generally a value unlikely to be guessed for use in hand-tuning."
primitive parameters present a view to search techniques of a numeric value with an upper and lower bound. these upper and lower bounds can be dependent on the configuration instance.
"the results database is a fully featured sql database. all major database types are supported, and sqlite is used if the user has not configured a database type so that no setup is required. it allows different techniques to query and share results in a variety of ways and is useful for introspection about the performance of techniques across large numbers of runs of the autotuner."
"to provide robust search, opentuner includes techniques that can handle many types of search spaces and runs a collection of search techniques at the same time. techniques which perform well are allocated more tests, while techniques which perform poorly are allocated fewer tests. techniques share results through the results database, so that improvements made by one technique can benefit other techniques. this sharing occurs in technique-specific ways; for example, evolutionary techniques add results found by other techniques as members of their population. opentuner techniques are meant to be extended. users can define custom techniques which implement domainspecific heuristics and add them to ensembles of pre-defined techniques."
"the first challenge is using the right configuration representation for the problem. configurations can contain parameters that vary from a single integer for a block size to a much more complex type such as an expression tree representing a set of instructions. the creator of the autotuner must find ways to represent their complex domain-specific data structures and constraints. when these data structures are naively mapped to simpler representations, such as a point in high dimensional space, locality information is lost which makes the search problem much more difficult. picking the right representation for the search space is critical to having an effective autotuner. to date, all autotuners that have used a representation other than the simplest ones have had custom project-specific representations."
"other autotuning systems include spiral [cit] for digital signal processing patus [cit] and sepya [cit] for stencil computations, and oski [cit] for sparse matrix kernels."
"we ran experiments using gcc 4.7.3-1ubuntu1, on an 8 total core, 2-socket xeon e5320. we allowed flags such as -ffast-math which can change rounding or nan behavior of floating-point numbers and have small impacts on program results. we still observe speedups with these flags removed."
"together, these operators define the order of execution (as a perfectly nested loop) and storage layout within the domain of each function, independently. finally, the granularity of interleaving and storage between stages are specified by two more operators:"
"some parameters include manipulators, which are opaque functions that make stochastic changes to a specific parameter in a configuration. figure 2 provides an overview of the major components in opentuner. the search process includes techniques, which use the user defined configuration manipulator in order to read and write configurations. the measurement processes evaluate candidate configurations using a user defined measurement function. these two components communicate exclusively through a results database used to record all results collected during the tuning process, as well as providing ability to perform multiple measurements in parallel."
"in the halide language, these choices are specified by applying schedule operators to each function. the composition of potentially many schedule operators for each function together define the organization of computations and data for a whole pipeline."
"as a somewhat different example, we use opentuner in an example from physics, namely the quantum control problem of synthesizing unitary matrices in su (2) in optimal time, unlike other examples, which use opentuner as a traditional autotuner to optimize a program, the unitary example uses opentuner to perform a search over the problem space as a subroutine at runtime in a program. the problem has a fixed set of operators (or controls), represented as matrices, and the goal is to find a sequence of operators that, when multiplied together, produce a given target matrix. the objective function is an accuracy value defined as a function of the distance of the product of the sequence to the goal (also called the trace fidelity). figure 12 shows the performance of the unitary example on both easy and hard instances of the problem. for both types of problem instance opentuner is able to meet the accuracy target within the first few seconds. this is example shows that opentuner can be used for more types of search problems than just program autotuning."
"the benchmarks show interesting behavior and varying sets of optimal flags. the matrix multiply benchmark reaches optimal performance through a few large steps, and its speedup is dominated by 3 flags: -fno-exceptions, -fwrapv, and -funsafe-math-optimizations. on the other hand, tsp takes many small, incremental steps and its speedup is spread over a large number of flags with smaller effects. the fft benchmark is heavily hand optimized and performs more poorly at -o3 than at -o2; the flags found for it are primarily disabling, rather than enabling, various compiler optimizations which interact poorly with its handwritten optimizations. while there are some patterns, each benchmark requires a different set of flags to get the best performance."
"we have shown opentuner, a new framework for building domain-specific multi-objective program autotuners. opentuner supports fully customizable configuration representations and an extensible technique representation to allow for domain-specific techniques. opentuner introduces the concept of ensembles of search techniques in autotuning, which allow many search techniques to work together to find an optimal solution and provides a more robust search than a single technique alone."
"the halide project previously integrated a custom autotuner to automatically search over the space of schedules, but it was removed because it became too complex to maintain and was difficult to use in practice. halide's model of schedules is based on defining three things for each function in a pipeline:"
"the benchmark measures the speed of solving a large random dense linear system of equations using distributed memory. achieving optimal performance requires tuning about fifteen parameters, including matrix block sizes and algorithmic parameters. to assist in tuning, hpl includes a built in autotuner that uses exhaustive search over user-provided discrete values of the parameters. we run hpl on a 2.93 ghz intel sandy bridge quad-core machine running linux kernel 3.2.0, compiled with gcc 4.5 and using the intel math kernel library (mkl) 11.0 for optimized math operations. for comparison purposes, we evaluate performance relative to intel's optimized hpl implementation 3 . we encode the input tuning parameters for hpl as naïvely as possible, without using any machinespecific knowledge."
"the method manipulator (line 23), is called once at startup and creates a configurationmanipulator object which defines the search space of gcc flags. all accesses to configurations by search techniques are done through the configuration manipulator. for optimization level, an integerparameter between 0 and 3 is created. for each flag, a enumparameter is created which can take the values on, off, and default. finally, for the remaining bounded gcc parameters, an integerparameter is created with the appropriate range."
"first, we added error checking to gracefully handle the compiler or the output program hanging, crashing, running out of memory, or otherwise going wrong. our tests uncovered a number of bugs in gcc which triggered internal compiler errors and we implemented code to detect, diagnose, and avoid error-causing sets of flags. we are submitting bug reports for these crashes to the gcc developers."
"opentuner is divided into two submodules, search and measurement. the search driver and measurement driver in each of these modules orchestrate most of the framework of the search process. these two modules communicate only through the results database. the measurement module is minimal by design and is primarily a wrapper around the user defined measurement function which creates results from configurations."
"the second challenge is the size of the valid configuration space. while some prior autotuners have worked hard to prune the configuration space, we have found that for many problems excessive search space pruning will miss out on non-intuitive good configurations. we believe providing all the valid configurations of these search spaces is better than artificially constraining search spaces and possibly missing optimal solutions. search spaces can be very large, up to 10 3600 possible configurations for one of our benchmarks. full exhaustive search of such a space will not complete in human lifetimes! thus, intelligent machine learning techniques are required to seek out a good result with a small number of experiments."
"where vt is the list of uses of t in the sliding window history. vt,i is 1 if using technique t the ith time in the history resulted in a speedup, otherwise 0."
"until today, production uses of halide have relied on hand-tuned schedules written by experts. the autotuning problem in halide is to automatically synthesize schedules and search for high-performance configurations of a given pipeline on a given architecture."
"we represent reorder_storage as a permutationparameter over the original (pre-split) variables in the function. the reorder operator for each function is a scheduleparameter over all the post-split variables, with the permutation constraint that the split children of a single variable are ordered in a fixed order relative to each other, to remove redundancy in the choice space."
"as an example, the selector [insertionsort, 500, quicksort, 1000 where quicksort and mergesort recursively call sort so the program dynamically switches between sorting methods as recursive calls are made on smaller and smaller lists. we used the general selectorparameter to represent this choice type, which internally keeps track of the order of the algorithmic choices and the cutoffs. petabricks programs contain many algorithmic selectors and a large number of other parameter types, such as block sizes, thread counts, iteration counts, and program specific parameters. results using opentuner compared to the built-in petabricks autotuner are shown in figure 10 . the petabricks autotuner uses a different strategy that starts with tests on very small problem inputs and incrementally works up to full sized inputs [cit] . in all cases, the autotuners arrive at similar solutions, and for strassen, the exact same solution. for sort and tridiagonal solver, opentuner beats the native petabricks autotuner, while for poisson the petabricks autotuner arrives at a better solution, but has much higher variance."
"2z sr 2 c l p n i (s)dz due to the nonclosed-form laplace transform. we propose to make a simplification assumption that, from the second time slot onwards, a d2d user will become active if there is one or more active txs multicasting the message within its sensitivity region. we show that this approximation allows tractable"
"we use algorithm 2 to train n tree-structured neural networks for regression. following the same idea as random forest for regression, we aggregate the predicted values of the n independently trained neural networks. specifically, for an input x, the predicted numeric value is by aggregating the predicted value from the n neural networks as"
the accuracy of this approximation will be validated in section v by comparison with simulation results which is generated based on the exact system scenario rather than the approximated spatial model adopted in the analysis.
"proof: the network will be in outage if none of the d2d users has successfully received the message from the drone at the first time slot t 0 . from theorem 1, we have"
"from figure 5 (a) and figure 5 (c), we can see that after a given number of time slots, the link coverage probability of the cell edge user and the network coverage probability are higher if the d2d user density is higher. figure 5 (b) demonstrates that the mean local delay of a d2d user at 2 km from the ground projection of the drone decreases from 4.736 time slots to 1.806 time slots, when the density of the d2d users increases from 10 per km 2 to 100 per km 2 . with a higher density of d2d users, there are more d2d users in the cell that have received the message successfully and more active txs located inside the d2d user's sensitivity region. so the message is delivered to the cell edge user within fewer time slots."
"the overall architecture of a three-layer neural network for classification is illustrated in figure 1 . for ease of explanation, we name the nodes as n i j, which means the jth node in the ith layer. a parent node n i j is connected to two child nodes n i+1,2 j −1 and n i+1,2 j . for example, as shown in the figure, n 22 is the second node in layer 2 and is connected to two child nodes n 33 and n 34 . the network contains three types of nodes:"
"in spite of the successes of neural networks in specific domains, this success has not been replicated across all domains. in fact, methods like random forests [cit] regularly outperform neural networks in arbitrary domains [cit], especially when the underlying data sizes are small and no domain-specific insight has been used to arrange the architecture of the underlying neural network. this is because neural networks are highly prone to overfitting, and the use of a generic layered architecture of the computation units (without domain-specific insights) can lead to poor results. the performance of neural networks is often sensitive to the specific architectures used to arrange the computational units. although the convolutional neural network architecture is known to work well for the image domain, it is hard to expect an analyst to know which neural network architecture to use for a particular domain or for a specific dataset from a poorly studied application domain."
"where (21) comes from the fact that h follows exponential distribution with unit mean. combining (19c), (20) and (21), we can arrive at theorem 3."
"we consider a drone-initiated d2d-aided multihop multicast network where a drone is deployed to broadcast an emergency alert message to all terrestrial d2d users at the first time slot. after that, the d2d users that have successfully received the message become the active transmitters (txs) for the next time slots to multicast the emergency alert message through multihop. the main contributions and findings of this paper are summarized as follows:"
"the remaining of the article are organized as follows. in section 2, we introduce the randomforest-inspired architecture. in section 3, we give the detailed design of the proposed framework nnrf for classification followed by training algorithm and time complexity analysis. in section 4, we extend nnrf for regression. in section 5, we conduct experiments to demonstrate the effectiveness of nnrf for classification and regression and analyze the parameter sensitivity on nnrf. in section 6, we briefly review related works. in section 7, we conclude with future work."
"in this section, we introduce the basic architecture of the neural network used for the learning process. throughout this article, matrices are written as bold capital letters such as m, w i j, and vectors are denoted as bold lowercase letters such as p and p i j . m(i, j) denotes the (i, j)th entry of m while m(i, :) and m(:, j) denotes the ith row and jth column, respectively. similarly, p(i) denotes the ith elements of p."
"to evaluate the regression performance of the proposed framework nnrf, we use root mean square (rmse) and mean absolute value (mae), which are classical evaluation metrics for measuring the quality of regression. the smaller rmse and mae are, the closer the predicted numerical value is to the ground-truth value."
"in figure 6 (a), figure 6 (b) and figure 6 (c), we evaluate the effect of the radius of d2d sensitivity region on the link coverage probability at the cell edge user after time slot t n, the mean local delay of a d2d user and the network coverage probability after time slot t n respectively."
"similarly, we use back-propagation to train the decision-tree-structured neural network for regression. as decision-tree-structured neural network for regression and that for classification have similar structure except the last layer and the loss function, the derivation of back-propagation for regression is also very similar to that for classification; the detail of the latter is given in section 3.2. we thus omit the detailed derivation for regression here. the algorithm to train a dlayer tree-structured neural network for regression is the same as that for training a tree-structured neural network for classification, which is given in algorithm 1."
"although the number of nodes might seem large, we will see that the required value of d is often quite modest in real settings, because the neural network nodes are able to simulate more powerful splits. furthermore, because of the tree structure of the neural network, the number of parameters to be learned is quite modest compared to the number of nodes. this is an important factor in avoiding overfitting. another parameter input to the algorithm is r, which is the number of features that are randomly selected to perform the split at each node. in a traditional random forest, the bag of features to be used for a split at each node is randomly selected up front. similarly, while setting up the architecture of each neural network, each node in the tree has a bag of features that are randomly selected and fixed up front. thus, the architecture of the neural network is inherently randomized, based on the input features that are selected for each node. as we will see later, this property is particularly useful in an ensemble setting."
"the total number of layers in the neural network is denoted by d, which also represents the height of each decision tree in the random forest that the neural network is simulating. thus, one input parameter to the algorithm is the number of layers d of the neural network. the neural network is structured exactly like a binary decision tree, with each node simulating a split and having two outputs out of which exactly one is active. these two outputs feed into a unique node of the next layer of the neural network. therefore, the number of nodes always doubles from one layer to the next. thus, the total number of nodes in the neural network is given by 1 + 2 1 + · · · + 2 d −1, which is equal to 2 d − 1. in addition, there is a special output node that takes as its input the 2 d −1 nodes in the final layer and combines them to provide a single prediction of the class label."
"in this paper, we considered a drone-initiated d2d-aided multihop multicast network in public safety scenarios, where an emergency alert message is broadcasted by the drone and then multicasted by the d2d users which have successfully received the message through multihop. a general analytical framework for link coverage probability and mean local delay for a d2d user was presented in terms of the link success probability, the network outage probability and the laplace transform of the aggregate received signal power. an approximate and yet accurate analytical model was proposed for link success probability, network success probability and network coverage probability. the accuracy of the analytical results was verified by simulations. the results showed that the link performance and the network performance improve by raising the deployment altitude and the transmit power of the drone and increasing the density and the sensitivity radius of the d2d users. future work can consider that the drone broadcasts the message above any random location in the network region."
"the classification experiments are conducted on 13 publicly available benchmark datasets, which includes 8 uci 1 datasets, i.e., forest type mapping (foresttype), speech record of 26 alphabets (isolet), sonar signals of mines vs. rocks (sonar), chemical analysis of wines (wine), wisconsin diagnostic breast cancer (wdbc), vehicle silhouettes (vehicle), hill valley (valley) and sensorless drive diagnosis (drivediagnosis), three image datasets, i.e., images of 20 objects (coil-20) 2, images of handwritten digits (usps), 3 images of faces (msra), 4 one bioinfomatics dataset (bioinformatics) [cit], and one hand movement dataset (movement) 5 . we include datasets of different domains and different format so as to give a comprehensive understanding of how nnrf performs with datasets of various domains and format. the statistics of the datasets used in the experiments are summarized in table 1 . from the table, we can see that these datasets are small datasets with different number of classes. deep-learning algorithms with large amounts of parameters may not work well on these datasets."
"before looking into the network coverage probability, we first investigate the network success probability at time slot t n . to evaluate the network success probability, we need to calculate the expectancy of the link success probability over the spatial distribution of the distance between a d2d user and the ground projection of the drone. the expression of the network success probability is given as (13) remark 2: to the best of our knowledge, it is not easy to evaluate the integration"
"the rest of the paper is organized as follows: section ii describes the system model and assumptions. section iii focuses on the link performance of a d2d user. section iv details the analysis of the network coverage probability. section v presents the results and the effect of the system parameters on the link performance and the network performance. finally, section vi concludes the paper."
"random forest [cit] (rf) is an ensemble-based method that aggregates the results of many decision trees. generally, each decision tree is a binary tree that decides which path to take based on an input feature at each node. it has been successfully applied in various domains such as bioinformatics [cit], remote sensing [cit], and compound classification [cit] . it is considered a generalist method, which has been demonstrated to beat even the best classifiers on small datasets for general domains [cit] . for example, [cit] compared 179 classifiers on the entire uci collection of datasets and found that, overall, random forests perform the best. in addition to classification, random forest is also very powerful for regression. random forest is less likely to overfit than neural network because of the ensemble, bootstrap, and randomized feature sampling used in each decision, which introduces diversity to the model to reduce overfitting. however, unlike neural networks, which learn powerful features by applying nonlinear activation function on input features, rfs only uses one feature to make decision at each node. in this sense, random forest is not as powerful as neural network. therefore, it is natural to combine the learning ability of nns and the ability of reducing overfitting of random forests by designing a random forest-structured neural network."
"there are several interesting directions that need further investigation. first, in this work, we conduct experiments to demonstrate the effectiveness of nnrf for classification and regression. a detailed theoretical analysis of nnrf, such as its rate of convergence and representation capacity, is worth pursuing. second, in this article, as an initial attempt of designing random forest-structured neural network, we have not tried the widely adopted tricks such as batch normalization [cit] or dropout [cit], which has been proven to be very useful for deep nets. thus, another direction is to introduce these tricks into nnrf for training deeper decision-tree-structured neural networks."
"exhibit very characteristic spatial/temporal behavior, which can be exploited by carefully designed neural network architectures. such characteristics are certainly not true for all data domains, and in some cases a dataset may be drawn from an application with unknown characteristic behaviors."
"to evaluate the regression ability of nnrf, we compare the proposed framework with other classical and state-of-the-art regression models. the details of these regression methods are listed as follows:"
"we now turn to the delay metric. for the considered droneinitiated d2d-aided multihop multicast network, the local delay at the d2d user x i is a discrete random variable with a probability mass function as follows:"
we compare the proposed framework nnrf with other classical and state-of-the-art classifiers to evaluate the classification ability of nnrf. the details of these classifiers are listed as follows:
"proposition 1: based on the system model in section ii and the definition of the link coverage probability after time slot t n, we have"
"where (19a) follows the probability generating functional of ppp and (·) in (19b) denotes the complete gamma function. using the fact that h follows an exponential distribution, (19c) can be worked out by taking the expectation over h ."
"since there are only one active path for each input data sample, the cost of performing forward propagation up to depth d using equation (6) is o(rd). since p only has two nonnegative elements, the cost of softmax in equation (7) to get q is o(c). therefore, the cost of forward propagation is o(rd + c) for each data sample in each tree. similarly, the cost of backward propagation using equation (13) and equation (14) is also o(c + rd). thus, the total cost of training n tree-structured neural networks with depth d is o(t (c + rd)nn ), where t is number of epochs for training each neural network. following the common way of setting r in random forest, r is usually chosen as √ m [cit], where m is the number of features. effects of different choices of r can be found in section 5.5.2. thus, the total cost is approximately o(t (c + √ md )nn ), which is modest, and thus the training is efficient. construct f i j by selecting r features at random from the m features of x *"
"domain-specific insights are not the only way in which one can engineer the architecture of a neural network to reduce the parameter footprint. in this article, we show that one can use inspiration from successful classification methods like random forests to engineer the architecture of the neural network. furthermore, starting with this basic architecture, one can improve on the basic random forest model by leveraging the inherent power in the neural network architecture in a carefully controlled way. the reason is that models like random forests are also capable of approximating arbitrary decision boundaries but with less overfitting on smaller datasets."
"neural networks (nn) are very powerful in learning or extracting nonlinear features for many machine-learning and data-mining tasks when abundant training data are given. it has achieved great success in computer vision, speech recognition, and natural language processing. for example, in computervision, convolutional neural networks (cnn) have become the state-of-the-art model for image classification [cit], semantic segmentation [cit], image representation learning [cit], and image generation [cit] . similarly, in natural language processing, lstm has shown its great ability in sentence classification [cit], sentence generation [cit], and machine translation [cit] . despite the success of neural networks in specific domains, the success of neural networks are not universe across all domains. first, neural networks, especially deep nets, have massive weights that need a large number of datasets to train and are prone to overfitting on small datasets. thus, for domains with small datasets, neural networks usually do not work well, while domains with small datasets such as bioinformatics are pervasive. second, the success of current deep nets such as cnn and lstm actually rely on the domain insights to design specific structure to reduce the parameters to alleviate overfitting. for example, cnn uses convolution layers and max pooling layers for reducing parameters and learning translation-invariant features from images. similarly, lstm shared the weights in each cell to reduce the parameter and designs the forget gate to capture the long-term dependency. however, for general domains whose domain insights are not clear or difficult to be integrated to neural networks, deep nets may not work well. therefore, designing neural networks that work well on general domains with small datasets is important. random forest, which is robust to overfitting on small datasets, is a good choice to inspire design, which will be discussed in the next subsection."
"theorem 3: following the system model in section ii, the laplace transform of the aggregate received signal power at the d2d user x i from all the active txs inside its sensitivity region is given as (9) at the bottom of this page, where 2 f 1 (·, ·; ·; ·) is the gaussian (or ordinary) hypergeometric function."
"in contrast, methods like decision forests are considered generalist methods in which one can take an off-the-shelf package like caret [cit] and often outperform [cit] even the best of classifiers. a recent study [cit] evaluated 179 classifiers from 17 families on the entire uci collection of datasets and concluded that random forests were the best performing classifier among these families, and in most cases, their performance was better than other classifiers in a statistically significant way. in fact, multiple third-party implementations of random forests were tested by this study and virtually all implementations provided better performance than multiple implementations of other classifiers; these results also suggest that the wins by the random forest method were not a result of the specific implementations of the method but are inherent to the merit of the approach. furthermore, the datasets in the uci repository are drawn from a vast variety of domains and are not specific to one narrow class of data such as images or speech. this also suggests that the performance of random forests is quite robust irrespective of the data domain at hand."
"in this article, we propose a fundamentally different approach to design a basic architecture of the neural network, so that it has the property of random forests and has reduced overfitting. we design the decision-making function based on linear combination of input features followed by activation function and thus is more powerful in making decisions."
definition 1: the link success probability at time slot t n is the probability that the received snr at the d2d user x i is higher than the threshold γ . it can be expressed as
"the previous section gave the overall architecture of the neural network; in this section, we give the inner working of the neural network for classification. we will extend the neural network for regression in section 4. next, we first introduce the inner working of decision making in each type of node, which guarantees that only one path will be activated. we then introduce how to efficiently perform back-propagation followed by time and space complexity."
"where p a is the transmit power of the drone. g is the aerial link fading power gain, which follows gamma distribution. z is the euclidean distance between the drone and the terrestrial d2d user. the path-loss of the aerial link pl a (·) is given in (1)."
"is the set of parameters to be learned in the treestructured neural network t and t (x(l, :)) is the estimated class distribution of the input data x(l, :). r l (θ ) is a regularizer associated with x(l, :) to avoid over-fitting, which is defined as"
"it has been shown by the measurements that aerial links experience different channel characteristics comparing to terrestrial links [cit] . the aerial links between the drone and the d2d users can be either los or non-line-of-sight (nlos) with different probabilities of occurrence p l and p n, which are determined by altitude and type of the drone, elevation angle, type of propagation environment, density and height of buildings [cit] ."
"as there is only one path activated for forward and backward propagation, the time complexity of nnrf for regression is o(trdnn ), where t is number of epochs for training each neural network. the number of parameters of nnrf for regression is o(2 d +1 r ), where d is usually chosen within 3 and 6."
"insights: figure 3 (a) shows that the increase in the drone transmit power improves the link coverage probability of the cell edge user. with a higher transmit power of the drone, the link coverage probability of the cell edge user approaches 1 faster. from figure 3(b), we can see that the benefit of increasing transmit power of the drone is more significant for the d2d users further away from the ground projection of the drone than for the ones closer to the ground projection of the drone. figure 3 (c) illustrates that a higher drone transmit power provides a higher network coverage probability. with a higher transmit power of the drone, the network coverage probability approaches 1 using less time slots. this is because that more d2d users successfully receive the message after the first time slot t 0 if the drone broadcasts with a higher transmit power. therefore, there are more active d2d users which multicast the message from the second time slot t 1 onwards and the message spreads over the network region more quickly."
"the algorithm to train a d-layer tree-structured neural network for classification is shown in algorithm 1. we first draw a bootstrap sample in line 1. from line 2 to line 5, we draw the input feature for each node of the neural network. from line 8 to line 9, we update the parameters using back-propagation. following the same idea as random forest, we aggregate n independently trained neural networks for classification, which we name as nnrf. the algorithm of nnrf is shown in algorithm 2. for an input x, the predicted class distribution is by aggregating the predicted class distribution from the n neural networks,"
"in the previous section, we have focused on the link performance where the distance between the d2d user and the ground projection of the drone is known. now we turn to study the network performance charaterized by the network coverage probability after time slot t n . p n cov measures the average probability that a randomly chosen d2d user in the network has successfully received the message after time slot t n, in other words, it presents the fraction of active d2d txs in the network after the given time slot."
"after the first time slot t 0, the drone leaves the network region and all d2d users that have successfully received the emergency alert message become the txs and multicast the message. however, the network will be in outage if none of the d2d users has successfully received the message from the drone at the first time slot t 0 ."
"in this section, we conduct experiments to evaluate the effectiveness of the proposed framework nnrf for classification and regression. we begin by introducing datasets and the experimental setting, and then we compare nnrf with state-of-the-art classifiers to demonstrate the effectiveness of nnrf for classification. we also compare nnrf with classical and representative regression methods to show the effectiveness of nnrf for regression. further experiments are conducted to investigate the effects of the hyper-parameters on nnrf for classification and regression, respectively."
"we study the impact of the density of d2d users on the link coverage probability at the cell edge user, the mean local delay of a d2d user and the network coverage probability after time slot t n in figure 5 insights: figure 5 (a) illustrates the link coverage probability after time slot t n at the cell edge user located at 2 km from the ground projection of the drone for different d2d user density. figure 5 (b) plots the mean local delay of a d2d user against its distance from the ground projection of the drone with various d2d user density. figure 5(c) shows the network coverage probability after time slot t n for different density of d2d users."
"in the first time slot t 0, the drone broadcasts an emergency alert message to all terrestrial d2d users. at the end of the first time slot, terrestrial d2d users attempt to decode the message. user x i decodes successfully if and only if the received signal-to-noise ratio (snr) is greater than a threshold γ . the instantaneous snr at the d2d user at the first time slot t 0 is given as"
"in figure 4 (a), figure 4 (b) and figure 4(c), we investigate the effect of deployment height of the drone on the link coverage probability at the cell edge user, the mean local delay of a d2d user and the network coverage probability after time slot t n respectively."
"in conventional neural networks, the nodes in the input layer are cleanly separated from the hidden layer. however, in this case, we will propose a neural network in which a clear separation does not exist between the nodes of the input and hidden layers. the internal nodes are not completely hidden, because they are allowed to receive inputs from some of the features. rather, the neural network is designed with a hierarchical architecture, much like a decision tree. furthermore, just as a random forest contains multiple independent decision trees, our approach will use multiple independent neural networks, each of which has a randomized architecture based on the randomized choice of the inputs in the \"hidden\" layers. as we will see later, each neural network can be trained extremely efficiently, which is what makes this approach extremely appealing."
end for 6: end for 7: repeat 8: forward propagation to get cost 9: backward propagation to update parameters 10: until convergence 11: return tree structured neural network
"to assist terrestrial networks for rapid spread of emergency information. furthermore, device-to-device (d2d) communications is an effective technique for network coverage improvement in terrestrial communication systems by enabling direct communications between nearby mobile terminals [cit] . therefore, efficient and rapid emergency information dissemination can be achieved in public safety scenarios by exploiting both d2d communications and drone mobility. the investigation of drone-initiated d2d-aided multihop multicast networks for the rapid spread of emergency alert messages in public safety scenarios is a timely and important open problem in the literature which is addressed in this work."
"the literature review covers four aspects related to this work: (i) use of drones for information dissemination, (ii) use of drones for coverage in wireless networks, (iii) analysis of drone communication networks using stochastic geometry and (iv) multicast transmission without assistance from drone. generally, the papers in the first two categories use optimization techniques and have a different focus from our work. the aerial channel model adopted in this work is similar to those adopted by papers in the third category. the last category, which includes related work on d2d using stochastic geometry, is most relevant to this paper."
"in this section, we provide the mathematical formulations to analyze the link performance at the given d2d user, which incorporate the analysis of both phases. we are interested in the following two performance metrics that directly reflect the perceived experience of d2d users:"
"for the considered multihop multicast transmission scheme, time is slotted and the emergency alert message is delivered using multiple time slots to the d2d users on the ground through two phases."
"insights: figure 4(a) shows the link coverage probability after time slot t n at the cell edge user located at 2 km from the ground projection of the drone for different drone height. from this figure, we can see that the link coverage probability of the cell edge user after certain time slots is higher if the drone is deployed at a higher altitude at the first time slot t 0 . with a higher altitude of the drone, the link coverage probability of the cell edge user approaches 1 after less time slots. figure 4(b) plots the mean local delay of a d2d user against its distance from the ground projection of the drone with different height of the drone. the figure shows that the increase of the deployment height of the drone decreases the mean local delay more significantly for the d2d users further away from the ground projection of the drone than for the ones closer to the ground projection of the drone. although the signal propagates a longer distance as the height of the drone rises, there is higher probability that the link between the drone and the user around the cell edge is in los and thus experiences a lower path-loss. figure 4 (c) illustrates the network coverage probability after time slot t n for different deployment height of the drone. from the figure, we can find that a higher drone altitude provides a higher network coverage probability. the network coverage probability approaches 1 using less time slots when the drone is initially positioned at a higher height. this is because, if the drone is at a higher altitude, more d2d users around the cell edge receive the message and become active tx after the first time slot t 0 and the message spreads over the network region faster."
"from figure 2(b), we can see that the link coverage probability at the d2d user close to the ground projection of the drone is very closed to 1 after time slot t 5 . therefore, we focus on the link coverage probability at the cell edge d2d user, i.e., user located at 2 km from the ground projection of the drone. in the results presented later in this work, we investigate the effect of some important system parameters, the transmit power and the height of the drone and the density and the sensitivity radius of the d2d users, on the link coverage probability of the cell edge user. figure 3 (a) plots the link coverage probability after time slot t n at the cell edge user located at 2 km from the ground projection of the drone for different drone transmit power with simulations. figure 3(b) shows the mean local delay of a d2d user against its distance from the ground projection of the drone with different transmit power of the drone and simulations. figure 3 (c) illustrates the network coverage probability after time slot t n for different transmit power of the drone with simulations. from figures 3(a) and 3(b), we can see that our analytical results for link performance provide a good approximation to the simulation. the small gap between them comes from two reasons: (i) ignorance of the inhomogeneity of the active txs inside the d2d user's sensitivity region, and (ii) ignorance of the correlation between the locations of active d2d txs across different time slots, as discussed in remark 1. from the figures, we can see that the gap between the simulation and the analytical results is small with a maximum relative absolute error less than 2.2% which validates the assumption made in remark 1. figure 3 (c) also shows that the approximation we made in remark 2 in the analysis of the network performance provides good accuracy. we only show the numerical results in the later subsections, since the numerical results are verified by comparison with the simulation."
"recall that the link coverage probability after time slot t n, p n cov (x i ) is defined as the probability that the d2d user x i has successfully received the message after n + 1 time slots, which is shown as follows:"
"following the definition of the laplace transform, the laplace transform of the aggregate received signal power distribution at the d2d user x i is expressed as conditioned on the location of the d2d user x i, there are two possible cases for l p n i (s). when x i r c − r d, the laplace transform of the aggregate received signal power distribution at the d2d user equals to"
"insights: figures 6(a) and 6(c) show the link coverage probability at the cell edge user located at 2 km from the ground projection of the drone and the network coverage probability after time slot t n for different radius of d2d sensitivity region. from these two figures, we can find that the link coverage probability of the cell edge user and the network coverage probability after a given number of time slots are higher if the d2d sensitivity region has a larger radius. this is because, there are more active txs located inside the d2d user's sensitivity region if the radius is higher. figure 6 (b) plots the mean local delay of a d2d user versus its distance from the ground projection of the drone with different radius of d2d sensitivity region. the figure shows that the increase of d2d sensitivity radius drops the mean local delay more significantly for the d2d users further away from the ground projection of the drone than for the ones closer to the ground projection of the drone. this is because, with a larger d2d sensitivity radius, the message multicasted by the active txs located closer to the ground projection of the drone reaches the cell edge user within fewer hops."
"however, since only one path in the tree is activated at a given time (i.e., only one of its incoming outputs is nonzero), the output node only uses the one input in practice."
"where (17a) comes from the fact that the aerial link between the drone and the d2d user has a probability p l of being in los and a probability p n of being in nlos, respectively. using the fact that the fading power gain for the los and nlos aerial link, g l and g n follows gamma distribution with parameters m l and m n respectively, (17b) can be worked out by the complementary cumulative distribution function of gamma distribution and we can arrive at theorem 1."
"we report the running time of nnrf and the compared methods. since the running time of nnrf and rf are dependent on the number of trees, for fair comparison, we choose the number of trees of both nnrf and rf to be 150. it is noteworthy that trees of nnrf can be parallelly trained. thus, the running time of each decision-tree-structured neural network (nndt) of nnrf are more important. therefore, we also report the average running time of nndt. the results on valley and isolet are shown in figure 4 . from the figure, we can observe: though the running time of nnrf is larger dnn, it is noteworthy that the average running time of decisiontree-structured neural network in nnrf is much smaller than dnn. thus, if we run these 150 trees in parallel on a machine with many cpu cores, the running time of nnrf can be significantly reduced. in addition, nnrf belongs to neural networks, which suggests that the training time can be further reduced if we run nnrf on gpus."
"like a decision tree, only one path is activated in the neural network at a given time. this particularly important, because it means that the back-propagation algorithm only needs to update the weights of the nodes along this path. this is a crucial property, because it means that one can efficiently perform the updates for a single path. therefore, the training phase for a single neural network is expected to work extremely efficiently. however, like any random forest, multiple such \"miniature\" neural networks are used for prediction. the overall prediction step uses an ensemble approach like a random forest. each test instance is predicted with the different neural networks that have been independently trained. these predictions are then averaged to provide the final result."
"random forests and neural networks share important characteristics in common. both have the ability to model arbitrary decision boundaries, and it can be argued that in this respect, neural networks are somewhat more powerful when a large amount of data is available. however, neural networks are highly prone to overfitting, whereas random forests are extremely robust to overfitting because of their randomized ensemble approach. the overfitting of neural networks is an artifact of the large number of parameters used to construct the model. methods like convolutional neural networks drastically reduce the number of parameters to be learned by using specific insights about the data domain (e.g., images) at hand. this strongly suggests that the choice of a neural network architecture that drastically reduces the number of parameters with domain-specific insights can help in improving accuracy."
"where p d is the transmit power of the active d2d users. h i is the fading power gain between the inactive d2d user and the ith active d2d tx within its sensitivity region, which follows exponential distribution. the fading power gain h i is assumed to be independent during successive time slots. i is the euclidean distance between the inactive d2d user and the ith active d2d tx within its sensitivity region."
"to investigate the effects of the size of randomly selected features, r, on classification performance, we first set d as log 2 c + 1 and n to } is non-integer, we round it to the nearest integer. we only show the results in terms of macro-f1 and micro-f1 on movement and usps as we have similar observations on the other datasets. we conduct fivefold cross validation and the average macro-f1 and micro-f1 are reported in tables 9 and 10 . from the two tables, we make the following observations:"
"it is noteworthy that several methods have been proposed to simulate the output of a decision tree (or random forest) algorithm on a specific dataset, once it has already been constructed [cit] . in other words, such an approach first constructs the decision tree (or random forests) on the dataset up front and then tries to simulate this specific instantiation of the random forest with a neural network. therefore, the constructed random forest is itself an input to the algorithm. such an approach defeats the purpose of a neural network in the first place, because it now has to work with the straitjacket of a specific instantiation of the random forest. in other words, it is hard to learn a model, which is much better than the base random forest model, even with modifications."
"the reason that we design the regularizer to be instance specific is that for every forward computation, each instance x(l, :) will result in one active path, and only those parameters in the active path will be updated using back-propagation (more details in section 3.2). therefore, an instance specific regularizer is more appropriate. α is a scalar to control the contribution of the regularizer."
"similarly, we report the running time of nnrf and the compared methods on regression in figure 5, where the number of trees for rf and nnrf are both 150. nndt means the average running time of neural network-structured decision tree in nnrf. from the figure, we can also observe that the training of each decision-tree-structured neural network is very efficient. thus, if we can run these 150 tree-structured neural networks in parallel on a machine of many cpu cores, then the running time of nnrf can be significantly reduced."
"similarly, nnrf for regression also has three important parameters, i.e., n, r, and d. in this section, we investigate the impact of the parameters n, r, and d on the regression performance of the proposed framework nnrf. throughout the experiments, α is set to be 0.00005."
"in this article, we propose a novel random forest-structured neural network architecture nnrf inspired by random forest. like random forest, for each input datum, nnrf only has one path activated and thus is efficient to perform forward and backward propagation. in addition, the one-path property also makes the nnrf able to deal with small datasets as the parameters in one path is relatively small. unlike random forests, nnrf learns complex multivariate functions in each node to choose relevant paths, and is thus able to learn more powerful classifiers. we further extend nnrf for regression by replacing the softmax layer with linear regression layer and using the euclidean distance as loss function. extensive experiments on real-world datasets from different domains demonstrate the effectiveness of the proposed framework nnrf for both classification and clustering. further experiments are conducted to analyze the sensitivity of the hyper-parameters for classification and regression, respectively."
"in this section, the performance of pid control will be compared and discussed. firstly, the results from table 5.1, which are for the characteristics of the multivariable process outputs after optimising pid control using individually pid controller tuning, will be compared with the results from table 6.1, that shows the characteristic of multivariable process outputs after optimisation process for pid control using multi closed-loop pid controller tuning, to see which of these optimisation methods are better and more successful."
each objective f i is expressed according to the variables of real design x i which influence its value. the multiobjective optimization model obtained with rsm is:
"the table 3 presents the results obtained by the optimization process. the best solution is the number 1, while the 9 other solutions offer alternatives to user. these solutions meet problem constraints and, gives results which minimize \"bdf\" and maximize \"df\" while remaining in the field of each decisional variable. table 3 . optimal solutions"
"1. the selected objective function (itae) should be employed to this multivariable process. 2. genetic algorithms should be checked to be used for g(s) multivariable process and the itae objective function. 3. for solving the problems with typical set-point tracking and loop coupling performance specifications, we can suggest a number of closed loop test which will be in our case two closed loop tests, especially in cases where the process of the controller is non-liner and also different operating point should be evaluated in candidate controllers."
"we chose the terms \"iphone\", \"wp7\", \"android\" and create subnetwork of projects. the graph of this subnetwork (see figure 2) is not connected (contain 243 connected components) and most components are isolated vertices. when we extend the selected terms and create the new subnetwork in the context with terms \"iphone\", \"wp7\", \"android\" + \"silverlight\", than is obvious a importance of the term \"silverlight\" which connect more projects. fig. 2 . extracted subnetwork of projects for selected terms \"iphone\", \"wp7\", \"android\" and the other subnetwork have terms \"iphone\", \"wp7\", \"android\" + \"silverlight\""
"to apply the aco methodology for continuous optimization function problems, the field must be subdivided into a specific area, r, distributed by chance. next, we need to generate feasible solutions representing the initial ants, each forming a part of the research area to be explored."
"since the response surface is described by a polynomial representation, it is possible to reduce the optimization resolution process time by assessing the objectives with their models rather than using more complex empirical models such as those obtained through the fem analysis. although the specific form of response factor f is unknown, experience shows that it can be significantly approximated using a polynomial."
"recently the concept of social networks and online communities is becoming still more and more popular. as a result, the number of their users significantly increasing. reasons for communication between people and creation of social networks in our time are various: study, hobby, work, games and programming is not the exception."
"once this stage is completed, we will have all the equations which make up our multiobjective optimization problem. generally, such problems are as the following form:"
"from figure 4 it can be seen that the output graphs show no overshoot and short settling time which are the system targets. in addition, the optimisation of the second part of the shell oil process is completed. the next step is using the given pid parameters for the two processes g 11 (s) and g 22 (s) to apply them into multivariable process of the shell oil process as will be shown in next section."
"after the operation of the genetic algorithms is finished, we can have the following results. figure 14 shows the optimisation process for second loop of pid controller, and also it gave the minimum objective function which equals 1676.88. this value of objective function can be achieved with parameters of the second pid controller equals. by this result, the first and second closed-loop tests are completed. moreover, to test the pid controller for two closed-loop control, the achieved pid controllers parameters form second optimisation method in closed loop test 1and 2 should be applied. the results can be shown in two ways."
"we can immediately notice that even though \"modder\" and \"raouf\" do not participate on many projects with \"raja4567\" (they have one common project), the atributescore is 0.01176471 and 0.01204819. for example \"shankar00\" participate on 2 projects with \"raja4567\" and the atributescore is 0.02469136. user \"modder\" has only one common project with \"shankar00\", but atributescore is strong, probably because \"shankar00\" not cooperate with many other persons."
"the transfers function of first part of shell oil process g 11 (s) has been used in previous section with itae objective function. moreover, the minimum value of the objective function was equal 196.22. the pid controller parameters were equal;"
"this section shows a method of an automatic tuning for parameters of pid controller. in addition, this method can optimise pid control for any process, shell oil process for example. the method is briefly that applying the genetic algorithms with the chosen objective function, which is mentioned in previous section, to single part of shell oil process. as mentioned in section 2.1 the shell oil process has two singles process g 11 (s) and g 22 (s), and also each process gets pid controller. so, in this chapter the optimal process of pid control for shell oil process will apply to the two singles process g 11 (s) and g 22 (s). after optimising process, the given parameters of the both pid controllers will be tested to the multivariable process of the shell oil process. finally, the test results will be used in next section for comparing with another method's results."
"the action steps of this method are the same in the previous section. however, there will be some changes in the itae objective function. in previous section, the equation of itae used one value of the error which was calculated from one output for each closed-loop test but in this section the itae equation will use two values of error, the error values from y 1 and y 2 as shown below."
once the steps used in making a choice regarding the elements to be included in the resolution process are explained. we present the new proposed algorithm for our approach (see fig. 2a and fig. 2b ).
"in this section, we should note that, the set-points of the first closed-loop test should be ! ! ! ! !!!!! ! ! ! !, but the opposite should be applied to another closed-loop test."
"many programmers on the internet are looking for interesting ideas, or assistance when implementing their own solutions. online collaboration is no longer a novelty in our times and it is run by people all over the world. however, searching for suitable and capable people who could implement a particular idea at reasonable deadlines and high quality is an eternal problem."
"after running the genetic algorithms via ! !! !!!, we have got the results below: the best itae objective function for this optimisation process is equal (62.1068) and this value is achieved by using the following parameters of the pid controller."
the itae objective function was chosen as the main performance criterion for the remainder of this project. because it has a shorter settling time and overshoot than any other method in the whole group.
"numerical optimization is the search approach we adopted in order to determine the best mechanical design. several algorithms related to the problems at hand were developed, most of them single-objective. however, given the complexity of the products involved and the multiple objectives of the design considered, the researchers focused on the optimization algorithms for such problems. in short, the optimization problems have multiple objectives, and in many cases, there are multiple constraints. design processes often require expensive evaluations of objective functions. that is particularly the case when such performance indexes and their constraints are obtained through intermediate simulations by finite elements involving fine meshes, many freedom degrees and nonlinear geometrical behaviours. to overcome these difficulties, the response surface method (rsm) is employed [cit] to replace a complex model by an approximation based on results calculated on certain points in search space."
"in our field, the aco has been used very sparingly, and has been focused primarily on single-objective problems [cit] . for multiobjective problems, the aco has hardly www.intechopen.com been used at all [cit], and when used, has been mainly on combinatorial optimization problems. the importance of this work therefore lies in its attempt to adapt continuous ant colonies to multiobjective problems. fig. 1 . experimental setup and drawings of the selection of short branches by a colony of \"linephitema humile\", 4 and 8 min after the bridge was placed [cit]"
"from graphs in figures 6, 7 as individually tuning and figures 11 and 12 as multiloop tuning, we can have the following table: table 2 . the comparisons between individually and multi-loop pid controller tuning."
the approach we present makes it possible to effectively optimize a mechanical design problem. the approach performs much better when compared to using the desirability function. the results of the application allow it to validate the suggested design optimization method.
"additional information on optimization, as well as the goals of the study, is summarized in the following table 2 . constraints on objectives of study"
"the simulation process will take two methods and each method will have two closed loop test. the first method is running gas with itae objective function where the error will be calculated from y 1 in case of the first closed loop test, but in case of the second closed loop test the error will be calculated from y 2. furthermore, the second method of this simulation process the itae objective function will be calculated from both errors e 1 and e 2 for both cases of the closed loop test."
most projected solutions to the shell standard control problem currently published utilise the state-of-the-art quadratic dynamic matrix control (qdmc) algorithm developed by shell [cit] .
"in other words, terms or description of the project may not only help us to provide more information about projects, but also to determine the user's area of interests or abilities. as a result, the way we are able to compare user attributes determines the similarity to other network participants."
"using activities such as user links to the projects, we are able to determine with some probability an area of specialization and a work of each user. for example, if a user is working on three projects written in .net and one in java, we could include him in .net programmers with high probability, and less likely recommend him as a java programmer."
"oss (open source software) is a example of a dynamic network, as well as a prototype of complex networks emerging on the internet. by working through the internet, interactions between developers can be considered as relations in the synthetic network of collaborators. these relations arise when the developers join the project and begin to communicate with others. oss network consists of two entities -developers and projects. an examples of such oss social network established on the basis of interaction between the participants is codeplex."
"the objective of this chapter is to determine the best design for a mechanical system such as a plane wing, an engine, etc., or for an unspecified mechanical process that sometimes simultaneously optimizes several conflicting objectives. the aco, like the ga, requires an objective function which can be quickly assessed. we use rsm modeling to determine such objective functions, and the aco as the research method. reducing the resolution time in the optimization process requires a reduction of the preciseness of the assessment of objective functions, since we use an approximate modeling of our objectives instead of their exact representations."
"the result is a set of optimal pareto solutions. we present more than one solution to the user in order to provide him with a margin of makeover. abdul-wahab & abdo [cit], in their paper, present their 10 best solutions. we will do the same in order to make some comparisons."
"trail diffusion: in this step, the field of the global search is gradually reduced, as the search progresses. this reduction makes it possible to increase the probability of locating the optimum through more concentric search procedures. trail diffusion is similar to the arithmetic ga crossover. in this step, two parents are randomly selected from the parent population space. the elements of the child's vector can be any one of the following: 1. the child corresponds to an element from the first parent 2. the child corresponds to an element from the second parent 3. the child is a combination of the parents (eq.10) [cit] "
"evaluation of the whole relationship context of two persons d i and d j has two steps. first, we compute association between d i and select term t k, and between the second developer d j and t k separately. afterwards, because each part is already evaluated by real number, we combine both results in the same way; we can combine the whole result in equation one. in codeplex we see the description text for the developer as the all description of all projects he is working on, joined together. we obtain equation for the context score :"
"this equation evaluates the relation between developers depending on the selected words, which represent the context. so we get a evaluation for the new subnet, which is specified by selected terms."
"second part of figure 1 shows graph of the connected component which contain selected and highlighted developers. we can see that selected developers are not in the one community of fig. 1 . synthetic collaborators network for the terms \"iphone\", \"wp7\", \"android\" and selected subnetwork with developers \"modder\", \"raja4567\" and \"raouf\" collaborators. they are connected, but the relation is too weak. they are not suitable for recommendation. we used our algorithm for spectral clustering [cit] and we detect communities of more collaborated developers. than we can recommend the \"green\" community, which contain developers with the stronger relation in the context of selected words."
"the results were limited to the collaborators with whose the person has collaborated together on the project at least once. we show centrality value of selected nodes in the table 3 . these centralities characterize the position of vertices in the network. we show in the table 4 values of attribute score for person with nickname modder, in the table 5 for person with nickname raja4567 and in the table 6 for person with nickname raouf."
"the objective function is as an indicator of how well individuals perform in domain of the problem. in the minimization problem case, the smallest value of the associated objective function will be the reference for the fittest individuals. this raw measure of fitness is usually only used as the stage of intermediate to determine the relative performance of individuals in genetic algorithms."
"in this method we will have two closed loop test as mentioned in previous, to start with the first closed loop test we should follow the steps below."
"the above table (see table 3 ) present the 10 best results of our study. these solutions meet the constraints of the problem and give excellent results which minimize \"bdf\" and maximize \"df\" while remaining within the confines of each decision variable. it's interesting to observe the values of the decision variables in their respective fields. we can see that these best solutions are obtained under the following conditions (see"
"where r is a random number from 0 to 1, r is the maximum step size, t is the ratio of the current iteration number and that of the total number of iterations, and b is a positive parameter controlling the degree of nonlinearity."
"when an adequate model is obtained with the rsm approach, it then becomes necessary to consider the optimization step. the method used to find the best solution assesses several objectives simultaneously; since some such objectives are fundamentally conflicting vis-à-vis of another, we therefore need to establish a compromise. existing literature shows that desirability or metaheuristic functions are normally used, the most common being the genetic algorithm (ga). sun & lee [cit], present an approach which associates the rsm and ga with the optimal aerodynamic design of a helicopter rotor blade. the aco is a metaheuristic, which has been successfully used to solve several combinatorial optimization problems. we however see that very little exists in terms of documentation for optimization using aco, as far as multiobjective problems are concerned. some works lead us to believe that ant colonies can produce an optimum situation faster than the ga [cit] . in the literature, acos are used almost exclusively for \"travelling salesman problem\" (tsp), quadratic assignment problem allocation (qas),"
the objective of pid control design is to define the parameter of the pid controller ! ! ! ! ! !!!! ! !to meet a given set point of close loop system performance requirements.
"the iae and itae objective functions were created using the genetic algorithms. the result of this optimisation process is shown below.! from this optimisation of pid control, the best or minimum of the objective function (itae) is equal (196.22) . moreover, the pid controller parameters, which gave this value of the objective function, are equal: table 1 explains the steady state characteristics such as rise time and overshoot of the outputs of the first part of shell oil process for each case of the objective functions. moreover, the results of the pid control tuning using the zeigler-nichols method for the first part of shell oil process will be compared with other results. under the conditions of this experiment, it can be seen that the iae and itae objective functions perform almost identically. they have shorter settling times and overshoot. however, they have a longer rise time than the other controllers."
"in the following problem two discrete-time pid controllers both with integral antiwindup loops and derivative term filtering were used to provide the integral actions in order to reach the requirements of regulation to outputs y1 and y2. the manipulated variables u1 and u2 were chosen for closing the two loops of pid controller but u3 has additional requirements of minimisation, so will not be used in the loops [cit] . the inputs u1 and u2 are used to control the outputs y1 and y2, occur logically from process operation considerations [cit] . the 2 ! 2 matrix of transfer function, which illustrates the shell oil process, is shown below: (3) and (4) the whole structure of the shell oil process can be illustrated, as seen below. by studying the equation ! ! !!! above it is clear that the best pairing of manipulated and controlled variables is to control output y1 with input u1 and output y2 with input u2. moreover, the gains in the main diagonal of ! ! !!! are adequately big enough to ensure that the exchanges between the two loops will be smallest."
"the undirect connection between the user and the project is implemented through activities within the scope of the project. these activities are in the database codeplex table 2 ). we can represent codeplex as a bipartite graph of users and projects, where the edge between the user and the project is a user's activity in a project."
"after running genetic algorithms with suitable populations and generations. the results are shown in figure below. figure 9 shows the optimisation process for the first loop of pid controller, and also it gave the minimum objective function which equals 1470.42. this value of objective function can be achieved with parameters of the first pid controller equals. to test the pid controller for two closed-loop control, the achieved pid controllers' parameters in closed loop tests 1and 2 should be applied. then, the results are shown below in two ways."
"in the case of two factors, the linear regression model is one of the simplest available, and corresponds to a first-degree model with interaction, and which has the following form:"
each of the pid controllers which are tuned by the genetic algorithm outperforms to the pid controller tuned by ziegler-nichols method in terms of rise time and set- tling time but only the iae and itae objective functions overtake it in terms of overshoot.
the experiment will be undertaken to investigate which of the four objective functions gives the best results when used in combination with the genetic algorithms. each of these objective functions will be created for each individual performance criterion.
"in section 2; the design of shell oil process was presented with their 2!2 transfer function matrix as shown in equation 2.1, which will be used for multivariable process of shell oil. in the two previous sections the suitable parameters of the two pid controllers were achieved. in this section the multivariable shell oil process will be tested with two pid controllers. to simulate this multi input multi output shell oil process, we should use the matlab/simulink. the simulink model of this process is shown in following figure. to test the performance of the pid controller for this closed loop multi-process, we should follow the steps below."
"codeplex is microsoft's open source project hosting web site. you can use codeplex to find open source software or create new projects to share with the world. codeplex.com has 11 years old, it is ranked 2,107 in the world, a low rank means that this website gets lots of visitors. its primary traffic from united states and is ranked 3,175 in united states. it has 104 subdomains with traffic. it has 136,500 visitors per day, and has 436,800 pageviews per day. codeplex is mainly used by developers for collaboration on projects, sharing source codes, communication and software development. generally, registered users can participate in multiple projects, discussions, adding the source code and documentation, issue a release, etc. some of the users have defined a specific role within the project for which they work. each user has his own page, where he can share information about himself, his projects on which he currently works, and the most recent activities. the codeplex projects themselves can be considered as a very interesting source of information. in addition to the list of users and roles, codeplex enables register keywords, add description of the project, the number of visits, status, date of creation, url and other information about the project. all activities are carried out on codeplex by a particular user within a specific project."
"in figure 13, the optimisation process for first loop of pid controller is realized, and also it gave the best value of objective function which equals 3350.55, as mentioned above this value of objective function is sum of two errors (e 1 & e 2 ). in addition, it can be achieved with parameters of the first pid controller equals. by this result, the first closed-loop test is completed. next, the second closed-loop text should be run after some changes are required in the genetic algorithms m-file."
"database which was created as a result of data obtained from codeplex.com, consists of 6 main tables: user, project, discussions, recentactivity, membership and sourcecode (see table 1 )."
"in this study, two performance objectives are considered: the maximization of the distillate produced rate (df) and the minimization of the blow down flow rate (bdf). the operation variables which influence these objectives are presented in"
"abdul-wahab & abdo resorted to a two-level factorial design, carried out 64 experiments and five central-point tests with design variables coded on two levels: low (-1) and high (+1). the experimental design provides us with a linear regression model coded for each response in this study (see fig. 3 & fig. 4) . finally, the equations representing our objectives are:"
"to optimize this problem, we explored the ant colony algorithm (aco). some options are offered for this kind of problem, such as the desirability function and the genetic ga used by some authors, such as sun & lee [cit] or abdul-wahad & abdo [cit] . the literature shows that for many problems, the ant colony approach produces better results in terms of quality solutions and resolution speed, as compared to the ga. this allowed us to begin this research with the resolution of the multiobjective continuous optimization problem in mechanical design."
"in this section, a multi-loop pid controller will be optimised for multivariable process of the shell oil. after transferring the control problem into function optimisation problem using the objective function j itae and three parameters of pid controller loops in the closed loop of multivariable system."
"to express our objectives according to decision variables, we need to use modeling with rsm (steps 2 and step 3). we considered the experiments carried out by abdul-wahab & abdo [cit], which helped us to design our model."
"where is a uniform random number ranging from [0 to 1] the probability of selecting one of the three options depends on the mutation probability. thus, if the mutation probability is 0.5, option 3 can be selected with a probability of 50%, whereas the probability of selecting option 1 or 2 is 25%."
"some related work dealing with the recommendation in the social network. in the article [cit] authors studies people recommendations designed to help users find known, offline contacts and discover new friends on social networking sites. other approach is in the article [cit], where authors examine the dynamics of social network structures in open source software teams but data were extracted monthly from the bug tracking system in order to achieve a longitudinal view of the interaction pattern of each project."
"1. apply the parameters of the pid controllers, which have been found in the previous sections. 2. for multi-process we need first apply the set-point (r 1 (t)) into the first input (u 1 (t)), to realise the performance of the first output (y 1 (t)). however, there is no set-point applied to second input (u 2 (t)). by running the multi-process for 300 seconds, the outputs graphs will be shown as figure 6. 3. to test the performance of the second output (y 2 (t)), the set-point (r 2 (t)) should be applied into the second input (u 2 (t)), but the set-point (r 1 (t)) should be zero. after the multi process is generated for the same time in step 2, we can have the following figure 7. shell oil subsystem3 g12"
"1. apply the set-point into the box of the set-point r 1 with no set-point in r 2 . 2. to active pid controller of the second loop, we will apply the given parameters of the second part of the shell oil process g 22 (s) from section 5."
"ideally, the number of experiments carried out, either with the finite element model (fem) or using other approaches, during the application of rsm, should be as small as possible, in order to reduce data-processing requirements. properly selecting the points to be used for an approach-based on response surfaces method and ant colony system for multi-objective optimization: a case study 53 the simulation will allow a reduction of the variance of the coefficients of the mathematical model, which will in turn ensure that the response surfaces obtained are more reliable. to that end, we need to determine the experimental design to be adopted in order to obtain the most interesting simulation for this problem. the central composite design (ccd) was employed in the case of the second-order response surface, but other types of plans, such as the complete factorial design and the fractional factorial design, are also available for use."
"after choosing its destination, the ant proceeds across a short distance. the search direction remains the same from one local solution to the next as long as there is improvement in the fitness function. if there is no improvement, the ant reorients itself randomly to another direction. if an improvement in the fitness function is obtained in the preceding procedure, the position vector of the area is updated. the quantity of pheromone deposited is proportional to the improvement of the fitness function. if, in the search process, a higher fitness function value is obtained, the age of the area is increased. this age of the area is another major parameter in the caco algorithm. the size of the ant displacement in a local search depends on the current age. the search ray is maximum for age zero, and minimal for the maximum age, with a linear variation."
"the author wishes to thank the co-authors of this chapter. specifically, miss estinda-mpiga for his major contribution and the quality of work. thanks to all colleagues who have contributed from time to time, partially or more to obtain this manuscript."
"this project examined the possibility of the use of genetic algorithms for the optimal solution of control problems in framework of the function optimisation, which focuses on control of multivariable process. some indexes of performance and controller tuning methods were analysed and developed. moreover, the genetic algorithms were extended for enabling them to identify multiple solutions of optimal equivalent for a given problem."
"the genetic algorithms will be used for each objective function. the genetic algorithms will be adjusted with a population value of fifty and a generation value of fifty. the gas are applied to tuning the pid for the first part of the shell process g 11 as shown in equation 2. moreover, two examples of objective function performance will be presented."
"in codeplex, we can see two types of entities: users and projects. both are represented by tables that contain specific characteristics. the table user contains informations about users such as login, personalstatement, createdon, lastvisit and url of user page. the table project contains some characteristics of project in codeplex: tags, date od created on, status, license, pageviews, count of visits, description and url of project page."
"the percentage of global ants is an important parameter of caco, which can be changed depending on the problem at hand. a global search creates new solutions for \"g\" by replacing the weaker parts of the existing field. this process is composed primarily of two genetic operators. in the terminology of caco, these are called random walk and trail diffusion. in the random search process, the ants move in new directions in search of more recent and richer sources of food."
"then, genetic algorithms will be run to optimise pid control for the first loop of pid controller, and the outcomes are shown in following figure."
"the aco metaheuristic, called the ant system [cit], was inspired by studies of the behaviour of ants [cit], as a multi-agent approach for resolving combinative optimization problems such as the tsp."
"multi secondly, after the comparisons between individual optimisation of pid control and multi-loop pid controller optimising is achieved, the two methods of optimisation process for multi-loop pid controller will be compared and dissected. the following table shows some differences for these two methods. as we have seen in above comparisons table, there is an improvement in characteristics of the outputs for the second optimisation method rather than the first optimisation method. moreover, the itae values are the smallest in case of the second optimisation method. because in first method of optimisation is just minimising the itae objective function form one kind of error which is calculated from one output (y 1 or y 2 ), rather than the second optimisation method which used to minimise itae for two errors e 1 and e 2 which was calculated from y 1 and y 2 ."
"whenever we think about collaboration between two persons, we not only look at the relationship itself, but also at the context. it is clear that depending on context, the strength of relationship changes. therefore, we divide collaboration into two main parts developers' relationship and developers' context. we consider the relation between developers and the term describes the context between developers."
"in this paper we try to determine the strength of relationship or similarity between codeplex developers in the context of projects they work on. to determine the context, we used project key words, which in the case of the codeplex are extracted from project descriptions. we would find some developers or some community, which is specified by key words, for a recommendation."
"this book chapter presents a new multiobjective optimization approach for mechanical system design. various techniques have traditionally been employed to resolve this kind of problem, including an approach combining rsm, ga and a simulation tool such as fem. we have the aco, which allows the exploration of a combination which includes another optimization algorithm. the aco captured our interest because we were able to note in various works that in multiobjective optimization, it does produce better results than the quadratic programming technique and the ga. the aco thus appears to be an innovative and leading solution for design optimization, because it is completely generalized and independent of problem type, which allows it to be modified in order to optimize the design of a complex mechanical system, subject to various economical and mechanical criteria, and respecting many constraints. however, it must be recalled that the aco was developed to resolve discrete problems, and that its use on continuous problems is constantly under development; our study contributes to the development of the continuous aco for multiobjective problems."
"the performance of pid control as shown in section 3 was shown that the design of the pid controller's parameter using genetic algorithms has better results in the characteristics of outputs, such as rise and settling time, than using the classical methods. however, the classical methods, such as zieger-nichols tuning method, are good for giving the start point of determination for the parameters of pid controller. in addition, section 3 displayed that the analysis of objective functions using the comparison of their performance criterion for optimising the pid control optimising. this comparison was decided that the integral of time multiplied by absolute error (itae) performance criterion based objective function produced the most effective pid controllers when was compared with performance criterion of other objective functions, such as ise, itse and mse."
"we describe a developers' relationship as commutative operation on cartesian product of developer's attribute x x x, where output is mapped to the set of real numbers r."
"firstly, to optimise pid control for the first pid controller, as mentioned in section a, the first closed-loop test should be achieved. however, we should note that the parameters of second pid controller which have been achieved for process g 22 (s) in section 5 should be applied, as in previous section."
"in section 4, the method for automatic tuning of pid controller for single-input, single-output (siso) was presented, and also the suggestion parameters of the pid controllers are applied to multi-input, multi-output (mimo) for shell oil process. moreover, it not easy to optimise the pid control for multivariable process, but in sections 4.2 and 4.3 the genetic algorithms have successfully employed to optimise the multi closed-loop of pid control. the comparison table 2 shows that the optimisation of pid control for multivariable process produces better performance than the pid control optimisation for single process."
"secondly and always in table 4 following this application, and having obtained appreciable results, we can conclude that our algorithm functions correctly, while leading to coherent solutions, and that it has proven its effectiveness by obtaining better solutions than those of the authors, abdul-wahab & abdo [cit] ."
the projected pid tuning method is working by decreasing the objective function j itae which is the function for the parameters of the pid controller related with pid tuning problem.
"if we look at the data that we have in the table user, we are not able to define the user's profile. it consists of the field of interest, what he deals with, the programming language he uses and at what level. personalstatement attribute is used to describe the user, but from the total set of our users downloaded, there was not a single one, who would fill it up. on the other hand, the project has enough information defined -which fields are concerned, how long it lasted, whether it is completed, which technology it is used, etc."
"in a local search, the local ants choose the area to be explored among the areas of the matrix r, according to the current quantity of pheromones in the areas. the probability of choosing a solution \"i\" is given by: [cit]    "
"a comparison between the results obtained with the desirability function (\"df\") and the hybrid approach developed (\"df/caco multiobjective\") shows that the second gives better quality results. firstly, by observing the change in the response values we obtain for the various solutions (see table 4 ), we can see that the solutions achieved with the hybrid approach vary much less than those obtained with the desirability function of abdul-wahab & abdo [cit] . it seems that our solutions are closer to each other. the reason is that the hybrid approach causes small displacements during the ant's research process. thus when the fitness function decreases, the ants move over a short distance before re-test the function, if and only if, the obtained value is better than the previous one. otherwise, the process reorients itself in case of declining performance."
"more than 90% of all control loops involve pid controllers [cit], due to their simplicity and effectiveness in use they are used in many industrial applications [cit] . they are thought to be the most popular controllers used in process control today. [cit] s has had its associated limitations and disadvantages."
"shell oil subsystem g11 this section explains and describes a new technique for the automatic tuning of pid controllers for multivariable process, based on the genetic algorithms. the main advantage of this technique is that it allows the engineers to explicit specify the specifications of the required performance for a given problem of multivariable control, whereas the time-domain limits on the closed loop responses. this can be realised by designing a function optimisation problem from the control problem, using the objective function j itae which was described in section 4. next, the genetic algorithms will be employed to minimum the objective function j itae, in order to optimise pid control for multivariable process. in addition, the projected method can be valid to a wide range of multivariable processes. simulation results will be presented and compared with the simulation results from the previous chapter."
"step 1: system configuration determine the objectives of this study, the constraints and the variables which can influence these objectives. evaluate the field of application of these variables."
"in order to illustrate the performances of the recommended resolution approach used in this paper, we carried out the optimization of a multistage flash desalination process. the problem was taken from abdul-wahab & abdo [cit], and was solved using the experimental designs, and optimized using desirability functions."
"in the optimization design problem for a mechanical system, the number of design variables is very often equal to or higher than 3, and each one of them has a broad field of variation. consequently, in our resolution process, it is possible for the search field for each design variable to be gradually narrowed for as long as the stop criterion has not been encountered."
"the unknown parameters of this mathematical model, i values, are estimated through the least-squares technique, and the adjustment quality of the model is assessed using traditional multiple linear regression tools."
mutation: the replaced solutions are further improved by mutation. the mutation step i s c o m p l e t e d i n c a c o b y m a k i n g a n a d d i tion or proportional subtraction to the mutation probability. the mutation step size is reduced or increased as per eq. 9. [cit] (
"whenever this model is unable to describe the experimental reality effectively, it is common practice to use a second-degree model, which includes the quadratic effects of the factors involved: 22 01 12 21 2 1 21 1 12 2 2"
"using the matlab software (step 5), figure 6 allows us to say that we get the best solution after 310 iterations. the staircase shape of the curve (fig. 6 ) is explained by the memory effect that we used in the program code. thus, when iteration produces a worse solution than the last one, this last solution (previous iteration) is retained."
"at first, we have calculated the keywords for the \"modder\", \"raja4567\" and \"raouf\". we have selected only the some terms for illustration (see table 7 ). for comparison we marked some terms (bold text), which was used as a context between developers. in the figure 1 is whole network of collaborators for the selected terms \"iphone\", \"wp7\", \"android\". the edge weights are evaluated by score. this subnetwork has 199 connected components (communities) with collaborating developers."
"research presented in this article is oriented to the strength extraction between persons based on their context in the codeplex. the method was presented using the data collection from the codeplex database, which contains information of the activities of developers in the project. the proposed method is usable for the development of collaboration network. the description of this network is based on the set of terms (as the input), which are used in the description of projects by the given developer. using this method, we have obtained the new weight in the synthetic collaborators network. by means of the set of selected term, belonging to one (or more) persons, we can construct the subnetwork with only the context-related collaborators. this subnetwork can be very helpful in searching of the persons who are interested in the same area, defined by the selected term. it is usable for members of the project management, who need to find suitable developers specialized to certain area."
"thank you to the mechanical engineering department members at the \"école de technologie supérieure\" in montreal and those of the management sciences department at the university of quebec in abitibi-témiscamingue (uqat), canada."
"cpa is an iterative approach. resources are analyzed by a local analysis based on the busy period approach [cit] . in this approach, new output event models for each task are derived from a critical instant scenario. this scenario maximizes the response time of the currently analyzed task by activating interfering tasks with their worst-case activation (and execution) pattern. the response time jitter (maximum minus minimum response times) can be used to derive new output event models [cit] . a global analysis loop propagates event models between dependent tasks. the analysis ends, if all event models reach a fixed-point (do not change anymore). the analysis also ends if predefined constraints (e.g. number of analysis iterations or end-to-end latencies etc.) are violated, in which case the system is considered to be unscheduable."
"sdn network control is stream-based (also called flowbased in sdn literature). a traffic stream is defined to be a sequence of frames between a source and a destination, which receive identical service policies in the network [cit] . in this paper, we assume that the data plane (inside the switches) is controlled by flow tables similar to openflow's [cit] . each flow table entry comprises three fields: rule, action, and statistics (frame counters etc.). incoming frames are matched against the rules to determine the traffic stream they belong to, e.g. by their mac and/or ip addresses, vlan id, or tcp/udp ports. rules are implicitly prioritized by their order of appearance in the flow table to resolve ambiguities. actions define how a frame should be treated. possible actions include: forward the frame to a specific switch port, drop the frame, and request further instructions from the sdn controller on how to handle the frame. network configuration in sdn is the process of creating and distributing these flow table entries, e.g. when a new request has been received by the sdn controller."
"the speed up results shown in table 7 and figure 9 are calculated from both table 5 (the speed of training phase), and table 6 (the speed of testing phase) using equation (7) . here, we can see that the speed of dt4 in training phases is almost similar to that of the fpbst, this is due to the similar trees created by both methods, however the speed of the dt4 in the testing phases is significant 1.27 times of the fpbst testing phases on average, this is due to the disuse of the knn by dt4. it is interesting to note the high speed of the proposed dt0-dt3 methods, which is about one and half times faster than the fpbst, which might be due to the smaller size of the resultant trees and the disuse of the knn. it is also interesting to note that the training speeds of the proposed dt0-dt3 are not significant as their testing speeds; this is due to the extra time which is needed for the extra computations of the probabilities of the classes in each node."
"similar to the fpbst, the time complexity of training phase to build the decision tree (dt) by the proposed methods (dt0-dt4 and dt0+ to dt4+) is:"
"where the space consumed (s) is a function of n and d, which similar to the size of a normal bst. the test phase of the proposed method (algorithm 2) is the same for all the dts, as it searches the created dt for a test example starting from the root node to a leaf node, where similar example(s) are supposed to be there. however, it is different from the test phase algorithm of the fpbst, where knn algorithm is employed to classify the test example using those found in a leaf-node. the proposed dts have no need to use the knn, because the leaf-node has become able to decide the class of the tested example based on the pre-calculated probabilities it has, since the name (decision tree) suggests. disusing the knn with the proposed dts allows for more speed. therefore, the time complexity of the test phase of the proposed dts for each tested example is:"
"we discuss the explicit flow configuration protocol in figure 2a, exemplified by admission control. in an admission control scenario, traffic streams are either allowed to enter the network (forward action), blocked (drop action), or must request admission before entering the network (request action). we are interested in the worst-case timing behavior of the request action. when the first frame (frame 1 in figure 2a ) of an incoming traffic stream arrives at switch 1, this switch determines (based on its current flow table) that this stream requires admission control and generates a request (req) addressed to the sdn controller. the sdn controller receives the request and checks if this stream is allowed to enter the network. in case the stream is denied network access, the controller notifies switch 1 immediately (not shown in figure 2a ). if the controller decides to grant network access to the stream, the network must be (re)configured to support this new traffic stream. for this, the sdn controller sends configuration messages (conf ) to all involved switches to update their flow tables. these configuration messages can carry multiple flow table updates, so that, for example, new forwarding rules for the requesting traffic stream and dropping rules for other (now obsolete) traffic streams can be distributed concurrently. each switch must confirm this update via an acknowledgment (ack) message to the sdn controller. the controller must wait for all acknowledgments (hatched time interval in figure 2a ) before it finally sends the confirmation (en) to switch 1 to enable the flow table entries, which allow the requesting traffic stream to enter the network."
"the problem of big data classification is similar to the tradition classification problem, taking into consideration the main properties of such data, and can be defined as follows, given a training dataset of n examples, in d dimensions or features; the learning algorithm needs to learn a model that will be able to efficiently classify an unseen example e. in the case of big data, where n and/or d are very large values, tradition classifiers become inefficient, for example the k-nearest neighbors (knn) [cit] took weeks to classify some big data sets [cit] ."
"observe that, by lemma 3, n in evidence normal form is a cpt. we may denote evidence normal form γ · n simply as n with evidence γ understood, since γ only serves to select configurations of n agreeing with the evidence. we now turn to denoting semantics."
"the experimental results show that the proposed decision trees improve the performance of the fpbst in terms of classification accuracy, training speed, testing speed and the size of the model (the tree in our case). we also made another simple enhancement on the fpbst algorithm in the training phase, which is the swapping of the furthest pair of points based on their classes rather than being based on their minimum/maximum euclidean norms. this makes the resultant decision tree more coherent in terms of the distribution of the classes, making them closer to each other as possible as could, such enhancement further improved the accuracy of the proposed decision trees compared to that of the fpbst as the results suggest."
"first, the stage of feature extraction is considered, since the column basis matrix and row basis matrix with orthogonality are regarded as orthogonal projection matrices in the k2dnmf method, so and are used for feature extraction operations. for any given new image sample, the feature matrix can be written as:"
"e advantage of feature mapping is that it can transform the nonlinear relationship of sample data in low-dimensional space into a linear relationship in high-dimensional space [cit] . in addition, by introducing a kernel function, one can avoid carrying out the feature mapping and compute the inner product in the feature space. more knowledge about feature mapping and kernel functions are introduced in the following part of this section."
"us far, if a matrix whose initial value is nonnegative, a pair of nally converged nonnegative matrices and can be obtained by repeated iterations. e expression of the column basis matrix can be obtained by :"
"in this theoretical paper, we present semantics in inference (si), an algorithm for denoting semantics during exact inference in discrete bayesian networks. si works by introducing the notion of evidence normal form to organize how each potential was constructed. si then decides semantics of the potential by performing one d-separation test. formal properties of the si algorithm are obtained, namely, polynomial time complexity, soundness, completeness, and strong completeness. si can be utilized for clarity of exposition in bayesian network literature, since the semantics of potentials can now be articulated."
"the speed up results shown in table 7 and figure 9 are calculated from both table 5 (the speed of training phase), and table 6 (the speed of testing phase) using equation (7). here, we can see that the speed of dt4 in training phases is almost similar to that of the fpbst, this is due to the similar trees created by both methods, however the speed of the dt4 in the testing phases is significant 1.27 times of the fpbst testing phases on average, this is due to the disuse of the knn by dt4. it is interesting to note the high speed of the proposed dt0-dt3 methods, which is about one and half times faster than the fpbst, which might be due to the smaller size of the resultant trees and the disuse of the knn. it is also interesting to note that the training speeds of the proposed dt0-dt3 are not significant as their testing speeds; this is due to the extra time which is needed for the extra computations of the probabilities of the classes in each node."
"algorithm 1 shows the pseudo code for the training phase of the fpdt method, which works well for dt0, dt1, dt2, dt3, and dt4 depending on the input (dtype), and algorithm 2 shows the pseudo code for the testing phase of the fpdt method, which is the same for dt0, dt1, dt2, dt3, and dt4, as these methods differ in the way of creating the decision tree only, i.e., the training phase. the training phase of the fpbst and the new dt0-dt4 use the euclidean norm to regularize the resultant tree by swapping p1 and p2 if the norm of p2 is less than that of p1 (step 21 in algorithm 1). this is normally done to let the examples, which are similar to the point of the least norm to be sorted to the left side of the tree, and the others to be sorted to the right side of the tree, so as to have similar examples adjacent as possible as could in the resultant bst. having known that the euclidean norm is sensitive to the negative numbers (negative and positive similar numbers result the same euclidean norm), the examples with many zeros or similar repeated numbers [cit], we opt for an alternative of the norm to decide which goes to left and which goes to right. here we propose the use of the class of the example, so we check the classes of p1 and p2 to see if p2 has the minimum class, if yes, we swap p1 with p2, otherwise they remain as they are. such a swap allows for regularizing the resultant decision tree with the minimum cost, as creating the norm cost extra o(d) each time, while obtaining the class of an example costs o(1), and at the same time we get more coherent trees in terms of the classes distribution, since the examples of minimum class are forced to be sorted to the left and those with the maximum class are sorted to the right, this might have a good effect on the probabilities of the classes. this improvement is applied on all the proposed dt0-dt4 making new decision trees dt0+, dt1+, dt2+, dt3+, and dt4+."
"its centralized architecture allows sdn to realize globally optimal network management based on the current network state. as each switch communicates directly with the sdn controller, there is no need for complex (and potentially lengthly) distributed network control protocols (e.g. (rapid) spanning tree protocol or shortest path bridging), which first have to reach a common understanding of the network state before they can take appropriate actions. [cit] research and innovation programme under grant agreement no 644080. centralized network management include admission control (e.g. to prevent/limit network congestion by blocking nonor less-critical traffic) and run-time reconfiguration (e.g. to recover from faults in safety-critical networks by rerouting critical traffic around a failed component or link). the latter becomes increasingly important with the advent of highly automated and autonomous driving, where current fail-safe safety strategies must be replaced by fail-operational ones."
"most of the proposed work in this domain is based on divide and conquer approach, this is a logical approach to use with big datasets and, therefore, most of these approaches are based on clustering, splitting, or partitioning the data to turn and reduce the very large size to a manageable size that can be used for and efficient classification. one major problem associated with such approaches is that the determination of the best number of clusters/parts, sine more clusters means fewer examples and, therefore, faster testing. however, fewer examples also means less accuracy, as the examples found in a specific cluster might not be related to the tested example. on the contrary, few clusters indicate a large number of examples per each, which increases the accuracy but slows down the classification process if the knn is used."
"the idea behind accumulating the probabilities is to remove the effect of unbalanced datasets, as some datasets contains more examples of a specific class than the other classes, and this will increase the probability of the dominant class, since it is calculated in the root node and accumulated along the depth of the tree, so by moving deeper, less number of the dominant examples remain."
"each step of this protocol introduces a certain delay. the processing delay on the cpus of the sdn agents and the sdn controller (the step's actual execution time plus interference from other requests) is (symbolically) indicated by the red boxes. communication is done via ethernet and experiences delay (own frame transmission times plus interference from other traffic streams) in the network, which is indicated by the blue arrows. the entire network configuration process from the arrival of the first frame of a traffic stream to its final admission has a certain sdn configuration latency r + sdn . from the perspective of the requesting traffic stream, the network configuration latency r + sdn of the explicit flow configuration protocol can be hidden by configuring the network before this stream enters the network. however, this does not reduce the actual configuration latency and only works if the network change is known beforehand. in automotive systems, however, communication scenarios are typically predefined at design time. so, alternatively, all switches could be preconfigured to support all possible traffic streams. admission control for a given traffic stream can then be realized by enabling or disabling a higher-priority dropping rule in the flow table of the switch at which this traffic stream enters the network. this reduces admission control to a simple req/en handshake, as shown by the predefined flows protocol in figure 2b . fault recovery can be realized by the explicit flow configuration protocol, e.g. by switching to a set of preconfigured alternative flows for a particular fault. instead of an arriving frame, the network reconfiguration process is triggered by the detection of a fault. here, the switch sending the req message might be different from the one receiving the en message."
"where d is the dataset tested, x is the method that we wish to calculate its speedup factor, and t is the time function, which returns the time consumed by the method x on the dataset d. the accuracy comparison results are shown in table 4 . tables 5 and 6 show the time consumed in the training and testing phases, respectively, while table 7 shows the speed-up comparison results."
"in order to statistically analyze the accuracy results of the dt+ methods compared to that of the fpbst (table 8), we used the statistical test for algorithm comparison (stac) (http://tec.citius.usc.es/stac/) [cit] . here we opt for the t-test paired as being commonly used to determine whether there is a significant difference between the means of two groups; the first group is fpbst results and the second group is the dt+ results. however, to do the t-test, our data should satisfy: independence, normality, and homocedasticity [cit] ."
"when the number of training samples are chosen as 1000, the average detection accuracy of each method in the testing samples number (300, 400, 500, 600, 700, 800, 900, and 1000) were selected, this is probably because the orthogonality of column basis matrix and row basis matrix is considered in the objective function of k2dnmf, which is important to improve the robustness of the algorithm."
"big data analytics has received a great deal of attention recently, particularly in terms of classification, this is due to the main properties of big data: volume, variety, and velocity [cit] . having a large number of examples and various types of data, big data classification attempts to seize these properties to obtain better learning models with fast learning/classification [cit] ."
"we review the current limited understanding of semantics in inference. kjaerulff and madsen [cit] suggest that in working with probabilistic networks it is convenient to denote distributions as potentials. in fact, the use of potentials is built into the standard inference algorithms (see the so and ve algorithms, for instance). for example, suppose query p(j) is posed to the esbn [cit] . even without evidence being considered, the initial step of ve is to regard cpts as potentials, i.e., p(u ) is factorized as"
"as can be seen from table 4, one or more of the proposed methods dt0-dt4 slightly outperform the fpbst in terms of accuracy when testing on all datasets except for the satimage, which works better with the fpbst, however, the difference is not significant (less than 1%), and it might due to randomness of the train/test examples, on average, we can see that the dt1, dt3, and dt4 perform slightly better than the fpbst. the maximum average classification accuracy is attributed to the dt4; this is due to the nature of the dt that created by the dt4, which continues the recursive process until there is only one class or similar classes per leaf-node. we are not favoring the dt4 as its size is similar to the bst of the fpbst, however, its accuracy is not significantly higher than the other dts and the fpbst, for example the dt3 outperforms all methods in terms of the number of datasets tested. we can say with some confidence that the proposed approach (using the decision tree instead of the knn-regardless the creation method of the decision tree) performs well on all the evaluated datasets, and this performance is almost similar to the fpbst in some cases or slightly better in other cases."
"cpa derives (among other metrics) worst-case execution times for each task, based on the interference from other tasks on the same resource [cit] . the sdn configuration figure 3c shows how the predefined flow protocol from figure 2b can be modeled in cpa by using the same principle. as expected, this model is less complex."
"by expanding equation (21) according to the calculation method of equation (8), the following optimization problem containing the double constraints of equality and inequality can be derived as follows:"
"as can be seen from table 6 and figure 8, the consumed time in the testing phase for dt0-dt3 is less than that of the fpbst, this is due to the smaller size of these threes, and the disuse of the knn classifier, it is interesting to note that the dt4 speed in the testing phase is almost similar to that of the fpbst, this is due the large and equal size of their trees. it is also interesting to note that there is no significant difference in the time consumed by the proposed dt0-dt3 in the testing phases, since they are almost the same except for the method of calculating the probability for each class. figure 8 . illustration of data from table 6 (time (ms) consumed by the fpbst to test the entire test examples compared to that of the proposed methods), the time axis is logarithmic base-10."
"in this paper, we propose a new approach to improve the performance of the fpbst when classifying small, intermediate and big datasets. the major improvement includes the abandonment of the slow knn, which is used with the fpbst to classify a small number of examples found in a leaf-node. instead, we convert the bst to be a decision tree by its own, seizing the labeled examples in the training phase, by calculating the probability of each class in each node, we used various methods to calculate these probabilities."
"where (cnd) is the time consumed to find the approximate furthest points, as the constant c is the number of iterations needed to find the approximate furthest points, which is found experimentally to be in the range of 2 to 5 [cit] . the (log n) time is consumed along the depth of the dt."
"where γ is the product of 1 and all evidence potentials in f (ψ), and n is the same factorization as f (ψ) except without products involving evidence potentials."
"as mentioned above, the proposed dt0-dt4 have been further improved attempting to provide more regular trees in terms of class distribution, this improvement includes the enforcement of the examples that are similar to the furthest point of the lower class to be sorted to the left-side of the tree, and those which are similar to the other furthest point with the higher class to be sorted to the right-side of the tree. we conducted several experiments to measure the effect of this improvement on both accuracy and speed. here we choose the dt of the best performance on each dataset and compare its performance to the fpbst, the comparison results are shown in table 8 . as can be seen from table 8, the accuracy of the proposed dt after the improvement has increased by about 2.38%, this is due to the sorting of the examples based on their classes, which is the only change that has been made to the decision trees. however, there is no improvement in the speed of both training and testing phases, since swapping the furthest points based on their classes need the same computation of swapping them based on their minimum/maximum norms, so there is no extra calculations needed by the new improvement, and that why the time consumed by both phases is not improved."
"all datasets used contain numeric data, i.e., real numbers and/or integers. the sizes of these datasets are in the range of 625 to 11,000,000 examples; the dimensions are in the range of 4 to 5000 features. table 2 shows the descriptions of the datasets used."
"all datasets used contain numeric data, i.e., real numbers and/or integers. the sizes of these datasets are in the range of 625 to 11,000,000 examples; the dimensions are in the range of 4 to 5000 features. table 2 shows the descriptions of the datasets used."
"an extra (2nd) time is consumed by comparing each example or feature vector (fv) to the local furthest points (p1 and p2). this time can be added to c to make it in the range of 4 to 7, however, c is still a constant and the overall time complexity can be asymptotically approximated to:"
"defined networking the focus of this paper is the evaluation of the general suitability of the sdn concept for real-time networks. as sdn (exemplified by openflow) was not designed with real-time requirements in mind, we propose a simplified (but analyzable) sdn scheme. we differ from openflow in two key points: (a) as discussed in section ii, udp is typically chosen over tcp to transport latency-critical traffic. hence, we present two udp-compatible protocols to realize sdn-based network control. (b) in section v, we argue that certain messages are assumed to be smaller than in openflow."
"koller and friedman [cit] state that it is interesting to consider the semantics of the potentials constructed during inference. they mention that sometimes the probabilities are defined with respect to the joint distribution, but not at other times. as no practical algorithm exists for deciding the semantics of inference, all inference algorithms denote the intermediate factors constructed during inference as potentials. potentials have no constraints [cit] meaning they do not have clear physical interpretation [cit] ."
"the sdn configuration latency depends on various parameters, which we explore in the following. specifically, we investigate: (a) the number of sdn requests per switch: in our evaluation, each switch sends a certain number of independent sdn requests to the controller. we assume that, in the worstcase, these requests are generated and sent concurrently. each request is modeled by a periodic event model with a period of 1s, e.g. for a periodic 5ms stream every 200th frame would cause an sdn request. we also assume that each request requires all switches in the network be updated with new forwarding rules. (b) the size of sdn messages: openflow, for example, defines different message types (e.g. to send requests to the controller or to reconfigure flow tables), which can be of variable size (depending, e.g., on the number of flow table entries to modify or the number of actions). thus, we also evaluate different sdn message types and sizes. we assume that there are two different types of sdn messages. in openflow, the requesting frame (e.g. frame 1 in figure 2 ) can be sent as part of the request message (packetin message in openflow) payload to the sdn controller to aid decision making. this, however, leads to larger sdn overhead, as larger frames potentially cause more/longer interference in the network. in this evaluation, which is in the context of realtime systems, we try to reduce the sdn overhead and assume that request, acknowledge, and enable messages only carry enough information to unambiguously identify a traffic stream, consequently, the ethernet priority of sdn traffic has a decisive impact on the sdn configuration latency, as it determines how much interference it can experience. likewise, the impact of sdn traffic on non-sdn traffic also depends on this priority. we investigate the impact of sdn's ethernet priority by comparing setups where sdn traffic is mapped to the highest ethernet priority (higher than control traffic), where sdn traffic shares a priority with control, and where it shares a priority with camera traffic."
"in section 3.1, we obtained a nonnegative column basis matrix and a nonnegative coe cient matrix by the decomposition of ( ), so that the ith sample image can be easily derived as:"
"similar to the fpbst, the training phase of the proposed method (fpdt) creates a binary decision tree (dt), which speeds up searching for a test example comparing to the unacceptable time an exhaustive search, particularly when classifying big datasets. we use the same euclidean distance metric (ed) for measuring distance, to compare the results of the proposed method to those of the fpbst."
"it is worth mentioning that the stac platform accepted the ho regardless the very small p-value, however, we rejected it here due to the p-value being less than the level of significance, which is (0.05) in our case. we further verified this decision using other platforms such as http://www.statskingdom. com/160meant2pair.html, in addition to our own calculations using microsoft excel software."
"in order to statistically analyze the accuracy results of the dt+ methods compared to that of the fpbst (table 8), we used the statistical test for algorithm comparison (stac) (http://tec.citius.usc. es/stac/) [cit] . here we opt for the t-test paired as being commonly used to determine whether there is a significant difference between the means of two groups; the first group is fpbst results and the second group is the dt+ results. however, to do the t-test, our data should satisfy: independence, normality, and homocedasticity [cit] ."
"the main idea of sdn is the separation of a network's control and data planes. the data plane is responsible for frame forwarding, e.g. the switch fabric and flow (forwarding) table, while the control plane makes the actual routing decisions and configures the data plane accordingly. in sdn, the network's control plane (red components in figure 1 ) is consolidated in a (logically) centralized sdn controller and only the data plane (blue components in figure 1 ) remains in the switches. the sdn controller runs a network operating system, which manages and controls the entire network by configuring the switches. inside an sdn switch, there is an sdn agent. this agent implements the communication with the sdn controller and updates the flow table inside the switch. the content of this flow table defines the forwarding operations performed by the switch's data plane, i.e. which incoming traffic stream (identified for example by source and destination addresses and/or port numbers) is forwarded to which output port."
"let b be any bayesian network. theorem 4 states that for nearly all choices c of cpts for b, the si algorithm correctly denotes the semantics of potentials constructed by ve during exact inference on b."
"since the feature mapping function is unknown, the nal result of the column basis matrix cannot be calculated, which does not a ect the e ective expression of the underwater target feature information."
"as can be seen from table 8, the accuracy of the proposed dt after the improvement has increased by about 2.38%, this is due to the sorting of the examples based on their classes, which is the only change that has been made to the decision trees. however, there is no improvement in the speed of both training and testing phases, since swapping the furthest points based on their classes need the same computation of swapping them based on their minimum/maximum norms, so there is no extra calculations needed by the new improvement, and that why the time consumed by both phases is not improved."
". and are the row basis matrix and the coe cient matrix corresponding to the row basis matrix, respectively. it is expected that the row basis matrix still maintains the orthogonality. erefore, equation (20) can be further transformed into the following optimization problem to be solved:"
"where ⟨ *, * ⟩ representative the kernel function. in feature space, data standardization can be done through mean centering and variance scaling of kernel matrix [cit] ."
"the relatively small size of the dt created by the proposed dt0-dt3 and dt0+-dt3+ shall serve two purposes, (1) decreasing the space needed for the tree; and (2) speeding up the classification process, since searching a smaller tree is faster than a larger one. this is also complying with the number of leaf-node, as being significantly smaller than that of the fpbst, dt4, and dt4+."
"suppose that the testing sample is given, the feature matrix can be easily obtained according to equation (37), then the matching degree between and can be computed to judge whether or not belongs to the underwater target image. e matching degree is de ned as follows:"
"so far, k2dnmf row direction decomposition has been completed. by combining the results of the column decomposition of k2dnmf described in section 3.1, e whole process of the k2dnmf algorithm is obtained and displayed"
"our data is independent, since it comes from different methods (fpbst and dt+). to test the normality of our data, we used the shapiro-wilk test because it performs the best, especially for samples of less than 30 elements [cit] . the null hypothesis for normality would be: the samples follow a normal distribution. with significance level of 0.05, table 9 shows the normality results. as can be seen from table 9, according to the p-value of the shapiro-wilk test we accept the null hypothesis, i.e., both fpbst and dt+ accuracies follow normal distributions. to test the homocedasticity of both fpbst and dt+ accuracies we opt for the levene test [cit], the null hypothesis of the homocedasticity of our data is: all the input populations come from populations with equal variances. with significance level of 0.05, table 10 shows the homocedasticity results. as can be seen from table 10, since the p-value is greater than the level of significance (0.05), the null hypothesis is accepted, and therefore the tested data is homocedastic. since we verified normality, homocedasticity, and independence of our data we can apply the t-test table 11 shows the t-test results. table 11 . t-test results of group 1 (fpbst) and group 2 (dt+) accuracies obtained from table 8 ."
"ethernet is considered to become the communication backbone for future automotive networks, as traditional buses such as can or flexray cannot keep pace with the increasing bandwidth and scalability requirements of advanced driver assistance systems and infotainment systems. the introduction of ethernet enables access to a large set of concepts and protocols from other ethernet domains, such as sdn [cit] ."
"update r gw as in equation (30) now, if a matrix whose initial value is nonnegative, a pair of nally converged nonnegative matrices and can be obtained by repeated iterations. here can be decomposed into the form of sub-matrices align."
"recently, we proposed three methods for big data classification [cit] . all of these methods employ an approach based on creating a binary search tree (bst), in order to speed up the big data classification using a knn classifier with a smaller number of examples, those which are found by the search process. the real distinction between these methods is in the way of creating the bst. the first uses the furthest-pair of points to classify the examples along the bst, the second uses two extreme points based on the minimum and maximum points found in a dataset, and the third uses the euclidean norms of the examples. each has its own weakness and strength. however, the common weakness is the use of the slow knn classifier."
"as mentioned above, the proposed dt0-dt4 have been further improved attempting to provide more regular trees in terms of class distribution, this improvement includes the enforcement of the examples that are similar to the furthest point of the lower class to be sorted to the left-side of the tree, and those which are similar to the other furthest point with the higher class to be sorted to the right-side of the tree. we conducted several experiments to measure the effect of this improvement on both accuracy and speed. here we choose the dt of the best performance on each dataset and compare its performance to the fpbst, the comparison results are shown in table 8 ."
"the improvements made in this study allow the proposed dts to be more suitable for variety of real-world applications such as image classifications, face recognition, hand biometrics, fingerprint systems, and other types of biometrics, particularly, when the training data is very large, where traditional classification methods become impractical."
"although 2dnmf [cit] has been successfully applied to the eld of the target detection, it does not perform well when image data contains a strong nonlinear characteristic. for this purpose, the kernel two-dimensional nonnegative matrix factorization (k2dnmf) has been proposed, which is a nonlinear extension of standard two-dimensional nonnegative matrix factorization. in addition, this paper is not simply introducing the idea of kernel method, we explore the di erent interpretations of k2dnmf when column basis matrix factors are restricted to having di erent properties. meanwhile, k2dnmf not only maintains the nonnegative and low-rank properties of the column basis matrix factor and the row basis matrix factor, but also exerts the orthogonal constraint on these two matrix factors respectively leads to a good subspace approximation of the original data matrix in the feature space. in the phase of underwater target detection, k2dnmf could accurately extract the e ective information of the underwater target and identify the target with an e ective classi er, thereby reducing the computational complexity. experimental results demonstrated that in comparison with the traditional underwater target detection method, k2dnmf had better feature extraction ability and higher detection accuracy for underwater target images collected by uuv vision system. e remainder of the paper is organized as follows: in section 2, we brie y reviewed the feature mapping method. in section 3, the k2dnmf method was proposed and its algorithm was described. in section 4, the underwater image data collected by the uuv vision system was used to evaluate the performance of the k2dnmf method for underwater targets detection. finally, a brief conclusion was summarized."
"two stages are involved in the underwater target detection using the k2dnmf method, the feature extraction stage and feature classi cation stage of the underwater target respectively."
where p (u ) is defined by a different bayesian network b -the one given in figure 2 (right). our objective is to stipulate semantics in the current bayesian network b -the one on which inference is being conducted.
"in order to evaluate the proposed methods and compare the results to the fpbst on big data classification, we use some of the well-known machine learning datasets, which are used by state-of-the-art work in this domain. these datasets are freely available for download from either the support vector machines library (libsvm) data [cit] or the uci machine learning repository [cit] . the datasets used are of different dimensions, sizes, and data types, such diversity is important to evaluate the efficiency of the proposed method in terms of accuracy and time consumed."
"to evaluate the proposed methods (dt0-dt4 and dt0+-dt4+), we programmed both algorithms 1 and 2 using ms vc++. [cit], and conducted several classification experiments on all the datasets described in the data section. we utilized a personal computer with the following specifications:"
"in the recent decades, more and more attention has been paid to the target detection by using uuv vision system [cit] . e underwater target detection aims to hunting and processing the target of interest, which may be a good way to eliminate potential threats and avoid the damage [cit] . for a long time, many scholars have devoted themselves to the development of uuv vision technology and thus, many e ective methods for uuv vision technology have been developed and applied to deal with problems in the real environment [cit] . among these studies, the uuv vision system-based target detection is one of the most concerned topics in the eld of uuv vision technology, the three-dimensional of uuv model equipped with vision system is shown in figure 1 . e traditional model-based target detection method largely depends on the prior knowledge of the detection target, but the knowledge acquisition in connection with the detection target is o en very di cult, which limits the application of the model-based target detection method in practical problems. erefore, the urgent need for research methods on images data itself, namely, the desire for target detection based on algebraic methods has emerged, which has led to the target detection techniques of multi-variable statistical analysis."
"in order to evaluate the proposed methods and compare the results to the fpbst on big data classification, we use some of the well-known machine learning datasets, which are used by state-of-the-art work in this domain. these datasets are freely available for download from either the support vector machines library (libsvm) data [cit] or the uci machine learning repository [cit] . the datasets used are of different dimensions, sizes, and data types, such diversity is important to evaluate the efficiency of the proposed method in terms of accuracy and time consumed."
"in this paper, we compare the performance of the proposed methods to that of the fpbst, as the goal of this paper is to improve the performance of the fpbst, in terms of speed, space used, and classification accuracy. for this end, we evaluated the proposed methods dt0-dt4 by employing them to classify the machine learning datasets stated in table 2, using 10-fold cross-validation, so as to be able to compare their performances to that of the fpbs."
"ideally, the 10-fold cross-validation approach selects the training data set randomly; however, nalepa and kawulok [cit] discussed other interesting methods for selecting the training data such as data geometry analysis (clustering and non-clustering), neighborhood analysis methods, evolutionary, active learning and the random sampling methods. our choice belongs to the random sampling methods as being the most used."
"we use cpa [cit] to analyze the worst-case behavior of sdn communication. the cpa system model comprises three components: resources, tasks, and event models. resources model processing or network resources (e.g. cpus or switch ports) and provide service according to a scheduling policy. tasks are mapped to resources and compete for their service. per activation, a task consumes service varying between its best-and worst-case execution time. task activations are abstracted by event models. an event model is defined by a tuple of event arrival functions η − (δt) and η + (δt), which give the lower and upper bounds on the number of events (task activations) in any half-open time interval [t,t + δt). in contrast to a specific event trace, an event model captures all possible event arrival scenarios within its bounds. a system is modeled as a directed graph, where tasks correspond to nodes and task dependencies are modeled by edges. whenever a task finishes its execution, it propagates an event to its dependent task(s), i.e. its output event model becomes the input event model of its dependent task(s). if a task activation depends on events from multiple tasks, these events must first be joined by a junction with and semantic [cit] . tasks without predecessors must be stimulated by an event model from an external source."
"to seek solution for this problem, a new subspace method called nonnegative matrix factorization (nmf) [cit] was proposed. at present, nmf algorithm has been smoothly applied in the eld of pattern recognition and image processing. di erent from the traditional matrix factorizations, the core objective of the nmf is to nd the product of two nonnegative matrix factors which is then used as the approximation to original data matrix. it is precisely the introduction of the nonnegative conditional constraints on the matrix factor, so that the local features learned by nmf learning can reconstruct the original image data information through superimposition method, and the subtraction operation is no longer needed to eliminate some information. however, there are still two obvious shortcomings in the application of nmf algorithm in the eld of target recognition. first, the two-dimensional image matrix must be transformed into a one-dimensional image vector, which can cause the problem of large dimension. second, the matrix-to-vector transformation may result in the loss of information hidden within the two-dimensional image matrix. erefore, in order to solve the two problems, the 2dnmf [cit] method was invented. 2dnmf considers the column and row information of the image matrix in two direction and nds the nonnegative matrix factors in two directions. erefore, comparing to the nmf [cit] method, 2dnmf is better than nmf in computational e ciency and detection accuracy."
"it is interesting to note from table 5 and figure 7 that the proposed dt0-dt3 consumed less time in general than the fpbst and the dt4, this is due to the smaller decision trees created by these methods, however, the time saved while building the decision tree by dt0-dt3 is not significant on some datasets, this is due to the extra calculations of the probabilities of each class for each dataset. it is also interesting to note the time consumed by the dt4 is almost similar to that of the fpbst and sometimes longer; this is because it has a similar tree size to that of the fpbst, with extra time for calculating the probabilities. figure 7 . illustration of data from table 5 (time (ms) consumed by the proposed methods to build their dts compared to that of the fpbst), the time axis is logarithmic base-10. figure 7 . illustration of data from table 5 (time (ms) consumed by the proposed methods to build their dts compared to that of the fpbst), the time axis is logarithmic base-10. as can be seen from table 6 and figure 8, the consumed time in the testing phase for dt0-dt3 is less than that of the fpbst, this is due to the smaller size of these threes, and the disuse of the knn classifier, it is interesting to note that the dt4 speed in the testing phase is almost similar to that of the fpbst, this is due the large and equal size of their trees. it is also interesting to note that there is no significant difference in the time consumed by the proposed dt0-dt3 in the testing phases, since they are almost the same except for the method of calculating the probability for each class. table 6 (time (ms) consumed by the fpbst to test the entire test examples compared to that of the proposed methods), the time axis is logarithmic base-10."
"while this claim is almost always true, there are a few exceptions to refute it. for one counter-example, eliminating variable a using the cpts in table 3 yields:"
"column direction decomposition of k2dnmf is completed in feature space and the matrix factors e and h are obtained, then the column basis matrix c is expressed; 5:"
"our data is independent, since it comes from different methods (fpbst and dt+). to test the normality of our data, we used the shapiro-wilk test because it performs the best, especially for samples of less than 30 elements [cit] . the null hypothesis for normality would be: the samples follow a normal distribution. with significance level of 0.05, table 9 shows the normality results. table 7 (speed-up results-training and testing phases-of the proposed methods dt0-dt4 compared to the fpbst)."
"generally, concerning with algorithms on the matrix decomposition of two-dimensional image data, such as bdpca [cit] and rc2dpca [cit] algorithms that divide the original data space along both directions in the row and column into several subspaces, attempt to nd the subspace approximation for the original data along both directions in the row and column. in this section, the augmentation matrix ( ) will be decomposed to nd low-rank matrices in both the column and row directions of the high-dimensional feature space, thereby establishing a model for underwater target detection."
"in this section, we evaluate the general suitability of the sdn network control concept for real-time ethernet. as sdn's centralized network control introduces additional traffic, we evaluate both the sdn configuration latency, to investigate if the network can be configured in a timely manner, and the impact of the additional sdn traffic on non-sdn traffic (i.e. the traffic that is controlled by sdn), to quantify sdn's overhead on existing traffic. we explore, for both protocols, different parameters: the number of sdn requests per switch, sdn frame sizes, sdn processing times, and sdn traffic priorities. our analysis is based on the cpa models introduced in section iv-b. we use the worst-case latency guarantees derived from cpa as a comparison metric. table i summarizes the normal (non-sdn) traffic in the network, which has been provided by daimler ag. the traffic is categorized into control and camera traffic. control traffic is assumed to be latency-critical and is mapped to a higher ethernet priority than camera traffic, which we assume to have more relaxed latency requirements. there are different kinds of traffic streams in the network: unicast, multicast (the notation n(d) specifies that there are n multicast streams with d destinations), and broadcast. for each traffic class, table i gives its minimum, maximum, and average payloads and periods. we assume that ipv4/udp is used to route data, so that an overhead of 28bytes must be added to the payload. although traffic is considered to be periodic, we use a periodic with jitter event model [cit] for traffic streams entering the network. to allow, in the worst-case, an occasional burst of two frames, we set the jitter to the period."
"by comparing (1) and (4), it is clear that semantics are destroyed even before the cpts in computer memory are modified. the notation used for potentials does not convey the semantic meaning of the probabilities comprising the potential. darwiche [cit] ascribes meaning during inference by representing each potential by what we will call evidence expanded form, except that products involving evidence potentials are taken. let ψ be any potential constructed by ve. the evidence expanded form of ψ, denoted f (ψ), is the unique expression defining how ψ was built using the multiplication and marginalization operators on the bayesian network cpts together with any appropriate evidence potentials."
"the contribution of this paper is the evaluation of the general suitability of sdn for real-time ethernet. for that purpose, we discuss two protocols for basic sdn-based network configuration. we show how these protocols can be modeled and formally analyzed in a compositional performance analysis (cpa) framework [cit] . finally, we evaluate each protocol regarding its timing guarantees, impact on other traffic streams, and scalability by using a realistic automotive ethernet setup."
"as can be seen from table 11, since the p-value is less than the level of significance (0.05), the null hypothesis is rejected to the favor of ha. thus, there is a statistically significant difference between the fpbst and the dt+ accuracies. in addition, the t statistic is shown in negative value because the mean of the fpbst accuracies is less than that of the dt+, therefore, we can say with some confidence that the dt+ methods outperform fpbst in terms of classification accuracy."
"in order to demonstrate the ability of each method to recognize targets when the number of training samples is changed, di erent numbers (300, 400, 500, 600, 700, 800, 900, 1000) were selected from the underwater target image dataset to test in testing set 2. e average detection accuracies are shown in table 4 and plotted in figure 6, where the numbers in parenthesis are the standard deviations. figure 6 and table 4 show that when the number of training samples increases, the performance of all methods were improved. e detection accuracy of k2dnmf increases from 79.92% with 300 training images to 90.20% with 1000 training images. for other methods, the detection accuracies of bdpca, rc2dpca, 2dnmf, pnmf, and mknmf increase from 73.12%, 74.63%, 71.86%, 72.90%, and 78.33% with training number 300 to 81.80%, 84.00%, 84.60%, 85.80%, and 88.20% with training number 1000 respectively. from figure 6 and table 4 can also be seen that when the number of training samples exceeds 50% of the samples in the image dataset, kernel methods are competitive to the linear method, that is, pnmf, mknmf, and k2dnmf outperform bdpca, rc2dpca, and 2dnmf. further observations demonstrate that among the selected training samples, the k2dnmf algorithm is the best overall performance in the detection accuracy compared to other methods. figure 7 visually show that the standard deviation distribution of the detection accuracy with di erent methods when the same number of trainings samples were used in testing set 2. it can be seen from the experimental results of figure 7 the standard deviation of the detection accuracy of k2dnmf is smaller than other methods when the training methods regardless of which of the training sample numbers was selected, demonstrate the stronger robustness of the k2dnmf method."
"the fpbst builds its bst by finding the furthest points p1 and p2 [cit], assuming that the furthest points are the most dissimilar points, and therefore, are more likely to be belonging to different classes. thus, sorting other examples based on their distances to these points might be a good choice, as similar examples are sorted nearby, while dissimilar examples are sorted faraway in the created bst."
"if sdn is used to manage real-time networks, it is subject to strict timing requirements. admission control must be able to decide in bounded time if a traffic stream is allowed to enter the network. safety-critical systems usually demand an upper bound on the fault recovery time. this is usually defined to be the time from the occurrence of a fault until it has been resolved. it comprises the time to detect the fault and the time to execute counter measures such as network reconfiguration. the avnu alliance suggests a fault recovery time of less than 100ms for critical automotive control applications [cit] . other sources suggest less than 50ms [cit] . real-time systems require the verification of these timing requirements. here, formal worst-case analysis methods are typically chosen over simulation-based approaches, as simulation usually does not guarantee to expose all corner cases, whereas a formal analysis gives safe upper bounds on a system's worst-case behavior."
"although the fuzzy controller with the advantage of expert reasoning doesn't require accurate mathematic models, the fuzzy rules and membership functions are unalterable, which can't adapt variable traffic flow constantly."
"step 4: calculate gbest according to (13) step 5: as if the stopping criterion is not satisfied, move the particles according to (9) and (10), assess particle positions and update pbest i and gbest according to (12) and (13)."
"step 3: detect the vehicle number l i of the phase i by the end of g i, and select the next phase j according to the method with normal transitpriority."
"the 30-sensor deployment result graph started with a 92% coverage rate and reached almost 95% after few iterations and remained practically stable until 150 iterations. after that, the algorithm started looking for a better result until iteration 225 where the final optimal coverage rate was 96%. during the optimization of coverage by using the pso algorithm, the coverage optimization went through four stages, first the deployment of 20 sensors, then 30 sensors, followed by 35 sensors, and finally 40 sensors. the simulations results are as follows:"
"output the optimal fitness and the corresponding particle position. in the simulation condition mentioned above, the detection radius r will be constant and equal to 20 m, and the number of sensors used will vary between 20 and 40 to determine its impact on the performance of the coverage. the iterations-coverage rate and the coverage effect are shown in table 2, and figures 3-6. v."
"according to figure 10a, it is noted that the sensor (13) that left the network was at the neighborhood of sensors (34), (25) and (28) so they then covered the space already covered by the sensor (13) but their sensing range was not enough to cover all the space. for this reason, we found a coverage hole between cell (27) and (33), as shown in figure 10b ."
"as the sensors used in this work are embedded in public mobile phones, so these nodes move randomly, which is why the network is able to crack at any time (mobile phone is turned off, leaves the roi, is disconnected, etc.). to define the coverage hole introduced by the off-grid sensor and its exact location, we will introduce the vd algorithm to provide each sensor with their own cell. in this way we can define with accuracy the coverage hole location."
"the algorithm is guided by personal experience (pbest), overall experience (gbest), and the present movement of the particles to decide their next positions in the search space. in d dimensional search space, we have m particles at certain speed flight. to find the optimal solution, pso algorithm initializes a group of random particles through the iterative. with the speeds of each generation of particles, the location updates the objective function as follows [cit],"
"in reference [cit], 20 sensors from total 40 sensors were used to completely cover an area of 20 . therefore, the number of deployed sensors was optimized by 50%. in our work, the deployment of 40 sensors from a total number of 100 sensors was enough to fully cover an area of 100m 2 . therefore, we optimized our deployment by 60%. it is obvious that the pso algorithm successfully found the best deployment strategy for this application. the position and the number of deployed sensors were determined to fulfill the goal of maximum coverage of the monitoring area."
"as the sensors used in this work are embedded in public mobile phones, so these nodes move randomly, which is why the network is able to crack at any time (mobile phone is turned off, leaves the roi, is disconnected, etc.). to define the coverage hole introduced by the off-grid sensor and its exact location, we will introduce the vd algorithm to provide each sensor with their own cell. in this way we can define with accuracy the coverage hole location."
"author contributions: the work described in this article is the collaborative development of all authors. a.n contributed to the idea of data processing, designed the algorithm, writing and analysis. l.f supervised the application and provided technical advice on simulation process. g.m made contributions to programming and data measurement. l.w provided important advice on the formula derivation as well as the paper revision."
"step 5: according to experience of policeman and figure of intersections, building rules of fuzzy control. determine the green increase time δg according to the fuzzy rules and the values of l i and"
"the main objective of this project is to ensure the best quality of service, and this begins with full coverage of the monitoring area. the process of deploying the sensors has an impact on the performance of the wsn because it affects the network coverage, communication cost, and management resources, so the deployment strategy of a wsn is a major problem."
"step 1: assign the minimum green time g min and maximum green time g max for each phase. the green time of any phase will be greater than g min and less than g max . assume the current running phase is the phase i, and has been running for g i ."
"after we had conducted our review of the relevant literature, we then began to choose the algorithms that would suit us in our work to allow us overcome the aforementioned problems. therefore, for the first part of coverage optimization and deployment strategy, the particle swarm optimization (pso) will be applied to fulfill the coverage over the region of interest (roi) in section 2. the second part of our application will be divided into two sections: a vd to detect the location of the coverage holes holes, and to then use the point in polygon (pip) for coverage hole healing, which will be expanded in section 3. finally, we will use matlab language to simulate our system in section 4, and give the conclusion in section 5."
"the 20-sensor deployment result graph started with 84% coverage, after that the graph started to increase until it reached 87% at iteration 230 then jumped directly to the optimal result 88% and stabilized."
"the 30-sensor deployment result graph started with a 92% coverage rate and reached almost 95% after few iterations and remained practically stable until 150 iterations. after that, the algorithm started looking for a better result until iteration 225 where the final optimal coverage rate was 96%. during the optimization of coverage by using the pso algorithm, the coverage optimization went through four stages, first the deployment of 20 sensors, then 30 sensors, followed by 35 sensors, and finally 40 sensors. the simulations results are as follows:"
"the sensor indicated in figure 8 was randomly chosen to exit the coverage network. the vd algorithm was reapplied to the remaining 39 sensors in the area to find the coverage holes introduced by the outgoing sensor. the result is shown in the figure 9 . it was assumed that, after a period of time (one hour), the base station updated the positions of the 40 sensors deployed and that one of the sensors came out of the network. therefore, the next application started by randomly selecting a sensor to exit the network and then to apply the vd algorithm on the remaining 39 sensors in the network to discover the location of the coverage holes. the results of this application are shown in figures 9 and 10 . it was assumed that, after a period of time (one hour), the base station updated the positions of the 40 sensors deployed and that one of the sensors came out of the network. therefore, the next application started by randomly selecting a sensor to exit the network and then to apply the vd algorithm on the remaining 39 sensors in the network to discover the location of the coverage holes. the results of this application are shown in figures 9 and 10 ."
"the pso algorithm allowed us to optimize a total coverage of the roi and to collect the best positions of the deployed sensors. after a period of time (one hour), the base station updated these positions leading to one of the deployed sensors out of the coverage network. following this, the vd will be used to define the position of the coverage hole."
"the next step in this application was to search for the sensors that were originally in the coverage hole cells (cell 27 and 33) by applying the pip algorithm. following this, we reapplied the coverage code to find the best sensor to enhance the coverage. after applying the pip algorithm in the coverage hole cells, we found three sensors in this area as shown figure 11 . after finding five sensors between cells 27 and 33, we then reapplied them to the coverage algorithm to ascertain the best sensor that could cover the hole and join it to the monitoring area network. the result is shown in figure 12 . after finding five sensors between cells 27 and 33, we then reapplied them to the coverage algorithm to ascertain the best sensor that could cover the hole and join it to the monitoring area network. the result is shown in figure 12 ."
"the previous phase of work allowed us to identify the positions of the sensors used to build the air pollution monitoring wireless network and ensure full coverage of the roi. after a period of time (one hour), it is necessary to ascertain if any of the sensors change its position or leave the roi. updating the sensor locations allow to check the positions of the deployed sensors. if they remain their positions we keep the same deployed sensor for roi coverage otherwise we move to the next phase of the work which is the holes detection and healing process. the second phase of project will be divided into two parts: the first is holes detection and coverage based on the vd, and the next is to repair the holes using the pip algorithm."
"in this paper, a method called transit-priority fuzzy neural network control (tp-fnnc) is applied to the signal control of intersection. in this method, the special vehicles will be absolutely preferential. in the normal transit-priority, the phase selection is according to traffic urgency of every phase; the design of a fuzzy controller for green increase time considers the vehicle number in the current phase and next phase. a four-layer neural network is used to implement this fuzzy controller. simulation results show that this fuzzy neural controller plays good performance. the mean passenger delays and mean special vehicle delays are reduced obviously."
the sensor indicated in figure 8 was randomly chosen to exit the coverage network. the vd algorithm was reapplied to the remaining 39 sensors in the area to find the coverage holes introduced by the outgoing sensor. the result is shown in the figure 9 .
"the pip algorithm allows us to check, by way of a program, whether a particular point is inside a polygon or outside of it. a common way to approach the problem is to count how many times a line drawn from the point (in any direction) crosses the edge of the polygon. if the line and the polygon intersect an even number of times (or not at all), the point is outside. if they cross an odd number of times, the point is inside. this is true even for complex shapes that have a lot of coordinates and a very precise edge [cit] . this process can be seen in figure 2 ."
"the 20-sensor deployment result graph started with 84% coverage, after that the graph started to increase until it reached 87% at iteration 230 then jumped directly to the optimal result 88% and stabilized."
"the 20-sensor deployment result graph started with 84% coverage, after that the graph started to increase until it reached 87% at iteration 230 then jumped directly to the optimal result 88% and stabilized."
coverage hole healing using the algorithm of pip to find sensors inside the hole then to use them to revalue the coverage of the hole using the coverage algorithm used in the first phase of work.
"a fuzzy logic traffic signal controller for an isolated intersection using a two-stage fuzzy logic procedure is designed [cit] of which the performance is better than the traffic-actuated controller. a multi-phase adaptive control algorithm is presented based on the learning ability of fuzzy neural control [cit], which can not only decrease the average vehicle delays but also adjust the signal period automatically. a model called fuzzy logic multi-phased signal control (flmusic) has been developed for isolated signalized intersections and obtains encouraged results [cit] . an adaptive fuzzy logic signal controller (afc) is presented for the urban traffic network [cit] . schmöcker [cit] have presented a multi-objective signal control method using fuzzy logic of which the membership functions are optimized by a genetic algorithm using the vissim microscopic traffic simulator. the results of a case study in london prove that the method is practical and efficient."
there are two kinds of transit-priority. one is special transit-priority which is an absolute priority. the other is normal transit-priority which is a relative priority.
"the previous phase of work allowed us to identify the positions of the sensors used to build the air pollution monitoring wireless network and ensure full coverage of the roi. after a period of time (one hour), it is necessary to ascertain if any of the sensors change its position or leave the roi."
the traffic signal control is based on fuzzy logic technology in the paper. the multi-phase fuzzy control method for traffic signal with transit-priority can be described as follows:
"the 35-sensor deployment result graph started with a 95% coverage rate, and reached almost 97% just after few iterations. after that, the algorithm started looking for a better result until iteration 280, where the final optimal coverage rate was 98%."
"the pip algorithm allows us to check, by way of a program, whether a particular point is inside a polygon or outside of it. a common way to approach the problem is to count how many times a line drawn from the point (in any direction) crosses the edge of the polygon. if the line and the polygon intersect an even number of times (or not at all), the point is outside. if they cross an odd number of times, the point is inside. this is true even for complex shapes that have a lot of coordinates and a very precise edge [cit] . this process can be seen in figure 2 . after having selected the sensors which are inside the cell of the coverage hole, we will apply to these sensors the algorithm of coverage already applied in the previous phase of this project (from function 3 to 8). after having selected the sensors which are inside the cell of the coverage hole, we will apply to these sensors the algorithm of coverage already applied in the previous phase of this project (from function 3 to 8)."
"artificial neural network is a system which made up of many nodes called neuron, it can simulate basal features of brain, and deals with information by adopting parallel and distributed mode, its response speed of hardware"
"the normal transit-priority simulation results of three methods are shown in fig.3-4. fig.3-4 shows that mean passenger delays (mpd) of tp-fnnc are increased by 23% than fcc at best. the delays using the last two methods have evident improvement comparing with the method of fixed timing control. this paper's method has best performance in decreasing passenger delays than any other methods, and its mean vehicle delays is also less than others."
"the fuzzy controller includes three parts: fuzzy model, fuzzy reasoning model and fuzzy decision model. the process of fuzzy is to turn measured value (exact value) to fuzzy subset. in the paper, the input variables of fuzzy controller are l and δl, where l is the vehicle number of the lane, δl(δl＝l i+1 -l i ) is the difference between the l of current phase and next phase. the linguistic values of l are q 1 (zero), q 2 (very few), q 3 (few), q 4 (medium), q 5 fuzzy reasoning summarizes people's control experience, different control may be adopted according to different measured value, for traffic control, seven rules may be acquired according to its features. table iv shows the 33 fuzzy rules by summarizing practice and expert's experience."
the pso is inscribed in the family of evolutionary algorithms and is one of the most popular meta-heuristic optimization algorithms inspired by nature. this technique is used for any problem to explore a search in space to find the set parameters that maximize/minimize a particular goal [cit] .
"using the vd algorithm, the monitoring area was divided into several cells using the selected sensors for full coverage. each sensor had its own cell that gave us a 40-cell area with no coverage hole found."
"implementation is very high, meanwhile, it has the following functions: adaptive, self-learning and faulttolerance, etc. the fuzzy controller above can be implemented by a four-layer neural network. the fuzzy controller network is a feedforward network that encodes the decisionmaking in the fuzzy rule base. the activation functions of the network are different fuzzy set operations. the first layer is input layer. the nodes represent input linguistic variables l and δl."
"with the rapid development of communication technology, network and remote sensing technologies can help us to improve air pollution monitoring systems that are usually designed in a wireless mode. currently, monitoring the physical infrastructure of data centers is very important so as to enable effective maintenance, the prevention of any downtime, accurate event information, as well as efficiently planning appropriate energy requirements. these technologies are usually designed in a wireless mode and create a lot of air pollution. furthermore, physical electronic devices that are used alongside complex electronic devices create a lot of air pollution within a specific frequency spectrum [cit] . wireless air pollution monitoring systems operate in several modes however, these modes are expensive to install and maintain due to their complexity. although wsns is initially used for military and industrial reasons [cit], it has been developed for a growing number of industrial applications in recent years."
"a vd is a partition of a space into small regions called the polygon of voronoi based on closeness to points in a specific subset of the plane as each region contains one, and only one, site (point) and each point belongs to its region (cell) [cit] . in this application, the vd devised the roi into n cells where each sensor is the center of its own cell. to detect the coverage hole, we use the following theorems shown in figure 1 ."
"in this work, we use the joint monitoring probability of the node set to measure whether each pixel of the target area is covered. p th is the expected coverage threshold, then:"
"after defining the location of the cell that contains a coverage hole using the vd, the next step is to find other sensors inside this cell in order to reapply the coverage algorithm and select the suitable sensors to deliver full coverage in the roi. to complete this stage, we will use the pip algorithm."
"in most of these methods, the phase sequence of traffic signal control usually is fixed. while the fixed phase sequence method will generate the unnecessary delays when the number of vehicles in one phase is few and others is large. so, some methods [cit] which the sequence of phases is changeable and flexible are presented in order to decrease vehicle delays more effectively at the intersection. however, the control objective of those methods is to average vehicle delays, which means that the bus with more passengers and the car with fewer passengers will be treated equally. so it is unfair for the bus passengers. especially, some special vehicles such as ambulance, fire truck, police vehicle and other emergency vehicles are also treated as the social vehicles, which greatly influence the efficiency of these emergency vehicles."
positions are already known) are used to partition the roi into 39 cells and to discover the location of the coverage hole using the vd assuming that the theorems 1 and 2 are already defined.
"the network training process is that: input the values of the algorithm in network and the sample values showed in table 1-4. initialize the weights of the network to 1, the fuzzification of the condition part is performed. train the weights of the network in term of the error gradient descent. if the trained error is less than the demanded trained error, the training can be end and the weights are outputted, else the training of the weights will continue."
"the 40-sensor deployment result graph started with 97% coverage and increased to 99% at iteration 120 and maintained that rate until iteration 200. after that, the algorithm started looking for a better result. in this application the monitoring area was totally covered, and the rate reached 100%."
"the next step in this application was to search for the sensors that were originally in the coverage hole cells (cell 27 and 33) by applying the pip algorithm. following this, we reapplied the coverage code to find the best sensor to enhance the coverage. after applying the pip algorithm in the coverage hole cells, we found three sensors in this area as shown figure 11 ."
"when the detectors in the red phase detect special vehicles arrival, this phase will become special phase, the current green phase must be interrupted, and the special phase will be run. after the special vehicles pass through, the next running phase selection will be done using the phase selection method with normal-transit-priority, by this time, the phase with more urgency will be selected."
"a vd is a partition of a space into small regions called the polygon of voronoi based on closeness to points in a specific subset of the plane as each region contains one, and only one, site (point) and each point belongs to its region (cell) [cit] . in this application, the vd devised the roi into cells where each sensor is the center of its own cell. to detect the coverage hole, we use the following theorems shown in figure 1 ."
"the sensor indicated in figure 8 was randomly chosen to exit the coverage network. the vd algorithm was reapplied to the remaining 39 sensors in the area to find the coverage holes introduced by the outgoing sensor. the result is shown in the figure 9 . it was assumed that, after a period of time (one hour), the base station updated the positions of the 40 sensors deployed and that one of the sensors came out of the network. therefore, the next application started by randomly selecting a sensor to exit the network and then to apply the vd algorithm on the remaining 39 sensors in the network to discover the location of the coverage holes. the results of this application are shown in figures 9 and 10 ."
"according to figure 10a, it is noted that the sensor (13) that left the network was at the neighborhood of sensors (34), (25) and (28) so they then covered the space already covered by the sensor (13) but their sensing range was not enough to cover all the space. for this reason, we found a coverage hole between cell (27) and (33), as shown in figure 10b ."
"the effectiveness of the network coverage optimization strategy will be determined through a simulation experiment. using a computer with a 1.10 ghz processor, the coverage optimization algorithm of the wsn is carried out in matlab [cit] ) environment."
"at the end of the displacement of the particles in a given iteration, the new positions are evaluated and the two vectors pbesti and gbest are reindexed. according to equation (11) in the case of a minimization of an objective function then equation (12) in the psog can be achieved:"
"updating the sensor locations allow to check the positions of the deployed sensors. if they remain their positions we keep the same deployed sensor for roi coverage otherwise we move to the next phase of the work which is the holes detection and healing process. the second phase of project will be divided into two parts: the first is holes detection and coverage based on the vd, and the next is to repair the holes using the pip algorithm."
"the objective function in this work is the monitoring area coverage rate φ, the node in the monitoring area establishes the wireless sensor network coverage optimization model according to the previous formula."
"using the vd algorithm, the monitoring area was divided into several cells using the selected sensors for full coverage. each sensor had its own cell that gave us a 40-cell area with no coverage hole found."
"to heal the coverage hole found by the vd algorithm, the pip algorithm was applied to find the sensors inside the hole cells. the pip algorithm found three sensors in these two cells and then the coverage algorithm was reapplied to select one of these three sensors. we then latter selected a sensor that covered the hole and increased the coverage rate to 99.13%. to heal the coverage hole found by the vd algorithm, the pip algorithm was applied to find the sensors inside the hole cells. the pip algorithm found three sensors in these two cells and then the coverage algorithm was reapplied to select one of these three sensors. we then latter selected a sensor that covered the hole and increased the coverage rate to 99.13%."
"the notion of gbest (global best), is modeled on the pso global version (psog) where all the particles of the swarm are derived from the particle i. at the beginning of the algorithm, the particles of the swarm are initialized randomly/regularly in the search space. subsequently at each iteration, the particles move, merging the three components:"
"after defining the location of the cell that contains a coverage hole using the vd, the next step is to find other sensors inside this cell in order to reapply the coverage algorithm and select the suitable sensors to deliver full coverage in the roi. to complete this stage, we will use the pip algorithm."
"for the fixed-phase-sequence multi-phase traffic signal control at the intersection, when different-direction traffic flow is unbalanced, it often happens that one phase with few vehicles gets the right of way, and other phases with more vehicles have to wait. it will increase more vehicle delays. with changeable phase sequence control, this situation can be avoided."
"according to the results presented above, the curves of the coverage rate, with and without optimization according to the number of the deployed sensors, are shown in figure 7 ."
"this paper proposes a real-time traffic signal intelligent control method with transit-priority. the objective of this method is to reduce the delays of passengers and special vehicles. first, the phase with more urgency is preferential to be selected as the next running phase by the end of current phase. this embodies transit-priority idea. second, the green increase time of current phase is inferred by a fuzzy controller of which the inputs are the vehicles number of current phase and next phase. multi-layer neural network is used to realize this fuzzy controller. compared with the traditional fuzzy control method and fixed-time control method, simulation research shows that this method obtains a good performance in decreasing the delays of passengers and special vehicles."
"the appreciation of the quality of its position is stopped by the value of the \"objective\" function at this point. it is essential that this particle can memorize the better position by which it has already passed, formulated as follows:"
"using the vd algorithm, the monitoring area was divided into several cells using the selected sensors for full coverage. each sensor had its own cell that gave us a 40-cell area with no coverage hole found."
"the population in the pso algorithm is called the swarm, and each individual in the group is called the particle. the displacement of any particle (as indicated above) is governed by very specific rules and conditions and is influenced by the movement of other particles in the neighborhood."
"output the optimal fitness and the corresponding particle position. in the simulation condition mentioned above, the detection radius r will be constant and equal to 20 m, and the number of sensors used will vary between 20 and 40 to determine its impact on the performance of the coverage. the iterations-coverage rate and the coverage effect are shown in table 2, and figure 3 to figure 6 . during the optimization of coverage by using the pso algorithm, the coverage optimization went through four stages, first the deployment of 20 sensors, then 30 sensors, followed by 35 sensors, and finally 40 sensors. the simulations results are as follows:"
"in reference [cit], 20 sensors from total 40 sensors were used to completely cover an area of 20 m 2 . therefore, the number of deployed sensors was optimized by 50%. in our work, the deployment of 40 sensors from a total number of 100 sensors was enough to fully cover an area of 100 m 2 . therefore, we optimized our deployment by 60%. it is obvious that the pso algorithm successfully found the best deployment strategy for this application. the position and the number of deployed sensors were determined to fulfill the goal of maximum coverage of the monitoring area."
"the 30-sensor deployment result graph started with a 92% coverage rate and reached almost 95% after few iterations and remained practically stable until 150 iterations. after that, the algorithm started looking for a better result until iteration 225 where the final optimal coverage rate was 96%."
"this project is defined in order to find the best distribution of the deployed sensors to obtain full coverage of the monitoring area. to achieve our objective, the system implementation will be passing through three basic phases:"
"the pso algorithm allowed us to optimize a total coverage of the roi and to collect the best positions of the deployed sensors. after a period of time (one hour), the base station updated these positions leading to one of the deployed sensors out of the coverage network. following this, the vd will be used to define the position of the coverage hole."
"in this paper, the selection of next phase is according to the traffic urgency of every phase. the traffic urgency of phase is related to the vehicle type, the passenger number of vehicles, the vehicle number and the stopping time of the vehicles in this phase. all vehicles have urgency weight. the phase with highest urgency will be selected as the next running phase."
"in the modern urban transportation system, traffic congestion is more and more serious. traffic signal control plays an important role in alleviating this situation. it is difficult to build accurate mathematical models for a traffic system which is a time-variant stochastic complex system. a fuzzy control system, which imitates the fuzzy concept of the human brain and successful control strategy, is applicable to the time-variant traffic control system. pappis [cit], and many further researches using fuzzy logic technology are taken placed for the traffic signal control."
"server application. the server application is implemented in nodejs, instantiating the centralized server of our model. nodejs is a runtime environment to develop javascript server-side applications and offers a rich set of readily available utilities through a package ecosystem, called npm, structured into packages and modules. npm packages are accessible directories, uniform resource locator (urls), and/or repositories containing npm modules. modules are javascript folders or files containing code that can be run by a nodejs program. our implementation relies, among other, on the following npm packages as follows:"
"training sets are formatted as data tables; table 1 is provided as an example to summarize these data sets. the first column is the identity column, which presents the sequences of the peptide. each row of the data table corresponds to one peptide sequence. the feature columns list the corresponding values for each peptide depending on the amino acid properties and the summarizing function. the final column is the label of antibacterial activity. a condition is a value interval for a feature. the intersection of conditions is a rule, as shown in fig. 1 ."
"specific features were developed for the central server and, more specifically, for the nodejs application described in section \"network infrastructure\" to perform data validation, storage, and task control operations. the server application was able to record data from visitors through initial and final questionnaires, and robot session logs, by storing the states of the rca in real time. the information we collected and stored in the mongodb was structured as follows."
"motivation and contributions: a cs has powerful computation and storage capabilities. with a cs, doctors can utilize medical data from sensors and machine learning algorithms to perform efficient auxiliary diagnosis to reduce their heavy workloads. patients living in remote areas can also hold high levels of disease diagnosis. in order to preserve dc from medical sensors and pp, and avail of a cs to complete auxiliary diagnosis with machine leaning algorithms over encrypted medical data, we propose a secure and efficient md system, which can carry out the following functionalities."
the aaindex1 has 544 properties with one value for each of the twenty naturally occurring amino acids [cit] . a database of all properties is available in the r package 'seqinr' [cit] . we constructed an autocorrelation matrix of these properties to provide pairwise correlation comparisons for all 544 properties. we filtered properties using an absolute correlation value cutoff. we randomized which property to keep by randomizing the order in which the properties were compared.
"according to our encoding scheme in gf(2 ), the user encodes two one-bit integers x 0 and y 0 as polynomials. then, x 0 and y 0 are encrypted, and uploaded to cs. cs performs homomorphic comparison with comparator:"
"application level. the application level accesses the hardware features through service level apis and enables adequate interfaces between visitors and robots. it relies on two main components: controllers and uis. a/v and rca controllers are managing the traffic of a/v and robotic control streams, and ui features aim at representing these streams and handling visitor inputs."
"the visitor environments are also varied and multiple features can be required to develop a successful interface to enable real-time robot control and engage the visitor with a certain sense of presence in the robot environment. the objective of developing a simple, easy-to-use, and effective interface to fulfill these purposes is usually challenged by, for instance, the technological skills of the visitors involved, the level of interaction required with the robot environment, and the multiple visitor devices and platforms targeted by the interface."
"1. to prove the suitability of the proposed architecture based on open-source technologies (html5, webrtc, iot protocols, and rcas) to integrate videoconferencing and robotic control features in real time; 2. to demonstrate the cross-platform compatibility of the solution deployed; and 3. to intensively test the application in a challenging test environment with novice users, multiple client devices, and uncontrolled network conditions."
"homomorphic retinal image processing of dr in our iot system cannot reveal the contents of retinal images of dr, including the pixel value, the feature point location, and the extracted feature vectors. in the meantime, homomorphic machine learning cannot reveal the classification results of retinal images of dr. the reason for getting the above results is that image processing and machine learning are implemented with homomorphic evaluation based on ideal lattice over encrypted data. the hospital internal data interaction makes use of the authentic and secure channel between the h's servers and doctor's computers [cit], so it cannot reveal the contents of retinal images of dr."
to increase the complexity in the environments and situations where the application is evaluated: experiences have demonstrated the validity of the system to operate in a relevant test environment with multiple realistic conditions and next step is to address its deployment in more ambitious environments and more complex situations. taking the telepresence robot to real homes and evaluating remote social interactions with the primary user are the extensions planned in this line.
in this study we test our rough set theory classification method to differentiate antibacterial peptides from apd2 [cit] (antimicrobial peptide database 2) and randomly selected peptides from the uniprot database [cit] . these benchmark datasets are available online [cit] .
"a first set includes 176 trials carried out with rhodon. the aim of this set was to test the overall performance of the system and to validate its integration with the ros architecture installed on rhodon. for security reasons, we have not considered the free access of inexperienced users to the rhodon platform. a set of 127 exploration tasks belonging to the open pilot study conducted on the giraff platform. the giraff platform is more suitable to be teleoperated by novice users as it is specifically designed for domestic environments. trials include 116 sessions successfully ended, accumulating a total of 2.6 km in 9 h 40 min. the 11 sessions that were not completed include 6 sessions that exceeded the time limit, 3 sessions in which the connection of a/v was refused because of a corporate firewall, and 2 sessions in which the users canceled before ending the task."
"3) finally, for the previous constructed tree, we implement homomorphic break to form different clusters, which correspond to the encrypted multiretina-image matching results of dr. namely, we complete the lesions detection. by our hcs scheme, we can compute."
"iot system uses our quantum-resistant homomorphic encryption scheme. therefore, the encrypted data stored in cs, h's servers, s i, and doctors' computers cannot reveal the contents of retinal images of dr for patients."
"in this section, we present she, speed-up robust features (surf ) (feature extraction of a medical image), and machine learning schemes. these schemes are the basis for constructing our efficient iot system."
"it also implements two local services, namely, a message broker enabling the server application to communicate with clients using the publish/subscribe messaging pattern 34 and a database to store application data and session logs. the utilities selected to implement these services have been mosquitto mqtt message broker and mongodb database."
"in using the rough set theory approach, we modified existing approaches by combining the features of mlem2 (modified learning from examples module, version 2) method [cit] with a feature of the module irim (interesting rule induction module) to potentially improve our selectivity and specificity [cit] . we modified the mlem2 method by adding the ability to limit the condition number for each of the rules, a feature of irim. because the irim method exhaustively searches all possible rules given the number of conditions, it cannot be used for large numbers of conditions or large numbers of peptides because the runtime grows exponentially with the number of conditions."
"the inherent heterogeneity of robotic telepresence systems requires a generic architecture to structurally deal with the combinations of the network, robot, and visitor environments. in order to provide a model to cope with such heterogeneity, we first group the functionality of the identified components into essential building blocks and, then, establish the connections between these blocks and their interfaces."
"hardware level. in this case, the hardware elements are mainly desktop/mobile devices with screens of different sizes and varied input elements (e.g. keyboard, mouses, or touch screens). it includes cameras and microphones for enabling videoconference, which is a common requirement in applications considering users in the robot environment."
"on the other hand, server-relayed communications are composed of (i) the http channel, used to communicate with the server application through request/response messages; (ii) the mqtt channel that establishes a bidirectional communication between visitors and robots transmitting robot commands and sensory data; and (iii) the relayed-media channel, included as a fallback mechanism for exchanging multimedia streams when point-topoint communications are not available."
"q1. the robot was easy to teleoperate q2. the robot behaved as i expected q3. it felt natural driving the robot q4. i performed the task without stress q5. the task was easy q6. crossing doors was easy q7. taking the photo was easy q8. the interface was simple and intuitive answers: (strongly disagree (à2); disagree (à1); undecided (0); agree (þ1); strongly agree (þ2)) part two, free-text question:"
"hardware level. the hardware level is composed of the physical devices in charge of processing server operations. multiple configurations can be considered, ranging from a single central computer to distributed computer networks."
"lastly, in regard to the third environment, the network environment, the next step is to adhere the architecture and technical features presented to standard ict infrastructures of aal deployments. that means an extension of the middleware to support other application frameworks in the field like, for instance, the univeraal. 40 the enhancement of the current network infrastructure with features to discover, manage, and configure robotic telepresence services is relevant to increase the robustness and flexibility of applications. the study of security and privacy issues across the distributed communication infrastructure is another aspect to be addressed. the research projects mpo-wer, 41 co-living, 42 and giraffplus 43 define and implement effective middleware solutions for aal contexts and we will take them as initial references."
"the experience enabled us to test the overall performance of the web-based application developed in a relevant test environment. the application instantiates the architecture proposed in this work and the results achieved fulfill our initial objectives. to prove that, we have presented a pilot experience with a considerably high number successful exploration tasks, and a high variability in the characteristics of the users, devices, and internet connections involved. the good rate of successful task executions and the positive evaluation received from users confirm that, in general terms, we conveniently tackled the challenges addressed, which means that our approach is suitable to build a full-stack solution for robotic telepresence based on open-source technologies, accessible over the web, and with cross-platform compatibility for visitor and robot environment applications."
"in general terms, the results of the experience permit us to evaluate positively the cross-platform performance of the developed application. first, the visitor interface successfully performed for a wide range of configurations: different versions of five oss (windows, linux, mac os, android, and ios) and four web browsers (chrome, firefox, chromium, and opera). second, communications across the distributed architecture and local system integration at each environment were effectively supported by the iot standard protocols. concretely, the nodejs server, the web visitor interface, and two different rcas (i.e. ros on linux and moos on windows) were able to share data using the mqtt clients through a simple api. third, although we only used one server configuration in the experience (windows þ nodejs), it is important to mention that the nodejs environment is available for windows, mac os x, and linux platforms. nodejs extensions are also cross-platform and ready to install from generic javascript package managers like the open npm repository (www.npmjs.com). thus, the portability of the server-side solution between windows, mac os x, and linux machines is straightforward."
"it integrates efficient and robust middleware communications based on internet of things (iot) protocols like http, websockets, and mqtt. the use of these standards provides well-structured mechanisms for communication and it permits the inclusion of third-party components."
"for example, aal and telecare applications usually deal with nontechnical visitors who require an interface as intuitive and easy to use as possible. in contrast, professional applications focused on environmental monitoring or mobile video surveillance account for the possibility of training the visitors to control more complex interfaces; thus, the goal is not on the ease of use of the interface but on its robustness and efficiency to support the operator task."
"the world has been changed by iot. taking advantage of iot and she, we implement privacy-preserving dr detection early. in this article, we provide an simd she fv scheme. based on the simd she fv scheme, we realize efficient homomorphic comparison and division schemes. with the help of our findings, we provide the privacy-preserving homomorphic surf and fast multiretina-image matching schemes, which can perform efficient feature point detection and image matching for retinal images of dr. finally, by means of homomorphic fast multiretina-image matching, we implement an efficient privacy-preserving remote diagnosis for diabetes. computation of diagnostic process is homomorphic evaluation in our iot system. hence, pp is protected. at the same time, our encryption scheme based on lattice, which is quantum-resistant, can preserve dc even in the era of quantum computation."
"1) system starts and calls for the camera connected to rp to acquire patients' retinal images of dr. rp calls for the encryption algorithm, performs encryption operation and uploads the encrypted retinal images of dr. h's ca center distributes the digital certificates for the public key, which is used for s i to execute our simd she scheme. we show this as process 1 in our iot system architecture. at the same time, h's servers and doctors' computers build a secure and authenticated channel for secure data and doctors' final diagnosis transmission. we show this as process 6 in our iot system architecture. such secure and authenticated channel is easy to implement by post-quantum key exchange protocol [cit] . this is beyond the scope of this article."
"in this section, with the aid of our homomorphic surf and fast multiretina-image matching schemes, we provide experiments about homomorphic lesions detection of the dr and privacy-preserving remote dr diagnosis. fig. 5(a) shows the efficiency comparison of lesions detection between the original multiretina-image matching and our homomorphic multiretina-image matching schemes. utilizing our homomorphic multiretina-image matching scheme, a doctor can obtain the results of lesions detection and diagnosis in 7 min. it is more efficient than manual work in hospital, and greatly reduces the doctors' workloads. fig. 5(b) shows the communication cost comparison of lesions detection of retinal images of dr between our homomorphic surf scheme with hcs and garbled circuit schemes. the results prove that our hcs scheme greatly reduces the communication cost between cs and the hospital's server."
"mosquitto mqtt broker is an open-source message broker that implements the mqtt protocol, providing compatibility for tcp/ip and websockets. in addition, it provides security (optional) and different levels of quality of service."
"visitor profiles. during the registration process, the server stored user data and their skills on using icts through an online web form (see table 2 ). in subsequent sessions, visitors could directly access the system using the given credentials. in those cases, the ict form was skipped and a new session was stored attached to the id of the visitor logged in."
"service level. services in this case are reduced to the core features of the os but, in most cases, the process followed to install the main application extends the system features with small pieces of software, that is, plug-ins."
"the mqtt package integrates a client mqtt library enabling nodejs apps to communicate with the mqtt broker from the service layer. the easyrtc package implements a webrtc signaling server based on the socket.io library for enabling the connectivity and relaying a/v media streams between webrtc clients. the express package provides lightweight and robust tools to develop http servers, particularly focused on single-page and hybrid applications. the passport package contains an extensive set of authentication strategies prepared to be integrated with express applications. finally, the mongoose package includes effective utilities to integrate nodejs apps with mongodb. this package has been used to provide data persistence, for example, server status, session logs, and visitors/robots information. middleware. the middleware provides convenient interfaces for allowing visitor and robot clients to connect with the http server in the server application, sign up/authenticate in the database to gain access to the network infrastructure, retrieve the web resources (i.e. media, html, java-script, and css files), discover and connect to other clients connected to the network infrastructure, broadcast/receive robotic control data using the mqtt publish/subscribe mechanism, and broadcast/receive relayed a/v streams when two clients cannot communicate directly."
"our iot system preserves the confidentiality of the retinal images of dr and pp [cit] . in our threat model, we mainly consider the following threats."
"is the feature set for the i (i) th retinal image of dr. because pairwise matching for retinal images of dr is inefficient, our application implements the multi-image matching scheme called multi-image matching via density-based clustering [cit] to classify the retinal images of dr. the core part of multi-image matching via density-based clustering is the quick-match scheme. it is an efficient data clustering scheme with linear time complexity."
"however, transmission, storage, and utilization of the data acquired from sensors of iot involve data confidentiality (dc) and users' privacy [cit] . with the acquired data from sensors in a house, malicious attackers can control and take advantage of house appliances, and even spy on a user's life. egregious attacks can destroy industrial and agricultural production, and even threaten a user's life in intelligent driving. for industrial and agricultural users, commercial competitors can attack sensors to destroy industrial and agricultural production of their competitors. in the field of intelligent driving, once attackers acquire data from vehicle sensors, they can not only obtain driving states and tracks of vehicles but also maliciously cause traffic accidents by controlling vehicle sensors."
"antibp [cit] was one of the first online available services for antibacterial peptide prediction. antibp uses a sliding window of 15 residues to predict the classification using support vector machines (svm) [cit], quantitative matrices (qm) [cit] and artificial neural networks (ann) [cit] . the strength of this approach is that the order of amino acids impacts the prediction. however, the weakness to having a constant window of amino acids is that the predictions are peptide-length dependent [cit] . to overcome the peptide length dependence, another method camp (collection of antimicrobial peptides) [cit] was employed to use descriptors that summarize composition, physicochemical properties and structural features of the peptides. camp uses multiple machine learning approaches for these features such as svm [cit], ann [cit], discriminate analysis (da) [cit] and random forest (rf) [cit] . however, the descriptor approach is insensitive to the sequence order arrangement. for example, full-length sequence descriptors can be sensitive to the overall charge of a peptide but not its charge distribution. iamp-2 l (antimicrobial peptide prediction two-level) [cit] partially addresses the order insensitivity by calculating the autocorrelation of amino acid property values within the amino acid sequence. other descriptors do not account for the order of the sequence [cit] . because the iamp-2 l classification algorithm is based on a fuzzy k-nearest neighbor algorithm, clusters that are invariant for descriptors that include correlations would be sequence-order insensitive. this approach is also sequence-order insensitive to sequence rearrangements that preserve the correlation structure from the original peptide. evolutionary feature construction [cit] (efc) method addresses this need by achieving order-sensitive classification by combining order sensitivity and length independence by selecting common chemical property sequence patterns for antimicrobial peptides. length-independent classification is achieved with a support-vector machine method through physicochemical descriptors selected by fcbf (fast-correlation based filter selection) [cit] . while this method does combine order-sensitivity and length-independence, it does not completely address either of these issues. order-insensitivity is possible based on the rearrangements of amino acids that are indistinguishable by the pattern recognition scheme of compressing 20-amino acids into four categories."
"our method has high specificity and similar accuracy for antibacterial classification as other current methods. when using a classification method for the discovery of antimicrobial peptides, the specificity of the method is more important than its selectivity [cit] . our method prioritizes specificity with low false discovery rate (fdr) by classifying sequences that do not meet any rule in the applied rule set as inactive (fig. 3) . in fact, there is only one method, which provides lower fdr compared to our method, i.e. efc + 307-fcbf. however, our method results in similar specificity starting with fewer physicochemical properties. the robustness of this method may be potentially improved with ensemble learning and voting scheme approaches. if our method provides unique descriptions of activity, then it will reduce the overall fig. 3 false discovery rates of comparative antimicrobial peptide classification methods. cln-mlem2 achieves a low false discovery rate among currently available antimicrobial peptide classification methods false discovery rate of the ensemble method and voting scheme approaches. cln-mlem2 has been shown to be useful for the learning task of predicting antibacterial activity from a peptide sequence. this learning task is related to multi-instance learning. a classic literature example of a multi-instance learning problem is in drug activity prediction [cit] . active molecules have at least one conformation that interacts with a drug target, while inactive molecules have none. the challenge is to identify which conformations interact with the drug target. each drug has one molecular formula, but it can have many conformations. each peptide also has one sequence but many physicochemical property values. the cln-mlem2 method has found the most relevant physicochemical property features that relate to the activity of the peptide sequence. this cln-mlem2 method can also be applied to the multi-instance learning case of describing the conformations of peptides are active."
"the telepresence robot provides the visitor with the sensing and acting capabilities required to perform and to achieve a certain feeling of presence within the robot environment. it consists of an mr base endowed with a videoconferencing set; a network adapter; and, in some cases, additional sensors, actuators, and interfaces for human-robot interaction. our architecture considers the heterogeneity in the functionality and component configurations that can be required in telepresence robots by grouping them into hardware, service, application, and middleware levels."
"on the one hand, although our proposal based on opensource solutions is convenient for small and medium robotic applications that can benefit from cost-free videoconferencing services and full control of the system design, applications aimed at reaching a robust, efficient, videoconference with optimized a/v quality at large-scale should, still, adhere to professional videoconferencing services. the efforts required to enhance our application with the scalability and reliability of purpose-specific services are not worth it for multidisciplinary teams focused on multiple aspects of robotic telepresence applications. thus, the focus should be placed on searching the most convenient and flexible professional ict infrastructure to integrate robotic streams."
"such as the advanced encryption standard (aes) scheme, the triple data encryption algorithm (dea), the elliptic curve integrated encryption scheme (ecies), etc. (see [cit] . after acquiring encrypted patients medical data by the medical sensors, doctors decrypt the encrypted medical data, and do with the decrypted medical data to make a diagnosis for patients diseases. another case is that doctors want to make full use of a cloud server (cs) for the auxiliary diagnosis to reduce their heavy workloads. when doctors use a cs to perform auxiliary diagnosis with machine leaning algorithms, a cs first decrypts the encrypted medical data acquired from medical sensors, and then utilizes the decrypted medical data and machine learning algorithms to perform efficient auxiliary diagnosis directly. neither of these cases preserves dc from the medical sensors, because the medical data is decrypted when performing medical diagnosis (md). therefore, there is the possibility of breaching pp. currently, the rapid development of quantum computation and computer heralds the advent of the era of quantum computation [cit], while the traditional encryption schemes are not quantum-resistant encryption schemes, such as triple dea, ecies, paillier, etc. (see [cit] . in order to perform the auxiliary diagnosis with encrypted data and machine learning schemes via a cs, and protect users' privacy in the era of quantum computation, an efficient quantum-resistant homomorphic encryption scheme is one of the best options."
"however, in spite of the fact that web-based solutions are, in general terms, the most effective approach to develop cross-platform compatible and rapidly accessible solutions, the achievement of such generalization is not always possible and, in general, it is achieved at the cost of specific limitations in the overall application performance. specifically, direct access to local resources and efficient management of the hardware are the main limitations of pure web solutions in comparison to approaches accounting for native components. thus, despite native applications require specific downloads and operate for a unique platform, applications targeting the intensive use of the local hardware and efficient computing should rely on this conservative approach. hybrid applications are the third alternative to deal with these aspects; they are web applications wrapped by native software components. this approach offers a balanced solution between the advantages and disadvantages of the other two. thus, to evaluate the suitability of the hybrid approach, it is required a detailed analysis of the client application requirements and native wrapper specifications in order to find out if all the features required are available."
"and e(h 0 ) with scale (l 0 ) and location (e[(x 0, y 0 )]) can be computed with homomorphic evaluation in the box space as follows:"
"learning: in our medical iot system, all computations are performed with homomorphic evaluation. a doctor can directly utilize a cs to perform auxiliary md by homomorphic image processing and machine learning. this greatly reduces the doctors' workloads, and improves the diagnosis efficiency of doctors and service capability of hospitals. 3) quantum resistance: quantum computation model makes many hard problems (in classical computation) used in cryptography much less difficult. the quantum computer would render all widely used traditional encryption schemes insecure [cit] . our homomorphic encryption scheme based on lattice is quantum resistance. all of the medical data in our iot system is also secure under the quantum computation model and a quantum computer. medical data can be securely transmitted and stored in our iot system for a long time. to realize the above functionalities, we use somewhat homomorphic encryption (she) proposed by fan and vercauteren (fv) as the basic encryption scheme [cit] . the fv scheme is based on ideal lattice (quantum resistance), and has small public key. in the meantime, we employ single instruction multiple data (simd) to pack multibits into a single ciphertext, and perform parallel homomorphic evaluation. packing technology can compress ciphertext for transmission and storage. parallelism improves the efficiency of homomorphic evaluation. with the help of above optimized fv scheme, we design the new efficient homomorphic comparison and division schemes. homomorphic comparison scheme (hcs) has low communication cost between two parties. utilizing our designed homomorphic schemes, a cs can implement medical image processing (mip) (simd is very efficient for image processing [cit] ) and machine learning about auxiliary md over the encrypted medical data."
"to begin the defined-condition number mlem2 (modified learning from experience module 2) method, we generate multiple summaries of the amino acid sequences of the given active and inactive peptides by selecting non-correlated amino acid properties in the aaindex1 [cit] (amino acid index 1). among the 544 properties of the aaindex1, many of the properties are highly correlated. the autocorrelation matrix of the aaindex1 properties was calculated as the pairwise pearson correlation value of each pair of properties in the index. the heat map of correlation values for the autocorrelation matrix is shown in fig. 2a . positive correlation is magenta and negative correlation is teal. non-correlated amino acid property pairs are white. the autocorrelation matrix shows that most amino acid properties are highly correlated. we studied how many amino acid properties are below a correlation threshold for all other amino acid properties (fig. 2b) . we performed 60 repetitions with random initial properties of eliminating properties more correlated than a threshold. we found a very tight trend of how many uncorrelated properties there are for a given cut-off value. for further study, we selected a correlation cut-off of 0.65, which resulted in 74 properties remaining from the original 544 properties."
"it provides an open-source, web-based user interface (ui) for robot teleoperation, which enriches the solution with interesting features like crossplatform compatibility, accessibility, and maintainability. the web application presented relies on nodejs, html5, and webrtc technologies to enable these features. the implemented robotic telepresence system is featured with self-localization. for that, the system includes interfaces with robotic control architectures (rcas), based on, but not limited to, the widely used ros 5 and moos 6 robotic frameworks."
"note that the robots considered in our evaluation were equipped with laser scanners for self-localization, but no collision detection/avoidance algorithms were included in the rca. thus, the visitor had the responsibility to not crashing the robot with the furniture nor people around."
"making use of bfs, we can complete a simplified approximate computing of the hessian matrix. the convolution of bf with the discrete image i can be computed with the integral imagẽ i:"
"protein and peptide sequence-based classification methods have been extensively developed to improve the understanding of the functionality of polypeptides [cit] . by using rough set theory, our method builds rules that distinguish between active antibacterial peptides from inactive antibacterial peptides. the developed method was benchmarked against methods including a recently published method efc [cit], based on motif-recognition, as well as against a larger set of methods from publicly available prediction servers. the first benchmark test is a tenfold cross validation on a dataset used in previous studies [cit] with the positive sequences clustered from the apd2 (antimicrobial peptide database 2) [cit] to 115 clusters and the negative sequences from the pdb [cit] clustered to 116 clusters. each cluster is represented by one sequence. the results were compared with efc-based methods and support vector machines given subsequences of lengths 5 to 8 amino acids. table 3 demonstrates that our method has high selectivity and accuracy in comparison to the performance of the svm methods, and comparable selectivity and accuracy in comparison to the efc method. a trend of decreasing mathew's correlation coefficient (0 for random guessing and 1 for perfect performance) as the length of the subsequence increases is seen in table 3 . our subsequences in cln-mlem2 are 3 amino acids long and may have helped to contribute to our improved performance for using a single length of subsequences instead of combining four different lengths in the efc method. we further tested our modified mlem2 method against a larger variety of classification methods. the second benchmarking test uses the iamp-2 l dataset [cit] . like the dataset used for the first benchmark, this dataset is derived from the apd2 database. however, instead of choosing a single sequence from each cluster, the sequences were narrowed by removing sequences with greater than 40% similarity as measured by cd-hit [cit] only with cluster of more than 250 sequences. this resulted in a testing positive dataset of 848 unique sequences. the negative sequences were from a uniprot search of cytoplasmic proteins, also with less than 40% similarity. 2405 unique sequences were included in the negative dataset. the positive training data set was the s1 set (\"antibacterial\") from iamp-2 l, which has 1274 unique sequences. the negative training set of data was the non-amp data set from iamp-2 l, which has 1440 unique sequences."
"network environment configurations can vary according to the requirements of particular application contexts. the main distinguishing aspects are the signaling procedure between end points, the communication channel, and the network topology."
"hri studies are always complex due to their demanding requirements to set up a convenient evaluation environment 35 (i.e. participant selection, robot autonomy assessment, study length, replicability, and appropriate selection of statistics). they combine interdisciplinary aspects from technical and social fields and it is hard to follow/develop an effective methodology to cover all of them with accuracy. the scope of the study done in this first stage of development is limited, aiming to illustrate the feasibility of our approach and to identify open issues to be addressed in next iterations. both the system and its evaluation can be significantly improved in multiple aspects, from which we highlight the following:"
"we make use of the gaussian pyramid to construct the scalar space. the gaussian pyramid is divided into different octaves, which are divided into a lot of levels. different octaves have the same image size. but the template sizes of bf are increased gradually. different levels of an octave have the same filter, but fuzziness is different."
"this article contributes in (i) identifying the sources of heterogeneity found in robotic telepresence systems, (ii) defining a generic and modular architecture that deals with such heterogeneity with special emphasis on the convenient integration of multimedia and robotic control streams, and (iii) presenting an instantiation of the proposed architecture into a web-based robotic telepresence application."
"most of the solutions found in the literature tackle robotic telepresence by relying on proprietary software (e.g. skype or vsee) in combination with specific plugins to integrate robotic control commands, 33 which is the approach followed by social robot and giraffplus platforms. the strength of these solutions relies on the efficient management of networking, videoconference, and realtime communications provided by expert companies, but this strategy has several disadvantages when it comes to the design of features for research prototypes like the aforementioned. in general, prototypes following this approach are excessively influenced by the application programming interfaces (apis) provided by manufacturers which, in most cases, lead to slow development cycles and hinder the reusability of the system features achieved."
"to increase the robot autonomy and develop adaptive user interfaces. the enhancement of the robotic platform with autonomous features and assisted maneuvers are the basic means to ease the teleoperation task. in addition, given the different visitor teleoperation skills, the design of adaptive interfaces providing specific configurations for different user profiles or situations is the adequate complement to autonomy in order to improve the teleoperation experience. autonomous navigation to specific points, assisted driving, and autonomous docking are some of the most relevant features to be integrated in the next iteration."
our iot system mainly performs privacy-preserving auxiliary md through image processing and machine leaning schemes with a cloud service. the execution process is as follows.
"the hardware level is composed of the sensors/actuators, processors, and firmware that work at the lowest level. the service level includes the os, a webrtc compatible web browser, and the additional drivers that can be required to integrate additional robotic hardware. for the application level, we rely on a hybrid structure that combines a web application with off-the-shelf robotic control architectures. finally, the middleware level communicates the robot client with the web server from the network infrastructure through http request/responses, transmits and receives a/v data over websockets using the webrtc api, and relies on the mqtt data channel enabled by the central message broker to exchange robotic control data."
"in this section, we provide the efficiency analysis of our homomorphic surf scheme for feature detection of each retinal image of dr. at the same time, we compare communication cost of the feature detection via our hcs scheme with secure comparison protocol based on garbled circuit between the cs and the h's server. the retinal image databases of dr (dr1 [cit], retidb database [cit], and messidor database [cit] ) include five abnormal findings in the eye fundus caused by dr: 1) microaneuryms (mas); 2) hemorrhages (hms); 3) hard exudates (hes); 4) soft exudate (se); and 5) neovascularization (nv). red lesions include mas and hms. bright lesions include hes and se [cit] . we mainly consider the retinal image of dr (containing mas, hms, hes, ses, and nv) for feature detection with the original surf and our homomorphic surf schemes. fig. 4(a) and (c) shows the same retinal image of dr. fig. 4(b) shows feature detection of a gray retinal image of dr with original surf scheme. fig. 4(d) shows the result of the feature detection (with our homomorphic surf scheme) for a gray retinal image of dr in the encrypted domain (the detection results recovered from encrypted data to decrypted data). the results of feature point detection shown in fig. 4(b) and (d) demonstrate our homomorphic surf scheme can efficiently detect the feature points over the encrypted retinal images of dr as the original surf scheme [cit] ."
"homomorphic encryption does not support homomorphic division directly. we can transform division into addition and multiplication, and then perform additive and multiplicative homomorphic evaluation over the encrypted data. in the following, we provide the detailed repeated-squaring and conversion algorithms for homomorphic division scheme (hds)."
"in order to detect different abnormal findings in the eye fundus caused by dr, such as mas, hms, hes, ses, and nv, we chose different parameters for our homomorphic surf scheme. the parameter modification is based on a certain number of running testing results about homomorphic surf and fast multiretina-image matching schemes for pois in the roi and the lesions detection. in this way, according to the feature points detecting results of the encrypted retinal images of dr, we can implement the lesions detection about different abnormal findings in the eye fundus caused by the dr for our efficient homomorphic fast multiretina-image matching scheme. fig. 4(e) shows the efficiency comparison of feature point detection between the original surf scheme [cit] and our homomorphic surf scheme. in about 10 min, our homomorphic surf scheme can finish the feature point detection over the encrypted retinal images of dr. fig. 4(f) shows the communication cost comparison between our hcs scheme and secure comparison protocol based on garbled circuit [cit] during the homomorphic feature point detection. the result proves that our hcs scheme greatly reduces the communication cost. our hcs scheme used for homomorphic feature point detection is more efficient than secure compare protocol based on garbled circuit [cit] ."
"connectivity between visitor and robot environments requires different signaling processes depending on their accessibility to the network. for example, in small lans, messages can be issued using local internet protocol (ip) addresses. however, in large networks or public connections, the devices operate behind network address translators (nats) and/or security firewalls, which prevent the direct communications. under these circumstances, the connection requires networking techniques like the interactive connectivity establishment and additional network elements providing session traversal utilities for nat (stun) and traversal using relays around nat (turn) services."
"in this section, we describe an experience with the robotic telepresence application deployed in a varied set of realistic and heterogeneous conditions. the purpose of this preliminary study is to rapidly and informally assess, from a multidisciplinary perspective, the overall suitability of the architecture, implementation, and technology selection presented in this work to cope with the heterogeneity in telepresence robot applications. to that aim, we have designed a web experiment that enabled us to perform intensive, unsupervised tests with users from outside the research laboratory, facilitating the access to novice users (a total of 64 participants) and ensuring a high variability in the devices and internet connections involved. three specific objectives were set for the results of the trials as follows:"
"visitor id. id of the registered visitor driving the robot. visitor device. data of the visitor device in the session. visitor's device information contains useragent and x-forwarded-for \"ip\" parameters of the interface http requests. these parameters allow the server to distinguish the os, web browser, and public ip of the visitor device (except in cases where the user or the network in the visitor environment hides this information for privacy reasons). the registered sessions include connections from 51 different ips and multiple visitor device configurations. table 3 shows a summary of the stored configurations. robot id. the telepresence robot used: rhodon or giraff. rca states. a set of time-stamped data from the rca, including the robot pose, charging status, speed, and so on. these states are sent at a frequency of 4 hz."
"thus, a common alternative followed by researchers in the area is to avoid the use of these proprietary multimedia services and stream the audiovisual data using technologies and protocols ready to use with state-of-the-art rcas like, for example, the ros web video server application (http:// wiki.ros.org/web_video_server). the main problems of this approach are the low efficiency in multimedia communications and their limited networking features, which reduce the validity of the solution to operators and robots that are connected within the same local area network (lan)."
communications. communications consist of a message bus comprising a point-to-point and a server-relayed channel. point-to-point communications are performed through a websocket channel exclusively used for exchanging multimedia streams. this channel allows efficient bidirectional communications over a single socket when a direct communication between clients is found during the session establishment.
"we can directly compute e[ cos(θ k )] and e[ sin(θ k )] via homomorphic evaluation. in this way, there are four values in each subregion"
"to conduct more accurate, specific user studies: after a preliminary feasibility study has been carried out with satisfactory results, next step is to go in-depth with more specific evaluations. to that aim, it is required to focus on specific aspects of the targeted application and, in this line, an important step is to involve potential users in the evaluation loop, for example, health-care professionals or families in aal applications. a convenient participant selection with detailed user profiles and balanced samples is essential to obtain relevant insights, draw formal conclusions, and generate useful inputs for next iterations."
our approach simultaneously groups peptides and classifies them. we benchmark our rule set performance to other classification methods. some available classification methods are either sequence-order insensitive or lengthdependent. the rule sets our method generates combine order-sensitive descriptors with length-independent descriptors. we achieve comparable or improved specificity and selectivity to currently available methods with lower false discovery rates. the high specificity of our method aids novel antibacterial peptide discovery because a low false discovery rate reduces the number of bacterial assays.
"mongodb is an open-source database with crossplatform capabilities. mongodb relies on json documents with dynamic schemas, which facilitates the integration of data. we have implemented a structured database classifying these documents into four collections: user profiles, robot profiles, session data, and system logs."
"especially in the health and medical ecosystems, patients' health condition and medical information, which can only be accessed by their attending physicians, are sensitive [e.g., acquired immune deficiency syndrome (aids), hepatitis, cardiopathy, diabetes, etc.]. the leak of sensitive health condition and medical information could cause great distress and discrimination against patients, and indeed affect their treatment, work, and life [cit] . to protect dc and patient privacy (pp), we can utilize traditional encryption schemes to encrypt all of the data acquired from medical sensors. encrypted data from medical sensors is transmitted and stored in iot."
) includes the following four algorithms: 1) she.enc; 2) she.add; 3) homomorphic repeatedsquaring; and 4) conversion algorithms. she.enc and she.add algorithms are the same as in our simd she fv scheme. algorithms 2 and 3 present the homomorphic repeated-squaring and conversion algorithms in detail.
hardware. the network infrastructure is supported by a conventional pc that acts as a centralized server. this server is publicly addressable through its ip and stores the application data.
"the network environment exhibits diverse requirements in terms of connectivity, security, and network topology. our implementation deals with connections over lans and the world wide web, coping with firewall/nat traversal issues. if necessary, the communication channels can be secured, and multiple configurations in the network topology are considered, that is, one-to-one/one-to-many/ many-to-many connections. the robot environment is a structured, noncontrolled scenario where a number of people are sharing the space with the robot. the presented implementation considers two different robot configurations, including different platforms and hardware (giraff/pioneer), os (windows/linux), and robotic control architectures (openmora/ros). this is made possible since the middleware and interfaces levels within the architecture are easily adaptable and cross-platform compatible. the visitor environment considers multiple devices: visitors' own pcs, smartphones or tablets, and different technological skills. this situation is coped in our implementation by providing users with a crossplatform and adaptable web interface that operates without installing any software. figure 4 depicts the structure of the presented robotic telepresence application. next sections provide an in-deep description of its implementation."
"middleware level. in this case, the middleware connects the robot to the network infrastructure, exposing the robot api to visitor interfaces and communicating with the server."
"in this section, we provide our iot system architecture including a high-level overview. our iot system architecture contains the following three modules as shown in fig. 2 : encryption scheme, application, and management over the encrypted retinal images of dr."
"as a result, a functional robotic telepresence system has been successfully deployed and tested by inexperienced participants who accessed the robot through their personal devices and internet connections. the pilot experience enrolled 64 participants asked to remotely explore an unknown working office environment, without the intervention of any technical staff. during the exploration, the users had to approach some objects and dock the robot in the recharging station at the end of the experience. the reported tests cover the following components: two mobile robot platforms running different operating systems (oss), that is, windows and linux, and robotic frameworks, that is, ros, and moos, different user devices for teleoperating the robots, os, browsers, and internet connections. the aim of this pilot experience is to prove the suitability of our approach in the deployment of a functional and practical robotic telepresence system under heterogeneous conditions but not a thorough user study. the results that support the viability of our work consider the number of exploration tasks accomplished by inexperience users who only received a few indications on how to guide the robot."
"but the above methods need complex clustering (k-means) process (or hard-sum and soft-max) to create bovw (or max-pooling) for training two-class classifier (svm) of the lesion detectors, and a trained two-class classifier for the final classification about the individual lesion detectors."
"simd technology [cit] can implement the same operation on inputs parasynchronously. namely, in our simd she scheme, simd technology can perform homomorphic evaluation for -bits plaintext simultaneously."
"when cs performs the homomorphic multi-image clustering, it employs hcs and hds schemes. based on the security of our hcs, hd, and homomorphic surf, the homomorphic multi-image clustering scheme is coa and kpa security."
"service level. this level encapsulates the features required to support the server application. firewall/nat traversal, relayed communications, and data persistence are common services operating at this level. they can run in a particular server machine or integrated with the main application from cloud services provided by external parties."
"the obtained results are coherent with our initial expectations. the current version of the telepresence application and the experiment setup do not focus on the ease of teleoperating but on proving the correct, overall performance of the features described in this work. thus, it makes sense the fact that statements that evaluate generic aspects of the experience (q1, q4, and q8) are the best scored. however, table 4 . questions answered by the visitors after the experience."
"to prove the suitability of our approach to cope with heterogeneity in the robot environment, in this work, we present two implementations considering two robotic platforms. one is the so-called rhodon, a mobile robot built upon a pioneer patrolbot base, endowed with a robotic arm, a videoconferencing set, a range laser scanner, and a linux pc. rhodon is controlled using ros and includes self-localization abilities."
"for homomorphic image processing and machine learning (feature detection and description, feature point matching, clustering algorithm, etc.), homomorphic comparison over the encrypted data is a basic operation."
"on the other hand, applications with relaxed requirements in terms of quality and real-time performance, aimed to operate within lans, can avoid the efforts required to set up the webrtc services presented in this work. in such contexts, the communications and image-streaming features enabled by state-of-the-art rcas can be effective to evaluate simple prototypes. however, the cost of this simplicity is the reduced accessibility caused by the lack of convenient features to offer the networking and efficient data exchange required in real-world deployments."
"middleware level. it ensures the accessibility of the server application to visitors and robots through a set of apis. these apis enable a communication bus across the network layers connecting visitors and robots (i.e. gateways, switches, and routers). this bus integrates two types of communication: point-to-point and server-relayed communications. point-to-point communication channels exploit the optimal path available across the network, while serverrelayed channels increase the point-to-point path with at least one additional end point to provide an alternative network path if direct connections between visitors and robots are not available."
mirror snapshot. image obtained when the visitor performs the mirror subtask. user commands. this field registers the visitor's connectivity and the input method used to steer the robot at any moment.
"we seek to combine overall sequence chemical properties and motif properties to be able to account for how all of the residues may affect the chemical properties while still retaining the ability to separate classifications based on the ordering of the residues. if only chemical properties are evaluated by the sum or mean of the whole sequence, then the rules generated are sequence-order insensitive. by considering sub-sequences of the peptides, then the ordering of the chemical properties within the sequence can be used as a feature. we calculate two types of sequence property summaries from the selected amino acid properties in the aaindex1 (amino acid index 1) after removing the correlated amino acid chemical properties. first, we calculate overall property summaries as the mean and average of the properties of the amino acids present in the sequence. secondly, we calculate motif properties as the maximal subsequence sum of a given length of the amino acid sequence. our cln-mlem2 method can combine overall sequence properties and motif properties within a single rule. each rule forms a class of either active or inactive peptides."
"in total, 64 users were registered, 52 men and 12 women. the youngest user was 6 and the oldest one 62 years old. the range from 15 to 30 years represents 56% of the participants, followed by the range from 30 to 45, which represents 24%. participant ages out of the two main intervals are 12%, while 8% decided not to report the birth year. regarding the questionnaire on the usage of common icts, most of the participants answered they use computers, smartphones, web browsers, and instant messaging applications on a daily basis, while the answers for videoconferencing tools and video games are heterogeneous. in the case of videoconferencing, 20% selected daily usage, while more than 40% selected one of the options below once a month. the percentage of daily usage answers provided by registered users is reduced to 11% in the case of video games, and about 60% of them answered they play video games once a month or less."
"for the security of our iot system, we mainly consider our simd she fv, hcs, hds, homomorphic surf, and clustering schemes according to our threat model."
"the rest of this article is organized as follows. section ii presents our system model and threat model. then, we provide security and privacy requirements of our system in section iii. in section iv, we describe the preliminaries and related work before providing system architecture in section v. then, in section vi, the detailed system scheme is offered. next, we give the detailed security and privacy analysis of our system in section vii, and describe the system performance evaluation in section viii. finally, we provide the conclusion of this article in section ix."
"medical image acquisition and encryption (miae): in our iot system, h utilizes s i and lens to acquire patients' retinal images of dr. we show this as process 2 in our iot system architecture."
"our iot system refers to computing about fixed point real numbers. in general, homomorphic encryption based on rlwe only supports integer and polynomial computing. in order to implement our iot system, we need to encode fixed point real numbers into polynomial under certain accuracy and reliability."
"the network infrastructure provides functionality to establish the conditions for communicating visitors and robots, support the connectivity and communications across the network, and provide security mechanisms and data persistence."
"in the us, over 23,000 deaths each year are associated with drug-resistant bacterial infections [cit], which are expect to reach 10 million annually [cit] . the rise of antibiotic-resistant bacteria has prompted increasing interest in antimicrobial peptides as a solution to this critical issue [cit] . over 2800 antimicrobial peptides have been discovered from natural sources in the last decade [cit] . antibacterial peptides derived from these natural sequences have shown both broad-spectrum and improved activity against targeted bacteria [cit] . antibacterial peptidemimics are introduced as another source to the existing peptide libraries by incorporating additional backbone chain atoms for more structural flexibility and resistance to protease degradation [cit] . this list extends by exploring the post-translationally modified antimicrobial peptides offering chemical properties beyond the naturally occurring amino acids [cit] ."
"in this section, we provide the security and privacy requirements of our iot system for the dc and pp. the security and privacy requirements include the following aspects."
"evaluating the performance of the rules being generated is performed by calculating the pr, the training set accuracy performance of the rule. the pr is the ratio of the size of the sets of peptides described by the intersection of all the conditions in the rule that meet the targeted label to all the peptides described by the intersection of the conditions (eq. 1). the cln value is the user-defined condition-limit number, which limits the number of conditions in each of the rules. the value of pr must be at or above α, the user-defined minimum training accuracy a rule must have to be included in the rule set. [cit], the features derived from the 544 amino acid properties in the aaindex1 [cit], and the classification label of antibacterial activity from the positive or negative training data set. a n denotes a sequence, b n indicates the sum of the sequence for an aaindex1 property, c n indicates the mean and d n indicates the maximum sum of three adjacent residues in the sequence"
"we used previously studied, publicly available datasets of antimicrobial peptides [cit] to test our method of finding physicochemical boundaries for antibacterial activity. see table 2 for the inducted rule category with the largest membership of the studied dataset. the rule category is the conjunctive expression of each of the conditions up to the user-defined condition-limit number (cln) with the rule applying to antimicrobial peptides whose property values are within the range of the values given in table 2 (eq. 2). this rule has a high selectivity of 97.8% with a false discovery rate of 2.2%. all sequences that do not match any rule for the applied rule set are classified as non-antibacterial."
"three main components ( figure 3 ) enable the system functionality: the network infrastructure, the telepresence robot, and the visitor device. the structure of each component is divided into four levels: hardware, service, application, and middleware. in a nutshell, hardware levels contain physical devices and firmware features while service levels comprise the interfaces with such hardware layer and the processes and utilities required to support the execution of the application. in its turn, application levels include the logic and process management features. finally, middleware layers are responsible for enabling the communication between the different environments."
"the setup of the test environment was/is also a valuable experience that allowed us to put in practice the flexibility provided by the modular design of the core application. it enabled the ease integration of extensions to model a particular context or use case, specifically, the set of application specifics required to automate the web experiment. the core application was endowed with user registration, mechanisms to design self-guided teleoperation tasks, and automated survey collection with an effort equivalent to % 0: 5 person/month. the major part of this flexibility is enabled by the separation of concerns provided by the modular architecture presented in section \"a robotic telepresence architecture,\" the cross-platform compatibility of the core technologies used in the implementation, and the rich, well documented ecosystem of ready-to-use extensions that are available for the open-source technologies selected."
"nonetheless, the traditional encryption schemes cannot support direct computational operation for the encrypted data [cit], 2327-4662 [cit] ieee. personal use is permitted, but republication/redistribution requires ieee permission."
"application level. it includes the application that exploits the services provided by the upper level and integrates the logic and interfaces required to coordinate robot and visitor operations. in our model, two main blocks are envisaged: the session manager, which deals with concurrency, authentication, authorization, and persistence; and the data manager, in charge of enabling the transmission of audio/video (a/v) streams and robot commands. services for connection recovery, error logging, and data management are also considered in this level."
"for the surf -based dr detection schemes [cit], the training of retinal lesion detectors is the critical factor. all the existing schemes are to take full advantage of the following schemes: surf, clustering (k-means) [or hard-assignment coding/sum pooling (hard-sum), and softassignment coding/max pooling (soft-max)], bag-of-visualwords (bovw), and two-class classifier [support vector machine (svm)] schemes. the key ideas of the existing schemes are to use the surf scheme to detect the low-level points of interest (pois) with a region of interest (roi) containing specific lesions indicated by experts for dr. then, utilizing clustering (k-means) (or hard-sum and soft-max) to create bovw (or max-pooling) for training two-class classifier (svm) of the lesion detectors. finally, a trained twoclass classifier is utilized for the final classification about the individual lesion detectors."
"the second platform is the commercial telepresence robot giraff, which is built upon a manufacturerdeveloped base, endowed with a videoconference set and a tilting head. the hardware of the standard giraff has been enhanced with a laser scanner. it is controlled through an moos-based robotic architecture instead of ros. this decision is because the drivers provided by manufacturers are only available for windows os. the moos architecture includes a controller for the robotic base, mqtt compatible communications, and self-localization capabilities. table 1 summarizes similarities and differences in the implementations for each robot."
"next, we describe the setup done to conduct the trials as well as the collected data. then, the section concludes with a discussion on the objectives proposed for the experience, limitations of the current study, and lines of future work."
"a full-stack web application was designed our telepresence system for public use during 1 month. visitors were enabled to access the application by clicking on a public link at our group's home page, then, they were requested to register in the user database and/or log in to gain access to the robot. once logged in, the application displayed some illustrations giving instructions on robot controls and the task to be carried out. after that, the visitor could take the control of the robot and freely steer it to accomplish the specified task. the robot environment is composed of two interconnected rooms and a corridor. two doors connect the rooms and the corridor; the visitor had to steer the robot to pass through both of them. additionally, one of the doors was sometimes intentionally closed, so the visitor had to manage to find an alternative path to complete the task. to help the visitors, a floor map of the robot workspace and real-time robot localization is displayed during the session (see figure 6 )."
"service level. this level represents the os that provides the programs running in the application level with basic interaction with the hardware. in addition, this level also includes possible software components like specific hardware drivers. application level. the application layer includes the so-called telepresence robot application that integrates the components required to interpret sensory inputs, control the robot actuators, and expose multimedia and robotic capabilities to external interfaces. the main blocks of a telepresence robot application are: a/v media, rca, and ui. the a/v component processes local and remote media streams, which involves coding/decoding, failure handling, and interface mechanisms. the rca covers the software architecture that enables the particular robotic capabilities of the system. finally, the ui component consists of the mechanisms and apis required to detect user events and transform the inputs into convenient commands."
"our future work will iterate on the development cycle to overcome the limitations reported in section \"limitations and open issues\" putting the focus on robotic telepresence applications for aal. we aim at enabling our application to operate in field environments within this context and, in this regard, the path provided by state-of-the-art research projects in the field will guide the design of extensions to this work."
"dc: in our iot system, patients' retinal images of dr are transmitted and stored at different devices (cs, s i, h's servers, and doctors' computers). the encrypted retinal images of dr for transmission and storage cannot be broken by the quantum computation model (or a quantum computer), because our"
"on the other hand, telepresence robots playing the role of teleoperated robots are considered robotic embodiments of the remote operator in the local workspace. the focus is placed on the positive effects, the physical embodiment of the operator can bring into remote assistance services (e.g. health-care and monitoring services), and the goal is to make them, the operator and the user nearby the robot, feel in some way they are sharing the same physical space. in general, the means to achieve that in current robotic telepresence systems rely on the combination of videoconference with a wide field of view and the initiative yielded to the operators through the movements they can command to the robot using teleoperation features in real time. thus, the specific challenges in these approaches are to generate an adequate level of situation awareness in both parts and enabling the remote operator to actively visit the robot environment while ensuring its safety. the correct integration of videoconference and robot teleoperation features are at a premium in these approaches, as well as the usability of the robot controllers, which also includes the study of visualizations and autonomous navigation abilities to assist the operator complex maneuvers. example of projects following this approach is excite 32 (http://www.aaleurope.eu/projects/excite; active and assisted living program) and giraffplus 16 (http://www.giraffplus.eu/; fp7 program)."
"our method also acts as an embedded feature selection tool by limiting the physicochemical properties in the rules to a user-defined number [cit] . this embedded feature selection property may make cln-mlem2 useful for feature selection for other methods in the field, with the capability of setting the limit of the number of features to select. our proposed method, cln-mlem2 has a low false discovery rate compared to comparative antimicrobial peptide methods as shown in fig. 3 . efc method also has a low false discovery rate when including the physicochemical properties, but a doubled false discovery rate when the pattern recognition component is used alone."
"the combination of open-source technologies presented in our work is a third approach that offers a balanced solution which, at the cost of an initial setup to integrate webrtc services, provides a suitable technical integration that is adaptable, cost free, and ready to be deployed in relevant contexts of robotic telepresence."
"in this first exploratory study, participant profiles were used only to check their variability as well as to validate and refine the registration features of the platform for further studies."
"threats to confidentiality of the retinal images of dr mainly comes from the security of our simd homomorphic encryption scheme, encrypted data transmission, and storage and processing of the encrypted retinal images of dr in our iot system. malicious attackers can use the quantum computation model (or a quantum computer) to analyze and attack the encryption scheme. in the process of encrypted data transmission, a malicious attacker can eavesdrop and obtain the encrypted data. a portable device (a sensor in our iot system), doctors' computers, and a cs store the encrypted data and homomorphic evaluation results over the encrypted data. a cloud service provider, a portable device provider, a computer provider, and a malicious attacker can obtain the encrypted data. 2) cs is honest-but-curious: cs can also keep a detailed record of each operation and the result of each operation performed by our schemes in our iot system. cs may want to recover the plaintext of the encrypted data, and reveal the content of images of dr and the final diagnostic results. especially, in the honest-but-curious model [cit], cs can correctly perform all the schemes in our iot system. in the meanwhile, cs takes advantage of all the operating recordings and acquired encrypted data to perform analysis for the benefit of a cloud service provider."
"in this section, we use homomorphic fast multiretina-image matching to simplify the whole process of lesions detection and dr detection simultaneously. with the help of the encrypted rois containing specific lesions indicated by expert, we can implement the parameter modification of homomorphic surf and fast multiretina-image matching schemes based"
"the robot environment claims, in its turn, to be the major source of heterogeneity. the structure and elements of the workspace where the robot operates largely condition the design of the robotic platform, including hardware, that is, sensors and actuators, and software, that is, navigational algorithms, control architectures, and so on. not only the physical environment but also the type of interaction required and the end users coexisting with the robot (if any) affect to the design."
"using a rough set theory approach that combines the algorithm of mlem2 (modified learning from examples module, version 2) [cit] with the algorithm irim (interesting rule induction module) [cit], we developed a method that investigates the sequence-function relationships. the main difference in from other rst methods is that it uses local coverings to generate rules, which are different from the lower and upper approximations in the basic rst methodology. irim is a method that optimizes for rules that have the most training set sequences that apply. this is different from mlem2 in that irim may not provide a rule that applies to every training set sequence. we achieve high specificity performance with our condition-limit number mlem2 with the fewest chemical property features among benchmarked methods. our method was tested against publicly available prediction servers camp amp prediction [cit], iamp-2 l [cit], and a motif-searching algorithm efc method [cit] with and without fcbf. the approach produces physicochemical boundaries that create definitions of similarity among antimicrobial and non-antimicrobial peptides."
"after these general remarks, we proceed with some specific ones. we discuss aspects related to each of the objective set for the experience, highlighting the pros and cons of the features applied in the presented approach. as said at the beginning of the discussion, the number of successful executions in the exploration tasks, achieved under heterogeneous conditions in the devices, internet connections, and visitor skills prove the correct performance of our approach to support videoconferencing and teleoperation features in real time. the exploration task included accurate maneuvers of real-world contexts like going through doorways, approaching a specific location, and docking the robot to the charging station. these are unattainable maneuvers when applications lack of a correct integration of media and control streams; hence, we can confirm the approach presented is correct in this aspect. however, it is important to consider the following pros and cons before adopting this approach."
"overcoming the novelty effects: the use of new technologies always requires to climb a learning curve. exposing the application to potential users to analyze their evolution until they feel comfortable with the technology is an essential step to determine which aspects of performance are part of an acceptable learning process and which ones demand a better design. however, this type of evaluation is one of the most challenging given the vast efforts needed to find a convenient environment to maintain the setup for a long period and a selection of participants committed to collaborate on a regular basis."
"these issues are normally coped by implementing solutions strictly tight to the particular requirements at hand, that is, a specific robotic platform, software architecture, and so on. this option hinders the portability of the solution and normally prevents its extension to a different robotic telepresence system. the use of current open-source tools, like webrtc and html5, provides the needed flexibility to cope with such heterogeneity but at the expense of an extra effort for specifying and designing a complete functional architecture."
"we propose a third alternative that solves the trade-off between these two approaches. the alternative relies on the integration of the open-source webrtc technology, html5 apps, standard iot protocols, and state-of-the-art rcas. this combination provides efficient multimedia communications in real time and permits a compact integration of multimedia and robotic control features. the drawback with respect to other options is the additional effort in the specification and design of the system architecture, which is one of the contributions of this article."
"the considered exploratory task consists of start at checkpoint 0 (charging station), pass through checkpoints 1-4, and go back to checkpoint 0 to dock the robot. in order to guide the visitors, the interface displays messages and visual cues to indicate the next step. in addition, a secondary task is considered for the case of the giraff robot, consisting in finding a mirror near checkpoint 4, facing the robot to it and taking a snapshot of the robot reflection by means of an emerging that the interface renders automatically at this point of the task. the exploration task ends when the participant accomplishes the docking maneuver at the charging station or after a predefined time out."
"common particularities can be identified according to the considered application field. thus, in the context of assisted living, the robot environment usually includes narrow, cluttered, and dynamic spaces where the robot has to behave in the presence of sensitive users with limited physical or cognitive skills, who might be technologically illiterate. thus, the design must focus on the simplicity and dependability of the robotic platform which should be featured with smooth and gentle movements and a noninvasive appearance. in telework applications, workspaces tend to be larger, less cluttered, and populated with technological-skilled users. in contrast, these applications demand higher levels of interaction since the final purpose is to allow remote coworkers to collaborate efficiently. to achieve that, telepresence robots must be enhanced with additional ict features like screen sharing, document presentation utilities, and laser pointers."
"the network infrastructure relies on a client/server architecture to support the communications between visitors and robots. according to the model of the network environment presented in section \"a robotic telepresence architecture,\" it has been instantiated as follows."
"in this section, we first present our encoding and simd she schemes. then, our simd homomorphic comparison and division schemes are constructed through our encoding and simd she schemes. ultimately, we present our simd homomorphic surf, clustering, and multi-image matching schemes."
"in binary classification there are two different descriptions of performance based on the two possible error types, false positives and false negatives. sensitivity refers to the likelihood of correctly predicting a positive result, while specificity refers to the likelihood of correctly predicting a negative result. sensitivity deals with avoiding false positives, while specificity deals with avoiding false negatives. selectivity, which can be directly derived from specificity, is the likelihood of incorrectly predicting a negative result, a false negative. further details about performance measures are included in additional file 1."
"the visitor device aims at presenting the robot environment data and translating the intentions of the user into robot commands. thus, the essential elements of the visitor device are the mechanisms to capture the visitor inputs, the transformation of such inputs into robot api commands, and the data structure to be exchanged between visitors and robots. notice that, given the close relation between the visitor device and the telepresence robot, both models are equally structured."
"our iot system acquires patients' retinal images of diabetic retinopathy (dr) by the camera sensor connected to a raspberry pi (rp) [cit] . the acquired retinal images are encrypted with our homomorphic encryption in an rp. the encrypted retinal images are transmitted in our iot system, and stored in a cs, which performs efficient auxiliary diabetes diagnosis. we proved that our iot system is the known plaintext attack (kpa) [cit] and the ciphertext only attack (coa) [cit] security in honest-but-curious [cit] . kpa security is an attack in which an attacker has the samples of both the plaintext and corresponding ciphertext. an attacker conducts an analysis with the samples of both the plaintext and corresponding ciphertext to get the secret key used to encrypt and decrypt the information. coa security is an attack in which an attacker has only the knowledge of ciphertext. an attacker conducts an analysis ciphertext to get the secret key used to encrypt and decrypt the information. the honest-but-curious model is a secure computation (or a protocol) where the attackers are restricted to follow all of the computation steps (or a protocol), but after implementing computation steps (or a protocol), they may analyze the data they have received to try to recover inputs. in our iot system, encrypted retinal images of dr are just like in a black box. homomorphic image processing and machine learning algorithms cannot reveal image content and diagnostic results. a good balance of storage overhead, communication cost, computational efficiency, and privacy protection is achieved by our iot system."
"the explosion of available antimicrobial peptides brings the new challenge of selecting which antimicrobial peptides to use [38, [cit] . with the large increase in the number of available peptides, there is an opportunity to classify peptides with respect to their similarity. we define similarity by the physicochemical properties of the peptides, which we show can differentiate between active and inactive peptides. each rule generated is a category of peptides with boundaries of physicochemical properties chosen so that no rule category is a mixture of active and inactive peptides beyond an allowed limit. we generate rules until all peptides in the training set are covered by at least one category."
"we propose a novel method that addresses order sensitivity by calculating the physicochemical properties of sub-sequences in addition to using descriptors of physicochemical properties for length independence. our method therefore combines order-sensitivity and length independence as a new approach. we analyze these descriptors using rough set theory (rst). rough set theory is a heuristic method for discovering rules, which distinguish between outcomes. these rules show which data features and data values are useful to distinguish between outcomes. to the best of our knowledge, rst has not yet been studied to classify peptide or protein sequences based on their activity. our rst implementation uses features that summarize the physicochemical properties of the full-length sequences, which are sequence-order insensitive, and features which summarize constant-length subsequences, which are sequence-order sensitive. rst selects combinations of both kinds of descriptors into a single rule. each rule defines its own cluster including the classification of the peptide's activity or inactivity."
a web-form was presented to the participants inviting them to evaluate the experience. form responses were automatically stored and attached to their user profiles in the database.
"cross-platform compatibility without installing additional plug-ins nor software: based on the core features enabled by current web browsers (e.g. web-sockets, webrtc, and multimedia apis), our web-based visitor interface deals efficiently with hardware, visualization, and communication issues. instant access and updates: since the user interface is delivered online by the central server, visitors can retrieve the most recent version by just accessing the server url. simple and responsive design suitable for multiple sizes and resolutions in the visitor device: the interface provides different views, automatically adapted to the client device, and integrates controllers for keyboard, mouse, and touch-screen events. figure 5 displays a view of the visitor interface. presentation elements comprise a minimalist layout composed of two viewports. the main one renders the video stream received from the robot environment and overlays some elements, for example, graphic helpers, application alerts, and/or status messages. the secondary viewport displays a schematic map of the robot environment with the aim of improving the visitor situation awareness. the map includes a floor plan of the robot environment enriched with some structural elements, along with the robot pose. additionally, the map can be hidden, maximized, centered over the robot position, or scrolled to display a specific region."
"in this article, we have analyzed the sources of heterogeneity in robotic telepresence designs and proposed a modular architecture to generalize such heterogeneity. we have examined the most common application domains and reviewed a collection of state-of-the-art systems in the field. according to the information reviewed, robotic telepresence contexts have been presented as a combination of three distributed environments: robot, visitor, and network environments. our work deals with generic aspects of robotic telepresence designs at each environment but, in particular, we placed our focus on the limited opensource solutions enabling the integration of videoconference and robotic control features in real time. webrtc, a recent open standard for web real-time communications, provides effective mechanisms to perform such integration, and we have proposed a suitable approach based on this technology. the approach combines webrtc communications with an html5 web interface, a set of iot protocols, and two state-of-the-art rcas (i.e. ros and moos). based on this approach, we have developed a functional open-source web application for robotic telepresence and we have put it into practice through a pilot experience. the application was intensively tested by inexperienced participants over the internet and it performed successfully across multiple internet connections, devices, webbrowsers, and two state-of-the-art rcas. we also reported the limitations of the current work (e.g. the lack of a formal user study) and the set of open issues that will be addressed in the next stage of development. throughout the general discussion, we explained the pros and cons of the presented approach with respect to the alternatives available in the field. lastly, given that our future work will focus on applications of robotic telepresence in aal contexts, we describe our plan for next iterations in the development cycle."
"in this work, we have addressed the problem of developing an open-source solution able to integrate conveniently efficient videoconferencing and robotic control features and present a particular instantiation into a web-based application for robotic telepresence playing a role of an avatar. however, note that the inclusion of control robotic components into the modular architecture makes it suitable for covering also other options, for example, intelligent robots enabling robot-mediated communications."
"however, despite the overall positive feedback, a more questioning visitor perspective is found when the results are organized by topics or examined individually. more specifically, the group of statements evaluating the complexity of the task is the worst rated and the only one that accounts for some negative rates (8.7%). the central tendency in this group of answers is closed to 0.4, lower than the ones obtained for the rest of the topics (that means a 10% lower within the [à2, þ2] scale). the balance between nonpositive and positive answers (28/78) is also worse than the value presented by other topics and the spread of the responses within this group is higher. the statement q7 (taking the photo was easy), contained within this group, is clearly the worst scored with a central tendency of 0.82 (q7 and q6 are the only two statements with an average score lower than \"agree\") and a balance between nonpositive/positive answers of 38/62. to improve data readability and highlight these aspects, figure 8 shows a graphical representation of the results. responses are organized in charts of stacked bars where the different colors of the bars represent the rate of responses given by visitors per scale value."
"finally, three different controllers can be chosen for steering the robot: (i) a keyboard controller, (ii) a mouse controller, and (iii) a touch-screen controller. the keyboard controller commands linear and angular robot speeds, while the mouse and touch-screen controllers generate a trajectory to the selected destination on the main viewport, enabling smoother movements than the keyboard controller."
"the implementation of the visitor client also instantiates the four-level structure described in section \"a robotic telepresence architecture.\" the hardware level, in this case, faces the heterogeneous nature of the visitor devices, that is, laptop, desktop, tablet, including different peripherals to capture the user events, that is, keyboard, mouse, and touch screens, and media displays with varied size and different resolutions. the service level integrates the os and a webrtc compatible web browser; no additional plug-ins nor drivers are required. an html5 web application is the unique component operating at the application level and, lastly, the middleware level, which is encapsulated by the web application, connects the app to the public apis offered by the central server and the robots."
"to characterize the effects of the communication channel on the application performance: a deeper study on the effects of delays, packet loss, media quality, and video/control synchronization on the telepresence experience is an important step toward the achievement of a successful application. to conduct this study, it is necessary to develop specific application features to monitor data streams, visitor actions, and robot states conveniently. the design of a test environment to obtain unbiased results is also a challenging aspect in this study. setting up environments to, for instance, compare the performance of different protocols in real networks or to assess how factors like delay, jitter, packet loss, and network latency affect the telepresence experience are two interesting extensions to this work."
"based on our threat model, confidentiality of the retinal images of dr mainly relies on the security of our simd she fv scheme, transmission, storage, and processing of the encrypted retinal images of dr in our iot system. our simd she fv scheme is a quantum-resistant encryption scheme based on lattice. the data is encrypted during the transmission process. the processing over the encrypted retinal images of dr in our iot system is homomorphic evaluation. therefore, confidentiality of the retinal images of dr in our iot system is guaranteed even in the era of quantum computation."
"a first extension is the enhancement of the rca with autonomous behaviors to increase its navigation performance, assist visitor in driving tasks, and reduce maintainability. the integration of collision avoidance, collaborative control, and autonomous docking features are the improvements planned for the next stage. these features are especially relevant to aal contexts where the telepresence robot operates within the personal space of sensitive users and, thus, safety is at a premium. in the same direction, given that robotic telepresence is applied as an assistive technology with the aim to improve certain aspects of personal routines and daily life activities of people with special needs, the maintenance efforts should be minimal to be worth its use. projects like spencer 31 and giraffplus 16 deal with these aspects of robotic telepresence, provide features that have been tested in real-world deployments, and our solution is compatible with them. thus, a next step is to integrate such features in our rca. the integration of an intelligent avatar remains beyond our goals for the next development cycle."
"on the one hand, approaches considering telepresence robots as mobile avatars emphasize on the robotic and cognitive abilities required by the robot to interact as an autonomous assistant, partner, or coworker. their distinctive features are a set of decision-making abilities that enable the robot to act on its own initiative in specific tasks. they also account for the basic features of robotic telepresence (i.e. communicating with other ict systems and remote users) but, despite most of them can communicate people nearby and remote users via videoconference, in general, these approaches do not consider real-time teleoperation features. the remote operator is a passive interlocutor and the robot acts as a third agent supporting the remote human-human interaction. relevant projects following this approach are spencer 31 (http://www.spen cer.eu; fp7 program), where the robot is responsible for passenger guidance and help in busy airports, as well as in social robot (http://mrl.isr.uc.pt/projects/socialrobot/) and grow me up (http://www.growmeup.eu; from fp7 [cit] european research programs, respectively), where the social robot platform 29 provides care services for the independent and active living of older persons within a smart environment."
"1. the robot environment, where an mr platform endowed with a videoconferencing set, enables the virtual visit to the robot workplace; 2. the visitor environment, where a networked device provides the visitor with an interface to interact with the robot; and 3. a network environment that supports the communication between visitors and robots."
"the objective was to explore the suitability of the application to operate in a test environment as much realistic as possible. we made it by testing the application with novice users using personal devices from their home locations to control the robot in a realistic office environment, which stressed the technical integration achieved and helped us to rapidly assess the overall correctness of the system. the results of the experience were satisfactory with fairly high number of sessions (176 with expert þ127 with novice users) where (i) the control features were executed in multiple platforms including desktop and mobile devices, (ii) the visitor client application was served to a great number of ips, (iii) the interface performed reasonably well with expert and novice users, and (iv) a high rate of success was achieved in the execution of accurate maneuvers like door crossing, approaching an object (mirror), and docking the robot to the charging station. as it was expected, the uncontrolled variables in visitor and network environments and the challenging maneuvers to be performed during the teleoperation task enabled us to detect some implementation issues and design failures that remained unnoticed during lab tests. as outputs of the experience, novice visitors evaluated the performance of the application developed with quite positive rates, highlighted some weaknesses of the system and the analysis of their responses revealed a relevant set of open issues to be addressed in future work. despite the preliminary nature of the user study lacks of validity to make formal considerations, the study establishes a valuable initial reference to conveniently design the extensions of this work for the next stage of development."
"the increase in multidrug resistant bacteria usage has prompted an intense search for agents that can be used to treat infectious diseases. there is growing interest in antimicrobial peptides as novel agents to treat infections, and this interest has led to an exponential growth of known antimicrobial peptides. however, peptide selection is becoming another challenge with the drastic increase in the number of these peptides discovered from natural resources, their modified version as well as computational derived ones. we developed a method, cln-mlem2, for generating rule sets to describe the similarity among antimicrobial peptides by physicochemical boundaries. our cln-mlem2 method allows the user to limit the number of physicochemical properties used to set the boundaries. discovering where the boundaries of physicochemical properties are among active peptides generates new categories of antimicrobial peptides."
"for two positive integers a and b, we can denote them as polynomial [a(x) and b(x)] by the above-mentioned encoding scheme. e is our simd she fv scheme. the corresponding ciphertexts are e(a(x)) and e(b(x)), respectively."
"communication channel specifications also vary for different applications. thus, in cases where the information exchanged is personal or confidential, the communication must incorporate security mechanisms. in applications with intensive exchange of data and/or bandwidth limitations, the transmission efficiency is an issue to be considered. lastly, the topology of the network infrastructure required also affects the design of the communication channel, ranging from connections between a unique robot and one possible visitor to large networks enabling many-to-many communications between multiple robots and visitors. in most cases, a centralized server provides cloud services to coordinate the connections across the network topology."
"because our simd she scheme is malleable [cit], tamper attacks about encrypted data transmission and storage are not considered for security in our iot system. pp: retinal images of dr contain the patient's medical information. in our iot system, transmission and storage of the encrypted medical images cannot reveal the contents of the retinal images of dr. during homomorphic image processing and image classification with machine learning, the cs and h's servers cannot reveal any information of the retinal images of dr either. during the auxiliary diagnosis with an s i, a cs, and h's servers, the retinal images of dr stored in an s i, a cs, and h's servers are encrypted by our homomorphic encryption. any information of the retinal images of dr cannot be revealed by an s i, a cs, and h's servers. for the doctor's final diagnosis, h's servers and the doctor's computers preserve the security of diagnostic results by authentic and secure channel [cit] . pp is protected during the hospital's internal data interaction. hence, pp is protected in the whole diagnostic processing."
"the architecture presented in section \"a robotic telepresence architecture\" is generic enough to cover a wide range of robotic telepresence requirements. in this section, we present an instantiation that fits into a given application which deals with a number of restrictions within the three environments to treat with the most typical heterogeneity issues. the presented robotic telepresence application belongs to the monitoring/surveillance context in which visitors steer a mobile robot to accomplish a particular task without the need of interacting with the people around. concretely, the characteristics of each environment in this application are as follows:"
"do you have any additional comments or suggestions? evaluation rates decrease as statements relate to driving aspects (q2 and q3) and challenging maneuvers (q5, q6, and q7). the results are also coherent in the variability of the responses, which comes from the heterogeneity of our study in multiple aspects like visitor skills, network stability, users' device, and so on."
"while many antimicrobial peptides have been discovered at the laboratory bench, computational methods have been integrated into this search to find many more candidates. encrypted antimicrobial peptides are an example in which known active peptides are queried against dna repositories to find new antimicrobial peptides [cit] . among many methods, grammar-based methods and regularexpression-based match sequence patterns are used to identify functional similarity [cit] . computer-aided molecular design [cit] approaches using quantitative sequence activity relationships [cit] (qsar) predict the antibacterial level of peptides given key chemical properties. artificial neural networks (ann) have been used both to generate new sequences and to distinguish between active and inactive sequences [25, [cit] . they are often used in the classification of antimicrobial peptide sequences [cit] . while anns are flexible enough to model many kinds of complex relationships, they lack transparency about how classification choices are made. determining the boundaries of the similar antimicrobial peptide clusters remains difficult despite many existing machine learning methods."
"services. the central server integrates both cloud and local services. concretely, it relies on two cloud services, namely, the stun and turn services provided by google's open servers."
"exploration tasks. a set of internal states of the whole web application was stored while participants performed the second step, the exploration task. the aims of the data collected at this stage. the collection of states was automatically registered and stored automatically by the server application, and each session included the following data:"
"patient (p i ) and sensor (s i ): p i is a potential diabetic patient. a camera (raspberry pi noir camera v2), a handheld condensing lens and rp constitute a sensor (s i ), which acquires patients' retinal images of dr [cit] and encrypts them."
"hardware level. the hardware level entails the hardware devices, electronics, and computer firmware working without the need of an os. cameras, microphones, mr bases, screens, and additional sensors/actuators are included in this level."
"robotic telepresence arises as a promising approach for diverse applications including assisting elder people, telework, and remote surveillance. a robotic telepresence system results from the combination of information and communication technologies (icts) and mobile robotic (mr) solutions. on the one hand, icts permit users to remotely control mobile robots within realistic environments, overcoming the still limited performance of autonomous robots in these scenarios. 1, 2 on the other hand, mobile robots help in enhancing the capabilities of traditional icts, enabling users to interact within the robot environment as if they were physically present. 3, 4 a successful robotic telepresence system must face a number of issues that can be grouped into three main categories: (i) providing accessible and easy-to-use interfaces to facilitate the telepresence experience, (ii) integrating robust multimedia features and real-time communications under different network conditions, and (iii) considering efficient and adaptable solutions to provide the requested robotic abilities."
"can be computed as (( (e(b(x) )) 2 ) 2 · · · ) 2 mod (x). finally, we can compute [e(b(x))] n mod (x) with log 2 n + h(n) − 1 multiplications, where h(n) is the number of one-bit for the binary representation of n."
"a decrease in selectivity of the classification will cause longer computer search times, while a decrease in specificity will increase the number of necessary experimental activity assays. since the cost of experimentally testing peptides is much greater than the computational time of searching for antimicrobial peptides, methods that have high specificity are preferred. in addition to the high specificity of our method, our method creates categories of antimicrobial peptides. categorization of peptides aids in the selection and in the design of antimicrobial peptides by providing similarity groupings according to physicochemical property boundaries. peptides that match multiple active categories can combine more physicochemical property values associated with activity."
the article concludes with a discussion about the benefits and limitations of the current study and with the planned future work. demonstration videos of the trials conducted during the pilot experience are available at: https://mapir.isa.uma.es/work/telepresence.
"1) high efficient and complex homomorphic evaluation: efficiency of encryption scheme is very important for tiny appliance of iot (e.g., fast encryption and decryption, low storage and communication cost, efficient homomorphic evaluation, etc.). image processing and machine learning algorithms require complex homomorphic evaluation, such as homomorphic comparison and division over the encrypted data. our designed homomorphic encryption scheme supports small encryption key and encrypted image size, low communication cost, and efficient and complex homomorphic evaluation."
"another problem concerns the heuristic of using a threshold iteration count in order to distinguish spinning read loops from ordinary loops. if the spinning read loop dose not spin long enough to reach the threshold value, the detector misses the spinning read loop and generates false positives. on the other hand, if the threshold value is too low, ordinary loops in the program could be mistaken for spinning read loops, which also results in missed races. thus, without exploiting the semantic information by dynamic code analysis just before runtime, one may easily miss synchronizations or misinterpret them, since actual spinning reads may not happen at all at runtime or might not reach a preset threshold value."
"if we switch off the support of pthreads library indicated by option nolib, synchronization primitives are no longer intercepted directly and therefore unknown to helgrind + . in this case the detector acts as a pure happens-before detector. we symbolize this situation with nolib+spin(7) option in the table above. only one additional test case fails (one false positive). however, we observe that the best results are achieved when using the new feature as a complementary method to our race detection algorithm (shown as lib+spin (7)) ."
"all our experiments and measurements in this section were conducted on a machine with 2x intel xeon e5320 quadcore at 1.86ghz, 8 gb ram, running linux ubuntu 8.10.2 x64. all programs were compiled with gcc 4.2.3. no source code annotation was used. we employed helgrind"
"instances of a fragment f are obtained by applying γ to each document in c. the collection of the resulting documents form the fragment f, which is valid if all documents generated by γ are well-formed (i.e., they must have a single root)."
"being a runtime race detector, helgrind + monitors all read/write accesses. the state of a variable indicates whether it is used by only one thread (state exclusive ) or several (state shared). when entering a spinning loop, the states of the variables affecting the loop condition are set to a special state called spin. two cases are possible. in the first case, the counterpart write does not happen before entering the spin loop. in this case, helgrind + waits for the first write operation that affects the loop condition. if the write operation is performed by another thread rather than the spinning thread, then this is the counterpart write. when leaving the loop, helgrind + records a happens-before edge between the spinning read loop and its counterpart write."
"furthermore, all unknown synchronization primitives that are not supported by our detector will be identified, i.e. synchronization primitives provided by any other library rather than pthreads. in other words, our general approach based on spinning reads detection results in a universal race detector that is aware of all synchronization operations (any library) in the program by identifying them as low level synchronizations. thus, we overcome the serious limitation of prior works which makes detectors limited to only synchronization primitives of a particular library."
"fragmentation design. when fragmentation design is used, the most frequent queries must be known to derive a good fragmentation schema -one that benefits the most frequent queries. experiments in literature show that queries that do not benefit from the fragmentation design suffer large impacts on performance [cit] . only a few work in literature present algorithms for xml fragmentation design [cit] . however, these algorithms were not used in parallel query processing and they do not present correctness rules for the resulting fragmentation. this makes it difficult to check the correctness of the fragmentation schema designed by the algorithms."
"mapreduce [cit] and hadoop 1 have been extensively used to execute operations in parallel based on ad-hoc sub-sets of data. originally defined for searching web log datasets, mapreduce is being progressively used for other types of datasets. horizontal data fragmentation, allocation and indexing techniques [cit] have been proposed to improve hadoop's performance and experiments have been performed over a variety of data, including tpc-h olap queries [cit] . other types of fragmentation have also been used to process large xml datasets using hadoop [cit] . in fact, mapreduce has been compared [cit] and combined [cit] with parallel databases approaches, always aiming at achieving better response times for query processing. such strategies can be used to process large xml datasets if xml documents are correctly fragmented and reconstructed. the term xml fragment appeared in the beginning of the last decade denoting a well-formed piece of an xml document [cit] (in fact, the first draft related to the w3c [cit] ). the main goal at that time was to ease the communication between applications, so they could exchange pieces of documents instead of entire ones. after that, the term xml fragment was used (with several different definitions) in the context of distributed databases [cit], and distributed query processing issues began to raise researchers' interest. in essence, these definitions can be summarized as follows: an xml fragment is a (not necessarily well-formed) piece of an xml document or subset of an xml collection."
"the remaining of this paper is organized as follows. section 2 provides intuitive definitions for xml fragmentation, and section 3 formalizes them. section 4 discusses fragmentation techniques, while section 5 discusses their main features, comparing them regarding several aspects. finally, we discuss open problems in section 6."
"the above method is a general approach that is able to detect synchronization operations executed in the program, which are implemented ultimately by spinning read loops e.g. locks, barriers, etc."
"the structure of the paper is as follows. section ii describes the basics. we deal with the problems of using synchronization operations by presenting various examples and distinguishing true races from false races in section ii-a. our general method for identifying ad-hoc synchronization and unknown synchronization primitives is presented in section iii. the algorithm to detect the common construct of ad-hoc synchronization is discussed in section iii-b. in section iv, we evaluate our method with respect to accuracy and performance. we describe our experiences with helgrind + . related work is discussed in section v."
"in the previous section, we found that the spinning read loop and its counterpart write are the common construct for ad-hoc synchronizations. helgrind + identifies spinning read loops just before runtime with binary instrumentation. later on, runtime analysis establishes the correct happens-before relations between spinning read loops and counterpart writes so that the detector is aware of synchronizations."
"lee, kim and kang [cit] analyze the classical holes and fillers approaches for stream query processing [cit] and claim they are inefficient in terms of memory consumption. to overcome this limitation, they propose to use a labeling scheme to connect vertical fragments of an xml document. in a xml tree, a labeling schema (such as global order or dewey encoding [cit] ) can be used to connect parent and child. in the same way, lee, kim and kang use a labeling schema to connect vertical fragments, where each fragment is a subtree of the original tree. despite the ad-hoc nature of this approach, the authors mention the importance of the reconstruction property for correctness. although there are not an explicit notion of holes and fillers in this approach, each fragment carries an id that is used to connect it to its parent, which is similar to the other holes and fillers approaches."
"according to ozsu and valduriez, ad-hoc fragmentation works well when data is already distributed. however, since there is no clear fragmentation predicate, there is less opportunity for distributed query optimization. an alternative that addresses this issue is structured fragmentation, which is based on the concept of fragmenting an xml data collection according to some properties of the schema [cit] ."
"finally, most of the approaches deal with stored data [cit] instead of streams, and use native xml databases to store the data [cit] . approaches based on mapreduce use hdfs to store the data [cit] ."
"second part of the table i depicts the results when using different number of basic blocks for detecting spinning read loops. by increasing the number of basic blocks, the number of false positives are decreased considerably. we got the best result with seven basic blocks and increasing it further will not improve the results. this is because the test suit uses function templates and complex function calls. thus, spinning read loops in most test cases contain more than three basic blocks."
"notice that, by definition, sd repositories may not be horizontally fragmented, since horizontal fragmentation is defined over trees (instead of nodes). however, the elements in an sd repository may be distributed over fragments using a hybrid fragmentation, as described later."
"programmers tend to implement their own synchronization primitives when available synchronization constructs are too slow. for instance, a programmer may write a spinning loop instead of using a library-supplied wait-operation, if the loop is entered only rarely. furthermore, libraries may lack certain higher-level synchronization constructs such as barriers or task queues, forcing programmers to implement their own. we call synchronization constructs implemented in application programs user-defined or ad-hoc."
"in the second case the counterpart write happens first: one or more variables affecting the loop condition are written before the loop is entered. helgrind + sets the states of the changed variables to exclusive and records the location of the write instructions. as soon as the instrumented spinning read loop is entered, the detector notices that variables affecting the loop condition have been changed. the loop itself terminates immediately. helgrind + records the happens-before relation as before and sets the states of the changed variables to spin. in this case no actual spinning happens-the loop condition is evaluated only once."
"data fragmentation is characterized by physical changes to the dataset, that is, the dataset is fragmented and allocated to multiple computational nodes [cit] . in their book [cit], ozsu and valduriez present a detailed introduction to the subject of xml distributed query processing and xml fragmentation. they classify fragmentation in two groups: ad-hoc and structured. ad-hoc fragmentation does not take the document schema into consideration. edges are arbitrarily removed from the document. structured fragmentation, on the other hand, is one that uses schema property(ies) to define the fragments [cit] . in this work we are focused on large-scale distributed and parallel query processing. in structured fragmentation, each fragment is usually defined by using over the dataset. in this case, the specification of queries and fragments share the same operations, which makes it easier to automatically decompose queries to run in parallel over the fragments."
"efficient document processing is a must when large volumes of xml data are involved [cit] . to achieve good performance in query processing, lots of initiatives focus on indexing [cit] and query optimization [cit] . in addition to these initiatives, distributed query processing can further improve the performance when large collections of documents are involved. in critical analytical scenarios, complex queries, such as olap (on-line analytical processing), cannot effectively benefit from indexing techniques since queries involve several attributes and are ad-hoc."
"hybrid fragmentation. the idea of hybrid fragmentation is to apply a vertical fragmentation followed by a horizontal fragmentation, or vice-versa. an interesting use of this technique is to normalize the schema of xml collections in sd repositories, thereby allowing horizontal fragmentation."
"in our previous works [cit], we proposed a dynamic hybrid approach that employed heuristics to combine lockset algorithm and happens-before. we presented two memory state models optimized for short-running and longrunning applications with an automatic technique that is able to detect synchronization caused by inter-thread event notifications (condition variables). the current work is a complement to our previous works. it identifies ad-hoc synchronizations and unknown synchronization primitives that are hidden to race detectors. this work could be used as complementary method to any other race detector."
"prior work can be divided into two categories with respect to the data race detection approach: static and dynamic. static approaches are not accurate, produce many false positives and false negatives. they consider all potential thread interleavings including those that are not feasible [cit] . dynamic approaches report only races that actually occur during the program execution. they are either based on the lockset algorithm [cit] or happens-before analysis [cit] . the lockset algorithm produces too many false positives but it is simple and can be implemented with low overhead. on the other hand, happens-before analysis is difficult to implement. it may miss races, i.e. produce false negatives, as it is sensitive to the order of execution. recent dynamic approaches [cit] have combined these two methods into a hybrid method with the strengths of each. but no one really succeeded and they still produce many false positives and even miss races."
"the result of our experiments on parsec is demonstrated in table iii . the presented numbers are distinct program code locations that produced at least one potential data race, which are called racy contexts. because of the large memory consumption and computational cost, we did not perform simulations with the native input set. instead, we used the simsmall or simmedium inputs for all simulations and ran each program five times, averaging the results. all numbers for read/write instructions are totals across all threads. the authors of the parsec benchmarks claim the programs to be race free, however we cannot be absolutely sure that they are. table iii provides the false positives under the assumption that the programs are race free."
"even in cases where fragmentation design was performed, it cannot be easily adapted to changes in the query processing environment (for example, the addition of processing nodes). additionally, pruning irrelevant fragments limits parallelism and performance gains, since several computational nodes remain idle when they have fragments that are irrelevant to the query that is being processed. in fact, experimental results [cit] show that the pruning algorithm does not improve query performance."
"in this section, we present the results and evaluate our approach by applying it to a number of benchmarks. we show that by implementing the new method in helgrind +, we are able to report true races and improve the accuracy by eliminating synchronization races and false alarms. we evaluate the overhead caused by the method. the overhead is reasonable."
"given the large amount of approaches that propose fragmentation alternatives in literature, in this paper we survey the existing approaches, comparing their features and establishing a timeline, so the reader can understand the evolution of the proposed solutions, their characteristics, pros and cons. it is important to note that, when we refer to distribution, we are not considering distributed query processing in data integration scenarios [cit], since their main goal is to provide data access, not necessarily in a high performance fashion. we also do not consider approaches that fragments the document solely for internal query processing [cit] . instead, we focus on approaches that aim to improve the performance on processing collections of documents by distributing data to several nodes in a network and use parallel processing strategies such as mapreduce."
"the results in table iii are as expected. compared to basic version with option lib, the number of warnings produced by enabling the new feature spin is reduced considerably in many programs. in case of dedup, facesim, streamcluster, vips and raytrace all false warnings are eliminated. two benchmarks freqmine and vips use unknown libraries: openmp and glib. the number of warnings decreases to two and zero respectively. in x264, dedup the basic version of helgrind + produces more than 1000 warnings (only a maximum of 1000 warnings is reported by the tools), whereas with the new feature (lib+spin) only 19 for x264 remain. ferret generates only two warnings. nine out of 13 applications do not produce any warnings."
"we examine the warnings produced by our race detector with the new feature. all other warnings are benign races that can be counted as false warnings. the reasons for false warnings in some cases are synchronization constructs e.g. a task queue defined by the programmer that do not match the spinning loop pattern. for instance, consider ferret that uses a task queue and contains two benign races: a variable is used as counter for input packets. a single thread modifies it, while other threads read it without any synchronization. another benign race is a variable that is used as signal to show if the input is read completely. in both cases, the condition variables are not used in a while loop."
"but what if synchronization primitives are not supported by the detector? then the usual detectors are not able to intercept them, know nothing about ordering effects, and therefore may produce numerous false positives. the number of false positives can be so high as to overwhelm the programmer."
we implement the presented approach into our race detector helgrind + . helgrind + is built on top of a dynamic binary instrumentation tool called valgrind [cit] . valgrind is a disassemble and resynthesize dynamic just-intime instrumentation framework. the framework translates binary code into a platform independent intermediate representation (ir). helgrind + instruments the ir and hands it back to the framework which resynthesizes machine code from the instrumented ir.
"recognizing the loops in the program is performed by means of control flow analysis. we construct a control flow graph on the fly based on the current super block and consider loops with three to seven basic blocks in the graph. we check whether they are spinning read loops or not. in our experiments, we found three to seven basic blocks deliver good results, since the spinning read loops are typically small loops with few instructions. decreasing this number may result in missing some spinning read loops and producing some false positives. on the other hand, increasing the number of basic blocks causes additional overhead. figure 5 provides a high level description of the algorithm for spinning read loop detection. the first step constructs the data dependency table d l for every loop l. d l (condition l ) returns all variables that the loop condition condition l depends on within the loop l. this analysis takes function calls into account."
"the idea of this paper is to identify a basic pattern that occurs in virtually all synchronization primitives and to extend the detection algorithm to handle this pattern. this pattern is the spinning read loop waiting for a condition to change. once this pattern is handled well, we can in fact remove all code from the race detector dealing with synchronization primitives built upon this loop. moreover, all synchronization libraries as well as ad-hoc synchronization based on the spinning read loop will be handled automatically, eliminating the need to enhance the detector for every library."
"+ to programs provided in datarace-test [cit], a test suite for race detectors. it provides more than 150 short programs (test cases) that implement different scenarios which could occur while running multithreaded programs. the scenarios represent tricky situations, which are difficult to discover by race detectors. currently, 120 of these test cases can be classified into two main categories: \"racy\" cases that involve at least one data race, and \"race-free\" cases. we examine and analyze the effect of each test case on helgrind + . table i shows the result of our experiment on the test suite. all test cases are short programs implemented in c/c++ using pthreads with a varying number of threads and executed without any annotation. the basic version of helgrind + denoted by lib option in table i failed on 40 test cases out of 120. the option lib denotes that helgrind + intercepts pthread synchronization primitives calls. it produces 32 false positives and eight false negatives. by enabling the new option spin(7) for detecting spinning read loops up to seven basic blocks and identifying ad-hoc synchronizations, 24 false positives and one false negative are removed, i.e., 105 test cases out of 120 pass. the removed false positives are all apparent races or synchronization races that arise from using ad-hoc synchronization. the removed false negative was because of spurious wake ups when using same condition variable between several threads. we consider spinning read loops up to maximum seven basis blocks. all synchronization primitives in programs are used from pthreads which helgrind + intercepts directly."
"now that we have discussed the main approaches on physical xml fragmentation, in this section we study the impact they had in literature by establishing a timeline. finally, we analyze the features of each of the proposed approaches, summarizing their differences. our goal is to try to obtain a uniform view on the xml fragments definitions and show the benefits of the different fragmentation design techniques so the designer can choose according to the target query-processing scenario."
"we compare these results with the results produced by drd 3.4.1 [cit] a pure happens-before detector. helgrind + achieves considerably better results. in particular, the number of false negatives with drd is more than doubled than the false negatives with nolib+spin (7) option. having false negatives is the main drawback of happens-before detectors."
"basically this is because they suffer from two serious limitations. firstly, they are not able to detect ad-hoc synchronizations implemented in user code itself. secondly, the detectors are restricted to synchronization primitives of a specific library and any synchronization primitive used from another library in the program is simply ignored. thus, they are not able to produce accurate reports and applying these detectors to real applications overwhelms the user with too many number of false alarms. our work removes these limitations by introducing a general approach for detecting ad-hoc synchronizations and unknown synchronization primitives. we are able to eliminate false positives including benign synchronization races and possible false negatives caused due to missed or incorrect synchronizations."
"to help the intuitive grasp on xml fragments we refer to fragmentation in the relational model [cit], where horizontal fragments are those obtained by table selection operations. this means that horizontal fragments are restricted by the selection predicate and follow the same schema of the original table. vertical fragments, on the other hand, are defined by projection operations, and thus follow a different schema of the original table. the same principle can be applied to xml databases [cit] . figure 1 and figure 2 establish a parallel between data fragmentation in the relational and in the xml models."
"overall, the results on various benchmarks confirm that helgrind + with the new complementary method is able to discover ad-hoc synchronization and synchronization operations of unknown libraries without modifying or upgrading the race detector. the programmer is not overwhelmed with too many false alarms and the results appear acceptable for real world applications."
"it should be mentioned, that our method is based on dynamic binary instrumentation. it does not need any programs source code or user interference such as source code annotations and therefore is non-intrusive in the build process. the whole code and semantic analysis are done automatically during just-in-time binary instrumentation."
"number of tuples/elements each. the goal is to favor parallel processing by using the largest possible number of processing units. still in this case, data skew is a problem, since the query processing time will also be highly influenced by selectivity factors and the node that contains the largest number of tuples/elements that satisfy the query predicate can be overloaded."
"a vertical fragment is obtained by applying the projection operator (π) [cit] to \"split\" a data structure into smaller parts that are frequently accessed in queries. observe that, in xml repositories, the projection operator has a quite sophisticated semantics: it is possible to specify projections that exclude subtrees whose root is located in any level of an xml tree. a projection over a collection c retrieves, in each document of c (notice that c may have a single document, in case it is of type sd), a set of subtrees represented by a path expression, which are possibly pruned in some descendant nodes."
"fragmentation techniques provide a way to \"cut\" the database into pieces (fragments). this way, queries can also be \"cut\" into sub-queries that run in parallel over smaller portions of data, thus achieving better performance when compared to its serial execution over the whole document. successful parallel query processing in relational databases has been directly achieved through table fragmentation. by using the relational algebra to define fragments, algebraic query processing on these fragments can be done with correctness rules [cit] . however, there is no consensus in the database community as to what an xml fragment precisely is. in fact, several approaches in literature present definitions of xml fragments [cit] . they can be classified according to the way they fragment the data. regardless of the fragmentation type, the fragmentation unit is usually a collection of elements, which can be of multiple documents (md) or single document (sd) [cit] ."
"a good race detector should avoid false positives associated with ad-hoc synchronization and synchronization races. in this section, we propose a dynamic detection method that is based on the fact that the spinning read loop is the common pattern of almost all synchronization constructs and a major source of synchronization races. the method identifies spin-loop synchronization correctly even if the spinning read is actually not entered (recall that programmers assume spinning loops are entered rarely). we first discuss the underlying pattern and then present our detection algorithm. the algorithm identifies all spin-loop synchronization operations, including those in libraries."
"the execution time of instrumented code versus the actual execution time is typically slowed down by a factor of 10 to 50 on helgrind + . we measured the execution time of instrumented code on different modes. the measurements are shown in figure 6 (b). there is also some overhead of helgrind + over the basic mode (lib option). in the worst cases, x264 and vips on helgrind + with lib+spin and nolib+spin options increase the execution time significantly. in most cases, helgrind"
"in the next subsection we discuss ad-hoc fragmentation alternatives and then we focus on several approaches for structured fragmentation, following ozsu and valduriez [cit] fragmentation classification."
"additionally, each approach is positioned into a lane, according to its classification as constraintbased, holes and fillers, xpath-based, set-oriented or hybrid. this helps one to have a better understanding of how each type of approach evolved over time. while the timeline evidences that the subject has been continuously investigated throughout the years, table 1 and table 2 summarize the features of each of the approaches for structured (table 1) and ad-hoc (table 2) the first interesting feature to note is that, from all of the approaches, only two support fragmentation of md collections [cit] . these two approaches are equivalent in terms of ideas. the only difference resides in the formalism that is used to present them. ma and schewe [cit] also mention support to md collections, but this needs to be done by first defining a single sd view, and then applying the fragmentation over this sd view. this maneuver could be applied to the other approaches as well, but it creates an artificial layer that needs to be handled by the user (that needs to create the views), and also at query processing time (it may decrease query performance). all of the approaches support fragmentation of sd collections."
"these rules are important to guarantee that queries are correctly translated from the centralized environment to the corresponding fragmented one, and that results are correctly reconstructed."
"in this work, we have shown that the knowledge of all synchronization taking place in the program is crucial for accurate data race detection. we demonstrated that missing ad-hoc synchronization causes a lot of false positives. the presented dynamic method in this paper is able to identify ad-hoc synchronizations. it is also able to detect synchronization primitives of unknown libraries eliminating the need of upgrading the detectors. our empirical results confirm that our method could be used as a complementary method to achieve the optimum results removing false alarms with almost no false negatives. furthermore, the evaluation shows that the overhead caused by the new method in our race detector is moderate enough to apply in practical applications."
"using the method alone as a complete race detection approach results in a universal happens-before race detector that is able to detect all different synchronization operations with minor increase in false positives. a direction for our future work is improving the accuracy of the universal race detector by identifying lock operations, enabling lockset analysis. helgrind + is open source and available at http://svn.ipd.uni-karlsruhe.de/trac/helgrindplus."
"for every loop l: 1) d l (condition l ): the set of variables, on which the condition condition l of the loop l depends"
"we measured the runtime behavior and the memory requirements of our detector in different features on the basis of the parsec benchmark. firstly, we measured the memory usage of instrumented code run by the detector. all measurements are average values of five executions with two threads using the simsmall or simmedium inputs for all simulations. we used simmedium inputs for streamcluster and swaptions, as the runtime with simsmall was too short. figure 6(a)"
"modes. there is some overhead caused by the new features implemented for the ad-hoc feature. compared to drd the memory overhead is higher. however, the memory overhead in helgrind + is small enough that real world applications with higher memory requirements are still testable. optimizing our implementation could help reduce the memory overhead, which we intend to do as a future work."
"in distributed databases, queries are executed over one specific target fragment or in parallel by accessing different fragments of the dataset independently. data fragmentation can be very effective in distributed query processing where there is a well-known set of frequent queries. such queries must be analyzed so that a corresponding fragmentation design 2 can be achieved [cit] . a good fragmentation design is one that benefits the frequent queries, thus allowing for gains in performance. gains are focused on locality of access and data pruning rather than parallel processing. the price to be paid is poor performance for some of the non-frequent queries and limited opportunities for high performance parallel queries. to illustrate, assume an xml collection corders that contains information about customer's orders. queries are usually targeted at regions as follows: south america, north america or other continent. assume also that the collection is physically fragmented into three fragments, based on their location: one containing orders of customers from north america (allocated at node n1 ), one containing orders of customers from south america (allocated at node n2 ), and a third one containing orders from other continents (allocated at node n3 ). now, assume that customers from north america buy items usually over 1,000 dollars. on the other hand, south american customers' orders are usually around 500 dollars. now, suppose we want to run a query to retrieve the average total of orders with items above 1,000. this query will be distributed to the three fragments: sub-query s1 will be executed over the north america fragment, sub-query s2 will query the south america fragment, and sub-query s3 will run over the remaining fragment. since there are a lot more orders from customers in north america that satisfy this query predicate, s2 and s3 will probably finish their processing way before s1. thus, the total query processing time will be highly influenced by s1, and there is nothing n2 and n3 can do to help n1 processing s1, since they do not have the required data. if there is a following operation for these intermediate results, skew continues to be propagated. even if the fragments are replicated, since the sub-query s1 is already running in n1 over the whole north america fragment there is no way to stop this subquery (assuming query processing follows a static optimized execution plan) and redistribute the elements of this long processing fragment to idle nodes."
"if we switch off the support of pthreads (nolib+spin) so that the detector works as a happens-before detector based on identifying only spinning read loops (nolib+spin), approximately the same results are achieved. only in four cases the number of false positives increased slightly. drd produces more than 1000 warnings for some programs."
"a few failed test cases used ad-hoc synchronization. however, it is not easy to detect them, as they do not follow the standard pattern in ad-hoc synchronization and use a function pointer call to evaluate the loop condition in spinning read loops. this is a limitation in the implementation of our algorithm. we aim to remove this limitation in future work to further decrease the number of false positives."
"ad-hoc fragmentation approaches do not take the schema into consideration when defining the fragments. to generate the fragments, they either arbitrarily cut the document and mark it in a way that it can be reconstructed later, or use some kind of constraint over the document. we call these alternatives holes and fillers, and constraint-based, respectively."
"in addition, a race detector could be based only on this general approach to detect synchronization operations based on spinning read loops in a program. such a race detector is a pure happens-before detector. it cannot make use of lockset algorithm, because it is not aware of locks. in our case, if we turn off the support of pthreads so that synchronization primitives of pthreads are not directly intercepted, we will get a pure happens-before detector. our empirical results show that relying only on this general approach for identifying synchronization operations in the program might become too conservative in some situations. there may be obscure implementation of spinning read loops that are difficult to detect, leading to false positives. we used this approach as complementary method to our hybrid race detection algorithm [cit] to identify ad-hoc synchronizations together with synchronization primitives of unsupported libraries to achieve best results."
"step 2 examines for all variables v that the condition l depends on, if v is modified inside the loop. if there is an assignment to any such v, then the loop l is not a spinning read loop. otherwise, the loop is marked as performing spinning reads only, and the variables of d l (condition l ) are prepared for instrumentation."
"loops are converted to conditional branches at low level code. hence, for the implementation of our algorithm, we consider all conditional branches in the ir code. we search the control flow graph for loops that span a maximum number of three to seven basic blocks. then, we track the dependencies of each variable within these basic blocks by constructing a data dependency table. the data dependency table is built up with respect to registers at the ir level. all temporaries and addresses in basic blocks are traced to identify the registers they depend on. by means of the dependency table we can now check if the loop condition variable depends on a register that is target of a load instruction. we instrument the spinning read loop and insert the required instructions to intercept and analyze it at runtime, if the load addresses stay constant. helgrind + is also able to intercept calls to library functions. it instruments direct calls to library functions of pthreads (posix threads) library for runtime checking."
"a side benefit of this approach is that it can also be applied to unknown libraries. helgrind + currently uses information about the synchronization constructs of pthreads, but if application programmers use different libraries, then our enhanced helgrind + can also detect races reliably, provided the libraries are based on spin loops. note that even operating system calls such as wait that relinquish the processor are typically used inside loops and therefore detectable by helgrind + . a surprising result is that information about pthreads can be removed entirely from helgrind +, resulting in only a minor increase in false positives. thus, helgrind + with spin loop detection can be seen as a universal race detector."
"also, the method presented in this work is general and could be used as a complete race detection approach in a race detector. the resulted race detector is a universal happens-before race detector. compared to other happensbefore race detectors such as drd [cit], this method induces also happens-before edges when using ad-hoc synchronization or unknown synchronizations primitives, resulting in substantial accuracy."
"it is worth mentioning that the path expression p cannot retrieve nodes that may have cardinality greater than one, except when the element order is indicated. this restriction assures that the fragmentation results in well-formed documents, without the need of generating artificial elements to reorganize the subtrees projected in a fragment."
"the so called spin-lock synchronization is the most common and simplest synchronization construct [cit] . it employs a condition shared among two threads. one thread executes a while loop waiting for the value of the condition to be changed by the other thread. at the moment the value changes, the waiting thread is able to leave the loop and proceed. figure 4 illustrates the use of the spin-lock. thread 2 executes a spinning read loop on the shared variable condition, until thread 1 changes the value. the read and write operations are the source of a harmless synchronization race that need not be reported to the user. a number of publications analyzed different implementations of synchronization operations [cit] and observed that the spinning read loop is a common pattern used for implementing synchronization constructs. for example, the barrier implementation in figure 3 also uses a spinning read loop on a flag and a write ending the spin."
"we provide a dynamic method that detects ad-hoc synchronization constructs reliably, provided they use spin loops that examine condition variables. the method dynamically and automatically identifies these loops by analyzing the object code. the signaling thread cannot be determined through analysis, but it can be found dynamically by instrumenting the code. our method detects both reads and writes on condition variables and then establishes a happensbefore relation between signaling and signaled threads, thus preventing the generation of false warnings. the method has been added to the race detector helgrind + [cit] . the results on substantial benchmark suits confirm that helgrind + eliminates false warnings without missing true races."
"the general idea of our method is as follows. helgrind + searches the binary code just before execution to find all loops. this is done by building a control flow graph at preruntime. next, it narrows the set to spinning read loops based on the following criteria:"
"consider the example in figure 4 . condition is the condition variable in the spinning read loop. assume the spinning read is entered by thread 2 before the counterpart write. the happens-before relation is constructed based on the data dependency (write/read dependency) on condition. thus, the detector will be aware of this synchronization operation and no race will be reported on x. the warning regarding the benign synchronization race on condition is also suppressed, since it is marked as being in the special state spin."
"we also used shadow memory [cit] for each memory location to maintain the information needed during runtime, namely state information and vector clocks. a special state called spin in shadow memory indicates variables that are used in spinning read loops."
"structured fragmentation can be performed in several ways. in this survey, we classify the existing approaches according to the way they define the fragments, which can be: hybrid, xpath-based, and set-oriented."
"the example in figure 1 also produces synchronization races. to see why consider the typical implementation of the barrier primitive in figure 3 . the intentional races on variable counter are synchronization races and harmless. synchronization primitives require data races to enable competition for entering critical sections, for locking, for changing condition variables, etc. our aim is to refine helgrind + in such a way that it does not report apparent or synchronization races, while reporting all other races."
"conceptually, our method is divided into two phases: instrumentation and runtime. in the instrumentation phase, all loops in the program are recognized and then only the spinning read loops are selected to be instrumented. during the second phase, a runtime data dependency analysis is carried out to construct the happens-before relation between related parts."
"+ delivers approximately equal execution times in all modes. as the figure shows, compared to drd, there is an execution overhead. but in case of dedup and fluidanimate drd's execution time is much higher than the helgrind + . this is because many locks are used in these two benchmarks compared to the other programs. on average, the slowdown factor is reasonable to apply for different kind of applications and get accurate results."
"this type of fragmentation favors the distributed processing with the goal of restricting the access to a subset of the data by using pruning strategies. this happens in cases where a query accesses a subset of the fragments (orders from north america with items above 2,000 dollars, for instance). when the goal is high performance, horizontal and vertical fragmentation can still be used. the idea in this case is to generate uniform fragments, with similar database (how each fragment should be defined) is called fragmentation design."
"set-oriented. [cit] defined horizontal, vertical and hybrid xml fragments inspired by the analogous definitions for the relational model [cit] . their approach is called partix and explores the analogy between relations and collections of trees (both are sets). it supports both sd and md databases. fragments are defined by xml algebra expressions [cit] . a horizontal fragment is defined by a selection operation, while a vertical fragment is defined by a projection, plus an optional set of path expressions that point to subtrees to be pruned out of the fragment. a hybrid fragment is defined by a selection followed by a projection, or vice-versa. the definitions presented on section 4 are extracted from this work, which is pioneer in the sense of formally defining correctness rules for the fragmentation definition. as in the relational model, the same algebra is used to define fragments and query predicates, which helps query decomposition and predicate matching. by using the same algebra and correctness rules, they were able to define a query processing methodology that is capable of automatically processing queries over distributed and fragmented databases [cit] ."
"the first model we ran allowed all three parameters to vary [the boundary (a), the drift (v), and the non-decision time (ter)]. the estimated parameter values did not follow a normal distribution; we thus used a decimal logarithmic transformation and ensured it normalized their distribution using lilliefors tests (supplementary table 3 ) before we applied the threeway rmanova. the three factors were the beneficiary of the decision, the payoff associated with the decision and the difficulty (dots coherence). the boundary (a) and the non-decision time (ter) showed no effect of any factor. we thus ran a model where only the drift (v) was free to vary across conditions. once again, we analyzed log(v) using the same three-way rmanova. in order to compare the goodness of fit of our models, we also ran the intermediate models (either the drift and the boundary or the drift and the non-decision time were allowed to vary) and compared the sum of the individual bayesian information criterion (bic) of the models."
"here, we designed a new paradigm, enabling the use of ddms to investigate the influence of the payoff associated with and the person affected by a perceptual decision (figure 1) . the participants performed a random dots task (left/right direction categorization) to win low or high payoffs, for themselves or for a close relative. we tested which of the ddm parameters are modified between other-affecting and self-affecting decisions: the drift rate of the decision variable (encoding) or the decision boundary (read-out; figure 2) ? changes in the distance between the starting point and the decision boundary (a) would mean that people integrate beneficiary-related motivation through the read-out mechanisms, setting the decision rules prior to starting the evidence integration itself. alternatively, a direct influence of self/other motivation on the decisional process could affect the drift rate of the decision variable, which is an index of the quality of evidence used for the decision. this would suggest that sources of motivation (payoff for self/payoff for other) are integrated together with the evidence for the choice alternatives into a single source of evidence during the accumulation process. finally, a variation in the non-decision time would indicate that the beneficiary-related motivation acts on cognitive mechanisms outside of the decision process itself, such as primary encoding of the stimuli and motor execution."
"it may be that participants tried to imagine their relative receiving the payoff, although not instructed to do so. this would have required higher cognitive demands and redirect part of the attentional load and neuronal energy from the evidence accumulation process. using the game theory and public good games, studies show that taking into account another person into a decision engages the processes of mentalizing (or the theory of minds) [cit] . it could also be that, when performing a self-affecting decision, more attentional resources are spent on the task (because of a higher motivation, due to direct self-benefit), thereby increasing the efficiency of evidence accumulation. in a study on value-based decision making combined with ddm, it has been suggested that, when choosing on behalf of another, a dual process takes place. stimulus value integration, reflected in the drift rate (v), would be firstly computed based on selfpreferences and then adjusted to the other's inferred preferences [cit] . for others with similar preferences, rts were longer and linked to a change in drift rate. analogous mechanisms could have occurred during our experiment as well. the importance accorded to the evidence, reflected in the drift rate (v) of the decision variable, could have been initially lower during other-affecting decisions, or it could have been readjusted during the time of the decision. alternatively, rts for dissimilar others were also longer but associated with a higher decision boundary (a), which could have been implemented to overcompensate for an increased uncertainty about their preferences [cit] ."
"as content are strong indicators of claim veracity [cit], we apply deep learning to extract linguistic features from the claim c. to avoid the ambiguity of claims and obtain salient credibility information, we generate a latent distribution based on the extracted linguistic features. the output of this claim encoder is the prior belief of the veracity of the claim."
"in figure 5 (a) and 5(b) we show the classification performance of the ablated model against the full model on the rumoureval and the pheme test subsets. here, we observe that the auxiliary information extracted from people's replies has a large impact to the final performance our model. in fact, every evaluation measure is increased by at least 10.11%."
"accuracy is a common evaluation measure for classification tasks. however, it is less reliable when datasets suffer from class imbalance. the evaluation measures precision, recall and f 1 complement accuracy because not suffering from this problem."
"in this paper, we tackle the automatic misinformation detection task, which consists in classifying whether a claim is true or false. most existing models employ feature engineering or deep learning to extract features from claims' content and auxiliary information such as people's replies. however, these models generate deterministic mappings to capture the difference between true or false claims. a major limitation of these models is their inability to represent uncertainty caused by incomplete or finite available data about the claim being examined. we address this problem by proposing a bayesian deep learning model, which incorporates stochastic factors to capture complex relationships between the latent distribution and the observed variables. the proposed model makes use of the claim content and replies content. first, to represent the claim content we employ a neural model to extract textual features from claims. to deal with the ambiguity of the language used in claims and obtain salient credibility information, the model generates a latent distribution based on the extracted linguistic features. since no auxiliary information has been used so far, we interpret this latent distribution as a prior belief of the claim being true. second, to extract auxiliary information from people's replies content, we rank all the replies of the claim in temporal order, and summarize them using a long short term memory neural network (lstm). finally, after updating the prior belief with the aid of the lstm output, the model computes the veracity prediction and its uncertainty. this updated prior belief distribution is interpreted as the posterior belief."
(1) an effective representation of uncertainty due to incomplete/finite available data; (2) a temporal order-based approach to extract auxiliary information from people's replies; (3) a sgvb algorithm to infer latent distributions; (4) a systematic experimentation of our model on two realworld datasets.
"in order to capture the semantic information of all replies, we sequentially input the concatenated hidden states of each reply into a lstm. we use the lstm rather than a bilstm because the former gives high weights to recent input, which matches our assumption on the relative importance of the latest reply. specifically, the lstm takes the hidden states of each reply as input in a sequential way:"
"despite encouraging experimental results, online misinformation detection is still a challenging problem with many open questions. in this paper, auxiliary information comes from people's replies alone, we argue that the proposed model can be enriched by utilizing other auxiliary information, such as source credibility. also, the reply stances are a strong veracity indicator for a claim, since false claims are usually controversial and accompanied by opposite stances. we let for future work, the combination of features extract from credibility analysis and reply stances."
"when playing at a shooting range in a fairground, we accumulate sensory evidence (about target movement) until we can shoot accurately, and win the prize. now, if such decisions are made so that the prize goes to a close friend, will we process and use information in the exact same way? more precisely, how does motivational incentives for someone else influence the mechanisms engaged in making simple perceptual choices as compared to the same decisions associated with the same incentives, but for you?"
"the text of a claim can provide linguistic features to help predict its veracity. since misinformation and false claims are created for financial or political purposes rather than to report an objective event, they often contain opinionated or inflammatory language [cit] . in order to reveal linguistic differences between true and false claims, lexical and syntactic features at character, word, sentence and document level have been exploited [cit] compute psycholinguistic features using a bag-of-words paradigm. [cit] compare the language of true claims with that of satire, hoaxes, and propaganda to find linguistic characteristics of untrustworthy text. [cit] construct a content credibility corpus and examine a list of language factors that might affect web content credibility based on which a predictive model is developed. [cit] compare heterogeneous articles of the same story and reveal that pieces of information cross-referenced are more likely to be credible. [cit] extract features from claim tweets including bag-of-words, presence of urls, and presence of hashtags. a support vector machine (svm) is then used to distinguish between true and false claims. [cit] leverages a tensor decomposition to derive concise claim embeddings that capture contextual information from each claim; and uses these embeddings to create a claim-by-claim graph on which the labels propagate. textual content has been empirically proven to be a strong indicator of claim veracity, and thus can be used as a prior probability."
"multimedia features have been shown to be an important manipulator for propaganda based on misinformation [cit] . as we have characterized, online misinformation exploits the individual vulnerabilities of people and thus often relies on sensational or even fake images to provoke anger or other emotional response of consumers. visual-based features are extracted from images and videos to capture the different characteristics of misinformation. faking images are identified based on various user-level and tweet-level hand-crafted features [cit] . recently, various visual and statistical features have been extracted for news verification [cit] develop a convolutional neural network to extract text and visual features simultaneously. visual features include clarity score, coherence score, diversity score, and clustering score. statistical features include count, image ratio, multi-image ratio, hot image ratio, long image ratio, etc. this approach suffers from the problem that some misinformation on social media does not contain multimedia content."
"taking advantage of the ddm and the perceptual decisionmaking framework, we provided a mechanistic explanation of how others are integrated into the decisional process. our results indicate that the beneficiary of the incentive associated with a decision modifies how decisions are performed. decisions were faster for self than for others. as explained by the ddm, this was related to a higher drift rate (v) of the decision variable. in the present experiment, better sensitivity and faster rt were mirrored by higher drift rates. higher drift rates have been found to explain shorter rt in tactile discrimination as well [cit] . a change in the drift rate of the decision variable indicates a modification of the integration process itself, as branding does for economic value-based choices [cit] . our result indicates that sensory evidence is integrated faster for self than for others. in the example of the shooting range, if we aim to reach a target to win a price for a close relative, the decision process would not differ in the amount of evidence we would accumulate before making the decision to shoot, but rather in the efficiency of accumulation of the sensory evidence."
"the credibility analysis of the sources of a claim is an important auxiliary information. as misinformation is usually published by unbelievable individuals or automatic bots, credibility plays a crucial role in message communication [cit] . accurate and timely discrimination of such accounts inhibits the proliferation of misinformation at an early stage. tseng and fogg [cit] identify two components of source credibility, namely trustworthiness and expertise. trustworthiness is generally taken to mean truthful, unbiased and well intentioned. expertise instead is understood as knowledgeable, experienced and competent. thus, features that can reveal the trustworthiness and expertise of information sources are strong indicators of source credibility. with the aid of information source [cit] examine the credibility of tweets related to the fukushima daiichi nuclear disaster in japan. they found that tweets from highly credible institutions and individuals are mostly correct. useful account features can be derived from the account demographics, such as integrity of personal information, the number of followers and followees [cit] . besides, aggregating a group of account features are indicative, since spreaders of true and false claims might come from different communities [cit], such as the percentage of verified user accounts [cit] and the average number of followers [cit] . however, account demographics can easily be altered to decrease the similarity between credible and incredible sources."
"sensory encoding of information basically relies on the quality of the available evidence [cit] . a foggy weather would slow the rate at which the decision variable rises, as compared to clear climate conditions. reliability of the decision depends on the distance between the starting point of the decision variable and the decision boundary; the decision rules are set by the read-out mechanisms [cit] . reaching higher decision boundaries requires more evidence to be accumulated, thus leading to a better accuracy, but takes a longer time. which of the evidence accumulation stage (drift of the decision variable) or the read-out mechanisms (distance between the starting point and the decision boundaries) would be adjusted differently based on vicarious information (the beneficiary of the decision)? how is the perceptual decision process modulated when the source of motivation concerns a close relative rather than oneself?"
"participants performed a random dots (left/right direction categorization) task to win low or high payoffs, for themselves or for a close relative. rts and sensitivity (d') were collected and analyzed using three-way rmanovas, with \"beneficiary\" (two levels: other vs. self), \"payoff \" (two levels: high vs. low), and \"difficulty\" (two levels: difficult vs. easy) as factors."
"the remainder of the paper is organized as follows: § 2 summarizes the related work; § 3 defines the misinformation detection task; § 4 details the proposed bayesian deep learning model; § 5 derives the stochastic gradient variational bayes optimization algorithm; § 6 describes the used datasets and experimental setup; § 7 is devoted to experimental results, and; § 8 concludes the paper."
"in this subsection, we analyze the contribution of ranking the replies according to their temporal order. we compare this against a random order.specifically, we randomize the h m d before it is input to the lstm."
"misinformation has been existing for centuries in different forms of media, such as printed newspaper and television. recently, online social media platforms are also suffering from the same issues. recent work on misinformation detection have tried to understand the differences between true and false claims in various aspects: claim content, information source, multimedia such as affiliated images and videos, and other users' engagement."
"the posterior belief is generated by combining the claim and reply information via a mlp. the strong non-linearity of mlps make them suitable to find complex relationships between the claim and its replies. specifically, the mlp input is the latent claim variable z concatenated to the hidden state of replies h d :"
"in this subsection we evaluate the effect of the dimension of the latent variable z. to do this after setting a dimension for z we optimize the rest of the hyperparameters on the validation subset. in figure 8 (a) and 8(b) we show the effect on performance of the dimension of z on both datasets. we observe that the results are similar for both evaluation measures, accuracy and f 1 . varying the dimension from 1 to 5 the model brings a larger performance improvement than when varying it from 5 to 25. when the dimension is 15 the model obtains the highest accuracy, 81.22%, on the rumoureval test subset while when the dimension is 10 the model obtains the highest f 1, 78.78%, on the rumoureval test subset and highest accuracy, 80.33% and f 1, 78.78%, on the pheme test subset. these results also show that the increase in model capacity may not necessarily lead to an improvement in performance. the reason could be found on the limited size of the datasets, which might cause overfitting when the model is too complex."
"in § 4.1, we developed a prior belief of the claim veracity. in this section we show how to correct this prior belief by including its replies."
"in this section, we present our proposed bayesian deep learning model that effectively integrates claim and people's replies. we will first introduce how to encode claim content with deep learning and generate a latent distribution that is interpreted as a prior belief of claim veracity. we then describe the temporal-ordered approach to encode people's replies, which captures semantic variation along the time line. finally, we correct the prior belief with the aid of people's replies, the result of which process is interpreted as the posterior belief of claim veracity. figure 2 describes the proposed model."
"before going to the laboratory, the volunteers were asked to choose a close relative for who they would be willing to play for, on half of the experiment. at their arrival, the participants sat in the experimental room, were informed, and gave their written consent. their relationship with the chosen person was asked [seven participants chose one of their parents (mother or father), seven chose a sibling, eight chose their lover, three chose a friend, and two chose their roommate]. a few demonstration trials were shown, for them to see how the condition cue (payoff and beneficiary) was displayed. subjects were trained and then finally completed the task. it lasted approximately 64 min, in four blocks of 16 min each. all were debriefed when the task was over."
"a square was always present in the middle of the screen. on top of this square appeared the cue, which indicated the beneficiary and payoff conditions of the forthcoming trial. the dots were displayed inside the area defined by the square. the square and the cue were colored yellow or blue, according to the beneficiary of the payoff associated with the trial. the color was used to emphasize the beneficiary of the trial and was counterbalanced between subjects."
"payoffs for others could have been integrated into the perceptual decision process through a change in the decision rules, outside of the mechanism of sensory evidence accumulation and change the distance between the starting point of the decision variable and the decision boundary. other researchers also suggested that payoff can modify both stages, evidence accumulation and decision boundary. it postulates two processes, one for payoffs and another for stimulus information, and that on a given trial, attention is directed toward one of these information, never both [cit] . sequential-sampling models have previously been used to account for the effects of payoffs in a perceptual decision task with time constraints. these studies have reported changes in the distance from the starting point to the decision boundaries, a bias in the starting point of the decision variable, induced either by prior probabilities of being correct [cit] or by asymmetrical payoffs associated with the possible response alternatives [cit] . these changes were characterized by a shift of the starting point of the decision variable closer to the decision boundary associated with the alternative having the higher probability or associated with the higher payoff. the starting point is then further from the other boundary (for the other alternative at hand) and the decision variable is less likely to reach it, establishing a bias and a change in response proportion."
"in this subsection, we evaluate the impact of using a latent distribution into the claim encoder on the misinformation detection task. to evaluate the impact of the latent distribution p, we ablate p in our model and compare its classification performance against the full model. specifically, the ablation is done by taking the output of the bilstm hidden states, i.e., h c and give this as input to the output mlp. the rest of the model remains unchanged. since no latent distribution is involved, the ablated model is optimized in accordance with the conventional softmax loss minimization."
"in order to compare the performance of our proposed model against the baselines, we experimented with two real-world benchmark datasets, the rumoureval [cit] and the pheme [cit] datasets. both datasets contain twitter conversation threads about news (like the example shown in figure 1) . a conversation thread consists of a tweet making a true or false claim, and branches of people's replies expressing their opinion about it. a summary of the datasets statistics is available in table 1 . [cit] task 8 competition. this dataset consists of 325 source tweets the pheme dataset is constructed to help understand how users treat online rumour before and after the news is detected to be true or false. like the rumoureval dataset, we divide the pheme dataset into a training subset, a validation subset and a testing subset. specifically, 70% of the claims are randomly selected as training instances, 10% as validation instances and the rest as testing instances. users' replies are divided according to the claims."
"each trial began with the cue, which had a jittered duration from 800 to 1,200 ms and was used as inter-trial interval (iti). the cue consisted in a word announcing the beneficiary of the decision (\"him\" for others-affecting decisions, \"me\" for selfaffecting decisions) to the left of a rectangle filled proportionally to the payoff associated with the decision (full filled rectangle for 10€, one-fifth filled rectangle for 2€). this cue remained on the screen during the entire subsequent trial. after the cue, the first frame of the rdk to come (a picture of stationary dots) was shown for 1,000 ms. then, dots motion began and lasted for 2,000 ms, during which the subject had to respond. motion coherence was either 13% (difficult) of 15% (easy), for all participants. at the end of the 2,000 ms of dots motion, the feedback illustrated the payoff for 500 ms. if the response was correct, a pile of coins proportional to the payoff (2 or 10€), was shown together with the value of payoff itself (\"+2, \" \"+10\"). for incorrect responses and misses, a red-colored cross was displayed together with \"+0\" above it. at the end of the trial, a new iti was displayed, showing the cue for the trial to come."
"the misinformation detection task is a binary classification task. such tasks are commonly evaluated by the following evaluation measures: accuracy, f 1, precision, and recall."
"for doing the experiment and could win 2€ or 10€ more for themselves and also 2€ or 10€ for their relatives. the participants were asked to discriminate the left/right direction of coherently moving dots. they were instructed that they had to give one, and only one, response during the dots motion: if they gave more than one response or did not respond (miss), the program would consider it as incorrect. money was not accumulated over trials, nor was such accumulation shown to the participants. they were told that one trial of each of the beneficiary condition (self and other) would be randomly selected (by a computer program) to determine their final payoffs. the payoff associated with the trial would be won by the beneficiary, if it was a correct trial. participants were told (and believed) that the payoff for the other (as well as for themselves) would be sent after completing the experiment. in reality, the close relative received nothing and all participants received 20€ (as if the selected trial was won for himself and associated with a high payoff). this procedure (i) ensured that participants treated all decisions as equally relevant, both for themselves and their close relative; (ii) avoided any competition effects to arise between self and other interests. also, accuracy was implicitly emphasized by telling the participants that, although they would have to adapt to the given 2 s to answer, time should not be a problem since the duration of the stimuli was chosen based on previous experimental results (pilot study)."
"a valuable attempt at rectifying this epidemic of false claims has been tackled by some news websites, such as: snopes 1, polifact 2, and emergent 3, which have employed professional journalists to manually check and verify every potential false news. however, such manual approach is very expensive and way too slow to be able to check all the daily generated claims appearing on the web. thus, making automatic tools is in great need to speed up this verification process."
"in the last decades, the framework of sequential-sampling models, such as drift diffusion models (ddms), has proven to be a powerful approach to explain the process of making a decision [cit] . ddms successfully capture the complex relationship between choice and reaction times (rts) by decomposing these behavioral data into internal cognitive components of decision processing. in this framework, a decision reflects a decision variable drifting with a given rate (v), from an intermediate starting point (z) toward one of the decision boundaries at hands. each boundary is separated from the starting point (z) of a given distance (a) and acts as a decision threshold for an option, so that the response of a decision is initiated when the decision variable reaches one of the boundaries. in the example of the shooting range, the decision variable would accumulate information about the position of the moving ducks over time, and when (relative) certainty about their position is reached, the decision of pulling the target is made."
"we seek to answer the following four research questions, which will be guide the remainder of the paper: rq1 does our model outperform the state-of-the-art misinformation detection baselines? rq2 does the incorporation of the latent distribution outperforms a deterministic counterpart? rq3 does the auxiliary information from people's replies produce a more accurate posterior belief of claim veracity? rq4 is the temporal order better than random when encoding replies? rq5 is it beneficial to incorporate a latent variable to encode replies? rq6 how does the dimension of the latent variable z affect the model's performance?"
"our model inherit two advantages: first of all, the model incorporates a latent distribution, which enables to represent uncertainty and promote robustness; second, the bayesian model formulates all of its prior knowledge about a claim being examined in the form of a prior, which can be updated by more added auxiliary information generating more accurate detection results. to sum up, the proposed model advances state-of-the-art methods in four aspects:"
"although the digital news consumption has increased in the last decade, the increasing amount of misinformation and fake news has not certainly proven its quality. different from traditional media where news are published by reputable organizations, online news on social media platforms such as facebook and twitter are shared by individuals and/or organizations without a careful checking or with malicious intents. in figure 1 we show a false claim posted on twitter about an alleged shooting in ottawa. while some users showed surprise and asked for further clarifications in their replies, other users believed the claim and re-tweeted it as if it was true. this misinformation, when done on a large scale can influence the public by depicting a false picture of reality. hence, detecting misinformation effectively has become one of the biggest challenges faced by social media platforms [cit] ."
"we test our bayesian deep learning model against six state-of-theart models. in order to have a fair comparison, only those models using the claim content and users' replies have been selected."
in figure 4 (a) and 4(b) we show the classification performance of the ablated model against the full model on the rumoureval and pheme test subsets. we observe that the full model outperforms the ablated one by at least 7.77% on every evaluation measure. this demonstrates the better representation quality achieved by the use of the latent distribution.
"in figure 8 (a) and 8(b) we show the model performance comparison. we observe that the new latent distribution does not have an effect on the performance on the model for all the evaluation measures and dataset test subsets. based on this analysis, we conclude that the incorporation of the additional latent distribution for replies does not provide any additional improvement in performance."
"reaction times for corrects and rts for errors were analyzed separately, and rts were logarithmically transformed. logrt and sensitivity (d') normality distribution was ensured using lilliefors tests. logrt and d' were then analyzed using three-way repeatedmeasures analyses of variances (rmanovas). the factors were as follows: \"beneficiary\" (two levels: other vs. self), \"payoff \" [two levels: high (10€) vs. low (2€)], and \"difficulty\" [two levels: 13% motion coherence (difficult) vs. 15% coherence (easy)]. beneficiary and payoffs were overt factors, indicated by cues on each trial, but difficulty was not explicitly given to participants. during debriefing, we asked participants during debriefing how many difficulty levels they perceived. most of them perceived two levels; only two of them thought there were more and one did not conscientiously perceived any. all post hoc analyses were performed using lsd fisher tests. there was no effect of gender on behavior (supplementary table 2 ). although there could be effects of sex hormone variations on decision making in young women, we did not record the phase of the menstrual cycle in our sample. all statistical analyses were performed using statistica (statistica r, dell inc., 2015), except for normality tests and ddm fitting, performed on matlab r ."
"we now evaluate the contribution people's replies in the misinformation detection task. in order to examine its contribution we compare our full model with and without replies. specifically, we ablate the input coming from the replies to the final mlp, which now is used only to perform a non-linear transformation of the latent variable z."
"to avoid the ambiguity of claims, instead of a deterministic non-linear transformation, we generate a latent distribution, from which we sample a latent stochastic variable z. to embed linguistic information into the latent variable, we set the latent variable to be conditional on h c :"
"it is to be noted that we actually ran a first experiment using another anonymous, randomly selected, participant as \"the other.\" however, there was no main effect of the beneficiary on rt or on d' (supplementary table 1 ). since we were aiming to characterize how others are taken into account into the perceptual decision-making process, and based on the literature showing that familiarity increase vicarious effects [cit], we adapted our task with a close relative."
in this section we start by introducing 4 research questions. we then present the methodology used to answer them. the software used to run the experiments in this paper is available on the website of the first author.
"in this paper, we study the problem of misinformation detection on social media platforms. one major problem faced by existing machine learning methods is the inability to represent uncertainty due to incomplete or finite available information. we address the problem by proposing a bayesian deep learning model. when encoding claim content, we incorporate a latent distribution accounting for uncertainty and randomness caused by noisy patterns in the finite dataset. this latent distribution provides a prior belief of claim veracity. we also encode auxiliary information from people's replies in a temporal order through an lstm. such auxiliary information is then used to update the prior belief generating a posterior belief. in order to optimize the bayes model, we derive a minibatch-based gradient estimation algorithm. systematic experimentation has demonstrated the superiority of our approach against the state-ofthe-art approaches in the misinformation detection task."
"in figure 6 (a) and 6(b) we show the performance comparison of these two orders. we observe that the temporal ordered replies achieve better performance than the random ordered. besides, the random ordered model is still worse trnn yet better than multitask. this is probably because trnn takes the temporal structure of replies into the model while multitask fail to involve temporal information."
"the data used in the present paper will be available to any reader after publication. the datasets generated and/or analyzed during the current study will be available in the repository, on a permanent free-access web link."
"in contrast, our experimental setup was designed to avoid response probability manipulations toward one of the (left or right) alternatives, in terms of probability (through trials randomization) and in terms of payoff (by assigning the same payoff to both response alternatives). we aimed to compare identical decisions made by the participants, either for themselves or for another person. it would be interesting to adapt our paradigm to asymmetrical alternatives, with the payoff going to one of the beneficiaries depending on the correct answer. following our results, it could be expected that a bias toward the response associated with self-payoff would emerge. finally, a variation in the non-decision time (ter) would have indicated that the beneficiary-related motivation acts on cognitive mechanisms that are outside of the decision process itself, such as primary encoding of the stimuli and motor execution. non-decision time is usually referred to as reflecting the early encoding of the stimulus of interest and the execution of the motor response, once the decision process is completed [cit], both external to the visuo-motor decision process in itself. moreover, the non-decision time is thought to be necessary to account for speed-accuracy trade-offs [cit], and it has been shown that speed-accuracy instructions also modulate the non-decision time [cit] . variation in the non-decision time can mean that different strategies are applied [cit] and could include other components that influence the decision-making processes. however, the ddm cannot distinguish between different mechanisms within the non-decision time."
"considering the improved performance brought by the latent distribution for claims, in this subsection we answer whether it would be beneficial to incorporate a latent distribution also for replies. in order to answer this research question, we expand our model by adding a new latent distribution in the reply encoder. similarly to what done for the claim encoder, the new latent distribution is designed as a multidimensional gaussian distribution with mean and covariance matrix derived from the lstm output h d (as in eq. 3, 4 and 5). a new latent variable is sampled similarly as in eq 6 and input to the mlp to predicting veracity of the claim being examined."
"the ddm assumes that two-choice decisions are made by a noisy process that accumulates information over time from a starting point (z) toward one of two choice criteria or boundaries (here, corresponding to left and right response decision, respectively; figure 2a ). when one of the boundaries is reached, a response is initiated. the starting point and the decision boundaries are separated by distance (a). the evidence that drives the accumulation process, the drift rate (v), is derived from the representation of the stimulus. the better the quality of the evidence, the larger the drift rate toward the appropriate decision boundary, and the faster and more accurate the response ( figure 2c ). the components of processing acting outside the decision process itself, such as encoding and response output, are combined in a single parameter: the non-decision time parameter (ter). rt being the result of non-decision time added to the time it takes for accumulated evidence to reach one of the boundaries, and sensitivity coming from the reached boundary that determines which response is given, the model extracts the components of the decision process (values of drift rate, nondecision processes, and boundaries) from rt distribution and sensitivity data simultaneously."
"where 0 is a vector of zeros and i is the identity matrix. by making use of the latent variable (z), our model is able to capture complex noisy patterns in the data."
"we now present the people's replies encoder to obtain auxiliary information. this auxiliary information is claim-specific and is used to generate the posterior belief by correcting the prior belief of the claim veracity. replies on social media platforms are listed along the time line as shown in figure 1, where the earliest reply appears at the top of the list. truth about an event can be gradually manifest as more evidence emerges, thus we assume that the latest replies tend to be more reliable and more important than the earlier replies. based on this assumption, we design a two-layer recurrent neural network to encode replies: the first-layer applies a bilstm to summarize the semantic information of each reply and the second-layer applies a lstm to capture the temporal semantic variation of the replies."
"in order to train the proposed bayesian deep learning model, due to the analytical intractability of the posterior distribution, we develop a stochastic gradient variational bayes (sgvb) algorithm. a tractable evidence lower bound (elbo) objective function of our model is derived to approximate the intractable distribution. the model is optimized along the direction of maximizing the elbo objective function."
"on the basis of behavioral data alone, such as choices and response times, reliably identifying discrete latent states can be difficult or near impossible. [cit] aimed to identify contaminant trials -data points not generated by the process of interest -in a perceptual decision-making experiment. they defined a latent mixture model in a bayesian framework that attempted to partition trials that were sampled from the (diffusion model) process of interest from contaminant trials distributed according to some other process. in attempting to segment trials to latent classes, the diffusion model was only informed by the same choice and response time data it was designed to fit. for a representative participant, only 0.6% of their 8000 trials were classified as contaminants, indicating either a remarkable ability of the participant to remain on task (which is unlikely; see, e.g., [cit] ), or, more likely, to the limited ability of behavioral data alone to segment trials into latent states."
"psychological theory is advanced through empirical tests of predictions derived from quantitative cognitive models. as cognitive models are developed and extended, they tend to increase in complexityleading to more precise predictions -which places concomitant demands on the behavioral data used to discriminate between candidate theories. to aid discrimination between cognitive models and, more recently, to constrain parameter estimation, neural data have been used as an adjunct to behavioral data, or as a central stream of information, in the evaluation of cognitive models. such a model-based neuroscience approach entails many advantages, including precise tests of hypotheses about brain-behavior relationships. there have, however, been few systematic investigations of the capacity for neural data to constrain the recovery of cognitive models. through the lens of cognitive models of speeded decisionmaking, we investigated the efficiency of neural data to aid identification of latent cognitive states in models fit to behavioral data. we studied two theoretical frameworks that differed in their assumptions about the composition of the latent generating state. the first assumed that observed performance was generated from a mixture of discrete latent states. the second conceived of the latent state as dynamically varying along a continuous dimension. we used a simulation-based approach to compare recovery of latent data-generating states in neurally-informed versus neurally-uninformed cognitive models. we found that neurally-informed cognitive models were more reliably recovered under a discrete state representation than a continuous dimension representation for medium effect sizes, although recovery was difficult for small sample sizes and moderate noise in neural data. recovery improved for both representations when a larger effect size differentiated the latent states. we conclude that neural data aids the identification of latent states in cognitive models, but different frameworks for quantitatively informing cognitive models with neural information have different model recovery efficiencies. we provide full worked examples and freely-available code to implement the two theoretical frameworks."
"throughout the manuscript, we position our work within the theoretical context of mind wandering. over the past decade, the scientific study of mind wandering has received great interest from behavioral (e.g., [cit] and neural (e.g., [cit] perspectives, though there have been few attempts to integrate the two streams of information in a model-based cognitive neuroscience framework [cit] . the study of mind wandering is particularly relevant to our aim of identifying latent cognitive states as it is a phenomenon that has been studied under various, qualitatively distinct, hypotheses about how latent states give rise to observed performance [cit], 2015, which we expand upon below. mind wandering, therefore, serves as an excellent vehicle through which to demonstrate our methodological approach. our working hypothesis is that mind wandering is a neural state or process that affects the parameters of cognitive models, which in turn affect observed behavioral performance [cit] . our approach inverts this chain of causation: we fit behavioral data with cognitive models that are informed with neural data, and compare their fit to cognitive models that are not informed with neural data. this allows us to assess what can be learnt about mind wandering in a way that is not feasible without the discriminative power of the neural data."
"through the lens of cognitive models of speeded decisionmaking, we consider two approaches that use neural data to constrain cognitive models, which in turn helps to identify both when people mind wander and the effect it has on task performance. we note, however, that our methods generalize to any domain of study that utilizes neural data -or any additional stream of data, for that matter -to aid identification of latent datagenerating states and fit the behavioral data arising from those states with cognitive models."
"we consider two general approaches to incorporating mind wandering within a modeling framework. the first approach assumes that observed behavior arises from a mixture of discrete latent states, which may have partially overlapping or unique sets of data-generating parameters. we refer to this as the discrete state representation. one might think of the latent states as reflecting an on-task state, where attention is directed to external stimuli, or task-related thoughts, and an off-task state, where attention is directed to internal stimuli, or task-unrelated thoughts, similar to the perceptual decoupling hypothesis [cit] . alternatively, the latent states might reflect executive control, where an executive system oversees maintenance of goal-directed behavior, and executive failure, which occurs when the executive control system fails to inhibit automatically cued internal thoughts that derail goal-directed behavior [cit] . regardless of the labels assigned to the latent states, models assuming a discrete state representation aim to first identify the mutually exclusive latent states and then estimate partially overlapping or distinct sets of model parameters for the discrete states [cit] . we note that a discrete state representation is also considered outside the context of mind wandering. [cit] developed a hidden semi-markov model approach that used a continuous stream of eeg data to identify discrete stages of processing in associative retrieval."
"rather than relying solely on behavioral data, here we examine whether augmenting cognitive models with an additional stream of information -such as neural data, whether that involves single cell recordings, eeg, meg, or fmri -aids identification of latent data-generating states underlying observed behavior. our aim is to investigate whether the addition of neural data can improve our account of the behavioral data, and in particular the identification of latent states, rather than accounting for the joint distribution of behavioral and neural data [cit] . to this end, we condition on neural data; that is, we do not consider generative models of neural data. rather, we explore tractable and simple methods that augment cognitive models using neural data as covariates in order to gain greater insight into cognition than is possible through consideration of behavioral data in isolation."
"leading to finer-grained predictions for data. although increasingly precise model predictions are undoubtedly a benefit for the field, they also increase the demands placed on data to discriminate between competing models. the predictions of cognitive models have traditionally been tested against behavioral data, which is typically limited to choices and/or response times. such behavioral data have been extremely useful in discriminating between model architectures (e.g., [cit] . as model predictions increase in precision, however, we approach a point where behavioral data have limited resolution to further constrain and discriminate between the processes assumed by the models of interest. the problem of behavioral data providing limited constraint is compounded when one aims to study non-stationarity. cognitive models typically assume a stationary generative process whereby trials within an experimental condition are treated as independent and identically distributed random samples from a probabilistic model with a specified set of parameters. this assumption has proven extremely useful, both practically and theoretically, but is not supported by fine-grained empirical analysis (e.g., [cit] . recent work in the study of stimulus-independent thought, or mind wandering, provides a psychological mechanism that can explain these findings, at least in part, in terms of observed performance arising from two or more latent data-generating states. one prominent theory proposes that ongoing performance is driven by two distinct phases: perceptual coupling -where attentional processes are directed to incoming sensory input and completing the ongoing task -and perceptual decoupling -where attention is diverted from sensory information toward inner thoughts [cit] . the perceptual decoupling hypothesis of mind wandering proposes, therefore, that observed behavior is the end result of a mixture of discrete latent data-generating states. to gain insight into the processes underlying the phases of perceptual coupling and decoupling, the goal of the cognitive modeler is to use the available data to determine the optimal partition of trials into latent states."
where r(t i ) is the vector of set-point signals while q and r are positive symmetric matrices. the optimal value of the cost function (j ) can be written as (21) with
"a continuous-time receding horizon-based feedback linearizing model predictive excitation controller is designed for synchronous generators in multimachine power systems. the nonlinear dynamical model of multimachine power systems is augmented into a linear one using the partial feedback linearization approach and the receding horizon model predictive control scheme is then used for the linearized system. the designed control scheme is more practical, simple, and cost-effective in the sense that it eliminates the use of an observer for measuring the rotor angle. the designed controller is implemented on a large multimachine power system and simulation studies are carried out under different operating scenarios. simulation results clearly indicate the enhancement of the dynamic stability of the system with the designed controller in terms of providing adequate damping and improved settling time under different operating conditions. an extension of this work will deal with the implementation of the proposed control by considering the dynamics of steam-valve systems along with the dynamics of synchronous generators."
the approach presented in this section is used to design the feedback linearizing model predictive excitation controller for multimachine power systems as discussed in the following section.
"the mpc can be designed in a straightforward way for the feedback linearized system in equation (9) if it is exactly linearized. otherwise, it is essential to analyze the dynamics of remaining n − r states which are not transformed through the nonlinear coordinate transformation [cit] . if the dynamics of remaining n − r states are stable or do not have any impact on the stability, the mpc can be designed for the reduced-order feedback linearized systems."
"the light emitting diode (led) has caused a profound change within the lighting industry. this is due in part to the led's key properties of being physically small, highly efficient, digitally controlled and soon, very cheap to manufacture. being physically small the led can be positioned or embedded into luminaires, materials and even the very fabric of a building or environment [cit] . the price to pay for all this functionality and flexibility is complexity. in the past, the single light bulb was controlled using a single switch; on and off. led-based lighting systems can easily consist of hundreds separate light sources, with each source having many individually controllable parameters including colour, intensity, and saturation. with this high complexity, end-users cannot be expected to fully control all aspects of the lighting system. one direction that is being explored is to enrich lighting systems with sensor networks that will enable automatic lighting control that is based on contextual information [cit] . however in many situations, such as setting up an atmospheric light, an explicit user interaction will still be required. moreover, as functionality and complexity of light systems grow, the mapping between the sensors data and the desired light outcome will become fuzzy and will require an explicit user interaction for fine tuning the outcome or for adjusting the mapping between sensor input and light output. thirdly, explicit interaction can be desired to allow users to feel in control while interacting with intelligent lighting systems. the light switch therefore in many situations will need to be replaced by novel forms of interactions that offer richer interaction possibilities such as tangible, multi-touch, or gesture-based user interfaces. as proliferation of led light continues, it becomes more important to go beyond scattered design efforts [cit] and systematically study user interaction with emerging lighting systems. the goal of this workshop is to take the first steps in this direction."
"equation (35) indicates that the internal dynamic corresponding to the state δ i − δ 0i is zero, i.e, it does not affect on the overall stability. in (36), t qoi is always positive which clearly indicates that the dynamic corresponding to the state e di − (x qi − x qi )i qi is stable. finally, it can be said that a partial feedback linearizing excitation controller (pfblec) can be designed for the multimachine power system model and equation (30) can be used to derive the original excitation control input which can be written as follows:"
"where the values of a p i, b p i, and c p i are provided in appendix. the mpc can be designed based on partially linearized system (31)-(32) while the partial feedback linearizing control law (u i ) can be obtained from (30) . however, it is essential analyze the stability of remaining 2 states before designing the proposed controller."
"the complexities in modeling power systems depend on several factors such as the required degrees of accuracies, intended applications, etc. despite these factors, some assumptions are always made during the power system modeling as the actual behaviors of different components are complicated [cit] . the dynamical models of synchronous generators and ieee type-ii excitation systems are considered in order to design the excitation controller. therefore, the dynamics of both synchronous generators and excitation systems are considered to model power systems [cit] . the synchronous generators have both mechanical and electrical dynamics. by considering n numbers of synchronous generators in a multimachine power system, the mechanical and electrical dynamics (based on the two-axis model) of i th machine can be represented in terms of the following equations [cit] :"
"this linear control input (v) can be incorporated with the feedback linearizing control input to obtain the feedback linearizing model predictive control input. for i th subsystem, the feedback linearizing model predictive control input can obtained from equation (10) and written as follows:"
"from the simulation results, it is clear that the designed controller performs well under different operating conditions securing minimal post-fault oscillations and thus, capable of improving the transient stability of power systems under a wide range of operating conditions."
"finally, an integral action needs to be performed on equation (24) in order to achieve the control input (u) which can be written as follows:"
"the three-phase short-circuit faults at the generator terminal are considered as the most severe faults in power systems which affect the equilibrium between electrical and electromagnetic torque resulting the loss of synchronism among coherent generators as well as requires disconnection of the affected generators from the system. therefore, the output voltage and power of the affected generator will be zero during the faulted conditions which in turn will influence the speed deviation and rotor angle responses of the generator."
"the rest of this paper is organized as follows. 'problem definition' introduces the problem, and 'model formulation' represents the model formulation. problem-solving methodology is described in 'hybrid simulation annealing', and computational results are discussed in 'results and discussion'. finally, a conclusion follows in the last section."
"in this section, we discuss how procedural content graphs compare to existing approaches, namely, grammars and graph-based design tools. we also briefly discuss performance and feedback received on the use of our system."
"in the above model, equation 9 minimizes the total travel cost and total travel time. constraints (10) and (11) ensure that each customer location is serviced from only one vehicle. constraint (12) states that, if a vehicle arrives at a node (i.e. customer location), it should leave it, and by this way, the route continuity is ensured. constraint (13) states that standard normally distributed function of the driver's transit time should be larger than z a . constraint (14) imposes that the vehicle capacity does not exceed from its limit. constraint (15) states that the depot is the first location of each vehicle. finally, constraint (16) eliminates the sub-tours."
"this can be solved using an \"amount filter by,\" which performs that separation internally using attributes, enumerated via node augmentations as grouping criteria (see section 3.8). this approach can quickly become impractical, if such grouping would have to be performed, for example, per neighborhood, then the building, and then per floor. on the other hand, this requires all collective node operations to support such criteria analysis, as well as to have differentiating attributes configured every time."
"although each entity is expected to have its specific features, all entities share the possibility to incorporate custom attributes. attributes are represented via a hash map in a keyvalue fashion and enable us to store properties in an entity dynamically, extending its initial design. this process makes it possible to attach properties, such as \"name,\" \"index,\" and \"amount,\" which may only be relevant and applicable within the context of a particular graph. when an entity instance is used to create new instances (such as a split that cuts one shape into multiple ones), the new ones are called its descendants, while the original entity is considered an ancestor. attributes are always copied from the ancestors to their descendants."
"(iv) split rules [cit] . a split is an augmented node, which allows a flexible definition of the split sizes and the introduction of a separate output port for each split. snap shapes can also be introduced dynamically through specific augmentation types (section 3.8)."
"the primary type of information manipulated within a graph, transported through the edges between ports, is organized in form of entities. as their name implies, they represent independent and self-contained objects, carrying their own specific semantics."
"the visual interface facilitates intuitive graph manipulation. for example, the insertion of nodes is done through a quick search window that lists all available procedures. once the node is added, edges can be drawn by dragging the mouse between two ports. since ports can be of different types, visual guides on ports (changing their color and size) indicate which connections are allowed."
"signature; that is, they offer a fixed set of available control parameters, attributes, and ports. yet, an operation such as the split requires the enumeration of several slices and the customization of each one. in cga shape grammars, each slice carries information about size, flexibility, and output rule symbol. only by having all such information at once can the procedure precalculate the remaining available splitting size and adjust each slice size accordingly."
"generally speaking, augmentations are useful to indicate lists of constraints or guidelines that a node should consider in international journal of computer games technology to be assembled. (vi) custom import/export. load/save custom attributes that may be associated to the geometric data from/to a database, for example, geospatial data."
"we introduce procedural content graphs, a generic graphoriented approach to specify content generation procedures. while still retaining the expressive power of grammar-based solutions (such as recursion or natural rule divergence), it extends them by addressing many of their shortcomings. in particular, we make the following contributions:"
"for instance, to produce and manipulate geometric 3d data, a boundary representation (b-rep) featuring polygonal faces, edges, and vertices is adequate for most modeling operations. yet a set of nodes for crossings, connected by edges to nodes for streets, is a far more concrete and appropriate data structure for representing street networks. the flexibility to introduce and manipulate any kind of data types means that specific procedures can be developed to optimally deal with them. for instance, by using \"terrain\" and \"street\" entities, one can apply algorithms, such as the ones described by kelly and mccabe [cit] or [cit], which operate over such specific data representations."
"having introduced the basic functional aspects of pcgrs, this section will focus on several content manipulation and integration possibilities that have hardly been addressed, if at all, in previous approaches."
"another alternative consists in isolating data using encapsulation so as to enforce execution scopes. the idea is that the operation over each lot is handled independently. for each input lot, the sequence of graphs, including collective nodes, is performed, meaning that the entity collection of the \"merge\" and \"amount filter\" operations are executed only within the scope of each individual building. this is guaranteed by the fact that encapsulated nodes, as any other procedures, work isolatedly on its executed queues, before allowing the supergraph to proceed with its execution."
"in order to employ the aforementioned attributes in a pcgr (see section 3.3), the user has to define all attributes that entities will carry inside that graph. the attribute declaration (where name, type, and default value are stated) defines a key that is used to access the corresponding value in the entity's attribute hash table. in practice, an explicit storage of the value is not required if the value matches the default value of the attribute. simply, if the search in the hash table fails, the default attribute value is used. in this way, all entities in the graph can be considered to be always carrying all defined attributes."
"encapsulation of graphs is also very much facilitated. to include a graph in a supergraph, the user simply has to drag the graph's file into the supergraph's editor canvas and the encapsulated node will be instantiated. the user can still set the subgraph's parameters and gates afterwards and the changes will be reflected in the supergraph immediately. conversely, a user can select a subset of nodes and edges and choose the option to encapsulate that selection into a new node file. referenced control parameters/attributes will automatically be built into graph parameters/attributes, while connected ports will automatically be converted to gates and added to the encapsulated node's signature."
"(ii) single nodes will run several times, each time popping one entity from each of their input ports and executing a round with those entities, until at least one input queue is empty. (iii) collective nodes will be executed once, since the list of entities will be emptied in one go. this might lead to some entities being left at single input ports. orange fruit tree generated using a cyclic pcgr. the node numbering indicates the topological ordering of the nodes and the ones marked with an asterisk are bound to be executed multiple times, as they are found within a loop."
"the sequence in which nodes should be executed is mainly determined by the topological order of the directed graph, in the sense that the sequence of nodes is based on their dependencies (see figure 3) . hence, the first nodes to be executed are those featuring no input ports (called source nodes). for the ordering of the other nodes, the order is topological, meaning that for every edge that connects an output of a node to an input of a node, should be executed before . after this order has been precalculated, the execution of the graph follows a datapresence protocol. a node can only be executed if each of its input ports has at least one entity in the respective input queue. the execution process runs as follows:"
"(ii) conditional rules [cit] . they are achieved through condition nodes, featuring one input, 2 outputs, and a boolean parameter/expression. if they are evaluated to be true, the entity received as input is sent to the first output port, otherwise, to the second."
"the ability to manipulate several types of entities lies on the existence of nodes capable of creating, analyzing, or transforming them. this possibility is reflected in the typing of node ports, which provides the means to understand the node's purpose and compatibility. different entities can coexist within the same graph and, in some cases, even share the same nodes, ports, or edges, if they share the same ancestor type. figure 10 portrays an example where several entity types are manipulated within the same graph. using a street network entity as the starting point, roads/sidewalks and lots were derived, as shape entities, using a \"street to shape\" node. the lots were developed into whole buildings and the light-source entities generated at the location of the lamps above each building's main door. as for the door numbering, individual textures were generated using the \"text on image\" node and then placed on a plate next to the door using the node \"set procedural material to shape,\" which combines texture and shape, as a custom mesh material. the result is a complete urban scenery, which would otherwise require multiple environments for development and whose integration would require additional manual or scripting efforts."
"high-level primitives are defined in a \"professional mode,\" which still requires editing a complex text-based grammar, but interactive customization and combination was possible within a \"high-level mode.\" interactive design frameworks have also been investigated to simplify grammar definitions [cit], but they were still tied to the rule-based structure of the underlying grammar."
"vehicle routing problems are categorized as np-hard problems in which exact solution methods, such as solving with lingo, are not practical in large scale due to high computational cost. for this kind of problems, using meta-heuristic algorithms are efficient. in this paper, a hybrid simulated annealing algorithm was proposed to solve vehicle routing problems. the total travel time was limited to a definite probability percent, and also, other constraints, such as capacity and time distribution restrictions, are considered while the total cost of the transportation was minimized. based on the obtained results, the proposed hsa can be used for obtaining high-quality solutions with a reasonably computational cost."
"one of the most important concepts of fuzzy sets is α-cut. given fuzzy set a defined on x and any number α 2 0; 1 ½, the α-cut (i.e."
"as shown in section 4, procedural content graphs can represent and manipulate content as lists or groups of entities, enabling operations like, for example, aggregation, sorting, and advanced filtering, which, to the best of our knowledge, are either not achievable or very contrived using a grammar approach."
"the basis of our approach is a procedural content graph (pcgr), where nodes and edges describe the procedures and the data flow, respectively. in this section, we will first give an overview of the graph architecture, the handled data, and the execution process. we will then discuss novel concepts such as the control over lists and augmentations and, finally, explain how we can manipulate attributes and encapsulate graphs."
"simulated annealing is the process of physical annealing with solids in which a crystalline solid is heated and then allowed to cool very slowly until it achieves its most regular possible crystal lattice configuration (i.e., its minimum lattice energy state) and, thus, is free of crystal defects. if the cooling schedule is sufficiently slow, the final configuration results in a solid with such superior structural integrity. simulated annealing establishes the connection between this type of thermo-dynamic behavior and the search for global minima for a discrete optimization problem. furthermore, it provides an algorithmic means for exploiting such a connection [cit] .the sa parameters are as follows:"
"t 0 : initial temperature. α: rate of the current temperature decrease (cooling schedule). x: a feasible solution. c(x): objective function value of solution x. l: counter for the number of accepted solutions in each temperature; k: counter for the number of consecutive temperature trails, where t k is equal to the temperature in iteration k."
"entity types can also derive from others, following an inheritance pattern, which is relevant during the creation of procedures that operate on entity supertypes and not just specific types. for instance, node ports of type \"entity\" can accept any type of entity data."
"we start by reviewing previous work (section 2) and then present our graph-based approach (section 3) by describing the specifics of entity representation and data flow management. next, we present its main content manipulation and integration capabilities (section 4), providing various examples that would have been cumbersome or impossible to express with grammar-based solutions. we continue with an explanation of our implementation (section 5), containing further details of the designed framework and achieved flexibility, after which we discuss how our solution compares to other grammar and graph-based approaches (section 6). finally, we conclude and give an outlook on future work (section 7)."
"that same rule structure is captured in the graph connectivity of a pcgr: entities (the nonterminal symbols) emanating from a given output port (the predecessor) flow through established edge connections to input ports of other nodes (the successor), thereby further transforming them. the possibility to create any number of connections between any pair of output-input ports of compatible types induces that the same possibilities for rule convergence, divergence, and recursion still apply. as for other grammar features, they are easily reproduced in a pcgr as follows:"
"in the future, we would like to perform a formal user study to quantitatively investigate user friendliness. so far, our experience is that even people unfamiliar with other procedural design methods are able to grasp its functioning and to rapidly build their first graphs."
"in order to design graphs with our approach, we have developed a visual interface (figure 11 ). the editor is plugin-based, providing the possibility to add custom file editors and docking windows. the main plugin for graph manipulation provides interactive tools for the creation and editing of graphs, as well as for the visualization of their output."
"design. the generation process should depend not only on the properties of each entity, but also on its context. in pcgrs, this is best addressed using several input ports, each carrying data according to a specific meaning. figure 1 portrays an environment built around this concept, where the level of detail of each building is determined by its distance to the main path (using a similar approach as section 4.2). on the other hand, the decision on whether to design each façade depends on its visibility from the same path, as façades that will not be seen are best left out to reduce the memory and rendering overhead. the operation facilitating this verification, also employed in figure 8, hereby simply named \"is oriented to,\" accepts a list of shapes and a list of streets and calculates, for a given angle tolerance, if the shape is visible from any of the streets, without being occluded by another shape entity. the result is assigned to an attribute, but that distinction could also be done using several outputs. one important consideration is that, by gathering all the shapes and streets at once, this node can organize and optimize the verification internally using spatial data structures, such as quad-and octrees."
"when compared to text-based alternatives such as grammars, pcgrs offer a visual solution that makes data flow easier to understand, follow, and manage, especially for complex constructions. that goal has been addressed before [cit] but inherited the limitations from the underlying shape grammars, such as the impossibility to aggregate and manipulate entity lists. the houdini-based approaches [cit] somehow address this issue as well but impose restrictions on recursion and encapsulation (section 3.7). in addition, they rely on labels to filter and control the flow in an implicit form (rather than an explicit representation via edge and port connectivity), which introduces a management issue shared by grammarbased systems. moreover, since symbols, like labels, are not always easily given semantically relevant names, it is hard to manage and maintain a project, especially as it gets larger. as to the implementation of our framework and visual editor, construct is a proof of concept, not meant to be compared directly to commercial tools such as cityengine [cit] or houdini [cit], certainly not regarding design features (such as the sophistication of the available procedures) or interface accessibility. however, our framework has been designed with extensibility in mind and users are able to easily expand it with their own components. in particular, the flexibility of supporting custom entities, augmentations, and encapsulations brings about the integration of all components in one unifying system."
"for the example in figure 7, exactly 10 blocks per cluster were picked to build green areas. the \"amount filter\" node introduces this possibility, as it gathers entities as a list and isolates the first entities, starting at a given index, and forwards them to its first output port, while the remaining ones are sent to the second port."
"the framework consists of a set of assemblies developed using c#. the core contains the graph representation; basic entities, attribute types and procedures; execution algorithms; and the means to load and integrate procedure libraries. the definition of different entities (geometric meshes, surfaces, streets, images, lights, etc.) is split into different libraries, each incorporating specific procedures for manipulation and interoperation. by using c#'s reflection abilities, both library data and documentation are automatically extracted, all with the purpose of facilitating the development of the system. currently, construct implements: procedures for common operations in shape grammars (e.g., extrusion, splitting, translation, and rotation); texture synthesis (e.g., invert, add, subtract, etc.); surface manipulation (e.g., noise, smoothing, and leveling); street generation (e.g., buffer, translation, texturing, etc.); and light manipulation (e.g., instantiation, transformation). more importantly, it also provides many operations focusing on list processing, context-based querying, and type integration, discussed in section 4."
"otherwise,t α is defined as the maximum time in each α-level of the satisfaction function. for all of the drivers, t α is supposed to be the same. it is ideal that the total service time for each vehicle to be less than t α ."
"the \"order by\" augmentation node can sort entities by given attribute combinations. when used in conjunction with \"amount filter,\" it can be used to filter the smallest façades (figure 8 ) or the closest blocks to the cluster center (used for green area determination in figure 7 ). likewise, obtaining the largest façades or most distance objects could be obtained through the \"reverse\" node that inverts the entity list order."
"(i) if is contained in the list of visited nodes, mark the edge as \"cyclic.\" (ii) otherwise, mark the edge as \"visited\" and execute recursively at (a) with and a copy of ."
"as illustrated in these tables, the results show that increasing the driver's satisfaction grows the routing cost, resulting in the increase of the total cost of a distributing company. for this reason, increasing the driver's satisfaction and decreasing the costs are considered as objective functions which are in conflict with each other. improving each objective leads to the deterioration of the other objective. thus, definition of an appropriate satisfaction level can be considered as a decision management. paying attention to pareto set and following its data with regard to management utility, appropriate solutions can be obtained."
"although at a first glance it might seem cumbersome to provide attribute definitions for an entire graph, there are several important advantages to it. first, the user keeps an overview of all attributes that entities carry, which is often convenient. second, it is easier to optimize memory usage, because most intermediate values that nodes could attach to entities might never be used afterwards. third, it releases the need for namespace handling, as attribute keys are unique and tied to the scopes of nodes and graphs."
"the generic way for pcgrs to deal with such flexible design annotation consists of augmentations, which can be seen as extensions to the nodes. an augmentation is a node structure that aggregates parameters, attributes, or ports. again, for the split node example, an augmentation is the structure that holds the information about the slice and contains the output port where to send the result. any number of such augmentations can be added to a node, and doing so affects the number of its output ports."
"grammar-based approaches [cit] have proven useful for the generation of several kinds of pattern structures, yet their formal, textual representation is generally inadequate for artists [cit] . a large variety of different rules have to be defined in order to achieve a fine-grained control and, for complex designs, even small changes may require redefining many grammar rules and writing new ones. such large rule sets also lead to reduced readability and manageability: it becomes hard to find meaningful rule names, rule sequencing becomes hard to maintain, and the data flow becomes hard to follow (see figure 2) ."
"by their nature, augmentations cannot be reduced to a simple sequence of nodes or encapsulation. this is easier to understand, for example, for operations that require some kind of precalculation (e.g., split), evaluation (e.g., condition), or any other \"atomic\" organization (e.g., grouping or sorting)."
"an entity can typically aggregate other entities. for instance, \"b-rep meshes\" contain vertices, edges, and faces, which can be separated and processed individually throughout the graph. likewise, meshes can also be aggregated within other meshes, recursively. this recursive construction allows us to group elements (e.g., a city entity can contain a list of region entities, which in turn contain houses, all of which are represented by mesh data)."
"the graph structure is meant to allow the sequencing of procedures, where cycles represent recursive operations. after a procedure is executed, its output data is transmitted via the output ports along the connected edges to the subsequent input ports, where the data is stored in a queue. procedures are executed in several rounds until their data queues are depleted."
"1. travel time of each route is a normal variable with the specific mean and specific variance. 2. the driver's utility function depends on the travel time, and it decreases linearly with increasing the travel time."
"since traveling time of the vehicle cannot be determined certainly due to various factors, such as traffic congestion, weather condition, vehicle failure and the like, this parameter is considered by a probability function. obviously, increasing the total service time leads to decreasing the driver's satisfaction. in this paper, the total cost function of the vehicles is minimized, while the driver's satisfaction is increased by decreasing the probability of the service time. this problem is considered as a graph by n nodes, which represent customers, and connecting arcs show the route between customers. each customer has a specific demand, and they should be served by only one vehicle. due to its successful result in meta-heuristics, the model is solved by a hybrid simulated annealing (hsa) algorithm [cit] . the proposed algorithm shows much higher efficiency than the lingo 8 software in large scale problems."
"there are 2 types of input ports: single input ports (represented with round outline) consume one data object per round. collective input ports (represented with square outline) will consume the whole object queue, handling it as a list. this introduces the possibility to treat sets of objects instead of individual ones. a node that features at least one collective input port is called a collective node, as opposed to a single node."
"ordering, reversing, and shuffling. the order in which entities flow throughout the graph may be determinant to achieve a specific result. the \"cluster\" procedure (figure 7), for instance, automatically picks the first entities as source for the initial cluster centroids. as such, shuffling the city blocks beforehand ensures some randomness in the clustering process."
extending the framework by developing whole new lowlevel nodes and entities requires some programming experience. we have made the framework available for a variety of projects and confirmed that it was easy for experienced programmers to add such new low-level procedures for very disparate purposes. we can therefore expect that the amount of available operations will further increase when construct is publicly released.
"graph-based representations [cit] facilitate the understanding of complex rule sets, but in the context of grammars, such solutions still have manageability or flexibility issues [cit] and inherited the grammar limitations mentioned above [cit] ."
"after these steps the process can proceed as before, with the single exception that, for the topological sorting, the edges marked as \"cyclic\" are not considered."
"(iii) scope rules [cit] . pushing and popping scope states are achievable through an attribute of type list/stack, which is written to and read from, just like any other attribute."
"shape grammar approaches use a top-down approach, transforming one simple entity into many complex ones, but there are many cases where joining back entities into single ones is not only convenient, but also necessary to achieve certain designs. in a pcgr, data is organized into entities that can be assembled or decomposed as manipulation needs arise. in this sense, the gather nodes are the key to the accumulation and congregation of entities. hereby we define grouping as the process of collapsing various entities into a single container entity, according to certain criteria, such as spatial distribution, matching properties, or common attributes. grouping works primarily as a means to organize and structure entities, building a hierarchy, but one which can be ungrouped at any point; hereby, the original entities are recoverable."
"by this approach, the vrp can be extended in order to consider the limitation of the specific travel time and the distribution time of vehicles."
"1) crossover operator: this operator randomly selects two different routes of two vehicles of the feasible solution. then two nodes of these routes are replaced with each other while considering vehicle capacity and stochastic transit times limit. 2) mutation operator: this operator randomly selects two routes of two vehicles of the feasible solution. then, one node is deleted from one route, and it is added to another route while considering vehicle capacity and stochastic transit times limit [cit], 2006b also, a schematic diagram of the two genetic operators used in the proposed algorithm is represented in figure 2 ."
"on a negative side, users commented on the difficulty to find low-level nodes that they needed among the list of existing ones, an issue we plan to address by introducing a categorization of the existing procedures in the future. as expected, many other pointed issues lied on pure guibased deficiencies, such as the lack of keyboard shortcuts, poor autocompletion for the expression editor, and the need for long mouse trips between graph canvas and inspector window. therefore, these features will have to be addressed before more formal and complete usability studies can be performed."
vehicle routing problems (vrps) have a significant role in logistics and distribution industries. vrps include problems in which a fleet of vehicles presents service from one or several depots to different geographically located customers so that the total cost of transportation is minimized. it has been proven that vehicle routing problems are categorized as non-deterministic polynomial (np)-hard problems [cit] and that exact solution methods are not practical in a large scale due to high computational cost since most of the procedures have focused on using heuristics and meta-heuristics.
"the initial steps in such grammars are typically topdown, sequentially dividing shapes entities to define local scopes. while each rule can produce multiple shapes, it can only operate on one individual shape at a time. this implies that, once split, separate entities cannot be merged back nor queried anymore as a whole set. this limits the expressiveness of the approach, that is, the range of ideas that can be communicated and represented, such as (i) clustering, for example, to assemble buildings into a certain number of neighborhoods featuring different architectural styles or purposes;"
"all users were given only a few example graphs to examine and had to figure out for themselves how the system worked. their responses led us to conclude that the simple graph topology, node nomenclature, and reduced graph size (achieved through successive encapsulation of meaningful operations) are very positive and valued factors. to all questioned users, there was a clear preference of such visual representation over the text-based ones. examples that show control and merging of lists with geometric entities were considered most exciting to shape grammar users. they understood and indicated that this new possibility would allow them to execute operations based on relationships between entities, considering this feature to be very useful."
"however, during the graph execution the edges are considered. nodes connected to an output port of another node are added all the same to even if the edge connecting them is cyclic. again, the addition to the queue should follow the topological order."
"the traveling time for vehicles is a probability variable depending on the weather conditions, accident, failure of the vehicle and condition of the route. before describing the problem, two definitions are presented based on the fuzzy theory [cit] ."
"we have introduced procedural content graphs (pcgrs), a generic approach for the specification of content generation procedures, which retains the advantages of grammar-based solutions and addresses most of their limitations. in particular, we demonstrated that a variety of list-based operations, as, for example, merging, ordering, aggregation, and clustering, strongly contribute to a richer and more expressive design specification than that offered by other grammar and graphbased approaches. this is also patent in the support of custom entity types, which contribute to a more intuitive description of the content generation procedures. moreover, the introduction of augmentations, encapsulation and attributes, significantly helps keeping content design and development compact and flexible."
"(ii) averaging, for example, to find the center location of a set of buildings to divide them into downtown and peripheral areas; our graph-based approach introduces context-awareness verifications which can be applied at any step of the generation process. here, we perform visibility calculations to determine which façades are visible (marked in green) from a highlighted street (marked in yellow) and which are not (marked in red). we also calculate the minimum distance of each house towards that same street and decide on a level of detail (1, 2, or 3, as marked on the roofs) based on the distance. for applications focused on a main path, for example, racing games, this information could be used to guide the procedural content generation and include budget considerations in the construction of the virtual world."
"content creation is one of the most expensive factors for many game productions. in particular, urban modeling is an important challenge, as it has applications in various areas from city planning, training, and learning, to simulation, and entertainment. unfortunately, creating large-scale urban areas by hand is a very complex task that quickly becomes unmanageable in cost and time. although procedural methods have received much attention in game development, automating urban modeling remains a very difficult process, as it concerns the creation and integration of terrain, vegetation, roads and complex buildings [cit], each involving particular representations and content types (meshes, lines, textures, lighting, etc.)."
"regarding execution, a subgraph will always be executed as long as possible and, only when its execution queues are empty, will the supergraph continue execution. as a result, this complete local execution of encapsulated graphs can also be helpful to control the execution order, in cases where this is needed. once the encapsulated graph finishes execution, all attributes that have been defined within this subgraph (but not those mapped to the supergraph) are removed from all entities before proceeding to the supergraph."
"also, distributing companies are interested in the service time of each vehicle not exceeding from a specific time limit. increasing of the service time leads to decreasing of the driver's working utility. it is supposed that the maximum time working utility of the drivers is achieved when service time is less than t l in bound of [0, t u ] and that increasing the service time affects linearly on the decreasing of the utility so that, for service time more than t u, the utility level achieves its minimum. also, a driver's utility function versus time is illustrated by a fuzzy number as shown in figure 1 . this function states that the driver's utility decreases linearly as traveling time increases."
"(v) occlusion query tests [cit] . an occlusion query node uses its gather port to receive all the shapes, organize them into an octree, and test for occlusions (within a certain distance, defined in the node parameters). the occluded and nonoccluded shapes are returned via different output ports."
"the use of microsoft xna as a rendering engine has restricted the current construct visual interface to a 32-bit process. as a result, memory allocation is limited, making the design of both very large and very detailed scenes impossible to sustain and visualize. we intend to address this issue in the future to better test the limits of our system."
"we let four users test our system and performed active demo sessions with another five users, most of them having had some contact or actual experience with cga shape grammars (through cityengine) or houdini. due to their experience with shape grammars and with graphbased design tools, these participants were swiftly able to build basic structures and had simple results within a very short time ( figure 13 )."
"in general, the execution of pcgrs is fast and on par with tools as houdini and cityengine, for the same degree of geometric complexity. nonetheless, we have not made an exhaustive performance comparison, as it would go beyond the scope of this paper. also, performance depends considerably on the type of processing algorithms, some of which are, to the best of our knowledge, actually unfeasible with other approaches, as stated in section 4. most important of all is the fact that the overhead time for node sequencing and data transport is negligible. this means that the graph representation and data flow execution algorithm itself does not constitute a performance issue."
"we distinguish several node types (to be explained in further sections), which use slightly different representations in our figures. the rectangle, rounded-corner rectangle, and 4 international journal of computer games technology octagon refer to standard, encapsulated (section 3.7), and augmentation nodes (section 3.8), respectively."
"we now turn to the issue of the generality of pcgr, explaining how existing grammar specifications can be equally represented using procedural content graphs, often with the advantage of clarity and conciseness."
"recursion. topological ordering is only possible when graphs are acyclic. this means that, for recursive graphs, cycles have to be identified and some edges are hidden from the topological sorting algorithm. the algorithm to find such edges proceeds as follows:"
"the configuration of a procedure's behavior towards the input entities is performed via its node parameters. these can be initialized in several ways: with so-called graph parameters, which are constant and reusable within the scope of the graph; with a fixed value (a numeric value, a character string); with an attribute of an input entity; or with a more complex expression involving arithmetic operations or function calls (e.g., sin( ), rand( ), length(string), etc.) on any of the former."
"a different interactive alternative is the exploration of shape variations, layouts, and components [cit] and processing [cit] . grammars can also be derived from existing models [cit] and then interactively controlled [cit] . however, these approaches have a different focus compared to our work; we provide a framework to guide and support content creation."
"(3) retrieve all entities from nonconnected output ports for storage or display. as mentioned before, entities can also be discarded from the final result by setting the state of their output ports to blocked."
"the need to isolate one entity from a set occurs often when a particular detail is to be introduced at a given point, for example, placing the main door at one of many candidate façades of a house. this is illustrated in figure 8, where a common rule has to be found to address the various possible building shapes. the main door should face the street, yet many options exist (red, yellow, and blue arrows). deciding upon the smallest façades (yellow and blue arrows) reduces the number of possibilities, but to ensure that only one door is created, the \"amount filter\" node is required, so as to pick the first element from the list."
"different types of data can flow and coexist within the same graph. their manipulation possibilities depend on the availability of procedures that handle such data. nodes have typed input/output ports, to which incoming/outgoing edges can connect as long as the connected ports are of compatible data types."
"we implemented the procedural content graph approach in a prototype system called construct, which consists of a framework and a visual editor. in this section, we provide details on its architecture, functionality, and possibilities."
"the editor also features a debugging window to display log information produced by the nodes. when in debug mode, it can also provide information about the flow sequence, procedure execution times, and the amount of produced data. the live-execution mode, on the other hand, will automatically reexecute the pcgr for every change, helping the user understand the impact of modifications on the graph output. since the whole graph is executed on a different thread, interaction remains smooth."
"for the various examples used here, the total generation times were at most a few seconds, mostly just a few milliseconds, on an intel core i7-2670qm, 2.2 ghz laptop, featuring 16 gb ram, nvidia geforce gtx 560m. consequently, interactive graph modifications are possible and the user receives quick feedback. for larger or more detailed examples, such as the one displayed in figures 6 and 12, encapsulation and parametrization significantly help reducing the generation scope (to focus on smaller number of blocks or buildings at a time instead of a vast area), therefore ensuring a smooth interactive experience."
"several edges can connect to one input port (a convergence port) and arriving elements are queued for execution. if several edges leave an output port (a divergence port), the outgoing data is copied. the result of executing a graph is obtained by collecting all the data emanating from all unconnected output ports or leaf ports. ports can be blocked (represented with dark fill) in order to discard its results from the content pool. blocking is also applicable to nonleaf ports or even edges, which can be useful to temporarily (for instance, for debugging) or conditionally prevent the flow at a particular location."
"at this stage, construct has already been used in numerous research projects on (or using) procedural content generation [cit] . besides the ease-of-use, researchers point out the great benefit of being able to easily define their custom procedures and interactive tools."
"(i) a collective management of entities as lists or groups, for example, for sorting, advanced filtering, and aggregation operations; (ii) a flexible and extensible framework featuring parameters, attributes, and so-called augmentations to create custom graph nodes with compact manipulation possibilities; (iii) a visual graph-based framework for procedural content generation, supporting a transparent data flow control to manipulate and combine different data entities within a single pipeline."
"spatial clustering is another form of grouping that joins elements according to spatial proximity, as illustrated in figure 7 . given lots of the peripheral area of the generated city, 6 random lots were selected as initial centroids and, using a -means algorithm, the remaining lots were sorted according to their proximity to these cluster centroids. the obtained clusters were then used to construct alternating industrial and residential neighborhoods, with green areas located around some cluster centers."
"to manage graph, node, and port definitions, a separate inspector window is available, showing details of the currently selected element. when a node is selected, the user can edit its parameters (which features a parser of simple mathematical expressions and function calls), attributes, and augmentations. when a port is selected, the user can change its state to \"blocked\" (see section 3.2) or to \"gate\" (see section 3.7). a separate option to edit graph details is also available, letting the user define the graph parameters and attributes, as well as some metadata (name, description, etc.)."
"(iii) ordering, for example, to create green areas on the blocks closest to a particular point of interest or location; (iv) finding maximum/minimum, for example, to create the building garage door on the largest façade and the main door on the smallest one; (v) achieving uniqueness, for example, to attach a chimney to only one of many candidate roof sections; (vi) merging, for example, to merge corner walls of adjacent façades to build continuous balconies; (vii) context development, for example, build the lot gate right in front of the building main door; (viii) visibility testing, for example, to determine if a building detail will be seen from a particular location or path (see figure 1 )."
"in the geometric domain, we additionally distinguish the concept of merging and unification. merging is the process of gathering all faces, edges, and points of different shapes into a single shape entity, while unification does the additional step of finding and connecting common faces, vertices, and edges. a useful application of these operations is portrayed in figure 6, featuring balconies stretching across corners, a recurring issue hardly achievable using shape grammars. the façades are split into a grid of separate tiles, each containing information about their -index within the grid of the respective façade. these indices are used to conditionally filter the leftmost and rightmost tile from each façade, for nonground floors. the \"group\" node then assembles all these tiles by floor. within each group, tiles are merged and unified if they have overlapping edges (using the \"adjacency merge\" node). having the corner faces together within the same shape, the extrusion for corner vertices can be done according to the sum of the faces' normal, creating the seamless result intended for such balconies."
"(1) mark all edges as \"unvisited\" and \"noncyclic.\" (2) for each graph node, perform a depth-first graph iteration starting with an empty list of visited nodes :"
"a very important flow control feature, unaddressed by shape grammars, is the ability to determine the amount of entities flowing through a particular point of the graph. this measure is especially important for filtering entities according to absolute or relative quantities. it is also required when a sequential numbering or alphabetization of entities is to be achieved."
"to enhance the efficient uploading in wmsns, many works have paid attention to techniques such as data aggregation [cit], compressive sensing [cit], information fusion [cit], network lifetime [cit], and energy efficiency [cit] . instead of uploading raw data, ch tries to reduce the gathered information used for uploading along with guaranteeing quality of service (qos). those methods can increase the energy-efficient and bandwidth-efficient utilization. nevertheless, a safe solution to satisfy all users is to upload all of the raw data regardless of whether or not the information is needed by the users. hence, in this paper, we propose a distributed systematic network coding (dsnc) scheme for multimedia content uploading over wmsns to increase the uploading efficiency in terms of reliability and bandwidth-efficient utilization with low decoding complexity. the uploading mechanism consists of two phases: (1) multiple ons encode original packets into systematic network coding (snc) packets, and transmit them to the dedicated ch using a proposed bandwidth-efficient and channel-aware error control algorithm in the first phase, where each node uses a different frequency [cit] ); (2) instead of conventionally forwarding all the received snc packets from multiple senders to the sink, the ch decodes the snc packets and encodes them into innovative snc packets in the second phase. particularly, our contributions are listed as follows:"
"information entropy is used to measure the uncertainty of orientation filed. a reference point is defined as the point that has maximum curvature in the most internal ridge, while orientation entropy in the local area measures directional difference in a local area. orientation entropy in the local area  is defined:"
"( var is greater than threshold value, the block is considered as foreground, otherwise it belongs to background. figure 1 show the segmented images based on mean and variance algorithm."
"as already explained, we consider two performance metrics: overload and energy consumption. we first assess the impact of a micro switch-on/switch-off decision on both metrics."
"this paper presented a framework for the activation/deactivation of nodes in a heterogeneous network, where the number of switch-ons in a day is constrained. we first analyzed the limits of both online-and offline-only algorithms, demonstrating how optimality requires unrealistic hypotheses. we then proposed and evaluated an onlineoffline method consisting of three phases: training, offline analysis, and online decision. the training phase is done offline and allows one to evaluate the energy consumption and saturation probabilities of alternative configurations, taking into account intercell interference at large. the offline analysis provides the optimal (constrained) switching pattern, and the online decision algorithm evaluates the current network load and makes switching decisions, erring on the safe side as far as saturation probability is concerned. our framework works with arbitrary load curves and power figures, and it takes a predefined maximum number of switchoffs as an input. simulation results show performance savings between 10 and 25% with both synthetic and real traffic loads and with both uniform and nonuniform user distributions. future work includes developing and analyzing switching algorithms that takes into account more detailed information on the spatial distribution of ue, as well as the possible benefits given by enhanced intercell interference coordination (eicic) techniques at the enbs. both frequency-based (e.g., coordinated scheduling or partial frequency reuse) and timebased (e.g., provisioning of absolute blank subframes) can easily be taken into account into our framework. all it takes is to modify expression (4), by correctly accounting the number of overlapping rbs between couples of enbs depending on the eicic technique. finally, minimizing the number of topology alterations (i.e., switch-ons and switch-offs) required to achieve a target power saving in a network is a related problem that we plan to investigate in the future. user equipment."
"the software framework described in this paper is meant to be run at a centralized element, which performs resource planning. the location and nature of this resource planner depend on the operator choices for ran deployment. in a distributed ran, it should be a monitoring server in the operator's network. in a centralized ran (c-ran), it would naturally be a process running in the same data center that hosts the virtualized baseband units (bbus) of the nodes in an area. a software system that supports a decision framework such as the one described in this paper is being designed and deployed in a testbed as part of the flex5gware eu 5g-ppp project [cit] ."
"the output of the oa is fed to the online decider that monitors the system utilization and takes decisions on a per-snapshot timescale. more specifically this module decides which micro to switch on/off, based on both current and historical information. it includes an activation and deactivation procedure, performed sequentially. figure 10 reports the flow diagrams for such procedures. each box represents a processing step, whereas each diamond represents a decision. deactivation takes as an input a list of micro nodes, ordered by increasing load. each of those micro nodes m is switched off if all the following conditions hold simultaneously at snapshot k:"
"in this section, we derive the closed-form equations for decoding probability of our proposed dsnc. this performance metric is widely used to evaluate the reliability in transmission. the decoding probability ρ s,u of each on at sink is defined as the ratio of the number of the successfully decoded packets d s,u at destination (i.e., sink) over k u source packets. the expected value of decoding probability is defined as follows:"
"the number of successfully decoded packets of the uth on at ch d ch the total number of successfully decoded packets at ch, note that"
"fingerprint matching is still a very difficult problem, which mainly due to the large variability and skin distortions. in this paper, the method of the reference point detecting of all classes is improved, and the method is free to resolution and rotation. then, to improve the matching accuracy, this paper represents a fusion matching scheme which is based on fuzzy criterion. the fusion scheme could adjust the similarity scores obtained using minutiae-based matching algorithms and correlationbased algorithms. as a result, our scheme can integrate decisions from different algorithm, and utilize the advantages of every algorithm."
"the offline analyzer takes as input the above information, plus the power models of the macros and micros and the constraint on the number of switch-ons, and finds the energy-optimal activation pattern for each hexagon, that is, the intervals where switching micros off allows the highest power saving, given that the requested load is satisfied. the above information, plus the current load at the hexagons, is used by the online decider to enforce switching decisions. hereafter, we present the three blocks separately."
"grows above th m before reaches th sat . in this case, we can exploit efficiently, energy-wise, even though the macro node is not in saturation yet."
"all the quantities defined above are computed for each snapshot, thus values at snapshot will be represented as ℎ ( ) and ℎ, ( )."
"now, we extend our analysis for the second transmission from ch to sink. the closed-form expectation of decoding probability of on u is presented as follows:"
"as a first step we analyze the performance of our system with various values of the threshold th, then we will investigate the effects of max . figure 16 shows the power variation in the central triple of hexagons. as we can see the latter is not affected by the value of th, regardless of the number of switches max . this happens because our algorithm tries to follow the suggested energy-optimal pattern, thus mitigating the effects of erroneous settings of th . figure 17 instead shows the power variation in site-a for various configurations of max . as we can see the highest savings are achieved at low loads, when micros are switched off most of the time. as the load increases, the impact of max increases as well, but not in a significant manner. similar things occur with site-b, as shown in figure 18, with an impact of max that is even lower. this effect is due to the \"smoothness\" of the load curves and to the uniform distribution of ue among the system. when comparing site-a and site-b loads, higher saving is obtained with the former, as it has lower load requests throughout the day, thus allowing for longer periods with the micros turned off. to better understand the effects of max, in figure 19 we report the cell load of ℎ 0 in site-a with high load. moreover in the lower part of the figure we show three binary curves, each one representing the optimal energy-activation pattern for the micros, for three values of max, in the shape of three binary curves indicating the values of ℎ . there are two saturation zones (circled peaks at the top of the figure). in these cases every micro must be turned on so as to provide additional capacity. there are two more zones (boxed areas) where the macro is not saturated, but the amount of requested resources is such that having the micros on is energetically profitable. in the remaining snapshots the best solution is having the micros off. the first two conditions (saturation) are satisfied with every value of max, while the last one is fully achieved only for max, when micros are switched only when necessary, regardless of the number of switches."
"as shown in figure 7, compared with another two algorithms, the proposed scheme is evaluated by the false match rate (fmr) and false non-match rate (fnmr). as is shown, the eer of the fusion scheme is better than the single method. experiment results prove the proposed scheme is effective for fingerprints identification."
"the average sinr of each ue is computed through (4) . note that there is a circular dependence between the sinr and the rbs allocated to a ue. when a ue is allocated some rbs (say, in hexagon e), in fact, δ, grows, thus increasing the interference suffered by ue attached to x and reducing their sinr. this in turn increases their rb demand and so on. this means that the average sinr must be computed iteratively, until convergence is reached. the algorithm that does this is shown in algorithm 1."
"the large loss of information and serious nonlinear deformation cause great difficulties in incomplete fingerprint recognition. the single matching algorithm is not suitable for all fingerprint situations. in this paper, a fusion method based on fuzzy criterion is proposed for incomplete fingerprint matching. firstly, according to the local and the global fingerprint information, the proposed scheme detects the reference point of a fingerprint based on orientation entropy and poincare index, and then extracts roi. secondly, this paper presents a fusion matching scheme which is based on fuzzy criterion. in the matching step, the proposed scheme provides the robust reference point for the minutiae-based algorithms. besides, roi extracting with poincare index and orientation entropy is used in correlation-based algorithms. then, the fusion scheme based on fuzzy criterion adjusts the similarity scores obtained using separate minutiae-based and correlation-based matching algorithms, and obtains the optimum estimation. experiment results prove the fusion matching scheme is robust for the incomplete and nonlinear distortions fingerprint."
"by the conventional approach, the orientation map is analyzed blockwised. in each block, a dominant orientation is estimated through gradients of every pixel. the equations [cit]"
"for the experimental results reported in this paper,  was set to 0.5. it is possible to vary  to assign different weights to the individual matchers 3. the proposed methods"
"where the left-hand side is the increase of power when a micro is switched on and allocates rbs, and the right-hand side is the reduction of power because the macro allocates fewer rbs. however, and depend on which kinds of ue will switch to the micro and what their sinr would be if they were associated with the micro. this is of course unknown, both because you do not know where this ue is located and because the micro is switched off, which creates a chickenand-egg problem."
"( ) is a binary variable that counts switch-on operations. the objective function to be minimized is the expected overall power consumed by a cell throughout the whole day. constraint (i) states that micros should be on when saturation may occur, (ii) forces ( ) to one when the micro is switched on at, (iii) takes care of the wrap-around at the end of the day, and finally (iv) constrains the number of switch-ons to max . the above is an integer-linear problem (ilp) with θ( ) constraints and variables and can be easily solved using commercial solvers such as cplex [cit] . figure 9 : example of energy-optimal pattern computation."
"minutiae-based algorithm is the most common and widely used technique, which aligns two sets of minutiae considering the directional image around the minutiae. minutiae-based matching essentially aims at finding the alignment between the template and the input minutiae feature sets and results in the maximum number of minutiae pairings. in the alignment, the proposed scheme provides the robust reference point. as the rotation and translation during fingerprint input, the minutiae descriptor is nearly invariant. however, it is complex and computational."
"the remainder of this paper is organized as follows. related work is described in section 2. in section 3, we present our proposed dsnc. furthermore, we derive the closed-form equations of decoding probability as well as performance evaluation regarding the reliability in terms of decoding probability, redundancy, and psnr compared to random linear network coding (rlnc) and snc in sections 4 and 5, respectively. finally, in section 6 we conclude this paper along with future work."
"where t and i are the two fingerprint images corresponding to the template and the input fingerprint, respectively. where () f is the fourier transform of an image, 1 () f  is the inverse fourier transform, spof means the symmetric phase only filter."
"in this section we analyze the performance of the system described in the previous section. we feed our simulator with both synthetic and realistic load curves, specifying for each snapshot and for each hexagon, the number of active types of ue together with their traffic request in mbps."
"we say that a hexagon, during snapshot, is overloaded in a scenario if it cannot serve its requested traffic. this means that all the resources of the macro (and possibly of the micros, if they are switched on) are depleted. our first concern is to ensure that the network is able to satisfy the requested load. therefore, we need to take note of overloaded hexagons. moreover, we need to assess whether a scenario is more energy-efficient than another, hence to compute the number of allocated rbs at all active nodes, which contribute to the variable part of the power. obviously, both the above depend on the way ue has been dropped. we recall that ue dropping is random, according to a preestablished distribution. therefore, the trainer repeats the simulation of the same scenario for times, dropping ue independently at every replica. this way, we obtain an overload probability ℎ for each hexagon ℎ in the considered scenario, given by"
"based on the above analysis, we extent to the case of our proposed dsnc with multiple source senders. instead of simply treating each data flow individually, ch encodes all successfully decoded packets from source senders (i.e., all ons) in the first phase into innovative snc packets and transmits them to the sink in the second phase."
"as already outlined, several schemes have been proposed in the past few years to deal with macro and micro node deactivation. surveys [cit] provide a general overview and taxonomy, besides the customary wealth of references."
"(i) association phase. for the first iterations (lines 6-7) ue is allowed to select the serving node, according to a best sinr policy (line 7). while doing so, the procedure also allocates rbs according to (1) (line 10)."
"most works in the existing literature usually make some assumptions which render their scheme impractical: the first one is that the number of per-node switch operations is unlimited. while we can expect future base stations to be able to tolerate frequent power transitions, most of the ones that are currently operational have not been designed with this characteristic [cit] . this means that frequent switches will decrease their energy efficiency and possibly decrease their mean time between failures (mtbf), something that has a significant impact on opex. this calls for algorithms that can maximize the power saved by the network while limiting the number of power transitions for the node to a predefined maximum. another frequent assumption is to rely on a particular shape of the load curves (e.g., one with a clearly distinguishable peak-hour and off-peak valley). while this allows one to build an optimal algorithm, the same algorithm would make suboptimal decisions when the shape of the load curve is different. several algorithms assume clairvoyance: for instance, they assume that they know the exact amount of resources that a micro, currently switched off, will use to serve a given input traffic. this cannot be known in advance in practice and depends on the user positions and their perceived interference. finally, they often make local decisions neglecting their global effects: as anticipated, switching off a micro (based on local knowledge, e.g., few users connected to it) will save the network owner the power to operate it; however, the extra resources required to serve the same users at the macro will foster a higher interinterference in the surrounding area, whose net effect may well be to increase the power consumption. therefore, the global effects of switching decisions should be considered."
"thus, the percentage of rbs where will interfere is δ, /, whereas the remaining 1 − δ, / will be exempt from interference from . call, the power received by from node (which depends on the distance and angle between them, the propagation model and the transmitting power of ). then, the average sinr of ue in the snapshot is"
"using the solution described above, we can react to overload conditions at the macro node and to inefficient utilization of micro ones, by just looking at the instantaneous allocation of rbs. two considerations are in order."
"we derive the closed-form equations of decoding probability that are validated by various simulations. we evaluate the effectiveness in terms of three performance metrics: decoding probability, redundancy, and image quality measurement using peak signal-to-noise ratio (psnr)."
"correlation-based algorithm, usually, may be susceptible to the quality of image and nonlinear distortion. the fingerprint distortion and the noise problems are usually addressed by computing the correlation locally instead of globally. the correlation theory is described as follows:"
"in this paper, we design and evaluate an offline-online framework for heterogeneous cellular networks, whose job is to satisfy a given load demand with the minimum possible energy cost. our framework exploits offline information, that is, load curves, to select which configurations are feasible (i.e., able to carry the nominal load) at a given point in time. an optimization algorithm is then run on the set of feasible configurations, to decide the optimal switch-on points for the micro nodes, constrained to a maximum number of switchons in a day. to the best of our knowledge, ours is the only work that takes the latter aspect into account. the only work we know of that considers the number of switch-ons, that is, [cit], seeking to minimize the latter rather than taking it as a constraint. the online part of the framework, instead, measures the current load and enforces switch-on/off of micro nodes so as to satisfy the demand in an energy-efficient way. more specifically, it may bring forward or postpone switching instants with respect to the optimal ones computed offline, when it is more energetically efficient to do so. the scheme described in this paper is designed so as to require few hypotheses: it can work with load curves of arbitrary shape, it does not assume clairvoyance, and it takes into account global effects of switching decisions through an accurate estimation of intercell interference. we evaluate our scheme using both synthetic and real-life load curves taken from operator data. our results show that power savings up to 25% are achievable over a whole day and that neglecting intercell interference leads to overoptimistic results. moreover, we show that our scheme is robust against variation in the expected load."
"8 and 9 . note that, in the absence of switch-on constraints, the micro would be switched off at 6 and on again at 8 ."
"note that, at the first iteration of the association phase, no rbs have yet been allocated at any node, hence the interference is null and the association is distance-based rather than sinr-based. after the first iteration the interference is updated, hence the nearest node may not be the one with the best sinr anymore. this is the reason why the association phase is repeated times. however, has to be limited; otherwise some ue will end up oscillating indefinitely between two or more nodes. figure 8 shows how the sinr decreases with the iterations, quickly converging to a stable value."
"as is depicted above, the advantages and disadvantages of two algorithms are different, so we fuse the matching results from two algorithms. then fusion scheme with fuzzy criterion is as follows:"
"wireless multimedia sensor networks (wmsns) have attracted many researchers due to potential applications. apart from environmental monitoring, wmsns can enable some new applications such as object recognition, tracking, multimedia surveillance, automated assistance for elderly and family monitoring, and industrial process control, etc. [cit] . as with sensor networks, wmsns are composed of wirelessly connected devices that can collect information from environments at any time. however, the types of collected information are video, audio streams, images, and scalar sensor data. to enable the above practical applications, wmsns require sensors to upload collected information to users or applications. in general, the multimedia sensors are densely distributed and divided into several clusters. each of them consists of a single leader (i.e., cluster head (ch) node) and several ordinary nodes (ons). a cluster head operates as a relay between its members (i.e., ons) and base station (i.e., sink in wmsns) [cit] . the operation of data collection consists of two phases: first, each on uploads its data packets to the corresponding ch, and second, the ch transmits the gathered data to the sink."
"moreover, we obtain a vector of average allocated rbs, whose elements are the rbs allocated at each node in the hexagon, averaged over the n replicas:"
"where is the identifier of the node within hexagon ℎ. the above two quantities can be used to compare scenarios, specifically to identify the minimum-power one having the overload probabilities below a given threshold. unfortunately, an exhaustive comparison of scenarios is impossible. this would in fact require simulating as many scenarios as the powerset of the set of micros. in a network with few tens hexagons, micros are easily in the hundreds, which makes the task impossible. we therefore settle for a simple tradeoff, which proves to be effective nonetheless. we simulate only the two limit scenarios with all micros switched on and switched off. we denote these two scenarios using subscripts ↑ and ↓. the rationale beyond this choice is that the two configurations are expected to achieve, respectively, (i) the minimum overload probability (which should be null, unless the network is underprovisioned), but a high power consumption; (ii) a larger overload probability, with a considerably smaller power consumption."
"nevertheless, this method cannot guarantee the predefined level of qos in wmsns as the channel condition is getting worse (i.e., proved in section 5.2). to enhance bandwidth-efficient utilization under a predefined level of qos, we propose a bandwidth-efficient and channel-aware error control algorithm presented in algorithm 1 to select the optimal number of redundant packets. our proposed algorithm 1 is based on our analysis of decoding probability presented in section 4. firstly, we formulate the redundancy assignment problem as follows:"
"the paper is organized as follows. in section 2, some related works are presented. in section 3, our method is described in detail. section 4 provides the experimental results to analyze the performance. and the conclusion is in section 5."
"in this section, we will analyze the performance of our algorithm in case of nonuniform and time-varying distribution of ue within the system. for each hexagon, users are randomly dropped inside one hotspot of center c and radius r, as presented in figure 7 (a). value c is changed every five snapshots, alternating between two positions, closer to the macro (ℎ 1 ) and to the micro (ℎ 2 ), respectively. the value of r is instead picked randomly in the interval [20, 30 ], every snapshot."
"the dsnc can increase efficiently the multimedia uploading because of two advantages: (1) in the access link side (i.e., in the first phase from multiple ons to ch): on can upload a fraction of data even if the measured rssi used for physical uplink scheduling is not accurate. this issue causes transmission error and often happens in practical wireless networks due to measurement error and the mobility of ons [cit] . on the other hand, on can upload all data easily without using feedback and retransmission since ch can decode all transmitted packets as receiving a sufficient number of snc packets; (2) in the backhaul link side (i.e., in the second phase from ch to sink): the reliability performance can be achieved by the substantial improvement since the generation size used for rlnc encoding increases leading to the increase in reliability [cit] . as seen, the reliability improvement comes from the cost of computational complexity. however, this issue can be easily solved by the powerful computational capability of the current smart sink [cit] ."
"the aim of our framework is to plan and enforce switching operations at each snapshot for each micro node, so that (1) user requests are satisfied, if it is possible to do so."
"(2) as little energy as possible is consumed by the network, provided that ue requirement is met; (3) micro nodes are switched on up to max times per day."
"the simulation is done as follows: users are dropped in the hexagon according to the spatial distribution. figure 7 shows a uniform (a) and hotspot-based (b) spatial distribution. in the latter, ue is dropped in the portion of hexagon that intersects a circle of center c and radius r. note that spatial distributions are used only by the trainer, while the online decider totally does without them. at a snapshot boundary, ue is created or deleted to match the required number, and the position of the remaining ones is randomly changed within a configured area. this allows us to simulate random mobility within the cell or parts thereof."
"for performance evaluation, we performed various simulations that firstly validate our theoretical analysis. the analytical and simulation results matched well with a difference of 0.05%. secondly, we present the comparative performance analysis with the existing uploading schemes. each simulation was run 100,000 times in the matlab environment with a confidence interval of at least 95%. galois field size of 8 is selected to guarantee linear independence with a very high probability at application layer [cit] . there is no standard method for selecting a field size, but there are majority of works that consider the galois field size of 8 [cit] . table 2 describes the main simulation parameters."
". as the rotation and translation during fingerprint input, the minutiae descriptor is nearly invariant. hence, it can characterize the minutiae location and solve the problem of the fingerprint distortion on the acquisition sensor. the similarity function is defined as follows [cit] :"
"the total number of received rlnc packets at sink ρ s,u decoding probability of the uth on at sink ρ u,th decoding probability threshold of the uth on e(ρ s,u )"
"lte power saving schemes exploit the fact that cellular coverage is overlapping, due to both reliability and (mostly) performance design issues and that traffic load is highly variable. coverage is designed to carry peak-hour loads, which normally occur during working hours in business areas, while nodes use very few resources during (long) off-peak periods, for example, at night. therefore, some nodes can switch off during off-peak hours, and nearby nodes will increase their transmission power accordingly to guarantee coverage. when the load is high, the first node will be reactivated and the second one will revert to normal radius. a similar concept can be applied to heterogeneous cellular network deployments, where a macro node presides over a relatively large area and micro nodes provide additional localized capacity to cover hotspots within the former. micronodes can be switched on to offload the macro node (which is supposed to remain always on, lest coverage holes appear) when necessary or profitable from an energy efficiency point of view."
"correlation-based algorithm is based on the global patterns of ridges and valleys to determine whether the ridges are aligned. moreover, it requires less computational complexity than minutia-based methods, but vulnerable to the quality of image and non-linear distortion. in our scheme, roi extracted with poincare index and orientation entropy is free to resolution and rotation."
"in the context of homogeneous networks, many works advocate cell breathing (e.g., [cit] ): the radius of some cells is shrunk during off-peak hours, and nearby cells increase theirs accordingly to guarantee coverage. if the scenario allows it, a node can be completely switched off, which increases the saving. cell wilting and blossoming are procedures for graceful shut-down and power-up of nodes, whose aim is to avoid massive handovers during topology changes [cit] ."
"finally, we observe that the problem of energy-efficient resource allocation in lte base stations can be tackled at different timescales. besides the resource planning timescale (i.e., hours or days), which is the one of all the above works, we can also work at the frame timescale, that is, few tens of ms, for example, by scheduling mbsfn subframes in a frame [cit] . during off-peak periods, even short ones, the enb pools the few rbs that it has to transmit into as few subframes as possible, leaving the other ones blank, thus trading a modest reduction of responsiveness for a remarkable power saving. these solutions are complementary, rather than alternative, to the ones dealt with in this paper."
"we propose a bandwidth-efficient and channel-aware error control algorithm to enhance the bandwidth-efficient utilization. our proposed dsnc can be simply embedded on application layer. in the practical point of view, dsnc can take a significant step towards realistic deployment integrated into wmsns."
"as far as switching is concerned, as already specified, macro nodes cannot be powered off, because this would create holes in the coverage. micro nodes, instead, can be powered off, for example, during low-load periods. however, switch-on/off operations should be few, for several reasons: first, especially with older equipment, it takes a considerable amount of time before a micro node is fully operational (in the order of tens of seconds or minutes), due to the operating system boot procedure, control and management plane setup operations (e.g., path setup in the evolved packet core network), and so on, and it takes some time to shut it down as well. second, topology alterations often have unpredictable ripples in a multicell network: a switch-off may cause massive handovers (even when cell wilting is used, [cit] ), possibly overloading the corresponding macro and affecting nearby macros through a change in the interference pattern. third, switching operations increase the wear and tear of equipment, hence reducing its mtbf [cit] . for this reason, we assume that an upper bound of 2 ⋅ max switching operations must be enforced on all micro nodes. in other words, micro nodes can be powered up in at most max disjoint time intervals within a day."
"our framework obtains historical information from the network, processes it offline, and makes switching decisions online, based on the current load and the historical knowledge. the framework and its information exchange with the network nodes are shown in figure 6 . the offline part of the framework is composed of a trainer and an offline analyzer. the trainer takes as input the historical load curves (detailing the requested bitrate and number of users per hexagon) and simulates what happens at each snapshot when micros are switched on and off, respectively. the outputs of this simulation are, for each snapshot, an overload probability, that is, the probability that a given hexagon is unable to carry the requested load when the micros are switched off, and a resource consumption, that is, the number of rbs allocated within a hexagon when the micros are turned on and off, respectively (note that the trainer could also be realized in different way, which may not involve simulation at all, e.g., as a machine learning system trained with a large set of measurements from the network in various configurations. as long as the trainer is able to compute overload probabilities and resource consumption, the rest of our framework will be able to work with it)."
"in both cases, at the end of an iteration the interference is updated (line 13) through (3). if any δ, changes significantly from one iteration to the next, the interfchanges flag is set, to signal that the convergence has not been reached yet. both prx and, from (4) are computed using the itu pathloss and angular attenuation models [cit] ."
"switch-off schemes-for both macro nodes, in homogeneous settings, and micronodes in heterogeneous settingscan be either offline or online [cit] . offline schemes rely on knowing the evolutions of load in time (the so-called load curves, e.g., [cit] ) and decide the optimal switch-on/switch-off intervals based on the shape of these curves. they are often 2 mobile information systems used for resource provisioning purposes and, being based on load predictions, which may always prove wrong, need an online counterpart to react to unforeseen circumstances (flash crowds, accidents, etc.)."
"firstly, we remove background region and get a region mask of foreground region. mean and variance based algorithm can significantly reduce the number of basic image entities, and due to the good discontinuity preserving filtering characteristic, the salient features of the overall image are retained [cit] . moreover, a novel image segmentation method is devised and some new features are proposed to represent the local ridge structure of a fingerprint [cit] . but mean and variance based algorithm does not work well on too wet or too dry fingerprint images. steps for mean and variance based algorithm are summarized as follows:"
"all the nodes share the same spectrum and hence interfere with each other. we assume no coordination, either among or within hexagons. a node that has to allocate rbs to its users will pick them at random from a vector of rbs and schedule each ue on each rb with the same probability. cell coordination and coordinated multipoint (comp) techniques are a promising avenue of research to cope with interference and have been shown to increase the power saving opportunities [cit] . however, they work at a significantly faster pace than algorithms for node switching and are thus orthogonal to them."
"we assume that the operator possesses the following information: a per hexagon historical load curve, detailing (at least) the overall bitrate requested in a hexagon over time in a sample day. there may be several such curves, of course, for a single hexagon, depending on the context. for instance, working days and holidays have different load curves (see, e.g., [cit] ), and other human activities do affect the context: the typical example is an hexagon covering a major sports venue, whose load curve on the game days (e.g., sundays) depends on whether the local sports team plays home or away. moreover, the operator possesses a similar curve (or set thereof) detailing the number of users per cell over time, so that the average per-user bitrate can be inferred (a possible method to obtain such information is using monitoring systems that measure the values of specific kpis (e.g., number of types of ue) with a fixed time resolution [cit] . these measurements are repeated in different days with the same operating conditions (e.g., working days) in order to produce a daily profile of the average values for the given kpi.). note that these curves do not contain information about the position of single users within a hexagon. the latter may be known in a probabilistic sense, for example, uniform within a hexagon. we make no assumption on the shape of the load curves, and-in particular-we do not rely on their having a single peak during business hours. the time resolution at which these curves are plotted is that of a sufficiently large interval (e.g., 15 minutes), called a snapshot henceforth. we denote with the number of such snapshots in a day. furthermore, we assume that we can retrieve the current amount of allocated rbs at each macro/micro node, averaged over a snapshot."
the proposed dsnc enables to enhance the efficient multimedia content uploading and to solve the issue of heavy feedback signaling and retransmission caused by retransmission-based protocols in wmsns.
"the poincare index method takes the values 1/2, −1/2, and 0 for a core point, a delta point, and an ordinary point, respectively. the operation is carried on block-wise. for each block centered at pixel (, ) i j in the orientation field, the cumulative change of orientation (, ) pi i j can be computed along an enclosed curve (contains n p points) using equation 9 . if the value of (, ) pi i j is equal to 1/2, the pixel (, ) i j is considered as a core point. usually if there is more than one core in a very small region, an average core point can be computed instead. the poincare index method may induce the displacement of the core point because of smoothing the estimated orientation field. to resolve the problem, we extend the area centered at the average core point based on the curvature information adaptively. and the center of each block contained in the extended area is considered as a core point. therefore, a reference point set s 1 could be gotten. figure 4 is just an example of the extension. this will have a detailed description below."
"fingerprint-based identification is the most widely used biometric authentication technology (e.g., face, fingerprint, hand geometry, iris, retina, signature, voice print, hand vein, gait, ear, odor, keystroke dynamics, etc. [cit] ), because of the uniqueness, immutability and portability of fingerprints. however, the acquired fingerprints often have dirty parts, scars, creases and so on. for incomplete fingerprint, the loss of information and serious nonlinear deformation cause great difficulties in incomplete fingerprint recognition. so it is imperative to do some researches on this project [cit] ."
",ℎ ↑, and rb,ℎ ↓, and the node power models are then passed to a module called offline analyzer (oa), whose purpose is to process this information and to generate guidelines for online decisions. the oa makes independent decisions for each hexagon. it compares ℎ ( ) to a predefined overload probability threshold (which may also depend on the snapshot and hexagon) and computes a binary value ℎ ( ) that marks whether the overload probability is above or below the threshold. first it computes the expected power consumption of each of hexagons ℎ ↑ ( ) and ℎ ↓ ( ) in the two configurations as"
"figure 3 presents a proposed on's architecture that encodes multimedia content for uploading to ch in the first phase. let f ile u be the multimedia content (i.e., application data units (adus)) of the uth on, where each file is partitioned into multiple packets and delivered to the snc encoder. snc integrates snc encoder/decoder on application layer. based on the transmission requirements corresponding to rssi feedback from physical layer (phy), the snc encoder divides the adu into k u equal-length source packets which can be presented as a vector of k u elements:"
"we now analyze the behavior of the system with realistic traffic curves with 57 hexagons, each one composed of one macro and two micro nodes. each set of three load curves is applied to the central triplet of hexagons and is then replied in the rest of the system using the same pattern, as represented in figure 15 . with reference to the latter, all the hexagons of the same color will request the same data rate. however, ue is dropped randomly within each hexagon, thus hexagons will differ-possibly in a significant manner-in terms of requested resources (which also depend on the ue sinr). this approach allows us to have diverse levels of request resources throughout the system, while still using realistic values of requested data rates."
"among the above works, those advocating offline schemes often make assumptions on the shape of load curves, meaning that they only work when the load curves match those envisaged. our work does not make such an assumption, and can work with any shape of load curves. moreover, these works seldom consider intercell interference, which-as shown in the previous section-clearly influences the switching decisions. none of the above works considers (or may easily incorporate) constraining the number of switching decisions at a node, something which-as we will demonstrate-changes the picture completely."
"in this section, we will test how our solution reacts to variation in the expected load. more in detail, we want to check if the online decider is able to recover from wrong suggestions coming from the offline analyzer or to react to unexpected changes in the actual load of the system."
"the trainer considers one snapshot at a time. in a snapshot, it simulates the network with the given number of users per hexagon, their spatial distribution, and requested bitrate, in a scenario. a scenario is defined by the number of nodes that is switched on: for instance, a micro-off scenario is one where all micros are switched off, and a micro-on scenario is the one where all micros are switched on."
"similarly, it is unrealistic to assume that we can infer when it is energetically efficient to switch a micro off based on online data only. in fact, this would require knowing the sinr of each ue to the macro after the micro has been switched off. the latter is not equal to the one measured before the switchoff, due to the mutated intra-and interhexagon interference."
"regarding the processing cost, our measurements show that a single-day run (i.e., 96 snapshots) of the trainer occupies approximately 1.5 minutes of a single core on a desktop pc. solving the optimization problem at optimality takes less than one second and hence is negligible. the number of runs should be calibrated based on the required overload probability threshold: more runs allow one to estimate small probabilities with greater accuracy. now, the planning normally occurs on a daily basis, and a single core can run the trainer up to ∼950 times per day. this means that our framework can be made to work with arbitrary accuracy on off-the-shelf hardware in a day. more to the point, this very fact can also be exploited to compensate for unforeseen context mismatches. suppose that an unforeseen event occurs, such as a large traffic jam, which predictably changes the context for the rest of the day (e.g., by postponing by 1-2 snapshots the onset of the daily peak in all the cells of a business district). one can then run the offline trainer again, starting from the current time up to the end of the day, discounting the switching operations that have already occurred from max, and come up with an optimized planning for the rest of the day in a reasonable time, still executing tens of training runs on a multicore machine within a snapshot's worth of computation time."
"the above two observations can only be made with hindsight, hence are precluded to an online algorithm. however, we can again exploit historical knowledge, harvested from the daily load curves, to endow an online algorithm with prediction capabilities and increase its efficiency."
"predefine a threshold value t, for each block, if the block's orientation entropy is larger than t, the center of this block is decided as a possible reference point (shown as figure 5 ) and another reference point set s 2 could be obtained."
"note that each node may have its own power model parameters, thus accounting, for example, for different versions of the apparatus and different cost of energy. the same applies to the switching-off thresholds. however, for the sake of readability, we will assume that all the power models are the same and drop the superscript whenever this does not generate ambiguity. for instance, we will use th to denote the switch-off threshold."
"although rlnc can increase the reliability without using feedback and retransmission but there are two significant drawbacks: rank deficiency and high decoding complexity that are not suitable for multimedia content because of strict time constraint. to tackle these drawbacks, snc was proposed [cit] . the transmission of snc consists of two stages: a sender sends original packets in the first stage; upon sending out all original packets, the sender continues generating rlnc packets for transmission in the second stage. obviously, the issue of rank deficiency can be solved since the receiver can still decode some original packets even though the receiver cannot receive a sufficient number of linearly independent packets. on the other hand, snc can achieve the same performance as the receiver receives a sufficient number of transmitted packets [cit] . in addition, the second drawback (i.e., high decoding complexity) can be solved by brief propagation (bp) and gaussian elimination (ge) algorithms that are used for decoding with low computational complexity compared to rlnc [cit] . however, the mentioned studies only deployed the rlnc and snc for one-hop data uploading. our work is the first work investigating the dsnc to enhance the efficient multimedia content uploading over wmsns. furthermore, we derive a closed-form equation of decoding probability which provides a theoretical analysis for further understanding of the dsnc."
expected decoding probability of the uth on at sink u erasure probability of access link from the uth on to ch erasure probability of the backhaul link from ch to sink
"we proposed a distributed systematic network coding called dsnc for multimedia content uploading over wmsns. in addition, we derived the closed-form equation for decoding probability analysis. based on the analysis, we proposed a bandwidth-efficient and channel-aware error control algorithm to enhance the bandwidth-efficient utilization by dynamically determining the optimal number of innovative coded packets. the experiment results verified our mathematical equations and demonstrated that the proposed distributed systematic network coding outperformed the random linear network coding-based scheme and systematic network coding-based scheme in terms of decoding probability, redundancy, and image quality measurement. for future research, we extend our work to the vehicular sensor networks for multimedia collection and dissemination, in which the mobility challenge is taken under consideration."
"nevertheless, we can figure out an approximate threshold th by simulating the network offline using load curves and space distributions. thus, the insight gathered using offline simulations can surrogate the lack of sinr-omniscience required in an online-only approached."
"the rest of the paper is organized as follows: in section 2 we describe our assumptions and state our problem formally. the related work is discussed in detail in section 3. section 4 describes the proposed framework, which is evaluated in section 5. finally, section 6 concludes the paper."
"the orientation of the ridge flow changes abruptly in the singular region. therefore, reference points could be located at the trip points of the orientation field (see as figure 3 ). and extraction is preformed in the edge of orientation jumping regions instead of the whole orientation field. here, the orientation field is considered as an intensity image and log operator is used to detect the edge of the orientation jumping regions."
"since load curves represent what happens in a snapshot, whereas power is consumed by allocating rbs at each tti, a method to infer the latter from a snapshot's load value is required. rbs are computed as follows: first we infer a peruser requested data rate from the load curves, then we show how to compute the number of rbs needed to meet that data rate given the ue's sinr, and then we show how to compute the sinr of a ue undergoing interference using a statistical interference model."
"in lte, cell transmissions are arranged in time slots called transmission time intervals, (ttis), whose duration is 1 ms. in a tti, the node allocates frames, that is, vectors of (virtual) resource blocks (rbs) to its associated user equipment (ue) [cit] . ue is associated with one node at a time. each rb carries a fixed number of symbols, which translate to different amounts of bits depending on the modulation and coding scheme (mcs) used by the antenna on that rb. in general, more information-dense modulations (e.g., 64qam, yielding 6 bits per symbol) are favored when a better channel to the ue is perceived (i.e., one with less interference). the quality of the wireless channel varies over both time and frequency. for this reason, ue reports their perceived downlink channel state to the enodeb as a channel quality indicator (cqi), computed according to the measured signal to interference and noise ratio (sinr)."
"author contributions: p.c. proposed the dsnc scheme, mainly wrote the manuscript, and implemented the proposed scheme along with the compared schemes and ran the simulations. j.s. designed the performance evaluation method and improved the manuscript with performance analysis. j.(p.)j. verified the correctness of the proposed dsnc scheme and improved the manuscript technically. all the authors have spent so much effort on the improvement of the manuscript. they have reviewed with many good advices and approved the final manuscript."
"online algorithms look at the current load and decide when and where it is high enough to warrant a switch-on (or low enough for a switch-off) of some nodes. online schemes normally work at coarse timescales, for example, 15-minute slots, because switching operations are not instantaneous (in fact, they may take up to minutes [cit] ). moreover, the ensuing topology alterations may trigger avalanche effects, such as massive handovers, or cause interference ripples in neighboring cells, which are difficult to predict and manage. for instance, switching off a micro node at location x may indeed increase the load in nearby areas, because the macro node at location x takes over its load and hence generates more interference around itself."
the procedure is a cycle repeated for up to it max iterations or until convergence is reached. each iteration cycles through every ue. we distinguish two phases.
"our framework plans switching operations in advance (i.e., offline), based on historical load curves, so as to keep the macro overload probability below a predetermined threshold, and enforces switching operations online either to avoid overload or when it is energetically efficient to do so, by comparing the current load conditions to the expected (historical) ones."
"we first explain some of the practical challenges for a switching algorithm, which also sets the rationale behind our framework. then we describe the framework at a high level and each of its components in detail."
"the communication overhead is also small to negligible. a vector of 96 binaries must be communicated daily (or on demand, in the above cases) to the macros, and the macros themselves can issue the switch-on/off commands to their micros according to the daily plan. communications among macro and micro nodes belonging to the same hexagon can be realized exploiting the x2 communication and in particular the x2 application protocol (x2ap) [cit] . the latter defines a set of standard messages that enbs can exchange. for example, an overload condition can be signaled using the load indication information element, and an enb can be switched on using the cell activation procedure."
"we assume that each resource can be rented as a spot instance or as an on-demand instance, or as both, depending on the situation. for example, if a resource is running as a spot resource and the bid price is overbid, such resource will be eventually lost (after a termination notice grace period) and replaced with an on-demand resource until the spot price becomes lower than the bid price. in our model the transition between spot and on-demand happens according to the overbidtime(b) and underbidtime(b) functions, and some additional fixed parameters that depend on the type of resource rented. these additional parameters are explained in table 1 ."
where: -r id is relevance associated to question id (subgoal for goal 1); -q g is the set of questions (sub-goals for goal 1) related to goal g. -m(q) is the aggregation function of the metrics of question q:
"we begin by considering a model for the system under consideration. the system model we propose is composed of the following two parts: application and resources. our goal is to determine the rental and allocation policies, which consist in the amount of computational resources to be rented from a cloud provider, the mapping of the various application components to these resources, and finally the bid price for each resource."
in the previous sections we have seen that the main idea of this paper is to combine cost-aware cloud resources provisioning and application mapping into a synergistic autonomic solution that takes into account performance requirements and environmental random factors such as the prices fluctuation of cloud resources and user load.
"a specific type of representation that is necessary to mention and describe here is personification, identified in vir by the identifier ic11. the class comprises anthropomorphic figures, which symbolise and represent abstract ideas. widely used within the arts, personification appears in both byzantine and western traditions and is considered a typical communicative device to represent intangible concepts such as fortune, fate, prudence and other allegories. another typical use of personification that still survives today is that of national symbols: anthropomorphic figures that embody a nation and its values (e.g., marianne for france). figure 5 presents a map of information on the personification of the sea present in the narthex of the church of asinou, cyprus. the relationships described in figure 5 are quite similar to the ones used for the description of a representation, but, in this case, the symbolic link with a conceptual object is made explicit. as for the representation of saint george, the characteristics of a personification can also be shared by similar representations, so it is crucial to link the semantic information with external reference resources. in figure 5, for example, both the symbolic object and the personification are linked, with wikidata and iconclass, respectively."
"the next phase of the analysis aimed at investigating the number of bugs over the timeline of the project. the number of bugs were identified in the time period going from the publication date in the sourceforge project [cit], to the analysis date [cit], with a quarterly sampling. [cit] . this was not a relevant change and was not related to a detected bug. [cit], because there were discrepancies between the compiere inc. and the development community. from this point, a number of forks have been generated for obtaining a new projects based on compiere. taking into account some documents, it was possible to understand a little 'more of the history of this project: [cit] thanks to jorg janke; [cit], there were more than 1.5 million downloads and more than 100 partners. [cit] the company compiere inc. detected a significant capital from the new enterprise associates with the aim of increasing the success of the erp project and turned the project into a commercial software. [cit] the company changed its corporate structure by adding new managers, engineering a renewed and expanded its sales channels and services; the product line was expanded to include compiere professional edition and enterprise. as with many commercial enterprises system built around open source products, there was a dispute between the management company, who was trying to monetize investments, and the community of"
"besides the data discussed above, the additional aspect considered in the effort framework have been taken into account. specifically, numerous other elements have been considered for assessing the quality of the software project, and, in particular: table 9 shows the results of the product quality assessment of compiere. the data aggregated for the questions related to the product quality goal are reported in the table, together with those concerning the sub-goals, evaluated as the arithmetic means of the values obtained for the related questions. in correspondence of each sub-goal, the table reports the results of the: weights of the oss relevance; weights of the erp relevance; generalized and customized version of the framework. it is possible to observe in table 9 that compiere appears to be a software project that is reliable and well suited to the functional requirements. however, it presents a poor maintainability. its product quality is higher if it us considered as a generic open source project, while it decreases when the erp quality characteristics are considered."
"therefore, the adoption of a floss erp is very advantageous for sme [cit] . as an example, the possibility of really trying the system (not just using a demo), reduction of vendor lock-in, low license cost and possibility of in-depth personalization are some of the advantages."
"once we have found an optimal value for the bid price b, we can instantiate the ctmc in fig. 8 and use it as our random environment representation. states 1, 2, 4, 5 represent a resource in a normal working situation, therefore the qn will be evaluated using standard rates for the resource. state 3 represents the situation in which the resource is not able to process requests, and it corresponds to a qn with zero rates for that resource, meaning that all the requests will be put in the queue until the resource exits state 3. these enqueued jobs are expected to worsen both the mean response time and the response time distribution. our qn solver is able to support ctmc representations for the random environment and therefore our heuristic will take into account the effects of the possibility to lose a resource due to price fluctuations when calculating an optimal deployment for an application."
"the very first step taken during the construction of the ontology was the introduction of a way to sustain new relationships between the physical and visual domain, declaring the new class ic1 iconographical atom. the substance of an iconographical atom is that of a physical feature, embracing an arrangement of forms/colours, which is seen by an agent as a vehicle of a representation. the identity of the class is given by the pure act of selecting a region of space as the content form of an expression. an iconographical atom does not represent anything in itself, but is the physical container we examine when we recognise a ic9 representation. an iconographical atom is always the object of an interpretation, and the conceptual understanding of what it is depicted (the representation) is the result of a recognition. therefore, the representation cannot exist elsewhere than in the conceptual domain, because it is the idea formed in the mind of the viewer when looking at the iconographical atom. for such reason, we define a representation as the set of conceptual elements we use for associating the nuclear characteristics of a visual object with an iconographical atom."
"in the previous step we calculated the computational needs in terms of rates of the virtual resources. in this step we want to decide which real resources to rent to provide such computational needs at minimal expense. to make this decision we consider for each real resource y a mean price equal toĉ y, that can be obtained from historical traces using the estimation method we discuss in sect. 5. the goal is to minimize the sum of these costs while ensuring that the rates of all rented resources are large enough to allocate the rates found as the solution of the previous problem. this subproblem is a classical integer linear-programming problem (ilp) since the decision variables are integers, and the constraints and the objective functions are linear. this is a well-known np-hard problem in which we can find an approximate solution using any ilp solver. we implemented a function findresourcestorent to interface with the mat-lab intlinprog solver, which accepts the rates of the software serversμ and the system parameters s as inputs, and returns the resource assignment vector t."
"-linux instances have 100 % availability this happens because the termination notice is higher than the time needed to start an on-demand resource, therefore the resource will never be in the unavailable state (state 3 of the ctmc in fig. 8 ). -some resources have an infinite overbid time this happens when the current bid price has never been overbid in historical traces. this also results in a 100% availability since the spot instance is assumed to never terminate. -some resources have a bid price that is slightly higher than the on-demand price this is intentional since we want to avoid the situation in which a resource switches too often between spot mode and on-demand mode, which would cause a decrease in the availability and consequently in the amount of processed requests per price paid."
"it is clear that panofsky's methodology, and the revised version proposed by van straten, can be easily integrated with the theory of cognitive type and our addendum about semantic marks. the two methods should be then seen as complementary (table 1 ). in fact, semantic marks help us formalise the relationship between a percept and its interpretation, while panofsky's methodology provides a path for the reading of a work of art, defining a way to take into account the propositional assumptions of a viewer in relation to a visual representation. the division of the assumptions in layers of meaning is hypothetical, and just a formal way of proposing a reading, which panofsky uses in his attempt to eliminate subjective distortions. these distortions are, however, always present in the understanding of a visual work, and do not depend on the work itself, but the situation and social context of the assessment, as proven in section 3.3. a non-western centred approach to classification would provide different readings and understanding, and that is why it is important to clarify when and how the interpretation of visual signs take place. table 1 . correspondence between subject matter and act of interpretation."
"in order to make the assessment as most reliable as possible, all the found information have been collected and considered during the analysis. during the planning phase, the software project compiere, to be analyzed in a major detail, was chosen among the most relevant available projects. issues, such as the programming language, were considered to facilitate the metrics collection."
"the map in figure 6 of the relationship between the allegory of the immaculate conception by vasari, as held in the church of santi apostoli in florence, and the preparatory study made by the artist, helps us visualise the structure of this relationship. the two representations are linked together by a type of visual prototype, in this case, a \"studio for.\""
-dialog step requests: process and update data on the client-side through the graphical user interface; -update requests: higher priority asynchronous update requests that may be triggered by a dialog step request; -update2 requests: lower priority asynchronous update requests that may be triggered by a dialog step request.
"this paper extends an existing framework, called effort -evaluation framework for free/open source projects -defined for quantitatively evaluating the quality of floss systems [cit] . the extension regards a more accurate evaluation of the reliability characteristic. the effort framework was already applied with success for assessing floss erp systems [cit] b) ."
"with community trustworthiness, it is intended the degree of trust that a user can give to a community, about the offered support. support can be provided by communities by means of: good execution of the development activities; use of tools, such as wiki, forum, trackers; and provision of services, such as maintenance, certification, consulting and outsourcing, and documentation."
"the church, built in the picturesque setting of the lower troodos mountain in central cyprus, around twenty kilometres from nicosia, is richly decorated and displays a wide variety of frescoes ranging from twelfth (foundation) to the early seventeenth century. [cit], together with nine other richly decorated rural churches and monasteries in the area, which have been grouped by unesco as the \"troodos painted churches group\" [cit] ."
"finally, in the last step it is possible that the final solution computed is not feasible (i.e., it violates the constraints). in this case we need to search for bottleneck servers and scale them up by a factor α. the algorithm to find the bottlenecks tries to scale-up all the software servers one by one, thus resulting in o(m) queueing network evaluations for each search. each search guarantees that the bottleneck resources speed is increased, thus progressively reducing the violation of the constraints until an optimal solution is found. in some limit situations it is possible that an increase in the rate of a bottleneck resource does not reduce the violation of the constraints, which would prevent the convergence of our approach. these limit cases happen when the contribution to the response time added by the load balancing, the multiple number of processors, and the random environment is too large to be compensated by an increase in rate. examples of these limit situations are cases with very low resource rates or in which bid prices are continuously overbid and underbid. in our experiments based on real data we did not experience any of such limit cases, which leads us to think they are contrived examples."
"for the experiments described in this paper, the knn was set with 10 nearest neighbours, and each experiment was run 30 times using 10-fold cross validation. average 10-fold cross-validation over 30 runs is reported."
"future work includes applying the proposed approach to large image datasets using deep learning classifiers; and comparing the approach to more matrix approximation methods. the significance of the proposed classification approach is that it can make feature extraction methods more accessible on large-scale data which is becoming common in many applications such as natural language processing, image processing, and other data analytics tasks where feature extraction is required. his current project is related to the development of biologically compatible computational models of human sensory systems, including auditory signal processing, human tactile emulation, human visual processing, sensory processing modalities in cognitive robotics, and the implementation of neuromorphic systems on electronics hardware. his work finds applications in industrial robotics, data analytics, and medical systems. his research interests include artificial intelligence, computational neuroscience, and the modeling of biological information processing and cognitive robotics."
"few words need to be spent on another important relationship encoded in the ontology, that of prototype. representation, sometimes at least, has to be seen as the result of a long process that involves preparatory study and sketches of what, in the end, will be the final version of an artwork."
"low-rank approximation is used in manifold learning and dimensionality reduction algorithms that rely on the eigenvectors of the kernel matrix [cit] . the aim of low-rank matrix approximation is to obtain more compact representations of the data with limited loss of information, and using fewer dimensions than the original data [cit] . therefore, low-rank approximation methods construct an approximation of the original matrix which has a rank less than the rank of the original matrix. this section summarises the related methods for constructing low-rank approximation of matrices and describes the concept of kernel spectral clustering, svd, and the improved nyström and random sampling nyström approximation algorithms."
"the models described above were compared with reference to their compliance to the iso/iec 9126 standard, analysing the coverage and features they had in common. table 1 shows the results of the analysis. a standard characteristic was considered as covered by a model if it took into account at least one of its attributes. table 1 shows that not all the models considered take into account the iso standard quality characteristics. the highest coverage is exhibited by irca, but it does not provide an adequate operational tool for its application. the table also shows that the in-use quality is the least-considered quality characteristic. this is due to the difficulty of objectively measuring the metrics related to in-use quality, because they greatly depend on the user. the figure also shows the effort -evaluation framework for free/open source projects -framework defined for overcoming the limitations of the other quality models. the comparison of effort with the other quality models highlights that it covers the main quality characteristics and, in addition, it provides working support for applying the framework."
"to solve this subproblem we use a greedy algorithm that scales down the rates of all the resources as much as it can until one or more bottleneck resources are found for the class of jobs that is closest to the boundary of the constraints. at this point, the rates of the bottleneck resources are fixed, and the algorithm continues to scale down the remaining rates, until all of them have been fixed in the same way."
"in the previous section we have shown the optispot heuristic to decide how many resources to rent from a cloud infrastructure and how to map the application components to them. the approach requires to have a resource model, as explained in sect. 3. in particular, it is important to determine the value of the bid price b(r ) for each resource r . from b it is possible to derive the expected cost c(r ) and other information regarding the possibility to lose the resource, which are needed by our heuristic to evaluate the qn. to simplify the notation, in the remainder of this section we omit the resource type index r since we are always referring to a single generic resource type."
"the mhealth 1 (mobile health) dataset is a benchmark dataset for human behaviour analysis based on multi-modal body sensing. the mhealth dataset comprises body motion and vital signs recordings for ten volunteers of diverse profile while performing 12 physical activities: standing still (1 min), sitting and relaxing (1 min), lying down (1 min), walking (1 min), climbing stairs (1 min), waist bends forward (20x), frontal elevation of arms (20x), knees bending (crouching) (20x), cycling (1 min), jogging (1 min), running (1 min), and jump front and back (20x). sensors on each subject's chest, right wrist and left ankle were used to measure the motion experienced by diverse body parts, namely, acceleration, rate of turn and magnetic field orientation. all sensing modalities are recorded at a sampling rate of 50 hz, which is considered sufficient for capturing human activity [cit] . this dataset has been found to generalize to common activities of the daily living, due to the diversity of body parts involved in each activity (e.g., frontal elevation of arms vs. knees bending), the intensity of the actions (e.g., cycling vs. sitting and relaxing) and their execution speed or dynamicity (e.g., running vs. standing still). data from the subjects carrying out the activities were collected in an out-of-lab environment with no constraints on the way these must be executed, with the exception that the subject should try their best when executing them [cit] ."
"after having outlined the basic structure of vir in section 4.2, this section emphasises the capabilities of the ontology using examples from two datasets, one describing the wall painting in the narthex of the church of asinou and the second one describing the collection of the photographic archive of the harvard university center for italian renaissance studies. for each of the examples, we present an ontological mapping of the information using cidoc-crm and vir. while initially done in rdf (resource description framework), the maps have been transformed, for readability purpose, into graphical representations."
"we briefly touched the surface of the possible applications of the ontology. it is important to understand that such a flexible structure can be of use in diverse fields and scenarios, ranging from the constructions of intelligible labels to the definition of a schema for recording user annotation over visual objects. the latter is undoubtedly the most common use, especially thanks to the technological advancements that have emerged in recent years. the rise of iiif (international image interoperability framework) as a standard for viewing and sharing image collections has ignited the development of viewers' applications rich in annotation capabilities (mirador 4 is probably the most famous example). the possibility of classifying a user-defined spatial area as a representation, as well as correlating it with the iconographical attributes appearing within the image (throughout other annotations), is the first and a necessary step towards creating an iconographical corpus. the use of the ontology together with this type of software would greatly help in the creation of a dataset of attributes, subjects, characters and symbols, allowing researchers to automatically cluster this type of information and perform further research on the interconnections between these elements."
"for these reasons, it is essential to understand how we assign meaning to them, as well as how we identify and differentiate between diverse visual compositions. in order to answer this question, we are obliged to start analysing the perceptual process, and, specifically, the interpretation of a percept, which heavily relies on the act of recognition, classification and reference of the sense data to a model using a specific visual code."
"in this step we want to find a first approximation of the solution of the global problem by assuming that each software server m is deployed on a dedicated hypothetical resource that provides the minimum rateμ m to process requests such that the slo constraints are satisfied. in this step we do not consider the characteristics of the real resources (e.g., number of processors, prices, and the random environments information) since a decision on which one to rent will be done in the next steps. the goal of this optimization problem is to decide the minimal ratesμ m that fulfill the constraints on the mean response time and on the response time distribution."
"the associate editor coordinating the review of this manuscript and approving it for publication was berdakh abibullaev. matrices becomes a complex task. feature extraction algorithms are utilised to reduce the data into fewer dimensions and hence to deal with the 'curse of dimensionality', so as to reduce the complexity and improve the efficiency of operating on large matrices by constructing lower-rank matrix approximations of large matrices [cit] . therefore, the task of feature extraction or dimensionality reduction has become common in large-scale applications, including machine learning. furthermore, the idea behind creating low-rank approximations is that representing the data in a reduced dimensional space removes noise from the data, which then reveals intrinsic structures of the data. for this reason, it is important to create low-rank matrix approximations and utilise these, instead of the full-rank matrices. many methods have been proposed to construct low-rank approximation of matrices, and these methods rely on the eigenvectors of the kernel matrix."
"the rest of this paper is organized as follows. section 2 gives a motivating example. section 3 discusses the problem statement and defines the reference model. section 4 presents the optispot heuristic to provision and map application components to cloud resources. section 5 describes the bidding price strategy we used in our case study and how it can be represented as a random environment. our approach is later evaluated in sects. 6 and 7. section 8 surveys related work. lastly, sect. 9 concludes the paper and outlines possible extensions."
"if the slo constraints do not hold anymore, it means that the real resource parameters of the proposed allocation had a negative effect on the performance. this can be corrected by identifying one bottleneck server m * and increasing its rate by a scaling factor α, which is calculated proportional to the amount of constraint violation. the bottleneck software server is identified as one of the servers that, when scaled-up by α, have the best effect in reducing the constraint violation of the slo. to calculate the slo constraint violation we use finally, to actually determine bottleneck software servers m * we propose the findbottleneckm function, which is shown in fig. 7 . this function iterates all the software servers, trying to scale each one up by α and saving the information of the software servers m * that result in the best reduction of constraints violation. the algorithm then simply recalculates the new resource allocations that would be needed when scaling-up the rate of each software server. once the bottleneck software servers have been found, we just scale their rate up by α and go back to recalculate the real resources to rent."
"the recording of the information related to a heritage object is constructed throughout the registration of different media items (photo, video, text or 3d reconstruction), which function as an anchor and representative in digital space of the original object/phenomenon. the cataloguing, organisation and archiving of such information is of crucial importance, not only for their future retrieval, but also for exposing, revealing and integrating this set of information, as well as providing domain specialists with tools for clustering and organising them."
"it is clear that for each representation there are many possible variants, and simply classifying something as an instance of a specific iconographical type is not enough to characterise it and distinguish it from the network of similar depictions of the same type. the use of the same iconographical type does not imply the presence of the same character, nor the use of the same attributes and symbolic references. it is essential to describe each of these features in order to be able to cluster the representations on the basis of their characterising elements and their interconnections."
"also interesting is the solution developed by de [cit] during the analysis and documentation of the tomb of emperor qianlong in china. the initial investigation revealed that the engravings and iconographies of the tomb were arranged in order to reflect the buddhist tibetan funerary ritual; their layout and spatial position reflects the deposit of religious text within a stupa. to visually show this kind of relationship, a virtual stupa was created and put in a relationship with the final 3d model, in order to allow the interlinking between spatial elements and their conceptual counterparts. additionally, a graph-like interface was created in order to browse the conceptual elements linked to the physical representation. while not using formal representation methods, this solution demonstrates the possibility given by a semantic description of iconographical features. however, even in this case, no identity criteria for recognising and aggregating pictures were provided."
"the architecture proposed in section v is adopted to evaluate the hypotheses provided in section iv using the biomedical and mhealth datasets. classifier performance is evaluated in terms of accuracy and time with and without using the gram matrices derived from applying svd, and two versions of the nyström approximation algorithm -the improved nyström method which uses a k-means sampling procedure [cit], and the randomised sampling nyström which uses random permutation sampling procedure [cit] . experiments were performed using an intel(r) core (tm) i7cpu 3.3ghz, and 32 gb ram."
"cloud computing is a popular paradigm for offering compute capacity as a service. in particular, the cloud gives flexibility to decide and modify the speed, the number, and the lease time of virtual machines (vms). there are several pricing strategies for renting vms, among which are often mentioned two categories: on-demand pricing and spot pricing. on-demand pricing guarantees that a resource is available for a fixed price, which is proportional to the time the resource is rented. in spot pricing, instead, resources are offered at a variable price, called the spot price, which is arbitrarily decided by the cloud provider. spot pricing requires users to bid a maximum price they are willing to pay for. if the bid price is greater than the current spot price, the virtual machine will be charged at the spot price. however, if the spot price exceeds the bid price, the vm will receive a termination notice and eventually be reclaimed by the provider. the advantage of spot instances is that their price tends to be lower than the on-demand price most of the time, but from time to time, when the cloud provider has a shortage of resources, it can temporarily make the spot price steep (much higher than the on-demand price) in order to have most of spot resources back. this makes the decision of choosing a bid price both difficult and important. while a number of works have considered this problem in recent years [cit], the problem of deciding bid prices in light of performance requirements or constraints on the application architecture is more complex and still poorly understood. this paper, which extends [cit], aims at helping cloud users to take maximum advantage from spot instances by supporting the following decisions:"
"probably the most comprehensive solution is the one developed by the fototeca zeri in bologna [cit] for the pharos (international consortium of photo archives) project [cit] . while exposing the zeri photo, the authors developed two ontologies (f entry ontology and oa ontology) to map data coming from the two italian standards developed by the iccd (istituto centrale per il catalogo e la documentazione, or central institute for the cataloguing and documentation), the scheda f (scheda di fotografia, or photography entry in english) and scheda oa (scheda opera d'arte, or work of art entry in english). the two ontologies were mapped with cidoc-crm as well as hico (historical context ontology) [cit], pro (publishing roles ontology) [cit] and fabio (frbr-aligned bibliographic ontology) [cit], which guarantees the possibility of adding information related to, respectively, the provenance of assertions, the roles of the agents dealing with the artworks and the position of the object in relation to the frbr (functional requirements for bibliographic records) model. moreover, thanks to an extension and mapping between hico and prov-o (provenance ontology), the ontology allows the recording of information in regard to the influence between works of art. this work is excellent and touches diverse needs in the art history community, but it does not take into account a description of the features and attributes, which would greatly help in the retrieval and aggregation of visual items."
"art historians have long been studying visual cultures and their inner traits, recognising their commonalities and nuances and linking those to social arena. one of the results was the possible identification of the author of an artwork on the basis of his figurative and stylistic features. an author, in fact, learns and develops specific traits or features during their apprenticeship in a workshop, or by merely examining or studying their predecessor's works. the usage of a set of traits to depict a figure standardises compositions and features, creating a representational canon. one great example is renaissance art. in this period, thanks mainly to a rediscovered sensibility for the roman and greek period, artists and patrons felt the need to have a standardised and understandable canon of images, an easy instrument with which to get inspired and follow the design and conception of new works of art [cit] . while the need was, indeed, general, there were certain specific tasks, for instance, the representation of identifiable intangible concepts such as love or fortune, which benefited greatly from such formalisation. the illustration of these abstract ideas had to be done through the use of substitutes for the abstractions, such as symbols or personifications. the use of these visual devices as the embodiment of concepts and ideas, however, also fulfilled the communicative purpose of an image, providing the viewer with a possible reading of the scene. in order to do so, these figures needed to be acknowledged by a high number of people. achieving such a goal required a figurative normalisation in accordance with specific models. it was bearing this prospect in mind that in the 16th century manuals like \"le imagini colla sposizione degli dei degli antichi\" [cit] and \"mythologiae sive explicationis fabularum\" [cit] started to appear. a major milestone in this direction was the publication of ripa's iconologia [cit] in 1593. this work covers over 1200 personifications, comprising an extensive collection of visual representation drawn from both classical and contemporary works of art. ripa's book reported on not only visual representations (added only in the 1603 edition) together with their designated meanings, but included detailed descriptions of how they should look and why they should be depicted in that way. the impact that ripa's iconologia had on his contemporaries, as well as on artists in the later centuries, was remarkable, and started to lose its importance only with the advent of realism [cit] ."
"using this type of relationship, we can easily visualise the process of creation of an artwork through the use of the diverse prototypes/versions that follow one another until the final object come into being."
"we evaluate the real sap erp application described in the previous section under different scenarios characterized by a variable number of users to analyze the scalability; with different slos, to analyze the behavior in more challenging situations; and finally with capped overbid time and fixed minimum underbid time, to analyze the effects of the random environment when the chances for a resource to be overbid is increased. each experiment has been repeated in two different amazon ec2 regions (eu-west and us-east) and with different operating systems (linux and windows). in each scenario we measure the expected hourly price of the resources, the time needed to compute the solution on our system, and the number of queueing network evaluations. we repeat every evaluation 30 times with a different search starting point to ensure statistical confidence of the results and to show the standard deviation bars in each plot."
optispot can quickly find a local optimal solution. we validate the accuracy of this solution by considering the queueing network model of a real enterprise resource planning (erp) application and real recent historical data of amazon ec2 spot prices. we compare our results with an approach that uses a nonlinear optimization algorithm and show that our heuristics provides better results in less time.
"the pseudocode listing of the algorithm is shown in fig. 4 . the function receives as input an initial set of arbitrarily large feasible ratesμ init, and the system model s that contains all the parameters of the application and the resources described in sect. 3. it returns the optimal rates for each software server as vectorμ. the variable r is initialized as the set of all available resources that can be scaled. then, all resources are scaled down using a bisection method until the constraints are violated: minimum rates are increased when the constraints are satisfied and the maximum rates are decreased when the constraints are violated. when the minimum and maximum rates are close enough, the current bottleneck resources are removed from r and the process continues until r is empty. at this point the rate calculated so far is returned as our optimalμ. the auxiliary functions used in the algorithm (briefly described in fig. 5 ) are directly derived from the evaluation of the queueing network and simple operational analysis laws."
"the states of each resource r and the transition frequency among states are represented as the continuous-time markov chain (ctmc) in fig. 8: -state 1 the resource is available as a spot instance. spot price is paid. -transition 1 to 2 the bid price has been overbid, the spot instance receives a termination notice, and an on-demand instance is scheduled to start. -state 2 the resource is available as a spot instance (although it has received the termination notice) and an fig. 8 ctmc representing the different states of each resource. once the optimum bid b is determined, it also represent the random environment of the system. \"available\" states are the states in which the resource is available. the \"unavailable\" (red) state is a transition state in which a spot resource is lost and the replacement on-demand one has not been started yet (color figure online) on-demand instance is starting. both spot and on-demand prices are paid."
"a common step in kernel methods is the reduction of the data to a kernel matrix, also known as a gram matrix. the gram matrix is often used for machine learning tasks such as classification and predictive modelling. a significant drawback of kernel methods is the computational complexity associated with manipulating kernel matrices. this paper demonstrates that leading eigenvectors derived from svd and nyström methods, for reducing the dimensionality of data, can be utilised for classification tasks without the need to construct gram matrices. experiments were conducted with 14 biomedical and 10 mhealth datasets to compare classifier performance when taking as input matrices containing: 1) leading eigenvectors which result from svd and nyström methods; and 2) matrices which result from constructing patient-by-patient gram matrices. in the experiments using the 14 biomedical datasets, the results revealed that when the proposed architecture with a knn was adopted, svd achieved on average higher accuracy using fewer number of dimensions compared to nyström methods. the results revealed up to 34.86% improvement on the mhealth datasets when using svd in the proposed architecture, as opposed to using nyström methods. in experiments using the ten mhealth datasets, the results revealed that when leading eigenvectors are input into a deep sequential machine learning model for the task of human activity recognition, svd-g performed outperformed rg-g by 4.65% and outperformed ekm-g by 6.62%. these results demonstrate how the proposed architecture can make feature extraction methods more accessible on large-scale data such as the mhealth dataset using a deep learning model, and in particular a deep sequential model."
"in this step we check if the slo constraints still hold when considering the system allocated using the resource assignment vector t and the allocation matrix d found in the previous steps. in our implementation we use the line tool [cit] to evaluate the mean response time and the response time percentiles, which considers also real resource parameters such as the number of processors, the load balancing, and the random environment model that describes the possibility for a spot resource to be lost and replaced with an on-demand one when its bid price is overbid. if, after calculating the response times, the slo constraints still hold, we can stop here and return the decision variables t and d calculated so far. these will be used to reconfigure the system and apply the resource rental and allocation decisions."
"the results obtained with reference to the product attractiveness are shown in table 11 . compiere appears to be a software project with a good attractiveness. it obtained very high marks, and the best results are obtained for the diffusion, data portability and legal reusability. the worst results is related to the costs and support to the migration."
"the primary or natural subject matter, which identifies pure forms such as a configuration of lines or representations of an object, which could be called the world of artistic motifs. the collection of these motifs pertains to the pre-iconographical description of a work of art."
"the proposed work started from the idea of having a toolkit supporting the characterization and evaluation of oss projects. in this direction, it is important not only to consider the quality of the software, but also other distinctive features of the open source projects. therefore, it was decided to identify those data that are usually difficult to detect by the users, and that are useful for making some assessments of the projects of the oss repository sourceforge. in addition, it was decided to proceed to the customization of an already defined framework, effort, retaining its characteristic of generality, that allows to characterize any type of open source project regardless its application domain."
"some of these methods include independent component analysis (ica) [cit], principal component analysis (pca) (also called karhunen-loéve transform-klt), singular value decomposition (svd), laplacian eigenmap [cit], multidimensional scaling (mds) [cit], spectral clustering [cit], isometric multi-manifold learning [cit], kernel fisher linear discriminant analysis [cit], and the clustered nyström method [cit] . a common step in kernel methods is the reduction of the data to a kernel matrix, also known as a gram matrix. the gram matrix is then used for machine learning tasks such as classification, clustering, and dimensionality reduction [cit] . a significant drawback of kernel methods is the computational complexity associated with manipulating kernel matrices."
"the intrinsic meaning or content is the interpretation of \"the work of art as a symptom of something else which expresses itself in a countless variety of other symptoms, and we interpret its compositional and iconographical features as more particularized evidence of this 'something else'\" [cit] . the intrinsic meaning is defined by how cultural-historical developments are reflected in a representation, and such meaning is displayed independent of the will of the artist, who could be completely unaware of it. in a later stage, panofsky called this phase the iconological interpretation."
"the fourth and final step of the analysis is iconological interpretation, which deals with those symbolic values that are not explicitly intended by the artist, and are part of the visual culture of the time."
"nevertheless, while adopting a floss could represent an important competitive advantage for a company, it could be useless or even harmful if the system does not adequately fit the organization needs. then, the selection and adoption of such a kind of system cannot be faced in a superficial way."
"during the analysis and description of the visual information presented in our case studies, we quickly noticed that iconographic representations are dynamic objects that evolve over time. in order to easily demonstrate this to a wider public, we chose to focus, in both case studies, on a widely known iconographical character: saint george."
"the name is, of course, significant, because the scope of the ontology is the formalisation of the relationships between the visual representations and symbols that characterise a single artwork or are distinctive of a social arena. vir is grounded on the semiotic distinction between expression and content, and introduces class and properties for annotating pictorial elements that compose visual works and their denoted/connoted conceptual elements."
"while annotators built on top of iiif viewers or other 2d/3d technology are undoubtedly the most common example of use, the ontology could also be used in correlation with machine learning (ml) algorithms. this type of algorithm excels in assigning labels to pictures, but falls behind when structuring the information they produce. it is conceivable that ml algorithms could be used to define a series of attributes for each representation, using the ontology to record them in a database."
"in this case, it is crucial to ground the iconographical information within its history, defining the influence on the production of the painting of both the donor and the frankish occupation of the island. figure 7 documents the integration of the aesthetic information within the historical framework of production. the creation of the painting is linked in time with the lusignan occupation of the island, from 1192 till 1474. the period is, moreover, linked with two other spatio-temporal gazetteers, perio.do 2 and chronontology 3, which help in retrieval and also in the browsing and visualisation of further documented periods."
data for conducting the analysis have been extracted from the notre dame database. this database is hosted by the university of notre dame and includes data for 563.290 open source projects.
", where x is the signal, which is identified by a dimension (n) in a specific system (could be a specific projection system as well as a topological relationship). the identity of the signal is, moreover, defined by its differences from the surrounding area, because it is exactly this element which grounds its identification into a single unit."
"the second act of interpretation is the iconographical analysis, which requires more specialised knowledge and the use, in this case, of vocabularies of forms in order to describe the content of the image. these vocabularies do not have to be external resources, but they easily can be embedded in our knowledge repositories and inherited in a social arena (see bourdieu [cit] and lemonnier [cit] for a theoretical treatise on the subject). the recognition of the meaning of the image is based on identification of the diverse signs incorporated into the image, usually consisting of sets of attributes and characteristics. the combination of these attributes, such as objects, plants, animals or other icons/symbols, help identify a personification/character in a specific situation/narrative in a work of art. attributes can also help identify certain qualities (kindness, rage etc.) of the depicted character, or his belonging to a distinct group (blacksmith, noble, saint etc.). the use and harmonisation of this combination have helped to create iconographical types and defined archetypical situations, providing tools for the identification of diverse types of representations [cit] . attributes can be seen as a subset of the semantic marks formalised in 3.3. in that case, it appears that iconographical types are nothing other than cognitive types we use for describing and communicating stances about our visual world."
"these details are shown in table 4 along with the average accuracy and average time needed for the classifier when using each of the methods. fig. 6 shows the average time taken for the classification task for each method across various dimensions. the experimental results are consistent to support the hypotheses provided in section iv. however, before reaching a final conclusion it is worth exploring whether there exist any significant differences in classification performance when using the svd, ekm-nyström and rs-nyström methods."
"the very first component is the signal, which is an external stimulus, a datum, identified on the basis of its difference and its form. we will flatten its definition, for a functional purpose, using logic, as:"
"as mentioned before, the resemblance is given by a degree of similarity. therefore, the sets a and b, which are, respectively, the set of all the matching situations and the set of all the matching physical objects, which we can describe as:"
"this is a result of the method we used to calculate the optimal bid price, which tries to find the best trade-off between the actual price paid and the availability. another reason for the high value is the fact that the time during which a resource is unavailable is less than the time needed to start the new on-demand resource since the new on-demand resource is started proactively after the spot instance termination notice from amazon ec2 is received."
"-what type of virtual resources should be rented for a given application? -how to efficiently map the components of an application (e.g., web server vms, a database vms) to the rented resources? -what is the optimal bid price for each resource that allows to fulfill quality of service requirements?"
"in this paper we have presented optispot, a cost-aware approach to support run-time decisions for provisioning cloud resources and allocating application components among them. the benefit of optispot is that it is able to approximate a very complex problem using simple greedy algorithms that are lightweight enough to be used at run-time to support proactive and reactive system adaptation. moreover, we have shown a possible way to optimize the bid price that makes use of a markov chain representation of the system. we then used the same markov chain, instrumented with an optimal bid price, to have a representation of random environmental parameters such as the possibility for spot resources to be lost and replaced with on-demand ones. the random environment is used by optispot to predict system performance and make deployment decisions even when price fluctuations modify the resources and the system ability to process requests during state transitions."
"openhub, a public directory of open source projects and related developers, where it is possible to find the results of analyzes, reports and comparisons on demographic trends of the software. it also provides information on the issued license and number of committers and performs code analysis."
"in this analysis we have seen that our approach is able to outperform an exact algorithm that is based on the matlab fmincon interior-point solver. the reason for this result is that our heuristic is able to choose the resources with the best price/ecu ratio and to allocate the application components in such a way that they are not fragmented among cloud resources unless the number of resources is smaller than the number of components. if the number of resources is small, such as in the case of 1000 users, there is minimal difference between our approach and the exact one. as the number of users increases, or the slo becomes more restrictive, we need more cloud resources to fulfill the slo. when the number of resources becomes larger than the number of application vms, the exact approach is not able to choose the correct size of the resources since it tries to resize the partitions of multiple resources, leading to oscillations and slow convergence. the high number of partitions also results in a higher time to evaluate the fluid-approximated queueing network, which ultimately results in large total execution times. unfortunately, due to the limitations of fmincon we could not express a fitness function that was good enough for the exact approach to converge in every situation. however, in situations in which we observed convergence, the computation of the result was always significantly slower."
"in the remainder of the paper, svd stands for singular value decomposition, ekm for improved nyström method, and rs for the random sampling nyström method. the following hold:"
"the model includes a hierarchy of attributes. in correspondence to each first-level characteristic, one goal is defined. then, the effort measurement framework includes three goals regarding: product quality, community trustworthiness and product attractiveness. questions, consequentially, map the second-level characteristics, even if, considering its complexity and the amount of aspects to be considered, goal 1 has been broken up into subgoals."
"is the transpose of matrix v k, k is a diagonal matrix containing the k leading singular values of matrix x, and the u k and v k have orthogonal columns that contain the leading k left and right singular vectors of matrix x corresponding to its singular values. the aim is to generate an approximation x of matrix x based on a sample k n. it is important to identify a suitable number of k dimensions to retain and which are needed to create a good approximation of the original matrix with fewer dimensions and minimum error."
"once the evolution has been performed, for understanding the applicability of the evolved framework, it was applied to a case study conducted on a relevant selected open source erp project, compiere. the gathered data and results analysis provided a positive feedback with reference to the applicability and effectiveness of the new framework. they provided a better insight of the software project quality and the analysis of the bugs also suggested to deepen the compiere history and understanding its management mechanisms now, the effort framework considers many aspects of the oss quality. the only thing that it is not yet considers is the quality in use that could be subject of future studies. in future works, this aspect will also be considered. in addition, a more detailed analysis of its applicability will be performed, by considering additional oss projects."
"we present in section 2 the literature review related to the topic. section 3 is dedicated to a brief presentation of the theory of meaning, embracing a short explanation of the how we can classify percepts and how we re-use information in order to assign meaning to the visual object. section 4 presents the functional results of the theory, introducing the vir (visual representation) ontology, a cidoc-crm (cidoc conceptual reference model) extension for recording information about visual representation. in section 5 its application is discussed, followed by section 6, where limitations of the current approach are presented."
"furthermore, art historians have been studying the formalisation of visual cues, the creation of canons and models of depiction for quite some time, and they are also responsible for the formalisation of several resources used as a nomenclature system for artistic motifs and subjects."
"as previously stated, the selected project is compiere, an erp solution including also a customer relationship management (crm) component. it was designed for small and mediumsized businesses, government agencies and nonprofit organizations. this system is distributed and supported by compiere, inc. and the compiere partner network, a group of certified partners. the software source code is released under the gpl v2, as community edition. there are also three other editions standard, professional and enterprise. they are issued on an annual subscription basis for a fee and, in the case of the professional and enterprise editions, with commercial license. the various issues differ for the offered support, but there are differences also in terms of services, documentation, functionality, provided updates and upgrades. figure 1 shows the distribution of the bugs with reference to the priority, shown on the horizontal axis. it indicates that most of the bugs has priority 5, and this is justified by the fact that level 5 is the level of priority assigned by default from sourceforge. this should indicates that the bugs priority level is not always specified. if we consider the other priorities, it is noteworthy to observe that the bugs that have the higher priorities are more than those with the lower priorities, especially if level 7, with 304 bugs, is considered."
"the initial core of the ontology was later refined while analysing the dataset of the photographic archive of the harvard university center for italian renaissance studies. the archive of the centre holds a collection of around 250,000 photographic prints, focusing mainly on the italian art, especially painting and drawing, of the late middle ages and renaissance from 1250 to 1600."
"finally, research on service placement and load allocation has been specialized to take into account spot pricing models and the possibility to lose resources unexpectedly [cit] . with respect to these works we also solve the allocation problem in such a way to minimize the costs while maintaining the desired service level. our new contribution is that we adopt fluid-approximated performance models [cit], which can calculate response time distributions quickly enough to be used at run-time. we also use a random environment model [cit] to represent the effects of external events to the system, which for now is limited to price fluctuations, but that can be easily extended to other events expressible as stochastic models. finally, in our model we also consider the effects of having multiple cpus in cloud resources (as it is the case for amazon ec2) and the overhead due to load balancing in case of placement decisions that require resource replication."
"the n-ary construct allows us to relate the two representations, which are connected together using a class which we can further specialise, including the type of relationship that exists between the two representations. in our example we used the type \"preliminary version,\" but diverse types can be used. the same relationships that connect a preparatory study and the final artwork could be easily used for relating a copy to the original. the copy is, in fact, nothing else than a new object which uses another one as an example. the type of resemblance between the two is just a perceptual judgment, which does not change the process of reusing another object as a prototype for a new one. using this type of relationship, we can easily visualise the process of creation of an artwork through the use of the diverse prototypes/versions that follow one another until the final object come into being."
"for many readers, this concept will resemble that of the prototype [cit], but the difference is in their nature. the prototype is an instance of a class which is seen by a cultural group as the one that best represents the class itself. the ct instead works within the primary semiosis field, assessing the membership to a specific category based on perceived characteristics, and it is not an instance, but merely an idea of the nuclear traits. the prototype is an instance of a class which we assume best covers the characteristics of that class, while the ct is nothing other than the idea that allows us to define the membership itself, more closely to an underlying grammar for the construction of the class than to another concept."
"the recognition and categorisation of a datum is achieved by the relation of the datum to a reference type, using a schema to mediate between the concept and the manifold of the intuition. eco [cit] suggested that to comprehend this process, we should start examining how we classify the unknown. we will delve deep into his thesis, identifying the nuclear elements which enable us to build a shared understanding of our visual reality. for such reason, it is of paramount importance to first introduce, analyse and explain eco's theory. eco asked himself, and his readers, how we would be able to interpret and socially talk about something if we were to see it for the first time. he proposes a thought experiment using the aztecs' first encounter with the spanish knights. during this occasion, the aztecs were faced with an entirely new animal, mounted by individuals completely covered by metal plates."
"following his methodology, the signs that compose a representation are identified during the pre-iconographical phase through the identification of artistic motifs. this step was also identified by barthes, who called this immediate visual impact, which defines the primary subject matter, the denoted meaning of an image, and the process it originates, denotation [cit] ."
assessing the effectiveness of the changes introduced in the effort framework required the execution of a case study on a relevant open source erp (enterprise resource planning) project. compiere (www.compiere.com) has been considered system. it is widely used in small and medium enterprises. a description of the planning of the analysis and achieved results follows.
"this process is described using several types of attributes (e.g., study for, preparatory, version, prototype, studio), which we do not define as different single properties (which would create a semantic closed system), but we group together into a single relationship. we created the class k4 visual prototype together with the property k4.1 prototypical model as a n-ary construct for documenting the type of prototype used for the creation of an image. a preparatory sketch, for example, would be described as a prototypical version of the final artwork. the description schema is, fairly simply: representation a hasprototype prototype x prototype x hastype \"preliminary version\" representation b isprototypeof prototype x"
"effort is a framework defined for evaluating the quality of floss systems . it can be considered as a base framework to be specialized to a specific working context. effort has been defined on the basis of the gqm -goal, question, metrics -paradigm . this paradigm guides the definition of a metric program on the basis of three abstraction levels: conceptual level, referred to the definition of the goals to be achieved by the measurement activity; operational level, consisting of a set of questions facing the way the assessment/achievement of a specific goal is addressed; and quantitative level, identifying a set of metrics to be associated to each question."
"the theory outlined in section 3 will be used as a backbone for developing an ontology for the description of visual representations, and is going to be regarded as its main conceptualisation. developing an ontology, in fact, means primarily relying on a clear commitment to a particular conceptualisation of the world, and to reflecting this commitment in an information artefact which approximates the intended model. we translated the existential and identity commitments outlined in section 3 to construct an extension of cidoc-crm called vir: visual representation 1 ."
"the first constraint states that it is not possible to allocate to a resource a rate that is larger than the rate of its resource type. the approach can be seen as an autonomic feedback loop since it adapts the system at periodic interval by using the most updated prediction data available for the resource prices and the application load the decision variables and the system parameters described in fig. 2 as input, which are omitted to simplify the notation."
line graph illustrating the average knn classification performance across the various dimensions. approaches which did not use the gram matrices achieved higher average classification accuracy than those which used gram matrices. average accuracy is derived from running each method 30 times. svd-g needed fewer dimensions than other methods to achieve the highest classification accuracy across the datasets.
"using this modelling, we can easily cluster and browse information about representations created in a specific period or location, and, thanks to the class and property defined by vir, we can explore the use of specific symbols or iconographical types within historical periods."
"datasets were downloaded from online repositories containing high-dimensional biomedical datasets. some of the datasets used in the experiments are benchmark biomedical datasets commonly used for evaluating pattern recognition algorithms, whilst other datasets have been used in biomedical papers. the datasets and their characteristics are shown in table 1 . some of the datasets contained nan values, and these were replaced with a weighted mean of the k nearest-neighbour columns as part of the normalisation process. the nearest-neighbour column is the closest column in euclidean distance. if the corresponding value from the nearest-neighbour column is also nan, the next nearest column is used."
"these symbolic values can be analysed historically and ethnographically, and not only from an art historian's perspective. iconological interpretation adds a new level of meaning to a representation, the connoted meaning. if the denoted meaning previously introduced is about the object as expressed by form, the connotation is an interpretation on the basis of a socio-historical analysis of the symbols of an image [cit] . the codification, description and tracking of connotative references between visual and conceptual objects is another important aspect to track, because it is even more socially grounded than explicit reference. integration of the study of symbolic values in visual images could help us make sense of how we use semantic marks to classify reality, and how it does change on the basis of the context in which the visual classification takes place."
"the decisions produced by our approach are designed to be used to trigger allocation, deallocation, migration, and replication actions on one or more cloud infrastructures. in our model we assumed that these actions do not affect performance since we consider to keep the system running while they occur; however, this might not be true in every system. some future work we have in mind is to introduce in our models and heuristics the possibility to take into account possible overhead in terms of time, performance, and cost that can arise when actually performing adaptation actions on a real system. we also intend to investigate how the approach behaves in presence of different cloud platforms (e.g., federated clouds [cit] ), services, and alternative ways of expressing the slos. finally, another possible follow-up work is to extend our approach to decentralized cloud systems to improve the scalability and resistance to dynamism, which may contribute to support new emerging cloud paradigms such as volunteer clouds [cit] and edge clouds [cit] ."
"the class describes the process of recognition of a representation using a fairly simple schema: the above schema, as discussed in sections 3.2 and 3.3, is the base of the interpretative act, and the only variables are the context, the classified object (iconographical atom) and the value assigned (representation or attribute). the class ic12 visual recognition respects and reproduces this schema, making it possible to describe, for example, the representative value assigned to an image by different art historians, thus enabling the system to keep track of the persons assessing a specific object. moreover, the use of such a construct would help us record the set of features in a representation considered more salient by a viewer than others within a historical period. in the context of vir, we call those features attributes. the clustering of the attributes together with the representation they belong to is essential for the analysis of the association between semantic marks and the cognitive type they represent, in order to show their development and changes over time."
"adoption of free/open source software -flossrepresents a concrete solution to support any business, whatever the size. they offer customized solutions for enterprises, even with few people that can be up and running in two or three weeks."
"the paper is structured as follows: section ii discusses related works; section iii describes related manifold learning and low-rank approximation methods; section iv describes the problem definition and hypotheses; section v provides the proposed method and architecture; section vi explains the experimental setup which comprises descriptions of the datasets and experimental methodology. section vii discusses the experiments performed using benchmark multiclass biomedical datasets. section viii describes the results with multi-modal multi-sensor smart phone data (mhealth) to predict human activity; section ix describes the results when adopting the proposed framework with a deep sequential classifier and applied to the mhealth data for the task of human activity recognition. finally, section x provides conclusions and future work."
"the graph in figure 3 outlines the map of the information about the panel of saint george in the church of asinou. the map presents the description of the visual recognition of the representation of saint george, which assigns the status of a representation to the iconographical atom in the south lunette of the narthex of the asinou church. the representation is identified here as \"saint george.\" however, the recognition of a specific subject is dependent on the knowledge the interpreter has of the context of production. a felicitous recognition is conditioned by a grasp of the context of the artwork, because, while many would recognise saint george as the main subject of this wall painting, many others, not familiar with christian iconography, would only recognise a man riding a horse. an expert in byzantine iconography could instead quickly identify him as diasorites, a more specific iconographical type. it is important to underline that such diversity in classification in respect to the same representation is, indeed, possible. the modelling in figure 3 does represent only one of these possible recognitions (the one originally described in the record), but the same assertion, which assigns a representation value to an iconographical atom, is repeatable. we could have, therefore, an instance of ic1 iconographical atom acting as a hub of diverse visual interpretations carried out by different art historians who do not share the same knowledge on the subject, or who decide to diverge on the type of attribution (e.g., generic vs. specific). the (possible) selection of a chosen interpretation is not an ontological problem but an institutional one, and should be carried out on the basis of the provenance of the selected assertions. however, the recognition of a specific subject is dependent on the knowledge the interpreter has of the context of production. a felicitous recognition is conditioned by a grasp of the context of the artwork, because, while many would recognise saint george as the main subject of this wall painting, many others, not familiar with christian iconography, would only recognise a man riding a horse. an expert in byzantine iconography could instead quickly identify him as diasorites, a more specific iconographical type. it is important to underline that such diversity in classification in respect to the same representation is, indeed, possible. the modelling in figure 3 does represent only one of these possible recognitions (the one originally described in the record), but the same assertion, which assigns a representation value to an iconographical atom, is repeatable. we could have, therefore, an instance of ic1 iconographical atom acting as a hub of diverse visual interpretations carried out by different art historians who do not share the same knowledge on the subject, or who decide to diverge on the type of attribution (e.g., generic vs specific). the (possible) selection of a chosen interpretation is not an ontological problem but an institutional one, and should be carried out on the basis of the provenance of the selected assertions. an ic12 visual recognition results in the constituency of an instance of the class representation that is further described as portraying the character of saint george. this relationship is achieved using the property \"k24 portray.\" using the representation as a vehicle to record propositions about the visual object, we define its iconographical type, using the property \"p2 has type\" from cidoc-crm. in the example in figure 3, we used our own internal vocabulary, but it could easily be linked with external ones. more interesting is the possibility of defining the attributes of the representation, which in figure 3 are the horse and the spear. figure 4 presents the map of the description of \"saint george killing the dragon\" by vittore an ic12 visual recognition results in the constituency of an instance of the class representation that is further described as portraying the character of saint george. this relationship is achieved using the property \"k24 portray.\" using the representation as a vehicle to record propositions about the visual object, we define its iconographical type, using the property \"p2 has type\" from cidoc-crm. in the example in figure 3, we used our own internal vocabulary, but it could easily be linked with external ones. more interesting is the possibility of defining the attributes of the representation, which in figure 3 are the horse and the spear. figure 4 presents the map of the description of \"saint george killing the dragon\" by vittore carpaccio from the photographic archive of the harvard university center for italian renaissance studies. it is easy to see that in this representation, the figure of saint george is richer in attributes (castle, princess, lake and dragon) in respect to figure 3 . the two representations of saint george in figures 3 and 4 do carry their own diverse identities, but can be easily correlated using their shared set of elements. the correlation can be based, for example, on the depicted character. in this instance, they both describe the character called saint george, and even if the nomenclature in the two records is not the same (in figure 4 we have st george, while in figure 3 we have saint george), they both use external resources (in this case wikidata) to define the identity of the portrayed character. another important feature that can be used for correlating the records is the class attribute. the two attributes used for the description of the panel of saint george in asinou are also used for the description of saint george by vittore carpaccio. while the latter uses a larger set of elements, the spear and the horse are shared by both representations, and, if adequately modelled, we would be able to link the representations on the basis of the visual elements used to characterise the saint. attributes used for the description of the panel of saint george in asinou are also used for the description of saint george by vittore carpaccio. while the latter uses a larger set of elements, the spear and the horse are shared by both representations, and, if adequately modelled, we would be able to link the representations on the basis of the visual elements used to characterise the saint."
"generally speaking, some models mostly emphasize product intrinsic characteristics and, only in a small part, the other floss dimensions. vice versa, models have been proposed that try to deeply consider floss aspects, offering a reduced coverage to the evaluation of the product."
"used for the description of a representation, but, in this case, the symbolic link with a conceptual object is made explicit. as for the representation of saint george, the characteristics of a personification can also be shared by similar representations, so it is crucial to link the semantic information with external reference resources. in figure 5, for example, both the symbolic object and the personification are linked, with wikidata and iconclass, respectively. the grounding of such information using a formal ontology enables diverse possible combinations of queries, searching, for example, for a specific mix of attributes and symbolic content expressed by a personification."
"the richness in representations of the same subject with very different attributes has been discussed in section 4.2. while, when using the ontology, it is possible to assign identity to the various components of a representation and link the diverse types of depictions with the same subject or character together, it is not a fully resolved issue. if the attribute and the characters are not properly annotated, the machine can do very little to resolve a human error or bias. the co-referencing problem should be dealt with relying on human judgment, semantic automation (e.g., silk) or using the similarity constraints outlined in section 3.3: topological, feature, alignment or value information. while the feature can be easily defined using the ontology, the topology, alignment and value necessitate the help of diverse algorithms, such as the one used in machine learning, which can calculate the colour value present in two representation as well as their geometrical similarity, and then propose to the user the integration of the information."
"the objective of the formal ontology was the recording of statements about a series of wall paintings present in the church of panagia (mother of god) phorbiotissa, commonly known as asinou, in cyprus."
"low-rank matrix decompositions are important in the application of kernel methods to large-scale learning problems. high-dimensional data is represented in more than two or three dimensions and it can be difficult to manipulate and interpret. one approach to dealing with high-dimensional data is to assume that the data of interest reside on an embedded non-linear manifold within the higher-dimensional space. if the manifold is of low enough dimensionality, the data can be visualised in a low-dimensional space. manifold learning is also known as non-linear dimensionality reduction."
"specifically, we focus on applications developed according to the model-driven engineering approach, in which a performance model of the application can be automatically generated through model-to-model transformations."
"the results provide evidence to support the main hypothesis of this paper, that the leading eigenvectors which represent the factor weights of each patient or person, need only be input into a classifier, and that there is no improvement in classification performance to construct and use a gram matrix. furthermore, the fact that when adopting the proposed approach, classification accuracy is higher on various datasets of different types (including multi-modal multi-sensor mhealth data) allows for the assumption that the improved accuracy is dependent on the solution of the approximation methods and thus the theoretical properties of the methods and not the datasets. importantly, the results also conistenlty revealed the superiority of svd as a feature extraction method, when compared to nyström methods."
"the system monitors periodically the environment, then it tries to self-adapt the number of cloud resources rented, and the deployment of the software servers on them to optimize the prices and still meet the service requirements. to avoid performance degradation at run-time due to migration and reallocation of software servers, we assume that old resources are deallocated only when the new ones are fully initialized and ready to accept jobs. based on the considerations above we define our decision variables as follows:"
"one of the main aspects that denotes the quality of a project is the product quality. it is unlikely that a product of high and durable quality has been developed in a poor quality project. so, all the aspects of the software product quality have been considered in the framework, as defined by the iso/iec 9126 standard [cit] ."
"goal 2 is defined as follows: analyze the offered support with the aim of evaluating the community with reference to the trustworthiness, from the (user/organization) adopter's point of view. table 3 shows the set of questions related to goal 2."
"the secondary or conventional subject matter is the assignment of theme and concept to the composition of artistic motifs, which are recognised to be the carrier of a conventional (how specific themes and concepts are usually depicted in the visual arts) meaning. the subject(s) of a representation are identified in this layer, thanks to an iconographical analysis."
"this section provides the results when using the svd-g, rs-g, and emk-g methods to extract features from the mhealth datasets to predict human activity using a deep sequential machine learning model 2 . the structure of the model is shown in fig. 13 . the svd-g, rs-g and ekm-g approaches are compared and the results are shown in table 8 . dimensionality for all approximation methods was set to 10 dimensions, 2 the python code for performing svd on the mhealth data and then feeding the extracted vectors into the deep sequential model is provided here: https://github.com/gcosma/ieee_access_mhealth which is the same setting used in the previous experiments described in section viii. experiments were carried out using all 10 mhealth datasets described in section vi-b. mhealth datasets are large and 10-fold is not normally be recommended for large datasets, however in these experiments 10-fold was suitable because the datasets are considered to be 'limited', as described in section viii. as shown in table 8, performing feature extraction using svd-g resulted in higher classification accuracy compared to nyström methods. svd-g outperformed rg-g by 4.65% and outperformed ekm-g by 6.62%. comparing the average performance across the 10 mhealth datasets of the knn and deep sequential classifiers, shown in tables 7 and 8 respectively, the highest classification accuracy was achieved using svd-g with knn and the deep sequential model returning approximately the same accuracy. a 27.98% increase, was revealed when using the deep learning model as opposed to a knn model to classify the data derived from ekm-g; and a 16.47% increase, was revealed when using the deep learning model as opposed to a knn model to classify the data derived from rs-g. clearly, the leading eigenvectors derived from svd are of better quality, and easier to classify than those derived from nyström methods. furthermore, the top eigenvectors returned by nyström methods, ekm-g, and rs-g, required a more efficient classifier than knn, to classify the data, and hence the deep sequential model provided better classification accuracy values."
"goal 3 is related to product attractiveness and it is formalized as follows: analyze software product with the aim of evaluating it as regards the attractiveness from the (user/organization) adopter's point of view. two elements that have to be considered for evaluating a floss product are functional adequacy and diffusion. the latter could be considered as a marker of how the product is appreciated and recognized as useful and effective. other factors that can be considered are cost effectiveness, an estimation of the tco (total cost of ownership) [cit], and type of license. this aspects are considered for formulating the questions of goal 3 listed in table 4 ."
"in order to classify the statements about our case studies, we introduced eight classes (character, iconographical atom, attribute, representation, personification, visual recognition, verso and recto) together with twenty properties. we will briefly introduce some of these classes, and then use a few examples to demonstrate their usefulness in the description and mapping of information about visual representations."
"while the ontology can be used as a schema for any information system, it does provide the best possible outcome as a way to structure information in an rdf store where, thanks to the sparql query language, we would be able to integrate linked data coming from other sources. a straightforward example would be to use iconclass, probably the primary classification system in the domain of iconography, to obtain normalised terminological entry for the description of attributes and types. we could then easily use sparql to directly query an iconclass graph to find the necessary nomenclature for the definition of the chosen attributes, or the type of iconographical representation we are dealing with, relying on the structure of the ontology for their integration."
"in the next subsection, the planning of the analysis will be described. then, the subsequent subsections provide a discussion concerning the performed bug analysis and evaluated quality of the selected project."
"the impact of ripa's iconologia was not only to be searched for standardisation of the features and poses for the recognition of depicted types, but also on the influence that those standardisations had on the western-based vision of art. art historians became used to employing a type-based thinking for their studies as well as heavily applied prescribed literary sources for their figurative reading; they finally become \"hunters of prototype,\" famously criticised for leaving matters there, and not exploring them further. while the hunting of the prototype has been seen as an infatuation, from which many, fortunately, have recovered, it also helped deliver a methodology which, even if criticised, has not yet found a real challenger or an alternative [cit] . we are talking specifically about the work of panofsky, which helped establish the discipline of art history as we know it now, and it helped investigate those visual cues that we use to identify representations."
"regarding the specific context of erp systems, different collections of criteria for evaluating an open source system were proposed. some approaches generically regard erp systems, other ones are specifically referred to floss erps. birdogan and kemal propose an approach for identifying and grouping the main criteria for selecting an erp system [cit] ."
"-number of cpus, since having multiple cpus does not always correspond to a proportional increase in the system throughput; from left to right we show deployment strategies with an increasing level of flexibility and therefore increasing cost-saving potential -load balancing, since balancing the load among multiple vms does not always correspond to a proportional increase in the system throughput with respect to using a single resource of the same type; -availability, since a spot instance has a possibility to be lost and become unavailable for some time."
the gqm paradigm helped defining a quality model for floss projects and a framework to be effectively used during the evaluation of a software system. it considers the quality of a floss project as synergy of three main elements: quality of the product developed within the project; trustworthiness of the community of developers and contributors; and product attractiveness to its specified catchment area.
"\"oriented therefore by a system of previous knowledge but trying to coordinate it with what they were seeing, they must have soon worked out a perceptual judgment. an animal has appeared before us that seems like a deer but isn't. likewise, they must not have thought that each spaniard was riding an animal of a different species, even though the horses brought by the men of cortes had diverse coats. they must therefore have got a certain idea of that animal, which at first they called macatl, which is the word they used not only for deer but for all quadrupeds in general\". [cit] what is interesting from this passage is not just the recognition of the nature of a horse as an animal, but the difficulties that such collective recognition impose on the exchanges between the messengers and the emperor. the messengers integrated their description with pictograms and performances, aiming to describe not only the form of this new animal, but also its behaviour. after listening to them, the emperor had formed an idea of the macatl his messengers were talking about, but it would probably have been different from the one in the minds of his messengers. nonetheless, it was accurate enough to allow montezuma to talk about a macatl, to be able to recognise one and differentiate it from the spaniard riding it. moreover, he was probably able to recognise not only the single macatl, but the entirety of them as a single species, even if they had differences in colours, size or carried armour. gradually, he was able to acquire more and more knowledge about this macatl, about its usefulness in battle as well as its behaviour and origin (earthly or divine for example). finally, the aztec started using a specific word for it, modifying the spanish word caballo into cauayo or kawayo. the story above presents an interesting perspective of the perceptual process, specifically of the recognition and categorisation of new objects, and it allows us to see how, on the basis of the object's characteristics, we produce an idea of a percept. in his analysis, eco [cit] calls this idea cognitive type (ct). in this case, the ct would be the concept that an aztec used to recognise a horse as an exemplar of its kind. after having seen some horses, the aztec would have constructed a morphological schema of it, which comprised not only an image-like concept of the horse, but included its peculiar characteristics such as the neigh, its motions, its capability of being mounted and perhaps even the smell. these elements are the base used for creating the ct of the horse, which appears to be then a multi-sensory idea of what we see."
"the ontology proposed here is not exempt from limitations. the contextualisation of ct in the initial analysis was supported by the use of situation and situation type. the introduction of situation and situation type is indeed possible in rdf, but it would require the constant use of reification, which would drown the usefulness of the ontology in order to pursue an unnecessary purity. another solution would be the creation of the class situation, which would involve a set of physical entities in a definite configuration. a representation would then conform/not conform to the situation where the visual recognition takes place. the possibility of using situations for differentiating between the meanings of the visual would greatly help in those performance types where the use of the visual element is strongly symbolic. similar achievements could be done with a connotation. however, the data demonstrate that, for now, the desire is small for such a complex structure, and a bigger effort from the community is required before a real contextual model could take place. moreover, the necessity of grounding a visual recognition within a situation would considerably raise the complexity of both the recording and the querying of the data, without really great advantages from a practical perspective."
"-closed 2673 -deleted 36 -open 9 figure 2 shows the distribution of the bugs for their states at the date the data have been collected. it suggests that compiere has not only a low number of bugs, but it also has a very small number of open bugs. it can be noticed from figure 2 that the number of bugs that are in the state accepted is greater than that one of the bugs which are open. this difference is due to the number of bugs that have been accepted but not yet assigned to any developer."
"initially, no princess was involved, other than in a unique case and in a very different role, in the church of panagia tou moutoulla, cyprus, where the saint killed a crowned woman with the body of a snake [cit] . it was only in the 12th century that the laceration on the wheel and the other torments started to be replaced by the rescue of the princess. the origin of this iconographical type can be traced back to a georgian manuscript dated 11th century, and it is, indeed, in georgia that we can detect the first representation of saint george saving a princess from a dragon [cit] . it is important to underline that the depiction of saint george slaying a dragon does not have any privileged uniqueness, because several other characters were famously depicted killing a dragon [cit] . within christian imagery, many saints slew a dragon. the most famous ones are st. andrew, st. matthew, st. philippe and st. michael. however, they are not the only privileged ones, and many more can be listed. 1 the ontology is freely available, together with its documentation, from https://w3id.org/vir/ (supplementary materials)."
"this section describes the changes that have been introduced in effort for evaluating some elements that characterize the reliability of a software system. specifically, as it is described in the following, the main characteristics that have been taken into account for the analysis, regard the assessment of the external quality, community and short-term support offered by the developers. then, the study focused on the analysis of the available data project, regarding bugs, patches and releases. specifically, the fundamental aspects observed by the effort framework have been investigated and expanded with some factors that were not previously considered. in particular, the analysis presented in this paper analyses the following parameters:"
"we consider an environment that has r +1 available resource types. type 0 is a special virtual type used to represent unal-located resources that have zero price and zero rate. each resource is characterized by a certain rate (processing speed) and a certain number of processors. moreover, by using historical traces we can also associate to each resource a bid price for obtaining a good compromise between the level of availability, and the actual price that we expect to pay when bidding such bid price. a possible way to estimate a bid price for each resource will be discussed in sect. 5 . more details on the application parameters are described in fig. 2b."
"having outlined the gist of it, it is best to start formulating a formal analysis, because only through their definition can we comprehend their role in the perceptual process. a sm is the result of a semiotic process which works with three components:"
"in this experiment we vary the number of dialog users in the system from 1000 to 10,000. we fix a slo that consists of a maximum average response time of 80 ms and a maximum 80th percentile of the response time distribution equal to 320 ms. by looking at fig. 11 we can see that both our approach and the exact one tend to have a price that grows proportionally with the number of users across different zones and oses. the total number of queueing network evaluations tends to be similar for different number of users: in the case of our heuristic we have the convergence at around 30 evaluations, while in the exact approach we often reach the cap of 100 evaluations that has been set to keep the comparison fair. interestingly, we can see that the execution time is not proportional to the number of evaluations. the reason for this is that the actual time for one queueing network evaluation is proportional to each assignment of software server to a cloud resource. our heuristic intentionally reduces the level of fragmentation of the assignments to the resources to reduce the overhead due to load balancing, while the exact approach explores too many alternatives that include situations with a high level of fragmentation (i.e., software servers are assigned to a high number of rented resources)."
"the map in figure 6 of the relationship between the allegory of the immaculate conception by vasari, as held in the church of santi apostoli in florence, and the preparatory study made by the artist, helps us visualise the structure of this relationship. the two representations are linked together by a type of visual prototype, in this case, a \"studio for.\""
"the use of semantic marks to construct the cognitive type of visual items can be further examined throughout the lenses of art history, a discipline which has closely studied the history of representations and has provided us with some of the finest thought on the subject, thanks to major works by warburg, panofsky, gombrich, arasse and other scholars. in doing so, we follow eco's suggestion that iconography and iconology can be considered a fully formed chapter of semiotics [cit], as well as the thought of some other art historians who have noticed the congeniality of the analysis of peirce and saussure with the study of riegl, panofsky and schapiro [cit] ."
"3. choosing the allocation of the application components to the resources in this step we decide how to allocate the different application components into the rented resources to minimize the negative effects of allocation (e.g., the reduction in performance due to load balancing, as it happens in the third deployment example in fig. 1 ). 4 . analyzing the overall system and possible scaling-up of bottlenecks the performance of the overall deployed system is analyzed again taking into account the overhead added by the presence of multiple cpus and load-balancing. this is also the step in which we consider the effects of the random environment in terms of possibility of losing spot instances and replacing them with on-demand instances in case the chosen bid price is overbid. if this analysis shows that the chosen resources and allocation do not fulfill the quality requirements anymore, the application ecu requirements of the bottleneck software servers are increased to compensate, and new resources/allocations are decided."
"evaluation-matrix (http://evaluationmatrix.com) is a platform for comparing management software systems. open source erp guru (http://opensourceerpguru.com/2008/01/08/10-evaluation-criteria-for-open-source-erp/) is a web site offering a support to the users in the identification of an erp open source solution to be adopted in their organization. reuther and chattopadhyay performed a study for identifying the main critical factors for selecting and implementing an erp system to adopt within a sme [cit] . this research was extended by zirawani, salihin and habibollah, that reanalyzed it by considering the context of floss projects [cit] ). wei, chien and wang defined a framework for selecting erp system based on the ahp -analytic hierarchy process -technique. [cit] the analyzed models result to be quite heterogeneous, but they have the common goal of identifying critical factors for the selection of erp systems. the birdogan and kemal model is the most complete one. criteria considered from the highest number of models regard functionality, usability and costs, followed by support services, system reliability and customizability. this paper considers all the analysed limitations of the previously proposed quality models and uses them for enhancing the effort framework. in particular, not all the quality models adequately consider the reliability characteristic. therefore, the effort framework was evolved for considering that aspect."
"goal 1 is defined as follows: analyze the software product with the aim of evaluating its quality, from the software engineer's point of view. almost all the attributes of the questions reference regard the iso 9125 standard. this goal is analyzed by considering different six sub-goals concerning: portability, maintainability, reliability, functionality, usability, and efficiency. for reasons of space, table1 just shows the first two sub-goals and related metrics."
"the availability a(b) is calculated as the probability for not being in state 3, which is the only state in which the resource is not serving requests. the actual hourly cost c(b) is calculated as the sum of the costs for having a spot instance active plus the sum of the costs for having an on-demand instance active (o, defined in sect. 3, is the fixed on-demand price for the resource)."
"if we had to make a parallel, following table 1, we would say that an ic1 iconographical atom corresponds to the notion of datum, or the recognised physical container which is the subject of an assignment of status. the ic9 representation would instead correspond to the determination of the representation on the basis of the cognitive type and the semantic marks associated with it, which partially correspond with the iconographical description outlined by van staten."
"should use a membership function type which takes as an input the value of a similarity-based degree calculation. however, similarity is not, as it is commonly understood, a juxtaposition between two anatomically similar elements, but a more complex phenomenon. nevertheless, it is possible to map the correlation between elements in a multi-quality dimension, including, depending on the case, topological, feature, alignment or value information [cit] . the value information is based on different qualitative criteria, such as material property, colour, size or reflectance. for example, two representations portraying two different subjects could be grouped together if both had a golden background, or if the objects portrayed had the same size; topological information relating the closeness of two or more objects in a specific space reference; a local space, such as a portrait where two dots stand in proximity to each other, or a geographical space, such a country or a town. feature similarity implies the presence of a few distinctive features that are considered more salient than others by the viewer and are taken as a key for grouping some objects. this could be the case of wearing a hat with a feather or carrying a latin cross. in both cases, we use these elements to say that two objects are similar. the alignment similarity indicates the likeness of one or multiple parts of an object in respect to one or multiple parts of another object. it implies the possibility of juxtaposing the two parts together. we will not provide an indication as to which membership function should be chosen (gaussian distribution function, the sigmoid curve, quadratic and cubic polynomial curves etc.), because the methodology depends on what kind of similarity information is being taken into account. for a full account of the methods, refer to great commentary on the subject given by timothy j. ross [cit] . at last, having determined that the relationship between situations is a physical thing and its ct (for functional purposes, logically expressed as type), we have all the elements for defining the semantic mark of an object, which we define as a tuple:"
the goal is to decide a resource assignment vector t and an allocation matrix d that minimize the sum of the prices of all rented resources. a formalization of the optimization problem is the following:
"support tools with reference to the enhancement requests. finally, with reference to the attractiveness of the project, and the question related to the diffusion degree of the product, metric m 3.2.12 has been added for considering the integrated number of downloads made in the last quarter of the analysed timeline. this metric very important as it shows the interest degree that the project community has with reference to the considered software project."
"a post-hoc multi-comparison test was run alongside the friedman test to return the pairwise comparison results, and the results are shown in table 5 . the first two columns of table 5 show the groups that are compared. post hoc analysis was conducted with bonferroni correction applied. bonferroni adjustment was applied on the results because multiple comparisons are performed, and to reduce the likelihood of declaring a result as statistically significant when they should not be declared as such (a type i error). the fourth column shows the difference between the estimated group means. the third and fifth columns show the lower and upper limits for 95% confidence intervals for the true mean difference. the sixth column contains the p-value (adjusted after bonferroni correction) for a hypothesis test that the corresponding mean difference is equal to zero."
"as a haloed, beardless knight in different poses and scenes. initially painted standing in his military attire, he was later represented in various scenes such as the laceration on the wheel, the resurrection of the dead and the destruction of idols, which are strongly linked to his biography and his legends [cit] . while, currently, the most widely known iconographical type is surely \"saint george and the dragon,\" which portrays the saint slaying a dragon and saving a princess, it was only from the 10th century that he started to be represented on a horseback killing a dragon (figure 2 for a small overview of saint george iconography). initially, no princess was involved, other than in a unique case and in a very different role, in the church of panagia tou moutoulla, cyprus, where the saint killed a crowned woman with the body of a snake [cit] . it was only in the 12th century that the laceration on the wheel and the other torments started to be replaced by the rescue of the princess. the origin of this iconographical type can be representations of dragon slayers can be found in other mythologies. the small sculpture of horus on horseback, for example, depicts a scene very similar to the one of saint george slaying the dragon. this small sculpture portrays the egyptian god horus the moment before he stabbing the deity setekh/set, the egyptian god of the desert, who adopted the form of a crocodile to escape his nephew [cit] . even restricting ourselves to the christian imagery of saint george, many are the works of art depicting the saint, comprising many different perspectives, stories, characters and stylistic choices. only focusing, as is currently done in the visual classification domain, on description or annotation of the iconographical type (e.g., saint george slaying the dragon) would result in the creation of an all-encompassing description which includes a wide range of different characters and variations."
"ontologically speaking, the perceptual challenge revolves around two main subjects: the existence and the identity of a visual representation. it is necessary to define the subject of the percept, as well as to understand the mechanism used for the percept to be referenced as an instance of a class."
"many quality models for evaluating floss systems have been proposed in literature [cit] . nevertheless, they do not cover all the relevant aspects of software quality and working context of the evaluated software systems and are not always applicable to the specific context. an evaluation of these models is provided in, and the obtained results highlight that one of the characteristics that is not evaluated is the software reliability."
"we model the application as a closed queueing network qn of m software servers (representing the application compo- fig. 2 system parameters nents), a delay node (representing user think time), k classes of requests, and a set of constraints on the response time that we defined as service level objective (slo). a detailed list of application parameters is shown in fig. 2a ."
"unfortunately, eco does not explain in detail how cognitive types do work, or how we can use this grammar to relate sense data to a semantic and conceptual model in order to achieve a similarity-based recognition. therefore, it is essential to build up from his theory and define how we construct the identity of an element, correlating visual data to nuclear characteristics. in order to analyse this process, we introduce the concept of the semantic mark (sm). we define a sm as internal encoded functions which help classify external stimuli and discern their nature. sms are sense based and help classify the perceptual experience by correlating perceived signals to the ct of a situation, and to the ct of a physical thing. both sms and cts are based on equivalence-based criteria between the percept and a situation/object which are similar to. further recognitions are achieved by a similarity-based degree of the newly perceived sms and the sms that characterise a previously constructed ct. sms function as attributes of the identity of a percept. the number of signals received by the senses can be numerous, but the chosen ones that are used for the identification are fewer, and they present themselves as constituents of a perceptual manifestation. while a sm can be seen as another type of sign, it is instead an encoding of the percept on the basis of a classification, which reuses our experience and social ground for determining the significance of our reality."
"then, it has been performed an analysis aimed at investigating the behaviour of the community, especially to identify the bugs discovered and not yet assigned to any developer. it has been observed that in compiere 570 bug are unassigned and 2148 bugs are assigned to at least one developer. while, the number of developers assigned to at least one bug is 19."
"after the iconographical analysis, the methodology of panofsky passes over to iconological analysis, which comprises the socio-historical interpretation of the symbolic value of the painting, which is part of a bigger cultural visual history and is not a conscious process for the author. the indeterminacy of these symbolic values created some significant issues in the art historical community, because sometimes the use of symbols was strongly driven by the author's intention (as in 16th century dutch art for example). in order to overcome these issues, and to stay true to the idea that an author can use symbolic representation consciously, we prefer to adopt the revised scheme of van straten (figure 1 ) [cit] . van straten does not challenge the first pre-iconographical phase of analysis, focused only on the identification of the artistic motifs such as lines and shapes, but he concentrates instead on identification of the secondary subject matter and the intrinsic meaning. the iconographical analysis is divided into iconographical description (second phase) and interpretation (third phase). the iconographical description is the analytical phase, where the subject of the representation is established (for example \"saint george and the dragon\") but deeper meaning is not searched for. in this scheme, we can attribute an iconographical description to all works of art, in contrast with the analysis of panofsky, which recognises the possibility of assigning a secondary subject matter only to a limited set of works of art (landscape, for example, could not be iconographically analysed)."
"the approach proposed, allows description of diversity in interpretation, as well as the rationale used for the classification of visual items and their interconnection with other objects that share the same symbolic meaning or refer to the same personification/phenomenon."
"for example, queueing networks can be automatically generated from uml or palladio component model diagrams [cit] . the problem of executing the decisions, such as concretely migrating the virtual resources is out of the scope of this paper, which focuses on the decision problem."
"sometimes a representation with a clearly denoted identity can develop a different one within the same context. this process is called connotation. visual semiotics discern denotation and connotation as two different layers of meaning, where denotation expresses what is being depicted and connotation expresses the values and ideas of what is represented [cit] . the connotative layer has been the subject of many studies, with very diverse interpretations about its nature. barthes, for example, thought that there is no encoding/decoding function within the denotative layer because our object recognition originates from some form of \"anthropological knowledge\" [cit] . while appealing, this description seems to be tip-toeing around the subject, explaining a significant feature of the perception process using a fuzzy concept. we prefer the definition given by hjelmslev [cit] of a \"semiotics whose expression plane is a semiotic,\" so a function that relates the content of a signification to the expression of a further content. for these reasons, in this ontology we model the connotation as a relationship between the already established representation and a conceptual object, and not as a new relationship between a representation and an iconographical atom. the persistence of the connotation is transitory, because connotations are founded only on code convention and time-wise are less stable than denotation, because their duration is influenced by the stability of the convention itself."
"the effort framework was evolved to include software reliability aspects. this was done with the double aim of having the possibility of better analysing the software product quality, and understanding how an open source community is careful and reactive to the management of the open source project and its problem resolution. the evolution required the analysis of the reliability characteristic and accessible data in the available repositories."
"-a heuristic, called optispot, to jointly solve the bidding and allocation problem, which are in general np-hard; -what is, to our knowledge, the first application in the area of bidding of extended queueing network models that include a model of the operational environment. the latter, which is referred to as random environment model [cit], captures the stochastic nature of the operational environment, in which vms can be lost and restarted as a result of spot price fluctuations and the consequent temporary switch to an on-demand pricing model. -the use of advanced fluid analysis techniques to accurately approximate response time percentiles, which are commonly used to constraint performance in servicelevel agreements, but which are usually hard to compute in queueing networks. compared to more complex approximations for accurate percentile assessment, such as laplace transforms, this method is fast enough for runtime application."
"the depictions of the saint are quite heterogeneous, and display him in very diverse situations. spanning from the sixth century to this very day, representations of saint george have depicted him as a haloed, beardless knight in different poses and scenes. initially painted standing in his military attire, he was later represented in various scenes such as the laceration on the wheel, the resurrection of the dead and the destruction of idols, which are strongly linked to his biography and his legends [cit] . while, currently, the most widely known iconographical type is surely \"saint george and the dragon,\" which portrays the saint slaying a dragon and saving a princess, it was only from the 10th century that he started to be represented on a horseback killing a dragon (figure 2 for a small overview of saint george iconography)."
"at this point, we passed to analyze the average time for resolution of the bugs. this is particularly important for understanding the behaviour of the community. table 7 reports the medium value of the resolution time, measured in days, for the analysed project with reference to the levels of priority and state of the project, on the basis of the data collected in sourceforge. table 7 shows that the resolution time decreases, as the bug priority increases, and this is something to be expected. moreover, it is possible to observe that the resolution time increases when the project is stable and mature and this is justified by the higher complexity of the project at that level of maturity."
"the representation of an object can be of different form and nature, can underline one specific aspect or feature, can have different degrees of faithfulness and can use different grammars to encode the same type of information. the purpose of these representations is not to display and interact with a simulacrum of the heritage object, but to actually use them as instrument of analysis, to make statements about the world through them, to find metrics to compare objects, study their behaviour, subdivide them into units, reconstruct their holes, re-define their existence, put them into their historical context, study how they interact with their ecology and how they shape it, or how they could be and how they were not. representations of this type are not treated as simple depictions, but are instrumental to the knowledge we derive from our past, because we have the tendency to assign them the status of digital counterparts of heritage objects."
"the general idea of our approach is to decompose the main problem into simpler subproblems that are solved in an iterative way. each subproblem obtains its input from the solution of the previous subproblem, as shown in fig. 3 : the numbered blocks in the figure represent the subproblems we solve. our approach is then repeated at regular intervals as a method of pro-active self-adaptation, or in response to unexpected situations that cause a run-time slo violation as a method of reactive self-adaptation. a general idea of each subproblem we address is described as follows, while details are given in the next subsections. 1. choosing the minimum computational requirements for each application component in this step we decide the minimum computational requirements in terms of resource rates (e.g., amazon's elastic computing units, or simply ecus) that are needed by each application component to satisfy the quality requirement. at this stage we do not consider the available resources, but we just determine the ecu requirements of the application. 2. choosing the resources to rent in this step we calculate the bidding price that minimizes the cost for each unit of rate (e.g., 1 ecu) and, based on it, we decide which resources to rent. the sum of the ecus of the rented resources should be large enough to fulfill the ecu requirements of the application decided in the previous step."
"when applying svd on a matrix, its performance depends on the number of k dimensions chosen. choosing a larger number of dimensions than needed can result in including noise in the dataset, and choosing too few dimensions may remove important information."
"the article presents an overview of a functional theory of perception, defining the nuclear terms used in the classification of visual objects, and re-proposing some of the identified structures and process within a new ontology, called vir, which extends cidoc-crm for describing the diverse type of representations of and relationships between visual features. specifically, the ontology provides the possibility of defining relationships between prototypical objects used within a visual composition, iconographical objects, attributes of the representations, layering of diverse representations, compositionality, subject matter, personification, illustrations of a scene and others. examples are provided for the core elements of the ontology, in order to explain both its use and rationale."
"overall, table 8 reports the average time, expressed in days, to assign a bug to at least one developer. it can be noticed how the assignment time is kept nearly constant for each bug priority, regardless the project state. a shorter time is used for assigning the bugs with priorities 9 and 1. the quick assignment of bugs with priority 9 was expected, while the one regarding the bugs with priority 1 was perhaps due to the ease to find a solution. the allocation time for all the degrees of maturity of the product, is higher when the software is mature. this can be caused by the complexity and criticality of the bugs, and this generally makes the resolution complex and needing more experienced developers, who are not always active."
"-transition 2 to 3 the termination notice is expired and therefore the spot instance is no longer available. -state 3 the resource is not available, it is being started as an on-demand instance, but it is not ready yet to process requests. on-demand price is paid. -transition 3 to 4 the on-demand instance is now ready to receive requests. this transition can happen instantaneously in the case the time needed to start the on-demand vm (odstartuptime) is not higher than the termination notice time (termnoticetime). to avoid the possibility of having a non-positive period in the ctmc for this transition, we force a lower bound equal to eps (the smallest positive number that can be represented). -state 4 the resource is available as an on-demand instance. on-demand price is paid. -transition 4 to 5 the bid price has been underbid, so it is possible to start a spot instance again. -state 5 the resource is available as an on-demand instance, although a spot instance is currently starting. both spot price and on-demand price are paid. -transition 5 to 1 the spot instance is now ready to receive requests and the on-demand instance is terminated. -transition 5 to 4 the spot instance has been overbid before being fully started. so it is immediately terminated since an on-demand instance is still active."
"results of the analysis of the community trustworthiness regarding goal 2 are reported in table 10 . the results indicate that the compiere community does not appear to be very trusted, especially with reference to the offered documentation. however, it is necessary to specify that not all the documentation is freely available, and, therefore, it was not considered in the analysis. in any case, it can be stated that the company compiere inc. did not significantly \"suffer\" for the lack of interest of the developers."
"one of the earliest approaches to the analysis and interlinking of both knowledge representation and iconographical art was made by d'andrea and ferrandino [cit] . in the article, the authors attempted to map and reuse concepts from both cidoc-crm (cidoc conceptual reference model) and the d&s (descriptions & situations) extension of dolce, developed by gangemi and mika [cit] in order to extract image meaning, using d&s to add an information layer about the context and state of affairs of a specific representation. while the work was quite promising, no further development has, unfortunately, been recorded."
"the purpose of these graphs is to provide to the reader with an example of the applications of the developed ontology, as well as to display how the information present in just a small portion of a wall painting, if correctly described, can help create an information-rich environment that opens the doors to new ways to document a heritage object in relation to its functional and visual context. for easy reading, we specify that the letters k and ic represent, respectively, the properties and the classes of the vir ontology colour-coded in the graph in purple and orange, while the letters p and e, colour-coded in blue and green, represent the properties and classes belonging to the cidoc-crm ontology."
the link between visual representations and historical information within a formal structure that can be queried is the first step towards achieving a true digital iconological framework able to correlate visual culture and symbolism used.
"the remainder of this paper is organized as follows: section 2 describes the related works; section 3 reports a description of effort; section 4 describes the extension of effort for evaluating the reliability; section 5 discusses results obtained by applying the extended framework to a case study conducted on an open source erp system. finally, section 6 presents the conclusions."
"furthermore, the effort framework has been extended with the addition of a new question. specifically, table 6 reports this kind of extension. the added question has been defined with reference to goal 2. it is related to the level of efficiency of the developers in relation to the bug resolution. metrics m 2.6.1 and m 2.6.2 are evaluated in terms of days and represent the reactivity of the community developers to the errors. the last two metrics are related to the developer activity in the context of the bug management."
"influence on the production of the painting of both the donor and the frankish occupation of the island. figure 7 documents the integration of the aesthetic information within the historical framework of production. the creation of the painting is linked in time with the lusignan occupation of the island, from 1192 till 1474. the period is, moreover, linked with two other spatio-temporal gazetteers, perio.do 2 and chronontology 3, which help in retrieval and also in the browsing and visualisation of further documented periods."
"the third goal has the purpose of evaluating the attractiveness of the product toward its catchment area. the term attractiveness indicates all the factors that influence the adoption of a product by a potential user, who perceives convenience and usefulness to achieve his scopes."
"once data have been collected by means of metrics, they cannot be directly aggregated and compared because they have different scales. then, it is necessary to normalize them. the paper uses the min-max normalization and the values have been mapped to one-five scale. the guidelines for choosing the mapping ranges have been defined on the basis of the experience and information coming from the literature. this approach makes to lose the granularity of the information, but it is needed if a comparison is required. a more punctual evaluation can be performed by considering the effective values of the metrics. therefore, the normalized values are aggregated, according to the interpretation of the related metrics, so that one can obtain useful information for answering the questions. in particular, the following issues needs to be considered:"
to solve this problem we propose an algorithm that finds an approximate allocation by allocating the rates of the software servers having the largest non-allocated rate to the real resources having the largest available capacity in an iterative process until the rates of all software servers have been allocated.
"a specific type of representation that is necessary to mention and describe here is personification, identified in vir by the identifier ic11. the class comprises anthropomorphic figures, which symbolise and represent abstract ideas. widely used within the arts, personification appears in both byzantine and western traditions and is considered a typical communicative device to represent intangible concepts such as fortune, fate, prudence and other allegories. another typical use of personification that still survives today is that of national symbols: anthropomorphic figures that embody a nation and its values (e.g., marianne for france). figure 5 presents a map of information on the personification of the sea present in the narthex of the church of asinou, cyprus. the relationships described in figure 5 are quite similar to the ones used for the description of a representation, but, in this case, the symbolic link with a conceptual object is made explicit. as for the representation of saint george, the characteristics of a personification can also be shared by similar representations, so it is crucial to link the semantic information with external reference resources. in figure 5, for example, both the symbolic object and the personification are linked, with wikidata and iconclass, respectively."
"in order to annotate how the interpretation of a visual item works, we introduced the class ic12 visual recognition, which defines the act of recognition and interpretation of the subject matter of a representation."
"the descriptions of the ontology and the examples have been, until now, dedicated only to the description of the visual, overlooking another essential component in art, the historical aspect. in order to accurately describe a representation as a product of its time and space, and bind it to specific traditions or visual culture, it is essential to ground the visual information into a bigger historical framework. this approach enables users to focus not only on aesthetic attributes, but also on the development of symbolic forms or characters within a period. the church of asinou proves to be again an excellent example to explain the importance of such practice. the top part of the south conch in the narthex of the church hosts the panel \"virgin of mercy and latin donor,\" an iconographical type original from the west. this iconography, called madonna della misericordia, originated in italy in the early 13th century and was promptly disseminated in the mediterranean area by the crusaders [cit] ."
"in our approach we consider the most complex case and also consider that the deployment decision is not only affected by the size of the cloud resources, as in the deployment example above, but it should take into account also additional real-world characteristics that may affect the overall system performance:"
"while text-based objects have received a great deal of attention during recent years, little work outside the machine learning domain has been made in regards to images. as a result, the work of normalisation and integration of visual information continues to rely on old paradigms and practices. tools for the semi-automatic classification of a 2d/3d object, for finding duplicates in a collection or for assigning names to an artefact have been developed. nevertheless, most of these projects did not spend very much time reflecting on the significance of a representation, and on which basis we classify and interpret them. for such reasons, it is essential to reflect on the relationships between reality, person and image, analysing and integrating overarching theories developed within semiotics, art history, digital humanities and information science. following this objective, in this article we attempt to construct an inter-disciplinary framework of understanding in order to outline a theoretical model for meaning assignment to visual objects, as well as to construct an information model capable of recording it. the result is two-fold: [cit], 2 1192 theory of visual interpretation, while from a functional perspective, we construct a formal ontology for recording the image classification act."
"we argue that the interpretation of a visual message requires the connection of the sense data to a model using a specific visual code. the sense data themselves are selected portions of the continuum of reality, which floridi [cit], building on the work of mackay [cit] and bateson [cit], calls a datum."
"for example, assume that, after analyzing the performance model of the application and the expected load, we need vms with different computational requirements (expressed as amazon elastic computing units, ecus) for the application server and the database server. then, we have a very large decision space on how to deploy them in a cloud infrastructure if we have multiple types of resources characterized by different prices and speeds, such as in amazon ec2. figure 1 shows four examples of deployment characterized by an increasing level of deployment complexity. in the first deployment, we make the most intuitive decision, that is to choose the two cheapest resources that can fit the two vms of the application. in the second deployment we can take advantage of cheap large resources by deploying multiple vms inside a single large cloud resource. in the third deployment we can take advantage of cheap small resources by replicating application vms into multiple cloud resources with the help of a load balancer. finally, in the last deployment we can choose the cheapest vm of any size by combining the two previous deployment approaches, thus obtaining the highest degree of flexibility and cost-saving potential."
the problem of finding the real resources to rent (step 2) is np-hard and solved using an approximated ilp solver. the convergence and the complexity of this step therefore depends on the ilp solver used and its parameters. in this step no queueing network evaluations are performed.
"-overbidtime(b) mean time before the bid price b is overbid (i.e., an active spot instance is reclaimed by the cloud provider). -underbidtime(b) mean time before the bid price b is underbid (i.e., a previously reclaimed spot instance is available again). -spotcost(b) average cost for an active spot instance, when bidding b. the difference between this cost and c(r ) is that the former only considers the use of spot instances, while the latter considers the possibility for a spot instance to become an on-demand instance when the bid price is overbid, which is the actual cost incurred by the user. -a(b) expected availability of the resource when bidding b. this function estimates the percentage of time the resource is able to process requests."
the third match feature model is concat. this is a general model that can learn any weighted combination of the values of the two vectors:
recall that we employ unsupervised pretraining of representations for g-phrases. we can either freeze these representations in subsequent supervised training; or we can fine-tune them. we study the performance of both regimes.
"in contrast, for the clause coherence task, concatentation worked well and directsim worked poorly and we provided an explanation based on the specific properties of clause coherence (see discussion of figure 7 ). we conclude from these results that it is dependent on the task what the best feature model is for matching two linguistic objects. interestingly, indirectsim performs well on both tasks. this suggests that indirectsim is a general feature model for matching, applicable to tasks with very different properties."
"we evaluate paraphrase identification (pi) on the pan corpus (http://bit.ly/mt-para, [cit] ), consisting of training and test sets of 10,000 and 3000 sentence pairs, respectively. sentences are about 40 words long on average."
"multigrancnn bears resemblance to previous work on clause and sentence matching (e.g., [cit], but it is more general and more flexible. it learns representations of g-phrases, i.e., representations of parts of the textchunk at multiple granularities, not just for a single level such as the sentence as arc-i does [cit] . multigrancnn explores the space of interactions between the two chunks more exhaustively by considering interactions between every unit in one chunk with every other unit in the other chunk, at all levels of granularity. finally, multigrancnn supports a number of different match feature models; the corresponding module can be instantiated in a way that ensures that match features are best suited to support accurate decisions on the textchunk relation task that needs to be addressed."
the first match feature model is directsim. this model computes the match score of two gphrases as their similarity using a radial basis function kernel:
figure 7 (top table) shows that [cit] 's parameters are good choices for our setup as well. we get best result when both gpcnn and mfcnn have three blocks of convolution and pooling. this suggests that multiple layers of convolution succeed in extracting high-level features that are beneficial for clause coherence.
"our second contribution is that multigrancnn contains a flexible and modularized match feature component. this component computes the basic features that measure how well phrases of the two chunks match. we investigate three different match feature models that demonstrate that a wide variety of different match feature models can be implemented. the match feature models can be swapped in and out of multigrancnn, depending on the characteristics of the task to be solved."
we use dynamic k-max pooling to extract the k l top values from each dimension after convolution in the l th block and the k l top values in the final block. we set
"suppose the triple (x, y +, y − ) is given and x matches y + better than y − . then our objective is the minimization of the following ranking loss:"
"prior work that has addressed matching tasks has usually focused on a single task like qa [cit] or paraphrasing [cit] . [cit] are intended to be more general, but seem to be somewhat limited in their flexibility to model different matching relations; e.g., they do not perform well for paraphrasing."
"our motivation is that for a textchunk relation like clause coherence, the two textchunks need not have any direct similarity. however, if we map the representations of textchunk s 1 into an appropriate space then we can hope that similarity between these transformed representations of s 1 and the representations of textchunk s 2 do yield useful features. we will see that this hope is borne out by our experiments."
"in remaining parts, section 2 introduces some related work; section 3 gives an overview of the proposed multigrancnn; section 4 shows how to learn representations for generalized phrases (gphrases); section 5 describes the three matching models: directsim, indirectsim and con-cat; section 6 describes the two 2d pooling methods: grid-based pooling and phrase-based pooling; section 7 describes the match feature cnn; section 8 summarizes the architecture of multigran cnn; and section 9 presents experiments; finally, section 10 concludes."
"for many tasks, labeled data for training gpcnn is limited. we therefore employ unsupervised training to initialize gpcnn as shown in figure 2 . similar to cbow [cit], we predict a sampled middle word v i from the average of seven vectors: the textchunk representation (the final output of gpcnn) and the three words to the left and to the right of v i . we use noise-contrastive estimation [cit] for training: 10 noise words are sampled for each true example."
"paraphrase identification (pi) is a typical task of sentence matching and it has been frequently studied [cit] . [cit] utilized parsing to model the hierarchical structure of sentences and uses unfolding recursive autoencoders to learn representations for single words and phrases acting as nonleaf nodes in the tree. the main difference to multigrancnn is that we stack multiple convolution layers to model flexible phrases and learn representations for them, and aim to address more general sentence correspondence. [cit] claimed that elementary discourse units obtained by segmenting sentences play an important role in paraphrasing. their conclusion also endorses [cit] 's and our work, for both take interactions between component phrases into account."
"where s(x, y) is the predicted match score for (x, y). we use stochastic gradient descent with adagrad [cit], l 2 regularization and minibatch training. we set initial learning rate to 0.05, batch size to 70, l 2 weight to 5 · 10 −4 ."
"thanks to cis members and anonymous reviewers for constructive comments. this work was supported by baidu (through a baidu scholarship awarded to wenpeng yin) and by deutsche forschungsgemeinschaft (grant dfg schu 2246/8-2, spp 1335)."
"different match feature models may also be required by factors other than the characteristics of the task. if the amount of labeled training data is small, then we may prefer a match feature model with few parameters that is robust against overfitting. if there is lots of training data, then a richer match feature model may be the right choice. this motivates the need for an architecture like multigrancnn that allows selection of the taskappropriate match feature model from a range of different models and its seamless integration into the architecture."
"then, we study the performance variance of different multigrancnn setups from three perspectives: a) layers of cnn in both unsupervised (gpcnn) and supervised (mfcnn) training phases; b) different approaches for clause relation feature modeling; c) dynamic pooling methods for generating same-sized feature matrices."
q + pdc will also almost certainly reignite speculation about release dates of microsoft 's new products. q − pdc is indifferent to the release of longhorn.
"our unsupervised learning component (section 4, last paragraph) resembles word2vec cbow [cit], but learns representations of textchunks as well as words. it also resembles pv-dm [cit], but our textchunk representation is derived using a hierarchical architecture based on convolution and pooling."
"the output of dynamic 2d pooling is further processed by the match feature cnn (mfcnn) as depicted in figure 6 . mfcnn extracts increasingly abstract interaction features from lower-level interaction features, using several layers of 2d wide convolution and fixed-size 2d pooling."
"since pi is a binary classification task, we replace the mlp with a logistic regression layer. as phrase-focused pooling was proven to be optimal, we directly use phrase-focused pooling in pi task without comparison, assuming that the choice of dynamic pooling is task independent."
"in this paper, we present multigrancnn, a general deep learning architecture for classifying the relation between two textchunks. multigrancnn supports multigranular comparability of representations: shorter sequences in one textchunk can be directly compared to longer sequences in the other textchunk. multigrancnn also contains a flexible and modularized match feature component that is easily adaptable to different textchunk relations. we demonstrated state-of-the-art performance of multigrancnn on paraphrase identification and clause coherence tasks."
"for parameter selection, we split the pan training set into a core training set (core) of size 9000 and a development set (dev) of size 1000. we then train models on core and select parameters based on best performance on dev. the best results on dev are obtained for the following parameters: freezing g-phrase representations, direct-sim, two convolution layers in gpcnn, no convolution layers in mfcnn. we use these parameter settings to train a model on the entire training set and report performance in table 2 ."
"multigrancnn is based on two innovations that are critical for successful textchunk relation classification. first, the architecture is designed to ensure multigranular comparability. for general matching, we need the ability to match short sequences in one chunk with long sequences in the other chunk. for example, what is expressed by a single word in one chunk (\"reignite\" in q + in the figure) may be expressed by a sequence of several words in its paraphrase (\"fan the flames of\" in p). to meet this objective, we learn representations for words, phrases and the entire sentence that are all mutually comparable; in particular, these representations all have the same dimensionality and live in the same space."
"we briefly discussed [cit] 's arc in section 1. multigrancnn is partially inspired by arc, but introduces multigranular comparability (thus enabling crosslevel matching) and supports a wider range of match feature models."
the output of the last fully-connected layer contains more advanced semantic information than the convolutional features that are frequently used in other cross-modal tasks [cit] . we leveraged features in this layer to better capture the semantic information for topic modeling.
"3) end of a video chunk transmission: after a video chunk is successfully transmitted from the upload peer to audience, the server records the interest group j g, the upload peer k and his upload bandwidth kj y . the bank server records every trade and deposits the rewards in the account of the seller k and draw points from each accounts of the buyer."
"the purpose of this study is to improve the regression modeling aspect of kbp. empirically, different regression methods perform well in different scenarios, such as different number of training cases, presence of collinear features, and presence of outlier cases. in this work, we develop an ensemble learning method to combine the strengths of these individual models and improve kbp model robustness."
"another kind of incentive is based on virtual currency. the system first establishes an account for each user, then it charges users for every download and rewards them for every upload. researchers have devised many virtual currency incentive mechanisms, such as micro-payment [cit], dandelion [cit], scrip [cit], pace [cit] and so on. compared to barter-exchange, the virtual currency incentive mechanism enables data trade between any two peers hence saves the time of searching for proper counterparty. but in the incentive mechanisms mentioned above, the system has to track every occurred trade and modify the users' account balance accordingly, which adds extra load into the system."
"clinical treatment planning varies from case to case, with different sparing and coverage considerations. with the aforementioned kbp framework, we assume a linear model can successfully represent a majority of training cases. for some cases in the database, this assumption does not hold. we refer to these cases in the training dataset as outlier cases. in this section, we shall present our insight on outlier cases and provide an intuitive explanation of effects of outliers on knowledge-based modeling."
"we observe that the ensemble method consistently predicts better than or similar to the best performing individual model in every challenging situation. with improved robustness, the proposed regression method potentially enables end users to build site-specific, physician-specific, or even planner specific models, without manually screening the training cases. this eventually will allow each practice to build models that accurately reflect their own optimal oar sparing preference and capability, thereby eliminating the need for a universal model. figure 8 shows an example of improved prediction accuracy of the proposed method, compared with other individual models. in this case, stepwise and ridge perform poorly while lasso and elastic net perform reasonably well, and the ensemble model outperforms all individual models. note that in different situations, different models perform well, and the proposed model performs most consistently. improved dvh prediction accuracy usually results in better plan optimization guidance (i.e., optimization constraint generation), since it provides the treatment planning system correct information of the best achievable oar sparing without compromising ptv coverage."
"the experimental results are shown in table. 2. our proposed dtca achieves the best performance among all approaches. compared with state-of-the-arts, these results proof that the feature extraction strategy of dtca will be more applicable to multi-modality data in social media. surprisingly, m-dbm does not perform as well as expect, which might be due to the generative problem and feature extraction limits of its two channels. most of alternative approaches proposed in this paper also outperform state-of-the-arts, which implies the important role of deep network based structure towards the feature extraction of high-semantic information. results of combinatorial approaches also confirmed our assumption mentioned in previous section and give some interesting findings. bi-rnn-only greatly outperforms sg-only, which indicates that the bidirectional rnn used in our work is capable of learning the high-semantic feature of a tweets' text well. notably, the performance of vgg19-only is better than that of bi-rnn-only, which subverts the common assumption that the text content is more discriminative than the visual content. this result may be due to the informal and short format of tweets and the limit number of training data. the result of vgg19+bi-rnn validates merits of the cca based fusion scheme applied in our model. conclusively, deep network based channels play key roles towards highlevel semantic information extraction, and the cca-based fusion scheme in our work is efficient. fig.6 (left) reflects the relation between the accuracy and correlation rate. from the figure we can see that as the correlation increases, the accuracy increases and reaches the top with a correlation value between 0.7 and 0.8. after that, it starts to decrease. this result indicates that maximizing the correlation can help the learning of joint representations, while the difference between two modalities should be also maintained in some cases. fig.6 (right) reflects the relation between the accuracy and the dimensions of the joint representations. as shown in this figure, joint representations with too small dimension size are unable to capture information of two modalities well. when the dimension is too large, the joint representation contain more noises and results in a decreased classification performance."
"collaborative filtering system comparison is user preference description file, evaluation of a series of projects that the user as a vector, thus forming a matrix, by comparing the user describe the similarity degree between the vectors to form a similar user groups, and use of other users in the group to a higher evaluation of the project recommendation to the group is not on the project evaluation target user."
"towards learning the joint representation over multiple modalities, directly combining different types of feature vectors is infeasible due to the different scales, resolutions and dimensions. in addition, the direct aggregation of different types of feature vectors can lead to the classification boundary to be biased towards the modality with higher feature dimensions. thereby, correlation analysis is necessary in topic modeling, which can effectively integrate the deep features of different modalities by maximizing their relevance at a higher semantic level."
"this paper introduces the case based reasoning technology, the composition of each user preference vector evaluation of some project as a case, case attributes set by the user evaluation of each item weight composition. because of the similar user groups tend to assume a similar behavior, evaluation of fewer projects have many missing values, but some users to evaluate the, such as assuming that the user has to this project had evaluation, can be selected in the case of some attribute search and the user the target user evaluation of similar weight is the corresponding property."
"in this section we would describe our incentive mechanism more specifically. we first briefly introduce the system structure of the p2p vod system, then propose the pricing schemes of our incentive mechanism, and at last illustrate the dynamic mechanisms of the system."
"now we analyze the nash equilibrium of the system. inequality (8) shows that the increase of upload rate can bring positive growth of a user's utility, so a peer has incentive to upload as much as possible in order to maximize his utility, thus the largest upload rate peer i could achieve denoted by action max( ) i y dominants other actions. inequality (9) states that the more users download, the better utility they can get, thus the maximum download rate of peer i denoted as max( )"
"from table. 3, it can be seen that dtca outperforms all compared methods with a great margin, which is within our expectations. it is worth noting that other neural networkbased models, such as m-dbm [cit], do not improve the performance significantly compared to tagprop [cit] and multikernel svms which are base on traditional machine learning approaches. the reason can be twofold. first, all these compared methods use hand-craft features as visual and textual inputs, and those features might be valid for simple object recognition tasks but less capable of capturing high-level semantics which is essential in social topic modeling. second, shallow neural models are still inefficient under noisy and complex social data. in flickr, those user-generated examples indeed contain noisier and more complex information compared with traditional recognition tasks, not matter in terms of images or tags. the experiment results proof the shortage of these methods. conclusively, these experiments once again confirm the validity of our motivation and prove the effectiveness and efficiency of our model."
"in previous studies, it has been pointed out that automatic outlier removal requires further investigation (12, 23) . we propose to incorporate a model-based automatic outlier removal routine in the ensemble model to ensure model robustness and address the volatile nature of clinical data. we utilize the cross validation metadata native to the proposed ensemble method to identify and remove impactful dosimetric and anatomical outliers. the two scenarios of outliers have different impact on the training of regression models, as we illustrate in this section. note that by our definition outliers only exist in training sets, all cases in testing sets are predicted. cases that would be defined as outlier cases if they are in a training set can still be predicted by a trained model, but with less accuracy. these special cases can be identified with the same approach as we identify outlier cases (see model-based case filtering method), and case-based reasoning can be used to improve the outcome of treatment planning, but that is out of the scope of this study. we aim to improve prediction accuracy of the kbp framework with a different modeling technique, without significant changes to the overall workflow."
"at the beginning, the p2p system was just used to share files, recently the p2p media streaming systems [cit] are obtaining a growing number of viewer population in a short period of time. the media streaming system has characteristics that different from traditional p2p file sharing system. firstly, in a media streaming system, the peer with an advanced playback point can provide data for a behindhand peer but the latter has nothing to reciprocate, so the traditional barter exchange incentive mechanism is not suitable. secondly, the real-time characteristic of the media streaming requires that the action of peers' searching, trading and data transmission as quick as possible, and the extra load the incentive mechanism brings into the system should be as light as possible. thirdly, a media streaming system usually contains thousands of video files, and users scatter into these numerous video channels so it is difficult for them to cooperate [cit] ."
"pricing scheme is a basic issue of constructing a monetary system: we should first decide what to price and then how to set the price. as we discussed in section i, the key problem of a p2p vod system is to alleviate the burden of the server without degrading the playback quality of the peers. a light load of the server means a stable and economical system. so we want to construct a pricing scheme which can represent the contribution or burden of a peer bring to the server, which means that actions offloading the server would be rewarded and increasing server's burden would be charged accordingly."
"when the topic correlation of two modalities are maximized, f i or f t can be used as the joint representation. in this paper, we concatenate two types of features into the overall joint representation f j . in terms of topic classification, we use the cross entropy as the cost function:"
"the modeling and detection of topics in document corpus have been extensively studied over the past decades. in the literature, there exist several classic topic modeling models such as probabilistic latent semantic indexing (plsi) [cit] and latent dirichlet allocation (lda) [cit] as well as their variants."
"robustness to anatomical outliers figure 5 shows prediction errors, measured by wrmse, of individual models and the ensemble model. for bladder predictions, the ensemble model outperforms all individual models, while stepwise, lasso, and elastic net perform similarly. in the case of rectum predictions, the ensemble method again outperforms ridge, lasso, and elastic net, and performs similarly well as stepwise. ridge regression fails to predict accurately for either task."
"in radiation therapy, high quality treatment plans are crucial for reducing the possibility of normal tissue complications while maintaining good dose coverage of planning target volume (ptv). for intensity-modulated radiation therapy (imrt), it is especially important to fully utilize the healthy tissue sparing potential enabled by the advanced treatment delivering system. however, the optimal achievable organ-at-risk (oar) sparing is not known pre-planning, and planners need to rely on their previous experience, which makes the planning process subjective, iterative, and susceptible to intra-and inter-planner variation."
"rule based system in general is the user preference and information resource using the same set of keywords, they allow the system administrator to set the rules according to the static characteristics of a user and dynamic attribute, a rule is essentially a if-then statement, rules determine in different situations such as how to provide different services, the utility model has the advantages of simple, the disadvantage is difficult to guarantee the quality of the rules, and cannot update rules, with the increase in the number, the system will become more and more difficult to manage."
"topic modeling on social media data from students can measure the instantaneous thoughts and willings, and has been touted as a non-intrusive method for smart campus development. smart campus aims to enable intelligent, accurate, and customized education models for diverse students in the context of knowledge explosion. the cornerstone is to continuously monitor and analyse the status and activities of various students, which is subtle due to privacy protection [cit] . fortunately, social media services, a prevalent way for information dissemination and sharing among students, provide a ideal method to gather data from students and mine their real thoughts (i.e., topic modeling)."
"examples of correct and incorrect classification results are shown in fig.7 . from the correct results, we can find that dtca has been able to learn high-level semantic features between two modalities. the incorrect examples imply the ambiguity problem of twitter data. take the incorrect instance of ''film'' column as an example, the visual content is very like a screen-shot of a film and the textual content did not present an obvious tendency. hence, the labelling of such kind of examples heavily depends on individual recognitions and personal knowledge backgrounds. nevertheless, it is noticeable to find that dtca is capable to detect inaccurately labelling examples, for instance, the incorrect example of ''politics'' column which are labelled as ''music'' topic for it is from the 'rollingstone' account."
"the cosine measure looks at the angle between two vectors of ratings where a smaller angle is regarded as implying greater similarity. the adjusted cosine is used in some collaborative filtering methods for similarity among users where the difference in each user's use of the rating scale is taken into account. (b) the number of n-best neighbor top-n technique in a predefined selection. producing the recommendation since we have got the membership of user, we can calculate the weighted average of neighbors' ratings, weighted by their similarity to the target user."
"the p2p vod system comprises two main modules: servers and peers. the server is responsible for providing a centralized management of the system, providing video content, and playing the role of the central bank in our incentive mechanism. on the other hand, peers in the vod system can start watching a movie at any point of time, with small start-up times and sustainable playback rates. the structure of the p2p vod system is shown in fig.1 . as fig.1 shows, there are multiple video programs in the system. all the peers watching or sharing the same video file constitute a video interest group. in an interest group, peers can be further subcategorized into two categories: the audience peers which are in the course of watching the video, and the set of peers named upload peers which have already watched the video and just share the video data for the audience peers. considering that in mvc, one peer can watch some video program and at the same time store and share multiple video programs, so a peer can participate in multiple video interest groups. for example, in fig.1, peer k is an upload peer of interest group j g, and also an audience peer of interest group f g ."
"note that this numerical demonstration isolates the effect of outliers on regression on a single feature, and it simplifies the influence of outliers on the overall modeling process. in our clinical knowledge-based modeling, we extract nine features from each case to construct the feature vector x. however, not every feature contributes to the final model equally. in stepwise regression, relevant features are picked based on correlation with the outcomes variable (i.e., dvh pcs). in penalized regression methods, features are implicitly selected with less relevant features given very small regression coefficients as a result of the penalty term. the feature selection step, while not considered here, is also affected by outliers. when anatomical outliers are involved in the training process, the features selected are potentially different from the set of features selected, if the model is trained without outliers."
"overall, the contributions of this paper can be summarized as follows: first, we address the two key challenges of multimodal topic modeling on social networks which are modality independence and modality missing. second, we propose a novel topic detection scheme, termed dtca, with two innovative designs, namely, the filter gate component and the matrix-projection based component. third, a large-scale multi-modal dataset is proposed for topic detection on social media data."
"we analyze the relation of the utility function with these variables one by one: the increasing of download rate i x results in small start-up delays and sustainable playback rates which brings positive utilities; while uploading files consumes the limited network resources of the user which decrease user's utility; in the vod system, downloaded files which have been played have little value for users and occupy the disk storage. so we assume that the utility function is continuous, strictly increasing in download rate i x and strictly decreasing in the upload rate i y and disk usage amount i d ."
"cantly challenged the existing multi-modal topic prediction approaches, most of which have a key assumption that all modalities are completed and they are highly correlated in low semantic level. [cit] . therefore, it has become an emerging task to design new schemes towards cross-modal topic detection for data from social media platform like twitter and weibo."
"weighted root mean squared error measures the overall deviation of predicted dvhs from ground truth dvhs, which are clinically planned. weightings are introduced to emphasize higher dose regions of dvhs, which are generally considered to be of more clinical significance in oar dose predictions. here w nw w i j n j"
"it is noted that some new mechanisms, such as attention mechanism [cit], are left unexploited to make our model more compact. the main purpose of this paper is to demonstrate our capability in handling both modality independence and modality missing by designing a novel cross-modal correlation analysis module, while other cutting-edge components in respective modalities can be further integrated."
"mir-flickr is a large scale social multimedia dataset which contains one million images collected from the social photography site flickr. in our experiments, its sub-dataset flickr-25000 is used as the benchmark. flickr-25000 contains 25,000 images labeled into 38 classes, such as bird, sky and indoor and so on, and each image can have multiple labels. among those images, 22,489 images have social tags made by users on flickr, and those tags will be used as text information in our experiments. following [cit], we create five random splits of the 25,000 examples into train, validation and test sets. in each split, we use 10,000 images for training, 5,000 images for validation and 10,000 for testing. we perform multi-label classification task on this dataset, and use mean average precision (map) as the performance metrics. the experiment results are averaged on the five splits."
"in this section, we test the performance of our model on the topic classification task for social multimedia data. two datasets are used in our experiments. the first one is released in this paper, of which data is collected from twitter using the public twitter api. here we term it as topic modeling twitter, tm-twitter. the other is multimedia information retrieval (mir) flickr [cit], which is a conventional benchmark for multi-modal assignments. based on these two datasets, we compare our model with state-ofthe-arts, and the experiment results validate merits of our approach."
"as the key design of the proposed scheme, the filter gates are implemented in the deep visual and textual channels, which targets at identifying and evaluating the modality dependency."
"at present, there is much information filtering model. a typical information filtering model should include general information input source, user model, user interface, filtering algorithm, as shown in figure 1 ."
"when a new peer joins in the system, the bank server opens an new account and offers a freshman subsidy to help the new member downloading and watching the first video as a seed capital. the amount of the subsidy should be carefully designed: if it is set too small, the new peer could not afford to download a complete file, while if the subsidy is too large, whitewashers which leave the system and re-join the system as a new id would take advantage of the policy and simply make no contribution to the system. a reference value to set the subsidy is the median value of all the video files' market watching price."
"in clinical practice, planners do not necessarily have many cases for every treatment site. this is particularly true when a new treatment technique, such as simultaneous intensity boost, is recently utilized in the clinic and the existing model built for existing treatment techniques may not predict the achievable dvh accurately due to the oar sparing capability difference. sometimes models need to be built when only a small number of cases (~20) are available. it is critical that the regression model is capable of resisting overfitting the random variation of training cases. in this experiment, 166 prostate ptv cases are retrospectively retrieved from the clinical database. twenty prostate cases are used as the training set, and the remaining 146 cases are used as validation set to quantitatively evaluate the prediction accuracy of each model."
"in this paper, we first analyze the features of the p2p vod system and point out that the fundamental problem of the vod system lies in the heavy load of server and the difficulty to cooperate among peers caused by the random action and vcr functions of the peers. then we propose a new lightweight incentive mechanism based on currency. a peer gets paid if he shares the video file he completely downloaded with other audience peers in the process of watching that video, and if the peer watches a video, he has to pay money to the upload peers together with other audience peers in that video interest group. we neglect the data exchanges among the audience peers and only record the data exchanges between upload peers and audience peers. such pricing scheme can stimulate peers to share the video files downloaded completely which have a more stable performance. we demonstrate through game theoretical analysis that the lightweight currency-based incentive mechanism can lighten the burden of the server and increase the cooperation of peers."
"many ensemble models have been proposed over the years in the field of machine learning, such as random forest (17), boosting (18), bagging (19), and stacking (20) . the basic idea behind these ensemble models is to develop an array of simple models, often referred to as base learners, and combine these models to form a better (e.g., lower variance, higher accuracy, or both) model for prediction (21) . these models essentially seek to combine knowledge learned by different models via data resampling and/ or adding another layer of optimization."
"conclusively, both modality independence and modality missing do exist in twitter-like microblogs. due to the page limit, we skip the same quantitative validation in sina weibo, where the same conclusion still holds."
"collaborative filtering technology has proved to be one of the most effective for its simplicity in both theory and implementation. the paper gives an electronic commerce recommendation algorithm combining case-based reasoning and collaborative filtering. firstly, it uses case-based reasoning to fill the vacant ratings. then, it produces prediction of the target user to the target item using collaborative filtering. the presented algorithm combining case-based reasoning and collaborative filtering can alleviate the sparsity issue."
"building models for different treatment sites may face different challenges. for example, the number of cases required to train a model may be different. the more complex head and neck cases require more training cases to well represent the case population, while prostate cases have fewer oars and are generally easier to train. second, different treatment strategies are often used to treat different sites. for example, some sites require multiple ptvs while other sites require hard constraints. last but not least, the amount of intrinsic variance in head and neck cases are more than that of prostate cases due to potential trade-off considerations. as a result, dataset characteristics vary from treatment site to treatment site and individual model performances vary correspondingly. the ensemble model ensures the best performing model gets the highest weighting. all in all, each treatment site should be treated differently in kbp to get the best possible prediction accuracy, and the ensemble model helps to reduce the amount of effort required in terms of model selection. ideally, the ensemble method should be trained for each treatment site, since data characteristics change from dataset to dataset. however, if there are two datasets from two treatment sites with very similar characteristics, such as dvh variability, number of cases, then it is possible to re-use the model weight αk * directly. the main limitation of the proposed approach is the training time. two major components of knowledge-based modeling are feature extraction and model training. the feature extraction part of the proposed model takes on average 5 s for each case, and feature extraction is done only once. model training takes less than 10 s for each individual model. in the proposed model, individual model training is repeated by the number of component models times the number of in-model cross validation. as a result, in our hardware setup, it takes less than 10 min to run a single regression model, and it takes 30 min to run a 20-fold cross-validated ensemble model. the prediction procedure is very simple and takes less than 1 s to calculate therefore, once a model is calculated, it can be easily stored and applied to dvh predictions."
"the primary motivation of our ensemble model is to make kbp more robust and adaptive. in different settings, different regression models perform well, and none of these individual models consistently performs better than other models. for instance, stepwise regression is widely known to be unstable (22), but as shown in section \"results, \" it can significantly outperform other more stable models such as ridge regression in certain settings. however, it is not feasible to test out individual models every time a new model is fit. therefore, we propose an ensemble model, which performs well in all settings."
"we know that the peers in the p2p system are selfish and rational, so we deem each peer in the system as a player of the game. we also assume that every player's belief about the other players' strategies is correct. in a vod system, the parameters a peer can decide includes: which video group to join in, the download and upload rate, and the usage of disk space to get a maximum of his utility. other network parameters such as bandwidth, delay and jitter are also very important, we suppose in this paper that the delay and jitter are in a negative correlation with the download bandwidth so we can use the bandwidth to represent the other performance parameters for simplicity."
"in our bi-rnns, the processing cell 1 used is gated recurrent unit(gru) [cit] . gru can be considered as a simplified version of long short term memory (lstm)ï¡ [cit] which is 1 typically, a recurrent neural network is a generic term for a network that uses a recurrent structure. the data processing units represented as rnn cells used by different rnns. shown in fig.5(a) . it deletes some components of lstm such as the forget gate f and the memory cell c. this simplification reduces the training complexity, while largely retaining the discriminability of the lstm [cit] ."
"recommendation system based on preferences of the user's personal information; take the initiative to recommend personalized service to the user of electronic commerce. through the research on professional characteristics, intelligent user interest analysis, resource information may need to be recommended to the user, realize the personalized service of information. when the system load is small, the use of filtering algorithm based on improved case, the lack of the accuracy of recommendation information when the user evaluation, as shown in figure 2 ."
"there are several similarity algorithms that have been used in the collaborative filtering recommendation algorithm: pearson correlation, cosine vector similarity, adjusted cosine vector similarity, mean-squared difference and spearman correlation."
"the rating of the target user u to the target item t is as following: electronic commerce personalized recommender systems represent services that aim at predicting a customer's interest on information products available in the application domain, using customer s' ratings on products. peoples' experiences often do not enough to deal with the vast amount of available information. thus, methods to help find products of electronic commerce have attracted much attention from both researchers and vendors. in this paper, an electronic commerce personalized recommendation method using case-based reasoning is given. the method employs the case-based reasoning to fill the vacant and then uses collaborative filtering method to recommender."
"denotes the normalized weighting factor for bin i of dvh curves. for evaluation of dose to bladder and rectum, we use the linear relative weighting wj of 50-100 linearly increases from 0 gy to prescription dose. for evaluation of dose to parotids in head and neck cases, wi is set to gaussian centered at median dose, with sd of 2 gy. if wi is set to a constant number, then wrmse reduces to standard rmse."
"the advantages of collaborative filtering is to discover new information of interest to the user, the disadvantage is the existence of two difficult problem, one is sparse, both systems use early, because some system resources are not enough ratios, the system is very difficult to make use of fewer evaluations to find similar users, on the other one is the scalability of the system, with the increase of users and resources, the amount of computation will be more and more, so that the performance of the system will be more and more low."
the current state h j is computed by eq.13 where the represents the element-wise product. then the output and the next state h j is updated based on information of the previous and current state through eq.14:
"the procedure of data processing in the backward rnn ← − f is the same as that of forward rnn as shown from eq.11 to eq.14. we then collect and concatenate the final outputs of the forward and backward rnns as the final textual representation of a given tweet, as shown in eq.10. such representation contains semantic information of both individual words and the entire tweet."
"the ensemble method outperforms all individual methods significantly, as shown in figure 4 . note that ridge regression performs particularly poorly in bladder prediction, indicating that there is some intrinsic sparsity in the feature space, and ridge regression, which does not utilize that sparsity, underfits significantly due to over-shrinking of regression coefficients. stepwise performs poorly in rectum predictions, due to overfitting."
"forward selection, a type of stepwise regression, is the last individual model. it finds the most significant features to add based on the data step by step, hence the name. when adding features no longer improves the model by a certain preset p-value threshold, the feature selection step terminates. the selected features are fitted to the data with ordinary least square, while the rest of the features are discarded."
"moreover, we would also take isp-friendly into considerations as our next work. we would study how cross-isp traffic can be reduced while maintaining the performance of the vod system. another idea is for the isps to join in the system and get paid according to the link utilization of the network."
"inferior plans figure 6 shows, for both bladder and rectum prediction, lasso, elastic net, and the proposed ensemble regression method predict equally well, while stepwise and ridge are no longer usable due to significant amount of error."
"in step (iii) of the previous framework, stepwise regression is used to select features and estimate the linear model. the method automatically picks several most important features step by step based on the significance of features. this approach is easy to implement and the output is interpretable. with careful training data preprocessing and feature selection, stepwise has achieved good results in oar dvh prediction in research settings (6) (7) (8) (9) (10) (11) (12) . however, there are some theoretical issues about this procedure, which could potentially result in some instabilities of the overall model training process. while stepwise regression has been very successful in the context of kbp, potential disadvantages of stepwise regression are well documented. first, it potentially suffers from overfitting if the size of the training set is relatively small compared to the number of features. this is because the procedure attempts to fit many models and the p-values, which are used as feature selection criteria, are not corrected for the number of hypothesis tested. in addition, stepwise regression does not cope with collinear features well. if two features are highly collinear, stepwise usually selects just one and discard the other. ideally, if several collinear features are predictive of the outcome, all of these features should be selected to prevent overfitting and reduce model variance."
"as a comparison to our proposed ensemble model, we study four individual regression models, including ridge regression (13, 14), lasso (15), elastic net (16), and stepwise regression with forward feature selection. these models also serve as base learners for the final ensemble model. the latter three models share the same objective function"
"in clinical databases, not every previously treated case is helpful for predicting future cases even when the treatment plans are of high quality. if the anatomical features are very different from the majority of all cases than the linear assumption may not hold, as demonstrated in figure 2, and the anatomical features are potentially detrimental to the model. to simulate the effect of anatomical outliers on the plans, we train a model with 10 prostate cases treated with lymph nodes and 40 prostate cases treated without lymph node. the trained models are subsequently validated with 111 cases that do not involve lymph nodes."
"for our future work, we would consider to bring differentiate service levels into the system, for example, peers with higher points could enjoy the service of higher resolution of videos. differentiated service could further stimulate the peers in the system to share more in order to get better service."
"in this section, we will analyze our lightweight currencybased incentive mechanism from a game theoretic perspective. we continue to use the notations listed in section ii."
"in this experiment, we first replace the last softmax layer with a sigmoid output layer. since the experiment task is the multilabel classification, the sigmoid output layer will compute the probability that an example belongs to a specific class, which is denoted as:"
"through empty user rating calculations using case based reasoning, obtained the complete user rating. and then, the collaborative filtering algorithm to generate a user rating prediction, we use based on user."
"we observe that in a video interest group, the upload peers play a very important part in saving server's bandwidth: an interest group with no upload peer would definitely needs the server to transmit the video data. also, compared with an audience peer, we get that the former is superior to the latter. for an audience peer, it is possible that the peer has vcr actions, and probably cannot finish the file. so we suppose that the contribution of the upload peer is more than the audience peer when the upload bandwidth is the same."
bilateral parotid sparing to get better prediction accuracy (24) . we retrieve 228 bilateral parotid-sparing head and neck cases and 10 single-side parotid-sparing cases from our institutional clinical database. the sparing decisions are first obtained from clinical prescription documentations and subsequently checked in dose statistics to correct for decision changes. we randomly select 80 bilateral cases as the training set and then add 10 single-side sparing cases as mis-classified cases. the remaining 148 bilateral cases are used as the validation set.
"the detailed process of a request is as follow: a peer i chooses a video interest group j g to join in as a audience peer. the peer i sends a request for a certain chunk of video to the server, and the server replies i with a list of the peers in the video group and i chooses from the list the audience peers which can offer data for him. if there is no suitable audience peer, peer i searches the upload peer list. and if no upload peer can afford him, peer i has to request for the server to transfer the data for him."
"knowledge-based planning (kbp) (1) (2) (3) (4) (5) has been shown to be a powerful tool for guiding planners and physicians to optimal achievable oar dose volume histograms (dvhs) based on previous cases planned by experienced planners. in a previously proposed regression-based kbp framework (2), the workflow is as follows: (i) principle component analysis (pca) is conducted for oar dvhs in the training set, and the first three principle component scores (pcs) and corresponding basis vectors are stored; (ii) pre-determined geometry information related to treatment planning goals, also referred to as features, are calculated for each patient; (iii) pcs of oar dvh are fitted to features to generate a prediction model; (iv) features are calculated for new patients; and (v) best achievable oar dvhs are calculated for new patients using the fitted model and the previously calculated pca basis vectors."
"currency control of the system is also an open problem. if users own too much currency, they could easily get high quality services hence have less incentive to share, while if the currency is in a shortage, some peers would be too poor to afford any video and become bankruptcy. in our future work, we would like to use macroeconomic theory to analyze the influence of currency to the system dynamics and manage the currency in the system to control it in a suitable level."
"however, to the best of our knowledge, the existing work on cross-modal topic detection still retains on a straightforward assumption, as discussed in sec.i and demonstrated in sec.iii. therefore, such scheme is not applicable to topic detection for cross-modal microblog data."
"in this paper, we proposed a deep topical correlation analysis approach towards topic modeling in microblog-based social media, with the objective to track students' thoughts and serve the development of smart campus. what makes it different from conventional works are twofold: the topical feature extractions among modalities are separated, and only highlevel features will be used for data fusion; applying the cca based fusion scheme alone with a novel gate design makes dtca achieve a better performance in joint representation learning. in addition, dtca is also capable of tackling with the modality missing by using either retrieval methods or the non-linear matrix projection. our experiments confirm that dtca is a competitive approach for multi-modal data learning on social media data. in our future works, we will reduce noises of our dataset and extend it. meanwhile, we will also test our model on other tasks, such as the tweets annotation and the tweets retrieval."
"peer-to-peer (p2p) is very popular in the internet because of its powerful sharing and distributing ability. users share files or resources with each other in a p2p system, but they have strong motivations to be free riding if the system lacks of suitable incentive mechanism that stimulates sharing. such behavior is observed in existing peer-to-peer systems, for example, the study of gnutella shows that 85 percent of users share no files [cit] ."
"quantitative study was conducted to validate the modality independence and modality missing in preliminary. to that effect, we randomly collect 4,000 tweets from twitter with topic keywords of ''sport'', ''film'', ''music'', ''health'' and ''politics'' as similar to those used to build our benchmark dataset in sec.v to quantify the degree of modality missing, three cases are measured, i.e., the text-missing, image-missing and textimage missing. the last case denotes the missing of both visual and textual modalities, which typically only retains location tags or emotions. the result is shown in the fig.2 (a), from which it is clear that over 50% of sampled tweets have the problem of modality missing. we further measure whether the modalities are dependent to each other by using the following rule: if a figurative concept is presented in both visual and textual channel, modalities in this tweet depend on each other, otherwise they are independent of each other. a figurative concept can be any object or scene. as shown in fig.2, it is not surprising that the independent ratio is very high, reaching a ratio of 81.6% in the test set."
"most of the p2p vod systems require each user to contribute a small amount of storage instead of only the playback buffer in memory as in the p2p live streaming systems. in this case, users can upload videos which have been downloaded before when they are watching another different movie, or just help other video channels which are short of uploading resources by downloading that video and relay it if there is room on the hard disc. this is referred as multiple video cache (mvc) strategy [cit] . compared with single video caching (svc), mvc could greatly improve the cooperation and alleviate the burden of the server [cit] ."
"in this paper, we propose a new incentive mechanism for p2p vod systems based on virtual currency. in our system, the server plays the role of the central bank and keeps an account for each user. peer gains money if he shares a video file he has already completely downloaded before, and the audience members watching that video would together chip in to pay for the upload peer. we do not take into consideration the data exchange among the audience members watching the same video. neglecting the trade among audience peers lightens the burden of the bank server and also encourages users to share the video files which they have downloaded completely."
"the first type of outliers is anatomical outliers. in this study, we define anatomical outliers as cases with anatomical features that are distant from normal cases, and possibly come from a different distribution. in kbp, anatomical outliers refer to cases with uncommon anatomical features relevant to dvh prediction, such as abnormal oar sizes, unusual oar volume distributions relative to ptv surface. generally, anatomical outliers are more likely to deviate from the linear model, as illustrated in figure 2, and when they do, the effect of these cases are generally larger than normal cases due to the quadratic data fidelity term (first term in eq. 1) of the regression model. therefore, it is necessary to identify anatomical outlier cases that are detrimental to model building and remove those from the model before training. other than anatomical outliers, there are cases that are detrimental to model building due to limited oar sparing efforts and/or capabilities. these are considered to be dosimetric outliers in this work. dosimetric outliers include, but are not limited to (1) treatment plans with inferior oar sparing and (2) wrongly labeled data, such as 3d plans mixed in imrt plans."
"in game theory, a game consists of such components: (1) a set of players; (2) a set of strategies for each player; (3) a utility function that gives the players' utility to each list of the players' strategies."
"under such incentive mechanism, collusion action is not profit. collusion peers perform upload and download in pairs to get award. but in our incentive mechanism, the pair's total income is zero if there are only these two peers in the interest group; if there are more than two audience peers in the system, the peer disguising as an audience has to watch and share the data with other audience peers, but the income is the balance between the reward and payment, which is less than the two peers just act as upload peers."
"our dataset consists of five categories that are ''sports'', ''music'', ''film'', ''health'' and ''politics''. we take samples containing both texts and images from public news accounts, such as ''bbcsport'' and ''cnnpolitics'', and tag them based on their account categories. each account only publishes the content of a particular topic, not multiple topics. in addition, we manually check the collected data and filter out inappropriate data, for example, the image contains only characters. finally, we have 30,000 examples and 6,000 examples for each class. for each class, we have 4000 training examples, 1000 verification examples, and 1000 test cases."
"to handle the issue of modality missing, we first present a most straightforward solution. taking the case of textual modality missing for instance, we use the visual features generated by cnn to retrieve the most similar image in the dataset, and then use its corresponding textual feature. in addition, we apply the proposed filter gate in sec.iv-a to denoise the textual information and maximize the correlation between both modalities. then the adjusted textual feature can be regarded as a supplement of the missing textual modality. the same operations can be used for dealing with the visual modality missing."
"in summary, we propose an ensemble regression model to address two problems that we are facing in kbp. first, different individual regression models perform well in different settings, such as different number of relevant features, number of cases, and existence of outliers. it would be very labor intensive to manually select the optimal model every time a model is fitted. second, to ensure the most accurate model training, data-preprocessing, including anatomical and dosimetric outlier removal, is also necessary for individual models, and it can be subjective to decide which subset of cases should be removed from the training set if done manually. the proposed ensemble model utilizes multiple individual models on the same set of data and uses constrained linear optimization on the metadata to obtain the optimal weight for each individual model. in addition, the model automatically filters out cases in the training set that are not predicative of future cases based on metadata."
"to further improve the robustness of the ensemble model, cases with the highest s% median (of all individual models) internal cross validation wrmse error are dropped from the training set. the percentage threshold s is selected to balance the tradeoff between model robustness and accuracy. empirically, we find that 10% is generally a good choice, even though the number of actual outlier cases is unknown and may differ from 10% of the total case number. all the experiments in the following section are conducted with the pre-determined 10% threshold. the workflow of the ensemble model with model-based case filtering is shown in figure 3 . note that the whole process is done automatically without manual intervention."
"in our incentive mechanism, the cost for uploading as an upload peer is less than the income it gets, and rational peers would only join in a video interest group if the utility of watching that certain video is worth its cost. so we have the inequalities below:"
"the proposed gate component is inspired by the recent progress in deep neural networks [cit] . it serves as a denoising function over the features of different modalities. in other words, the designed gate will dynamically determine what information and how much of such information is fed to the next step. concretely, the filter gate operation is defined as below:"
"author contributions jz proposed the model, conducted experiments, and wrote the first draft of the manuscript. qw oversaw the workflow of the study and contributed in the clinical aspect of the study. tx extracted and pre-processed data for the experiments in the paper. ys provided suggestions regarding the study design. f-fy provided critics in the experimental design. yg contributed advice in the statistical methods and revised the manuscript."
"besides, under such pricing scheme, a peer may want to use all his upload bandwidth to upload files which he has already had to maximize his income. in order to prevent such case, the system should force audience peers to upload for other audience with a certain minimum bandwidth."
"for kbp, it is crucial to get reliable predictions even in the presence of sub-optimal plans. here, we simulate the sub-optimal plans with dynamic conformal arc plans. compared with imrt plans, conformal arc plans have evidently inferior oar sparing capability. our training data consists of 40 prostate imrt cases and 10 prostate conformal arc plans, and the validation set includes 110 prostate imrt plans. the experiment is designed to test the model robustness in the extreme settings to evaluate the model robustness in challenging situations."
"in the following parts, we firstly address the two key issues of this paper, modality dependency and modality missing, in sec.iv-a and sec.iv-b. sec.iv-a introduces a novel gate component designed to capture the dependency between modalities, while sec.iv-b provides two approaches to deal with the modality missing. then we illustrate a cca-based topic modeling scheme in sec.iv-c. lastly, the implementations of the visual and textual channels are described in sec.iv-d."
"where f j is the joint representation learned by dtca, i and d indicate the image and text tags respectively. then the cost function in eq.7. can be replaced with:"
"possible future research topics include the optimal selection of models as well as the optimal number of models in the ensemble. in this study, we limit the number of models included in the training set to avoid overfitting. while too many models in the ensemble warrant overfitting the data, the current number of models (9) is very conservative. with the regulation of the nonnegative constraint, the proposed approach could potentially see further performance improvements if more models are included in the ensemble. we expect the optimal number of models in the ensemble to be dependent of the size of the dataset. in addition, the proposed methodology can be easily expanded to more complicated non-linear models. we use linear models in the ensemble due to the limitations of training dataset size. as more cases become available, more complicated models become viable."
"dosimetric outliers do not follow the same conditional distribution as normal cases and are expected to be easier to be identified with cross validation. increase of dosimetric outliers in training data tends to shift the overall model toward inferior plan dvhs and gradually make the plan less optimal (23) . in this section, we evaluate the robustness of individual models and the ensemble model with training set contaminated by two types of dosimetric outlier plans: (i) inferior dose sparing and (ii) mis-labeled sparing decisions."
where f can be a high-level semantic feature extracted from the visual or the textual channels. w g and b g are the weight and the bias of the gate. values of elements in g range from 0 to 1. the filtered features f can be represented as:
"notice that the price of upload bandwidth is a global uniform price, so the reward an upload peer gets only depends on the bandwidth he actually shares, and the reward has nothing to do with what video file he shares. while the payment of an audience peer depends on the number of audience in that video interest group. we can see from (4) that the payment decreases as the number of audience increases. this is easy to be understood: audience peers in a popular video usually have better cooperation between each other and needs less support by server. so the payment of watching a popular video is cheaper than an unpopular video."
"the data sharing between audience peers are also important, but the system would have to bear a very large cost if we record each trade and refresh the account balance of the buyer and seller accordingly. also, in an interest group, an audience peer could download data from other audience which have earlier playback points, and at the same time offers data for the peers with lagged playback points. if we consider in a long run, an audience peer shares as much as he gets in the system. so we ignore the data exchange among audience peers and only focus on the data transfer from the upload peers to the audience peers."
"the implementation of visual feature channel can be constructed based on any form of convolutional neural networks. considering the trade-off between the discriminability and the efficiency, we use the structure of vgg-19 [cit] in this paper. we follow the standard setting of vgg with 19 layers, while removing the last softmax layer."
"content based systems are the use of resources and user interest similarity to filter the information, the user preference description and resource description preference using the same representation. the utility model has the advantages of simple and effective, the disadvantage is difficult to distinguish the resource quality and style, and not for the user to find new resources of interest."
"with the rapid development of electronic commerce and internet popularization, the information technology has penetrated into every corner of our social life, is with hitherto unknown speed and the ability to change our life and working way, we are really in an \"information explosion\" era. in the face of the vast resources on the internet, people often feel be at a loss what to do, not know what course to take, the emergence of the so-called \"information overload\" and \"information lost\" phenomenon. in view of all kinds of the information, only to rely on manual methods to collect and collate the required information is clearly not enough. so, automatic collection and all kinds of information become a challenge and opportunity for development of information industry is facing consolidation needed. especially for the scientific research and teaching personnel speaking, with the increase of science and technology literature exists in electronic journals and online document form, how to make full use of existing network resources, from the electronic document complex in timely and accurately access the latest information technology and their research areas related to appear very necessary."
"this retrospective study uses anonymized clinical plan data and has received permission from duke university medical center's institutional irb. all clinical plans were planned using varian eclipse™ treatment planning system (varian medical systems, inc., palo alto, ca, usa). all experiments were performed on a pc with intel xeon e5-2623 cpu and 32 gb of ram running windows 10 enterprise 64-bit operating system. in order to quantitatively evaluate the robustness of these regression methods in various challenging clinical environment, we test the aforementioned models with limited training set size, training sets contaminated with anatomical outliers, and training sets contaminated with dosimetric outliers. in our outlier robustness tests, we purposefully mix pre-defined outlier cases into the training set and validate the final model with normal cases. the reason for adding outlier cases is to add controlled variation to the dataset and evaluate the robustness of the proposed model. details regarding types of data used in the experiments are summarized in table 1 ."
"to learn the deep textual feature, we apply a bi-directional recurrent neural networks (bi-rnns) based on gated recurrent units. as illustrated in fig.3, words in tweets are firstly converted to embedding vectors, which are then fed into bi-rnns one by one. in such a way, word information is propagated in two directions. compared to the traditional single-direction rnn, bi-rnns can obtain information not only from the previous words, but also from the following words. which therefore can capture the inter-word associations [cit] ."
"media streaming systems can be subdivided into two groups: live streaming and video-on-demand (vod). live streaming allows video content to be transmitted in real time to all requesting users. vod systems provide more flexible and convenient service to users by allowing them to watch any kind of video at any point in time. vod systems need to accommodate a large number of users watching the same video asynchronously, and watching different part of the same video at any given time. so the challenge of providing vod services with p2p networks lies in alleviating the burden of the server, which determines if the vod system can satisfy users' demand and keep stability as well as extensibility."
"in this section, we illustrate the effect of outliers on the overall regression model with one-dimensional simulated data. figure 2a shows that anatomical outliers follow the same underlying x-to-y mapping. however, the true underlying relation may not be well approximated by linear regression outside the normal x range. attempting to fit linear regression with anatomical outliers mixed in the training set will potentially deteriorate the model. therefore, the actual effect of anatomical outlier in different feature directions in the context of kbp needs careful assessment. figure 2b illustrates the effect of dosimetric outliers. dosimetric outliers in the training set are expected to increase model variance and deviate the model."
"the interpretation of each of the above diagrammatic elements is provided by their corresponding mapping functions. these functions take the above identifiers and return sets (or elements -for spiders) that correspond to them. figure 4 shows the map function for zones: fun zmap :: \"('e, 'a) sd_scheme ⇒ zone ⇒ 'e set\" where"
"we formalised the theory of spider diagrams in isabelle/hol (see section 3.1). this enabled sentential reasoning about diagrams and also simplified translation from spider diagrams to sentences (see section 3.2). translation from sentences to diagrams, proof automation, and proof reconstruction, however, is still work in progress. diagrammatic reasoning will be done in speedith, our own external reasoner, which is currently in development (see section 4)."
"the user will be able to invoke the diagrammatic reasoner at any time during the proof, with an option to do so in an interactive or fully automated mode. additionally, the currently active statement (lemma or proof obligation) will be automatically visualised as a diagram in an embedded window of the gui."
"in contrast to the approach of openproof our aim is not to keep the two reasoning systems separated, but to integrate them using heterogeneous representation and reasoning. in addition, we want to formalise diagrams and some of their inference rules in the logic of an lcf-style [cit] higher-order theorem prover. with this we aim to enable certified proof reconstruction of heterogeneous proofs."
"the goal of this project is to provide a proof of concept heterogeneous reasoner -to show that heterogeneous reasoning is feasible. ultimately, we plan to extend the heterogeneous framework to other domains with new diagrammatic systems (e.g.: constraint diagrams, uml, diagram chasing etc.)."
"our first step was to provide a formalisation of the theory of spider diagrams within isabelle/hol (files available from http://gitorious.net/speedith). this not only makes the translation between the two representations easier, but also allows for direct proof reconstruction within isabelle."
"heterogeneous reasoning was the next step in the development of diagrammatic reasoning systems. it merged the diagrammatic and sentential modes of reasoning and allowed proof steps to be applied to either diagrammatic, sentential or mixed formulae. in the paper reasoning with sentences and diagrams [cit], hammer laid the formal foundations for such heterogeneous reasoning systems."
"besides the above two main source parameters, the non-uniform slip rate time functions also provide various rupture delays and different seismic moment release in the finite fault. the various rupture delays reflect the change of the rupture velocities. the rupture delay can be obtained by the first-arrival picking technique. after the determination of all the source parameters, the ground motion can be calculated following the steps of the pre-existing stochastic finite-fault method [cit] ."
"we have picked the language of spider diagrams [cit] as the diagrammatic part of our heterogeneous reasoner because it has a formally defined syntax and semantics. spider diagrams are equivalent to first-order monadic logic and are equipped with a number of purely diagrammatic inference rules, which have been shown to be sound 1 . spider diagrams consist of the following basic elements (see figure 1 ):"
"for one actual earthquake, the fault rupture process including low-frequency slip rate time function, seismic moment on the finite fault model can be inversed based on seismic records. the slip rate time functions with different time history shapes are normalized, then used as the time window functions substituting the saragoni-hart function for simulating the ground motion. the normalized slip rate time functions containing the low frequency characters of the source such as multiple rupture peaks make the shape of the simulated time series more similar to real ground motion."
"we provide a formalisation of spider diagrams with translation to sentential formulae. heterogeneous proof automation, and a translation from first-order monadic formulae to spider diagrams is still work in progress."
"diagrams are often employed as illustrations in \"pen and paper\" reasoning. in the past, they frequently formed essential parts of proofs. eventually, with advent of proof theory, their role became almost exclusively that of a visual help. still, the intuitive nature of diagrams motivated the design of formal diagrammatic reasoning systems -for example, spider diagrams [cit] and constraint diagrams [cit] . consequently, some purely diagrammatic theorem provers have been developed, diamond [cit], edith [cit] and dr.doodle [cit] are some examples."
"our goal is to enable formal interactive heterogeneous reasoning in a general purpose theorem prover. we investigate three aspects of interactive heterogeneous reasoning: a) the direction of proofs (e.g., from a diagrammatic assumption to a sentential conclusion and vice versa), b) expression of statements that contain mixed sentential and diagrammatic terms, and c) mixed application of diagrammatic, sentential, and heterogeneous inference steps."
"in order to build a heterogeneous reasoning framework, we first chose an existing diagrammatic reasoning language called spider diagrams (see section 2) . the second part is the sentential reasoner, for which we chose isabelle [cit] ."
"a compound spider diagram is a diagram that consists of spider diagrams, which are called unitary spider diagrams, coupled with logical connectives. figure 2 is an example of a compound spider diagram. formula 4 illustrates the semantics of the diagram in figure 2 :"
"stress drop/ stress parameter ds is a very important parameter affecting the frequency and the magnitude of the simulated ground motion [cit] . based on the fault rupture process, the non-uniform stress drop distribution can be calculated according to ripperger and mai [cit] . substituting uniform stress drop with the non-uniform stress drop distribution into the stochastic finite-fault method makes the simulated results agree to real records better."
the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
"the implementation of the method stochastic finite-fault methods are effective ground-motion time-series simulation methods. the rupture process of a large fault has a non-uniform slip, a non-uniform stress drop, a non-uniform rupture velocity, and a non-uniform source time function at different points in the fault. its implement in the stochastic finite-fault method makes the simulated strong ground motion hold the specific physical meaning and reproduce the real ground motion. thus the new method of non-uniform stochastic finite-fault method (abbreviated as nnsim) causes two main modifications: 1) replace the predetermined time function with different source time functions for each subfault to improve lowfrequency results including the envelopes of accelerations and velocities. 2) replace the uniform stress drop with a distribution of non-uniform stress drops to reduce the dependence of simulated results on a single stress drop. in order to simulate the ground motion some key parameters need to be obtained as follows:"
contours and zones are both outlined shapes representing sets. contours are labelled with alphabetical letters. zones are not labelled and represent intersections and complements of contours.
discussion. we outline a work-in-progress of an integration of a diagrammatic language with diagrammatic inference rules into a sentential theorem prover. this enables formal heterogeneous reasoning with mixed diagrams and sentences.
"our key motivation is to provide different points of view on formulae and to enable reasoning about diagrams sententially or vice versa. we believe that heterogeneous reasoning will not only serve as a pedagogical tool for introduction to logic, it may also improve intuitiveness and readability of formulae (and proofs) in specific domains of verification -analogous to other domain specific languages. another motivation for heterogeneous reasoning is the ability to augment diagrams with sentential reasoning wherever diagrams fall short."
"shaded zones indicate that a zone is a subset of its spiders (i.e., the set this zone represents may contain only spiders with a foot in it)."
"ultimately, we want to enable formal graphical reasoning as is depicted in figure 7 . sentential expressions are drawn as diagrams if the translation is feasible. more importantly, the external diagrammatic reasoner can be invoked in an interactive mode within the proof body like any other tactic in isabelle. these tactics will invoke our diagrammatic reasoner, which in turn will return a proof trace for proof reconstruction. we also want to enable visual and interactive diagram construction and application of diagrammatic inference steps. for this purpose we will use speedith (sources available from http://gitorious.net/speedith) both as a standalone reasoning tool as well as a visual add-on to isabelle's graphical user interfaces."
"spiders are single existentially quantified elements. one spider is a finite collection of dots that are connected with lines. the dots are called feet, which indicate the zones in which the spider may live."
"the degree of grey coefficient (grey relational grade) ranks the alternatives and the alternative that has the highest grey relational grade is selected. the highest grey relational grade signifies the closeness of a comparability sequence for an alternative to the ideal sequence. the grey relational grade is calculated by taking into account the weight of each attribute. if all the attributes have equal significance then the weight of each attribute is 1/n, where n is the number of attributes for each alternative. in general, however, all the attributes may not have same weight and we obtain the weight of each attribute through ahp. this is called an integrated ahp and gra approach, and has been shown to be reliable and practically feasible [cit] ."
"in this article we have identified a number of techniques that can be effectively used to create support tools for policy makers. [cit] 33 needs to be put into their adaptation to the policymaking field and into their integration to provide unified tools. user acceptance is an important aspect to be considered as policy makers hardly trust ict tools that have been designed by people that do not have any expertise in the policy domain. for this reason the systems should be design in close contact with the policy makers and the validation phase should deeply involve policy makers, planners, and stakeholders."
"this research is conducted using a single case. generalisations to other cases should be made carefully. in different situations, some of the work order components might need to be shipped to a fabrication shop before they go to the maintenance site; therefore, they require a different delivery schedule and process. this scenario of the work order components' delivery process was not included in this study. a similar study, perhaps undertaken within a different industry sector, would go some way to provide an objective verification of the findings. this research is also limited as the processes are configured under a sap system and application to other systems used by maintenance or warehouse operation may be difficult."
the other information suggested to be shared in the new process is the weekly workshop schedule. this information could be sent to the warehouse operation in advance to enable planning of resource utilisation and anticipate future workloads. combining these various initiatives should ensure a more efficient use of resources and result in real financial savings to a company.
"mechanism design theory is ubiquitous and affects almost all aspects of policy with implications at two levels. firstly, it tells us when markets can be expected to lead to desirable outcomes and whether other institutions should be considered instead. secondly, mechanism design theory offers useful design guidance for alternative institutions when markets fail. the most conspicuous policy areas that have benefitted from mechanism design are listed next."
"budget balance: the sum of all agent payments equals zero, therefore, no money is extracted or injected into the system. this is particularly important for any self-sustaining mechanism where no external benefactor exists to subsidize the system."
"the plan outcome could also be constrained. for example for an energy plan, the overall amount of energy produced is an expected outcome. as well, bounds for each activity could be easily imposed meeting eu requirements or national guidelines."
"logistics problems are messy and ill-structured, which is the nature of most of the business problems in real-world situations (näslund, 2002) . logistics functions and operations tend to be largely cross-functional. therefore, this research uses empirical interpretive research methods by applying a design science research approach. [cit] believe that logistics studies should use multiple research approaches, which result in better findings and promote synergies produced from the combination of the methods used. the combination of approaches will help to (1) enrich the development of the conceptual model; and in (2) using triangulation to enhance the interpretation of the finding [cit] ."
"the algorithm then goes back to step 1 and the repeats the process periodically. figure 12 . proposed power/energy management algorithm fig. 13 shows the decrease in energy consumption achieved by the ue when it is operating for 600 sec, as a result of using dtx mode in step 5 of the above algorithm for long, medium and short duty cycle types. it compares the energy consumed by ue for (i) voip only (iii) combination of voip and video traffic and (iii) combination of voip, video and data traffic with the corresponding energy consumed by ue when dtx mode is not used for these traffic for 600 sec. the results have been by obtained by using dtx analytical model equation (10) . fig. 13 also helps to determine the impact of the dtx mode on the energy saving at the ue. we calculated the energy savings as: voip only traffic approx. 73%; voip and video traffic 62%; and voip, video and data traffic 49%. a) power optimization we explain the power optimization feature in our algorithm for ue dtx mode by computing the parameters that pertain to ahp and grc. to do this, the problem is first translated into an ahp hierarchy as shown fig 14. level 2 of the ahp represents the three alternatives from which one has to be selected for optimized energy management, and which results from a trade-off between the different criteria. we follow steps 2-5 stated under ahp, discussed earlier, to obtain the weights for the three criteria and the global priorities (weights) for the three sub-criteria. for this we use equations (11) and (12) ."
"this component is extremely computationally demanding, needing to simulate a huge number of agents acting, interacting, and making decisions in a complex environment. high-performance computing might be a driver for obtaining realistic and accurate simulations."
the current process of handling the storage and picking of the maintenance work order components has several gaps that can be improved. these gaps affecting the delivery performance and at the same time increase the warehouse operation costs.
"we considered a realistic scenario of the 3g lte system consisting of enodeb, ues, epc and a server in a hexagonal layout as shown in fig. 9 . the main objectives of the simulation study were to evaluate the:"
"however, [cit] considered hundreds to a thousand of tcp flows in an optical network based core router. whereas, in our case, we have considered only 3 flows of which only one is tcp, as the ue is unlikely to generate many more flows. from this observation, we can infer that it needs a very large number (hundreds) of bursty tcp flows to result in anomalous blocking probability behavior."
the third gap is the inefficient warehouse operation. the two main processes of warehouse operation that have a direct impact on the performance of the work order components delivery are the storage process and the picking process.
"as stated in the literature review chapter, the area of integration between maintenance and logistics is still unexplored [cit] . this research studied the possible opportunities in this area. by studying the interface between maintenance planning and one part of the logistics functions, which is warehouse operation, many opportunities are addressed to improve collaboration between maintenance planning and logistics. using the available information in the sap system, both warehouse strategies and maintenance planning processes can be optimized. the suggested process in this research ensures the utilisation of the available information to enhance the cross-departmental functions and improve the resource utilization."
"the dense and sparse functions in matlab helped us to first generate the matrix as per required buffer size (i.e. value of k states). however, values of transition probabilities of the order of 10 -4"
"second, how should the picking be conducted to support this process? this focuses on how the picking process should take place when the work order is executed. the current picking process will be studied to define areas of improvements, and then enhancement will be suggested."
"ever since the grey system theory was proposed by deng [cit], it has been used across many fields such as hiring of personnel, prediction of serial crime and stock selection. grey relational analysis (gra) is based on grey system theory and its main advantages are that it is computationally fast, simple and can handle unclear and incomplete information precisely. the term \"grey\" in gra means that the information is between black and white, where black represents no information and white represents all information. fundamental to the operation of gra is that it reduces the multi-attribute decision making (madm) problem into a single-attribute decision making problem by synthesizing all the attributes for every alternative into a single value. this makes the comparison and selection of an alternative computationally much simpler than ahp [cit] . the key steps involved in gra are: step1: the compared and a target reference sequence are generated by data pre-processing that involves normalization in order to make the compared and target sequence independent of the units and scale/compress the range value of the attributes step2: the grey relational coefficient (grc) between the reference sequence and the sequences that it is compared with is calculated. it essentially determines the closeness of each attribute to that of the corresponding attribute in the reference sequence."
"in the suggestion step, the research team envisions a creative solution to or modification of the existing components of the situation to overcome the identified problems. this step does not propose a complete solution to the problems; the focus is instead on highlighting the major components of the solution [cit] in this research, the required improvements to overcome the gaps identified in the previous step were be highlighted."
"the second matrix n defines how the impacts influence environmental receptors. each element n i j of the matrix n defines a qualitative dependency between the negative or positive impact i and an environmental receptor j. again the dependency can be high, medium, low, or null. examples of environmental receptors are the quality of surface water and groundwater, quality of landscapes, energy availability, wildlife wellness, and so on."
"these improvements can be developed based on three factors, which are order consolidation, regular communication, and scheduled deliveries. the new process tries to eliminate the gaps identified in the previous section, which are double handling activities, lack of collaboration, and inefficient warehouse operation."
"the simulation plays a vital role in determining any spikes in waiting times/delays that cannot be obtained otherwise, such as through steady-state based analysis. we used opnet 16.1 modeler to validate all the key results via exhaustive simulations. in our simulations, we used the traditional dumbbell shaped simulation topology as shown in fig. 2 . most of our simulations for m/g/1/k buffer size performance for a mix of tcp and udp traffic are confined to the shaded block and its elements, as our work is focusing on the user equipment. for tcp performance, we also included the enodeb protocol stack."
"the main issue with this technique is the need of a large amount of training data which should be analysed and tagged by a human expert. depending on the way we represent opinions (for example, positive vs. negative or an ordered score), different learning algorithms may be applied such as support vector machines, naive bayes, and decision trees to name a few. unsupervised learning is a technique that tries to find hidden structure in unlabelled data. in the context of text mining this would correspond to not having training data. the task of this technique is then to cluster texts that share similar feature values that in principles correspond to similar opinions. typical approaches to unsupervised learning include clustering (for example, k-means) and blind signal separation using feature extraction techniques for dimensionality reduction (for example, principal component analysis). semisupervised learning is a mixture of both previous techniques and it is particularly used approach when the labeled data is scarce. in the model training process, the unlabelled data is taken into account and used to train the model."
"a number of techniques for power management have been created around this approach, such as power saving mode (psm) for 802.11b [cit], dynamic voltage scaling [cit] and disk spindown [cit] . psm is not effective when application data is received at a frequent and steady rate, and it also increases the rtt to multiples of 100ms [cit] ."
"the traditional goal of mechanism design is to determine the rules of a game in which an overall equilibrium (or equilibria) is reached according to some desirable system-wide properties, given that all participating agents are self-interested [cit] . a social choice function (scf) describes the properties that the outcome should possess. some typical desirable properties include the following:"
"step 3: if the difference between the standard qos parameter values (such as mean packet delay, blocking probability) and the corresponding estimated values for the actual traffic is lower than a particular threshold then no power management will be conducted. if the difference is higher than the threshold, then our energy management technique will be initiated."
"policy planning is the science of efficient placement of land use activities and infrastructures for the sustainable growth of a region or a nation. plans are classified into types: agriculture, forestry, fishing, energy, industry, transport, waste, water, telecommunication, tourism, urban, and environmental plans. each plan defines activities that should be carried out during the plan implementation. activities are roughly divided into six types: infrastructures and plants, buildings and land use transformations, resource extraction, modifications of hydraulic regime, industrial transformations, and environmental management."
"we investigate equation (10) by using typical values of the parameters as shown in table ii . we determine how energy consumption in the m/g/1/k queue system of lte ue is governed by the factors such as blocking probability, increment in waiting time and dtx/drx t on time in the dtx mode equation (10) . energy consumed by a ue in terms of mwh (milli watt hour) is primarily determined by the transmitter power-on time and secondarily by qos factors. t on duration affects the time for which the transmitter will be in the active state, and the longer the transmitter is in the active state then more number of ttis (sub-frames) can be transmitted resulting in an increase in energy consumption by the ue. this is observed in fig. 8, which shows two graphs of the energy consumption as a function of (i) t on time indicated in red color and (ii) effect of mean packet waiting time (top x-axis) indicated in black color. from equation (10) it can easily be seen that a progressive increase in mean waiting time of a packet causes the net t on time to decrease by the same amount. as the net t on time decreases, the energy consumption by the ue for the uplink traffic decreases. we found that the blocking probability did not impact much in the steady-state probability range (refer fig. 4) ."
"a number of techniques have been proposed for performing environmental assessment of a given plan, namely probabilistic reasoning [cit] ) and fuzzy and multivalued logic ."
"social aspects [cit] play an important role such as environmental sensitivity, feeling of belongingness to a group, feeling of freedom from energy providers, importance of creation to agent trust in the government, and future and perceived bureaucracy. these aspects, together with economic and financial considerations can be used to model agents that react to energy policy instruments and mechanisms to come up with a simulated renewable energy diffusion corresponding to instruments and mechanisms."
"one of the instruments widely used for assessing a regional plan are coaxial matrices [cit], which are a development of the network method [cit] . these matrices are defined by environmental experts. one matrix m defines the dependencies between the activities contained in a plan and positive and negative impacts (also called pressures) on the environment. each element m i j of the matrix m defines a qualitative dependency between the activity i and the negative or positive impact j. the dependency can be high, medium, low, or null. examples of negative impacts are energy, water, and land consumption, variation of water flows, water and air pollution, and so on. examples of positive impacts are reduction of water/air pollution, reduction of greenhouse gas emission, natural resources saving, creation of new ecosystems, and so on."
"the survey contained seven questions for evaluation using a three-point scale (agree, disagree, neutral). the target population was the company's maintenance planners, workshop schedulers, maintenance engineers, warehouse supervisors, and warehouse specialists. this represents more than 2,500 employees. the survey was sent to a random sample of 200 participants representing the target population. participants worked in different maintenance departments supporting several types of plants, such as gas plants, oil refineries, marine terminals, and oil pump stations. these are employees who would be directly affected by the implementation of the suggestions and as such would provide a realistic view of the validity applicability of the suggestions. a total of 62 responses were received, which represent 31% response rate. the responses were then combined under three categories agree, disagree, and neutral. using a descriptive statistic, the responses under each category were analysed. the results are shown in the table below."
"beside understanding which policy instruments are available, the region has also to decide how to distribute the available budget, that is, the mechanism to be adopted. in many regions, for example, incentives are distributed to stakeholders by means of periodical auctions that indeed do not result from a specific strategy, but rather from extemporary actions. in these auctions the bids are ranked on the basis of various criteria (including the cofinancing percentage), and the first n bids that satisfy the budget constraint are funded. this mechanism is not necessarily a truthful one, that is, a mechanism in which agents truthfully report their private information. therefore, together with the plan, we have to define a proper set of policy instruments, the budget allocated to each of them and a corresponding mechanism to distribute the money. each solution has a cost and its own impact on the society. understanding the impact of these instruments is very complex, but essential for devising the proper instrument portfolio that achieves the plan objectives."
"our work is client-centered i.e. based on ue, and can be applied both to the drx and dtx modes. we will explain our work in the context of dtx mode for uplink traffic transfer, because: (i) the emergence of applications, such as those related to telemedicine and social networking are expected to generate substantial uplink traffic and (ii) not much work to date seems to have addressed dtx mode. our work will facilitate the conservation of battery power in the ue, and for this it focuses on using the dtx feature along with the quality of service (qos) metrics computed for the finite m/g/1/k uplink queue system that serves traffic mixes of video, voip, and general tcp. (note: in section iii, we justify the assumption for m/g/1/k model in our work.) specifically, the expected waiting time, blocking probability and throughput of the mixed traffic are determined through an analytical method. these computed parameters are then used to modulate the dtx cycle i.e. on/off times, by taking into account the qos bounds of the traffic -number of packets that can be dropped, how long the packets can be buffered i.e. delay. by estimating these parameters, we can dynamically shape the traffic i.e. increase delay, blocking probability within the allowable bounds of qos, even prior to the arrival of the traffic packets in the ue buffer. we then optimize the duty cycle of the dtx mode by applying an integrated analytical hierarchy process (ahp) and grey relational analysis (gra), which to our knowledge has not been applied so far in this area. (note: dtx cycles consist of power on/off periods i.e. periods during which the ue goes into active (awake) and sleep modes. ) an indispensable part of the uplink communication in the ue is the mobile ram of finite capacity that buffers the packets for transmission to the base station (enodeb) in lte. mobile rams are expensive [cit] and consume power, so a simple, cost-effective and power efficient strategy will be to have a single buffer of suitable size to serve the mix of heterogeneous services' packets originating from the ue instead of having multiple rams to buffer each traffic type or have an arbitrarily large size ram. the traffic for heterogeneous services generated by the ue has a characteristic packet interarrival distribution, packet size distribution and on/off duration distribution. tcp traffic burstiness is generally characterized by long-range dependent (lrd) distribution such as pareto or lognormal. thus, if poisson arrival of packets are assumed then we can consider the queue system formed in a finite buffer that stores a mix of tcp and udp data traffic to have a generalized service time distribution and be of m/g/1/k type [cit] ."
"or less were approximated to zero. we limited the buffer size to 15 kb, which on one hand kept the complexity of evaluating the transition probability matrix within tractable limits. on the other hand, as shown later in fig.3, a buffer size of 15 kb, the mean packet delay is limited to around 12 ms, and this is conducive in providing quality of service (qos) support for real-time services such as voip that are sensitive to end-to-end packet latency. larger buffer sizes would not result in qos improvement, so the cost to benefit ratio does not justify increasing the buffer size further. there is also literature [cit]"
"such integration can be internal or external. the internal integration is the integration of several departments of a single organization; external integration is the integration of an organisation with other organisations [cit] . if an organisation is seeking a development of efficient external integration, then an efficient internal integration is believed to be required (giménez [cit] ) . [cit], in their study of the impact of logistics integration on performance, concluded that companies appreciate the importance of the internal integration and collaboration to the success of their external integrations."
"step 1-the topmost level is the 'goal', the second level are all the main 'criteria' on which the goal is basedeach of the criteria can be divided into the sub-criteria. the third level is the 'alternatives' i.e. the different available choices from which one needs to be selected in order to achieve the goal."
step 6 -gra is used to determine the degree of grey coefficients (grey relational grade) for each of the alternatives. the grc gives the deviation of the attributes (parameters) of each alternative from the ideal reference. the weights obtained from the ahp for each of the criteria/sub-criteria are used in the computation of the degree of grcs.
"environments with asymmetric information describe situations in which some agents hold private information that is relevant to all parties [cit] . this information can be directly relevant in that it directly affects the payoffs of the bidders. for example, when the bid taker knows the quality of the items for sale but the bidders do not. asymmetric information can also be indirectly relevant by helping each agent to gauge the expected rational behavior of others and thereby solve his strategic uncertainty."
"the analytical result trend is similar to the simulation results in fig. 4 and is obtained by using equation (7) . the disparity between the simulation and the analytical results in fig. 4 increases with an increase in the buffer size due to transition probability matrix becoming more approximate with a higher number of states in the markov chain. the traffic intensity used in equation (7) is the combined traffic intensities for voip and video traffic mix, and the steady state probability for state zero is for the traffic mix of voip, video and tcp data traffic. it is interesting to see that the ftp traffic relative to the persistent tcp traffic with the same traffic composition of 0.49:0.40:0.10 initially has a lower average packet delay up to around 33 kb buffer size and thereafter it becomes higher. this can be explained with the help of fig 5(b), which shows the blocking probability of realtime traffic in the mix for ftp traffic, thus we explain fig 5(b) first."
"this study shows that business process improvements can be made by focusing on cross-functional process integration. not only is business process improvement facilitated by cross-functional process integration but if this is done appropriately then substantial financial savings can be realised. we have shown that warehouse operation strategies do have an impact on the work order performance through the on-time delivery of its components. in addition, regular communication and utilisation of the available information can also improve the scheduling of work order execution."
"our mission is to consider all submitted preprints that demonstrate good scientific method and sufficient data to support their conclusions, including studies replicating previously published work and/or showcasing negative results. for each recommended preprint, pci c neuro will publish a recommendation text and the full reviewing process. in addition, a progress page logging all submissions and processing timelines will feature on the website for added transparency. finally, pci c neuro will deviate slightly from the original pci template by accepting parallel submissions of preprints also submitted elsewhere, rather than requesting that authors postpone submission to journals until the pci review process is complete. importantly, however, we aim to collaborate with journals, such that preprints that are reviewed and recommended by pci c neuro may subsequently be fast-tracked for publication should the journal editors be satisfied by our reviewing standards. this symbiotic relationship is already in place for other pci platforms and a number of journals, and we look forward to building our own collaborations within the field of neuroscience, starting with neuroanatomy and behaviour."
"step 2: the mean waiting time of a packet in the m/g/1/k queue system of the ue will be determined by our analytical method, as explained in section iii"
"another way to use opinions from citizens without the need of their direct involvement is to use opinion mining [cit] on data extracted from public blogs, forums, and the press. social networks could also play a fundamental role to understand not only opinions, but also arguments [cit] supporting them. people opinions might represent an extremely important information for policy makers and might influence not only the planning process, but also the implementation instruments and mechanisms."
"the focuses on the system development which can lead to theoretical contributions [cit] . however the focus is on the process of creation of an artefact. in design research, the researcher invents a new solution [cit] . this new solution eliminates facets of the current reality to achieve a desirable future state [cit] general methodology for design research consists of five steps: awareness of the problem, suggestions for improvement, development of enhanced processes, evaluations, and articulation of a conclusion. the current study follows these steps."
"we present and discuss some of the key results obtained through simulations. fig. 10 shows the energy consumed for different commonly used applications in ue located closest (refer fig 9) to the enodeb during 600 seconds simulation run. in order to assess the impact of commonly used applications available in mobile handhelds on the energy consumed by the ue, we used models for applications such as voip, http and ftp that have been recommended by 3gpp and ngmn [cit] . we can see from fig. 10 that a persistent and high bandwidth application like ftp has the highest energy consumed whereas bursty lower rate application like interactive voip has the lowest power consumed. it must be mentioned here that a voip packet is generated every 20 ms and so the power is spread over 20 ttis. therefore, the energy consumed by voip shown in fig. 10 is after dividing it by 20. we also observed that an interactive application such as two-way conversation voice consumes more power than one-way voice communication. this is because the transmitter does not go much into the sleep mode. fig. 11 shows the energy consumed by the ue for uplink transfer of tcp data at different commonly used rates, when the ue is located closest to the enodeb and when it is at the cell edge boundary (refer fig. 9 ). it can be observed that for lower rates the ue at the cell edge consumes more power than the ue closer to enodeb. the reason for this lies in the physical layer, a 3g lte -ue at the cell edge uses a lower modulation and coding scheme (mcs) index than the ue closer to enodeb. this is because of lower signal power received from the enodeb due to attenuation. the lower mcs index corresponds to lower transport block size [cit] and thus more number of ttis need to be used, which results in more power being consumed to transmit the data."
"the importance of applying decision support systems to regional planning derives from the huge economic impact that wrong decisions can have. to better understand the amount of money these plans have to manage, let us consider an example: [cit], the eu structural funds and the cohesion fund, aimed at supporting the regions of europe and their integration, distribute a total budget of € 347.41 billion. each european region can take full advantage of several million euros managed and distributed by"
"even though interdepartmental integration is very important and it improves process performance, many of the operational cross-functions involve logistics activities are unexplored [cit] . maintenance is one of these operational functions that remain unexplored."
"markov chain representing states of a m/g/1/k queue system for composite heterogeneous traffic mix is shown below in fig. 1 . dark thick curved lines show the bunch of transitions occurring from state 1 onwards to each of the successive states. in lte ue the traffic will be a mix of tcp data traffic with udp traffic such as voip or video or interactive video telephony. the traffic models adopted by us are: (i) tcp-poisson arrival and pareto/log normal service time distributions [cit] (ii) video-on/off with poisson packet arrivals and deterministic service time distributions (fixed packet size) [cit] and (iii) voip-on/off talk spurts and silence periods [cit] . although voip traffic is cbr, it can be considered as poisson arrival based on the impact of the approximation, and aggregation level of voip flows e.g. in internet [cit] . we consider voip traffic to have poisson"
"the ideas expressed in this article are a result of the eu fp7 project called epolicy: engineering the policy making life cycle which focuses on developing decision support systems for aiding policy makers across all phases of the policy-making process. in the following, we will discuss some ai techniques and how they can be used to aid specific parts of the policymaking life cycle"
"the other information suggested to be shared in the new process is the weekly workshop schedule. this information will be sent to the warehouse operation in advance to be able to plan resources and to be able to anticipate future workloads. only 42% of the respondents agreed that sharing this information with the warehouse operation will positively improve the delivery process. this is the lowest percentage received in this survey; however, 40% of the respondents did not see any effect of this information on the process performance. in addition, the proposed process suggests the use of the weekly workshop schedule to plan the delivery trip. only 47% of the respondents agreed that dispatching delivery trips based on the weekly workshop schedule will improve the delivery process performance. on the other hand, 39% of respondents did not see any effect of this suggestion on the process performance."
"while the overall concept is similar to journal peer review, the key difference lies in building a large community of recommenders, such that no one individual must bear the workload of a traditional journal editor; pci recommenders typically handle up to two review processes per year. by dividing the workload across a large taskforce, pci not only benefits from a broad and varied pool of expertise, but also allows its reviewing services to remain free and community-driven."
"regulatory economics: in baron and myerson's seminal work on regulatory policy making, they used mechanism design to derive optimal regulatory schemes ensuring the provision of public services at least cost [cit] . later research showed that, when cost realizations are observable, simple mechanisms can achieve this objective. such results have improved actual regulatory schemes and the design of contracts between international institutions."
"in this sub-section, we present the analytical and simulation results for performance evaluation of a mix of voip, video and tcp data traffic obtained by using opnet for its validation. the analysis becomes more and more complex with larger buffer sizes, because an increase in the number of states in the embedded markov chain results in a squared increase in the size of the transition probability matrix. to overcome this, two steps were taken. first, low transition probabilities (below 10 -4 ) fig. 3 shows the impact of buffer size on the mean packet delay in a mix of tcp, video and voip traffic when the traffic intensity proportions are 0.79:0.16:0.04. it can be seen that as the buffer size increases the mean delay time monotonically increases as well, due to the increase in queue length. we can also observe that the mean delay is higher in case of 0.79 tcp data in the mix (fig. 3) than for 0.49 tcp data in the mix (fig 5a.) . this is because the tcp packet size is ten times the voip packet size and 2.5 times the video packet size and the tcp traffic intensity is higher, which increases the overall occupancy of the buffer relative to the case for 0.49 tcp proportion for the same buffer size. as fig.3 shows, the analytical result from equation (6) closely follows the simulation result."
"as an example consider in figure 4 the trend of incentives provided by the italian government for three classes of plants (class 1 refers to plants with an installed power less than or equal to 3 kilowatts, class 2 refers to plants whose power is between 3 kilowatts and 20 kilowatts, and class 3 refers to plants whose power is between 20 kilowatts and 200 kilowatts). figure 5 shows the installed power for the same classes of plants. we can see that there is no correspondence between the trends."
"third, what information that can be shared across departments to improve this process? this focuses on the available information within the sap system will be identified. after that, information that can be shared among departments to enhance process performance will be evaluated."
"we now describe a case study of application of the decision support system. [cit] along with the detailed corresponding model. we restrict the example to the regional plan chapter devoted to the electric energy production from renewable energy sources. the considered electric power plants are minihydroelectric plants, photovoltaic plants, thermodynamic solar plants, wind generators, and, again, biomass power plants."
interdepartmental integration is required when there is a business process that is executed across several department boundaries. interdepartmental integration is the joining of several departments within an organisation to perform a function that share part of its process. a department receives an input and then processes it to add value to it. after that the output will be passed to the next department in the chain for further processing [cit] ).
"this research can be extended in two main ways, through the completion of the design research cycle and through extension to include other elements of the maintenance-logistics processes. the evaluation in this research used a survey to evaluate the opinions held by respondents about how the proposed process would improve operations. therefore, it does not evaluate the real performance of the suggested process. this represents an initial phase of a larger design science project which will involve the updating of processes in the workplace. thus, the current study can be improved through completing this design science cycle and conducting summative evaluations of effectiveness of the changes."
"concerning the environmental assessment, we plot in figure 3 the value of the receptors in significant points of the pareto front. each bar represents a single environmental receptor for a specific plan plotted in the pareto frontier of figure 2. in this way it is easy to compare how receptors are impacted by different plans."
"social participation is a key aspect for a democratic process. for this reason, a number of e-participation tools have been developed and are currently used to enable public consultations. clearly, citizen participation in the definition of public policies might be fostered by the use of mobile services, such as visualization of big amount of data in an intuitive way or the possibility of customizing the participation actions only in some contexts."
"auction: auctions are one of the first and most prominent applications of mechanism design theory. they benefitted greatly from nobel prize winners maskin and myerson's contributions regarding the challenge of how to allocate some item(s) among bidders when the value of the item(s) to a bidder is private (known only to that bidder). the objective of the auction designer may be to maximize revenue, or to ensure that the items are awarded to those who value them the most thus maximizing economic efficiency. governments use auctions to allocate a country's natural resources that include mineral deposits, exploration rights, timber, frequency spectrum or property. governments also use reverse auctions to procure goods and services from private sector suppliers. mechanism design theory has been instrumental when guiding the design of a set allocation and payment rules for auctions across many applications."
"the proposed process suggests that information about the complete received work order components at the warehouse should be shared with the maintenance planners. the maintenance planner will then use this information to schedule the work order execution. sharing this information among the two departments, maintenance and warehouse operation, will improve the internal integration within the company."
"the behavior of agents often depends upon whether they can accurately predict the actions of other agents. in an independent private-values model, each bidder knows how much he/she values the object for sale but his/her value is not dependent upon the bids of others [cit] . each bidder derives a value from only his/her own personal tastes and not external factors such as resale value. the revelation of other agents' types does not influence each agent's private type."
"we strongly believe a number of ai techniques could be effectively used for aiding governance and policy making: the literature reports attempts to use agent-based simulation [cit], opinion mining [cit], visual scenario evaluation [cit], and optimization [cit] to support specific cases of this process, but there is large space for improvement. what is totally missing at present is a comprehensive tool that assists the policy maker in all phases of the decision-making process. the tool should compute alternative scenarios each consisting of both a wellassessed plan and the corresponding implementation strategies to achieve its objective. we need a tool that is able to integrate and consider, at the same time, global objectives and individual/social reactions. these two perspectives could be, and often are, in conflict, and possibly game theory could be used to find an equilibrium between the two parts."
"a policy is typically a set of rules that is designed to facilitate the achievement of certain goals or objectives on the part of a country or organization. a policy should aid decision making and should evolve as the objectives change over time. a protocol is more specific than a policy because it defines a set of procedures to be followed for the accomplishment of an identified task. it is well-defined procedure that controls how tasks are achieved. [cit] 29 completion of a task brings clarity and certainty to the state of a task. protocols tend to be observed repeatedly and can be refined based upon delivery against an overarching policy that itself also changes as an organization's needs evolve. protocols are often considered to be an effective way to establish standard and repeatable processes for large organizations whose management requires mechanisms for controlling large groups of individuals. policies and protocols are thus interwoven management tools with strong dependencies. the game-theoretic analysis of deliberation and negotiation and the normative theory of deliberative democracy both view contention for resources from different perspectives but have developed in mutual isolation. [cit] confronted the arguments raised by normative theorists opposed to the perceived relevance of underlying assumptions in game-theoretic work. they found that the game-theoretic approach is particularly well suited for providing insights about the feasibility of deliberative institutions and practices. game theory is improving our understanding of decision making and, in particular, how economic agents react to a set of rules. the central solution concept surrounds an equilibrium in which agents do not have an incentive to unilaterally deviate from a specific action [cit] . recent research has extended the range of solution concepts to address broader environments that include uncertainty, stochastic dynamics, and other complicating factors. we survey extant work related to game theory as a framework for assessing the efficacy of policies and protocols."
"on the other hand, we have also semantic approaches to opinion mining [cit], using natural language-processing techniques to understand the text and extract opinions. in this case, a text is parsed and the meaning of each word is extracted depending on the context. clearly, understanding an opinion contained in a text is far more complicated that simply understanding the text, but basically it relies on the same techniques."
"we made use of opnet modeler 16.1 to conduct a simulation study of the energy consumption by ue uplink transmission (rf modem) in the 3g lte system over a 600 second simulation time. we focused on the energy (10) consumed by the transmitter, and ignored other components, such as display, cpu, memory. the 3g lte system simulated consists of detailed models of enodeb, ue, evolved packet core (epc) and application server, all of which closely replicate the functionality of protocol stacks used in the lte system. in the analytical model's equation (10) we considered the main traffic's energy requirement only, without the power consumed in a ue for signaling, scheduling, hybrid arq and other inherent routine activities in the 3g lte system."
"as described previously, pci brings together communities of researchers to peer-review and recommend preprints in their field. but how does it work on a practical level? the process is in fact very similar to that of peer-review for a tradional journal, with the exception that pci does not publish the manuscript at the end, but only the reviews and a short recommendation piece explaining the importance of the work (figure 3 ). in brief, after having uploaded their manuscript on a preprint server such as biorxiv, authors can submit it to pci c neuro for review. if the work is considered interesting by one of the many pci c neuro recommenders (that act like journal editors), they agree to manage the preprint and send it out for review. as with a journal, there may be several rounds of review, which could end in a negative or positive outcome. after each revision, the authors will upload the updated version of their mansucript to the preprint server, until the rec-ommender reaches a final decision. if the preprint is accepted for recommendation, the recommender writes a short citable commentary (the recommendation), which is published along with the reviews and author responses. reviewers, but not recommenders, may choose to remain anonymous if they wish."
"an algorithm for efficient energy management of the dtx mode for ue uplink data transfer. the algorithm is based on the combination of analytical hierarchy process (ahp) and grey relational analysis (gra). the rest of the paper is organized as follows: section ii reviews the related literature associated with mixed traffic performance in wireless networks and with energy management. section iii explains the assumption for m/g/1/k model, our analytical model for heterogeneous traffic mix in a m/g/1/k queue system as well as simulation models. it also presents and discusses the results of the m/g/1k study. section iv presents our analytical expression for power usage in ue lte and discusses key results obtained from it and simulations. it also explains our energy management algorithm and demonstrates the effectiveness of the integrated ahp and gra approach in our algorithm to accomplish energy optimization. section v concludes the paper."
"mechanism design can be considered as inverse game theory whereby the rules of the game are decided by an authority so as to fulfil some objective. two typical goals of auction design are either revenue maximization or maximization of social welfare. the goal of maximizing revenue is an obvious one and features in auctions where the identity or private valuations of the winning bidder(s) matter little when compared to the revenue received by the bid taker. in some circumstances, however, the bid taker may wish to achieve certain social objectives, but because these individuals' actual preferences are not publicly observable, the analysis of such auctions can become more complicated. the mechanism design problem is to elicit these preferences so that they may be aggregated into social preferences to form a collective decision."
"the publishing system as it currently stands does not disseminate scientific findings in an open or efficient manner. indeed, publisher-imposed paywalls prevent not only the public from accessing the latest research, the vast majority of which is in fact funded by public money, but also the scientists themselves, who must pay both to publish and to read their own articles. while the recent launch of the 'open access' publishing initiative, plan s, is beginning to challenge this system, the cost of the resultingly mandatory open access is born not by the publishers, but by the scientists and academic institutions who must often pay increasingly larger sums in order to publish their work free of paywalls [cit] . hence, one of the main advantages of preprints is in enabling the free dissemination of scientific findings. a second important benefit is the speed at which these findings can be shared. indeed, a preprint becomes accessible to all within just a few hours of its initial submission, while it will take an average of five months to a year to publish an article in a journal [cit] . unsurprisingly then, it has been estimated that preprints could accelerate scientific innovation in the next 10 years five-fold [cit] . a third factor is the visibility of research, which appears to be enhanced by preprinting, with articles deposited on biorxiv prior to publication receiving more citations and attention on social media than those going directly to journals [cit] . finally, preprints are providing an alternative way to evaluate scientists' productivity, with most large funding bodies, such as the national institutes of health [cit], wellcome trust and european ). the question asked was, \"would a system of commentary, evaluation, or ratings make preprints potentially more valuable for the community (note: this is not a current feature of arxiv)?\""
"part of the research involves the search and creation of a new process design, or artefact, and so it falls under the design science research paradigm. while there have been multiple approaches proposed to guide design science research projects [cit] was adopted as it is a relatively succinct, clear, and integrative approach. it provides coherent and easily followed steps that match those needed to complete the research. this research paradigm is inherited from problem-solving approaches, in which the researcher uses imagination and empirical facts to invent the desired situation [cit] . although [cit] stressed that design research should address a specific problem class or type. within a problem class, several stakeholders' views are considered in the solution. this differs from a normal problem-solving approach where a specific problem with a single stakeholder view is solved. [cit] went further to identify the features of the problems that can be studied using design research:"
"this study sheds light on the challenges involved in the integration of maintenance and logistics functions. it is clear that there is a great deal of complexity surrounding the activities within these functions. the research indicates to logistics researchers that the design science approach can provide significant benefits in terms of providing a structured method to analyse and improve existing operations. the results also indicate that not all communication would be considered equally valuable; notification of complete orders was considered vital by most respondents, but the sharing of a weekly workshop schedule was not considered to be useful. these results are based on a survey and so there may be some misunderstanding on the parts of the respondents, or it may show that the information that is considered valuable is information in 'chunks', which require no interpretation or analysis, such as a 'notification'; a shared schedule would require interpretation and comparison with existing work. the implication is that cross-functional process design must manage the more mundane analysis and interpretation elements, and instead focus on clear communication of information to users."
"the impacts of a policy plan on the environment are evaluated with the so-called strategic environmental assessment (sea) [cit], which relates activities performed in the region to environmental indicators. this assessment procedure is currently performed by environmental experts after a plan has been designed. taking into account impacts a posteriori enables only corrective interventions that can at most reduce the negative effect of wrong planning decisions."
"beside assessing the plan proposed by the experts, we also provided new, alternative plans. in particular, we searched for optimal plans, both with respect to the cost, and to the quality of the air. since we have two objective functions, we plotted the paretooptimal frontier. the pareto frontier is shown in figure 2, together with the experts' plan produced manually. figure 2 shows that, although the plan devised by the experts is close to the frontier, it can be improved. in particular, we identified on the frontier two solutions that dominate the experts' plan: one has the same cost, but better air quality, while the other has same air quality, but a lower cost. table 1 contains the plan developed by the region's experts, while table 2 shows the plan on the pareto curve that has the same quality of air as the plan of the experts. note that with the optimal plan, we can save 191m euros (6.3 percent of the experts' plan cost) and obtain the same air quality. the energy produced by wind generators is almost doubled (as they provide a very convenient ratio (air quality)/cost, we have a slight increase in the cheap biomass energy, while the other energy sources reduce accordingly."
"to improve warehouse operations careful consideration should be given to storage strategies. there are various possible storage strategies including random storage strategy, dedicated storage strategy and class-based storage strategy. the proposed process suggests the use of a dynamic version of the dedicated storage strategy. when the first item of a work order components received at the warehouse, the system suggests a storage location based on the work order reference number. this location then will be dedicated to stage the components of that work order only. this approach can serve the goal of the suggested process of storing all the work order components in one storage location."
"this facilitates combinations between heterogeneous service packets when the system transits from one state to another. further, we have considered two cases for the proportion of voip, video and tcp data traffic intensities (0.04:0.16:0.79 and 0.10:0.40:0.49) to illustrate the influence of real-time traffic on throughput. the mean packet size of a pareto distributed packet is [cit] :"
"the cost of the plant, instead, depends mainly on the installed power: a solar plant has an installation cost that depends on the square meters of installed panels, which in their turn can provide some maximum power (peak power)."
"the topic of energy/power management in the client wireless devices has gained popularity due to the widespread prevalence of ieee wlans, wimax, umts (3g) networks. the main hardware responsible for a significant use of power in client handheld devices is the wireless network interface card (wnic) [cit] . a fundamental approach to decrease energy consumption in a mobile handheld is to transmit the data in bursts, which increases the packet transfer delay as the periodic transmitter on (tx/rx active) state is punctuated with an off (tx/rx sleep) state(s)."
"in particular, the field of mechanism design is a branch of economics whose primary application is the design of protocols for the sale or procurement of items. it is particularly relevant to policy making because it concerns the design of protocols for implementing policy objectives in specific settings. [cit] for having laid the foundations of mechanism design theory. this brought recognition to the founders of a field that has contributed enormously to policy making and governance. 2 [cit] s when he examined how a planner should reach a decision when the quality of the decision relies on information spread among numerous people. mechanism design theory formulates this problem mathematically and studies properties of allocation and payment rules. among hurwicz's key insights is the idea that the self-interested agents must find it in their interest to reveal private information. this insight informed a contemporary intellectual debate concerning the relative merits of capitalism and socialism. it helped governments understand the importance of incentives and private information and to consider effective regulation of . mechanism design theory now provides a general framework to study collective decision problems and many specific subfields have been studied. we examine the emergence of mechanism design as a rapidly evolving economic tool that aims to improve the effectiveness of economic protocols given a model of game-theoretic rational decision making by agents."
"even though this approach might require more storage space inside the warehouse, it will substantially reduce the time needed to pick the components during the picking process; hence improving the overall process delivery performance. it is therefore recommended that a dynamic dedicated storage strategy be used to maintenance function within a warehouse environment."
"step 1 -a software agent operating within the ue will inform the ue energy management algorithm regarding the applications that are currently active in it, such as voip, interactive video or ftp."
"the schema we devise is depicted in figure 1 . moreover, the policy maker should take into account the global view of the policy, namely financial aspects, objectives, environmental impacts, and constraints and generate alternative scenarios. on the other hand, the society can participate in the policy-making process through e-participation both in the exante phase during the definition of the policy and in the ex-post phase for providing feedback on different scenarios. clearly, we should be able to come out with an equilibrium between the global and the individual point of view. in this case game theory could play a role."
"ahp has been used in many fields since it was first proposed by saaty [cit] . for instance, it has been used to select between the umts (3g) network and wlan i.e. most suitable network so as to provide the user with the best available qos at any time in different scenarios [cit], or to select intermediate nodes in application-specific routing in a wireless sensor network [cit] . in ahp the problem is first structured into three main hierarchies."
"however, performing the strategic environmental assessment during the plan construction means combining the evaluation and the planning models. this can be easily done in a constraint-based model as the one presented above."
"below, we first discuss significant literature on the performance of mixed traffic primarily in cellular wireless networks, after which we discuss the literature for energy management in wireless networks including 3g lte."
"3gpp has defined the drx/dtx power saving modes [cit] in 3g lte to reduce the power drain on battery and extend battery lifetime. the main concept behind the drx/dtx is to control the time for which the receiver/transmitter is switched on (awake), as it is a major battery power consumer. the drawback of drx/dtx modes is extending the mean transfer time of data, which results in reduced data throughput. thus, the parameters for drx/dtx mode such as the on time, drx cycle duration, inactivity timer and short drx cycle are carefully chosen in the network and passed on to the ue by enodeb over the downlink via rrc signaling. a typical drx/dtx cycle is illustrated in fig 7 . we propose analytical equation (10) to model the dtx cycle operation on the m/g/1/k uplink queue system for composite traffic mix and to obtain the power/energy consumed by the uplink queue system. equation (10), uses the fundamental concept behind equation (9) i.e. power consumption is based on the awake/sleep states of the ue and the number of frames (bits) in each state."
"a strategic equilibrium is a profile, or combination, of strategies such that if other players conform to the equilibrium strategies (that is, other players are rational), no player has an incentive to unilaterally deviate from his equilibrium strategy. 3 game theory provides several solution concepts to compute the outcome of a game with self-interested agents. a solution concept is used to predict the strategies agents will choose in order to maximize their utility, thus determining an equilibrium position for the game. these concepts assume knowledge about agent preferences, rationality, and shared information about one other. the best known concept is that of a nash equilibrium, which states that in equilibrium every agent will select a utility-maximizing strategy given the strategy of every other agent [cit] . a nash equilibrium is self-referential and constitutes a profile of strategies that form optimal reactions to other agents' optimal reactions. nash equilibrium is the pure form of the basic con- cept of strategic equilibrium; as such, it is useful mainly in normal form games with complete information. when allowing for randomized strategies, at least one nash equilibrium exists in any game with regular payoff functions. 3 a game may possess one or more nash equilibria."
"the storage process does not utilise the available information in the system to optimise the selection of the storage location. during the storing process, the storekeeper does not have information about the work order components stored previously within the staging area. the receiving documents contain the work order reference number, but they do not suggest an optimal storage location for the received item. the sap system does not have a standard strategy to store the received items based on a reference to a work order."
"development programs: mechanism design theory has also heavily influenced the design of aid programs in poor countries [cit] . traditional solutions to community problems such as lending, land sharing arrangements and resource management have been improved following the contributions of mechanism design theory [cit] . for example, mechanism design helps evaluate the relative performance of different cross-reporting and joint liability in microfinance arrangements [cit] )."
"the simulation model plays a key role in complementing the analytical study by way of taking into consideration a number of attributes that are difficult to model analytically, such as path loss and fading due to distance separating the ue from enodeb, modulation and coding scheme (mcs) used, signaling information exchanged and the topology of the scenario: regular hexagonal, grid or random layout."
the current state with no integration of processes between maintenance and logistics operations has been identified as a source of concern and frustration for workers.
"step 7-if the degree of grc new of an alternative duty cycle is greater than the degree of grc current of the currently used duty cycle, then the currently used dtx duty cycle will be replaced with this alternative."
"the sharing of available information. as soon as all the work order components are stored in the warehouse, a notification message is sent to the maintenance planner. the process also suggests that the warehouse operator receives an advanced notice of the workshops schedule."
"the storage process uses the simplest storage strategy, which is the random storage strategy. although this strategy provides the best space utilisation of the storage location [cit], it impacts on the cost of the picking process by generating less efficient picking routes. these routes have a longer travel distance as items to be picked are distributed among the storage locations."
"it should be noted that the respondents may have a potential bias toward open science, and a larger survey would be very useful. however, based on this, it seems that the preprinting system as it currently stands is proving essential for the free and rapid dissemination of scientific findings, but is not providing authors with the feedback and discussion they need to improve and advance their research."
"the regional planning activity can be easily cast as a combinatorial optimization problem. there are a number of technologies supporting decision making and optimization in the policy planning field [cit], namely constraint programming, mixed integer linear programming, and metaheuristics. they are extremely useful for a number of reasons: first, because they provide a tool that automatically performs planning decisions, taking into consideration the budget allocated on the plan by the regional operative plan, as well as national and eu guidelines. second, because they can take into consideration environmental aspects during plan construction, avoiding trial-and-error schemes. third, because they enable the generation of alternative scenarios. scenario comparison and evaluation also comes for free."
"to support this process improvement three questions have been formulated. first, what is the best storing strategy to supporting this process improvement? this focuses on identifying the best possible way of storing work order components after receiving them. several strategies were proposed in related literature [cit] . throughout the analysis of the storing process, one of these strategies is selected."
"for each energy source, the plan should provide: the installed power, in mw; the total energy produced in a year, in ktoe (toe stands for ton of oil equivalent); the total cost, in m€. the ratio between installed power and total produced energy is mainly influenced by the availability of the source: while a biomass plant can (at least in theory) produce energy 24/7, the sun is available only during the day, and the wind only occasionally. for unreliable sources an average for the whole year is taken."
"another limitation of the research was that the artefact was not fully implemented; the new process was designed, and this design was the artefact. thus, the new process was not working during the evaluation step; therefore, caution should be used when interpreting the evaluation results. the evaluation survey is measuring respondents' opinions about the proposed process and not the actual performance of the process after implementation."
"the common-values model was subsequently introduced, where the true actual value is the same for everyone, for example, the oil in a drilling rights auction, but bidders have different private information (signals) about the true actual value [cit] . in this model a bidder would change his/her estimate of the value if he/she learned another bidder's signal. common valuations often occur in auctions for rights to natural resources [cit] . if a bidder's signal was significantly more than all other bids for example, he/she may reestimate the value of the item, therefore, the bidder's ex-post valuation may be decreased. if it decreases to below his/ [cit] . the winners in common-value auctions are necessarily the most optimistic bidders when payment is conducted using a first-price scheme. this can sometimes result in winning an item at a cost of more than the ex-post valuation [cit], which may result in serious losses for the winner [cit] ."
"the effective traffic entering the queue system i.e. the throughput after taking into consideration the blocking probability is given by the numerator of the first term (outside the brackets) in equation (10) . under steady state conditions of the queue system, the effective throughput divided by the channel capacity is the utilization of the system. the multiplication of the first term within the brackets with the utilization represents the energy that is consumed by the packets (frames) that are awaiting service i.e. during the sleep period of the dtx cycle. the second term within the brackets after multiplication by utilization represents the energy that is consumed while serving the frames during the on (awake) period of the dtx cycle i.e. duty cycle, by the transmitter. equation 10 will follow the non-linear characteristic of the waiting time of packets (frames) for an increase with buffer size (refer fig 3) and is valid under steady state condition of the queue system. the significance of our equation (10) is that it expresses the power/energy consumed by the ue in terms of qos factors, namely blocking probability and mean waiting time of a packet in the m/g/1/k queue. it also takes into account the service time and the utilization of the traffic. in the literature, we could not find any similar equation expressing drx/dtx power consumption in terms of qos and other related key factors."
"in 3g lte systems, there will be a high prevalence of diverse mobile terminals such as smart phones, laptops and ipads. each one of these devices has a different battery capacity to support different applications over varying lengths of time. in the development of mobile communications technology battery performance has not kept pace with the advancements made in computing power. the wide variety of applications that may run simultaneously on 3g lte handsets necessitates the ue to be used over prolonged periods of time. reducing power consumption by switching off the ue transmitter has been a commonly accepted method, as transmitting circuits waste little power during switch-on/switch-off [cit] . there is an increasing motivation for the ue to make use of the drx/dtx (discontinuous reception/ transmission) framework feature provided by 3gpp to conserve battery power and cope with energy requirements of the applications. the main aim of our work was the creation of a dynamic algorithm based on dtx/drx, for effective energy savings by the ue."
"individual rationality: no agent attempts to take part in a trade that fails to increase, or at least leaves constant, his/her own utility [cit] . this is an important property if agent participation is voluntary."
the literature pertaining to drx/dtx mechanism for energy management of the ue is fairly limited as it is still an emerging area for research. almost all the work found so far in the literature pertains to the drx mechanism to conserve power. drx operates on the downlink data sent from enodeb and received by the ue. the parameters to operate the drx mechanism at the receiver are evaluated by enodeb and passed on to the receiver through radio resource control (rrc) signaling [cit] . this in turn requires interaction with the scheduling mechanism to meet the qos requirements of the traffic for different applications.
"almost 40,000 [cit], and this trend shows no sign of slowing down ( figure 1a ). it is becoming clear, then, that preprints are changing the way that scientists communicate their research, both within the academic community and with the general public, from a fee-oriented publishing system towards a free, rapid and open dissemination model. however, despite the many benefits of preprints outlined here, it is apparent that one important factor could further enhance their utility: scientific discussion and systematic validation of the presented data."
"after reviewing the literature on the warehouse operation strategies and processes, maintenance work order cycles, and interdepartmental integration, it is clear that the integration between warehouse logistics operations and maintenance planning has not received substantial attention. it was also indicated in the literature that the maintenance and warehouse operation interface largely involves inventory and work order components staging processes [cit] . this consists of storing, picking, and delivering the work order components. while maintenance planners attempt to ensure the availability of all needed components before scheduling work order execution, to avoid operation disruption, they are challenged with the use of the just-in-time and other management approaches and technologies used in warehouse operations to reduce inventory [cit] . the ontime availability of the materials and spare parts required for a maintenance job is a key element to its success [cit] . therefore, by improving the storing and picking processes, which are major components of the cost and performance of order fulfilment, the on-time delivery of the work order components can be improved [cit] ."
"to compute the environmental impact, we can sum the contributions of all the activities and obtain the estimate of the impact on each environmental pressure. in the same way, given the vector of environmental pressures, one can estimate the influence on the environmental receptor by means of the matrix n, which relates pressures with receptors. we can impose constraints on receptors and pressures. for example, we can say that the greenhouse gas emission (that is a negative pressure) should be constrained by a given threshold."
"to design a constraint-based model, we have to define variables, constraints and objectives. variables represent decisions to be taken. to each activity we associate a decision variable that defines the magnitude of the activity itself. policy makers frequently make decisions regarding income gathering and expenditure. the most complex challenges for policy makers include tax compliance management and efficient expenditure of funds to support social objectives. we first need to understand the imperatives of the players in a setting ruled by policy makers. we can use game theory to model their actions and reactions in this environment. game theory is a mathematical theory of strategic interaction where multiple players must make decisions that may affect the interests of other players. an auction is an example of a game in which bidders are competing agents, each of whom is seeking to maximize his/her utility [cit] . the bid taker sets the rules for the game in such a way as to achieve his/her objective, which is often revenue maximization but may also be the fulfilment of some social objective. the bidders, on the other hand, will strategize so that their expected surplus is maximized [cit] ."
"usually, the performance of this function is affected by how well these departments communicate and support each other throughout the process. to get the best outcome from an interdepartmental integration, departments must collaborate toward achieving a common goal or predefined objectives. the communication must often be richer than can be achieved through regular communications like faxes, meetings, and emails. however, cooperation and willingness to support staff in the involved department is also required [cit] . therefore, with collaboration, information and resources are shared, assets are efficiently utilised, and knowledge and skills are improved [cit] ."
"3g long term evolution (lte) is the first true next generation mobile network (ngmn) alliance compliant mobile network technology [cit] . it is a high capacity all ip based wireless network that is expected to dominate for many years to come [cit] . notable attributes of 3g lte are: uplink and downlink peak data rates of 50 mb/s and 100 mb/s respectively, better spectrum efficiency [cit] and flexibility, cost effectiveness of infrastructure, low power requirement by wireless terminals such as mobile phone or laptop computer, robustness to channel variations etc [cit] . these attributes make 3g lte network a strong candidate for the support of mission critical and commercial services such as m-health, e-commerce, mgaming and smart grids."
"process improvement through cross-functional process integration is often overlooked as a way of improving efficiency within an organisation. as shown in the literature review, the area of integration between maintenance and logistics is still substantially unexplored [cit] . by studying the interface between maintenance planning and one part of the logistics functions, which include warehouse operations, a number of opportunities have been identified to improve collaboration between maintenance planning and logistics. using the available information in the sap system, both warehouse strategies and maintenance planning processes can be optimized. the suggested process in this research ensures the utilisation of the available information to enhance the cross-departmental functions and improve the resource utilization."
"based on literature [cit], the packet sizes for voip and video traffic were considered to be 100 and 400 bytes (mean size), respectively and tcp mean packet size to be 1000 bytes [cit] . permissible packet sizes were carefully selected to be an integral multiple of the voip packet size so as to represent any packet size as a multiple of a voip packet."
"einstein is quoted to have said that imagination is more important than knowledge, and -in a chapter on imagination -beveridge recommended stimulating the mind with discussion [cit] . there can indeed be no doubting that discussion is beneficial to scientific advancement, and the more open, the better. while biorxiv does provide a platform for such discussion to take place, it seems that few preprints are in fact discussed in any kind of detail. indeed, although thousands of manuscripts are shared on biorxiv each month, the commenting on the site appears to have reached a plateau ( figure 1a ). this is unlikely to be due to saturation of users available to comment, since preprint downloads have grown commensurately with their uploads [cit] . critically, this lack of commenting means that authors are still having to rely on expensive journals for feedback and input on ways to improve their manuscripts, and that while some journals are now adopting open peer-reviewing, much of this valuable discussion still occurs behind closed doors. [cit] found that 68% of 392 participants agreed that commentary, or some form of evaluation or rating system for biology preprints would be valuable for the community ( figure 1b ) [cit] ."
"as tcp data in ftp occurs in short bursts, it does not obstruct the real-time traffic, which results in an extremely low blocking probability for real-time traffic observed in fig. 5(b) . in fig 5(a) 2 where, s is the approximate tcp throughput in packets/sec, p is the packet error probability, rtt is the average round trip time. the tcp throughput is higher in case of traffic composition of 0.49 tcp traffic intensity because, as explained earlier with regards to fig 5(a), it has a lower mean packet delay relative to the traffic with a 0.79 tcp traffic component. thus, for the same tcp window size, more packets per unit time are transferred in case of 0.49 tcp traffic share than with 0.79 tcp traffic."
"the first gap is the double handling of the staging process at two locations within the process. the warehouse operation at the materials management department stages all the work order components until the planner requests it. as each component has a single picking document, the work order components will not be sorted together. thus, each item is delivered separately. therefore, the maintenance department must sort and consolidate work order components together after receipt. moreover, work orders might be delivered to the maintenance site partially. therefore, items received need to be stored again until all the components are available so that a complete task can be completed. the lack of batching and consolidation of work order components during the picking process creates this inefficient use of resources."
"motivated by observations on the m/g/1/k buffer and in particular its limitations [cit], we have conducted a study of the m/g/1/k finite buffer for a mix of voip, video and tcp data. although the buffer size study presented in this paper is directly applicable to lte ue uplink buffer, it can also be suitably applied to other wireless technologies. the results of our work can lead to cost savings of mobile rams through proper dimensioning as well as in the savings of the limited power supply that is generally available in the ue. to the best of our knowledge, no one else has conducted such a study for a m/g/1/k queue system formed of composite heterogeneous traffic in an uplink ue buffer of a wireless network. also, most of the work in the literature is not client (ue) centered."
ftp traffic has an on/off characteristic where the off time has a mean duration of 180 secs and the on time based on parameters considered in table i is of around 10-12 sec in duration.
"an expression has been created for power consumed by the ue, while the packets are waiting in the queue and being transmitted. from this expression, we obtain (a) the impact of increase in average waiting time of a packet in the queue on energy consumption (b) the impact of increase in blocking probability of the traffic on energy consumption and (c) the impact of increase in transmitter's (rf modem) on time on energy consumption. vi)"
"this research therefore tries to answer the following question: \"how can work order components of storage, picking, and delivery processes be improved to better serve the conflicted needs of maintenance and warehouse logistics operations?\" the objective of this research is to analyse the work order components storing and picking processes, taking into consideration the viewpoints of the warehouse operation and the maintenance planning personnel, in order to resolve the conflict in their needs through the improvements of these processes."
"to design a constraint-based model, we have to define variables, constraints, and objectives. variables represent decisions to be taken. to each activity we associate a decision variable that defines the magnitude of the activity itself. we distinguish primary and secondary activities: some activities are of primary importance in a given plan. secondary activities are those supporting the primary activities by providing the needed infrastructures. in case of the energy plan, primary activities are those producing energy, namely renewable and nonrenewable power plants. secondary activities are those supporting the energy production, such as activities for energy transportations (for example, power lines), and infrastructures (for example, dams, yards). primary and secondary activities are of course linked by constraints stating the amount of each secondary activity per unit of primary. in the model we can state constraints limiting the available budget either on the overall plan, or on parts of it. for instance suppose we have already partitioned the budget into chapters; we can impose the budget constraint only on activities related to a given chapter."
"problems within the current process and identify the possible ways to overcome them. therefore, the outcome of these interviews will be used to develop a conceptual model, which will be used to generate the questionnaire afterward. interviews have particular methodological strengths and weaknesses [cit] . the strengths of the interview method include:"
"three of the suggestions proposed in the new process are believed to have an effect on improving the performance of the work order components delivery process, based on the survey conducted in this study. these suggestions are:"
"environmental policy: global coordination of pollution control is essential given that we all share a single atmosphere and ocean system. mechanisms for internalizing the externalities of pollutants such as carbon dioxide form an integral aspect to any solution that will curtail pollution. economic theory has informed efforts such as the kyoto protocol but more effort is required in order to overcome political hurdles. in related applications, mechanism design theory also informs the design of sustainable management of natural resources such as fishing or tree-felling."
"step 2 -in this step, weights are allocated to each of the criteria and sub-criteria with respect to the element in the level above. in ahp the weights are allocated on a 9 point relative scale [cit] . as the weight assignment is based on subjective judgment, a consistency ratio (cr) checks the consistency of weight assignment later."
"in the development step, the suggestions proposed are to be implemented in an artefact. depending on the situation studied, the development of the artefact could be a new model, process, method, or system [cit] ."
"extracting the opinion in a text document based on a vector of features derived from a document corresponds to the modeling task in a standard data analysis framework. depending on the type of data we have available for this task, different techniques may be applicable to achieve this goal."
"research council, to name a few, now accepting them on cvs for grant applications. this is a crucial development, as publishing in journals currently constitutes the most important metric of academic achievement, and early career researchers in particular are thus highly dependent on this painfully slow process to obtain career advancement."
"however, these desirable properties may directly conflict with one another. for example, budget balance and efficiency conflict in vickrey auctions, which achieve only the latter property."
"in addition to the long picking routes generated, the picking process does not account for the integrity of the work order components. the system does not generate a consolidated picking list by work order; for example, a work order containing 70 components will have 70 picking documents. all documents are printed in several copies, which contribute to the overall cost of the process. each picked item must be packed separately as there is no consolidated list. thus, extra costs are incurred for the additional packing materials, labelling, and processing time. similar to the storage strategies, the sap implementation does not have a standard picking strategy to pick items by work order reference."
"we now describe a case study of application of the decision support system. [cit] along with the detailed corresponding model. we restrict the example to the regional plan chapter devoted to the electric energy production from renewable energy sources. the considered electric power plants are minihydroelectric plants, photovoltaic plants, thermodynamic solar plants, wind generators, and, again, biomass power plants."
"there are mainly two core technologies for supporting the implementation step of the policy-making process that can be used either in isolation or as an integrated solution: social simulation and mechanism design. we will briefly discuss the former next, and the latter in the following section in more detail."
"concerning objective functions, there are a number of possibilities as suggested by planning experts. from an economics perspective, one can decide to minimize the overall cost of the plan, subject to budget constraints. on the other hand, one could maintain a fixed budget and maximize the plan outcome. finally, the planner could decide to produce a green plan and consider environmental indicators such as the air quality, or the quality of the surface water. the system partitions the budget on activities to obtain a sustainable plan for a given receptor. clearly, more complex objectives can be pursued, by properly combining the above mentioned aspects. an example is to use a multicriteria objective taking into account for example the cost and the air quality. in this case, we come up with a pareto optimal frontier. the pareto frontier of the emilia-romagna"
"in general, after a plan is created and assessed, the policy maker should define actions for the plan implementation. it is often the case, in fact, that goals at the regional level conflict with goals at the subregional level and with individual or business goals. thus it is not enough merely to define an optimal regional policy goal; in addition there must be implementation through instruments that aim to ensure that actors behave in a way that will lead to the goal. policy implementation instruments include the following: (1) regulatory instruments: self regulation by voluntary bodies, standards imposed by formal standards bodies, legislation; (2) economic instruments: taxes, fees, and user charges, certificate trading, procurement policies, subsidies; (3) cooperation instruments: voluntary agreements, producer and consumer associations; and (4) information instruments: labelling schemes, reporting requirements, advice services, technology transfer. policy makers have a choice of which of these policy instruments to implement, either individually or in combination. each has advantages and disadvantages, depending on the context, and may have unintended and unforeseen consequences. the selection of the best instruments to achieve a specific goal may be difficult."
"in a traditional publishing model, commentary and feedback from the scientific community exist in the form of formal peerreview, which importantly serves not only to improve the quality of the scientific research, but also to validate the findings. preprints typically undergo no such assessment, making it difficult for readers to distinguish those manuscripts which are scientifically sound from those that are not, and also for authors to demonstrate the quality of their work (i.e. for job interviews and grant applications). while it is unclear what is inhibiting commenting on biorxiv's native comment field, the need for systematic scientific commentary and validation is becoming more recognised and new platforms for preprint peerreview and feedback are beginning to appear and gain traction. many of these initiatives are listed on asapbio's new reimagine review site, where their principal functions are neatly categorized into three domains: (1) curation of interesting work, (2) validation of soundness, and (3) feedback to authors. one such initiative is peer community in (pci), a non-profit organisation that focuses on the latter two categories by providing an open peer-review process for preprints. pci works by bringing together communities of researchers to review and recommend preprints in their field on a voluntary basis, thus keeping costs to a minimum and enabling free preprint peer-review. this, together with the fact that authors can actively submit their work for review, and that pci does not publish the articles and thus claims no exclusivity over them, makes pci a truly unique, community-driven reviewing platform, fully decoupled from the current for-profit publishing system. it is already well established in a number of fields, notably evolutionary biology and ecology, but no such platform currently exists for the neuroscience community, despite its huge potential benefit."
"the study also shows that careful consideration should be given to the picking process. organisations should refrain from piecemeal picking but rather ensure that all the work order components are picked and sorted together before the shipping. this can be achieved by printing a single pick list for all the work order components. picking all the components in one picking route shorten the distance travelled by the picker, hence, improve the picking process efficiency. importantly, picking all the work order components together also preserves the integrity of the order. moreover, the use of pick-and-sort sorting strategy during the picking process eliminates the need for the sorting process afterward. as all the items are stored together in one storage location, the picking and sorting process will require less effort from the storekeeper. the study also confirms that sharing of relevant information in a timely manner would improve the process."
"we denote the state of system as the number of packets (jobs) left in the system after a departing packet (job) has been served. as such, state zero means that zero packets are left after a departing job (packet). the probabilities of packet (job) arrivals are calculated distinctly for each traffic type from equation (1), which is a standard equation for a m/g/1/k queue system. the arrival rates for the tcp, voip and video traffic are considered to be mutually independent poisson arrival processes. this helps to express the packet arrivals of each traffic type as an independent event. the change of state of a system ( fig. 1) can occur due to different combinations of traffic type arrivals, thus we have to take all possible combinations into account for calculating the transition probabilities. note: as stated earlier the change of state of the queue system was represented through combinations of different packet types expressed as an integral multiple of voip packet size of 100 bytes."
"surveys can be a qualitative technique, used to capture an understanding of the phenomenon at a point in time [cit] . within this approach, the researchers \"sample many respondents who answer the same questions, measure many variables, test multiple hypotheses, and infer temporal order from questions about past behaviour, experiences, or characteristics [...] an association among variables is measured with statistical techniques\" [cit] . surveys are an accepted tool to study society and organisations [cit] . as this research involves research in an organisation, the survey is considered to be a useful tool."
"preprints are early versions of scientific manuscripts, preceding peer-review and final publication in journals. they are posted on online servers that are freely accessible both to the scientific community and to the general public. many such servers exist, supported by various research organisations, that archive preprints for different scientific fields -arxiv for maths and physics, chemrxiv for chemistry, paleorxiv for paleontology, biorxiv for biology and very recently medrxiv for health sciences, to name just a few. hence, in most if not all scientific fields, researchers now have the possibility to share their findings before publication thanks to preprints; but what are the advantages of doing this?"
"predicting the behavior of the traffic was also proposed to reduce power in 3g lte without affecting the user's experience in terms of delay and throughput qos [cit] . this prediction information could then be used to switch the rf modem of the 3g lte ue into on (active) and off (sleep) periods. this may be feasible for constant bit rate (cbr) traffic such as voice, as it is fairly predictable. however, other traffic, such as web browsing, is less predictable, and the varying traffic characteristics combined with changing traffic load in the network makes the prediction difficult. three network algorithms have been proposed to adapt the various drx parameters and have been shown that with prudent setting of parameters and their adaptation efficient power savings can be achieved [cit] . the three algorithms proposed are static, semi-static and dynamic drx. in static drx, the drx parameters and drx cycle length are kept constant for the entire webbrowsing session. in the semi-static case the drx cycle is kept constant but the drx on duration parameters are optimized. finally, in case of dynamic drx the inactivity timer is used and the on duration is set to one transmission time interval (tti). it has been shown that dynamic drx is the most effective amongst the three algorithms, as it involves the use of the inactivity timer [cit] ."
"from the three gaps discussed earlier in this section and from the interviews conducted with maintenance planners and warehouse operation personnel, the two stakeholders of the process, the following improvements are suggested:"
"another, interesting observation is that the power consumed by the ue at the cell edge decreases at higher server ue ue epc enodeb figure 11 . energy consumption in ue based on tcp data rates data rates compared to the ue closer to the enodeb. this is because for higher data rates the signaling from enodeb assigns a higher mcs index to the ue at the cell edge, due to which a higher transport block size is used. this results in fewer number of ttis transporting the data. by using fewer ttis at higher data rates the energy consumed by the ue at the cell edge is lower than by the ue near enodeb. in fig 11, we also compare simulation results with the result obtained by using the analytical model of equation (10) . it can be seen that the analytically obtained result closely follows the simulation results for both the ue closer to enodeb and the ue at the cell edge. however, at higher data rates i.e. [cit] kbps (a to b in fig. 11 ), our analytical model does not take into account the mcs feature and the path loss based on the distance between ue and enodeb. in spite of this, our analytical equation is rather accurate in the commonly used data rate range, i.e. up to 2 mbps."
"supervised learning is a technique that takes (manually) labeled data constituting the training set, and produces a model that can be seen as an approximation of the unknown function that maps the values of the variables into labels. this type of models can be used to assign labels to new unlabelled samples."
"further, in equation (10), we assume that the arrival rates for all the traffic generated in ue are independent and follow poisson distribution. this assumption enables us to add the arrival rates for all three independently generated traffic flows in the ue i.e. audio (voip), video and tcp data. also, it can be noticed in equation (10) that whatever amount of time we decrease t on by, we increase the mean waiting time of a packet in the m/g/1/k queue by the same amount. that is, essentially we decrease the t on time to decrease the power consumed, but at the cost of an increase in the mean waiting time of the packet in the m/g/1/k queue system. as stated earlier, this is the underlying principle behind the dtx/drx power saving mode in 3g lte, and our equation (10) satisfies it."
"social simulation can be used for assessing the social impact of policy instruments and mechanisms. in fact, not only economic aspects affect the agent decision."
"there are a number of problems in this process at present. first, the planning step and the environmental assessment are performed in sequence: in case a plan contains negative effects on the environment, only corrective countermeasures can be applied a posteriori. if planning and environmental assessment were performed at the same stage, an environmentally well-assessed plan could be produced instead. second, the implementation instruments are decided without any proper strategy nor assessment of their effect on the society. these effects are indeed checked during the monitoring phase to measure whether they are conformant with the planning objectives in an ex-post fashion. third, the steps are always performed manually with no (or very little) information and communications technology (ict) support."
"the table shows that the overwhelming majority of the respondents are of the view that the suggested improvements would either be beneficial or they are neutral. in particular, the first five suggestions more than half of the respondents are in favour of the change. the suggestions for sharing and using the weekly workshop schedule to plan and dispatch delivery trips did not receive a significant positive response. on the other hand, almost the same number of respondents had a neutral view, indicating that they were ambivalent about the impact the suggestions might have on enhancing the delivery process. overall a relatively small percentage of respondents disagree with the proposed improvements. the proposed process suggests that information about the complete received work order components at the warehouse can be shared with the maintenance planners. the maintenance planner will then use this information to schedule the work order execution. this suggestion received the highest agreement percentage from the respondents at 87%. sharing this information among the two departments, maintenance and warehouse operation, will improve the internal integration within the company. this integration is believed to be a requirement for the development of efficient external integration [cit] ."
"the second gap in the process is the lack of collaboration and the use of the available information. even though all the information about the staged materials is available in the sap system, the maintenance planner does not use this information to develop the weekly workshop schedule table. instead, the materials will be requested from the warehouse in patches. then, when all the components are available at the maintenance site, the schedule will be developed. this approach disregards the opportunities for collaboration with the warehouse operation from the use of available information. moreover, the weekly workshop schedule table is not shared with the warehouse operation personnel, who are then unable to anticipate the forthcoming workload. the planning and utilisation of the resources at the warehouse operation become suboptimal. however, warehouse operations might be challenged with a huge number of requests for work order components delivery. the performance of these deliveries might be poorer if advanced planning by sharing information between maintenance planning and warehouse operation was instituted."
"as an example, the matrices currently used in emilia-romagna, a region of italy participating to the epolicy project, contain 93 activities, 29 negative impacts, 19 positive impacts, and 23 receptors and assess 11 types of plans. as far as computational demand is concerned, managing linear constraints is easy (this is clearly an approximation of reality). however, if we consider nonlinear relations between activities and pressures and nonlinear relations between pressures and receptors, the model and its solution would be much more computationally challenging. an example of nonlinear dependency is the one on the landscape. if the landscape has been already compromised by an activity (for example the construction of a big biomass power plant), the addition of another activity would not result in a double negative effect. basically the effect on the landscape presents a saturation after the first activity that negatively impacts on it."
"further cycles of the design science approach were not undertaken due to time constraints. based on the feedback from the survey, a pilot study would be undertaken with the most promising of the proposed processes, with rigorous assessment of outcomes of the designed processes. comparisons with extant processes would then determine which of the processes should be implemented in a full redesign of the maintenance and warehouse operation processes."
"step 4: the dtx cycle on time is modulated by the value of mean packet delay in ue's m/g/1/k queue system and the blocking probability. that is, to reduce energy consumption, the value of mean packet delay and blocking probability are increased but within limits such that the resulting mean packet delay and the blocking probability will still be below the maximum values allowed by the qos of the application. as explained earlier with regard to equation (10), if the mean packet delay value is increased by 1 unit of time, the t on time of the dtx cycle is decreased by the same amount and energy consumption is reduced."
"before any implementation, these plans have to be environmentally assessed, under the strategic environmental assessment (sea) directive. 1 sea is a method for incorporating environmental considerations into policies, plans, and programs that is prescribed by european union policy."
"physical and mathematical sciences are the forerunners in preprinting and scientific validation outside of the traditional publishing system, and their many platforms have waxed and waned over the years [cit] . perhaps we can learn from them, as the evolution of preprints in biology has so far echoed that in physics. physics platforms were typically driven by single individuals, and often peaked within two years from launch before disappearing in the following years [cit] . while having multiple experimental preprint review platforms will no doubt also benefit the field of biology -because many substitutable options leads to a healthier publishing ecosystem [cit] -we should still aim for sustainability: to establish self-sustaining systems of self-organizing peer-review, dependent on and run by the wider community rather than a few individuals [cit] . the key in our view consists of approval by the field, an operating system that is intuitive and easy to use, low workload, adequate mitigation of perceived risks to reputation and journal submission and reliable stewardship far into the future under a managing board that is refreshed periodically. we hope that pci in general, and pci c neuro in particular, will help to drive this new era of open science, facilitating the free and efficient dissemination of validated scientific findings, not only for the scientific community but also for the general public."
"the integration of a global perspective taking into account regional needs, financial constraints and objectives, and the individual viewpoint would be a real added value of a decision support system. regional policy decisions foster global objectives. these objectives may include moving in the direction of the kyoto protocol aimed at the stabilization of greenhouse gas concentrations in the atmosphere at the level that would prevent dangerous anthropogenic interference with the climate system, or the 20-20-20 [cit] in europe: reducing by 20 percent its greenhouse gas emissions, having a 20 percent share of the final energy consumption produced by renewable sources, and improving by 20 percent its energy efficiency. these are clearly global objectives that may not be perceived as a priority by individuals. the smooth interaction of the global and individual levels in a unified and flexible computeraided tool constitutes a political innovation and could produce a huge impact in terms of optimal resource allocation and land use activities."
the global priority/weight of each alternative choice is obtained from a synthesis of priority computed for each alternative choice across all the criteria and subcriteria.
"we have modeled and validated the performance of composite traffic belonging to the (i) interactive class in two cases: continuous uplink data and ftp, and (ii) conversational class: live video and voip. we considered the arrival rates of the data (tcp) packets to be generated by a poisson process with exponentially distributed inter-arrival times [cit],the arrival process is markovian in the m/g/1/k system. to generate a general service time schedule for the traffic mix of different traffic types, we considered the data packet sizes to be pareto distributed for http traffic [cit] . the expression for truncated pareto distribution is given in equation (4). in the simulation, we have generated tcp truncated pareto distributed packets with a mean size of 1000 bytes and voip packets of fixed size 100 bytes and video packets of fixed size 400 bytes [cit] . the tcp packet sizes are generated by using the following parameters of p(k, α) from equation (5)"
"several modeling techniques, often collectively referred to as social simulation, have successfully been used to represent the responses of societies to policy interventions. agent-based modeling (abm) [cit] ) is the most appropriate to represent complex social dynamics because of its capacity to capture interactions and responses in a spatial environment. however, increasingly methods of social simulation are moving towards a common ground, with agent-based modeling incorporating aspects of system dynamics and microsimulation. an agentbased model is a computational method for simulating the actions and interactions of autonomous decision-making entities in a network or system, with the aim of assessing their effects on the system as a whole. individuals and organizations are represented as agents. each agent individually assesses its situation and makes decisions on the basis of a set of rules. even a simple agent-based model can exhibit complex behaviour patterns because a series of simple interactions between individuals may lead to the emergence of more complex global scale outcomes that could not have been predicted just by aggregating individual agent behaviours."
"the proposed energy management method obtains the qos parameters, namely mean packet delay and blocking probability in the ue uplink queue system for a traffic mix of video, voice and tcp data as was explained in section iii. the obtained parameters are then used in table v, which are then processed by the gra to select the best alternative i.e. the best set of qos, channel and application parameters that will result in the goal of optimal power management i.e. table vi. to increase the accuracy of the selection, the gra makes use of the weights obtained by using the ahp in table iv. the proposed framework also provides flexibility to the user to obtain the qos, channel and application parameters as per their own method(s) and then incorporate within it."
"this section outlines how the data were analysed to present opportunities for improvement and focuses on the use of the gap methodology, indicating where the present processes fail to achieve the desired outcomes."
"assessment of physiological variables other than v'o 2,peak and hr during an eib test may be useful. v'e,peak, fr, rer and arterial oxygen saturation (sa,o 2 ) may give valuable information about dynamic hyperinfl ation and breathing pattern and be helpful in the diagnosis of differential diagnosis to eib."
"the data quality elements are linked to individuals for processes and results. these individuals are used in swrl rules and sparql queries. classes for features with constraints are defined. sfos are devised to define which restrictions will be applied to dataset according to specifications by a domain expert. they are mostly hierarchical. sfos import sdqo and data ontologies. the data ontologies are likely to have some faults. the system should be robust and stay consistent. the system is designed to be robust and user-friendly, therefore usable. the common parts of the sfos are moved to sdqo. most of the rules are in the sdqo. the use of sdqo enables domain-independent, easy-to-update quality assessment with the sfos. especially with an ontology editor, it is expected that domain experts can quickly understand how to manipulate and update their sfos, when necessary. sfos typically do not use the whole capability of the sdqo. when the rulesets change, even without needing to change the sdqo significantly, it is expected that the new sfo can be implemented. thanks to the prevailing owa, sdqo is easy-to-update, when necessary. with owa, truth of statements does not change."
"dedicated hardware accelerators can achieve orders of magnitude speedup when compared with cpu based implementations. the processing element (pe) of these accelerators usually contains tens to hundreds of alus such as graphics processing units (gpus) or deeply pipelined data paths on the field programming gate array (fpga) devices. at the expense of program control flexibility, these pes can outperform the modern cpus by massive parallelization and off load the most computational intensive task from the cpus. the large amount of onchip fast caches and tightly coupled off-chip memory also contribute to performance improvements. high performance clusters (hpcs) with dedicated accelerators offer practical solutions in real world applications [cit] ."
"for example, vhdl programming skills and knowledge of the internal resources of the fpga are critical for optimizing the kernel performance on fpga accelerators. the local memories of these accelerators form a complex memory hierarchy. without a unified flat memory space, the programmers need to explicitly manage the memory systems and move data between them. this overhead reduces productivity, constrains the achievable speedups, and increases the difficulty in kernel optimization. integrating different types of accelerators in the system worsen the situation. to avoid this complexity, most applications utilize a single type of accelerator even when other types of accelerators sit idle in the system. these difficulties and complexities appear repeatedly in many applications and systems. a general programming framework is desirable to unify the application development practice and improve developer productivity for heterogeneous clusters."
"there are also code patterns which depend on parameters of the application. these parameters are usually defined before the implementation of the modules. for example, the structure and size of the main data variables in an application are usually known in early stages. thus we can allow users to enter the reference, type, size and read/write patterns of these variables. code segments such as creating the shared memory and filling in the contents can easily be generated according to user information. another parameterizable code pattern involves global communications. there are common communication schemes as suggested by the mpi function calls. we allow users to select from these common schemes and related them to data variables. code segments of sending and receiving these data can then be generated automatically."
"the qualitative data are analyzed from the interview sheet, observation sheet and diary notes to describe the improvement of the students' achievement. the qualitative data is analyzed to know the students' problems in teaching learning process."
"after analyzing the data, the finding of this research showed that reciprocal teaching method was able to improve the students' reading comprehension. the data showed that the students score increased in every test. it means that the actions were done successfully. it was supported by the mean of orientation test was 61.79, cycle i 72.24 and cycle ii 81.71."
"a module can be either for computation or for control. the computation modules, m comp 1..m comp 3, carry the application kernels which are accelerated by the targeted pes. the control module, m ctrl, commands and synchronizes the m comp to complete the application. an application consists of at least one m ctrl and one m comp ."
"the first section of the example acf indicates three available modules in the application. key attributes, such as the path to executable in the file system and the target pe types, are also included. each module is assigned a unique identity. the second and third section in the acf create two groups of workers for the application. there are three workers mapped to the three modules in each group. like the module, each group is given a unique identity. the ofst and size attributes indicate the starting offset and the size of the data assigned to each group. the data assignment includes key information for an array data structure, while the actual interpretation of these two attributes depends on the implementation of user modules. the data are further divided and assigned to the workers in the group using similar work attributes. for example, the second worker in group 1 is an instance of module 1 which will process two data entries starting from the fifth entry. if the assigned data size is 0, the workers, which are usually taking the control role, will not process any data. the last element in group 1 requests the m ctrl in the group to send the finished data to the m ctrl in group 0."
"reading is one of the four language skills that is important to achieve to show that one is able in learning a language. since reading is an activity to draw meaning or take information from printed or written text, one who does reading activity is expected to know better after reading. in fact, many students are still having problems with reading. as a proof, when the researcher did a teacher training practice [cit], there were students who were not interested in reading, they were unable to find the meaning of what they read and they did reading when they had homework only. these students' failure may be not only caused by the students' lack of knowledge to comprehend the text, but also caused by the teacher's strategy in teaching."
"similar to a stand alone application targeting a single accelerator, m comp interfaces to the vendor specific device driver to initialize and configure the pe; download the data to the accelerator local memory; monitor and synchronize the hardware; and collect the results from the accelerator. the major difference of m comp from a stand alone application is its interaction with m ctrl . figure 3 shows an example of the interaction during an application execution using the fpga accelerator. instead of reading data and storing results actively, m comp starts being idle. m ctrl prepares the data and signals m comp to start working. after finishing accelerator computation, m comp reports the status to m ctrl and becomes idle again. besides controlling the m comp in the group, m ctrl also communicates with m ctrl in other groups and performs data read/write. a single m ctrl can master multiple m comp although only one m comp is shown in figure 3 . another important task of m ctrl is to launch the associated m comp as described in section 4.2."
"a is a subclass of c, an sfo class; b is a subclass d, an sfo class; these are subclasses of some sdqo classes that take part in swrl rules establishing the relevant error properties involving faulty features in a, b and forbidden overlaps relation."
"the acf is in xml format and is placed in a networked file system such that all m ctrl can access it. it is created by application programmers to chain up all the required modules in the applications. the acf provides the controlling modules three types of important information: (i) the available modules, (ii) the groups and workers with them, and (iii) the communication between groups. an example of acf is shown below. its content forms a recipe showing how the application will be executed in the cluster."
"when performing an eib test or a test for assessment of v'o 2,peak at a fi xed inclination of the treadmill, it is common to use an inclination of 3 o or 5.3% for children, untrained subjects and athletes [cit] . as regards athletes within endurance sports, with the exception of runners, both eib tests and v'o 2,max tests are usually performed at 6 o or 10.5% inclination to minimise the effects of seasonal changes in v'o 2,max caused by changes in running technique and running economy as a consequence of seasonal shifts in the type of training, e.g. running and skiing [cit] . on the other hand, a steep inclination can cause local fatigue in the thigh and leg muscles, thus limiting the achievement of v'o 2,peak, especially in children and untrained subjects [cit] ."
"the rich set of apis makes the framework very flexible but also requires attention to implementation details. a method that would reduce the need for such details is desirable to further improve developer productivity. observing that there are similar pattern in the applications structures, we decide to provide a templatebased code generation tool. there are code patterns which are required in all applications, such as the parsing and analyzing of the acf and the termination of workers. these processes are relatively static from application to application. thus we put them into the static section which will be emitted directly from the template codes."
"exercise-induced bronchoconstriction (eib) infl uences daily life activities and sports activities in children, adolescents and adults. to enable optimal choice of treatment, an accurate assessment of eib is therefore important. eib consists of bronchoconstriction occurring immediately or soon after physical exertion [cit], and is best assessed by a standardised exercise test. running on a treadmill for 6-8 min at a submaximal work load is a commonly used test [cit] . lately it has been maintained that an exercise load corresponding to 95% of maximum heart rate (hrmax) is preferable to obtain a high sensitivity [cit] ."
"however, assessment of eib and v'o 2,peak in a single test may reduce the burden for the patient by saving one test day; simplify diagnostic and monitoring procedures of the patients and the costs for the health system. the following text will therefore focus on the assessment of eib and exercise capacity by using an eib test protocol."
"in patients with obstructive chronic respiratory disorders, including asthma, assessment of physical fi tness may give important information of the severity of illness and the ability to master physical exercise. particularly important information can be obtained when this type of testing is combined with measurements of breathing reserve (br), made by measuring maximal voluntary ventilation before exercise and comparing it to v'e,peak achieved during exercise. the difference in is defi ned as br: normal br in healthy subjects is 20-40% [cit] ."
"despite its simple mechanism, this template-based code generator can produce a complete application based on a few user inputs. this will reduce development time and help users focus on the accelerator kernel and control logic optimizations."
"the life cycle of a distributed application is presented here. after all necessary modules are created by their specific building tools and the acf is created to configure the execution scheme, the application is ready to be launched."
"the intra-node communication between pes is based on pcie system bus where the gpu and fpga use separate channels and thus can transfer data simultaneously without blocking. the cluster wide communication is based on gigabit ethernet through the nic port on each node. the versatility and flexibility of gigabit ethernet is at the expense of high latency and indeterministic communication. to address this problem, a second inter-node network is added using the four gtp interfaces in the fpga platform."
the m ctrl worker can be terminated according to user program code or by sending an append message to it. the m ctrl worker will broadcast the append message to the group and wait until all other workers have terminated before terminating itself.
"according to the set of rules depending on appropriate and chosen elements for data quality, sdqo is also responsible of processing and integrating data quality elements with associated procedures and implementing the procedures in accordance with the geospatial data and the quality elements. sdqo prepares the resulting spatial data quality ontology which relates data quality results with tested data and prepares it for queries or publishing."
"while sdqo:resultfordata object property create a relationship between quality results and data, sdqo:hasresult relates data quality elements to results. sdqo:datahasresult is inverse property to sdqo:resultfordata."
"different test protocols are commonly used in the diagnosis of eib and in the assessment of exercise capacity (v'o 2,peak). recently one study has shown the possibility of combining assessment of eib and v'o 2,peak into one eib test protocol, even though the gold standard in assessment of v'o 2,peak is an incremental protocol. additional and useful information about asthma can be obtained and differential diagnoses to eib can be assessed by measuring different physiological variables during the eib challenge. the following article will focus on the advantages of measuring exercise capacity during an eib test, after a short discussion of different test protocols and ergometers."
"in sdqo, there are three top classes directly below owl:thing, one for data, one for data quality elements, and one for data quality results and processes. these are ogc:spatialobject, sdqo:dataqualityelement, sdqo:dataqualityresult, respectively as shown on figure 3 ."
"this research was conducted by administering six meetings in two cycles. there were four steps in each cycle namely; (1) planning, (2) action, (3) observation, and (4) reflection. each cycle was carried out in three meetings. so, there would be six meetings all together."
"once the m ctrl worker receives this message, it marks the data segment processed. if all data are marked processed, the m ctrl worker enters the data collection stage. depending on the application, it may send the data in the shared memory to remote m ctrl workers as instructed by the acf, or writes them to a file, or iterates to send datrdy to the m comp workers again."
"the control logic and the computations are the core elements of an application. these control patterns are often different between applications. however the development framework imposes limitations on how the control mechanism is implemented. since the framework forces an event-driven flow based on message passing, the main loop in a module usually consists of the process of idle listening and message parsing. the code generation tool creates this basic main loop structure as a skeleton or place holder, allowing users to provide application specific control later."
"the application programmers can instruct the m ctrl workers to periodically send information requests to a predefined node. the returned information is a complete data structure in the acf format. by doing so, users can dynamically adjust the execution scheme to obtain the most desirable trade-offs fro this application."
"a. it is important to the english teacher to use an appropriate strategy in teaching reading such as reciprocal teaching method, for instance reading narrative text."
"sfo is designed to be manageable by domain experts to be even without semantic web expertise. a gui is created for users to input rules according to the specifications. once rules are defined by the help of gui, an owl file (sfo) is created as a result of translations from gui to csv and owl files respectively. the translations and the assessment framework are out of the scope of this study. the sfo and its components will be explained in detail."
"the workload is distributed to gpu and fpga in an asymmetric way, since the measured performance of fpga is higher than that of the gpu. this schedule is static and produced after each kernel is fully tested."
"the sdqo:geomclassifiedfeature is defined to classify features classified according to their geometries. it has three subclasses, sdqo:calcpoly, sdqo:calcline and sdqo:calcpoint, declared to be owl:disjoint. any feature that is associated with a geometry that has a valid ogc:aswkt value, is automatically put in the correct one according to the ogc:aswkt value, using a swrl rule. this information is to be used in other rules."
"once the worker for an m ctrl instance is created, it reads and parses the acf file. by default, it maps its rank in the mpi system to the group id. this can be altered by sending the whoami request to the rank0 node. then it creates other worker instances by loading the corresponding modules into memory. after that, it creates the message queue for all the workers as a control channel. this m ctrl worker may read the data from network shared data storage or receive data from to minimize the data transfer overhead, the m ctrl worker will store the data in shared memory which all workers are allowed to access. application specific data protection and synchronization are implemented in the user code section. the controlling worker then sends a datrdy message to each m comp worker, and tells them the remapped data offset and size in the shared memory."
"since the application produces random variables to initialize the design, there is no external input for both the fpga and gpu kernels. the random sequences are generated within the acceleration kernels. the workload is distributed by assigning different sets of paths to the m comp workers. the workers partially reduce the results of the assigned paths and send them to a single data sink where the final expected values are computed. figure 5 shows the achieved speedup of the hardware accelerators based on the proposed programming framework. the results of a single gpu and a single fpga are compared with that of a four-thread software version running on the quad-core cpu. the hybrid cluster result is a comparison of a 16-node heterogeneous accelerator cluster to a 16-node cpu only cluster. since there is no gpu implementation of the pub/sub application, the cluster version of pub/sub utilizes the fpga accelerators only."
"another benefit of this distributed module structure is that the original tool chains of the accelerators are preserved as shown in figure 2, improving productivity. integrating the accelerators in a unified program description usually requires an abstraction layer involving source-level translation or syntax extension. in this framework, the absence of this abstraction layer improves ease of use and efficiency. application developers for different accelerators can use the most familiar tool chains to develop and optimize modules, based on the unified communication api."
"there are four different types of communication apis in the heterogeneous accelerator cluster as shown in figure 4 . the first type is at the lowest level, involving communications between the m comp workers and the targeting accelerators. the api for this type is provided by the accelerator vendors or the application programmers. the functions include configuring fpga devices with bitstreams and copying memory contents from host memory to gpu memory."
"two generic classes are created in the sfo; they are set as subclasses of the relevant classes in sdqo. if there are already six \"c1,c2, forbidden,crosses\" pairs, the created classes are sfo:classcrf07 and sfo:classcrs07. here \"cr\" stands for \"crosses\", \"f\" stands for \"first\" and \"s\" stands for \"second\". restrictions are among anonymous classes in owl. sfo:classcrf07 is set as a subclass of sdqo class sdqo:classcross1 and sfo:classcrs07 is set as a subclass of sdqo:classcross2."
"it is desired to make a robust framework that can be applicable to spatial data, independent from the domain. this will reduce redundancy and increase interoperability. hence, there is a need to create ontology for the designed framework. the components and details of the framework is out of the scope of this paper. the main focus is the explanation of devised ontologies. these are; specification ontology (sfo) and spatial data quality ontology (sdqo). following sections describe sdqo and sfo and briefly explain how to use them for quality assessment with an example rule."
"the choice of test protocol and ergometer should be based on the purpose of the exercise test and the patients performing the tests: children, adults, elite athletes, and untrained or obese patients. if the reason is to provoke eib, an eib test will be used. if the purpose is to measure physical fi tness, an incremental test protocol is commonly used."
"improving students' reading comprehension is not an easy task. teaching reading nowadays is directly faced to some problems above. reading comprehension centers on the ability to derive meaning from what is read. without comprehension, a student does not really read. it is the teacher's responsibility to help the students to comprehend the text. teacher needs to assess students' reading needs. teacher should design and redesign courses in order to make an interesting learning process for students."
"a sfo has the class hierarchy reflecting the associated specifications. furthermore, these specification hierarchy classes have superclasses, still in sfo. these classes are created to deanonymize the restrictions. they are the top classes with sfo's iri. they are subclasses to sdqo classes and establish the connection to sdqo. furthermore, these top classes should have associated geometries under the appropriate simple features class, such as sf:polygon or sf:linestring."
"the workload is evenly distributed to fpga accelerators. since there is no correlation between the matching processes of different events and no final results need reduction, the design scales linearly with the increase in nodes."
the structure of the dialogue and interactions of the group members require that all students participate and foster new relationships between students of different ability levels.
"until now, assessments of eib and v'o 2,peak have been performed with two different test protocols on separate days. one study has shown the possibility of combining assessment of eib and v'o 2,peak into one exercise test [cit] . stensrud and carlsen [cit] compared v'o 2,peak by applying an eib test protocol and a stepwise protocol and conclude that neither v'o 2,peak nor peak minute ventilation (v'e,peak) differed between the two protocols (fi g. 1) and thus gave useful information about both eib and physical fi tness."
"all these development frameworks and programming models require an efficient way of utilizing the accelerators and re-compiling the code when the accelerator types change. also, the workload distribution are often managed by the runtime systems which have little knowledge of the computation and communication pattern of the application."
"based on the observation list, the researcher has good ability to opening the class. the researcher motivated the students in teaching learning process. the researcher provided the material clearly and systematically. the researcher could organize the classroom effectively. the researcher gave enough chance to the students to asking questions or giving comments. the researcher had good interaction with the students. the students paid attention to the teacher's explanation and instruction. the students also gave good response to the teacher. the students' interaction was good also in group discussion and felt interesting in using reciprocal teaching method. a complete analysis can be seen in appendix d."
"previous studies have compared different test protocols for assessing v'o 2,peak both in children and in adults. in spite of a general consensus that an incremental test protocol lasting between 8-12 min will elicit the highest v'o 2,peak with the lowest perception of diffi culty and discomfort, studies have not found a difference in v'o 2,peak between different test protocols [cit] concluded that v'o 2,peak was not different with a constant workload protocol lasting between 4-10 minutes as compared to an incremental ramp protocol. [cit] suggested that v'o 2,peak can be reached at a constant workload corresponding to 105% as well as 95% of maximum workload. this is in agreement with cooper [cit] who maintained that short bouts of high-intensity exercise are the physiological way of studying children, rather than repeated stepwise exercise testing. different test protocols may [cit], with permission from the publisher. ) 80 100 90 110 130 120 140 160 150 180 170 190"
the input events are independent and distributed to different m comp workers for parallelization. the outputs contain lists of triggered subscribers which are also independent. there are only fixed-point number comparison operations in the algorithm. the performance is limited by the fpga off-chip memory bandwidth and the synchronization process between the two steps.
"based on the explanation above, the writer is interested to conduct a research which is entitled \"improving students' achievement in reading skill by using reciprocal teaching method\". in this research, the writer wants to know, how far the reciprocal teaching can improve the students' achievement in reading."
"even though no signifi cant differences in v'o 2,peak or v'e,peak were found, it should be remembered that the results may not apply to all asthmatic subjects. anti-asthmatic treatment has to be withheld before eib testing according to european respiratory society and american thoracic society guidelines [cit], and this may infl uence v'o 2,peak if bronchoconstriction occurs during the test. however, several previous studies have shown that bronchoconstriction occurs soon after a 6-8 min exercise test and not during the to air fl ow, the lung may not have enough time to empty before the next inhaled breath and fr will thus increase to maintain the ventilatory demand. rapid breathing, for any reason, will trigger this vicious response, and even a mild episode of increased dyspnoea may result in anxiousness and fear leading to the rescue breathing pattern and rapidly making the dyspnoea worse. learning to recognise and control the emotional aspects of the breathing pattern may help to control breathing."
"since each module targets a single type of accelerator, the management of adding and removing accelerators to and from the application is similar to managing files in a directory. this modular structure ensures that a module will continue to function correctly regardless of other modules. this feature provides flexibility in application development. for example, the application can start functioning as soon as any of the accelerator modules is ready, and it can be gradually improved as new modules become ready. unlike other systems which hardwire the application in a single executable, re-compilation of the whole application is not necessary in this framework when updating the accelerators."
"a user will launch the m ctrl module through the mpi system. the remote process manager of the mpi system is used to create multiple instances of the m ctrl module in the cluster nodes. there is no indication of which nodes are used in the application, since this information cannot be determined at compile time. the available resources are dynamic in a multi-user multi-application environment. thus the programming framework can integrate well with third party resource management tools, such as the torque/maui system installed in the axel cluster. there is also no limitation to a one-toone mapping between groups in software and nodes in hardware, as long as there are no conflict of accelerator requirements."
"based on the curriculum in senior high school grade tenth, there are some types of text that must be taught, they are: reports, narrative, analytical expositions. but in this case, the research focuses on narrative text by using reciprocal teaching as method in teaching."
"elite athletes are more likely to report asthma and asthma symptoms than agematched controls. the prevalence of asthma and bronchial hyperresponsiveness (bhr) is higher among elite athletes assessed both by questionnaire and objective measurements [cit] . on the other hand, it is known that reduced physical fi tness and physical activity are important for the development of chronic disorders, including asthma [cit] . increasing prevalence of overweight, obesity [cit] and asthma [cit] in children and adolescents is reported, and a number of longitudinal studies have reported increasing risk of developing new asthma or asthma symptoms in obese children and adolescents [cit] . it was recently suggested that the lack of physical activity, more than the obesity itself, increases the risk of asthma [cit] ."
"isolating the accelerator kernels in independent modules introduces an integrity problem of the application, since none of the modules have knowledge about the capability or even existence of other modules. this information, excluded from program code, is captured in a auxiliary meta file call the application configuration file (acf). the acf plays an essential part in this framework."
"the last type covers communication between remote workers from different groups. this set of apis is a warper around the mpi functions calls. the main idea is to encapsulate the mpi related details and provide a group-worker view for global communications. currently, only point-to-point and all-to-all schemes are supported."
data producers can select the rules that should be applied to a specific spatial task in a domain. formalized rules for a domain can be stored and reusable for further assessments.
"pulmonary hyperinfl ation or dynamic hyperinfl ation is usually defi ned as an abnormal increase in functional residual capacity (frc), for example volume at the end of a tidal expiration [cit] . this is seen in patients with obstructive pulmonary diseases, including asthma. with increased airway obstruction causing increased resistance be needed to assess physical fi tness in children and adolescents with different diseases. whereas a gradual increase in speed and inclination of the treadmill may be benefi cial for subjects with cardiac diseases, a more rapidly increasing protocol may be better suited for asthmatic children and adolescents [cit] ."
"this research finally brought a finding that the score of the students were increasing in each test. the mean of orientation test was 61.79, cycle i was 72.24 and cycle ii was 81.71."
"more knowledge about different climatic conditions in relation to eib and exercise capacity is needed in order to give optimal training advice and treatment to asthmatic patients in relation to physical activity. elite athletes often practise altitude training in unfavourable environments. more knowledge is also needed in relationship to regular physical training of asthmatic subjects, especially in countries with sub-arctic climates, where the winter season can be quite cold. further more it is not unusual for children and adults to take part in activities such as mountain-climbing, skiing and tracking in medium or higher altitudes where atmospheric pressure is lower than at sea level."
"a straight forward implementation using the framework will turn the application to static load distribution, since the subsets of data assigned to each worker and its associated accelerators are explicitly specified in the acf."
"further, in english curriculum, educational unit-oriented curriculum (kurikulum tingkat satuan pendidikan, ktsp) of senior high school level states that there are four language skills that should be achieved in learning process namely, listening, speaking, reading and writing. reading is one of the four skills that must be mastered. it is stated that the students are intended to comprehend the text."
"reciprocal teaching method offers a way to help students to be active and easier to interpret meaning or information in reading. this method benefits both the teacher and the students. reciprocal teaching method enables students to comprehend the text with or without the teacher, then for teacher reciprocal teaching helps him/her design an interesting teaching process."
"obtaining additional information by using exercise testing in the laboratory in the diagnosis of asthma exercise [cit] 36], and v'o 2,peak may thus not be affected. if bronchoconstriction occurs during the exercise test v'o 2,peak may be affected and a new test for measuring v'o 2,peak with pre-medication may be performed."
"sdqo:fixedreffeature has owl named individuals to be used in rules as reference markings. for instance, attribute tests can make use of reference individuals. sdqo:restrictedfeature: \"features according to the property restrictions\". sdqo:restrictedfeature has four direct subclasses, sdqo:interobjectprrf, sdqo:intradataprrf, sdqo:intraobjectprrf and sdqo:interdataprrf. rules and given subclass relations determine which features these classes have. the labels and the descriptions are listed in table 1 . \"intra\" classes are for restrictions within a single class and \"inter\" classes are for restrictions in class pairs. there are sdqo datatype properties for attribute tests such as sdqo:dataprop01."
the findings of this study are important and useful for: a. the english teachers to improve their interest and ability in teaching reading by applying reciprocal teaching method b. the students to improve their interest and comprehension in reading by practicing reciprocal teaching method c. the researcher who wants to conduct a research related to this study.
three example applications are implemented on the axel cluster using the proposed programming framework. these three applications have different computation requirements and communication patterns which exercise various aspects of the cluster. all fpga acceleration kernels are developed using vhdl and implemented using the xilinx ise 11.5 tools. the gpu kernels are compiled using cuda 2.2 tools. the accelerator results are compared with an amd phenom quad-core cpu. all floating point computations are performed in ieee-754 single precision format.
"specification classes are sfo classes such as \"sfo:road\" and \"sfo:building\", that exist in the specifications. they can have subclasses; sfo:permanentlake can be a subclass to sfo:lake."
"users can create application specific attributes in the acf and retrieve the values later in the m ctrl using the provided apis. also, the interpretation of the predefined attributes is controlled by user program. some structures, such as the output element, can even be bypassed in applications if a specific scheme has been captured in the program code. the acf gives developers significant freedom to configure the execution of an application in the cluster."
"it is known that cold, dry air increases eib, and that humid air reduces eib in subjects with asthma (fi g. 2) [cit] . however, few studies exist concerning the effect of different climatic conditions upon exercise capacity in subjects suffering from eib, and these studies have given confl icting results [cit] . hypoxic gas inhalation has been reported to enhance bronchial hyperresponsiveness and result in broncho constriction in some animal models, and in humans with asthma [cit] . the data on humans have so far been confl icting [cit] . several authors have, on the other hand, reported reduced exercise capacity in healthy, trained and untrained subjects in a hypobaric environment due to reduced sa,o 2 [cit] ."
"peak oxygen uptake, presented as ml per kg per min, is the best measure of physical fi t ness or exercise capacity and a good predictor of the subject's potential to move and lift the body, but it does not refl ect cardiac performance [cit] . the to tal oxygen uptake (l per min) correlates with ca rdiac output, myocardial oxygen consumption and blood fl ow and both measures are thus useful [cit] ."
"with sfo, more general relations are translated into is-a relations in the scope of sfo, if possible. the feature pairs causing the errors are identified. below, a sample translation is given."
"the challenge here is that the acf data are sent independently from the actual assigned data. to eliminate the synchronization requirement between scheduler control and data transfer, the m ctrl workers are forced to accept data through global communication apis only when the control and data are from the same sources. using these methods, a dynamically scheduled monte carlo simulation is implemented and various scheduling schemes are evaluated."
"meaning: \"class1 is of line-type, class2 and class3 are of polygon-type, features in class1 cannot have the crosses relation with the features in the classes class2 and class3\" 6210.12 is the timestamp, which is the same throughout the process (the minimal one)."
the inputs to the system are a set of random variables. each of these variables contributes to a step in the pricing path. the put and call payoffs of all the simulated paths are collected to compute the final expected value. there is no data dependency between simulated paths.
"in these studies, swrl rules are used with a domain dependent quality management framework. [cit], proposes a rule -based system to increase the quality of the osm data with rules created by sqwrl."
"the quantitative data was taken from the result of reading test which was carried out in two cycles. the improvement of students reading comprehension by using reciprocal teaching method can be seen from the orientation test, cycle i and cycle ii as follows. the improvement of students' score in reading comprehension could be seen from the mean of the students for each meeting by using this formula:"
"on the other hand, poor physical fi tness in contrast to enthusiastic parental expectations is a frequent occurrence among children and adolescents participating in sports and can easily be assessed by measuring exercise capacity."
"while creating the sdqo, terminologies which are used in other studies are considered. although, all ontologies are created with the same intention, quality management, they are specific to different application domains. they have some common concepts such as \"quality dimension\", \"quality metric\" or \"quality result\"."
"based on the background of the study, the problem of the study proposed as the following \"is the use of reciprocal teaching method significantly improve the students' reading skill?\""
"after receiving the datrdy message, the m comp worker start loading the computation to the targeted hardware accelerator. the processed results from the accelerator are also stored in shared memory. an m comp worker indicates the completion of its work by sending a datrdy message back."
"furthermore, geosparql object properties such as ogc:sfoverlaps are used for relations between spatial classes. ogc:sfintersects is declared to be a super property to intersection type properties such as ogc:sfoverlaps. sdqo as in protégé ontology editor is shown in figure 6 ."
the purpose of reciprocal teaching as general is to facilitate a group effort between teacher and students as well as among students in the task of bringing meaning to the text. the strategies chosen not only promote reading comprehension but also provide opportunities for students to learn to monitor their own learning and thinking.
"if an athlete complains about respiratory symptoms during exercise and no bronchial hyperresponsiveness or eib is found, one of the above conditions is a possible cause."
"another differential diagnosis related to eib is exercise-induced arterial hypoxaemia (eiah). [cit] reported a prevalence of 50% in elite male runners [cit] . eiah is defi ned by a reduction in sa,o 2 of 8-10% from before to after strenuous exercise [cit] . this occurs especially in highly endurance-trained athletes and is thought to be primarily due to diffusion limitations and/ or ventilation/perfusion mismatch [cit] ."
"the proposed framework can be adapted to different clusters with arbitrary heterogeneous accelerators. to realize and evaluate the framework, we perform our experiments on the axel cluster. the axel cluster adopts the nonuniform nodes uniform system (nnus) approach in which heterogeneous pes are hosted in a single node. all nodes are uniform with the same abilities. the single program multiple data (spmd) programming paradigm is suitable for this nnus architecture. since all the nodes are identical at system level, multiple instances of user application can easily be distributed to the multinode cluster. figure 1 is an overview of the axel cluster which currently includes 16 nodes. there are three different types of pes in a single node: an amd phenom quad-core cpu, an nvidia tesla c1060 card [cit], and a xilinx virtex-5 lx330 fpga hosted on an adm-xrc-5t2 card [cit] . each node independently runs a copy of the linux operating system. although only gpus and fpgas are currently included in the cluster, it is possible to support other type of accelerators in a similar system architecture."
"b. for the students, it is suggested to use reciprocal teaching method because this method trains the students to learn independently with or without teacher and can increase their social interaction with other students especially in their group."
"the third type covers large amounts of data communication between local workers. based on the shared memory posix ipc, special apis are needed to map array variable in user applications to the created shared memory blocks. the number of memory copies and data movement overhead is significantly reduced while all the synchronization and protection of the data are not explicit to application programmers."
"exercise induced laryngeal stridor (eils) is a frequent differential diagnosis to eib and the condition is most often seen in young well-trained girls participating in endurance sports. the prevalence in athletes is reported to be 5.1% [cit] stensrud and carlsen [cit] reported a prevalence of 17.5% in athletes referred to a pulmonary clinic with exercise-induced respiratory symptoms. the symptoms of eils are inspiratory stridor occurring during maximum exercise and stopping when exercise is terminated. during exercise, audible inspiratory sounds can be heard from the laryngeal area, with no effect of bronchodilators or other asthma medication. hyperventilation is often seen in relation to eils and measurement of v'e (which often shows a sudden drop close to maximum), v'o 2, fr and rer during exercise may serve as an additional tool to confi rm the diagnosis of eils. the correct diagnosis of eils should be confi rmed by direct fi breoptic laryngoscope during exercise."
"in conclusion, an eib test protocol can be used for both provoking eib and for assessing exercise capacity. physiological variables measured during the exercise challenge may give additional and valuable information about the asthma disease and how the patients master their disease. additional diagnosis or differential diagnosis to eib may also be assessed by measuring exercise capacity."
"initial step to create ontology for quality management is to define the concepts. for this purpose, motivating scenario and competency questions are defined. followingly, existing ontologies are researched for both geospatial and data quality management concepts. the following sections explain the motivating scenario and questions, ontologies for quality management and geo-ontologies respectively."
"assuming that parallelism is achieved by distributing and processing small subsets of data concurrently, the performance gain depends on the sequence of workers that take the longest time. a static schedule may not be efficient since workers finished earlier cannot help. this problem of unbalanced load cannot easily be addressed at design time when the acf is created. the estimated performance of the accelerated kernels is usually not sufficiently accurate and the runtime performance may also be affected by input data or parameters. also, in a cluster system, new computing resources may be available due to the termination of other applications. a static schedule cannot take advantage of new resources."
"spatial data producers want to produce data compliant to the rules in the regulations/specifications. producers should formalize the rules, assess the data and have the quality result as a report."
"the proposed research targets a cluster with fpga and gpu accelerators [cit] . based on this hardware architecture, we study the software stack of heterogeneous clusters. our main aim of this work is to create an easy to use, flexible and efficient framework for developing distributed applications. the framework should be supported by a set of application programming interfaces (apis) such that developers can utilize the accelerators in a familiar software environment. abstraction and modularization of the framework will help to minimize the effort of including or excluding different types of accelerators in the applications. more importantly, these different accelerators can work collaboratively on a single application to maximize the efficiency of the system. the major contributions of this work include:"
"the rest of this paper is organized as follows. section 2 reviews the development framework in previous accelerator clusters. section 3 presents the architecture of the cluster platform in this work. section 4 explains the structure and execution model of the distributed applications in the programming framework. section 5 presents the application programming interfaces for building applications. section 6 evaluates the results and performance. finally, section 7 summaries the findings and achievements."
"despite these obvious advantages of accelerator clusters, the number of systems and applications is not comparable to the number of cpu-centric hpcs. these accelerators are usually more expensive than cpu in terms of unit price. to maximize the cost effectiveness of a system, the potential of the pes must be fully exploited. this can be a challenge to application developers since, unlike cpu programming, it requires information of the pe's internal hardware architecture, different vendor-specific tool chains and even different programming languages."
"in evaluating the students' reading achievement, the writer will use multiple choice tests. the writer administrated 25 items of multiple choice tests. in scoring the reading test, it is determined that the ranging from 0-100 by accounting the correct answer. the correct answer is will give 1 while the wrong answer is will give 0 and by applying this formula:"
the most distinct character of this proposed framework from most other systems is the multiple executables structure in a single application. one module in the framework is a full featured executable for a specific task targeting a specific pe. the active instance of a module is called a worker. a group of workers collaborate on a single node. a running application is a collection of groups of workers. parallelism is achieved by distributing the workload to the active workers of the application. figure 2 illustrates this distributed application structure.
"in this study two types of ontologies are introduced. the sdqo ontology, contains the necessary rules and the concepts related with the quality assessment. the sfos are simple ontologies, developed keeping in mind the reusability of rules with different kinds of datasets. there can be one or more sfos (eg. for different scales) for each institution. devised ontologies and their relations are shown on figure 2 . this section continues with the subsections to explain sdqo and sfo."
"fr and rer may give helpful information about abnormal breathing pattern. patients with eib are reported to breathe faster during exercise in cold air than in normal indoor conditions and deeper and slower during exercise in humid air [cit] . it has been observed that patients with additional diagnoses to asthma, such as anxiety, eating disorders and obesity, show an abnormal breathing pattern during exercise. obesity itself is a common cause of breathlessness on exertion and it probably acts synergistically with airway obstruction in the generation of breathlessness [cit] . frc is reduced in obese patients, due to abdominal fat tissue contents on the position of diaphragm, resulting in reduced expiratory reserve volume [cit] ."
"the second type covers the control messages between workers within a group. since all workers of the same group reside in the same node, this api is built on top of the message queue posix inter process communication (ipc). the 1k byte message packet has the format of a destination worker id followed by the message type and user defined contents. besides system predefined message types such as datrdy and append, user are free to create custom types. the user defined content area is useful for small and frequently changing data such as system parameters."
sdqo:subnr property is the main data property for assigning the classes to be tested with the subclass number. it has subproperties such as sdqo:subnroverlap and sdqo:subnrmustwithin. these subproperties are used in defining top classes in sfo.
"the tidal breathing loops obtained during exercise, most often during running on a treadmill, are related to the maximal expiratory fl owvolume loops obtained before running [cit] . limitations in physical fi tness may be set by the baseline lung function, and assessment of fl ow limitation and end-expiratory lung volume during exercise may give important information about the possible training effect that may be obtained [cit] . the simultaneous assessment of tidal breathing loops during exercise and assessment of br is useful to show the patient whether there is a pulmonary limitation. obtaining additional information by using exercise testing in the laboratory in the diagnosis of asthma conditions compared with hot and humid conditions in eight male asthmatic subjects. stensrud and co-workers [cit] concluded that v'o 2,peak was reduced in cold air and increased in humid air compared to a normobaric environment in subjects with diagnosed eib (fi g. 3)."
"to enable a dynamic scheduler to adapt to the actual kernel performance and run-time conditions, special arrangement in the distributed application must be made. first, the work load distribution must not depend on the acf, and should be acquired by the m ctrl at run time. this can be supported by assigning 0 to all size attributes in the acf. second, there must be a central facility to coordinate the load distribution process. every time a group finishes the assigned work, the m ctrl in the group will send the datrdy message to the scheduler. depending on the recorded performance of this group, the scheduler assigns a new batch of data for the group to process. various scheduling schemes can be applied to achieve different optimization goals. a second level of scheduling process is performed by every m ctrl worker to balance the load of the local m comp workers. as discussed before, the data structure in acf can be used to alter the data size assigned to each group."
there are large amounts of local memory associated with each pe. this creates a physically non-uniform memory hierarchy within a node. the bandwidth between the pes and their associated local memory is much higher than the communication bandwidth between pes. thus data partition and distribution are critical for performance in the cluster.
"sfos define specification classes and generic classes for data to be tested. sfo generic classes are defined according to the spatial relations related to data. spatial relations should be defined between feature classes. in specifications, a feature class might have topological relations with; itself, a feature class, more than one feature class."
 to explain how to get more information out of an exercise challenge.  to obtain knowledge about different exercise test protocols and ergometers.  to obtain knowledge about the importance of assessing different physiological variables during exercise.
"there is no data dependency within a simulation iteration. so the fpga design can be deeply pipelined and high throughput is achieved. the particles are arranged into small groups and assigned to different m comp workers including the gpu and fpga kernels. each worker computes the acceleration vectors of the assigned particles and broadcasts the results. the final updates of positions and velocities are performed by a cpu-centric worker. since all workers need to access the most update positions of all the particles for the current simulation, the all-to-all broadcast global communication increases the overhead."
"reading for comprehension, when accomplished by skilled readers requires very rapid and automatic processing of word, strong skill in informing a general meaning representation of main ideas and efficient coordination of many process under very limited times constraints. in reading process, the students as the reader should has a good grasped of how to understand the text easily, how to make a summary, and relate those to their background knowledge as appropriate. therefore, the teacher should be able to conduct a suitable strategy to improve the students' ability reading comprehension."
diary notes were written for personal evaluation about the situation of the class while the teaching-learning process. diary notes were written up by the researcher in every meeting during the research. it contained the researcher's personal evaluation of applying reciprocal teaching method during the teaching-learning process.
"mostly used ontologies for geospatial domain are; geonames [cit] ), w3c geo (w3c [cit], geoowl (w3c [cit] ) and ogc geosparql [cit] . the geonames ontology and w3c geo support only point type geometries. geoowl, the updated model of w3c geo, is created compatible with georss feature model. only geosparql supports other geometry types such as polygon and lines and basic spatial relations. the geosparql ontology has been selected for defining geospatial concepts and relations shown in figure 1 . its classes are used as superclasses to the classes in the designed ontologies."
"this paper presents a framework for improving productivity and efficiency of application development in a cluster environment targeting heterogeneous hardware accelerators. the modular structure for modules and configuration files enables rapid development and easy extension. current and future work includes improving run-time optimization such as dynamic load balancing, and supporting a wide range of applications and hardware platforms."
"the eib test in the study of stensrud and carlsen [cit], was performed at two different inclinations of the treadmill, 5.3% and 10.5%. subjects ran for 8 min without warming up. the starting running speed was approximately 70-80% of estimated hrpeak. the speed was subsequently adjusted during the fi rst 4 min to achieve a workload corresponding to the maximum speed the subjects were able to sustain during the last 4 min: about 95% of estimated hrpeak. v'o 2, v'e, rer, respiratory frequency (fr) and hr were measured during the eib test. lung function was measured by maximal forced expiratory fl ow volume loops before and 1, 3, 6, 10 and 15 minutes after the test."
"high quality spatial data is essential in providing better analyses and making better decisions involving such data. [cit] to categorize and define the quality concepts, quality evaluation methods and solutions for producing better data."
"n-body simulation is a process to model the interaction between n particles under gravitational forces in space [cit] . in this example, we implemented a second order 3d n-body simulation application on the axel cluster. to compute the acceleration vectors of moving particles, the distances between them and every rest particles are computed."
monte carlo simulation is a useful tool to solve complex problems [cit] . we have developed an option pricing application using monte carlo simulation. the major computation is to simulate the paths of option price movement in a mathematical model. this application includes both fpga and gpu acceleration kernels and supports dynamic scheduling using the method described in section 5.3.
"the synthesis methods developed in the previous sections are based on closed-form formulae for the computation of the parameters of the pid controller. clearly, the strength of this approach -which provides an exact answer to the control problem design with standard steady-state and frequency domain specifications -can at first sight be also considered its weakness, because often a full knowledge of the dynamics of the plant is not available in practice. [cit] (but that can be carried out on any of the standard diagrams for the frequency response), [cit] . as already mentioned, a graphical version of the method presented in this paper can be implemented using any of the frequency domain plots usually employed in control to represent the frequency response dynamics, i.e., bode diagrams, nyquist diagrams or nichols charts. this is due to the fact that the formulae used to derive the parameters of the pid controller are expressed in terms of the magnitude and the argument of the frequency response of the plant at a given crossover frequency, which is readable over any of these diagrams. given the point of the nyquist plot corresponding to the desired gain crossover frequency, i.e., we can estimate m g and ϕ g . these values lead to the parameters of the pid controller."
"in a recent survey, statistics show that inverter parts are still the less reliable components, especially power semiconductors and gate drive systems [cit] . regarding multi-phase systems, the occurrence of faults is thus more likely and a proper detection system is required."
"if we are able to solve problem 2.1, the desired compensator c(s) ensures that the frequency response of the loop gain transfer function satisfies (45), but it does not automatically guarantee that the closed loop system is stable. in fact, the conditions (45) do not exclude the existence of other points where the polar plot of the loop gain frequency response l( jω) intersects the unit circle. therefore, an a posteriori verification is necessary to ensure that the closed loop is stable."
"a possibility to carry out the design of the pid controller in the case of unconstrained integral constant is to exploit the remaining degree of freedom so as to satisfy some further time or frequency domain requirements. here, we consider two ways to exploit this freedom: the first is the one where the ratio t d /t i is chosen, so as to ensure, for example, that the zeros of the pid controller are real; the second is the one where a gain margin constraint is to be satisfied."
"as for the design of pi controllers, the synthesis techniques presented for the imposition of phase margin and crossover frequency of the loop gain transfer function can be used for pd controllers only when the steady-state specifications do not lead to the imposition of the proportional sensitivity k p . the transfer function of a pd controller is given by (2). to find the parameters of the pd controller, the equation"
"the topology under study is depicted in fig. 1 . the drive is composed of a single energy source, a five-leg inverter and a five-phase star-connected pmsm."
"in this system, high brightness strip light sources were used to weaken the influence of light on measurement and make the feature of the object more prominent. then we analyze the errors in this measure system and get the factors which impact the measurement accuracy. we use effective data processing methods to reduce errors and improve measurement accuracy. the simulation results show that the method can reach the need of measurement accuracy of the relative position and attitude for spacecraft; it is more suitable in on-board real-time computation."
"the essence of stereo matching is to search for the corresponding point of a given point in two different images. the two points are the projections of the same space point. the common image matching methods include matching based on gray-scale image, image-based features and interpretation of the image, or the combination of various matching methods. in this paper, a combination of various methods is promoted, including epipolar constraint, the gray correlation based on regional, the reference difference gradient restraint and the exclusive constraint [cit] ."
"according to the point and line duality, extraction of the sense line corresponds to the hough points in space, so the space in the hough using point tracking to achieve image space of feature tracking."
"in addition, the further user study with more participants is required to evaluate robustness of pass-images from the viewpoint of more powerful attackers, i.e. video cameras. we also need to consider the effect of age on visibility of high frequency components."
"this paper provides a unified and comprehensive exposition of this technique, not only for pid controllers in standard form, but also for pi and pd controllers."
"first, we consider the case where the steady-state specifications do not lead to a constraint on the integral constant of the pid controller. in order to compute the parameters of the pid controller, we write g( jω) and c pid ( jω) in polar form as"
"in the second step, the llx band of the decoy image and the lh1, hl1, and hh1 bands of the pass-image are merged. we set zeros for the middle frequency sub-bands, such as lh2, hl2, and hh2, to add some blur effect for the decoy image. if the decoy image has many edges, these edges are mixed with the edges of the pass-images and it becomes difficult even for the legitimate user to recognize the passimage. we obtain a blended image for our authentication method after the inverse dwt (idwt) and merging of color planes."
in the solution to this problem there is a degree of freedom: by equating the real and imaginary parts of both sides of (6) we obtain two equations
"from (7) and (16) (18) is not polynomial in ω p, and it needs to be solved numerically. theorem 3.2. consider problem 2.1 with the additional specification on the gain margin gm. equations (6) and (15)"
"in this section we formulate the problem of the design of the parameters of a compensator belonging to the family of pid controllers such that different types of steady-state specifications are satisfied and such that the crossover frequency and the phase margin of the loop gain transfer function are equal to desired values ω g and pm, respectively. consider compensators described by the one of the following transfer functions: i) pid controller in standard form:"
"an advantage of this method is storage space reduction for the pass-image. only the high frequency components, that are lh1, hl1, and hh1 bands, are required to be stored in the system. for the authentication, the system randomly chooses a decoy image and mixes it with the pass-image to generate a blended image. in this study, we used 5/3 [cit] . low frequency signals and high frequency signals are given by following equations."
"we evaluated memorability of pass-images overwritten on the blended images. before the user test, 5 pass-images are given to a user and he memorizes these images for 10 minutes. during the user test, a set of 4 blended images, that are generated using 2-level dwt, is shown to the user at one challenge. one or no pass-image is included in the set and the user is asked to specify the location of the passimage. one authentication trial consists of 7 challenges. we carried out this test 5 times. therefore, we obtained 35 answers from each participant. we did not add a limitation on the number of decoy image appearances. therefore, the same decoy image could appear several times. participants of this lo-fi test are 4 male students and all of them are 20s. table 1 shows the correct answer ratios for this test. this result shows that most of the participants could recognize pass-images correctly."
"regarding three-phase star-connected drives, a conventional analysis shows that, in case of open-circuit fault, the faultyphase current is equal to zero during a part of the period, while both healthy currents are opposite in magnitude during this part of the period [cit] . the α1β1 current trajectory is a circle in the healthy state, possibly affected by harmonic components at a frequency equal to 10 . in case of fault, the trajectory changes, but not significantly compared with three-phase drives. further, the pattern changes with the pulsation of the phase currents and the controllers gains. the analysis of the current trajectory in the α1β1 reference frame is therefore not appropriate for the fault recognition."
"the paper is organized as follows: section 2 describes the basic constraints from observing a single plane and the calibration procedure. we start with a closed-form solution, followed by nonlinear optimization. radial lens distortion is also modeled. in the next section, the hough transform and related problems was described. section 4 provides the experimental results. both computer simulation and real data are used to validate the proposed technique."
"entering a user name and a textual password is a major method for the computer login procedure. unfortunately, this method is vulnerable to spyware and key-loggers. in addition, it is difficult to remember long complex textual passwords. studies have shown that users tend to use short passwords or passwords that are easy to remember [cit] ."
"the system designed and developed in this paper can be widely used in industrial inspection, robot vision, games and other areas. limitations of the work are obviously, the spacecraft we discussed in this paper is in the shape of regular geometric structure， the next step, in order to obtain these objectives more robust acquisition algorithm ， we will continue to research spacecraft identification method."
many spacecraft have an orbit control subsystem (ocs) which applies forces in order to maintain the orbital parameters within certain bounds. rendezvous and docking relates to the procedure within which two spacecrafts-the chaser spacecraft and the target spacecraft come close and combine into one integral spacecraft. guidance and control schemes for the final approach phase require precise information about both the attitude and position parameters of the target spacecraft relative to the chaser.
the block diagram of the control scheme is shown in fig. 2 . the torque reference is used to generate optimal current references [cit] . a conventional proportional integral (pi) controller and emf compensation are used for the current control.
"in the preliminary study [cit], we presented a concept of an image synthesis method for a user authentication system using graphical passwords. in this method, we combine low frequency components of a decoy picture with high frequency components of a pass-image by using dwt. for human eyes, it is difficult to recognize the subtle high frequency components. especially in the case that the person is far from the screen, the high frequency components become less visible. however, former user study was conducted in a short term and memorability of the graphical password in longer term was an open question. in this paper, we evaluate longer term memorability and shoulder-surfing robustness of our graphical password."
"in this paper, a method for fault detection and isolation of inverter faults is investigated for multi-phase drives. the method is applied to a five-phase permanent-magnet synchronous machine (pmsm) drive. the faults under consideration are: open-phase faults and open-switch faults. first, an analysis highlights the main fault characteristics. major differences compared with three-phase drives are evidenced. then, relevant fault indices are proposed. these are based on the unbalance of the currents and their instantaneous frequency. simulations and experimental results are presented throughout the paper."
"the parameter k p is the proportional sensitivity constant, while t i and t d are the time constants of the integral and derivative actions, respectively. the problem we are concerned with can be stated in precise terms as follows. problem 2.1. consider the feedback control architecture in figure 1, where g(s) is the plant transfer function. design"
". this situation occurs, for example, in the case of a type-0 plant when the steady-state specification not only requires zero position error, but also that the velocity error be equal to (or smaller than) a given non-zero constant. a similar situation arises with constraints on the acceleration error for type-1 plants."
now we solve the same problem by imposing a gain margin equal to 3. we compute m p and ϕ p as functions of ω p :
"(3) other frame, feature tracking based on hough transformation. the present accumulator has peaks that may have values greater than the specified threshold. the simplest way to find these peaks is to compare the values of the peaks with the threshold value. then the peaks with the highest values are compared with other peaks in the neighborhood using predefined windows. we can draw straight lines from these peaks."
"robots are main key to future space exploration, engineers and scientists are focusing on building advanced robots that will delve deeper into space. one of their main tasks of space robots is tracking and capturing the target spacecraft and space junk, the key technology of which is identifying and locating the space target. in most cases, the space target is the non-cooperative target which has no characteristic cursors. to the space robots, tracking and capturing these targets is the main task. with two ccd cameras, binocular stereo vision system can simulate the vision system of human, it can realize stereoscopic vision from the projection points in the two ccd cameras which are projected from one point in the space. robot can complete the scheduled tasks by passing 3d coordinates of the point to robot control system."
"matching problem can be reduced to a one dimensional search by using the epipolar geometry of the cameras. an epipolar plane is a plane that contains the point p and the optical centers of the left and the right cameras. the intersections of such a plane with the left and the right images define a pair of epipolar lines. any point on the left image can be found on the corresponding right epipolar line [cit] . the problem is thus reduced to a one-dimensional matching problem along each pair of epipolar lines. let the ccd be symmetry to the xoz plane, this is to say that the correspondence points are in the corresponding line of the two ccd. a geometric constraint relationship exists on the same objects' images under the same world coordinate system. in stereo vision, we can use image points matching to recover this relationship, on the contrary, use the relationship to restrict firstly, we solve the epipolar equation on the basis of camera calibration, which contributes to reduce the matching region to an area around a straight line. secondly, a gray score is chosen as the comparability test to filter the parts of the false matching points. in the end, the last matching points are got by the reference difference gradient restraint. finally, after applying the epipolar constraint, the most correlated criteria of gray value can be used to filter a large number of false matching points. it is agreed in this paper: determine a feature point from one image as the center of a window, which calls the feature point neighborhood window. the realization of this method is as shown in figure 4."
"to overcome the difficulties and approximations of trial-anderror procedures on bode and nyquist plots, and of the three above described design methods, a unified design framework is presented in this paper for the closed-form solution of the feedback control problem with pid controllers. in this paper, simple closed-form formulae are easily established for the computation of the parameters of a pid controller that exactly meets specifications on the steady-state performance, stability margins and crossover frequencies, without the need to resort to approximations for the transfer function of the plant."
"the features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3d viewpoint, addition of noise, and change in illumination. the features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. solar panel bracket is generally has the obvious line feature. compared to the point features, line feature has obvious characteristics: small amount of information and small amount of calculation etc. this paper presents a method based on hough transform for line feature extraction and tracking to find line feature of bracket."
"a lot of researches have been done in the field of vision-based pose estimation of spacecraft rendezvous and docking, and a lot of methods and algorithms have been proposed, especially for the final approach. other researches focusing on the pose estimation also received preferable results, but most of the proposed works focus on pose estimation based upon at least 4 feature points. some scholars have proved that the relative pose could be estimated through an iterative algorithm from image information retrieved from feature targets fitted in a certain arrangement, but the algorithm lacks both estimation and correction of the measurement error. as to above problem, an approach to identifying the rectangular plane of the non-cooperative target and extracting its 4 apexes based on stereoscopic vision was proposed in this paper. we propose a novel feature tracking and pose estimation framework for the final approach of rvd from monocular images. feature targets were tracked and pose information was iteratively solved from the three point problem. in order to test the real-time performance and accuracy of the proposed pose estimation algorithm, a video camera system composed of featured target spacecraft, camera on chaser spacecraft and other image devices is utilized."
"there were 6 participants and they played a role of an attacker. first, they complete a short training session on how each graphical password system is used. next, the participants try to gain pass-images through shoulder-surfing while the experimenter carried out the authentication correctly. the experimenter chose a pass-image out of 5 images every 5 seconds. this challenge is repeated 7 times. the participants stood about 50cm behind the experimenter. each participant was permitted taking notes during the shoulder-surfing. after the shoulder-surfing session, the participants were asked to carry out the authentication as attackers using same pass-images. they could use their notes if they wanted. table 3 shows the false acceptance rate (that is, attack success ratio). this result shows that proposed method is secure against the shoulder-surfing. participants indicated that they could not recognize high frequency components of a pass-image because they are far from the screen."
"besides hardware-based detection methods such as desaturation monitoring of the transistors and voltage measurements, many fault detection methods have been proposed for three-phase inverters [cit] . these methods are generally based on the analysis of the phase currents and allow open-circuit faults to be detected, such as open-circuit switches or open-circuit phases."
"where x(n) is a pixel value in position n. samples belonging to the low and high frequency sub-bands are represented as l(n) and h(n) respectively. examples of a decoy image, a pass-image, and a blended image obtained using this dwt are shown in fig. 4 ."
"the world coordinates of space point p can be solved by the equation (2) with the least square method, which is shown by the equation (3):"
"in the last fifteen years, three important sets of techniques have been proposed to deal with requirements on the phase/gain margins and on the gain crossover frequency, to the end of avoiding the trial-and-error nature of classical control methods based on bode and nichols diagrams. the first one is a graphical method hinging on design charts, and exploits an interpolation technique to determine the parameters of the pid controller. [cit] . a second important set of techniques that can handle specifications on the phase and gain margins relies on the approximation of the plant with a first (or second) [cit] ."
"there were 40 participants and we divided them into four groups to evaluate the effect of test intervals. the number of participants and intervals of user tests for each group are shown in table2. on the day \"0\", participants practiced the authentication procedure several times until they became familiar with the usage of the system. then, they started the user test of the authentication for each method. the participants in group a carried out the authentication test without the practice 1day later, 1 week later, 2 weeks later, 3 weeks later, and 1 month later. the intervals of the authentication test were longer for other groups. 6 shows the effect of test interval for success ratio. fig. 6(a) is the result of the proposed method, fig. 6(b) is the result of the harada's method 1, and fig. 6(c) is the result of the harada's method 2. fig. 6(a) shows that participants could choose almost correct pass-images irrelevant to the test intervals in our method. on the contrary, the success ratio decreases as the interval becomes longer in the harada's methods. this is due to the memorability of the pass-images."
"suppose we use our authentication method for pdas or web-based services. when a user wants to use a service, s/he may start login procedure at his/her desk and his/her friends or coworkers may see the computer screen when they walk behind his/her desk. our method aims to prevent such observers from knowing the user's pass-image. if a passimage is faintly printed on a decoy image, the legitimate user who is close to the pc screen can see the pass-image but someone who is far from the screen cannot recognize the pass-image. our method tries to utilize this property. fig. 1 shows an example of a set of images for a challenge using our graphical password method. beforehand, users register several images as their pass-images. on the screen, a set of images are shown to the user. one image in the set consists of low frequency components of a decoy image and high frequency components of a pass-image. the other three images are blended images which consist of low frequency components of decoy images and high frequency 28th [cit], december 8-10, 2010, nagoya, japan 978-1-4244-7135-5/10/$26.00 ©2010 ieee components of other decoy images. the user is asked to choose the image in which his/her pass-image is vaguely blended in the image. if s/he does not find any of his passimages in the set, s/he is supposed to choose \"no passimage\" as his/her answer. this challenge is repeated several times and if all the answers are correct, s/he will be successfully authenticated. the number of challenges and registered pass-images depend on the requirements for security and usability. if the number is greater, the system becomes more secure but becomes less usable because the user has to properly remember more pass-images and takes more time for the authentication."
"the pinhole model is the most common model of the camera imaging model which is a kind of optical imaging model. geometric relations of imaging cameras can be shown by fig.1 . point c o is the camera heart, x-axis and y-axis is parallel for x-axis and y-axis of the image, z-axis is camera optical axis, which is perpendicular for the image plane. the optical axis intersecting with the image plane is o, which is the origin of the image coordinates in millimeter units. the camera coordinate system is composed of point c o, x-axis, y-axis and z-axis. the origin of the image coordinates in pixels units is in the upper left vertex."
"the phase currents and the electrical pulsation are the inputs of the fault detection and isolation (fdi) block, which will be described in section iv. it has to be noted that the diagnosis can be used to generate optimal current references next to the fault isolation, but this is beyond the scope of this paper. before the fault occurrence, the five phase currents are balanced and are composed essentially of fundamental and third harmonic components. upon the fault occurrence, the faulty-phase current is equal to zero, while the other phase currents increase to compensate for the loss of one phase. it can also be noticed that the healthy phase currents are no longer balanced following the fault occurrence."
"the first type of lens effects a perspective projection of the world coordinates into the image, just like the human eye does. this combination of camera and lens is called a pinhole camera model because the perspective projection can also be achieved if a small hole is drilled in a thin planar object and this plane is held parallel in front of another plane (the image plane)."
"to avoid a drift of the pll at low and zero current, the pll is inhibited at low currents and the output is set by default at below a given threshold; therefore only deviations compared with the electrical pulsation are observed. this index is thus suitable for the isolation of open-switch fault. fig. 8 shows these frequency-based indices for the same simulation as for fig. 7 ."
"to obtain the 3d information of the object, extrinsic and intrinsic parameters of the cameras was needed, which was obtained by the camera calibration. we use a flexible technique to easily calibrate a camera. it only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. either the camera or the planar pattern can be freely moved and the motion need not be known."
"a signal-based technique for fault detection and isolation of inverter faults in multi-phase drives (1) where are the voltages at the machine terminals, the stator resistance, the phase currents, the inductance matrix and the electromotive force (emf) due to the permanent magnets."
"first, dwt is applied to each color plane in a decoy image and a pass-image. the llx band, that is the lowest frequency band of x-level dwt, contains the average information of the input image. in contrast, the lh1, hl1, and hh1 bands contain mostly the edges of the images. names of each sub-band in 2-level dwt are shown in fig. 3 ."
"v, which is related to intrinsic parameters of the camera; 2 m is determined by the location of the camera and the world coordinate system, which is related to extrinsic parameters. according to the principle，relationship between the world coordinates of space point p and the image coordinates in pixel units of its projective points can be shown，in which， zlc,zrc is the z axis coordinates of point p in the camera coordinate system respectively，ml, mr are projection matrixes of left camera and right one."
"this approach may be employed even in absence of graphical descriptions of the plant transfer function. indeed, in the very same spirit of the ziegler and nichols method, we may \"perform an experiment\" on the plant by feeding it with a sinusoidal input with frequency ω g, i.e. the desired crossover frequency. from the steady-state output we can estimate g( jω g ), and hence m g and ϕ g and we can thus readily apply the proposed method."
"in the experiment the serving satellite model move upward along x and z, making variable motion along x axis and sinusoidal motion along z axis. fig.9 shows the video sequence of the serving satellite. from the sequence we can see the changing perspective of its solar panels stand from global to local. fig.10 is the result of the self-adaptive video frequency sequence edge detection, by the test result fig.11 we can see that through reasonable light design we can ensure that the first frame of image will get rid of the influence of the solar array after self-adaptive edge detection, which can reduce the computation load of feature extraction. linear feature extraction will then be switched into linear feature tracing. the uniqueness of feature guaranteed that the tracing procedure will not be influenced by the solar array. fig.12 and fig.13 demonstrates the measure data of x and z axis. fig.14 shows the mean deviation of their relative position. from the results of experiments, we can see that the relative deviation is less than ±20mm and ±2degrees, which can satisfy the requirement for measure accuracy."
"to address this issue, several authentication methods have been proposed. in this paper, we focus on graphical passwords [cit] which use pictures as passwords instead of using alphanumeric characters. pictures are difficult to steal with key-loggers. in addition, remembering pictures are easier for human than remembering textual passwords. although graphical passwords are generally easy for legitimate users to memorize, they are also easy for observers or attackers who stand behind the users to memorize. to alleviate this risk, graphical passwords, which use degraded images, have been proposed [cit] . these proposed approaches utilize the property that degraded images look like noise or ink blots and they are difficult for observers to memorize. on the contrary, legitimate users are able to find his/her pass-image easily because the knowledge of the original clear image becomes a clue to remember the pass-image. these methods are effective to enhance the security level against observation attacks. however, a tradeoff exists between the security level and the memorability of pass-images. as a result, the system becomes less usable."
"multi-phase machines are now used in various industrial applications and in transportation systems [cit], mostly due to their advantages such as increased fault tolerance and power density [cit] ."
"we proposed a graphical password using the property that it is difficult to recognize subtle high frequency components with human eyes. our method is difficult for observers who stand behind a user to know the pass-image, while it is easy for a user who is just in front of the computer screen. we also discussed an image blending method for this graphical password system. we conducted a lo-fi tests and a user test. the lo-fi tests revealed the feasibility of this system and suitable parameters for the image blending. the results of the user test showed participants could complete the authentication faster than existing methods while the authentication success ratio was higher than them. in addition, the user test of shoulder-surfing shows our method prevents attackers from stealing pass-images."
"as shown is fig 2, camera calibration is a necessary step in 3d computer vision in order to extract metric information from 2d images. we can classify those techniques roughly into photogrammetric calibration and self-calibration. we use the technique which only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. the pattern can be printed by a laser printer and attached to a reasonable planar surface (e.g., a hard book cover). either the camera or the planar pattern can be moved by hand. the motion need not be known. both computer simulation and real data experiment have been used to test the proposed technique and satisfied results have been obtained. compared with classical techniques, the proposed technique is considerably more flexible: anyone can make a calibration pattern by him/herself and the setup is"
"firstly, the counter of the rectangular plane was extracted by using sobel operator and self adapting threshold. after words, the four edges of the rectangular plane were extracted by using the hough transform, through which we can compute the coordinates of the four apexes of the rectangle plane. according to the coordinates, we can obtain the coordinates in the right image, as shown in fig.3 . finally, we can obtain the 3-d information of the target through the 3-d reconstruction according to the coordinates in the right and left image."
how to generate the blended image is the key in this authentication system. fig. 2 shows the flow of our image synthesis method. we used a discrete wavelet transform (dwt) to extract low and high frequency components from images because dwt requires less computation than dct to split the frequency components of two input images and to blend those frequency components for generating a blended image.
"this paper has presented a method for inverter fault detection and isolation in a five-phase pmsm drive. as the current trajectory in the α1β1 frame is not much affected by the faults, the proposed method is based on the unbalance of the phase currents as well as their instantaneous frequency, obtained via signal processing techniques. relevant fault indices have been proposed and simulation and experimental results validate the proposed approach."
"we will study the details of the relationship between the visibility of the pass-image and the complexities of the image content in the future work. hybrid images [cit], which blends two images of similar structure but slightly different, are good candidates to compare with our method in terms of image content and visibility of pass-image because our method blends two images of completely different structure."
"prototypes of the graphical password system was implemented as an web application using apache and php. an authentiocation process consists of seven challenges and four blended images and an icon of \"no pass image\" are displayed for each challenge. in this test, each participant played a role of a legitimate user. they memorized 5 passimages, which were imposed by the system, and answered their location using the mouse pointer."
". in this case, the conditions in theorem 3.2 are satisfied, and the parameters of the pid controller can be computed in closed form as"
"there are several advantages connected with the use of the methods presented here for the synthesis of pid controllers: i) [cit], steady-state performance specifications can be handled easily; moreover, the desired phase/gain margins and crossover frequency can be achieved exactly, without the need for trialand-error, approximations of the plant dynamics or graphical considerations; ii) a closed-form solution to the feedback control problem allows to analyse how the solution changes as a result of variations of the problem data; moreover, the explicit formulae presented here can be exploited for the selftuning of the controller; iii) very neat necessary and sufficient solvability conditions can be derived for each controller and each set of specifications considered, and reliable methods can be established to select the compensator structure to be employed depending on the specifications imposed; iv) the formulae presented here are straightforwardly implementable as matlab parameters of the controller is carried out via standard manipulations on complex numbers, and therefore appears to be very suitable for educational purposes; v) the closed-form formulae that deliver the parameters of the pid controller as a function of the specifications only depend on the magnitude and argument of the frequency response of the system to be controlled at the desired crossover frequency. as such, this method can be used in conjunction with a graphical method based on any of the standard diagrams for the representation of the dynamics of the frequency response, e.g., the bode, nyquist or nichols diagrams; vi) in the case a mathematical model of the plant or a graphic representation of its frequency reponse are not available, the technique presented in this paper can be used on a first/second order plus delay approximation of the plant. the extra flexibility offered by the design method presented here consists in the fact that the formulae for the computation of the parameters are not linked to a particular plant structure. thus, differently from other approaches based on first/second order approximations, when a more accurate mathematical model is available for the plant, the formulae presented here can still be used without modifications, and will deliver more reliable values for the parameters of the compensator."
"proof: a necessary condition for the problem to admit solutions is that ω p is a solution of (18). from (7) and (8), and from (16) and (17), we obtain"
"looking further at fig. 6 and fig. 7, it is difficult to find a decision criterion which is suitable for both faults and which allows the faults to be distinguished. consequently, additional indices are proposed. these are based on the instantaneous frequency of each phase current. benefits of the quadraturesignal generator are used through the addition of a synchronous reference frame phase-locked loop (pll) [cit] . the following fault index is proposed, for example for phase a: (8) where is the instantaneous pulsation of the current in phase a. this index is equal to zero in healthy operation and equal to one in case of open circuit fault."
"the performance metrics parameters computed in this work is done by means of calculating the perfect classification (pc), missed classification (mc) and false alarm (fa) respectively. the overall performance of the classification system is expressed in terms of classification accuracy and is written as"
"we have shown the importance of geometric stiffness for the dynamic simulation of stiff objects. this previously overlooked linear tensor only changes the stiffness matrix, and provides accurate first-order force approximations, with simple implementation which can be used in all elasticity-based and constraint-based implicit simulation methods."
"the parameter, performance index (pi) indicates the influence of fn and fp in the classifiers. table 13 and table 14 from table 15, it is analyzed for normal cases that a least error rate of 2.08% is obtained when pls non linear features and back tracking search optimization is classified with modest adaboost classifier. it is also analyzed that a least error rate of 8.33% is obtained when em-pca features and back tracking search optimization is classified with optimized nbc. also, a least error rate of 1.04% is obtained when isomap features and back tracking search optimization is classified with modest adaboost classifier."
"where h is the length of the time step. this method diverges for high stiffnesses or large time steps. a way to remedy this is implicit integration [cit] ]. the simplest version, very popular in computer graphics, is implicit euler, which replaces the current with the future accelerations in eq.(2). this requires the solution of the linearized equation:"
"weight 10t 100t 10 6 t 10 9 t 10 12 t 10 15 t without max err 5e −14 x x x x x with max err 5e −14 1e −13 9e −9 7e −9 8e −6 0.009 table 1 : error in percents of cable elongation to lift a box with different masses, with and without geometric stiffness for a fixed 0.01s time step."
"in group search optimizer, there are 3 main evolution operations such as producing, scrounging and dispersion [cit] . the procedure is given in the pseudocode 4. in this algorithm, a m j is the j th member at the m th iteration, ϕ m j is a head angle and d m j is a unit vector. the scanning of three points expressed in the following equations are scanned by the producing operator as"
the optimized values are fed to the classifiers for schizophrenia classification. the classifiers used here are adaboost classifier and its variants followed by naïve bayesian classifier and its variants.
"this approach provides an appealing, unified treatment of elasticity and constraints. notice the contrast with the implicit integration of stiff springs: while increasing stiffnesses leads to an increasingly high-valued, nearly singular matrix in eq.(4), the corresponding decrease of the compliance in eq.(13) keeps the equation numerically tractable. consequently, high stiffnesses are more easily handled using the compliant constraint-based approach than the implicit integration of stiff elasticity. moreover, matrix c has a numerical regularization effect which makes the constraint-based approach tractable even in degenerate cases."
"for the final representation of dynamic features by means of a vector of lower dimensionality, we employ the quantisation technique known as bag-of-features (bof) [cit] . we perform k-means clustering on all training dynamic feature vectors, belonging to either the native-speaker class or to the non-native-speaker class, to construct a \"vocabulary\" of visual \"words\". k-means is initialised five times. the cluster centroids yielded by the iteration with the best convergence form the final vocabulary. for every speech segment processed, each dynamic feature is assigned to the closest \"word\", in terms of the euclidean distance. the visual speech example is finally represented by a histogram encoding the frequency of occurence of each \"word\"."
the gram-schmidt orthonormalization process is carried out here. the first vector is normalized initially and then the remaining vectors is transformed into weighted normalized vectors after subsequent iterations. (1) mean vector estimation:
"in machine learning modalities, one of the simplest and easiest to implement is nbc [cit] . based on the bayes theorem, with assumptions of good independence among the features, it is expressed as"
"based on the estimated posterior probability, the class label for each observation is predicted by the classifier. therefore, with the maximum posterior probability each observation is assigned to the class."
"locating and tracking the mouth roi, as well as registration, i.e., removing variations due to head movements, is an indispensable step that precedes appearance features extraction. this is achieved here in the following steps. facial point tracking: we initially track 113 characteristic facial points, using the appearance-based tracker [cit] . these are manually annotated in the first frame and tracked for the remaining frames. out of these points, we only use 34 points that correspond to the lower face region, specifically their 2d spatial coordinates ( fig. 1(a) ), along with the coordinates of their pose-free version ( fig. 1(b) ), all provided as a part of the tracker's output. six base points (see blue points in fig. 1(b) )), are relatively invariant to facial deformations -the two \"ear-level\" points on the face boundary, the two points where the jaws are attached to one another, the tip of the nose and the center of the mouth (calculated based on the location of 16 points representing the lips contour) -and serve to register the face region and calculate the pose-free points. this is done for all 34 pose-free points, which are subsequently used."
"the proposed framework is evaluated in a subject-independent fashion on native and non-native speech episodes in english from the mobio database [cit] . this bimodal database was recorded at six sites in five countries in two phases, each comprised of six sessions. each session includes different scenarios, such as short-response questions, pre-defined text, and free-speech questions. in total, 192 audiovisual recordings in english, almost exclusively captured on mobile phones, are available for each of the 150 participants. in all cases, the acquisition device is handheld, which implies high variability in pose and illumination. additional challenges are posed by the varying appearance of subjects and background, as well as the different recording conditions. in the current study, we choose to include only the visual speech samples from phase i in which all subjects read the same pre-defined text, thus establishing a text-dependent experimental scenario. the data used are balanced over the two classes, with 135 samples belonging to 28 native english speakers and 137 to 28 non-native english speakers. the paragraph read in all such recordings is the following: long silence segments in the beginning and end of the recordings are removed by applying a voice activity detector on the corresponding audio stream. the mean and standard deviation of duration over all 272 samples used is 22.5 and 3.4 seconds, respectively. the video stream, which is provided in variable frame rate encoding, is converted in a sequence of still frames, corresponding to approximately 15 frames per second, for almost all samples."
"[ [cit] ] proposed a jacobi-style solver with a similar way of updating constraints at each iteration. it results in a very efficient and stable method for specific cases when the matrix of eq.(4) can be precomputed (constant mass matrix, constant material, constant set of constraints). this work also introduces continuum-based materials to positionbased dynamics, however it only handles simplified physics (linear compressible materials) and moderate stiffness."
"in the more challenging situation shown in fig.(3), our solver is able to realistically handle 1 : 100 mass ratios with large time steps (0.04s), while traditional constraints are not able to stably simulate the cable even for a 1 : 1 mass ratio. ten and one hundred distance constraints are respectively simulated at 500 fps and 100 fps using an ldlt solver. the cable swings naturally, with a slight damping due to integration."
"where the first term is the material stiffness matrix representing the change of constraint force magnitude (e.g., ±kuu t for a spring), and the second term:"
"in this type of feature extraction, there are 4 substages: derivation of em step, orthogonalization step, data projection step and pca eigenspace generation step for preprocessing, estimation of mean vector µ and mean subtraction to compute the difference between the input data and mean is done. from a large amount of high dimensional data, em-pca can easily extract a few eigen vectors and eigen values [cit] . for a q dimensional variables, the covariance structure can be captured with the operation less than q(q + 1) 2 dimensions."
the weak hypothesis f t (x l ) equals f k t (x) for the instance x l . (c) the weights are then updated as
"in the next section, we detail our background and motivation through an introductory example. the principle of our method is then explained in section 3. its application to a wide variety of cases is then presented in section 4. we conclude and sketch future work in section 5."
"in order to describe appearance within the mouth rois, we examine five appearance features: pca, dct, dwt [cit], lbp [cit] and hog [cit], all applied to pixel intensities."
"the bag-of-token model was utilized to optimize the n bayesian standard algorithm here [cit] . based on the volume 8, 2020 non-negative number of occurrences of token q in the observation, the value of each feature q is calculated. the estimated probability is expressed as"
"the significance of the features like pls non linear regression, em-pca, and isomap among the schizophrenia and normal groups can be analyzed for non linearity and overlapping through the extraction of statistical features like mean, variance, skewness, kurtosis, geometric mean, harmonic mean and non linear features such as sample entropy, approximate entropy, renyi entropy, fuzzy entropy, shannon entropy and singular value decomposition (svd). table 1 demonstrates the average of parameters at different features for schizophrenia and normal cases. it is observed from the table 1 that all statistical parameters and entropies among the schizophrenia and normal cases are numerically overlapped and also exhibits the presence of non linearity as projected by higher values of variance and kurtosis. the sample entropy and approximate entropy indicates the peaked and trough regions of the features among the schizophrenia and normal groups. hence it is worth to investigate on the correlation property of the features among the classes. one such type of analysis is canonical correlation analysis (cca) which will act as bench mark parameter for the features among the classes. table 2 shows the cca of different features like pls non linear regression, em-pca and isomap for schizophrenia and normal cases. as observed in the table 2, the average value of cca is very low and therefore cca exhibits the existence of no correlation among the features in the two classes. hence it is wise to perform heuristic optimization techniques on the extracted numerical, non linearly overlapped uncorrelated features to attain the segregation of the features among the classes. since eeg signals are complex and non linear in nature, these signals will exhibit erratic and random response which can be conceptualized using chaotic theory. chaos is a field of study which is well known as nonlinear dynamics. a nonlinear system is represented by the nonlinear time domain equations comprising the dynamic property of the variables in their non linear form. correlation dimension (cd) is a famous study in chaos theory and it is calculated for eeg signals in this study. the table 3 shows the average correlation dimension values for extracted features among the normal and schizophrenia cases. it is evident from the table 3 that the correlation dimension values for both the classes are separated. the higher value of cd is due to the presence of randomness in the features. as for the schizophrenia cases the low value of cd is exhibited by the existence of dampness in the features."
"the consolidated result analysis of isomap with group search optimization with six types of classifiers for schizophrenia cases is tabulated in table 8 . since the optimization is done through the nature inspired algorithms and the classifier's output is plugged with false alarm in the range as low of 1.04% to higher value of 48.25%. gaussian nbc classifier attains higher parametric values like accuracy of 98.77%, gdr of 97.52% and pi of 97.45%. the optimized nbc classifier is ebbed to the lower parametric values like accuracy of 75.87%, gdr of 51.75% and pi of 6.747% and this lower performance indicates the presence of false alarm of 48.25%. as indicated in the table 8 that all the classifiers have nil missed classification in this optimization technique and this makes the classifiers with low sensitivity and low threshold, and with high specificity one. table 9 depicts the consolidated results of accuracy (%) among the classifiers at various optimization techniques with different features for normal cases."
"interestingly, the compliant formulation of eq. (21) can be applied to continuous media mechanics methods such as fem, to combine accuracy and stability. in continuum mechanics, the elastic potential energy of deformable materials is typically integrated in space using gaussian cubature, i.e., as a weighted sum of energy densities computed at sample locations in the material:"
"a monotonic logarithmic discriminant function is chosen and is analyzed as for each class from the training data, the mean vector and covariance matrices of the discriminant function are calculated, and a hyperplane is used to separate the data."
"as reported in the tables 1 and 3, the extracted features are uncorrelated as shown in the results attained from correlation dimension values but the features are non linear and overlapped. the dynamics of the features are explored by calculating the entropy values and the singular value decomposition (svd) values. these calculated components indicate that the features are in need of optimization techniques for further processing. therefore, a simple threshold may not be useful in the classification."
"where φ is a measure of departure from the rest configuration (e.g., (l −l) for a spring), x are the independent kinematic degrees of freedom, c −1 is the stiffness (e.g., k for a spring). the elastic forces are computed as the opposite gradient of this potential energy:"
"evaluation of the proposed method is conducted through a subject-independent experiment. half of our data is used for training, while 25% of the speech samples is held out as a validation set, that serves for parameter tuning. the remaining 25% is used for testing. all sets are balanced in terms of gender. the distribution of speakers and samples for each class across the three sets is shown in table 1 ."
"we have presented a novel approach for visual discrimination between native and non-native speech in english. to the best of our knowledge, this work is the first attempt to address this binary accent classification problem by means of visual features only. we have shown that dynamic appearance descriptors, after undergoing vocabulary-based quantisation, are sufficiently informative to render the target task feasible. our experiments on a challenging corpus, that includes visual speech samples captured by mobile devices, illustrate the efficiency of the proposed fully-automatic method even in such challenging in-the-wild scenarios."
"motivated by the aforementioned ideas, we address discrimination between native and non-native speech in english, formulated as a binary visual-only classification problem. to the best of our knowledge, there has been no previous work that addresses this task through the visual modality only. furthermore, our approach has the advantage of not relying on either speech transcriptions or language-specific modelling, as opposed to other visual-only speech processing works [cit] . appearance descriptors are extracted at each frame from a rectangular region surrounding the speaker's mouth. dynamic information is encoded through concatenation of static features in a temporal window."
"the implication of diagnostic test is positive and if the subject has disease, it is termed as the sensitivity or the true positive rate is expressed as"
"results on the test set are reported in table 2 . our results show that dynamic appearance features address quite accurately the task of discriminating native from non-native speech. this behaviour is to be expected, as appearance information from the mouth roi reveals fine movement and tale-telling transient features, such as bulges and wrinkles. in other words, different pronunciation and articulation patterns can be visually identified by the positioning and configuration of the mouth and flesh around it. image transform-based features, namely dct, pca and dwt, stand out as the best-performing, with dct achieving the highest scores in terms of all measures. thus, frequency decomposition as well as eigen-decomposition of global texture variation provides discriminative information for the target task. this conforms to results stemming from the lipreading research [cit], which similarly show the robustness of the above descriptors in capturing essential information in visual speech. the high accuracy achieved by block-based dct could be attributed to its ability to encompass the richest local frequency information, that corresponds to the most prominent texture and edge structures in the mouth region. lower performance is yielded by lbp and hog, which capture local texture and edge orientation information, respectively. these are less informative as they do not capture buldges and folds representing the speaking patterns."
"the backtracking is one of the stochastic search technique [cit] and is shown in pseudocode 3. for solving different optimization problems, backtracking search algorithm is used as it has a single control parameter and a very simple structure. it is a population-based technique and it has a memory where population from previous generations are present which helps in the analysis and generation of search direction matrix. three basic genetic operators are present in this bioinspired technique such as mutation, cross-over and selection. a random mutation strategy is employed in this bsa that uses one direction individual for every target individual and is expressed as"
"the term geometric stiffness is generally employed in buckling analysis when linear approximations cannot handle large displacements for static analysis. it is associated with the geometrically non-linear part of the strain calculation. it is sometimes called stability coefficient matrix or initial stress stiffness matrix, see [cit] for more details."
"note that the geometric stiffness matrix is not symmetric in this case. this is due to the fact that rigid transformations are not a vector space. while non-symmetric linear systems are relatively easy to solve (using a lu decomposition for instance), non-symmetric lcps are more difficult to handle as most solvers expect at least a symmetric matrix. to cope with this issue, we use the symmetric part of the geometric stiffness: ( k + k t )/2. we found this approach to have virtually the same stabilizing properties as using the exact, nonsymmetric geometric stiffness matrix, as shown in fig. 5 ."
"where b is a constant. around 75% of candidates for the remaining of the population is selected in the scrounging operator and in the dispersion operator, random walks are volume 8, 2020 utilized. the random distance is expressed as"
"a feature-level normalisation scheme is applied on each speech sample, for all utilised image descriptors. in particular, the mean vector over the whole utterance is subtracted from the feature vector corresponding to each frame. this technique is in accordance with the feature mean subtraction, commonly used in the lipreading research [cit] ."
"in table 2 we analyze the conditioning of linear systems for cloth animation modeled with both distance constraints and triangular fem. the maximal condition number is always much smaller with our approach (kkt system) than with implicit stiffness, which could explain why the numerical solver converges more easily."
"principal component analysis (pca), when applied to pixel intensity values, models the intensity variation over the training mouth rois through an optimal linear transformation, in terms of minimum mean squarred error. our feature vector corresponds to the principal components accounting for the 95% of the total variance."
"from table 18, it is analyzed for schizophrenia cases that when pls non linear features are classified with classifiers, a high pi of 68.34% was obtained, a high classification accuracy of 89.55% was obtained, a high gdr of 78.9% along with an average error rate of 20.89% was obtained if classified with modest adaboost classifier. if em-pca features are classified with classifiers, then a high pi of 66.13% along with a high classification accuracy of 88.99% with low gdr of 66.67% and error rate of 22.01% was obtained if classified with real adaboost classifier. high gdr of 73.38% with the modest error rate of 26.56% is attained in the gaussian nbc classifier for em-pca feature extraction. the arrival of this situation is again attributed to the averaging effect of optimization methods for em-pca feature extraction. if isomap features are classified with classifiers, then a high pi of 75.55% and a high classification accuracy of 91.106%, gdr of 82.21% with error rate of 26.87% is obtained when classified with real adaboost classifier. to select a better classifier a compromise strategy is needed among the classifier parameters like pi, accuracy, gdr and error rate. generally, lesser the error rate means better accuracy among classifiers. as observed from the results of table 17 and table 18 the performance of classifier parameters are compressed due to the averaging effect of the optimization methods for the both cases of normal and schizophrenia. in other words, it implies that the optimization methods enhance the classifier performance to a higher level. the main reason why the combinations of isomap-backtracking search optimization-modest adaboost classification for the normal cases and the isomap-flower pollination optimization-real adaboost classification for schizophrenia cases gives the best result is because of the intrinsic property of isomap algorithm which has been explored well by both backtracking and flower pollination algorithms to optimize the features well with very less redundancy and so upon classification with the versions of adaboost algorithm, it gives the best result."
"we present a new approach to take transverse directions into account while only solving a regular linear equation system. this formulation allows stable and fast simulation of nonlinear energy-based constraints, such as the ones issued from the continuum mechanics. we first introduce concept of geometric stiffness using a simple example, then we show how we can use it to stabilize any constraint derived from an energy such as potentials and elastic constitutive laws."
"the right-hand term represents the time derivative of the constrained value (the velocity error) that would occur if no constraint forces were applied. the unknowns µ represent the constraint impulse intensities necessary to cancel this error. the equation matrix in eq. (9) is positive semi-definite, since it is singular in case of redundant constraints. when eq.(9) is solved using a block gauss-seidel solver, the method is usually called impulse-based [cit] ]. this approach is easy to implement, straighforwardly handles constraint redundancy, and can be fast."
"experimental findings have shown that appearance-based features, calculated in the area around the speaker's mouth, can efficiently encode valuable information from visual speech [cit] . however, their performance largely depends on the accuracy of the region of interest (roi) tracking."
"while simulating a long chain (e.g. 50 rigid bodies) is very hard using regular constraints, it is possible with our solver. the accompanying video shows the same example with hinge joints. without geometric stiffness, the chain keeps vibrating even using a mass ratio of 1 : 1 and a tiny time step. using the geometric stiffness, the same chain is very stable even for large time steps. it is possible to create very long chains and to add joint limits similar to the ones found in bicycles."
"constraint-based elasticity unifies elasticity and constraintbased simulation, and leaves a modeling choice to the simulation designer. large material stiffness is better handled using lagrange multipliers with a compliance matrix in the bottom-right matrix block, however this creates a larger equation system. we call this the compliance formulation. low stiffness is thus better handled in the top-left block using a stiffness matrix as in regular elasticity, which we call the stiffness formulation. although our approach combines lagrange multipliers and penalty forces, it should not be confused with the augmented lagrangian method [cit] which requires several linear solves and only handles hard constraints in its standard formulation."
"the diagram in figure 2 illustrates how mean f1 varies on the validation set with increasing values of vocabulary size, for each different appearance feature. as can be seen, the optimal number of clusters for all features, except for lbp, is 16. this finding suggests that broader clusters of the vector space are more suitable to quantise dynamic appearance of mouth configurations. this is intuitive, taking into account that most practical definitions of visemes, i.e., the equivalent in the visual domain of audio phonemes, agree on a total number of visemes in the range 11-16 [cit] ."
"the extracted features are then optimized through nature inspired optimization algorithms. the main purpose of any optimization algorithm is to search and locate the best solution to a particular problem. with the help of various agents, the search process is done generally which in turn evolves iteratively depending on certain rules of the algorithm involved. therefore, in our work, four optimization algorithms are used specifically. the degrees of exploration with which the members of the group or search space can move across can be ascertained well with these four algorithms. in engineering, academia and industry, multi-objective optimization is quite challenging to solve and therefore sophisticated techniques were implemented to tackle it in an efficient manner. applications of these four optimization algorithms have been used in various applications like engineering, medicine, astronomy, banking, financial risk management etc [cit], but this is a new attempt to use them for schizophrenia eeg signal classification."
"using a minres solver, the stiffness version never converges (even after 10000 iterations), while our method converges in about 1000 iterations. it is fairly stable and several hundred kg can even be attached at the bottom of the cloth without stretching it."
"hard constraints can be used to model thin deformable objects one can bend in the transverse (unconstrained) direction, while being theoretically inextensible in the constrained direction. however, when the tensile force is large with respect to the particle masses, instabilities in the transverse direction(s) occur, making the method impractical for such applications."
it is observed from the table 9 that the real adaboost classifier attains higher accuracy of 98.176% in pls non linear regression feature extraction with flower pollination optimization method and the low accuracy of 76.22359% for nbc classifier. likewise for em-pca feature extraction with back tracking search optimization the classifier optimized nbc attained higher accuracy of 95.83% and low accuracy of 76.87% is reached for gaussian nbc classifier in em-pca feature extraction with flower pollination optimization method. as in the case of isomap feature extraction modest adaboost classifier with back tracking search optimization attained higher accuracy of 98.77% and low accuracy value of 76.67% is reached in adaboost classifier with flower pollination optimization method. table 10 depicts the consolidated results of accuracy (%) among the classifiers at various optimization techniques with different features for schizophrenia cases. it is observed from the table 5 that the optimized nbc classifier attains higher accuracy of 98.176% in pls non linear regression feature extraction with flower pollination optimization method and the low accuracy of 76% for gaussian nbc classifier. likewise for em-pca feature extraction with back tracking search optimization the classifier gaussian nbc attained higher accuracy of 97.65% and low accuracy of 76% is reached for modest adaboost classifier in em-pca feature extraction with eagle strategy using different evolution optimization method. as in the case of isomap feature extraction real adaboost classifier with flower pollination optimization method attained higher accuracy of 98.77% and low accuracy value of 75.875% is reached in optimized nbc with group search optimization method.
"constraint-based simulation is very popular for implementing joints in articulated rigid bodies, and to enforce inextensibility in some directions of deformable objects such as cables or cloth. its mathematical formulation makes it numerically robust to infinite stiffness, contrary to elasticity-based simulation, and some compliance can be introduced in the formulation or obtained through approximate solutions. unfortunately, when the constraint forces are large, constraintbased objects are prone to instabilities in the transverse, unconstrained directions. this occurs when pulling hard on inextensible strings and sheets, or on chains of articulated bodies. the spurious vibrations can lead to unrealistic behaviors or even simulation divergence. they can be avoided using small time steps or complex non-linear solvers, however this dramatically slows down the simulation, while many applications, especially in interactive simulation, hardly allow for one linear solution per frame. the simulation speed can only be maintained by relaxing inextensibility, or using implicit elastic bending forces, however this changes the constitutive law of the simulated objects."
"compliant constraints with geometric stiffness allow the stable simulation of arbitrarily stiff behaviors and large mass ratios with larger time steps than previous methods. they are compatible with standard elasticity, and provide an alternative way to model constitutive laws. this is especially useful for large and infinite stiffnesses, which can be simulated using a single linear solution at each frame, while previous methods require complex non-linear solvers, smaller time steps or modified constitutive laws with ad-hoc parameters. this approach does not require pre-factorizations, and thus allows creating or deleting constraints on-the-fly, which is useful for artistic control. we have successfully applied it to mass-springs systems, 2d and 3d continuum-based elastic models as well as articulated rigid bodies with kinematic loops, with accelerations up to several orders of magnitude with respect to previous methods. we believe that its combined simplicity, efficiency and versatility, make it a useful base tool for mechanical simulation."
"dynamics of visual speech carry valuable information related to accent patterns and modes of articulation. in order to capture dynamic characteristics of speech, we concatenate ±l adjacent feature vectors around the current frame into a large vector. the value for l is set to 2 in this study, i.e., a 5-frame window with a corresponding duration of 1/3 secs is used."
"features after the optimization is prevalent. but the presence of non linearity still makes the optimized feature to devise better segmentation by a chosen group of classifiers. table 6 shows the cca for various optimization techniques with different features among schizophrenia and normal cases. if the cca value is greater than 0.5, then it exhibits correlation among the classes. as indicated from the table 6, except for pls non linear regression with flower pollination optimization and eagle strategy using different evolution optimization, all the other methods preserve the non-correlative behavior of the extracted features."
"the 2 bayes classification rules are as follows: y), y is assigned to h 2 with µ j as the mean value and z j as the covariance matrix, for the gaussian probability distribution function [cit], it makes it more feasible for analysis and is expressed as"
"to help motivate our contributions and to review the previous work at the same time, we illustrate these approaches on a simple constraint: the distance between points."
"all the above mentioned related research has relied exclusively on acoustic features, ignoring information from the visual cue. however, features derived from the visual modality have been successfully used for speech recognition [cit] . such findings underline the contribution of visual information to speech perception by humans, especially when the auditory stream is noisy. furthermore, visual speech alone has proved sufficient source of information for human observers to identify the language spoken by a talker. for example, [cit] report several experiments, where the participants perform much better than chance in visual discrimination between english and spanish. newman and cox [cit] employ appearance and shape features, along with language phonotactics modelling, and show that automatic language identification is feasible even through visual features only. however, their approach depends on the accurate sub-phonetic or phone-level recognition of visual speech."
"the extension of our approach to such constraints is straightforward. the geometric stiffness can be set in the (top-left block) dynamic matrix in the exact same way as for bilateral constraints, while the signorini conditions are applied to the constraint velocities and impulses. in particular, in the case of springs the transverse motion will still be penalized bilaterally, while the extension motion will undergo unilateral constraints. the resulting kkt system can be readily solved using a general qp solver. when the dynamics matrix is easily invertible it is possible to compute the schur complement in order to obtain an equivalent, but smaller linear complementarity problem (lcp) that can be solved by various algorithms [cit] . coulomb friction can also be solved using constraint-based methods [cit] . our method is compatible with all these numerical solvers."
"cables significant improvements are achieved on the springs (i.e. distance constraints), most likely because they have two transverse directions for only one material direction. to test the distance constraints with our solver, we model an inextensible cable with null bending stiffness and mass properties close to that found on a crane (500kg for 10m). we attach one extremity and we load the other with a heavy object, creating large mass ratios between the two. starting the simulation with a tight cable, our constraints are able to pull very large weights with negligible extension using an ldlt solver. for a fixed 0.01s time step, without geometric stiffness, the simulation quickly diverges as shown in table 1 ."
this algorithm is developed and inspired from the flower pollination process of the flowering plants and it is usually extracted to multi-objective optimization [cit] . the four rules are utilized in this algorithm for simplicity reasons.
we detail the computations for one gauss point of a 3-dimensional material which deformations are measured using green-lagrangian strain. computations for 1d and 2d materials and other strain measures follow the same methodology.
"accent manifests itself in speech through a set of pronunciation, intonation, lexical stress, rhythmic and articulation patterns, present in a common language of a group of people. determining whether a speaker is native or non-native from a spoken utterance, without knowledge of speech content, could arguably enhance speech recognition, as a preprocessing accent-biasing step, or serve speaker verification purposes, as a biometrics problem in its own right [cit] . moreover, it could provide assistance to intelligent applications that require voice/conversation adaptation [cit] ."
"ragdoll the wild ragdoll presented in fig.(1) is made of 12 rigid bodies articulated with joints limits (unilateral constraints). it is swinging on an articulated rigid liana composed of 10 ball-and-socket links. the simulation runs at 70 fps with a 0.01s time step using a projected gauss-seidel lcp solver (on the schur complement computed from a ldlt decomposition) with 200 iterations. it swings naturally, and remains stable even when strongly shaken by user interaction. conversely, neglecting the geometric stiffness produces an unstable simulation."
"potentially all non-linear constraints can be stabilized using geometric stiffness. we present here some examples to demonstrate the robustness of our method in various challenging situations. we stress that all the results presented in this section were obtained using a single linear system solve per time step, without extra constraint stabilization passes, using one thread of an intel(r) xeon(r) cpu @ 2.40ghz. our solvers are available in the compliant plugin of the sofa open-source library [cit] ]."
"in summary, simulating very stiff objects or constraints remains challenging. the implicit integration of stiff elastic forces is numerically hard, and requires complex non-linear solvers to achieve the desired stiffness. hard constraints, or their extension to compliant materials, are numerically easier but instabilities occur in the unconstrained directions due to non-linearity. position-based approaches handle nonlinearity thanks to iterative local solutions which unfortunately are not efficient for stiff objects. non-linear solvers have been proposed, but they require several linear solves per iteration and they do not accurately handle elasticity."
"where w denotes an input matrix of wavelength signals, a represents a matrix containing regression coefficients and the bias vector is represented by g. the matrix a has the following form:"
"some limitations of our work are left for future work. stiff damping is a straightforward extension. more non-linear constraints will be handled. the stability, while dramatically better than before, is not guaranteed since geometric stiffness only provides a more accurate first-order operator, not a real non-linear solution. in the compliant formulation, the mass-stiffness matrix looses its block-diagonal structure, which makes it more complex to factor. numerical criteria for choosing between constraint and elasticity could be useful."
"from table 16, it is analyzed for schizophrenia cases that a least error rate of 3.64% is obtained when pls non linear features and flower pollination optimization is classified with optimized nbc classifier. it is also analyzed that a least error rate of 4.68% is obtained when em-pca features and back tracking search optimization is classified with gaussian nbc classifier. also, a least error rate of 1.04% is obtained when isomap features and flower pollination optimization is classified with real adaboost classifier."
"most related early work has approached accent identification through a classification framework whose goal is to assign a speech example either to the accent of mother tongue or to one of separately modelled foreign-language accents [cit] . these approaches use hidden markov models (hmms) modelling, on the phoneme or word level, trained on cepstral or prosodic acoustic features, and are evaluated on isolated word databases. trajectory-based classifiers are, alternatively, used to capture accent-sensitive spectral dynamics [cit] ."
"one of the famous metaheuristic strategy for optimization is eagle strategy [cit] . a combination of intensive local search and crude global search is used by the eagle strategy which employs varies algorithms to match different applications. utilizing a levy flight random walk, the global search space is explored initially by this strategy. once a promising answer is found out, then utilizing a local optimizer such as hill climbing or de, an intensive local search is employed. here in this work, de is used. then in a new region again this two stage method is initiated which starts with the exploration of a new global space followed by a local search. a balanced end for find the current best solution h * end while output the best solution found. and good tradeoff between global search and fast local search can be obtained for different search stages and therefore any algorithm of our choice can be utilized here. therefore to produce good results, the combination of various algorithms is efficiently utilized here. the switch between the local and the global search is controlled by the only parameter s e . it serves the dual purpose of exploration and exploitation. it is a simple strategy or method and not an algorithm. to explore the search space in a much more effective and diverse manner, the algorithm utilized for the global exploration should have more randomness. as the system converges, the speed also rises. to utilize the local exploitation in an intensive manner, an efficient local optimizer is used. with a less number of functions, the main intention is to reach the local optimality as fast as possible. the procedure of the eagle strategy using different evolution is expressed in pseudocode 2."
"to study the effect of feature extraction and classifier performance irrespective of the optimization methods, an averaging of optimization results is initiated for the both normal and schizophrenia cases. the results are tabulated in the table 17 and table 18 . from table 17, it is analyzed for normal cases that when pls non linear features are classified with classifiers, a high pi of 77.26% was obtained, a high classification accuracy of 91.47% was obtained, a high gdr of 82.94% along with an average error rate of 17.05% was obtained if classified with adaboost classifier. if em-pca features are classified with classifiers, then a high pi of 73.05%,along with a high classification accuracy of 89.27%, gdr of 72.66% and error rate of 21.46% was obtained if classified with optimized nbc classifier. as in the same em-pca features, high gdr of 75.36% with the error rate of 24.63% is attained in the real adaboost classifier. this peculiar situation is due to the averaging effect of optimization methods for em-pca feature extraction technique. if isomap features are classified with classifiers, then a high pi of 82.08% and a high classification accuracy of 92.905%, gdr of 84.81% with error rate of 14.18% is obtained when classified with nbc classifier."
"characterized by abnormal behavior, decreased ability to understand reality, strange speech etc, schizophrenia is a very dangerous problem to the human community. people diagnosed with schizophrenia also have additional problems like depression, anxiety, lack of emotional expression and motivation. in this work, a comprehensive analysis of schizophrenia classification from eeg signals is done well with the help of feature extraction, optimization techniques and suitable classifiers. the methodology adopted here in this paper is quite promising and easy to implement. the average performance measures among the classifiers at various optimization techniques with different features for normal cases and schizophrenia cases were explained in the work. the average pi and average error rate too was computed and presented among the classifiers at various optimization techniques with different features for normal cases and schizophrenia cases in this work. the individual results show that for normal cases, isomap features when optimized with backtracking search optimization algorithm and classified with modest adaboost classifier, a classification accuracy of 98.77% is obtained. for schizophrenia case, individual results show that when isomap features are optimized with flower pollination optimization algorithm and classified with real adaboost classifier, a classification accuracy of 98.77% is obtained. future works aim to work with different feature extraction techniques, optimization techniques and a plethora of other machine learning techniques to classify the schizophrenia from eeg signals."
"to study the effect of feature extraction and optimization techniques, irrespective of individual classifier performance an averaging of the parameters like accuracy and gdr is initiated among the six classifiers. the same is shown in table 11 and table 12 . from table 11, it is understood that a highest classification accuracy of 90.46% and a gdr of 80.92% is obtained if pls non linear regression features are optimized with flower pollination algorithm for the normal cases. and also the lowest accuracy of 82.11% and gdr of 64.23% is attained in the em -pca feature extraction with group search optimization method. from table 12, it is understood that a highest classification accuracy of 90.105% and a gdr of 79.35% is obtained if pls non linear regression features are optimized with eagle strategy using different evolution optimization for schizophrenia cases. as shown in table 12, em-pca feature extraction with group search optimization method reached the low value of accuracy 76.82% and gdr of 39.58% for schizophrenia cases. this is due to the accumulation of more false positive in the schizophrenia case."
"one of the techniques to attain an accurate classification is through means of ensemble mode-based classification such as adaboost [cit] . the learner's instability is greatly attributed to the effectiveness of the boosting. a base classifier is employed sequentially for boosting based on a weighted version of the training sample set. the weak learner is referred to as the base classifier. to solve complicated behavior classification problems, boosting a set of weak learners can be done easily. as a weak learner, decision tree is applied. in adaboost, equal weights are assigned to all the training samples. according to the weak learning error rate obtained in the previous iterations, the weights in each epoch are updated."
"partial least square (pls) method is quite advantageous to perform ordinary multiple linear regression as the collinearities in the predictor variables are assumed here which is nothing but the combination of the original input data in a linear manner [cit] . based on a covariance criteria, the input variable matrix is decomposed and it is mainly relied by the pls. the latent variables or factors are found out by the pls that are correlated with the output variables and descriptors of the input variables. using the following linear equation, the concentration of the component is expressed as follows:"
"where in class m, the weighted number of occurrences of token q is expressed as β 1 and in class m, the total weighted numbers of occurrences of all token q is expressed as β 2, the total number of instances in the training set is expressed as n ."
"assume the eigen vector matrix v . to estimate the input parameters v for orthogonalization, this process is used. e-step and m-step are the two important steps used here. unless the differences between the variance is less than the threshold value or is equal to the threshold value, the em step is carried out repeatedly. the eigen vectors will be selected as the input in a random manner before entering the step and is expressed as;"
"knee figure 8 shows the simulation of a knee with ligaments and muscle-tendon units using rigid bodies in contact, connected by curved springs. the spring particles as well as the muscle, tendons and ligament surfaces are attached to the bones using linear blend skinning. the independent degrees of freedom (dofs) are the translations and rotations of the bones, and the spring forces are propagated to them using the transposed of the skinning jacobian js. since ligaments and tendons becomes very stiff after an elongation of about 5%, we model these bi-phasic materials by combining two 1d linear elastic laws: a soft one formulated as a stiffness, and a stiff one above a strain limit (compliance formulation with unilateral constraints). applying varying strain limits allows us to simulate hamstring muscles contractions to flex the knee. the simulation runs very responsively at 11 fps, using an active-set minres-based qp solver. the geometric stiffness kg of the embedded springs is propagated to the dofs as j t s kgjs. without it, the simulation is stable only for very small time steps, or with large damping ratios, resulting in less interactivity or instabilities as can be seen in the accompanying video."
"as the eeg signal is quite chaotic, to explain the most important features of the morphology of the eeg signals, nonlinear features are utilized. instead of analyzing regular features like detrend fluctuation analysis (dfa), hurst exponent, recurrence quantification analysis (rqa), entropy, fractal dimension, kolmogorov complexity, hjorth component, lempel-ziv complexity, auto regressive (ar) coefficients, wavelet transform, eigen vectors etc. [cit], here in our paper pls based nonlinear regression features, em-pca features and isomap features are extracted before optimization is proceeded. the feature extraction based on isomap was implemented for applications like video manifold [cit], semi-supervised local multi-manifold computation [cit] and electromechanical equipment fault prediction [cit] . the feature extraction based on em-pca was implemented for applications like face recognition [cit] and classification in bci [cit] . the feature extraction based on pls-nlr is useful in applications like development of relevance feature vector machine [cit] and denoising application [cit] . owing to the versatility of these feature extraction techniques, it has been preferred in our works rather than the conventional techniques."
"for every training instance x l, the weak hypothesis f t (x l ) be f k t (x), where k is the index of partition that x l falls into. weight updation is expressed as"
"the feature of high-dimensional space mapping is determined by the kernel and the regression performance of it is affected. to develop a strong nonlinear regression model, different kernels are utilized generally such as linear, gaussian, polynomial, inverse multi quadratic, semi local, exponential, rational, knode etc. in our work, gaussian kernel is used and is expressed as"
"when the data points are present close to a low dimensional non linear manifold embedded in a high dimensional space, and linear approximation cannot be used to adequately represent the non linear structure, the standard multidimensional scaling (mds) cannot be used. recovering the low-dimensional structure of a non linear manifold is quite difficult with mds. therefore, isomap has been designed to discover the structure of high dimensional data and then trace its embedding in a low-dimensional euclidean space making it as a class of non linear embedding schemes. in isomap, instead of considering the euclidean distance, the geodesic distances between the points are extracted [cit] . by constructing a sparse graph, the computation of the geodesic distances are done in which every node is connected to its closest neighbours. between each pair of node, the geodesic distance is considered to be the shortest path length in the graph so that the connections would be more feasible. then to the classical mds, these approximate geodesic distances are then utilized as input. the algorithm for the extraction of features through isomap is expressed in algorithm 2. from the fig. 2 it is observed that the histogram plots of the em-pca extracted features for normal case shows the gaussian normal behavior for the underlying eeg signals."
"step 2: shortest path computation: on the manifold m, the geodesic distances e m (i, j) is estimated by isomap between all pairs of points by means of computation of their respective shortest path distance e w (i, j) in the graph w ."
"in this work, we show how to perform stable and efficient simulations of both extensible and inextensible constraintbased objects subject to high tensile forces. the key to transverse stability lies in the geometric stiffness, a first-order approximation of the change of direction of the internal forces due to rotation or bending. neglecting the geometric stiffness, as usually done in constraint-based simulation, is a simplification of the linearized equation system, which in turn is a simplification of the exact, non-linear implicit integration. in case of thin objects, this leaves the transverse directions unconstrained, leading to uncontrolled extensions after time integration, introducing artificial potential energy. while this is acceptable for small stiffnesses or short time steps, this may introduce instabilities in the other cases. in this paper, we show that solving the complete linear equation allows high stiffnesses and large time steps which were only achievable using much slower non-linear solvers before. we show how to handle the geometric stiffness in a numerically stable way, even for very large material stiffness. the implementation is easy to combine with existing implicit solvers, and can provide several orders of magnitude speed-ups. moreover, it allows a unification of rigid body and continuum mechanics."
"such measurements are generally obtained through wellknown or hand-tailored benchmarks. they measure key platform characteristics such as network latency and bandwidth, software overhead, memory speed, floating point performance or energy usage. these benchmarks generally execute precise sequences of operations and rely on data captured from cpu hardware counters, by dynamic binary instrumentation or from mpi tracing hooks, for instance."
"simulations are performed with ldpc redundancy 50%, file size 224kb, error rate 1.3% (substitution: 0.4%, deletion: 0.85%, insertion: 0.05%) and with 15% reads being random sequences (to simulate unaligned reads). table 3 shows the results, where the bch parameter represents the number of bit errors the bch code can correct. we use 2 bit error correction because it offers the best balance between writing and reading cost for this error rate. as discussed in section 3, we use a couple of heuristics to handle reads with insertions and deletions without relying on consensus. during index decoding, we attempt to correct a single insertion or deletion error with a bch code which typically can only correct substitution errors. for most files, we observed that this step is able to correct 5-10% additional indexes. table 4 shows the results with and without this step. we see that this step reduces the reading cost by around 5% for all three ldpc codes without affecting the writing cost. we also use a synchronization marker to recover part of the read if the consensus length is incorrect. we conducted real experiments with and without a marker, but as described in section 4.1.3 the results were not conclusive due to differences in the coverage variance and error rate across experiments. in simulations, we did see that using a marker leads to around 10% improvement in the reading cost while having little impact (2-3%) on the writing cost."
"besides the energy issue, the spectrum scarcity problem is another bottleneck for the development of iot due to the explosive increase of communication devices. moreover, the current fixed spectrum allocation strategy has resulted in a low spectrum utilization efficiency [cit] . to alleviate the spectrum scarcity problem, cognitive radio (cr) has been proposed [cit] . in cr, the secondary network can coexist with the primary network on the condition that the interference imposed on the primary user (pu) is tolerable. there are three operation paradigms in cr, namely, opportunistic spectrum access, sensing-based spectrum sharing and spectrum sharing [cit] . in this paper, we focus on the spectrum sharing paradigm due to its easy implementation in practice."
"for (7), a reasonable explanation is given as follows: sparse cca maximizes the correlational information between the two projected models to achieve specific and smooth distributions, while training an auto-encoder to minimize reconstruction error amounts to maximizing a lower bound on the correlational information between inputs and learned features. the dpcae model gives a trade-off between the information captured in the (input, feature) mapping within each model on the one hand, and the information in the (feature, feature) relationship across modalities."
"dpcae could be further improved by using other neural network structures, e.g., self-organizing map network (som), which is a competitive unsupervised neural network, and also can map the high-dimensional feature representations to the low-dimensional space and maintain the topological formation of the input in the high-dimensional space [cit] . however, directly using the som network to multi-modalities learning of snps-fmri may face many challenges because brain imaging and single nucleotide polymorphisms data cannot have some output coming back as input. despite the difficulties of using som networks, using recurrent neural network structure might be an attractive way to exploit the complex non-linear information or simple linear information within snps-fmri data [cit], another prospective schedule is to compare dnn-based methods with methods grounded on deep boltzmann machines [cit] . multi-label learning arouses great interests in many applications, it can not only enhance the uncertainty but also improve the similarity measurement of multi-label data with labels information [cit] . but the lack of the labeled data and the complex structures of various data may make learn the uncertainty and representativeness accurately become hard, a multiple kernel active learning framework could be used to solve this problem [cit] . we will consider applying multi-layer belief networks or som networks to represent original data and then seeks their correlations, while also linking the data representation with phenotypical information."
"building on our previous experience with network measurements (see section iii), our first concern was to thoroughly randomize each parameter of the micro-kernel (stride and size). figure 8 shows the first experimental results on a pentium 4 cpu (see figure 5 for more details). each dot represents one measurement (42 repetitions for each configuration) of the bandwidth as a function of the buffer size. the color indicates the stride, while the solid lines represent smoothed local regressions indicating measurement figure 7 with a pentium 4. our measurements our much more noisy despite the controlled environment. we obtained similar behavior on other kind of architectures."
"traditional cca algorithm can only determine the maximal correlation in a series of canonical independent vectors, figure 1. illustration of the canonical correlation analysis model. in this model, two sets of data include the same number of samples and different numbers of high-dimensional features. using loading vectors u, v to get canonical variable, and perform the correlation analysis to obtain the maximal correlation."
"the sequenced data usually includes some number of reads that do not align properly to the original oligonucleotides. this can be due to synthesis/sequencing errors or due to phix spike-in (known phix virus dna added to the sequencing pool for quality control purposes). the decoding algorithm identifies and removes such reads during the primer removal step and the index decoding step. since the fraction of unaligned varies from experiment to experiment, the results in table 1 included only the aligned reads for the sake of comparison. if the decoding is performed using all reads, the reading cost increases depending on the percentage of unaligned reads. for example, the first experiment in table 1 included roughly 13% unaligned reads and the reading cost increases from 2.73 bases/bit to 3.16 bases/bit when these are included in the decoding process. to understand the impact of the bch code parameter on the performance of the scheme, we conducted real experiments exploring a range of these parameters. unfortunately, due to the differences in the coverage variance and error rate across experiments, we were unable to reach any definite conclusion from these experiments (results available in supplementary material section 3). here we provide some simulation results for these parameters."
"firstly, comparing the proposed algorithm and the other algorithms, and then demonstrate related work on deep principal correlated auto-encoders (sections a-d), sections a describes that classification difference of two data types between different subjects, sections b gives a description of the performance measures about clustering and classification respectively, sections c describes receiver operating characteristic of multi-modality data, and the last section describes that the maximal correlation of various advanced methods on the different number of dimensions."
"comparing the power of the dpcae algorithm to that of other representative algorithms, including cca, sparse cca, sparse mcca (smcca), deep cca (dcca), minimumdistance auto-encoders (distae), deep canonically correlated auto-encoders (dccae), and deep principal correlated autoencoders (dpcae) in terms of both fmri classification and the classification of snps data. dpcae's network uses mcic dataset to train and then the trained network was used to classify. some meaningful preprocessing methods, including mainly data augmentation, data standardization, and so on, were implemented on the multi-modality data [cit] . the dpcae model applied two modal data to study brain development of schizophrenia. it needs a large sample size in order to train deep networks. however, collecting fmri data is very expensive, and therefore the sample sizes of existing fmri cohorts are limited. a reasonable method to generate more valid data is data augmentation. data augmentation is a widely used method in deep learning fields, notably when dealing with images. for application of deep learning in image classification, data augmentation methods, e.g., image rotation, image reflection, scaling, are continually applied to generate added 'real' images."
"i. introduction hpc architectures and application have become increasingly complex and evaluating the requirements of modern hpc applications or planning platform upgrades requires rigorous performance characterization. in such context, the ultimate goal is to predict the performance of an application on a given platform, enabling users and researchers to study scalability, deployment optimizations, extrapolation, and what-if scenarios. a common approach consists in convolving platform characteristics with application characteristics through a simulator [cit] . however, faithful predictions require to instantiate such simulators with faithful measurements obtained on existing platforms."
we studied the performance of clms algorithms for parameter estimation over wireless sensor networks where links between nodes and the fusion center are impaired by fading and noise. the analysis and the numerical simulations show that the proposed bc-clms algorithm is stable and converges to an unbiased estimate for small adaptation step-sizes.
"it turns out that the seemingly simple cache memory benchmark code, presented in figure 6, is such a complicated application. in this program, elements of the array are accessed in a loop of nloops iterations. increasing the number of repetitions should proportionally increase the overall time to run the program, thus the nloops parameter should not have any influence on the final bandwidth. however, it proved to be the opposite for the intel sandy bridge (i7-2600, see figure 5 for more details) machine with ondemand governor (linux version 3.1.0-1-amd64 with debian 3.1.8-2), as shown in figure 10 . in fact, when the amount of work required from the processor is low, the operating system decides to use the lowest frequency, thus the array is accessed slower and the resulting bandwidths are low (top left plot of figure 10 ). on the contrary, when the operating system anticipates a large number of accesses, it increases the frequency to the maximum value, which leads to the highest bandwidths (bottom right plot). the most interesting results can be observed for the intermediary values of nloops where operating system dynamically changes its decision about the optimal frequency for a given code block. therefore, for each of the 42 repetitions of the same buffer size, the measured bandwidth varies between several modes and the performance variability is higher. this illustrates that even for simplistic codes it may be extremely difficult to anticipate which frequency will be used, and thus what will be the corresponding performance."
"and γ e,k are slack variables. using the sdr method [cit], problem p 1 can be relaxed as problem p 2, given as"
"the current implementation is written in python with the libraries for ldpc codes, bch codes, barcode removal and multiple sequence alignment written in c/c++. all experiments were done on a server with 40-core intel xeon processor (2.20ghz) and 256 gb ram. for the 50% redundancy ldpc code (experiment #3), encoding 224 kb of data takes 1m30s and uses 4.1 gb of ram."
this suggests that there could be other deviations hidden when the full potential of the machine is not reached. note that the code complexity (manual vectorization and loop unrolling) required to reach such peak performance makes its use for classical program performance extrapolation quite questionable.
"this measure tries to capture the syntactic similarity between two sentences using dependencies. previous experiments showed that converting constituents to dependencies still achieved best results on out-of-domain texts [cit], so we decided to use a 2-step architecture to obtain syntactic dependencies. first we parsed pairs of sentences with the lorg parser 4 . second we con-verted the resulting parse trees to stanford dependencies 5 ."
"where d(x, y) is the spherical distance in km. between x and y, and k is a normalization factor set to 10000 km. to obtain similarity values between 1 and 0."
"to stress test the framework, we performed simulations at increased error rate of 6% (substitutions, deletions, insertions 2% each) along with 15% reads being random sequences (to simulate unaligned reads). we encoded a 224kb file with 50% redundancy ldpc code and a bch code capable of correcting 3 errors (writing cost 1.07 bases/bit). given the high error rate, the parameter for the ldpc decoding (see section 3) was set to 10% instead of the default value of 4%. the decoding succeeded at a reading cost of 10.5 bases/bit showing that the framework can be used even at relatively high error rates, although it might not be optimal in this setting."
"all subjects were commanded to collect blood samples and extract candidate genes and genotyping. at the fewest 200 ng of candidate genes were used to type each schizophrenia patient and healthy control in the light of the manufacturer's protocol. after professional clinical techno-logy handling, the particular candidate genes were fluorescently labeled and detected making use of an appropriative scanner. then some nonspecific hybridized fragments were separated by washing while these residual specific hybridized candidate genes were further processed [cit] . to permit the pursual of subjects, capability control was used by statistical analysis, the genome-wide data management system was used to import raw genotypic data. at the end of the process, experimenters created 1,140,419 snps loci dataset. after capability control procedures, the dataset result in 777,365 snps loci left for our experiment finally. lots of snps with linkage disequilibrium (ld) were spotted to reflect the genetic variant at different loci in learning high-dimensional features of samples. a subset of these simulated snps data showed the representative ld framework reflected the genetic variant, where special genetic markers that approximate one another on the candidate genes were in stronger ld, issuing in a representative block-like framework."
"our participation in the english task was hampered by some technical problems which did not allow us to complete the parsing of the tweet data in time. as a consequence of this and some errors in the scripts launched to finalize the experiments, the submitted results were incomplete and we were able to detect the problem only after the submission. we show in table 3 the official results of run1 with the addition of the results on the onwn dataset calculated after the participation to the task."
"despite the simplicity of our memory benchmark (see section iv), the factor set revealed much larger than what we initially thought. these parameters, grouped by features, are presented in figure 13 in a modified \"cause-and-effect\" diagram [cit] . we have written a small independent program which considers all the factors listed in figure 13 . this code randomizes the order of the experiments, combining all involved factors. as output, it generates a csv file that is the textual representation of the design. the csv is used as an input by the benchmark which is, in the second stage of our methodology, a c program based on the code listed in figure 6 . it executes all the experiments and reports the memory bandwidth for every factor combination. the analysis is carried out using literate programming in orgmode/r."
"to compare the power of dpcae with that of other advanced models, dpcae is applied to integrate snps dataset and fmri dataset, and use other advanced models to integrate snps dataset and fmri dataset as an instance for comparison. after that, the experiment compared the power of maximal correlation of dpcae and other advanced models for different number of dimensions. in the experiment, 5-fold crossvalidations were used to test the data set. 80% of samples were randomly selected from all samples as the training set, and the remaining samples were used as the test set. in order to minimize the adverse effect on the results caused by the difference between the training set and the test set, we chose the result with the smallest correlation coefficient difference between the training set and the test set in the 5-fold crossvalidations. here both dpcae and other advanced models were performed 50 times, and the maximal correlation was calculated, each using 50% of the snps data and fmri data for learning projections, 30% for adjusting hyper-parameters (regularization parameters learning rate, the total number of neurons in all intermediate layers and so on), and 20% for eventual testing. fig. 9 shows different maximal correlations acquired using traditional cca, scca with l 1 penalty, special scca with more than two data sets, deep cca with deep network including non-linear mapping, minimum-distance autoencoders, deep canonically correlated auto-encoders and deep principal correlated auto-encoders. the dccae model is inclined to find slightly non-linear relationship in the first little margin, after which the dpcae model exceed them fig.9 ."
"recently, an emerging technique called physical-layer security has been proposed to improve the security of the wireless communication systems [cit] . it exploits the channel characteristic to achieve secure communication. however, the secrecy rate achieved by using physical-layer security is limited by the channel state information (csi) [cit] . in this paper, in order to improve the security of a cr with swipt, a cooperative protocol and an artificial noise (an)-aided transmit strategy are proposed based on a practical non-linear eh model."
"clinical imaging consortium (mcic) data sets include many biology factors about schizophrenia patients like clinical characterization, functional magnetic resonance imaging (fmri), single nucleotide polymorphisms (snps) and so on in the on-line data repository [cit] . the data was dealt with cross-sectional learning to verify mensurable imaging genetic biomarkers of schizophrenia. enlist patients early in the course of their disease is mcic data's extraordinary preponderance, therefore, the mcic data sets involve a remarkable ratio of many people with schizophrenia researched in early stages of the clinical sign as same as a comparatively equal sampling of disease persistence via standing and diagnosed sickness. the method satisfied the crucial medical purposes of the multi-modality schedule, research of the kernel cognitive skill deficits affiliated to schizophrenia and their correlation to the clinical manifestation of the disorder [cit] . in this research, two modalities (fmri, snps) were acquired from 184 subjects consisting of 81 patients with schizophrenia and 103 healthy controls [cit] . the real samples include schizophrenia patients and healthy controls, which were supplied with an informed agreement. healthy participants were free of any medical, neurological or psychiatric illnesses and had no history of substance abuse. by the clinical interview of patients for dsm iv-tr disorders or the comprehensive assessment of symptoms and history, patients met criteria for dsm-iv-tr schizophrenia. volume 8, 2020"
"there are two techniques that enabled us to easily spot the described phenomenon, while it would have probably passed unnoticed or misunderstood with classical approaches. firstly, if measurements had been done in a commonly used sequential order, they would wrongly suggest poor performance for a specific subset of buffer sizes. randomizing order in which single measurements inside one large experiment are performed solves this issue. secondly, it was important to postpone the data aggregation and to log all the relevant information during the experimentation stage. by looking solely at mean bandwidth values and variance, which is a simplistic approach dominantly used in our community, the performance obtained with real-time policy appears worse, but the existence of two modes is completely hidden. figure 12 shows the result of four consecutive experiments on an arm snowball processor using exactly the same source code and inputs. the 42 repetitions for each memory size (on each plot) are represented by boxplots, demonstrating little measurement variability (thanks to careful control and optimization of the experiments). astonishingly, the performance drop, as the buffer size increases, occurs at different places depending on the experiment, just as if some \"external entity\" was deciding at the beginning of the experiment the kind of very stable performance one would observe. although the lower and higher values of buffer size always exhibit a similar behavior, the middle part (from 50% to 100% of the l1 cache size) is highly unpredictable. after a long investigation, we finally discovered that the source of this surprising phenomenon comes from the way the operating system allocates physical memory pages on arm processors. in general, operating systems allocate nonconsecutive 4 kb physical memory pages, choosing them randomly from a pool of available pages. the setassociativity of that generation for arm processors is only 4, and the l1 cache size is 32 kb. in such a scenario, without doing the appropriate page coloring, a bad choice regarding physical pages causes more cache misses, hence the drop of overall performance. during one experiment run, although we do malloc/free repeatedly on each buffer, the same pages gets reused. hence, the buffers actually start from the same physical memory location for each memory size during one experiment, which explains why there is no variability in the results despite the measurement randomization."
"skampi and conceptual feature a domain-specific language (dsl) to describe how experiments should be accomplished. while skampi focuses only on mpi, conceptual has a much broader set of backends, including mpi. both make it possible to very rapidly generate complex benchmarking programs with a few lines of dsl code. unlike the previous two tools, netgauge provides a way to explicitly output all the necessary parameters to instantiate the loggp [cit] and plogp [cit] models for a given machine. netgauge supports many communication protocols including infiniband, myrinet/gm, tcp/ip as the ethernet datagram protocol (edp) and the ethernet streaming protocol (esp), and mpi."
"we, in this paper, propose a clms algorithm that runs over the noisy received data and achieves the optimal estimate (23). the basic idea in our development is to construct an objective function whose gradient vector is identical to that of the cost (9) with zero regression noise [cit] . the following objective function satisfies this criterion:"
"for the past few years, many researchers have worked on exploiting different variants of canonical correlation the associate editor coordinating the review of this manuscript and approving it for publication was farid boussaid. analysis (cca) models and apply them to analyze genome-wide association problems [cit] . for examples, these problems include identifying correlation between single nucleotide polymorphisms (snps) and genes [cit], as well as correlation between dna copy number change and functional magnetic resonance imaging (fmri) [cit], and so on. volume 8, 2020 this work is licensed under a creative commons attribution 4.0 license. for more information, see http://creativecommons.org/licenses/by/4.0/"
"in this section, we present the methods used for real experiments, combining ideas from the analysis in the previous section with additional techniques to handle the errors in the dna-based storage system. we add an error-protected addressing index to each oligonucleotide in order to determine its position in the encoded data. to resolve the issue of missing oligonucleotides and substitution errors, we use a large block length ldpc code [cit] . since the synthesis process also introduces insertions and deletions, we use synchronization markers in each oligonucleotide. we next describe the encoding and decoding scheme in the proposed framework (shown in figure 5 ). the impact of some of these elements on the performance are discussed in section 4.1."
"sets of the top k documents retrieved by s for texts p and q, respectively. let us define s p (d) and s q (d) the scores assigned by s to a document d for the query p and q, respectively. then, the similarity score is calculated as:"
"trends. it is clear from this graph that there is an enormous experimental noise for every buffer size. furthermore, even when ignoring the variability and focusing solely on mean values, the influence of the stride is ambiguous and bandwidth does not decrease by a factor of two as one could expect. these results are far from what was expected and they are very different from the ones presented in figure 7 . we thoroughly investigated the reasons behind this behavior and can now report several evaluation pitfalls. to illustrate different phenomena on the simplest cases, the rest of the results and figures in this section are limited to the measurements conducted with the stride 1."
"the experimental design deals with the factors that drive the system behavior under evaluation. randomization plays a central role: each factors' values and the order in which each factor combination is measured should be properly randomized. this guarantees that the presence of temporal anomalies in the setup remains independent of the factors' values. uncertainty is evaluated and reduced through replicated measurement of each factor combination. during the second step, the order on which measurements are taken must be dictated by the experimental design. thus, the benchmark engine reads each factor combination from its input, conducts the measurement on the target platform, and reports the details of every individual measurement in one or multiple output files, along with a lot of meta-data about the measurements and the environment (machine information, operating system and compiler versions, compilation command, benchmark parameters, network configuration, etc.). beyond increasing the chances for reproducing the experiments, these meta-data support better results interpretation. this is especially useful when comparing two experimental campaigns that have similar inputs and completely different outputs. a statistical analysis is carried out during the third stage of our methodology, after the experiment campaign execution has finished. we avoid doing any on-the-fly aggregation and keep all information, delaying the analysis, in order to spot the outliers and strange behaviors, instead of losing them."
"x gradually. these natures make s better-adapted for minbatch optimization with a special method which makes sure the model does not reach steady state early during optimization. in order to better conform to the datasets and prevent overfitting, on the one hand, l 2 penalty term is applied on w p and w q to increase the difference between each component of the weight vector, on the other hand, dropout is applied to the hidden layers in each training iteration, which is randomly omitted from the network with a certain probability, and in this way the hidden units do not rely on other hidden units to change their states."
"another issue regards the input message sizes used in the experiments, as they are frequently biased. using messages in powers of 2 may miss the real behavior of the network software stack. some values, such as 1024 for instance, may have special behavior coded into the network layers that are nonlinear when compared with close values directly smaller or larger than that one. such small changes in the data sizes might pass undetected by statistical analysis that is conducted without supervision. on benchmarks that use linear increments, such as netgauge and looggp, the bias issue is still present because the measurements depend on the selected starting message size and the increment value."
"such behavior seemed quite regular and sound so we envisioned to directly use the multimaps benchmark for characterizing memory capabilities. our goal was to improve the simgrid [cit] processor model using an approach similar to the one of the pmac framework [cit] . however, when trying to reproduce this approach with recent hardware, several unexpected difficulties forced us to change our initial plan."
"this is a per-class overlap measure (in this way, \"france\" as an organization does not match \"france\" as a location) calculated using the dice coefficient between the sets of nes found, respectively, in sentences p and q."
as f (f k (x ))f (f k (x )) and g(f h (y ))g(f h (y )) may be singular but they are essential when calculating the compu-
"the simplest network characterization strategy consists in measuring the time it takes to transfer a given amount of data between two endpoints. measurement variability should also be assessed since network stack interactions are complex. figure 2 presents this prevailing approach used by many tools (described in section ii). increasing the data size in powers of 2, the benchmarks measure n times the same experimental configuration. in this example, a simple send/recv is placed to illustrate the principle although more complex operations may be also used (e.g., non-blocking, one-sided, collective). the measured operation can also be a more complex pattern involving several message exchanges, as in the loggp [cit], the looggp [cit], and the plogp [cit] benchmarks that directly output model parameters. most strategies employ per message size measurements to calculate statistics (bandwidth, latency, etc.) in an online fashion. if piecewise linear modeling is investigated, the behavioral breaks are automatically detected during the experiment, using linear extrapolations from the already measured points or outlier definition schemes. at the end of the main loop, tools report the aggregated results per operation and per data size in textual or csv file format for external exploitation. the automatic detection of protocol changes depending on the message size has been the object of several strategies. netgauge [cit], plogp [cit], and looggp [cit], for instance, provide good examples. when linearly increasing the message size, and for every new measurement, netgauge checks for protocol changes by using the mean least squares deviation (lsq) between the previous point that started a new slope and the latest measurement. if the lsq has changed more than a factor defined by the analyst, netgauge waits for five new measurements before confirming the protocol change. this technique avoids that \"anomalous\" measurements mislead the detection of true protocol changes. in plogp, at every new measurement when increasing the message size in powers of 2, the implementation extrapolates the previous two measurements and checks if the difference between the new measurement and the linear extrapolation is within an acceptable range. if that is not the case, a new measurement is undertaken with a message whose size is the mid-value between the latest two measurements. this is repeated, halving the intervals, until the extrapolation is matched by measurements or a maximum number of attempts is attained. the looggp linearly increases the message sizes, using the same approach as netgauge, but adopts an offline analysis with user intervention. after removing outliers, a local neighborhood of a configurable extent is defined for each measurement. if a measurement has a maximum value in a neighborhood, it is considered as a protocol change. it is up to the analyst, using plots for t o (s) (overhead) and t g (s) (gap), to finally decide if detected points are real protocol changes or not. despite the analyst's mediation, authors state that the mechanism is sensitive to the neighborhood size and the message size steps during the measurement stage."
"previous studies assumed that the absolute value of the bandwidth depends mostly on the processor and memory bus frequencies, as the compiler will automatically optimize the corresponding simplistic kernel. nevertheless, there are some program optimizations that can have a significant influence on the bandwidth values, surprisingly sometimes even negative."
"the introduced measures were studied on the spanish subtask, observing a limited contribution from geographic context similarity and spectral distance. [cit] english task, even when trained on a training set derived from automatic translation, which include many errors. our participation in the english subtask was inconclusive due to the technical faults experienced to produce our results. we will nevertheless take into account the lessons learned in this participation for future ones."
"careful tuning, which is extremely hard without prior knowledge of the benchmark and the machine it is running on. we anticipated that we would have to modify many parts of the benchmark to understand more deeply the performance on modern architectures. a second issue was that the output of the benchmark is very verbose and it is unclear how the final plot is obtained from the measured data. finally, our initial results were far from the expected bandwidth peaks, which was surprising as we thought that achieving the maximal performance should be relatively easy with such simple programs. unfortunately, the benchmark only reported aggregated values, without raw data to allow us to better understand where the problem could come from."
"where g k (i) are scalar equalization coefficients. assuming negligible channel estimation error, these coefficients can be obtained, in practice by, e.g., the least squares (ls) method:"
"by a large margin. it's worth noting that dpcae should particularly have a superiority when pca technology is used to better fitting multi-modalities data, two deep networks are used to extract features of hidden units, two back propagation neural networks are used to extract top-level features representation and fine-tune the entire deep belief network in each modality respectively. the results indicate that dpcae model can indeed detect more correlation than cca-based models and other dnn-based models (the result is given in fig. 9 )."
"to test the performance of the proposed algorithm, we performed nine experiments with different parameters over a period of five months. the synthesis was done by customarray (http://www.customarrayinc.com/). the first two experiments were done on separate 12k oligonucleotide pools, while the remaining seven experiments were done in a single 90k pool. the 150 length oligonucleotides consisted of primers of length 25 on either end, which are used for pcr amplification and can also be used for random access. these were then sequenced with illumina iseq technology (considering only the first read in a read pair). the detailed experimental procedure is described in supplementary material section 4. before running the decoder, we used flexbar [cit] to remove the primers and detect reverse complemented reads. we encoded a variety of files in these experiments, including random files, an image file (figure 7) and texts such as the un declaration of human rights, darwin's origin of species and feynman's speech \"there's plenty of room at the bottom\" [cit] . to avoid long repeats/homopolymers in the oligonucleotides, the files were randomized using compression and encryption. details about the experimental parameters and the encoded files are available in supplementary material section 3. the code is available at https://github.com/ shubhamchandak94/ldpc_dna_storage. table 1 shows the results for selected experiments. the remaining experiments explored the impact of secondary parameters such as the bch code and synchronization marker, and are described in the supplementary material. for each experiment, we randomly subsample the reads and find the minimum number of reads for which decoding succeeds in 20 out of 20 trials (multiple trials conducted to ensure robustness -also see section 4.1.5). the last two columns show the writing cost (bases synthesized/information bit) and reading cost (bases sequenced/information bit). since a base can represent at most 2 bits, both of these quantities are lower bounded by 0.5. as discussed in section 2, reading cost represents the actual cost of sequencing more accurately than coverage (bases sequenced/bases synthesized), especially when comparing across different writing costs. table 1 also reports the error rates and the normalized 6 coverage variance for the experiments (see section 4.1.1 for details). the error rates vary slightly across the experiments, with deletions and substitutions being the most common forms of errors. the normalized coverage variance represents the deviation from poisson random sampling, with 1 being ideal and higher values representing more variation in the coverage of oligonucleotides. the coverage variance is usually due to a combination of synthesis bias and pcr bias, and can depend on the specifics of the synthesis process [cit] . the first two experiments were synthesized separately from the rest and have lower coverage variance. we also note that the first two experiments were conducted before the conception of the synchronization marker idea and thus have slightly different parameters, but we still include the results since they provide valuable insights into the impact of coverage variance and error rates on the performance of the scheme."
"where p(x tr, y tr ) is the joint distribution of two groups of data x tr, y tr after training, p(x tr ) and p(y tr ) are probability distribution of x tr and y tr respectively, h (x tr ) and h (y tr ) are information entropy of x tr and y tr respectively, while i (x tr, y tr ) is relative entropy of p(x tr, y tr ) and p(x tr )p(y tr ). according to the training data of different models, the classification error rate of the datasets was calculated:"
"the resulting combinations of operation type and sizes, one per line, are registered in a text file that is provided to the measurement engine (the code is available in the simgrid's platform calibration repository). this engine is a simple mpi program that registers the time it took to run the particular operation on the target network for all the message sizes indicated in the output. finally, in the third step of our methodology, a script written in the r language [cit] carries out a supervised analysis. the breakpoints are manually provided by the analyst and a piecewise linear regression is calculated for each of the three operations. the send and receive software overhead are measured using the blocking receive and the asynchronous send, latency and bandwidth are obtained using the ping-pong measurements. plots are generated so a human can check the linearity assumption, if the breakpoints are coherent, and the outcome of the regressions. despite being manual, this procedure guarantees that results are meaningful from an experienced analyst point of view."
"in this paper, a single cast link was considered. in order to improve spectrum efficiency, a multi-cast scenario or a nonorthogonal multiple access cr will be focused. we will study robust an-aided beamforming schemes for these networks to achieve secure communications and obtain a high spectrum efficiency."
"s.t. the corresponding constraint is consistent with (5) where γ is trade-off parameter. as shown in (6), dccae finds two regularized deep neural network representation f (x ), g(y ) and two reconstruction networks representations p(f (x i ))q(g(y i )). compared to cca and dcca, dccae further extracts features for better data fitting. this model considered the best consequence of multi-modality data correlation in a practical manner and stochastic gradient was applied to optimize the objective function."
"since it is not possible for a user program to fully take control of the page allocation strategy, we applied an alternative memory allocation technique for the buffer to better assess this phenomenon. in the initial algorithm, we used an individual malloc function call for each buffer size, running the experiment, and finally freeing the memory at the end of the loop. in the subsequent versions, we decided to do only one memory allocation at the beginning of our program for all the measurements. we allocated one large memory block (e.g., 2 mb) that is significantly larger than our maximum buffer memory size (50 kb in these experiments). after that, for each memory size and repetition, we randomly chose the starting point inside our allocated memory to consider it as buffer for the experiment. this way we managed to correctly evaluate the impact of different physical memory pages during one experiment run, avoiding to always use the same pages. this physical address randomization allowed us to obtain completely reproducible (although more variable internally) experiments, yielding always similar bandwidth values (typically, the top part of the left graph in figure 11 )."
"where fp is the number of trained datasets that are actually negative samples but are divided into positive samples by the classifier; fn is the number of trained datasets that are actually positive samples but are divided into negative samples by the classifier; and p + n is the total number of samples of trained datasets. the better nmi of clustering (92.1% for snps data, over 93.8% for fmri data) indicates that different subjects (e.g., schizophrenia patients and healthy controls). while projections by distae model perform somewhat better segmentation for some clusters, worse than other dnn-based models. for another, cca-based models can approximate the same variable, but the part of separate classes is failed, probably because the original data are too complex to be better applied by linear mappings. overall, dpcae shows the best result of nmi of clustering and cleanest embedding between snps data and fmri data using different methods. the model expects that an easy linear classifier can acquire better accuracy on dpcae projections. then, the linear svm is trained on the projected dataset (now using two-modality sample labels), and applying the projected tuning set to select the svm hyper-parameters [cit] . error rates (including error rates of snps data and fmri data, and abbreviated form of them are s-error and f-error) on the optimal classification of each model are given in table 1 . when the model we used gets the maximal value of nmi, the value of classification error rate of the model is the largest. in generally, the smaller the classification error rate, the higher the classification accuracy, so these error rates consistent with the clustering results. the result of dcl is very close to dpcae in classification accuracy and normalization of normalized mutual information, which may be due to the incorporation of phenotype information. but comprehensive indicators, deep principal correlated auto-encoders makes classification accuracy much higher than other methods, the result is sufficient to prove the correctness of the classification results of the two modal data. take the place of employing a hard nonlinear classifier on the feature representations of high-dimension, a very ordinary linear classifier that can be educated effectively on low-dimensional projections already acquires the best result of accuracy."
"section 2 motivates the proposed approach using a simplified theoretical model. building upon this, we present the complete encoding and decoding algorithms in section 3. in section 4, we present the results for real data (data obtained from synthesis and sequencing experiments) and also discuss the impact of various parameters and synthesis/sequencing non-idealities using this data as well as simulations."
"distorted data. we demonstrate the presence of a bias in the adaptive parameter estimates, consequence of the link noise, and introduce a bias-elimination scheme that also significantly decreases the steady-state mean-square error (mse) of the network. beside the application of our proposed bias-removal technique in centralized networks, it can be also used in networks with a distributed hierarchical structure, where the cluster heads act as fusion centers and distributed processing is performed at a cluster level."
"in order to acquire more efficient computing efficiency, the experiment just uses the first derivative information and employs the conjugate gradient descent method to solve the optimization problem [cit] . multi-layer belief networks algorithm which is made up of rbm and back-propagation algorithm is used to pass the conjugate gradient backward to each layer of the network during each iteration process. in addition, the regularization technique is employed to avoid over-fitting. here, we must compute the gradient of dpcae's formulation (7) for using conjugate gradient descent method and bp."
"in recent years, the amount of data being generated and stored is increasing at rapid rates. as a result of the impending data crisis where generation exceeds reasonable storage capacity, there has been significant interest in exploring alternatives to solid state disks and magnetic tapes as data storage media. interestingly, the cost of dna sequencing has been decreasing exponentially in the past ten years. dna is a robust method of storing information as demonstrated by every living organism and offers exceptionally high storage densities (100s of petabytes per gram [cit] ) and long-term durability (1000s of years [cit] ). the longevity of dna-based storage makes it ideal as an archival medium to store the knowledge gained by humanity over the millennia. along with high density data storage, dna-based storage systems allow efficient duplication of data and random access using pcr-based techniques [cit] ."
"a power minimization problem was formulated in a miso cr network based on a practical non-linear eh model. to guarantee the security of both the primary network and the secondary network, a cooperative jamming scheme was proposed. a suboptimal robust an-aided beamforming scheme was designed under the bounded csi error model. it was shown that the non-linear eh model may be provide better performance compared with the linear eh model. it was also shown that our proposed cooperation scheme can obtain a performance gain compare with the scheme without cooperation between the primary network and the secondary network."
"where tp is the number of datasets correctly classified as positive samples after training of each modality, while fp is the number of modal data wrongly classified as positive samples after training of each modality. learning based deep network, dcca and dccae performed better than cca-based models and minimum-distance auto-encoders but worse than dpcae in the aspect of classification, which may be due to the incorporation of dnn outputs and regularization parameters for assessment of example variance. while minimum-distance auto-encoders performed worse than dcca and dccae, which may result from weakness of the average discrepancy term between projected example pairs."
"the fmri dataset which used in the experiments was collected by fmri scanning, all people must early take part in a ''mock scanner'' task for adapting to the sensor motor setting. in the meantime, there are four fmri tasks need all subjects to complete until they adapt themselves to the session and the sensor motor environment, for instance, a sensory-motor task (sm), a breath-hold task, an auditory oddball (aod) and sternberg item recognition paradigm (sirp) [cit] . for the four fmri tasks, every part used a specific functional input equipment with equal terms for button presses to collect subjects' reaction, then all people were requested to respond to a suitable signal by pressing a button, at last, researchers connect the input equipment and specialized e-prime software to provide visual stimuli for subjects and collect subjects' reaction. it is worth mention-ing that researchers collected four runs of sirp task of fmri tasks each contained 177 time frames for five minutes and fifty-four seconds per run, four runs of the aod task of four tasks each contained 96 time frames for three minutes and twelve seconds per run, one run of breath-hold task and two runs of sm task each contained 120 time frames for four minutes per run."
"let us consider two texts p and q, an ir system s and a document collection d indexed by s. this measure is based on the assumption that p and q are similar if the documents retrieved by s for the two texts, used as input queries, are ranked similarly."
the rest of the paper is organized as follows. section ii introduces the several existing state-of-the-art methods and explain the reasons and advantages of our model. the collection and application of image gene data can be found in section iii. detailed discussions and analysis of the results were in section iv. the conclusions of the work and future direction of the work were in section v.
ii. problem formulation consider a network of n randomly distributed sensor nodes that are deployed over a geographical area to estimate an unknown parameter vector
"as seen from (22), the mean of the weight error vector of the clms algorithm in (14), i.e., e[w i ], converges to a nonzero vector. in what follows, we propose a bias-compensation scheme and develop a new form of centralized lms algorithm whose mean estimation error converges to zero."
"several tools enable the measurement of network parameters (latency, gap, and software overhead). the pallas mpi benchmarks (pmb) suite [cit], skampi [cit], conceptual [cit], netgauge [cit], and confidence [cit] all demonstrate unique qualities in such measurement task. the pmb suite provides a framework to measure a subset of mpi operations and is detached from a performance model. it characterizes network performance and helps identifying potential problems and improvements. pmb only reports mean values for each requested message size and number of repetitions."
"where (10) is equivalent to (7) . linear cca and linear sparse cca models seem to have the maximal correlation with optimal projection vectors u, v, as given in (2) and (3) . nevertheless, the main reason for reviewing the models of deep cca and dccae in this paper, is to analysis limitations of traditional cca which cannot obtain complex nonlinear relations. compared to general model and structure of deep neural network, our model joint deep belief network based on several rbms and linear dimension reduction method, the goal is not only to speed up machine learning, but also to discard dimensions that carry less information. moreover, differ from nonlinear deep canonically correlated auto-encoders, dpcae adjusts the network's in-layer parameters by using multi-layer belief network, and fine-tunes all network parameters from top to bottom layer by bp network. it acquire the best nonlinear correlation and data fitting in multi-modality data. in particular, the model of dpcae further optimize significant feature information and parameter information, it can generate an excellent presentation of multi-modality data relationship in an available way."
"like cca-based models need at lowest two modality data as input, here data-pair combination of fmri and snps data are used. for this data combination, we experimented with the property of each model, and the results of the experiment were given in fig. 7 . from this figure, the proposed method, deep principal correlated auto-encoders, acquired better classification accuracies than both cca-based models and other dnn-based models for classifying different subjects using mcic dataset separately. the classification accuracy is evaluated by comparing the trained datasets with the real labels. this classification method can observe the accuracy of datasets through the box diagram, and the calculation formula for the classification accuracy is shown in (15) ."
"the substitution error rate should be set according to the error rate after consensus, and was set to 4% for our experiments. this is higher than typical substitution error rates from synthesis/sequencing to handle cases like one insertion and one deletion in a single read leading to multiple erroneous bits."
"decoding the corresponding reads takes 56s and uses 190 mb of ram. most of the resources are consumed by the single-thread implementation of ldpc encoding and decoding, which can be efficiently implemented in hardware as is done for communication applications. since the code works in blocks, the time consumption is linear in the file size and the memory consumption of the ldpc coding is constant. therefore, the proposed scheme is scalable to large files."
"the function d(x, x max ) determines the minimum distance between a n-gram x and the heaviest one x max as the number of words between them."
the network can now seek the unknown parameter vector w o using the pre-processed data by minimizing: 1 this assumption is true when the time interval between successive iterations of the adaptive process is larger than the coherence time of the channels.j
"section ii presents some related work on performance prediction through simulation that motivated our investigation along with classical underlying network and memory models. this allows the reader to understand the rationale behind most benchmarks' structure. sections iii and iv present some performance characterization of both the network layer and memory hierarchy, illustrating the many pitfalls one can stumble upon when pursuing such task. section v presents the measurement and analysis methodology we used and which we believe to be more appropriate when characterizing the performance of modern hpc architectures for model instantiation purposes."
"as shown in (22), the estimation bias b is due to the term t f, which in turn is caused by the regression noise component v (u) k,i . assuming zero regression noise in (9), the unbiased optimal estimate of the network will be:"
"one may argue that such behavior can be avoided if the user takes full control of the processor frequency. however, this requires some programming effort and an expertise in the performance of every part of the application. every operating system might have a different mechanism for controlling the frequency, with varying latency for changes to take effect. moreover, in some cases, frequency changes require superuser rights that often are unavailable on production platforms. finally, the ondemand governor has many advantages and most users want to keep part of its benefits, even though its behavior can sometimes be unpredictable. 3) impact of operating system scheduler: despite the simplicity of the single-threaded benchmark, and the exclusive access to the platform while performing experiments, inevitably some external influence still exists. primarily, the operating system with its own processes runs on the processor. to minimize its influence, we removed all the unnecessary os services. additionally, the core on which our program is executed was strictly pinned, avoiding potential thread migrations during the execution."
"to incorporate the effects of fading and path-loss into the above results, we can use expressions (7) and (8) and repeat the analysis. doing so, we obtain similar results as before, except that in this case the power of the communication noises v"
"the model applies a 5-fold cross-validation pattern to select the weight-decay parameters. the total samples are grouped into 5 subgroups, and one subgroup is selected as testing sample and the remaining 4 subgroups are used as training sample in each stage. a calculated score is determined by the difference value between the correlation of training sample and that of the test sample, which is applied to assess the property of selecting the weight-decay parameters. it is very vulnerable when utilizing cross-validation to pick out weight-decay parameters µ p, µ q, which is immediately applied as the threshold value when updating loading vectors. the vulnerability is due to the non-uniform distribution of loading vector values."
"our past experience in evaluation of lightweight thread libraries had taught us that better and more stable performances can sometimes be obtained by using the real-time scheduling policy of the os. however, this proved to be the opposite when running experiments on an arm snowball processor with unix-like linaro os. the left plot of the figure 11 shows that there are two modes of execution. the first mode, the one with higher bandwidth values, is similar to the results we have obtained with other scheduling priorities. on the other hand, the second mode has the bandwidth values that are almost 5 times lower and they occur in approximately 20-25% of the measurements. there are 42 repetitions for each buffer size and the order in which single runs are executed is completely randomized. approximately the same number of the second mode executions is present for all buffer sizes. the right plot of the figure 11 shows exactly the same data as the left plot. the difference is that the x-axis now represents the order in which each measurement inside one experiment were conducted. the right plot of figure 11 indicates that the whole second mode occurred throughout a single period of time during the whole execution of the experiment. when repeating experiments with different input configurations, the same phenomena was regularly reproduced. this suggests that the second mode is almost certainly caused by an external process running in parallel and which is occasionally scheduled to the same core when the real-time policy is activated."
"a figure showing the correlation analysis of combining fmri data and snps data by sparse canonical correlation analysis algorithm. in this frame, two sets of data include samples size n and different numbers of features p, q by using the penalty to reduce the dimension. then, loading vectors u, v are used to get the canonical variable, and perform correlation analysis to obtain the maximal correlation."
"in this article we explain how opaque benchmarks typically work and explain how they should instead be organized to provide better information for the instantiation of performance models. we motivate this claim by reporting many pitfalls we encountered when we tried to reproduce earlier strategies for performance characterization of recent hardware. the encountered pitfalls include the impact of compiler optimizations, the dvfs policy, the operating system scheduler, and peculiarities of the machine architecture. our analysis relies on simple scripts, the r language [cit], and the org-mode's literate programming approach [cit] . our activity is registered in a reproducible research spirit so that although some of these experiments were done five years ago, we are still able to faithfully exploit them."
"the dpcae's experiment on genetic variants and brain imaging phenotypes focused on two works: classification (to classify different subjects using mcic dataset), and correlation analysis (to explore the correlation between snps loci and brain imaging data). the classification verifies the classification capability of the dpcae algorithm, while correlation analysis verifies the performance of the dpcae algorithm in terms of finding the maximal correlation. all hyper-parameters, including energy function, activation function type, regularization parameters, learning rate, trade-off parameters, maximum epochs, types of deep networks, the number of layers, and the total number of nodes in different network layers, were selected using conjugate gradient descent [cit] . the model is proposed by us and other advanced models can be divided into two categories generally, one kind is dnn-based models, which can detect non-linear relationship, including dcca, dccae, distae and dpcae, another cca-based models, corresponding to dnn-based models with only a linear network with no hidden layers, including cca, sparse cca, sparse mcca. we developed these programs using tensorflow as the deep learning framework. these programs were run under the tulane university high performance computing system, code named ''cypress'' with an operating system of linux. it is a 124node cluster, with each node providing dual 10-core 2.8 ghz intel xeon e5-2680 v2 cpus, 64 gb of ram, and dual xeon phi 7120p coprocessors."
"1. inner/outer code separation: here we first segment the data, apply an erasure-correcting outer code to the segments and then apply an errorcorrecting/detecting inner code to each segment. during decoding, we first collect the reads corresponding to the same index and take a majority vote at each position. then we apply the inner code decoding on each segment followed by the outer code decoding to obtain the decoded data. since l is generally small (a few hundred bits), this strategy suffers from the fundamental limits on short block length codes [cit] for the inner code. on the other hand, near-optimal erasure-correcting outer codes such as raptorq codes [cit] are available since n can be large."
"in order to verify the performance gain obtained by using our proposed cooperation scheme, fig. 5 is given to show the minimum transmission power versus the number of antennas volume 5, 2017 of the cbs achieved under the non-linear eh model and the linear eh model with or without cooperation between the primary network and the secondary network. the number of ehrs in the primary network is set to be 3. it is also seen that our proposed cooperation scheme can obtain a performance gain. as also shown in fig. 5, the minimum transmission power decreases with the increase of antennas of the cbs."
"in this paper, we presented our attempt to reproduce the approach of previous work in the context of network and memory modeling. the model instantiation generally builds on opaque benchmarks that can lead to many pitfalls. the investigated phenomena are so complex that simplistic approaches can lead to severely biased measurements that make simulation predictions unreliable. we explain how such biases may be avoided through relatively simple precautions. thorough randomization is an essential ingredient but should also be allied to a white-box approach with a clear separation of concerns (experiment design, experiment engine, result analysis), a careful documentation, and a capture of the environment."
"peng peng received the b.s. [cit], and the m.s. [cit] . he is currently pursuing the ph.d. degree in traffic information engineering and control with the department of electronics and control engineering, chang'an university, xi'an. his research interests include machine learning, pattern recognition, data mining, and the correlation analysis of imaging genetic data. vince d. calhoun is currently the president of the mind research network and a distinguished professor with the department of electrical and computer engineering, university of new mexico. he has authored more than 550 full journal articles and more than 650 technical reports, abstracts and conference proceedings. his works include the development of flexible methods to analyze functional magnetic resonance imaging such as independent component analysis (ica), data fusion of multimodality imaging and genetics data, neuroinformatics, and the identification of biomarkers for disease. among other things, he leads an nih p20 cobre center grant on multimodality imaging of schizophrenia, bipolar disorder, and major depression and an nsf epscor grant focused on brain imaging and epigenetics of adolescent development. he is a fellow of the institute of electrical and electronic engineers, the american association for the advancement of science, the american institute of biomedical and medical engineers, the american college of neuropsychopharmacology, and the international society of magnetic resonance in medicine."
"it is envisioned that the integration of energy harvesting techniques into cr can simultaneously improve the energy efficiency and the spectrum efficiency [cit] . this integration has been identified as a promising candidate for the future wireless communication systems. however, due to the broadcasting nature of cr and the dual function of radio frequency signals, cr with energy harvesting is vulnerable to be eavesdropped [cit] . thus, the security of cr with energy harvesting is of great importance."
"in a near future we plan to work on automating and combining various tools we have built to instantiate hpc network models while keeping the same white box and randomization methodology. one of the challenges will be related to the production of a coherent and easily understandable reports over a complex set of measurements, and allowing to reliably characterize a whole cluster."
"it can be verified that the hessian of j(w) is positive definite and hence this cost function is strongly convex. from (24), we then arrive at the following bias-compensated clms (bc-clms) algorithm whereμ is the new step-size:"
"recently, it has become prohibitive to constantly keep the processor frequency as high as possible, due to the extensive energy consumption. indeed, an increasing number of applications where lowering the processor frequency can lead to significant energy savings without affecting the overall execution time has been reported. therefore, to decrease the energy consumption, many processors nowadays operate with ondemand cpu frequency governor. this policy enables the operating system to scale the frequency up or down by choosing the most appropriate one from several possible values (modes). these changes may even occur during the application run if specific parts of the code have different computational needs. alas, from the user's perspective, it is very hard to know exactly how this mechanism works. it is, however, generally known that high frequencies are more effective for computation intensive code blocks, while low frequencies are preferred for memory-bound regions. nevertheless, there are also borderline cases which are hard to optimize and for which frequency prediction is extremely challenging."
"many network performance models [cit] have been developed over the last 30 years. memory performance modeling, on the other hand, is much more recent, especially in the context of multi-core machines. general approach proposed in the pmac framework [cit] to predict performances of hpc applications: application and machine signatures are convolved through a network and memory model."
"if we had used the second allocation technique from the start and concentrated only on maximum/median/mean values of bandwidths, we would have observed very regular behavior with a clean drop on l1 cache size. but we would have missed this physical memory allocation phenomenon, ultimately leading to incorrect model instantiating."
"roofline estimations [cit] are the simplest way to estimate memory access performance. the principle of such benchmarks is to saturate the memory utilization, effectively defining the peak access rate (gb/s). the performance modeling and characterization (pmac) framework [cit] relies on the multimaps benchmark to measure the memory bandwidth for different data sizes, strides and line sizes. by changing the data sizes, this benchmark captures different characteristics of the memory cache levels. pchase [cit] also assesses memory latency and bandwidth on multi-socket multi-core systems, captures the interference between cpus and cores when accessing memory, and ultimately provides a richer model."
"the system being measured may suffer temporal perturbations which may mislead completely the result interpretation. perturbations can be natural or caused by external activity in a poorly isolated system. repeating the benchmark would probably lead to divergent estimations. without a manual check, the measurements could be filtered out as outliers, or affect the online detection of protocol changes. an anomaly could pass as a break point by the heuristics implemented in netgauge and plogp, for instance. concerning results, there is no guarantee that the reported breaks are actually meaningful."
"are all mutually independent. we use r o k to denote the freespace transmission range of node k, as obtained using friis formula for a the given power budget, antenna gains and carrier frequency [cit] . furthermore, we let ς o k represent the threshold signal-to-noise ratio (snr) of the received signal from node k, defined as the snr of the received signal over a non-fading link with communication range"
"our initial objective was to build upon the multimaps benchmark. this benchmark is an upgraded version of the maps benchmark, which itself is derived from stream [cit] . figure 6 presents the pseudo-code of the main algorithm behind multimaps. it measures the time to execute the for loop in which it makes consecutive memory accesses (by stride) to an array of elements. at the end, it computes the memory bandwidth. two factors are expected to affect such bandwidth estimation: the buffer size and the access stride. together, they somehow capture the temporal and spatial locality behavior of the memory hierarchy. figure 5 . technical characteristics of the cpus used in this study. figure 7 shows typical multimaps results, in this case for strides 2, 4, and 8 on an opteron machine (see figure 5 for more details). as the size of the buffer increases, the memory bandwidth decreases. three plateaus directly correspond to the l1 cache, l2 cache, and main memory sizes. strides have no impact when all accesses are done inside l1. however, they play an important role when the array size no longer fits in l1, since bandwidth is almost reduced by a factor 2."
"for most of this work, we have reported the reading cost at which 20 out of 20 sampling trials were successful. this was done for the sake of comparisons and due to computational constraints. in this section, we study the probability of decoding failure as the reading cost is varied. we performed realistic simulations with a single ldpc block (32 kb) and computed the fraction of unsuccessful trials at each reading cost (simulation error model as in section 4.1.3). figure 11 shows the plots for three ldpc redundancies. we see that the failure probability falls rapidly from 1 after the reading cost exceeds a threshold, where this threshold is lower for codes with higher redundancy/writing cost. this is consistent with the typical behavior seen in coding theory [cit] and suggests that the scheme offers high reliability once the reading cost exceeds the threshold value. figure 11 : probability of decoding failure vs. reading cost for realistic simulations for three ldpc redundancies. * denotes that all of the 10,000 trials were successful, suggesting that the failure probability is below the marker shown."
"the internet of things (iot) has been recognized as a promising technique for promoting the development of the smart city [cit] . it consists of large numbers of sensors, which are powered by batteries that only have limited energy storage capacity. the limited energy storage capacity cannot perpetuate the lifetime of iot. fortunately, energy harvesting (eh) techniques have been proposed to provide prospective schemes for addressing this issue [cit] . particularly, radio frequency signals are exploited as sources for powering the energy-limited devices. there are two different research lines on energy harvesting. one is the investigation on the wireless powered communication networks (wpcn) [cit] . in wpcn, a so-called harvest-then-transmit protocol is adopted. the other is the focus on simultaneous wireless information and power transfer (swipt) [cit] . the energy and information can be simultaneously transmitted by using swipt techniques."
"traditional cca was widely used for linear correlation, but the number of high-dimensional features are considerably higher than that of real samples in the real datasets. for this reason, a lot of segmentation forms of penalized cca algorithm [cit] were introduced to solve the confronting problem, when applying the datasets with high-dimensional features but small-size samples. they performed different sparse penalty functions on the canonical vectors u and v, in order to acquire some meaningful sparse vectors. the form of sparse cca is proposed by, which is given by (3), the corresponding schematic illustration is provided in fig.2 ."
"the main factors to consider when instantiating a point-topoint communication model are message size and synchronization mode. to characterize these behavior, we relied on three kinds of operations: blocking receive, asynchronous send, and ping-pong. the send and receive software overhead is calculated through a blocking receive and an asynchronous send, respectively. the benchmark guarantees that the message has already arrived in the receiver when the receive operation is called. in both cases, the elapsed cpu time captures the overhead of buffering and sending data to the network card. the network latency and bandwidth are calculated using the results of the ping-pong operation. these three network-related operations are sufficient to calculate all the parameters for any logp-based model."
"1) an an-aided cooperative scheme is proposed to improve the security of both the primary network and the secondary network. in this scheme, the primary network allows the secondary network to access its spectrum band. and in return, the cognitive base station (cbs) transmits a jamming signal to improve the security of the primary network. 2) considering a practical non-linear eh model, a power minimization problem is formulated in miso cr with swipt where a bounded csi error model is applied. the problem is nonconvex and challenging. a suboptimal resource allocation scheme is designed by using auxiliary variables and semidefinite program (sdp) methods. 3) simulation results show that the performance obtained under the practical non-linear eh model may be better than that achieved under the linear eh model. it is also shown that the required transmit power increases with the csi error. moreover, simulation results demonstrate that a performance gain can be obtained by using our proposed cooperation scheme compared with that scheme without cooperation between the primary network and the secondary network. the remainder of this paper is organized as follows. section ii presents the system model. a robust resource allocation problem is formulated in section iii. section iv presents simulation results. the paper concludes with section v."
"this measure has been introduced in order to measure similarities between concepts with respect to an ontology. the similarity is calculated as follows: first of all, words in sentences p and q are lemmatised and mapped to the related wordnet synsets. all noun synsets are put into the set of synsets associated to the sentence, c p and c q, respectively. if the synsets are in one of the other pos categories (verb, adjective, adverb) we look for their derivationally related forms in order to find a related noun synset: if there exists one, we put this synset in c p (or c q ). no disambiguation process is carried out, so we take all possible meanings into account."
"complex and undisclosed hardware architectures, with usertransparent memory access, make it difficult to conceive a proper model because it is hard to obtain direct and faithful observations. as follows, we briefly discuss the efforts regarding network and memory performance characterization."
"schematic for simplified storage model. here l denotes the length of synthesized sequences, cw denotes the writing cost, cr denotes the reading cost and denotes the error rate. the index of each sequence is shown to its left, and the decoder has direct access to the index in the simplified model."
"the second optimization is related to the loop unrolling. this code transformation attempts to minimize branch penalty by unwinding the code inside the loop. usually, this reduces the execution time, but with a larger program binary size. common sense implies that automatic loop unrolling present in many compilers always leads to performance gains. however, as shown in figure 9, even for the simple code of figure 6, manually unrolling the loop proves to be very beneficial for performance on a recent intel core i7-2600 (see figure 5 for more details). figure 9 shows how increasing element type from 4 b int to 8 b long long int essentially doubles the bandwidth. the same trend, only a bit mitigated, continues with larger elements. loop unrolling also has a positive effect, as the bandwidth increases in all cases except one. for the larger vector size (four float64 elements) with loop unrolling, instead of the expected highest values, the actual results are extremely low. we did not fully investigate the reasons behind this anomaly, as such understanding is secondary to this work, and we were constrained by time. another very important phenomenon demonstrated by these plots is related to the point at which performance drops. one can observe that the differences between the buffer memory sizes that fit into l1 cache and the ones that are too large become more noticeable as the bandwidth increases. even worse, for the 4 b element type there is no drop at all when buffer size surpasses the cache size. the main cause for this behavior is that we are not using the full processor capacity in term of memory access. when loop unrolling is added and the element size increased, we approach the true performance limit of the processor."
"where data matrix x and y have been standardized one as the standard deviation and mean zero, and consider xu (xu) and yv (yv) as the canonical variables in this paper."
"de-peng han received the master's degree in electronic and control engineering from chang'an university. he has published several academic articles. he participates in and completes the scientific research project with chang'an university in central basic scientific research in colleges and universities were a special fund project based on mutual information of uneven illumination pavement crack automatic identification algorithm research, xi'an technology bureau industry-university-institute cooperation to promote engineering based on image analysis of cracks in the bridge foundation over a long distance monitoring methods research, and so on. in recent years, he has been engaged in the research work of bio-image genetic big data correlation analysis, and the research of machine learning and data mining."
"wheref ≈d t ⊗d * . as mentioned before, the steady-state msd of the network under fading condition can be computed by replacing p k r (u) v,k with r v,k given by (12) ."
"similar to the two experiments above, cca, scca, distae, dcca, dccae and dpcae are tuned to report the best result. the evaluation consequences are given in fig. 8 . in the fig. 8, the general linear models for evaluation include cca and scca, denoted in blue lines. for non-linear deep models, the experiment tries to evaluate the distae, dcca, dccae and our dpcae which share similar objective as the general linear models. the task on this experiment is to do data classification between the snps and fmri with different types of feature respectively. the property is measured in terms of roc curve. it defines false accept rate (far) as the x-axis and true positive rate true accept rate (tar) as the y-axis. the calculation formulas of far and tar are given"
"by taking a dpcae having three hidden layers for two modalities as an example, fig. 5 shows the pre-training process of the dpcae. the learning procedure of the dpcae can be grouped into two processes, the pre-training process and the classification process. moreover, the non-saturating sigmoid function is selected as the activation function of each layer in the dpcae."
"therefore, we decided to try to reproduce the essence of the multimaps approach, but with our own code. we wrote our own benchmark script inspired by multimaps, trying to mimic the spirit of the reported experiments. although we aimed at studying all levels of the memory hierarchy with parallel execution, we quickly discovered that there is huge number of challenges even for the simplest case. consequently, we restrict our investigation report to characterize solely l1 cache read bandwidth, for a singlethreaded program."
"current mpi and network driver implementations typically incur several synchronization modes (eager, detached, rendez-vous) and each of these might need a piecewise modeling. figure 4 depicts an example for the grid5000's taurus cluster (openmpi 2.0.1, tcp, 10gb ethernet). we depict the network modeling of the send overhead (left), receive overhead (center), and the network latency and bandwidth (right). raw observations (points) and the piecewise linear (the lines are bent because of the logarithmic scale) regression calculated from them (black line) are plotted as a function of the message size. the color indicates probable changes in the communication protocols. the receive operation (blue area on the center plot) for the medium message size has a much higher variability than for other message sizes. the same happens, but with a different pattern, for the send overhead (yellow on the left). in our case, since sizes were randomized (instead of taking measurements in an incremental order, as it is commonly done), we can safely conclude that this variability is a real phenomenon and not an artifact resulting from temporal perturbation."
"where c 0 is the most specific concept that is present both in the synset path of c 1 and c 2 (that is, the least common subsumer or lcs). the function returning the depth of a concept is noted with d."
"we observe that the bias, b, does not depend on the stepsize µ. therefore, reducing the step-size will not improve the accuracy of the estimated parameters."
"it is greatly known that rbm is a popular tool for representing dependency structure between stochastic variables, and the weights of the general rbm are optimized by minimizing the logarithmic loss function [cit] . differ from the multi-layer belief network based on stack rbms, the dpcae model updates weights by minimizing the objective function on each modality and maximizing the canonical correlation among the fourth hidden layer on each modality. as canonical correlation analysis (cca) has been used for describing the maximal correlation between two stochastic variants, it could be utilized to characterize the correlation among the hidden layers. in the dpcae model, activation functions in visible or hidden units is a smooth monotonic non-linearity function, such as non-saturating sigmoid nonlinearity function. additionally, as the multi-layer belief network shows the better representational learning ability and obtains a unified representation, then the model based on multi-layer belief network is updated to learn multi-modality data representations."
"we propose a three-step methodology according to the fundamentals of the design of experiments [cit] : (1) the experimental design, (2) the benchmark running engine, and (3) the results statistical analysis. we believe that separated stages, together with careful documentation and environment capture, enable us to avoid all pitfalls that we presented. note that mpi benchmarking tools structured in such a way have also recently been independently proposed [cit] ."
"since the covariance and cross-covariance of the data may not be available in practice, the estimate w o ctrl given by (11) can alternatively be sought using the following clms algorithm:"
"where tp is the number of datasets correctly classified as positive samples after training of each modality, fn is the number of trained datasets that are actually positive samples but are divided into negative samples by the classifier. all algorithms are evaluated in fig. 8 . the unsupervised cca and scca perform the worst followed by distae. furthermore, dcca and dccae perform much better. finally, the proposed dpcae performs the best, with effective improvement, benefited from the deep non-linear and penalty term. to all appearances, our dpcae exceeds other algorithms, demonstrating the efficacy of our deep multimodality scheme."
"several experiments have been tested in both cca-based and dnn-based representation learning. the results have discovered that on several experiments, all aspects of performance of dnn-based models consisting of dcca, distae, dccae and dpcae outperform that of cca-based models consisting of cca, scca and smcca. the best overall performer is a competent optimization algorithm for solving dcca with pca on multi-modality linear features learning and a multi-layer belief network based on rbm on multimodality nonlinear features learning introduced here, deep principal correlated auto-encoders (dpcae). it can be interpreted that dnn-based models can acquire preferable representations concerning the interrelated objective measured on multi-modalities. dpcae not only provides a flexible nonlinear mapping but also provides a simple linear mapping. another noteworthy preponderance of dpcae is that, like the cca method, it does not need an inner product. it is difficult to the pattern recognition problem faced by deep learning. there is lack of general conclusion about what conditions can be converged on the training set and what is the minimum upper bound of loss after many epochs. the main reason is that there are too many variables to describe the time complexity of deep learning. in the back-propagation algorithm, the gradient of parameters can only be obtained after a complete ''forward'' calculation and ''reverse'' calculation, and the parameters are updated. therefore, all the intermediate gradients need to be saved, and the space complexity of utilizing deep networks is high. therefore, the experiments on genetic variants and brain imaging phenotypes focused on two works: classification and correlation analysis. meanwhile, in many learning tasks of dnn-based models, for instance, classification and visualization, obtaining the high performance of correlation is not the last aim and the maximal correlated representations are applied in the learning of other experiments [cit] ."
"we observed that in many sentences, especially those extracted from news corpora, the compatibility of the geographic context between the sentences is an important clue to determine if the sentences are related or not. this measure tries to measure if the two sentences refer to events that took place in the same geographical area. we built a database of geographically-related entities, using geo-wordnet [cit] and expanding it with all the synsets that are related to a geographically grounded synset. this implies that also adjectives and verbs may be used as clues for the identification of the geographical context of a sentence. for instance, \"afghan\" is associated to \"afghanistan\", \"sovietize\" to \"soviet union\", etc. the named entities of type per (person) are also used as clues: we use yago 9 to check whether the ne corresponds to a famous leader or not, and in the affirmative case we include the related nation to the geographical context of the sentence. for instance, \"merkel\" is mapped to \"germany\". given g p and g q the sets of places found in sentences p and q, respectively, the geographical context similarity is calculated as follows:"
"finally, the authors of confidence [cit] note that many sources of performance variability can be found in modern hpc systems (e.g., os noise, network collapse or transient effects resulting from user timeshare) and focus on reporting the variability that users may actually face and which is hidden by common benchmarks. such information about variability could be used for simulation purposes provided its dependence on message size is properly characterized."
"chao wang received the master's degree in electronic and control engineering from chang'an university. he has published several academic articles, including the journal of computer science. the scientific research projects, he have participated in and completed include the natural science foundation project of shaanxi science and technology department, identification and positioning of multisensor fusion bridge substructure cracks, and the industry-university-research cooperation promotion project of xi'an science and technology bureau, research on remote bridge substructure cracks monitoring method based on image analysis, and so on. in recent years, he have been engaged in the research on the correlation analysis of genetic big data in biological image, actively tracking the variable selection, dimension reduction and correlation analysis of biological genetic big data based on machine learning, and also engaged in the research on intelligent algorithm processing, and have made some progress."
"the source code we used and raw data is available (network 1 and memory 2 ) to anyone interested in reviewing, running or improving our methodology on other platforms."
model and notation figure 2 shows the simplified storage model where nl message bits are encoded as nc w binary sequences of length l bits each. we sequence nc r reads of length l which are passed through a binary symmetric channel with error probability . here c w denotes the average number of encoded bits synthesized per information bit and c r denotes the average number of encoded bits read per information bit for successful decoding. clearly both c w and c r must be at least 1.
"on the basis of the empirical researches, it is appealing to think about again the essential learning performances of different function types and corresponding penalty terms. auto-encoder-based technology is grounded in the concept that the output variables should be in a position to exactly reconstruct the variables of the visual layer. for another, cca method only focuses on how fine multi-modalities' learning feature forecasts the other feature while neglecting the power to reconstruct multi-modalities. the cca method is anticipated to accomplish well when the two modalities are uncorrelated given the sample labels [cit] . the penalty terms in the miscellaneous algorithms also have a crucial impression. objectively speaking, the stronger dpcae penalty term is not strong enough yet; an even slightly fine penalty term would be to need the learned feature representations to be mutually independent."
"network performance modeling generally involves three aspects: the characterization of cpu and network usage as described for example in the logp model [cit], the synchronization mode between the sender and the receiver, and the presence or lack of linearity for each of the modes. in logp, o is the software overhead per byte and models the cpu occupation per message, l is the minimal transmission delay over the network (latency), and g is the gap per byte between two messages (i.e., the invert of the bandwidth). we can distinguish between three synchronization protocols: eager (totally asynchronous), rendez-vous (fully synchronized), and detached (an intermediate behavior). finally, piecewise modeling accounts that different values for the previous parameters may be used depending on the range in which the message size falls. the more elaborate models comprise all such aspects and are generally needed to capture the behavior of modern mpi implementations and interconnects."
"gang li is currently an associate professor and a postdoctoral fellow with the school of electronics and control engineering, chang'an university. he has published more than 20 academic articles, among which many were published in automation in construction, china highway journal, and the journal of transportation engineering. apply for hosting and complete research projects have a department of shaanxi province natural science fund project the lower part of bridge structure fracture identification of multisensor fusion and localization, xi'an technology bureau industry-university-institute cooperation to promote engineering based on image analysis of cracks in the bridge foundation over a long distance monitoring method research, chang'an university, central university basic scientific research business expenses special fund project uneven illumination pavement cracks based on mutual information automatic identification algorithm research, and so on. in recent years, he has been engaged as the project applicant in the research work of bio-image genetic big data correlation analysis, and actively tracking the work of variable selection, dimension reduction and correlation analysis in the biological genetic big data based on machine learning, so as to make breakthroughs in methods and technologies."
"nevertheless, most of these benchmarks have \"opaque\" and even sometimes arbitrary procedures, especially as hardware evolves. to achieve a minimal measurement duration and memory footprint, they perform both measurements and the corresponding statistical analysis in a black-box like manner, directly generating statistical summaries as output. no intermediary data is kept after the benchmark has finished the measurements and analysis. this absence of intermediary raw measurements makes any performance investigation and model validity assessment impossible to accomplish and ultimately leading to inaccurate models and wrong conclusions that easily go unnoticed. furthermore, modern architectures and operating systems have become intricate. it is thus essential to meticulously setup and control the environment to establish meaningful benchmark conditions. we believe the experimental design, the measurements, and the statistical analysis should be done through rigorous and open tools to provide sound and faithful data for model and simulation instantiation."
"yi-pu zhang is currently an associate professor and a postdoctoral fellow with the school of electronics and control engineering, chang'an university. during ph.d. study, he was involved in ph.d. thesis entitled algorithm research on identification of transcription factor binding sites, and participated in a number of scientific research projects as the main researcher. among them, in terms of data analysis, as the main participants completed on national natural fund project multicore system under control graphs model and algorithm of pattern recognition research, despite the focus in the study of the project for the parallel algorithm design, but the algorithm model analysis of genetic data in pattern recognition under the hidden markov model have a deeper understanding. in terms of big data processing, he participated in the project research on big data compression index and search algorithm of the national natural science foundation of china, which consolidated the theoretical foundation of pattern recognition for large-scale data."
"in (8), the constrains c1 and c2 are given to guarantee the security of pus in the primary network; c3 and c4 are imposed to guarantee the security of sus in the secondary network. c5 and c6 are the energy constraints for satisfying the harvesting energy requirements of ehrs in the primary network and the secondary network, respectively. since the energy harvesting form is nonlinear and complex, it is challenging to solve p 1 . moreover, all the involved channels are identified as the realistic channel estimation model, which considers the existence of the channel estimation error. this extremely increases the difficulty to solve p 1 due to infinite inequality constraints caused by the uncertain csi regions. in order to solve p 1, sd relaxation (sdr) and s-procedure are exploited."
"in this work, we first analyze the fundamental quantities associated with dna-based storage systems and understand the associated tradeoffs by theoretical analysis and simulations. based on this assessment, we propose a practical and efficient scheme to achieve an improved tradeoff between writing cost and reading cost by combining ideas from modern coding theory such as large block length ldpc codes [cit] with heuristics to handle insertions and deletions."
"in the work, we propose the dpcae model to effectively combine prediction and correlation using multi-layer belief networks. dpcae seeks the optimal network representation that can maximize cross-data correlation while minimize the data fitting error. as we demonstrate, the dpcae model overcomes the limitation of several existing models in that it can detect complex nonlinear relationship and acquire maximal correlation. as a result, the model can lead to better performance in both prediction and correlation detection. the superior power of dpcae on both correlation detection and classification makes it a suitable model for genomic data integration, here we apply the model to analyze the correlations of functional networks and the difference of multi-modality features between how different subject groups."
"the adaptive feature of the proposed algorithm is highly desirable in non-stationary signal environments where the underlying network parameters change over time. in summary, the contribution of this letter are as follow: a) development of a new clms algorithm for parameter estimation in wsn operating over fading channels; b) analysis of the developed algorithm and identification of the main technical issue under such condition, namely the estimation bias; c) derivation of the relationship between the bias estimates with that of the channel perturbations; d) development of a new bias-compensation technique to remove the bias and e) performance analysis of the developed bias-compensated algorithm, including the derivation of the stability conditions and the steady-state meansquare deviation (msd) expressions."
"the excellent classification accuracy (over 90% for snps data, over 95% for fmri data) indicates the subjects (e.g., schizophrenia patients and healthy controls). the most important factor must be pointed out here is that it could be seen from fig. 7 that the classification accuracy of fmri data is higher than the classification accuracy of snps data which might be due to the fact that fmri data is an imaging data which were pre-processed with spm5, while snps data may be deletions of gene fragments due to human or environmental factors with linkage disequilibrium which is not as accurate and consistent as fmri data. there is great interest in studying how different original features of imaging genomics impact the classification accuracy of snps data and fmri data, however, the data representation of dpcae is created by different deep networks in which nonlinear activation functions are used to each hidden layer. as a consequence, it is not easy to analyze how each original variable is displayed in the deep network representation and hence it is challenging to analyze the identification performance of each original biomarker variable."
we present the general context of hpc performance prediction through simulation and the most common underlying models. this motivates the presentation of common benchmarks for network and memory performance characterization that are detailed in sections iii and iv.
"in some scenarios, varying feature types are favorable for different modalities. for instance, in scenario of video and images, intensity and covariance of intensity are preferred for representing the videos and images respectively, or varying lighting pre-processing are preferred for varying images. in these scenarios, classification is conducted across feature type. to analyze two-modality data across feature type, different models conduct the experiment on the mcic dataset with two modalities."
where m is the total number of layers in the deep network. the formulation of deep cca is given in (5) and corresponding work-flow diagram is given in fig. 3
"where fp is the number of trained datasets that are actually negative samples but are divided into positive samples by the classifier, tn is the number of datasets correctly classified as negative samples after training of each modality."
"in this section we describe the measures used as features in our system. [cit] participation is less detailed than the description of the new ones. additional details on the measures may be found in . when pos tagging and ne recognition were required, we used the stanford corenlp 1 for english and freeling 2 3.1 for spanish."
"for simplicity, we work with bits instead of bases but the results can be extended to an arbitrary alphabet. furthermore, we assume that the decoder has direct access to the \"index\" of each read, i.e., the decoder knows the position of the encoded sequence corresponding to any given read (the numbers shown in figure 2 ). this simplifies the analysis considerably and can be achieved in a practical system by attaching the index (possibly with error correction) to each sequence (see section 3). as the theory of channels with insertions and deletions is not adequately understood [cit], we ignore insertion and deletion errors in the theoretical analysis for simplicity. in practice, we deal with insertions and deletions by converting them to substitution errors/erasures using msa and synchronization markers (see section 3). finally, we consider an ideal poisson random sampling model. this model does not capture the synthesis and sequencing bias seen in practice, but while this assumption changes the absolute values obtained in the analysis, it should not affect the tradeoffs and comparisons. as we see below, we can gain significant insights about the real system despite these assumptions."
"first, a closer inspection of the multimaps code revealed that it is much more complicated than the simple stream benchmark on which it was based. many parameters require [cit] exploiting the output of multimaps and illustrating the impact of the working set and of the stride on performance of an opteron. performance drops when array exceeds 64kb and 1mb."
"in this section, a robust power minimization problem is formulated in cr with swipt based on the practical non-linear eh model while the security of both the primary network and the secondary network are guaranteed. due to the existence of variable couple and the complex form of the harvesting energy, the formulated problem is challenging to be solved. a suboptimal solution is proposed to solve it."
"in this paper, we propose a novel model, deep principal correlated auto-encoders (dpcae), which uses two back propagation neural networks and two multi-layer belief networks to integrate the maximal correlation of the model simultaneously. it's necessary to note that two multi-layer belief networks are applied to extract features of hidden units of two modal data respectively. these extracted features are used for correlation analysis, and then the maximal correlation is selected to extract features with maximal correlation coefficients. two back propagation neural networks are added to receive the output eigenvectors of the multi-layer belief networks as its input eigenvectors, each layer of the belief networks can only ensure that the weight within its own layer achieve the optimal eigenvector mapping for the layer, but not for the entire multi-layer belief networks. the back propagation neural networks propagate the parameter information from the top down to each layer of the multi-layer belief networks, and fine-tuning the whole multi-layer belief networks in each modality respectively. comparing to the model of cca-based and that of dnn-based, our model joint multi-layer belief network is based on several rbms and linear dimension reduction method, which is not only to speed up machine learning, but also to discard dimensions that carry less information. moreover, different from nonlinear deep canonically correlated auto-encoders, the dpcae model adjusts the network's in-layer parameters by using multi-layer belief network and fine-tunes all network parameters from top to bottom layer by way of bp network, resulting in the better acquisition of nonlinear correlation in multi-modality data, while applying pca and penalty term for better data fitting. it overcomes the disadvantage of falling into local optimum and long training time due to random initialization of weight parameters [cit] ."
"to identify and clearly show the influence of different models on the performance of maximal correlation, we have combined the proposed model with other advanced models into a table. for dnn-based models, the experiment decreases the total number of intermediate units in all layers so as to the total sum of all parameters approximatively consistent with cca-based models. table 2 gives the maximal correlation, noting that the maximal correlation of both datasets increases monotonically with the number of parameters of all models."
"yu-ping wang received the b.s. [cit], and the m.s. degree in computational mathematics and the ph.d. [cit], respectively. after his graduation, he had visiting positions at the center for wavelets, approximation and information processing, national university of singapore, and the washington university medical school, st. louis. [cit], he worked as a senior research engineer with perceptive scientific instruments, inc., and then advanced digital imaging research, llc, houston, tx, usa. [cit], he returned to academia as an assistant professor of computer science and electrical engineering with the university of missouri-kansas city. he is currently a professor of biomedical engineering and biostatistics & bioinformatics with the school of science and engineering, tulane university, and also with the school of public health and tropical medicine, tulane university. he is also a member of the tulane center of bioinformatics and genomics, the tulane cancer center, and the tulane neuroscience program. his research interests have been computer vision, signal processing and machine learning with applications to biomedical imaging and bioinformatics, where he has about 200 peer reviewed publications. he has served on numerous program committees and nsf/nih review panels, and served as editors for several journals such as neuroscience methods. volume 8, 2020"
"in this work we propose practical and efficient error correcting codes for illumina sequencing-based dna-based storage that achieve a better tradeoff between the writing cost and reading cost as compared to previous works. the proposed scheme utilizes ideas from modern coding theory and combines them with heuristics to handle insertion and deletion errors. we believe that the tools, analysis and insights obtained in this project can be useful beyond dna-based storage to understand the error characteristics of synthesis/sequencing platforms and in developing better bioinformatics algorithms."
"overall, the market suffers from a combination of misaligned incentives and information asymmetry, which impedes market growth and liquidity. solving for all of these inefficiencies may be challenging, however we believe that the market would benefit from more transparency. one solution could be to implement a database managed by the issuer that would update and share loan-level information to all participants simultaneously to ensure transparency. the problem with such a solution is that the issuer controlling the data enjoys significant market power over other participants and may prevent the collaboration of multiple competing servicers, issuers and originators. another potential solution could be a new type of decentralized digital platform, such as the one powered by a distributed ledger. under such a system, no unique party has full control of the platform and its data. rather, the platform's ownership and governance can be shared among all the participants. such a system could reconcile this need for transparency and efficiency without assigning the same degree of control to the intermediary operating and facilitating transactions in the market, therefore separating the benefits of network effects from the agency costs they entail in terms of market power [cit] ."
"distributed ledger technology has the potential to fundamentally change the way markets operate, by reducing agency costs while bringing more transparency and accountability to each market interaction. in order to unlock such potential, there needs to be flexible privacy settings that can preserve confidential, proprietary information while allowing participants to verify data accuracy in a timely manner. we propose to adopt a zero-knowledge protocol, zkledger, that addresses the tradeoff between privacy and transparency. we introduce zkabs, a simplified architecture of a decentralized market platform based on zkledger and designed for the securitization industry. using widely accepted cryptographic zero-knowledge proofs, zkabs preserves the transparency of transaction at a granular level, while providing distrusting market participants with a suite of anonymized timely analytics on asset pool performance (net cash flow balances, average credit scores, variances, etc.). we argue that zkabs could alleviate market inefficiencies related to the lack of transparency over the quality of the underlying assets. among the potential benefits, zkabs allows investors to better price risk, regulators to monitor fraud and systemic risks and rating agencies to update their ratings in near-real time, making the securitization market more efficient. while our study shows promising applications, privacy solutions for distributed ledgers are an ongoing research area, which we believe open the path towards opportunities for future related work."
"a permisioned ledger due to the sensitive nature of the information disclosed and the types of participants involved in the securitization market, we adopted the framework of a permissioned ledger with a consensus protocol for append-only information which globally orders all valid transactions. financial institution consortia are considering the use of permissioned ledgers as they offer efficiency and security. under these settings, the consortium is responsible for operating the ledger, validating transactions and granting access to new entrants. in zkabs, participants cannot equivocate (assuming the consensus model is sound), therefore the information in the ledger is secure and publicly verifiable by participants. by \"publicly\" verifiable, we imply that anyone with the permission to access zkabs and get a full copy of the ledger can verify the inputs and outputs. figure 3 illustrates zkabs's permissioned distributed ledger architecture for an abs product issued by an spv and backed by auto-loans from the originator. each participant in zkabs has two dimensions of flexible settings: write/read permissions and privacy settings."
"there are several significant limitations to our research findings. first, there are some practical limitations about the implementation of zkabs. this paper does not provide a cost analysis of implementing and running zkabs. while we expect the benefits to outweigh the costs as more participants join the platform, each participant should perform a cost analysis and evaluate the return on investment before shifting to this type of technology. in addition, we do not discuss which actor(s) (e.g. industry consortia, thirdparty notaries, government, fintech company) would be leading this initiative, the legal implications and resulting consensus mechanism. dlt may be slower to adopt due to its decentralized nature and the industry is still exploring different types of operational and consensus mechanisms. in the finance industry, financial institutions have performed experimentation with their own dlt or via the creation of a consortium that operates on a same dlt (e.g. corda, hyperledger fabric, ethereum enterprise). in this paper, we do not recommend a specific infrastructure nor discuss the integration protocol: participants could leverage the existing dlts above to operate zkabs or create a new dlt. second, there are some technical limitations to the underlying technology: zkledger can guarantee the traceability and immutability of the information in the ledger, however the information's veracity can only be as good as its input data (an issue common to dlts). if there are manual errors during the input phase, the information in the ledger will not be accurate. the use of smart contracts and automation from the loan inception phase will greatly minimize the need for manual input and alleviate this concern but is out of the reach of this paper. finally, there are risks over data security breach when performing analytical queries in a repeated manner: a participant may be able to perform reverseengineering and guess changes in individual loan level datapoint by querying certain types of analytics (averages, sums, etc.) at high frequency. there is a need to define a limit of number of queries and interval of time between two queries for each asset in order to prevent this threat."
"pricing efficiency zkabs allows multiple spvs to join the same platform, which will push for data standardization and provide investors with easy-to-compare near-real time information across issuers and related asset-backed securities. as pointed out in the structured finance industry group's report [cit], this could fundamentally improve pricing efficiency and deepen the securitization market: \"security pricing could become more accurate with a potential narrowing of spreads as investors gain the ability page 15 of 20 to make near-real time assessments of security values by tracking shifting patterns in loan-level payments. the pool disclosure-the loans, with their performance and yieldsin the security's offering documentation could also be automatically and almost instantly updated to reflect the very latest portfolio performance. \" [cit] compared to another permissioned blockchain with the right access rights, zkledger provide investors the ability to perform anonymized analytics that are publicly verifiable, which provides additional security, transparency and reduces asymmetry of information in the market."
"currently, there are no available solutions on the market that enable investors to get near-real time performance data 10 . as we discussed in \"introduction\" section, investors have to perform cumbersome data standardization and offline analysis to price risk and perform benchmarking for this type of securities. in addition, they do not get asset pool performance updates in a timely manner, which can result in suboptimal investment decisions and a lack of liquidity in the secondary market [cit] . with zkabs, financial information about each individual loan is stored and updated in near-real time on zkabs decentralized platform. this information includes loan principal, annual payments, delinquency rates, credit scores, remaining term to maturity and other information (see fig. 4 for a sample of loan-level records for an auto-loan) and serves as a base for issuers to disclose periodic information to investors and regulators (e.g. offering prospectus, trace reports)."
"if they adopt zkabs, the issuers' incentive to include investors in their blockchains may change. with zkabs, issuers can hide sensitive loan-level data on their blockchain and still allow investors to perform analytics on the hidden data in order to get secure aggregate balances and ratios about the performance of their abs products."
"security goals zkledger maintains privacy: parties non-involved in a transaction cannot see transaction details. in addition, zkledger ensures completeness through its novel table architecture: when running analytics on the hidden data, the verifier can be ensured that no transactions are omitted. finally, zkledger maintains integrity by enabling distrusting parties to perform publicly verifiable analytics."
"zkledger overview zkledger [cit] ) is an open source protocol developed by the mit media lab digital currency initiative that solves the trade-off between transparency and privacy of current blockchains. it is the first system to generate cryptographically verifiable answers to arbitrary analytics queries without revealing confidential information. currently no other permissioned ledger allows for the ability to run analytics on masked data. other permissioned ledgers only share information on a need-to-know basis, thus there is no universally agreed upon ledger within these systems. the incompleteness of each participants ledger means query responses cannot be verified unless all transactions are announced to the verifier. the combination of zero-knowledge proofs and a distributed ledger is critical to developing flexible privacy settings and selective visibility. using a secure zero-knowledge proof scheme, zkledger provides its users with analytical tools that can run on hidden data. as a result, ledger participants do not need to access all the sensitive data in clear form in order to perform provable data analysis."
"in this section, we focus on a particular use case for zkledger: the potential for investors to get access to publicly verifiable near-real time analytics about asset-pool performance, without compromising individual loan data."
"in the blockchain and gain access to timely information about the underlying assets at different stages of the securitization process. unlike zkabs, this platform supports only participants on the sell-side (issuers, brokers, etc.) and does not support the buy-side of the industry (investors, auditors of investors, etc.). therefore, investors would not be able to verify independently the data accuracy or trade directly on the platform."
"through our interviews, we confirmed that investors would be interested in getting timely updates about the performance of abs products at the asset pool level (aggregate basis as opposed to loan-level basis) and may be willing to pay a premium for this kind of abs offering. there is currently no solution in the market that would enable issuers to provide publicly verifiable timely information at pool level, without revealing loan-level information."
"this paper does not intend to recommend any dlt architecture for the securitization industry as this would be an ambitious separate problem to address for each step of the securitization lifecycle, but rather, to provide a simplified architecture and governance system and analyze the benefits for market participants. such a simplified architecture would have the following components:"
"in this section, we introduce zkabs, a platform based on zkledger that aims to reduce the information asymmetry in the market. in our use case, zkabs is used as a model to share loan electronic records among distrusting participants in a secure and confidential way without compromising the independent verifiability of the data. we argue that the securitization industry would benefit from this technology in order to allow all market participants, including investors, to join the ledger while preserving confidential information. zkledger could allow investors and issuers to interact on the same decentralized digital platform and get access to near-real time updates about abs products' performance while preserving the confidentiality of the underlying loan-level data."
"this information is highly sensitive and issuers may not have the incentive to disclose such data at individual loan-level in near-real time to investors. further, investors may not want to see the loan-level data points and consumer information in clear form as they would become subject to data privacy regulations 11 ."
"we aim to address this dilemma by introducing a decentralized market platform, zkabs, powered by zero-knowledge proofs and designed for the securitization industry. in cryptography, a zero-knowledge protocol or proof is a method by which one party (the prover) can prove to another party (the verifier) that he knows a secret statement without revealing the secret itself [cit] . our platform is based on zkledger [cit] ), an experimental system developed by the mit media lab that leverages zero-knowledge proofs to preserve data privacy while providing its users with a suite of publicly verifiable analytics at the aggregate level (e.g. sums, averages and variances)."
"in our use-case, we focus on one underlying asset class: autoloans. as the platform grows, zkabs's flexible privacy settings could host multiple originators and abs asset classes (e.g. credit card loans, receivables, etc.) on the same platform while maintaining loan-level data privacy among competing originators and issuers. since zkabs is used as a means to store loan-level records, the data storage required is reasonable (e.g. the number of loans in an asset-backed security is typically constant until maturity). therefore, the scalability limitations of zkledger would not be an issue. zkabs could host all the participants of the us securitization market, and thus power a new type of decentralized digital platform with online analytics and benchmarking tools for the industry."
"read/write permissions participants that contribute to building and updating assetbacked securities (e.g. the spv and servicer) have modification rights to update loan-level payments, pool-level performance and rating information. other participants have readonly rights."
"this paper focuses on the benefits to investors in the securitization market of having access to near-real time performance data to perform their due diligence independently and efficiently. nevertheless, there are potential benefits for other participants that we do not address in details in this paper. [cit], recent advances have been made in the use of machine learning methodologies, such as networkbased models [cit] ) and clustering [cit], to assess and regulate systemic financial risk and provide early warning for risk exposure. these models consider various types of indicators including securitization market conditions and could therefore benefit from the near-real time network-level data collected through zkabs. regulators would be able to track fraud behavior and abnormal correlations using a customized suite of zkledger analytics; a benefit that they value significantly: \"from a regulatory perspective, access to a constantly updated, auditable set of agreed-upon data can also allow a myriad of regulatory benefits, including more efficient know-your-customer (kyc) and anti-money laundering (aml) checks. currently, complying with kyc requirements creates a great deal of duplicative data and work; whereas, if industry participants and regulators could agree on a consensus-based ledger as the repository of relevant data, it could allow for new service providers to facilitate kyc compliance and permit regulators greater insight into the relevant data and processes\" (u.s. [cit] ). another example would be rating agencies or credit scoring agencies, which could get access to performance data and update their ratings in near-real time accordingly. finally, through the implementation of smart contracts, issuers would also be able to export their data into the trace regulatory database in a seamless and low-cost fashion. currently, zkledger does not support private smart contracts, which is an ongoing area of research. a critical part to the success of blockchain applications in this industry is to ensure through assurance services (performed by an independent cpa auditor) that the underlying technology and analytics tools are designed and operating effectively. this is particularly important for zero-knowledge technology due to the privacy goals. current assurance services include examination of service organization controls, such as soc 1, 2 and 3 reports. a soc 1 report is a report on controls at a service organization which are relevant to user entities' internal control over financial reporting. a soc 2 report focuses on a business's non-financial reporting controls as they relate to the aicpa's trust services principles: security, availability, processing integrity, confidentiality, and privacy of a system. a soc 3 report covers the same areas as a soc 2 report but is a shorter report for general public use. a future contribution to the zkledger project would be to explore the types of assurance services required for zkledger applications and their characteristics."
abbreviations abs: asset-backed securities; aicpa: american institute of certified public accountants; aml: anti-money laundering; finra: financial industry regulatory authority; kyc: know-your-customer; mbs: mortgage-backed securities; soc: service organization controls; spv: special purpose vehicle
"overview today, the us securitization market represents $10 trillion [cit] and comprises a wide variety of securitized products such as mortgage loans, autoloans, credit card loans and consumer loans. securitization is the process by which cash fig. 1 overview of the securitization process flows from thousands of individual assets (e.g. auto loans, mortgages, student loans, etc.) from a loan originator are pooled together and transferred to a newly created remote special-purpose vehicle (spv) managed by a security issuer, and then sold as financial instruments (commonly referred to as \"asset-backed securities\" 3 ) to investors. by transferring the credit risk of the loans to the spv in return for cash, the originator is able to recycle capital into the origination of new loans. the spv finances the purchase of the underlying loans with a mix of equity and debt interests in the pool, structured in tranches 4 with different risk profiles. for instance, the senior tranche of an asset-backed security has the lowest risk since it has priority liquidation preference over junior tranches in case of default. rating agencies play a significant role in the process by rating these tranches based on the credit quality of the underlying assets and the reputation of the issuer and originator, using their proprietary rating protocols. these asset-backed securities are then sold to different investors depending on their risk tolerance -senior tranches (e.g. tranche aaa) are typically bought by central banks and traditional banks. mezzanine tranches have higher yields and tend to be bought by mutual funds, while a large portion of the junior tranche and equity remains with the issuer. over the life of the security, the cash flows generated by the underlying assets are collected by the loan servicersometimes the same entity as the originator -and used to repay investors and equity holders (see fig. 1 ). because there are multiple parties involved, there are time lags before investors get notified about the loan payments or defaults."
"smart contracts a \"smart contract\" refers to transactional terms and conditions embedded in computer code which allow automatic execution of the relevant transaction once precise conformity with those terms and conditions has been established [cit] . when used in conjunction with blockchain technology, the code itself is replicated across multiple nodes and, therefore, benefits from the security, permanence and immutability that a blockchain offers [cit] . a simple example of a smart contract is the automatic payment of monthly interests on a loan when the due date arises. the goal of this paper is to focus on zkledger application to reduce asymmetry of information among participants rather than to address the potential efficiency and security gains of automating the business logic of the securitization process. zkabs architecture currently does not have a smart contract layer due to the lack of legal framework surrounding their application and the ongoing research on privacy-preserving smart contracts. however, we can imagine that a smart contract layer could be added later in order to automate and bring on-chain several steps of the securitization lifecycle, such as collecting loan payments directly from the consumer, identifying non-performing loans, pricing and rating security. as mentioned in \"the inefficiencies of the securitization market\" section, zkledger is agnostic to the dlt used and could be implemented as a set of smart contracts on top of an existing dlt which already has a smart contract layer and then could easily link other smart contracts into zkabs. a smart contract layer could reduce operational errors and speed up data processing, particularly at the servicer level."
"regarding the technology, we leveraged existing literature review on zero-knowledge proofs described in \"zkledger, a privacy-preserving protocol section\" [cit] and reviewed relevant articles on pedersen commitments [cit] and sigma protocols [cit] . we received guidance from the zkledger developing team to validate our approach for designing a novel system based on zkledger for the securitization market. we took the example of the investor viewpoint to demonstrate zkledger's benefits. to build our model, we leveraged anonymized samples of servicer reports and loan tapes obtained during our expert interviews which enabled us to understand existing product for investors."
"these misaligned incentives are amplified by the lack of timely information available to investors. investor due diligence is a necessary component of an efficient market [cit] . through our interviews with the trading desks of institutional investors 5, we established that investors lack the price and liquidity discovery online tools to perform their due diligence independently and efficiently. investors receive information at the issuance stage (i.e. in the prospectus), but receive fewer and non-standardized asset-pool performance statistics through the life of the asset (european central bank publication n. 975 2008). the information reported in servicer reports lack standardization across servicers: some servicers still do most of their work on paper and scan document copies, which are then stored in siloed databases (servers, data warehouses, government offices) [cit] . this makes it increasingly difficult to reconcile the information among originators, intermediaries, investors, rating agencies and regulators, and results in market inefficiencies such as information delays, operational errors, and a lack of independence among the different parties. the performance updates are often released with significant time lags (e.g. there is a time lag of several weeks between the date of non-payment of consumer and the date the investor gets notified of the non-performance of his pool). such delays can be especially significant in the case of a transfer from one servicer to another, due to difficulty in reconciling data. in consequence, buy-side traders often have to download scanned documents from many servicers and standardize the data themselves to perform their analysis, which is very time consuming and requires a high degree of expertise. as one of our institutional investor interviewees 6 pointed out:\"it would be great to have online analytics tools to monitor loan performance and track the record of originators in a timely manner. \" the lack of timely information is even more pronounced in the secondary market, where often there is no price listed on market platforms or it is outdated by several weeks. in addition, there is opacity in tracking the flows of collateral and security ownership throughout the value chain and in financial markets. for instance, investors that manage arbitrage or relative value strategies are interested in information about security ownership such as concentration or composition for a specific collateral that they own partially. today, the investors would have to carefully search dealer inventories or speak to dealers in order to find the exact collateral. therefore, investors lack the information tools to make their investment decisions efficiently. in us, secondary markets for abs are much smaller compared to agency mbs [cit] . for abs, the difficulties and delays in accessing information on the underlying loans may be one of the main reasons to drive investors away from these securities or demand a higher risk premium. the possible 'near-time' solution of zkabs may reduce these issues, encourage more investors and thus increase the size of the abs market."
"we want to caution the reader about the notion of real-time updates. often times, proposed blockchain solutions include the promise of delivering real-time information to market participants. this raises privacy concern and security vulnerability as real-time updates might leak transaction contents. for instance, if an investor monitors the performance of an abs product every second, he could identify which loan in the pool was updated and reconstruct loan-level records. to preserve tuneable privacy, we introduce the concept of near-real time updates. in near-real time settings, the frequency of information release allows for multiple loan-level data points to be updated before they get published in the platform, therefore maintaining the privacy of the loan-level data. in the securitization market, loan-level data updates follow a cyclical pattern which depends on the type of underlying asset backing these securities: an autoloan typically has monthly payments while a credit card loan has a revolving structure and could be paid back any day. in addition, investors may have different needs depending on their risk profile: central banks and traditional banks usually invest in aaa tranches with very low default rate and are therefore usually focused on monthly updates. hedge funds and private holders who may invest in riskier tranches and short-term investments may be interested in more frequent updates about loan performance. near-real time should therefore be taken as a broad and flexible definition. in our use case, we take the case of auto-loan abs, which have monthly payments. originators offer consumers usually two payment dates (beginning or end of month), therefore near-real time frequency is defined as biweekly. as the platform scales and hosts multiple asset classes with different payment collection cycles (credit card loans, revolving loans etc.) and multiple types of investors, near-real time could be defined as daily."
"applied to the securitization industry, we argue that zkledger could preserve the confidentiality of individual loan data while providing participants with publicly verifiable near-real time analytics at the asset pool level. it could therefore power a decentralized digital platform where all market participants (e.g. investors, issuers, regulators, rating agencies) could get access to publicly verifiable market analytics in near-real time. we introduce the concept of near-real time frequency to address the security vulnerability and privacy leaks that real-time frequency solutions entail. in the securitization market, near-real time analytics is defined as data analysis based on up-to-date information and can range from bi-weekly updates to daily updates, depending on the types and volume of assets hosted in the system 2 ."
"our system has applications throughout the value chain: in the security construction and issuance steps, it could enable investors to pick up loans on an aggregate basis, without revealing data on the individual loans. in the trading phase, it could provide anonymity of trading in the primary and secondary issuance side and enable investors to get analytics about security ownership concentration. post-issuance, investors could get anonymized performance analytics and query about trends in the market at any point of time. overall, our system could reduce asymmetry of information and improve transparency in the securitization market. in summary, the contributions of this paper are:"
"as shown in fig. 5, investors see a hidden view of the ledger and cannot track loan performance at loan-level, thus zkabs protects the issuers/spv's proprietary and sensitive data such as borrower names and individual loan performance. however, investors can still perform analytics on the hidden data at the pool level as in fig. 6, which allows them to monitor the performance of their loans and improves their ability to price risk efficiently and independently. it allows investors to build their own queries at any point of time. for instance, an investor could query trends about loan default in texas for one particular asset class instead of waiting for the servicer reports. all queries (sum, mean, correlation, etc) are interactive and the party that the investor is trying to query must exchange information with the investor otherwise he cannot query the table, another form of information control for issuers and servicers."
"while there has been an increasing number of companies exploring blockchain-based solutions for their business, reports show that the industry is still nascent [cit] . a key headwind to blockchain adoption is the fact that organizations focus on the technology with hopes that it can redefine their business, instead of spending time on identifying practical business use cases that could benefit from the technology [cit] . they tend to start by applying the solution first rather than identifying a problem and proving that blockchain is the adequate solution for it, and therefore face challenges in market adoption. in order to capitalize on a blockchain, companies should spend more time identifying frictions and processes that could benefit from the unique characteristics of this technology [cit] . in this paper, we purposely spent \"research methodology\" section identifying the key market inefficiencies that could benefit from blockchain technology, before introducing a blockchain solution for the market."
"current distributed ledger solutions are either entirely viewable to all participants or are encrypted to hide sensitive data but do not support data analytics without revealing the content of the data in the ledger. for instance, in order to calculate the net balance of monetary transactions in a private blockchain with distrusting participants, one would need to download all transactions to verify their integrity. this raises privacy concerns for market participants in the us securitization industry (see fig. 2 ). due to the information asymmetry and diverging incentives between issuers and investors, there is a high degree of confidentiality and intellectual property surrounding the structuring of asset-backed securities. although investors can get access to loan tapes 7, with loan-level information, it is rather on an occasional basis and may present data quality issues (i.e. completeness, accuracy) (openrisk). investors may get curated off-chain information on an aggregate basis but are not expected to join the issuers' blockchain and thus will lack the tools to perform their due diligence independently. similarly, current blockchain applications do not yet allow for multiple competing issuers to join the same universally agreed-upon ledger and therefore face limitations for applications at the industry level."
"architecture zkledger works as an append-only ledger. the ledger can be represented as a table with each row being a transaction and each column a category of information (participant name, transaction amount, currency, etc.). the information stored on the ledger is not in plain text but hidden using commitments detailed below. figure 5 illustrates the table design for security issuers and investors in the securitization market. the ledger could be maintained by a third-party (who would not have access to the underlying data) or as a distributed ledger maintained by the participants [cit] . it can potentially be built on top of existing permissioned ledgers -such as hyperledger fabric 8 . the zkledger protocol includes a suite of analytics (e.g. sums, averages, correlations, variances, outliers, ranges and market concentrations) that each participant can query at any point in time."
"the securitization process involves transactions among numerous participants, with diverse incentives. we can distinguish broadly four types of parties: loan originators, intermediaries (e.g. issuers), third parties (e.g. credit rating agencies, servicers, underwriters, regulators and trustees) and investors. the incentives of these participants are not aligned with each other, which becomes the primary attribution of market inefficiencies. loan originators collect commissions on loan issuance and aim to offload their credit risk by selling the loans to investors [cit] . however, they are not directly evaluated based on subsequent loan performance and therefore may have incentives to misrepresent the quality of the loans and to engage in opportunistic behavior (european central bank publication n. 975 2008): since originators' profits increase based on the volume sold, they seek to achieve a high turnover of selling assets with reduced efforts in screening and monitoring borrowers (european central bank publication n. 975 2008). unlike investors, originators will not be directly impacted if the quality of the loans subsequently deteriorates. intermediaries collect transaction fees based on volume processed and have little incentive to balance the risk/reward trade-off that investors are seeking. further, third parties such as credit agencies and servicers may not be inclined to perform downgrades or act upon loan delinquency in a timely manner as they are closely involved with the issuers. finally, investors aim to maximize their returns and mitigate risk using correlation indexes while delegating the management of their securities to intermediaries and third parties. due to these misaligned incentives and asymmetry of information, investors bear the main risks and tend to rely on the reputation of the originators, issuers and servicers as well as rating agencies to support their investment decisions. it is worth noting that risk retention rules (code of federal regulations title 17 commodity and securities exchanges chapter ii part 246 [cit] ) which have already gone into effect have been designed to particularly put a stop to the originate-to-sell model and reduce the misaligned incentives. securitization usage has significantly dropped since their enforcement [cit] ."
"the jiangmen underground neutrino observatory (juno) is an upcoming neutrino detection experiment located in china that aims to determine the neutrino mass hierarchy by detecting reactor antineutrinos from two nearby nuclear power plants [cit] . the central detector, a liquid scintillator of 35.4 meters in diameter is situated with 700 meters rock overburden. it is surrounded by 18,000 20-inch photomultipliers (pmts) submerged in water, that are designed to detect the emitted light with high timing and energy resolutions. in order to preserve signal quality and reduce the number of cables in the detector, the receiver chain is integrated into the pmt housing."
"a digital processing unit of a highly integrated receiver chip for pmts in juno p muralidharan tionally other system events like counter overflows are encoded in the meta data field. in the main data processing unit, binary data from the analog to digital encoder is processed in three stages. by default, data from \"adc 1\" is transmitted. at every sampling instance, data samples are compared across three programmable threshold levels: noise level, high gain level and medium gain level in the data selector. if the signal amplitude crosses high gain threshold or medium gain threshold, data is selected from \"adc 2\" or \"adc 3\" respectively. when the signal amplitude falls below the thresholds, the data selector selects the data from the unsaturated adc with best resolution."
"the jiangmen underground neutrino observatory (juno) is a multi-purpose underground experiment based on a 20,000 ton liquid scintillator with the one main objective to determine the neutrino mass hierarchy. the signal detection is performed by photomultipliers with directly attached readout electronics. the central component for the digitization process is a receiver chip with a low power and large dynamic range analog to digital conversion unit. the concept and design of the included data processing unit and regulation circuit are presented."
"stuart j. [cit], where he is currently a chaired professor and the director of the centre for consumer and organizational digital analytics (coda), king's business school. a polymath, he is opposed to disciplinary silos and enjoys working across a number of academic disciplines. he has published five books (one a bestseller for butterworth-heinemann) and more than 150 articles in leading outlets. his recent research interests include the sharing economy, social media, big data, mobile communications, virtual reality, and virtual worlds. he has been involved in more than 50 academic conferences, as a programme committee member or track chair. he is a reviewer for many leading research grant bodies and journals and an associate editor of the information and management."
"the current is converted into a voltage signal and amplified further by the front-end transimpedance amplifier of the chip. the analog voltage signal is then sampled and encoded in 8-bit binary data by the analog to digital converter (adc) and the following encoder. based on the requirement specifications of juno [cit], an upper limit is set to a signal amplitude equivalent of 1000 photoelectrons (p.e.) per channel. until 100 p.e., a linear charge resolution in the range of 0.1-1 p.e. is required and above 100 p.e., it is expected to be 1%. the required high dynamic range is achieved by using three adcs with different programmable gains in parallel as indicated in figure 1 . while the combination of parallel adcs provides a large dynamic range, it is sufficient to only send data from one of the three adcs for signal reconstruction."
"the effective charge of the particle is measured by integrating the current signal over time from the baseline. biasing fluctuations and intersymbol interference may introduce an offset in this baseline. by correcting the offsets online, digitization error, intersymbol interference and reduction of adc range due to the offset is minimized. this is realized by a real-time digital sigma-deltabased adc baseline regulator that is included in the chip. an error signal is generated in a control loop by calculating the differences between the actual and the desired baseline. a control signal proportional to the error signal is then generated which in regular control loops would be used to adjust the input signal. however, this regulator design show in the figure 4 adjusts the reference voltages of the adc proportional to the offset."
"data originated during ot requires privacy, confidentiality, and integrity while in storage, or transmission or processing. also, a large volume of multimedia data is being generated during each ot session. in order to provide occupational data security, recent advancement in blockchain and offchain-based decentralized digital repository shows promising options [cit] . the new generation of blockchain and offchain solutions even guarantees availability and scalability of ot data [cit], proper end-to-end encryption, digital wallet with secure cryptographic public/private keys, and high speed transaction overlays such as lightning network (ln)."
"occupational therapy (ot) is intended to allow daily life activities independently [cit] . the purpose of ot is to allow an individual to live as close as possible to their normal day-to-day living. for effectiveness, ot governs therapeutic features such as type, length, and frequency of motor imagery and therapeutic exercises, and change in difficulty level or course of activities to support quality of improvement [cit] ."
"this work presents the architecture of a custom data processor, selecting data from three parallel adcs. a customized data selection performed by the processor aids to efficiently utilize the whole dynamic range while reducing the output data rate. the verification of the produced first prototype shows a processor complying to the required functionality. in addition to the processor, the architecture of an improved on-chip baseline regulator has been presented. with the proposed realtime baseline correction, the dynamic range of the adc is not reduced by baseline drifts and can be solely used for signal digitization and with a higher precision than the regulator in the first prototype."
motor imagery can help in actual occupational therapy activities [cit] . figure 2 shows a sample scenario in which a person is intending to perform an elbow flexion operation. appropriate motor signals are sensed by the eeg headset from the motor cortex area and the corresponding emg signal is sensed by the emg armband.
an on-chip data processing unit is included to judiciously select data from one of the three adcs and to generate meta data to identify the gain of the adc during data reconstruction. addi-
"in this paper, we propose a novel in-home occupational therapy environment, which incorporates off-the-shelf eeg, emg, eye tracking sensors, smart home iot sensors [cit], and kinematic gesture tracking non-invasive sensors to support forward and inverse kinematic actions. we have developed a 3d game environment in which the sensory data from the brain, muscles, joint range of motion and eye positions are fed to a digital avatar. the occupational therapy environment has been created with a subset of therapies that incorporate the brain commands, the hand muscle movement, and different hand and eye gestures to interact with the serious game environment in which different smart home iot devices can be controlled with gestures. the game environment consists of two phases: during the first phase, a digital model occupational therapist guides a subject with model occupational therapy movements. during this time, the subject develops motor imagery in his/her brain. during the actual therapy time, the subject performs the action, which is recorded by the multimodal sensors. at the end of the therapy session, a summary of the kinematic data and quality of improvement is saved in the secure therapy blockchain while the raw eeg, emg, and other kinematic data are saved in an off-chain repository for immutable storage. the therapeutic data can be shared with a remote therapist, which consists of an improvement in terms of motor movement, muscle power gain, and the ability to do certain motor tasks, as defined within the therapy."
"when a person's mental state of ''wanting to do certain action'' change, corresponding oscillatory components of eeg signals also change [cit] . event-related synchronization (ers) is a notion which is characterized by an increase of eeg signal power in a certain band of brain signal frequency [cit] and event-related desynchronization (erd) happens when the signal power decreases [cit] . for example, during and after the imagination of a left hand movement related to an ot exercise exhibits an erd and ers respectively in the beta and gamma frequency bands [cit] ."
"a highly integrated analog to digital conversion unit (adu) is developed in 65 nm cmos technology with three high performance 8-bit adcs that have a programmable gain and run in parallel in order to cover a large linear voltage input range of more than 80 db. after digitizing and signal processing, the signal is sent through a 100 meter ethernet cable."
a digital processing unit of a highly integrated receiver chip for pmts in juno p muralidharan ulated based on the sigma-delta modulation principle in the improved version. the average of the baseline then represents the desired baseline as shown in figure 5 . the error signal is scaled before the error integrator part of the control loop. the settling time of the control loop is proportional to the scaling factor used. a faster settling time is achieved by larger scaling factor (coarse tuning) and better tuning is achieved by using a smaller scaling factor (fine tuning). the regulator automatically switches between coarse and fine tuning based on the magnitude of the error signal. simulation results presented in the figure 5 show that the correction of baseline is achieved by the the described baseline regulator.
"each meta data word transmission increases the latency, filling the internal buffer. during the absence of a light signal, data samples below a noise threshold are identified by the processor and compressed by a factor of two in the third stage of processing. this counteracts the overhead generated by the meta data and reduces the occupancy of the buffer."
"brain computer interfaces (bci) have been applied to motor rehabilitation in stroke patients with promising results [cit] . in order to interact with different disabled patients, different modalities can be used such as manual interaction, using voice commands, gestures, eye movement, and thoughts [cit] . combining thought, gesture, eye movement, and voice as modalities for therapy allows various factors to be optimized, such as in different situations: when a disabled person is at home alone, is surrounded by people, or desires to do something him/herself [cit] . forward kinematics data provides the therapeutic gesture data, which shows the wellbeing or improvement of target body joints. inverse kinematics allow a subject to achieve a target goal in terms of forward kinematics [cit] . for example, if a person has a disability in moving the left hand, an iot-based door lock can be interfaced such that during occupational therapy, the door will open through the elbow flexion-extension movement of a certain range of motion. it is assumed that cortical areas control the movements of the contralateral limbs as well as playing a role in ipsilateral movements [cit] . in order to interface the brain signals with the bci, the international 10-20 system is widely used an industry standard [cit] (see figure 1) . the 10-20 system maps a certain portion of the brain with that of the spatial location of the interfaced electrodes. researchers use the 10-20 notations to represent certain eeg signals originating from certain portion of the brain in terms of 10-20 electrode ids. for example, researchers have found that imagination about movement of left and right-hand can be detected by placing electrodes in c3 and c4 or f7 and f8 locations [cit] . on the other hand, researchers have found that eyeball movement influences brain waves' f7 and f8 electrodes [cit] . the suggested placement of the corresponding electrodes is shown in figure 1 . one important aspect of knowing such distinct patterns, researchers can identify high level motor actions related to ot that is created within the brain and place the electrodes accordingly. for example, if an ot exercise requires both hand and eye movement motor actions to be monitored, the bci can be configured to acquire signals from c3, c4, f7, and f8 [cit] . existing occupational therapy research shows that motor imagery can help stroke patients who have problems moving their arms and hands, and legs after a stroke [cit] . stroke sufferers have shown increased neural awareness due to motor imagery exercise during their regular occupational therapy, instead of just regular therapy alone [cit] ."
"however, we have found several challenges while performing the tests with the dataset. the eeg data is extremely noisy as the off-the-shelf sensors such as emotive and muse have to carefully setup to intercept the frequency bands. for example, the beta and gamma wave's discrimination of eeg signal originated due to left and right hand movement motor imagery along with gaze movement captured from c3 and c4 electrodes and f7 and f8 electrodes exhibit different patterns. in addition, the pattern recognition algorithms and the filter volume 7, 2019 designs are highly sensitive to the available dataset. using a completely automated and tightly synched occupational therapy in which brain though is part of the therapeutic process is highly challenging given the fact that the brain signal controls certain hardware such as smart bulb, smart lock or other types of iot devices. in our future research, we will address these challenges of improving the recognition rate and the better correlation with brain and other therapeutic movements."
neutrinos produce photons by interacting with the scintillator. these photons generated inside the detector hit the front of the photomultiplier tube (pmt) and are converted to a current signal. this current signal is fed to the analog to digital unit (adu) which is directly attached to the pmt.
"during the analysis of the dataset as shown in figure 13, we have found that there is a strong correlation between the motor imagery session and the afterwards actual motor actions' data available from the additional sensory data. this follows the pattern shown in figure 2 and figure 11 . in other words, the excitation of neuronal activities through the motor imagery session in which a particular occupational therapy session is being shown in a screen is found to be mapped with the subsequent motor neuronal actions that is available through emg signal at the hand fcu, ed, pl, and ecr muscles, the eye movement data available from the eye tracker and the kinematic data available from hand gesture tracking sensors. this result shows that the occupational therapy can be augmented with the effective eeg, emg monitoring systems to enhance the interaction with the brain functionality. in particular, the neuro-occupational therapy research would bring more in depth knowledge about a disabled user's quality of improvement in all the arenas such as brain activity, disabled joints, muscle tone and other types of therapeutic gains."
"to run the aforementioned experimental design conditions, a custom-made stimulus presentation therapy environment was implemented using unity3d. the application was implemented to handle the following functions: i) present the stimuli via immersive and game-based event, ii) present baseline experimental conditions, iii) assign unique identification triggers to each of the presented conditions, and iv) send the triggers to the eeg/emg/iot/gesture tracking software components."
"in the second stage of the processor, data is prepared for reconstruction. the source of the sample as well as timing and trigger information is appended to the data and stored in the internal buffer. in the third stage, the data stream is formatted for the output. on the occurrence of trigger events or threshold crossings, meta data is sent ahead of the corresponding data. in the exemplary waveform shown in the figure 2 it can be observed that in addition to the processing time, the transmission of reset word and meta data adds an overhead before the sampled data appears on the output. an internal buffer is added to buffer data during the overhead time and is designed to cover even the most extreme scenarios of increased data rates that occur during galactic supernova events."
"this creates an impression in the motor cortex area, which is intercepted by the eeg sensor. when the user actually intends to start the action, the motor actions are recorded by the other sensors. reporting engine provides live and historical reports, which can be shared with one's community of interest. the recommender system is an ai-based system, which leverages the knowledge and data available in the blockchain, off-chain, knowledge base and suggestions available from the model therapist interface to assist in performing the occupational therapy at home."
"through our developed smart home cyber physical game environment. a subject sees a natural daily life environment with appliances which he/she interacts or intends to interact on a daily basis. the ot environment has been interfaced such that the ot exercise comprises those brain and gesture commands that will allow interacting with the surrounding smart home iot devices through virtual reality, augmented reality, or mixed reality serious game metaphors. this will make one engaged and immersed and allow a therapist to know how many of the daily life activities are performed by the subject. while a subject interacts with the iot appliances, the corresponding eeg/emg/eye position/skeletal data is recorded and analyzed by the system."
"besides the control of the configuration, data processing is a major task in the receiver chip to reduce subsequent processing efforts. the concept of the main data processing unit and the baseline regulator of the adu chip are discussed in this paper. the presented baseline regulator concept has been implemented in the second prototype and the simulation results are presented."
a chip prototype was designed and fabricated. the data processor was measured by the internal waveform generator that allows programming waveforms to various entry points in the processing chain. different possible scenarios were fed to the processor to verify the functionality. the measured raw data is then imported into matlab to interpret and visualize it as shown in figure 3 . the silicon verification of the main data processor demonstrated the desired data selection scheme and noise compression. a data reduction from 3 gb/s to 1 gb/s is achieved by a conditional data selection during signal measurement mode and a further data reduction to 0.5 gb/s is achieved by compressing data during the absence of the light signal [cit] . the main data processing unit from the first adu prototype was measured and verified for functionality in the lab.
"the brain generates rhythmical potentials in response to certain sensory-motor stimulus [cit], which can be interpreted by eeg. the motor imagery is very popular in augmenting the rehabilitation process of disabled people [cit] . in particular, the brain state signature of a disabled person, which is decoded by the interpretation of eeg signals can help during the physical rehabilitation process. although several methods of capturing neuro signals exist, such as eeg, magnetoencephalography (meg), functional magnetic resonance imaging (fmri), and near infrared spectroscopy (nirs), eeg data is widely used as neurofeedback for mapping the brain with a set of activities being performed [cit] . this is because of its non-invasive usage, ease of use, support of portability, and cost-effectiveness [cit] . being able to map brain activity with certain motor functions has the potential to support disabled people [cit] ."
"electroencephalogram (eeg) and electromyogram (emg) are used primarily to probe the nervous system [cit] . eeg data represent the electrical waves of the brain whereas emg data evaluate nerve and muscle function in the arms or legs [cit] . for example, eeg data available from the motor cortex area of the brain, which controls the muscles of the body that help in moving the arms, fingers, legs, and torso, can indicate initiation of the kinematic actions [cit] . in other words, knowing which part of the brain controls which parts of muscles allows the right therapy to be given to the muscles of interest [cit] . for example, broca's area in the motor cortex controls the muscles in the mouth so that a person can express him/herself in an intelligent and coordinated way [cit] . on the other hand, emg data provides an indication of the electrical activities in the muscle, which is being stimulated by the nervous system [cit] . emg measures the electrical activity of a muscle when a person does kinematic gestures [cit] . once a person contracts any muscle, for example, by making a wrist flexion and extension, the muscle around the wrist joint responds to nerve stimulation [cit] . each brain signal acquired against a thought can be divided into various frequency bands [cit], namely, delta δ (1 -3hz), theta θ (4 -7hz), alpha α (8 -12hz), beta β (12 -30hz) and gamma γ (30 -100hz). each frequency band represents a specific feature. each frequency range contains information related to a different aspect of human thinking. for example, the β and γ rhythms ranging from 12hz to 100hz are related to motor activities, more specifically the visualization of motion [cit] . the higher the frequency in brainwaves, the larger the number of neurons that fire up synchronously at the same time [cit] . when there is increased availability of β and γ waves, a person becomes alert with complete focus and an engaged mind and can have an active conversation, play sports, or drive a car [cit] . although all the types of frequency signals are generated at any given time, a particular type of signal becomes dominant which allows the body to determine what type of activity is to take place. for example, during the day time when one needs to do some hard work, the beta or gamma wave is assumed to have higher dominance over other signals [cit] . hence, researchers have found a correlation between eeg signal's β (12 -30hz) and γ (30 -100hz) waves and the emg signal available at the muscles during a typical working time [cit] . hence, wanting to do a motor task to perform a therapy can be correlated to a particular therapy performed by a subset of muscles around a subset of joints [cit] . understanding the eeg and emg signal gives more latitude of information during an occupational therapy [cit] ."
"forward kinematics data, after classification, are fed into a serious game to be able to follow the model therapist avatar. the objective of the controller is to move the suggested joints to the desired set point and return the kinematic data of the attempted action. the therapy engine controller [cit] measures the difference between the desired position of the model therapist in the cyber world and the actual position, and help drive the bci interface with a signal proportional to this. figure 8 (a) shows the collection of kinematic data from different joints while the occupational therapy exercise takes place. figure 8(b) shows a scenario a user is shown for target hand position in augmented reality view. the user follows the suggested skeletal position and adjusts his hand position, as shown in figure 8 (c). figure 9 shows high level bci data processing framework. frequency and spatial filtering is performed prior to feature figure 7 is applied to deduce inverse kinematic principles in order to follow the virtual occupational therapist in an augmented reality view. figure 9. bci data processing (image courtesy [cit] )."
"although much work has been done in the area of ot, understanding the brain computer interface (bci) and how it can help in certain types of disabilities remains an open challenge [cit] . bci leverages collaboration between the brain and any external hardware and software-based computing system [cit] . bci is used for mind state reading by probing brain activity, which is reflected in the electrical signals generated within the brain neurons. the signals portray a disabled person's mental desires to do an action [cit] . a bci intercepts these brain electrophysiological signals through an invasive or noninvasive computing hardware and software and finally maps each distinct brain signal with a certain action [cit] . in the case of ot, the bci is designed to understand mapping between the brain signals identified by the bci and the corresponding occupational therapy commands [cit] . the hardware interfaces with the brain, collects electrical signals, and relays them to a software component, which analyzes signals, maps the signals with a certain occupational therapy command, and actuates an external device or system that can be part of the ot environment. bci-based research has gained attraction due to its support of neurofeedback, external iot device interactions with brain signals, and the possibility of brain enhancement [cit] ."
"as shown in figure 13, we have found correlation among different obtained signals such as eeg, emg, kinematic, eye tracking sensors and the corresponding occupational therapeutic exercise. for example, in our experiment, the occupation therapy named ''70 degrees flexion followed by 75 degrees extension of left hand wrist joint'' was broken down into ''motor imagery'' and ''actual therapeutic movement'' sessions. during the motor imagery session, a subject follows a model therapist in the vr/ar mode. during this mode, the intention along with the eye movement is tracked from the respective eeg and eye tracking sensors. subsequently, the subject actually performs the wrist flexion-extension movement as shown to a subject during the motor imagery session. the recorded session data analysis shows that adding multi-dimensional occupational data bring more confidence into how a subject's brain, muscle and joints work as a combined unit while performing daily life activities in its normal state. since occupational therapy is aimed at helping each patient going to his/her normal life, tracking multiple human body parts that allows a subject in performing a certain high level task through occupational therapy will bring better insight about the quality of improvement."
"the remainder of this paper is organized as follows. section ii outlines some preliminary background. section iii describes the proposed research framework, while section iv presents the implementation, results, and discussion. finally, section v concludes the paper."
"figure 12 (a) shows the blockchain and off-chain therapeutic data repository architecture while figure 12 (b) shows a sample architecture of a transaction. while the blockchain stores key ot transactions and other performance related metrics, the off-chain is used to store raw eeg/emg/skeletal data, and other types of multimedia data related to the therapy. as shown in figure 12, a certain user's ot data related to one session is stored in one of the blocks in the blockchain, which includes a link of the raw ot data as electronic medical record (emr) within the off-chain to be able to maintain a global view of a particular ot session [cit] . the ot transactions can be automated using smart contract, a sample of which is shown in figure 12 (b) ."
"indexing a complex high-dimensional description vector from a large database creates a heavy computational burden. so the technique of dimension reduction is an essential step in a high-dimensional hue-sift feature space. principal component analysis (pca), for example, is a popular trick for dimension reduction. the aim of implementing pca is to achieve dimensionality reduction with a minimum loss of discriminate information. at the same time, we can carry out pca procedures to reduce the correlation between variables. this also has additional desirable advantages in that it can partly overcome the shortcomings of the following unsupervised learning algorithm."
"as many researchers have done previously, the the interesting points of the discriminative sift are selected as visual natural landmarks in the training images. however, there are a lot of similar or identical interest points in these consecutive training images. the volume of the high dimensional landmarks database is enormous and this may consequently produce many problems such as redundant feature matching and a heavy burden in searching for the right landmarks. hence, a redundant reduction strategy in the dataset is necessary in order to build a more compact feature space."
"some intuition for lemma 3.4.4 (in the case of our analog network example) is given by the following consideration. if v is a fixed point, or at least 0-approximate fixed point, of f, then v(0) is the \"initial value\" of the solution of the network equations. if, for example, f is a definite integral, or tuple of definite integrals, then v(0) is the tuple of constants of integration, which give this initial value. for an example of this, see remark 4.1.7(f)."
"ilie and ilie showed experimentally that the oc is closely related to the sensitivity of a pattern set. more precisely, they showed that for pattern sets with a given number of patterns of given lengths and weight, minimizing the oc practically amounts to maximizing the sensitivity. consequently, in order to find suitable pattern sets for hit-and-extend approaches in database searching, they proposed to search for pattern sets with minimal oc. the main advantage of this approach is the fact that the oc of a pattern set is much easier to calculate than its sensitivity."
", depicts how likely it is that a query view k q at step k is indicated by an k z, acquired at the i-th node. this probability can be computed by the ratio of the correspondence set to the total number of matched hue-sift features over the lfvms of all the locations."
"this principle formalises the fact that if the system's behaviour depends significantly on small perturbations in its data, then it cannot behave in a stable fashion and its physical observation cannot be reliable. this is because, for example, repeating an experiment or computation will involve small variations of physical data, and for the system to be observable the corresponding variation in behaviour must also be small. here observation is a form of classical measurement, of course. (see also discussion 4.2.14.)"
"natural landmarks are usually detected by the technique of interest points and characterized by local feature descriptors. local image features have received a lot of attention in recent years and they have already gained popularity and dominance in object recognition tasks nowadays. local features could be more discriminative and robust to image variations and clutter, scene dynamics and partial occlusion compared to global ones. there are a number of state-of-the-art descriptors, including scale invariant feature transform (sift) [cit], colour sift [cit], centre-symmetric local binary pattern (cs-lbp) [cit], histogram of oriented gradient (hog) [cit], speeded-up robust features (surf) [cit], gradient location and orientation histogram (gloh) [cit] . gloh, surf, hog, colour sift can be seen as various extensions or refinements of sift. these descriptors are dominant because they are able to capture local visual content characterizations through the distribution of intensity gradients. as is well known, the downside of sift and colour sift, especially the colour sift, is the high computational cost."
"we will construct a solution, namely a fixed point v of f, in stages. at stage k we will have a kτ -approximate fixed point, i.e., a stream v k such that"
"however, when the voting scheme is used by itself, the two lift-lobbies which share very similar appearances cannot be discriminated clearly. even for human-beings, without prior context information, distinguishing one such lobby from another is difficult. the second reason for recognition failure is the shortcomings of the local feature descriptions in the application of scenery recognition. the robot itself cannot determine whether the key points belong to the specific furniture, people walking around or places. only those location-specific key points are helpful in recognizing certain locations, while two other kinds of features belonging to specific furniture or pedestrians may not appear in the query views. for example, there is no poster on the board or the chair may be different between the query and the training samples."
"the common analog modules, such as those treated in [tz07], satisfy a modified version of (the usual notion of) invariance, i.e., invariance relative to initial values. first, we must divide our parameters into two classes: the \"system parameters\" and \"initial constants\". the latter can be thought of as initial values of (some of) the stream variables, and appear typically as constants of integration. unlike the system parameters, they appear in \"updated form\" in the formulation of the invariance property (definition 4.1.3 below)."
the study of continuity of stream operators (and their computability [tz11] ) provides a rich source of topics for future research. we mention two such topics here.
"in the general introductions to the hmm, the hmm training is to learn the hmm parameters by using baum-welch algorithm. the hmm parameters include the matrix of the transition probability and the matrix of the observation probability associated with each state."
"for the current pattern in the list, we randomly select a match position i and a don't-care position j. if swapping i and j does not improve the current pattern set, we move on to the next pattern in the list and proceed in the same way. this is repeated until we find a pattern where swapping the selected pair of random positions does improve p. in this case, the modified pattern is accepted, all values c r are updated, the patterns in p are sorted accordingly, and we start again with the pattern p r with maximum c r . if we reach the last pattern in the list without obtaining any improvement, we start again with the first pattern, i.e. the pattern with the largest c r, select new random positions i and j etc. processing one pattern p r in this way takes o(m · 2 ) time, since we look only at one single pair (i, j) and calculate the oc or v ar(n ) of the pattern set that would be obtained by swapping i and j in p r ."
"if the sensitivity of a pattern set is to be optimized, the run time of rasbhari is comparable to speed, since the most time-consuming step in both programs is to calculate the sensitivity of a current pattern set p which is done 5,000 times per program run in each of the two programs."
"we developed a program called rasbhari to calculate sets of binary patterns -or spaced seeds, as they are often called -for read mapping, database searching and alignment-free sequence comparison. for sequence-homology searching, rasbhari optimizes the sensitivity of pattern sets, i.e. the probability of obtaining at least one hit between a query and a database sequence that share a gap-free homology of a given length and with a given match probability between nucleotides. since the sensitivity of a pattern set is expensive to calculate, our algorithm optimizes the overlap complexity of the produced pattern sets which is closely related to its sensitivity. we use a hill-climbing algorithm, similar to the one used in speed, to minimize the overlap complexity. unlike speed, however, our algorithm does not calculate the overlap complexity of all neighbours of a current pattern set, but modifies those patterns first that contribute most to the overlap complexity of the current pattern set. if maximizing the sensitivity in database searching, we calculate the sensitivity of the current pattern set after a certain number of iterations and, finally, the pattern set with the overall highest sensitivity is returned. as a fast alternative, rasbhari can minimize the overlap complexity alone, without calculating the sensitivity of pattern sets. this option is useful in situations where large pattern sets are needed for which it would take too long to calculate the sensitivity. as a third option, rasbhari can minimize the variance of the number n of spaced-word matches in alignment-free sequence comparison which is used by various methods to estimate phylogenetic distances between sequences. we could show that, mathematically, the variance of n has a similar form as the overlap complexity of a pattern set, so the same optimization algorithm can be used to minimize both of them."
"so we adopt the robust rpcl algorithm [cit] to establish the indexing structure of the feature database. all the model views associated with the same place undoubtedly share many repetitive feature vectors. by using a robust rpcl algorithm to cluster similar features together, we can not only eliminate most of the redundant computational load of the matching, but also reduce the whole size of the feature database. the key idea behind robust rpcl is to incorporate four strategies into clustering learning: rival penalization, competition, \"limited randomness\" and agglomeration. for each initial cluster seed that is selected using the \"limited randomness\" trick, not only the winner seed is attracted by each sample, but also the rival (2nd winner) is forced slightly away from it. after the competitive learning and rival penalization steps, we try to agglomerate these neighbour clusters according to the variance of each cluster, until all of the remaining clusters are distinct from each other. further details of this approach can be found in the reference [cit] ."
"an operator f as in (3.1) is said to satisfy causality if the output is \"causally\" related to the inputs, in the sense that the output at any time depends only on the inputs up to that time. we will give an exact definition below."
"in the case of an sca network, we cannot use the above argument, since the module functions are not stream transformers, but functions from data tuples to data (see remark 3.1.5), and so the concept of causality does not apply to them. but in any case the network state function is clearly causal, by (3.15) (example 3.3.5(2))."
"however, the sift feature is designed mainly for gray images and ignores important colour content. colour information can provide valuable information in object detection and recognition tasks. lots of objects can be misclassified if their colour cues are omitted. some authors make use of the standard sift algorithm in each colour channel of the rgb colour space respectively; they then combine the individual results in a weighted manner. the primary drawback of this method is a relatively high computational burden that is three times that of the typical sift."
"theorem (abstract computability): if, in addition to the assumptions in theorem 2 (and under some further reasonable assumptions), f is whilecc * approximably computable, then so is φ."
"in this paper, we have described a method to estimate the position of a robot with regard to a topological map. we proposed a joint feature that is a combination of colour cues and local textures. in the training/exploration stage, each individual place was modelled by a lfvm, which was constructed based on the robust rpcl learning procedures. the hmm scheme, not only incorporates the temple order of navigation cues and the spatial context, but also recursively updates the posterior probability using all the available novel features, can increase the robustness of topological localization and can eliminate wrong decisions."
"for alignment-free sequence comparison, pattern sets produced by rasbhari lead to more accurate phylogenetic distances than the random pattern sets that we previously used. while this result may not be surprising, rasbhari is, to our knowledge, the first program that has been designed for this purpose and that can minimize the variance of the number of spacedword matches. we therefore integrated rasbhari into our web server for alignment-free sequence comparison [cit] ."
"as the robot proceeds, there are two different probabilistic models iteratively used to update the posterior probability t bel(x ) : a system model to incorporate the robot movements t bel(x ) and a measurement likelihood model to update the belief based on the latest sensory input. these two models repeat recursively updating t bel(x ) as below:"
"lh conceived and implemented the algorithm, cl evaluated rasbhari for phylogeny reconstruction, ro and sl evaluated the accuracy of read classification with clark-s, bm guided the study and wrote the manuscript. all authors read and approved the manuscript."
"in this paper we have used standard topological notions to model stream processing in continuous and discrete time, in a uniform way. an essential technique in this paper has been to lower the type of higher order stream operators by an \"uncurrying\" process (see remark 4.1.1, and the comment at the end of example 4.2.13(2)). this allows the use of standard and relatively elementary technical concepts from topology and (in future work) computability theory. we have used, as running examples, two simple, commonly found paradigms of stream processing, which we previously studied independently [tz07, ttz09] ."
"stream processing occurs everywhere, often without being recognised as such. there are many occasions where a theoretical analysis of computation has led to models of stream processing [ste97] ."
"a synthetic data set including four gaussian clusters of various sizes with a variance of 0.1 is shown in fig. 8 . the \"ο\" signs represent the positions of 6 initial cluster seeds; the \"*\" signs stand for each updated coordinate; the pentacles indicate the locations of the final cluster centres after the agglomeration operation. the six various colour curves denote the evolving trajectories of the six initial seeds. in this configuration, depicted in fig. 8, there are two redundant seeds. however, one seed oscillates as the result of under-penalization, which indicates that the penalizing effect on the redundant seeds is not enough. the top-right two prototypes merge into one cluster using the robust rpcl algorithm. the composite impact of competition and rival penalization on the six initial seeds achieved a balance after 15 learning iterations. in such cases, the resultant seeds do not change with the increasing learning iterations. thus the number of correct clusters and nearly exactly correct coordinates of four prototypes are correctly attained at the same time. it shows that, the robust rpcl algorithm can still work even in situations where there are improper parameters for the rpcl, and it is insensitive to the choice of learning iterations, while it is sensitive in the original rpcl algorithm."
"when the query image does not directly overlap with the previously learned views, if we apply the voting method without the hmm, there are few features that can be retrieved from those location models, making the probability of the places being an even distribution higher. this therefore causes a loss of confidence until the captured image is able to cover a previously trained location."
"we assume that f satisfies a causality condition (discussed in section 3), which is natural in the context of stream processing and turns out to be crucial in the proofs of the following theorems."
"the location recognition system allows for topological navigation in large scale complicated environments. various experiments were carried out to demonstrate the effectiveness of the presented algorithm. the robot system eventually builds a graph-like environment representation where the nodes of the graph correspond to the functionunits (in this case, lifts, corridors, offices in one building and so on), whereas the edges of the graph indicate the paths connecting the nodes in indoor environments. we try the first experiment (named experiment a in table 1) within a large office environment. the training images are divided into 19 places, as shown in fig. 3 . the training sets are built with six views per place. all the rest of the images are utilized as testing samples."
"each data type of the above form c[x, a] arises typically in some practical situation, and has its own special features. the algorithmic models that are characteristic of that situation determine, or at least suggest, a corresponding computability theory. for example, in the case that x is time, we have:"
"five more indoor location recognition experiments (denoted as experiment b, c, d, e, f, respectively) are conducted in five different office buildings. we re-trained the location feature vocabulary model, which is annotated with each individual place using views captured at an interval of 2m. we learn the representative vocabularies associated with those locations from interest point features. the query views are captured randomly along different trajectories under various lighting conditions, dynamic changes of the environments, including wall posters, furniture changes and the presence of people walking."
"the presented method only applied local features to find the most likely qualitative global localization in terms of individual locations. we are developing approaches to combine more cues to increase the recognition rate and conduct precise pose estimations and continuous tracking, which are obtained based on the results of topological localization."
"the hill climbing is continued until a user-defined number of triples (r, i, j) have been processed, then the current pattern set is returned; by default, 25,000 triples are processed. if we want to obtain a pattern set with maximal sensitivity, the described hill-climbing procedure is repeated 100 times, and for the pattern set with the lowest oc among the 100 obtained pattern sets, the sensitivity is calculated. to calculate the sensitivity, rasbhari uses program code from speed. again, this whole process is repeated 5,000 times, so for a total of 5,000 pattern sets the sensitivity is calculated during one program run. this is similar to speed, but in speed the time-consuming sensitivity calculation is carried out after one round of hill climbing. by contrast, we run our faster hill-climbing routine 100 times before we calculate the sensitivity for the best pattern set from these 100 runs. the final output of our program is the pattern set with the highest sensitivity from the 5,000 iterations."
"at the heart of our theory are questions about the computability of stream processing. there are several different approaches to computability on topological spaces, which converge [sht99] . in a companion paper [tz11] we will address the question of the computability of φ. we consider two models of computability on a, and hence on c[ì, a]:"
"discussion 4.2.14 (hadamard's principle; the significance of continuity). as explained in the introduction ( §1.4), the reason for the importance of establishing continuity of the fixed point function under the conditions given in theorem 2, is that it implies stability of the fixed point φ, as the solution to the specification under the stated conditions. the significance of this is related to hadamard's principle [had52] which, as (re-)formulated by courant and hilbert ([ch53, pp. 227ff.], [had64] ) states that for a scientific problem to be well posed, the solution must (apart from existing and being unique) depend continuously on the data."
"while topological approaches can be considered as an abstraction of the real world in the form of a graph, the robotʹs location is given by a node without metric information, i.e., there is no need to measure the coordinates of the environments. so the memory size can be reduced and it can be used in large-scale environments. the topological approaches guarantee robust performance against getting lost due to the multimodal representation of the robotʹs position [cit] ."
"remark 3.3.7 (different terminologies for contracting operators). in [tz07] we used the terminology \"weakly contracting\" and \"contracting\", in place of (respectively) the present \"contracting\" and \"strongly contracting\". our current terminology seems preferable, on the grounds of practical applicability, as explained in the previous remark."
"as the first step of our method, the environment model is built in the exploration stage. the space is defined as a topological graph where all the nodes and arcs correspond to a group of locations and neighbourhood relationships or linkages between them. each location is associated with a corresponding \"location features vocabulary model\" (lfvm) by learning the modified visual features. the method for building such a model is illustrated in fig. 1 ."
"and so the existence of the network stream transformer for scas with unit delay [ttz09, §4.4], which is defined there by a simple (simultaneous) primitive recursion, can be justified by, or reduced to, the theory of the present paper, using a fixed point construction based on contracting operators (see theorem 1 below). however, this is not really necessary! the primitive recursive definition of the network stream transformer in [ttz09, §4.4] is surely sufficient justification on its own for this function's existence."
"however the concept of contracting operators, as we have defined it, seems more useful in practice. for instance, the stream transformers in the two case studies analysed in [tz07, §4] for our analog network example (see [...]), as well as stream transformers in our sca example (example 3.3.5(2)), are all contracting, but (apparently) not strongly contracting."
hence theorem 1 can be applied to show the existence of a fixed point of the second case study of [tz07] (the iterated mass/spring/damper system) can be handled by similar considerations.
"1. the key-points in the training images are extracted and described; 2. the feature space is compressed in two directions by utilizing principal component analysis (pca) and a robust rival penalized competitive learning (rpcl) algorithm. such a compact low-dimensional feature space constructs the \"location features vocabulary model\" (lfvm) for each individual topological node (place); 3. the key-points in the current acquired image are detected from the camera video and the index of the location is determined (the right class); 4. the neighbourhood relationship between individual locations is modelled by a hidden markov model (hmm), which can be used to improve the location recognition rate."
simpler forms of theorems 1 and 2 were proved in [tz07] for a stronger notion of contraction of the operator f . the proofs here (especially of theorem 2) are much more intricate. the notion used here (unlike the stronger one) is satisfied by the case study associated with our running example of analog networks.
"usually, the gradient location-orientation histogram (gloh) descriptor performs better than the sift since it uses log-polar bins instead of square bins to compute orientation histograms using sift. however, little performance gain has been observed in our tests. in our opinion, this is due to seldom large rotations-in-plane in the video sequences captured by the wheeled robot."
"(1) partial and nondeterministic module functions. from considerations of continuity, we are led to consider module functions that are nondeterministic (or many-valued) and partial [tz04, tz05] . these features will complicate the theory considerably -for example, in the case of scas, it would require replacing a single global clock by a system of local clocks [ttz09, §8.2(1)]. however, they constitute an important generalisation, because of the desirability of continuity by hadamard's principle (see the introduction and discussion 4.2.14)."
", the transition matrix, means the probability of being at state i x based on the state at the immediately previous time step is j x . we define a(i, j) as inversely proportional to the distance between location i and location j . in the absence of a transition between two locations, the corresponding entry of matrix a was assigned a value of zero. in the final stage all the rows of the matrix were normalized. it illustrates the probability of there being movement between topological nodes and helps to improve the robustness of the recognition."
"for a huge range of spaces x and a, we can equip c[x, a] with the compact-open topology and consider the partial functions on c[x, a] that are computable or approximably computable with respect to the topology."
"in both homology searching and read classification, pattern sets generated by rasbhari are more sensitive than the default pattern sets, so more homologies can be detected and more reads can be correctly classified. at first glance, the increase in sensitivity that we obtained seems moderate; as shown in table 1, the improvement is usually in the first or second digit after the decimal mark. in database searching and read mapping, however, even small improvements in sensitivity can lead to a large number of additional hits. moreover, as these additional hits will be mostly in the 'twilight zone' of low sequence similarity, they may be of particular interest to the user."
"proof: similar to the proof of [tz07, lemma 2.3.9], part (ii). (warning! note that \"contracting\" in [tz07] means what we call \"strongly contracting\" in this paper: see remark 3.3.7 below. hence the proof is \"similar\", not identical.)"
"in our modified hill-climbing algorithm, we also swap a match position i with a don't-care position j in some pattern p r in each step of the algorithm, and we evaluate the pattern set that would be obtained by this operation. however, instead of looking at all possible triples (r, i, j), we look at those patterns first that contribute most to the oc or v ar(n ), respectively, of the current pattern set p. the contribution"
"hill-climbing algorithms to find sets of patterns with minimal v ar(n ) or oc both speed and our new algorithm start with a randomly generated pattern set p and use hill-climbing to gradually reduce the oc or v ar(n ) of p. after a pattern set with low oc is obtained in this way, its sensitivity is calculated, if one is looking for a pattern set with maximal sensitivity. this step is omitted in rasbhari if v ar(n ) or oc is to be minimized. the whole procedure is repeated, and the pattern set with the overall highest sensitivity -or lowest variance of n or oc, respectively -is returned."
"as can be seen in the last column in table 2, using hmm improved the recognition rate over the voting scheme from 5.5% to 8.4%. the recognition result of the 1st training sequence case using the voting scheme and hmm are shown in fig. 9, respectively. the recognition rates using the hmm strategy show a better performance than the voting scheme. the hmm, which incorporates all the observation cues contained in an image sequence, performs better than the voting scheme using the current image alone. the transition probabilities between the individual locations modelled by the hmm can increase the robustness of topological localization, reducing wrong decisions. it is obvious that if a robot using just image features gets lost, the probability of it having stayed at its previous location or its immediate neighbourhood and connected location the in topological graph will be much higher than those places far away or inaccessible. we can determine the route/trajectory of the robot from fig. 9 . the observed reality is that it crossed from office 901 (indicated by node 1 in fig. 2 ), then it reached room 904 (indicated by node 3 in fig. 2 ) across the corridor, and finally, it ended in room 905 (indicated by node 5 in fig. 2 )."
"the network stream transformers in our two running examples are contracting, as we have seen (examples 3.3.5). what about causality? to investigate this, we first define a related concept. let, again, f be as in (3.1)."
"where a is the matrix norm of a. (for convenience, we use the 'max' norm · ∞ .) applying this to the equations (3.11) for our first case study, we find that (3.13) holds, with a similar analysis applies to the second case study in [tz07], involving an iterated mass/spring/damper system. we omit details."
"let (x, d x ) and (a, d a ) be two metric spaces. let c[x, a] be the set of continuous functions from x to a."
"clearly, the semantic modelling of analog systems benefits most from our approach, as analog computers are both complicated and neglected. of course, there are many further examples of stream processing (such as dataflow networks and hybrid embedded systems) to be investigated."
"actually, in the formulation of hadamard's principle, \"continuously\" should perhaps be replaced by \"piecewise continuously\", to accommodate discontinuities at phase changes, for example, the gas/liquid interface in connection with charles's law."
"the application of vision sensors in robotics always raises the problem as to which kind of image features would be most discriminative. though some researchers have applied omni-directional cameras as their vision sensors, a regular camera is utilized in this paper. it is often argued that local feature-based representations are more robust against scene dynamics, background clutter and partial occlusion than globally derived features. a popular approach for obtaining local visual features that have robust attributes is known as \"interest point\" detection, which involves identifying interest points that can be reliably extracted from various viewpoints of the same scene. in this paper, we propose a modified feature descriptor, that combines the sift feature with dominant hue information, as a special natural landmark in single camera vision."
"for example, in case study 1 in [tz07], with the network n 1 (example 3.3.5(1)) there are 1 input stream variable, 3 non-input stream variables a, v, x (acceleration, velocity and displacement respectively), 3 system parameters m, k, d, and 2 initial constants x 0, v 0 associated with x, v respectively. note that there is no initial constant associated with the acceleration a. the network n 2 for the same system (example 3.3.5(1) again) is similar, except that it has only 2 non-input stream variables x, v (each with its associated initial value). hence, in general:"
"remark 4.1.1 (continuity of f ). the assumption of continuity of the operator f in theorem 2 and elsewhere is made with respect to the typing of f in (4.2). in fact, as a study of the proofs of theorems 2 and 4 will show, we need only assume continuity (or uniform continuity) of f for the first 3 arguments, i.e., for (c, a,"
"(v) analog fields: quite generally, x can be a continuous space modelled by a manifold, and data can be measurements from a normed vector space."
"as it is, the modulus of contraction for our network functions will be a family indexed by the stream inputs and network parameters (as in §4.2). the reason for form of the equational definition for a ′ (t) given in (3.9c) instead of the simpler and (apparently) equivalent"
"in the results, the modified hue-sift features are implemented for the experimentation but not for the final application. actually, any kind of local detector and local distinct feature, including the above mentioned surf, lbp and gloh, can be incorporated into this localization framework, with differing computational load and accuracy rates. we also tested using harris corners and found with these it could be greatly faster without a loss of accuracy. the point detector type (using sift detector or harris corner detector) does not have any influence on the accuracy of the proposed method."
"it has been shown that using colour cues and local textures can eliminate a large number of wrong matches. moreover, it is very beneficial to associate images from distinct view angles with the same node label."
"the extrinsic sensors are required to provide rich information to reliably distinguish between adjacent locations, for a robust localization system. most of the early work on place recognition was based on sonar. recent advances in robot vision have made the fusion of vision and other range sensors a viable modality, opening up possibilities for more robust detection. the task of vision-based topological localization for mobile robots is to automatically associate images with semantic labels. we expect that monocular vision itself can provide sufficient information without the need of additional sensors such as stereo sensors [cit], sonars [cit] or a laser rangefinder."
"note also, however, that the network stream transformer φ n of an sca network can also be obtained as a fixed point of a contracting operator! see example 3.3.5(2) below."
"in database searching, one wants to maximize the sensitivity of pattern sets i.e. the probability of finding at least one hit within a gap-free alignment of a given length l and probability p for a match between two residues. calculating the sensitivity of a pattern set is np-hard [cit] . the sensitivity can be approximated by dynamic programming [cit], but the run time of this algorithm is still exponential in the length of the pattern. in patternhunter ii, a greedy algorithm is used to find suitable patterns. [cit], ilie and ilie introduced the overlap complexity of a pattern set and showed experimentally that -for a given number of patterns with a given length and number of match positions -minimizing the overlap complexity corresponds to maximizing the sensitivity in database searching [cit] . in contrast to the sensitivity, however, the overlap complexity can be easily calculated. to find optimal pattern sets, ilie and ilie proposed a hill-climbing algorithm that minimizes the overlap complexity. they implemented their algorithm in a software tool called speed [cit], which is several orders of magnitude faster than competing approaches and is now considered the state-of-the-art in seed optimization."
"we could then develop a theory of fixed points of strongly contracting operators, which would in fact lead to much simpler proofs of theorems 1 and 2 below (existence and continuity of fixed points)."
"two lemmas (4.2.3 and 4.2.6) follow, showing how the properties of causality and shift invariance are inherited from f to the fixed point function φ. proof (outline): show, by induction on k, that the kτ -initial segment of the kτ -approximate fixed point, as constructed in the proof of theorem 1, depends only on the kτ -initial segment of the input, using causality of f . note also that this property (i.e., that the kτ -initial segment depends only on the kτ -initial segment of the input) is preserved by kτ -approximate uniform limits."
"it also can be seen that under circumstances of intentional changes in lighting conditions, back-ground clutter, the dominant hue-sift always performs best among the above features, consistent with their strong properties of illumination invariance and scale invariance."
" is computed to predict the current state of the robot using a system model. the system model here can specifically be defined as the robot motion model, the conditional density t t 1"
"the dominant hue-sift is slightly better than the rgb-sift in those tests, since we have dropped those interest points the saturation of which is below 0.2 and the intensity of which is below 0.1, or above 0.9, during the traverse stage and the building of the database stage. thus, the dominant hues play more stable roles than the rgb colour space."
"the basic mathematical theory of stream processing raises some intriguing questions about the role of natural assumptions on stream operators such as continuity, causality and shift invariance. the theory presented is designed to be close to examples of systems which are rich in physical properties."
"in read classification, the sensitivity of clark-s could be increased by 0.08 and 0, 07 percentage points, respectively, for the largest data sets that we used, hc1 and hc2. each of these data sets contains around one million reads, so the improvement in sensitivity that we achieved with rasbhari means that 800 more reads from hc1 and 700 more from hc2 could be correctly classified by clark-s. this is remarkable, since the classification accuracy of clark-s is already very high, so it is hard to further improve the program. an interesting question in the context of clark-s is how the length and weight of the patterns influence its accuracy. so far, it was difficult to investigate this question systematically, since the exhaustive method that the program uses by default, is too time consuming. with the massive improvement in runtime that we achieved with rasbhari, it is now possible to systematically investigate how the accuracy of clark-s depends on the parameters of the underlying pattern sets."
"global localization in the operation environment is a fundamental and challenging capability for any robot exhibiting goal-oriented behaviour. the basic methodology for self-localization is to compare sensor information with previously captured knowledge about the environments. the self-localization problem [cit] has been studied carefully in the field of mobile robotics and a variety of approaches with different sensors and capabilities have been developed. research into robot localization has diverged into different schemes, which are either based on geometric models [cit], a topological map [cit] or some hybrid method."
"the standard sift features detected in two consecutive views belonging to the same location (but with different view angles) are shown in fig. 5 (a) and (b). it demonstrates that the standard sift features are insensitive to changes in viewpoint. we can see from fig. 6 that, although most standard sift features possess enough discriminant information to match the image against the corresponding key points, some false matches are made due to the fact that it ignores colour cues."
"the space c[x, a] is \"locally metrisable\" by the pseudometrics d k defined by (2.2). there are also \"local\" concepts of limit and cauchy sequence. a sequence (u n ) of elements of c[x, a] is said to converge locally uniformly to a limit"
"the outline of the proposed algorithm is illustrated, separated into an off-line training stage and an on-line testing stage respectively, in the bottom and top part of fig. 1 . the physical layout of the large office environment is shown in fig. 2 . firstly, the robot is manually guided along a route during a training step. the image sequence captured by the robot camera is divided into individual locations. at the environment modelling stage, the focus of our approach is how to build a compact and, at the same time, discriminant feature-set model that is suitable for representing each individual location. such feature-set models consist of discriminative local features from different images that are captured from various viewpoints, scales and lighting conditions."
"computation is a general phenomenon that involves data, specifications, programs, systems and devices. whilst the diversity of these components seems unlimited there are common factors that can characterise computation, for example: data representation and coding; levels of abstraction defined by operations; semantic models and logics for reasoning about behaviour; subcomponents and architecture; modularity and compositionality; and physical properties such as time and space. some taxonomic order can be attempted by first classifying the nature of the data."
"let us consider a practical example. in [tz07] we analysed two case studies of mass/spring/damper systems (a simple and an iterated system, respectively). in this paper we reconsider the first of these (see figure 1 )."
"k-mers, i.e. words of length k, are used in many basic algorithms for biological sequence comparison. word matches are used, for example, as seeds in the hit-and-extend approach to database searching and read mapping [cit] . in alignment-free sequence comparison, sequences are represented as word-frequency vectors to estimate distances or similarities between them, e.g. as a basis for phylogeny reconstruction [cit], see [cit] for reviews. similarly, word statistics are used to classify dna or protein sequences [cit], for datamining [cit] and for remote homology detection [cit] . it is well known that many word-based approaches produce better results if spaced words or seeds are used instead of the previously used contiguous words or word matches. that is, for a pre-defined binary pattern p representing match and don't-care positions, one considers only those positions in a word of the same length that correspond to the match positions of p ."
"by contrast, if a pattern p r is shifted against itself, only shifts between 0 and r − 1 need to be considered, to avoid double counting of shifts 1, for example:"
"first, we consider the general case of the data type c[x, a]. in section 2, we study the local uniform topology on c[x, a], which is generated by the family of pseudometrics"
"theorem (concrete computability): if, in addition to the assumptions in theorem 2 (and under some further reasonable assumptions), f is concretely computable, then so is φ."
"now in [jz11] another analog network n 2 is constructed from the same mass/spring/ damper system (figure 1 ) by eliminating the \"acceleration\" stream a from the network n 1, using the fact that this stream can be defined as a linear combination of f, x and v (3.9a). so n 2 contains only two streams x, v (other than the input stream f ) and network state function f"
remark 4.2.12 (assumption of shift invariance). nicholas james (personal communication) has succeeded in proving theorem 2 without the assumptions of shift invariance (iii) and closure of u under shifts (vi). details will be given in a future publication.
"to conclude the proof, we must consider one more point: is the fixed point v constructed above actually a stream? in other words, is it continuous as a function from ì to a m ? but this follows from the above construction of v, as an iterated limit of sequences of sequences of approximations, all of which converge locally uniformly, and hence preserve continuity (in fact local uniform continuity), as does the end result v."
"owing to its effective invariant properties, sift has been successfully used in various tasks, including object recognition/categorization, content based image retrieval and other computer vision applications. mikolajczyk and schmid [cit] have shown that, of several current widely used interest point descriptors, sift is the most effective in retaining consistency across wide variations in viewpoint and scale."
"by ( induction step: assume φ k is continuous. we must show that φ k+1 is continuous, i.e., prove (4.14) for k ← k + 1. put"
"remark 4.2.1 (openness of u ). we will assume, for convenience, that u is open. although the theory could be (re-)formulated without this assumption (by referring to the interior of u, when necessary), it is a reasonable assumption which smoothes the exposition."
"one aim of this paper is to use basic topology to model stream processing. the standard ideas and methods of topology constrain our models to streams that are continuous, and also (for now) total and deterministic."
"we omit proofs, except to remark that the equivalence of the systems of open bases in (i) and (ii) can be seen by observing that any neighbourhood of a function tuple u of the form (2.9) contains a neighbourhood of u of the form (2.10), formed by defining"
"all the tests are conducted on a robot equipped with a 1.83 ghz laptop (1g memory). the image captured by the monocular camera is 640  480 pixels. in each localization, it took about 135 ms to detect interest points and extract the features (modified hue-sift in this case) in the captured images and 85 ms second to find the correspondent location in the reduced database, which contains only a smaller number of distinct prototype feature descriptors. another 15 ms is spent on the probability computation. currently, the proposed algorithm can recognize more than four frames per second without optimization."
"in this paper, a simplified colour sift descriptor is designed and adopted to recognize natural landmarks for its effective invariant attribute. we should take advantage of the invariant attribute and keep the number of reference features in the database to a minimum to reduce memory. another desirable property of this natural landmark feature is that it can easily be trained in different environments. this paper is organized as set out below. the next section describes the problem of the existing method and the outlines of the proposed method. details of the environment modelling are presented in section 3, which involves modified local invariant features, dimension reduction, the roust rival penalized competitive learning (rpcl) clustering algorithm and the construction of a \"location features vocabulary model\" (lfvm) for each location. how the lfvms are used to localize a robot in a topological graph using the voting method and hmm is described in section 4. section 5 gives details about the results of the comparative experiments. the proposed descriptors are compared with several state-of-the-art descriptors including sift, colour sift, gloh and surf. conclusions and discussions are given in the last section, 7."
"an important aspect of hadamard's principle is that it can be viewed as making classical experimental physics possible. suppose, for example, that one wants to verify any of the well-known relations of classical physics -hooke's law or charles's law, for example -by taking measurements and drawing a graph of the relationship between the \"independent\" and \"dependent variables\" -force vs displacement of a spring in the first example, and temperature vs volume of a gas (at constant pressure) in the second. (the first of these two examples was used implicitly in our first case study.) the experimental results, and consequent graph, only make sense on the assumption that the function that one is attempting to plot is continuous, so that small discrepancies or inaccuracies in the inputs produce only small variations in the outputs. moreover, this is needed to guarantee repeatability of experiments. the stability of measurements in the presence of noise is an essential feature for a physical system to qualify as an analog computer."
"examples 3.1.4 (network stream transformers). operators of the form (3.1) arise naturally in modelling networks of modules or processors, operating in either continuous or discrete time, as in our two running examples (2.1.1) of analog networks and scas, where a q is the space of parameters from a, and c[ì, a] p as the space of input streams. in fact these examples have a common form: the semantics of the network n is given by a network stream transformer φ n . this is obtained in two different ways in these two examples:"
"lemma 2.3.7. the inverse limit topology on c[x, a] is the same as that generated by the inverse system consisting of the family (a) the sequence (u n ) converges locally uniformly to u,"
"concrete, based on representations constructed from ae, and abstract, independent of representations, and based on effective approximability by a high level imperative programming language whilecc * (that is, the while language with a \"countable choice\" operator and finite arrays). the equivalence between these was established in [tz04] . with theorems 1 and 2 in mind we prove:"
"in digital computation, at the heart of our theoretical understanding are countable sets of discrete data that can be faithfully coded by strings and natural numbers, since the classical theory of computability and complexity is founded upon the data types of strings and natural numbers. however, from earliest times, many computations concern analog processes involving physical quantities; streams of messages and signals in time; and objects and scenes in 3-dimensional space. physical models of data, distributed in time and space, can be found in hybrid embedded systems, analog computers of the first half of the twentieth century [sma01, cla10], and new and unconventional technologies for computation that involve (for example) quantum or dna systems. we must compute on uncountable sets of continuous data, modelled and represented by constructions with real and complex numbers, on scalar and vector fields. computations with continuous data require special computability theories, involving the approximation of functions on topological, metric, normed and ordered spaces of various kinds."
"the main step in proving this is to show that for any neighbourhood n k (u, ǫ) in the local uniform topology there is a k such that"
"note that this is a simple recursion for an a m -valued function, equivalent to the (m-fold) simultaneous recursion defining m a-valued functions given in [ttz09, §4.4]. df f(a, x, t) then f,ü is a stream, or rather a stream m-tuple, and in fact, the unique fixed point of the contracting operator"
"there are o(m · 2 ) triples (r, i, j) to be considered to modify the current pattern set p. for each of these triples, the oc is to be calculated for the pattern set that would be obtained by swapping i and j in p r . to this end, the modified pattern p r has to be compared to the m − 1 remaining patterns in p which, for each pattern comparison, involves o( ) shifts of two patterns against each other. in each shift, the number of common match positions is to be counted, which takes again o( ) time. thus, calculating the oc of the pattern set obtained by swapping two positions i and j in a pattern p r takes o(m · 2 ) time, so finding an optimal triple (r, i, j) to determine the next pattern set takes o(m 2 · 4 ) time. this step is repeated a certain number of times; for the pattern set that is finally obtained by this hill-climbing routine, the sensitivity is calculated. this whole procedure is repeated 5,000 times, and finally the set with the best sensitivity is returned."
"how the known approaches will perform in the new environments is a debated and open problem. the proposed approach is different from other existing ones. in our approach, it is not necessary to match new images with one or more pre-recorded reference images, but will be matched with the feature-set pooled within the corresponding place. our approach is motivated by an analogy with learning methods using the bag-of-words [cit] representation for text categorization and probabilistic localization [cit] . the idea of adapting text categorization methods to visual categorization is not new. for example, an object class recognition method is proposed [cit] for the unsupervised learning of invariant descriptors of image windows. however, in the training step of their method, distinct views of the same object class must be segregated into different categories. in our approach, test images, the view angles of which are significantly different from the training views, can be classified into the same class that they would be if these images belonged to the same location."
"to improve the current pattern set p, the hill-climbing algorithm implemented in speed looks at all triples (r, i, j) where p r is a pattern in p, and i and j are a match position and a don't-care position in p r, respectively. for each such triple (r, i, j), the algorithm considers the pattern set that would be obtained from p by swapping i and j in p r -i.e. by turning i into a don't-care and j into a match position. the oc is calculated for all pattern sets that can be obtained in this way, and the one with the lowest oc is selected as the next pattern set p. this is repeated iteratively."
"the implementation of our approach is called rasbhari (rapid approach for seed optimization based on a hill-climbing algorithm that is repeated iteratively). experimental results show that pattern sets calculated with rasbhari have a slightly higher sensitivity in database searching than pattern sets calculated with speed, while the run time of both programs is comparable. in alignment-free sequence comparison, we obtain more accurate phylogenetic distances if we use rasbhari to minimize the variance of n for the underlying pattern sets, than we obtained with the randomly generated pattern sets that we previously used. in a third application, we used pattern sets generated with rasbhari in the program clark-s [cit] for short read classification. the sensitivity of the classification could be improved in this way, while rasbhari is substantially faster than the method that is used by default for pattern generation in clark-s."
"probabilistic algorithms [cit] deal with uncertainties and sensor errors, hence they allow the robot to recover from the kidnapped problem, and have been widely applied in the global localization problem. the authors make use of the probabilistic integration of the appearance and odometry data [cit] to cope with the absence of reliable sensor information."
"further, from causality of f follows causality of φ by lemma 4.2.3, and from shift invariance of f follows shift invariance of φ by lemma 4.2.6; hence (by lemma 3.2.6)"
"recently, we proposed to use spaced-word frequencies instead of word frequencies for alignment-free sequence comparison [cit] . we showed that phylogenetic trees calculated from spaced-word frequencies are more accurate than trees calculated from contiguous-word frequencies. as in database searching, our results could be improved by using multiple patterns. in our original study, we used randomly generated multiple patterns of match and don't-care positions. in a follow-up paper, we studied the number n of spaced-word matches between two dna sequences for a set of binary patterns [cit] . our data suggest that minimizing the variance of n for pattern sets improves alignment-free phylogeny reconstruction."
"one of the main aims of this paper is to develop the two theories that analyse properties of networks processing digital and analog streams, introduced in [tz07, ttz09], from a common standpoint."
"and for all n note that for all n, v (n) k+1 is kτ -equivalent to v k and is a kτ -approximate fixed point of f, by lemma 3.4.3 and induction on n. (here causality of f is used again.) putting"
"for each extracted key-point, a local one is extracted, the sift descriptor is built in the intensity channel of the surrounding 16 16  block; then the corresponding 16 bins hue histogram is computed. the hue histogram vector of the key-point is normalized into a standardized unit length to reduce the scale effect. since the dominant hue information in an image patch plays a more important role than other hues which occupy a relatively smaller percentage, the 16 bins hue histograms are further refined by only preserving the top three hue bins. then the joint feature vector is defined in the form of a combination of a conventional 128 dimensional sift vector with the top three dominant hue components, within the 16 16  image patch centred at the key-point:"
"topological localization refers to the identification of the discrete location of the robot. traditionally, most systems [cit] ssociate the reference images with the corresponding locations in the training stage. they are concerned with matching the captured image against the set of model images according to similarities between the detected features and those in the reference images. the robot moves autonomously along the path while localizing itself by performing a comparison between the learned and acquired views. the place, the model image of which best matches the input view, is then considered to be the right position. in this approach, vision based topological localization is treated as an image retrieval task [cit] . the aim of the method is to determine the model view that is most similar in appearance to the current captured view."
"although most video sequences have large deviations from the path of the original exploration trajectories and some lighting conditions change in the environments, the hmm can eliminate previous classification errors and achieve a recognition rate of over 91%. the advantage lies in that the hmm uses recursive probability inference to incorporate the temporal context, which can be modelled by the transition probability matrix. hence, even when some individual views were misclassified, the robot visiting order during the test trajectory can be determined correctly."
we can describe the environment model as a group of distinct places and spatial neighbourhood linkages between them. the environment is defined as a topological graph where the nodes and edges are denoted as a collection of places and neighbourhood relationships.
"the same remarks apply to the concept of shift invariance (to be defined later, in §4.1), which is crucial in the proofs of theorem 2."
"here, each individual place associated with several representative images is modelled by a location-relevant lfvm. so each lfvm is comprised of a group of hue-sift prototype features, which are learned by a robust rpcl of interest points. for a current captured view q and its associated features, a group of prototype features between q and lfvm(i) (the model associated with the ith place), c(q,lfvm(i)), is computed. the observation likelihood,"
"now much of the work in § §2.1 and 2.2 remains valid when one restricts attention to the compact sets in the particular compact exhaustion (k k ). (see lemmas 2.3.6, 2.3.7, 2.3.9 and 2.3.10 below. but see also remark 2.3.8.)"
"consequently, if we are looking at sets p of m patterns with fixed weight w and lengths r, then minimizing the overlap complexity of p is equivalent to minimizing the sum"
"another characterisation of this topology on c[x, a] can be given using the notions of an inverse system of topological spaces, and the inverse limit of such a system [hy61, eng89] . so consider the inverse system consisting of the family of topological spaces"
"where i f, i [cit]  are the 128 dimensional components of the standard sift vector, and 1 2 3 h, h, h are the index of the top three dominant hue components. the joint feature vector that we denote as \"hue-sift\", is more robust than the traditional sift with regard to colour and illumination variations, which will be verified in the experiment section. the dominant hue bins are utilized as validation criteria with which to reject the misclassifications. thus, we can reduce those false matches that are similar in local structures but different in hue components. we can see the examples in fig. 7, where the hue-sift features could be obtained by correct correspondences, while in some blocks, some classical sift features yield false matches."
"to show the effectiveness of the proposed prototype hue-sift feature, we firstly compared the proposed descriptors （dominant hue-sift）with several state-ofthe-art descriptors including sift [cit], colour sift (rgb sift, here) [cit], gloh [cit] and surf [cit] . this approach is tested with three different video sequences per office environment. the average recognition rates in three environments are reported in table 1 from the results shown in table 1, it can be seen that the proposed dominant hue-sift and rgb-sift always perform the best due to the fact that they have more discriminative power benefiting from colour information and the discriminative feature (here the popular sift descriptor). the proposed dominant hue-sift significantly outperforms the original sift, with improvements from 7.6% to 12.6%, proving the importance of the dominant hue within a patch in addition to the local discriminative feature. so, those matches similar in local structure but different in hue are considered as false sift matches and are rejected. fig. 7 in sub-section 3.1 shows examples of which hue-sift features yield correct matches while some standard sift features give wrong decisions."
"be that as it may, our fixed point construction applied to scas is along the lines of kleene's construction in the proof of his first recursion theorem [kle52, thm xxvi], which in fact gives a justification of definition by recursion. note, however, that this is obtained as the limit of a sequence of partial streams, starting with the empty stream, whereas the fixed point in our proof of theorem 1 is obtained as a limit of a sequence of total streams, starting with an arbitrary stream. (at stage n, the approximations by these two methods give identical values at the first n places.) thus, kleene's framework involves partial functions, unlike the framework here and in [ttz09] . see, however, section 5."
"thus, theorems 1 and 2 are the basis for a general method of giving semantics to interesting classes of analog networks. the freedom to choose topologies appropriate to the physics of the problem, and work with conventional approximation methods, is an attractive feature of this method, which, we feel, makes up for the previous apparent neglect of suitable semantics for analog networks. this paper seeks to compare, and partially unify, theories of stream transformers on c[ì, a] for discrete and continuous time ì. it is motivated by models of network stream processing in [tz07, ttz09] . the methods are those of [tz04, tz05, tz07, ttz09] . we have tried to make this paper independent of these articles; however, the motivation and technicalities are best apprehended in the light of our entire work."
"the research of the second author was supported by a grant from the natural sciences and engineering research council (canada). the second author also appreciates the generosity of the computer science department of swansea university, which has hosted a number of his visits for the purpose of collaborating with the first author."
"in preparation for the investigation of the continuity of the fixed point in this section, we consider another property of operators: invariance under time shift."
"the geometric approaches that utilize metric maps allow the robot to keep track of its exact position with respect to the map's coordinate system. most geometric approaches rely on an extended kalman filter (ekf) that needs good statistical models of the sensors and their uncertainties. in some cases, situations where the robot travels over a bump (e.g. a cable lying on the floor) are difficult to model [cit] . this scheme is vulnerable to inaccuracies and expensive computation, especially in large-scale environments."
"it is quite a challenge for artificial systems to rationally reason with incomplete and uncertain information. we may sometimes make false classifications because of changes in illumination, dynamic environment changes or when the camera pose between the query image and the reference view vary largely. we utilize a probabilistic inference, a bayes filter, to tackle observation with large noise, regarding robot localization in the presence of measurements with low resolution."
"the states of a dynamic system are probabilistically estimated from noisy observations by a bayes filter. in such cases, the state at time t is denoted by random variables t x, the sequence of time-indexed sensor observations about the state is described by"
"next, based on the proposed dominant hue-sift features, we developed three methods and evaluate them in six different teaching building environments, with three different video sequences in each environment. the results are listed in table 2, where the three methods are indicated as \"hue-sift\", \"lfvm+voting\" and \"lfvm+hmm\". the \"hue-sift\" method is to handle location recognition as an image retrieval problem, representing each location based on the dominant hue-sift features in terms of the model views. the \"lfvm+ voting\" method represents each location using the lfvm, the database containing the learned discriminative prototypes from the dominant hue-sift features. the location is determined by the lfvm the prototypes of which were frequently classified as matching. the \"lfvm+hmm\" approach exploited the temporal context by examining the spatial relationships between the locations. in the voting scheme, the location that wins the highest number of matched interest points against the query view is considered to be the correct place. each view in the image sequence acquired by the walking robot is labelled with the index of the corresponding lfvm. from table 2, it can be seen that the proposed lfvm based on the hue-sift feature is more discriminative than the standard sift feature and is very effective."
"in this paper we will study how functions φ are specified as fixed points and computed by topological methods. typically we deal with fixed points of operators with contracting properties that are derived from equations describing a system. in a companion paper [tz11] we will study concrete and abstract computability models for functions φ on the data type c[x, a], and compare them."
"by the metrisability lemma (2.3.11), c[ì, a] is metrisable. we also assume, from now on: we will consider operators on the function space c[ì, a], mainly the form"
"based on these three concepts, cps can be defined as a kind of problem solving, with the problem itself (the structure of (a) the external problem representation and/or (b) the mental representation of the problem), or the process of its solution having to be formalized as a set of many highly interrelated elements, i.e., a complex system. [cit] the complexity of relations can be quantified by the number of variables related to each other: for example, the mental representation of a criterion y depending on a predictor x could be expressed as a binary relation r(y,x), whereas a dependency on multiple predictors could be represented as a relation of higher rank, e.g., the ternary relation r(y,x1,x2), and thus would be considered more complex. structures more complex than quaterny relations are assumed to have to be processed by either conceptual chunking or segmentation in order to not exceed human processing capacity [cit] . one famous example for cps-that can be considered complex because the structure of the external problem representation (see fig.1 ) is to be formalized as a complex system-is the tailorshop (see, e.g., [cit] ), a computer simulated scenario of a small organization involved in shirt production. originally programmed by dörner [cit] s on his calculator it was implemented on many platforms and used in a variety of contexts."
"our conception of cps is inspired by the pioneering works of dörner, especially by the concept of operative intelligence (dörner, 1986) [cit], emphasizing (a) information generation (due to the initial intransparency of the situation), (b) information reduction (due to the overcharging complexity of the problem's structure), (c) model building (due to the interconnectedness of the variables), (d) dynamic decision making (due to the dynamics of the system), and (e) evaluation (due to many, interfering and/or ill-defined goals). in unison with dörner we want to emphasize that in order to develop a sufficient understanding of the problems humans have to face in their everyday lives, research on problem solving has to further elaborate on complex problems, with both a large amount of possible actions for the problem solver, and a lot of uncertain and surprising consequences in naturalistic environments. the more we learn about the process of problem solving, the more we have to acknowledge the complexity of both the process and the kind of problems that are involved in realistic problem solving in naturalistic environments."
"theoretically, general intelligence may be defined as \"the global capacity of a person to act purposefully, to think rationally, and to deal effectively with his environment\" [cit] . originally, general intelligence as a concept was proposed to explain covariance between a wide range of cognitive tasks, and reasoning as well as problem solving have traditionally been a part of the definition [cit] . research on intelligence is about the cognitive processes involved in solving tasks and problems and thus may con-"
"first, we applied the algorithms with random selection of one city as depot. experiment is conducted on each dataset and the number of salesmen; one city is selected as a depot randomly. each experiment was done 100 times."
"based on the results in table 1 and table 2 it can be seen the total travel distance in table 2, has the better total distance. so, the selection of the depot plays a role in generating a minimum distance. also, for different number of salesmen with same dataset, different city can be selected as depot that produce the minimum distance."
"in aco, artificial ants are used to construct solutions in a probabilistic way based on pheromone values. aco has undergone several developments. some of the aco algorithms have been developed for tsp, such as ant system (as) [cit], elite ant system (eas) [cit], max min ant system [cit], and ant colony system (acs) [cit] ."
"denotes the probability of ant k in city i will travel to city j at t th iteration. ! !\" is the amount of pheromone trail in the arc (i,j). ij η is the heuristic value of the arc (i,j), when all ants have completed their journey, pheromone trail will be updated as follows [cit] :"
"each interaction with the system may be considered generating an instance that could be stored in memory to implicitly guide future decisions in the face of similar system states-under certain circumstances (factors like time pressure, stress, uncertainty, and high cognitive load may foster the reliance on instance knowledge. [cit] ) . in addition to knowledge about instances, systematic strategy use may allow inference of knowledge about the system structure (see section on decision making strategies) which might come in handy under different circumstances (e.g., when trying to reach system states never seen before, maybe due to a large problem space and insufficient expertise)."
"at the beginning of any tour, the ants start at a depot and choose the next cities consecutively. the tour continues until all the cities are visited and the ants turn back to the depot again. every ant selects the next city independently. the city selection rule is based on acs. in acs, ants will depart from city i to city j, where city j is chosen based on the following equation [cit] :"
"to summarize research on information reduction, in cps omitting irrelevant task components and finding a parsimonious representation of the problem may enable and foster the search for a solution to a complex problem. because the search for a solution based on a viable parsimonious model of the problem involves processes like inductive and deductive reasoning, that are commonly subsumed under the concept \"intelligence\", the next section of this article will review the empirical and theoretical findings on how different aspects of intelligence influence cps before we will integrate the findings reported so far in a process model of cps."
"when it comes to gathering information (e.g., when the structural knowledge about the problem proves to be insufficient), some strategies may be especially useful for generating viable structural knowledge about the system. [cit] pointed out, systematicity in strategy use allows a problem solver to coherently infer the consequences of single interactions, i.e., to build viable structural knowledge about parts of the system structure. [cit], to \"vary one thing at a time\" (while setting the other variables on a constant value like zero)-commonly referred to as the votat-strategy-may be a strategy useful to systematically identify the effects of independent (exogenous) variables on dependent (endogenous) variables in certain scenarios (especially when each exogenous variable was contrasted to the other ones at least one time. setting the increments of all input variables to a value of zero from time to time may facilitate the detection of eigendynamics and indirect effects). systematic strategy use and generating (as well as using) structural knowledge might be especially important in complex systems when there is no (or even cannot be) sufficient implicit knowledge about a correct solution of the problem. but as human cognitive resources are limited, even detailed and extensive structural knowledge about all the aspects of a complex system may not be fostering cps per se as they may overcharge the human working memory. based on this crucial aspect of complex problems the following section proposes the most influential theories on how and why information reduction is an essential aspect of cps."
"the definition of cps proposed and applied in this article is based on the constitutive concepts \"complexity\", \"problem\", and \"problem solving\" which in turn are understood as follows: [cit] 1. the complexity of a system 2 may be defined as the number of elements and relations of the system [cit] . as dörner (1989) stated, \"the complexity of a domain of reality is the higher, the more features there are and the more these features are interdependent\" (dörner, 1989, p. 60, translated by the authors). 2. a problem is considered to exist, \"when a living creature has a goal but does not know how this goal is to be reached. whenever one cannot go from the given situation to the desired situation simply by action, then there has to be recourse to thinking\" [cit], p.1) . dörner has gone into more detail when he emphasized that \"barriers\" between the given situation and the desired goal state, i.e., the lack of knowledge, can be further classified according to the amount of (a) ignorance of the means/operators applicable, and (b) lack of concreteness concerning the goal state (see dörner, 1976, [cit] . 3. problem solving can be defined as successfully searching for an operation or a series of operations in order to transfer the given actual state of the system to a goal state [cit] ."
"the multiple traveling salesman problem (mtsp) is a generalization of the traveling salesman problem (tsp), which involves determining a set of routes for m salesmen who all start and end at a city called depot [cit] . each salesman will have to visit different cities exactly once starting from depot and turn back again to depot. mtsp has some possible variations based on number of depots, number of salesmen, time windows, and other restrictions."
"there is a large quantity of research on differences between experts and novices of a certain knowledge domain concerning the influence of different kinds of domain-specific knowledge on cps. in fields as different as reading, writing, arithmetic, mechanics, policies, jurisdiction, management, or debugging [cit] there has been a lot of research on the processes and kinds of knowledge involved in cps. what could have seemed to be a turning away from general aspects of problem solving in favor of more domain-specific problem solving strategies nonetheless produced a deep insight in some general effects of expertise on general problem solving."
"research on cps produced a lot of characterizations and operationalizations of complex problems [cit], but up to now there has not been a definition of complex problems commonly accepted in the scientific community [cit] . there is an ongoing debate about (a) what should be considered complex in cps and (b) how complexity might be measured in detail [cit] for a discussion)."
"k-means and crossover aco algorithm are applied and tested on several datasets from tsp problems of tsplib, namely att48, pr76, and rat99. the parameters are as follows."
"in summary, information processing theories on human problem solving have proposed some useful ideas and assumptions that are most relevant when building a process theory of cps. e.g., they try to explain when information generation and elaboration takes place, how it leads to viable internal representations (or models) of the problem system, and how the internal representation of the problem determines the solution strategies applicable. especially the distinction of structural knowledge and knowledge about instances proved to be very fruitful for thinking about the influence of expertise on cps. the next section will further elaborate on this distinction, and propose the most influential theories on how different kinds of knowledge may influence the process of cps."
"this process theory of cps summarizes what is known about the most important aspects of cps and is based on the theoretical and empirical contributions of the interdisciplinary field presented in the previous sections. as cps is a rather abstract concept, further research is needed to specify the process of cps concerning concrete operationalizations of complex problems (e.g., handling a complex mobile phone may be represented in other ways than regulating an economic system or managing a tailorshop). concerning this, it seems to be a fruitful approach to build cognitive models of the cps process (e.g., [cit] ) in order to develop a deeper understanding of cps processes taking place in real life. but even on a more abstract level our theory on the cps process may be subject to further research. it may be seen as a starting point for further experiments, in order to gradually improve our understanding of what cps is and how it works (e.g., experimental psychology may further contribute knowledge about variables or interactions with a significant impact on the process of cps). psychometrics may contribute to a better understanding of cps by developing reliable and valid measures for the processes that are assumed to be important for efficient and intelligent cps (greiff, in press; wüstenberg, greiff, & funke, in press ). those measurement devices in turn can be used to test process theories on cps in more detail."
"second, we used structured manner in depot selection. the selection of depot in a structured manner is done by setting the entire city into a candidate depot on each data and the number of salesmen. after travel distance of each depot are calculated, then select depot that has the minimum distance. the city depot will be called the mtsp depot. furthermore, experiment is repeated 100 times using the mtsp depot for each dataset and the number of salesmen. table 2 is the result of k-means and crossover aco implementation with the selection of depots in the structured manner. it appears that in the same dataset with different salesmen numbers, the city that selected as depot can be different. the total value of travel distance tends to increase as the number of salesmen increase, except in pr76 with 3 salesmen. the total distance travelled in pr76 with 3 salesmen is smaller than 2 salesmen."
the acquisition of structural knowledge about complex systems seems to depend on conscious thought and mental effort (corresponding to the germane load; [cit] ) . the acquisition of structural knowledge thus may be fostered by the intention and the opportunity to explore the system before or instead of having to achieve a certain goal
"we compared best solution results using k-means and crossover aco in table 2 and best solution using k-means and aco. the experiment using k-means and aco was conducted 100 times with table 3. based on table 3, it can be seen that the result of solving mtsp using k-means and crossover aco has smaller value than k-means and aco for each case."
"to divide the cities, k-means begins by setting the set of k centroid, which is randomly selected. within each iteration, each city will assign into particular cluster based on the euclidean distance between cities with the nearest centroid. after that, the centroid will be recalculated using the equation as follows [cit] :"
"so, during the process of cps, problem solvers seem to increasingly rely on strategies that are efficient and ecologically rational, i.e., they (1) rely on the correct solution if it is known automatically (instance knowledge), else wise (2) search for a solution based on the current problem representation (structural knowledge), or (3) gather new information about the problem (e.g., via random or systematic interaction with the system, via asking an expert, etc.). this conception seems to be consistent with the \"elshout-raaheim-hypothesis\" [cit], stating that correlations between problem solving and intelligence may be dependent on knowledge about the system in an inverted-u-shaped way (i.e., the correlation may be minimal when prior knowledge is very high or very low as consequently"
so after these considerations about how efficient cps may look like and what facets of intelligence may influence the cps performance we want to proceed by integrating the contributions of all the fields of research mentioned above in a process theory of cps.
"compared to tsp, mtsp is better in modeling the problems in everyday life as it can handle more than one salesman. since tsp belongs to the class of np-hard problems [cit], it is obvious that mtsp is also an np-hard problem. some metaheuristics approach had been performed to solve mtsp, such as modified genetic algorithm [cit], modified ant colony [cit], sweep algorithm and elite ant colony optimization [cit] ."
"so after having considered different kinds of knowledge that can be assumed to have an influence on cps, in order to build a process model of cps it seems promising to further examine (a) the circumstances determining which kind of knowledge (e.g., structural or instance based) problem solvers usually rely on to make their forecasts, plans and decisions, and (b) what strategy is chosen when no knowledge about the correct solution to a problem is available yet. answers to this question were proposed in the field of research on decision making and will be reported in the next section."
"where j n is the total number of cities in cluster j, j c is the centroid of cluster j, and q x is the q-th city belongs to cluster j. the process of grouping the cities to the nearest centroid and centroid recalculation will be repeated until the termination conditions of k-means are met."
"in this scenario, the problem solver takes the role of managing a small tailorshop, deciding what actions to take or what information to gather, aiming at the maximization"
"research on decision making has developed a set of decision making strategies containing viable strategies and heuristics for (a) generating relevant information and (b) making good forecasts and decisions in complex environments. when the goal is to specify an input or a series of inputs in order to regulate certain output-variables of a complex system, each possible input vector (e.g., an action in a complex scenario) can be considered an option, with several expected consequences (e.g., changes in the output variables). each consequence may have a subjective utility and an expected probability specific to the current context (i.e., the consequences of an action may be of different perceived use and certainty, dependent on factors like the perceived features of the situation). in complex scenarios there seldom can be an exhaustive evaluation of all possible options and their weighted consequences (due to time pressure and the tremendous amount of variables that would have to be considered). instead, decisions have to be based on strategies us-ing less information and only a small amount of computation (e.g., by taking the option which has the highest value on the most important consequence -the so-called \"take the best\"-heuristic). with regard to cps, it is of special interest to note that simple heuristics like \"take the best\" or simple tallying can actually achieve higher accuracy in predicting the best outcome than more complex algorithms under certain circumstances -e.g., low predictability of a criterion, combined with small sample sizes relative to the number of available cues, and dependency between cues [cit] ). so when it comes to predicting new observations (instead of just fitting data already observed) sometimes the \"less-is-more\"-approach holds to be true and it proves to be more accurate to make decisions based on only one good reason (i.e., \"take the best\") than using tallying, multiple regression or even heavy-weight nonlinear strategies like neural networks [cit] ). therefore, the question is not as much which strategy is the best but which is the best in a certain environment, i.e., under certain conditions. the applicability and/or the usefulness of some strategies-their ecological rationality [cit] )-can depend on the existence of prior experiences with the system, on the amount of detailed structural knowledge about the values and weights, on knowledge about the alternatives available, etc. thus, memory on the one hand constrains the set of heuristics applicable (each long-term and working memory can be considered to constrain what is possible in a certain situation) and on the other hand \"selects\" heuristics that are likely to yield accurate decisions in a mostly unconscious process [cit] ). furthermore, the ecological rationality of a heuristic in a given environment is assumed to depend on factors like the structure of the environment and feedback, amongst others. [cit], the ecological rationality can be learned by the decision maker via simple reinforcement learning. when goal-oriented decisions are dependent on former decisions and their consequences in an environment that may change both spontaneously or as a consequence of earlier actions, it is commonly referred to as dynamic decision making (ddm; [cit] ) . [cit] has given an overview of the research on ddm, stating that on the one hand human performance usually can be considered suboptimal, but that on the other hand systematic learning effects were found in almost all of the studies reviewed."
"even though instance-based learning often leads to successful system control (e.g., in systems like the sugar factory), it is of limited transferability as it does not involve information about the properties of the system structure [cit] . see fig. 2 for the difference between knowledge about (a) an instance of a system and (b) the structure of the system. structural knowledge is transferable and allows for building expectations about the consequences of certain decisions and actions in a given situation. it may be action-guiding even in hypothetical situations or in situations never encountered before."
"1. at first, the problem solver has to acquire knowledge about the problem. a. the problem solver is assumed to explore the system's behavior using a strategy that (a) she or he knows of and (b) seems to be most ecologically rational to her or him (e.g., random or systematic interaction with the system, reading the instructions, asking an expert, etc.). b. the exploration leads to (a) knowledge about the system's states and the actions taken (instance knowledge) as well as (b) an internal representation of the problem, containing the most important elements and relations of the system (structural knowledge) which usually is inferred from the instance knowledge. c. as the capacity of the problem solver's working memory is limited, the internal representation is object to information reduction. relations and elements that prove to be less relevant for system control in the course of exploration are assumed to be omitted in order to allow more efficient planning and forecasting. 2. when the problem solver has a certain amount of knowledge about the problem that has to be solved, she or he is assumed to apply the knowledge in order to reach her or his goals. a. the problem solver is assumed to us her or his internal representation to make forecasts about the system's dynamics in order to decide (a) if she or he has to intervene and (b) what intervention will have acceptable consequences in the current situation. when the current situation cues the correct intervention immediately (due to instance knowledge), the problem solver is assumed to rely on her or his instance knowledge instead. b. monitoring processes are assumed to detect (a) the progress in solving the problem and (b) the implications of feedback from the environment for the problem representation. when the problem representation proves"
"crossover is one of the operators in the genetic algorithm. crossover is a process of creating a new individual (heredity) through a combination of randomly selected genetic material from two or more parental chromosomes [cit] . if the offspring obtained are better than the parents, then the offspring will replace the parents. the crossover method used in this paper is 2-point crossover."
"the pheromone before and after the update become 2 parental chromosomes. parent 1 is a chromosome consisting of pheromones prior to the update. parent 2 is a chromosome consisting of updated pheromones. apply 2-point crossover between two parents. two child pheromones will be formed by crossover. use both pheromones to form route. if the result is better, then the pheromone will be used in the next iteration."
"in times of increasing globalization and technological advances, many problems humans have to face in everyday life are quite complex, involving multiple goals as well as many possible actions that could be considered, each associated with several different and uncertain consequences, in environments that may change dynamically and independent of the problem solvers' actions [cit] . in order to solve complex problems, people usually have to acquire and to apply knowledge about complex systems concerning the systems' structure and dynamics [cit] . examples for complex problem solving (cps) are easily found, e.g., using unknown complex technical devices (like a new mobile phone, a computer, a vending machine, etc.), managing complex organizations (like corporations or communities) or making predictions in complex environments (like forecasts of the weather, political elections or the stock market, etc.). [cit] s, when there was a shift of emphasis from simple, static, well-defined and academic problems (like the tower of hanoi or items of classical intelligence tests), to more complex, dynamic, ill-defined, and realistic problems [cit] . since then, research on human problem solving focused on interviewing experts of certain knowledge domains, on studying the effects of expertise on problem solving activities and decision making, or on simulating complex problems 1 based on real systems humans could have to deal with in their daily lives (like planning a day, managing an organization, fire fighting, and so on). along with more complexity in research on problem solving new questions arose: how does expertise and prior knowledge influence problem solving in complex situations? are there certain strategies especially useful for coping with complex problems? how is a complex situation represented in the human mind with its restricted capabilities? which facets of intelligence are most important for solving complex problems? some of these questions were addressed by different fields of research (e.g., research on problem solving, on expertise, on information reduction, on decision making, and research on intelligence), but in spite of a lot of fruitful research on cps in these areas, up to now most of this research has been conducted with a focus on empirical data mining rather than theoretical considerations [cit], without a clear-cut definition [cit] commonly accepted in the scientific community. the article at hand wants to contribute to the solution of this shortcoming: after summarizing the most important empirical and theoretical contributions to the field, we want to come up with a process theory of cps based on a formal definition, applicable to the interdisciplinary field. we want to consider (a) what is known about the most im-"
"based on table 1, the total value of travel distance tends to increase as the number of salesmen increases. however, in pr76 with 4 salesmen can result a smaller total distance compared to 3 salesmen."
"generally, traditional intelligence tests, aiming primarily at speed and quality of human symbol processing (i.e., fluid reasoning) as well as working memory capacity, were criticized for their primary focus on the results instead of the process of efficient problem solving behavior (dörner, 1986) . [cit] criticized that there may be more complex \"expertise abilities\" [cit] ) different from fluid reasoning, working memory and cognitive speed, which are not adequately addressed for by the tests that are assumed to indicate human intelligence. [cit] stated that the most important differences between the demands of classical tests for measuring intelligence and complex problems were the (1) polytelic situation, the need for an (2) active search for relevant information, for (3) specifying concrete goal states and for (4) choosing productive actions, as well as for (5) with his concept of operative intelligence dörner (1986) emphasized the importance of examining not only speed and precision of some of the basic intellectual processes, but also the more formative aspects of problem solving, for example (1) circumspection (e.g., anticipation of future and side effects of interventions), (2) the ability to organize cognitive operations (e.g., knowing when to do trial-and-error and when to systematically analyze the situation at hand; when to use exhaustive algorithms and when to rely on heuristics, when to incubate an idea etc.) or (3) the availability of heurisms (e.g., being able to build helpful subgoals, to constrain the problem space efficiently). this list of examples is not exhaustive, but it gives an idea of what is meant by the \"operative\" aspects that are not adequately addressed by traditional intelligence tests but may still be considered relevant for an organized course of intellectual processes (dörner, 1986) . with its explicit focus on gaining and using information and knowledge about the cognitive operations adequate, operative intelligence can be considered one of the most relevant expansions of intelligence as it is measured with current measurement devices:"
"these characteristic features of complex problems and the corresponding facets of cps [cit] ) can be considered a fruitful starting point for measuring operative intelligence, which in turn might be the most important determining factor of cps performance."
where f is the total cost (distance) of the best solution that has been found and ρ is the evaporation coefficient. then min τ is defined as follows [cit] :
"1. information retrieval and information integration: the problem solver needs a model adequately representing the system and the goal state to aim at. therefore she or he has to systematically generate, gather, and integrate information to adjust this model to the system. 2. goal elaboration and goal balancing: the problem solver has to specify and substantiate the often vague and global goals she or he wants to achieve. if some specified goals turn out to be contradictory, she or he has to find a satisfying trade-off or balance in only partially reaching the goals. 3. action planning and decision making: the problem solver has to decide what actions to execute, i.e., what decision making strategies to apply (see section on decision making strategies), and which kind of knowledge to rely on (see section on expertise). by forecasting future developments given the system's prior states and her or his own actions she or he can efficiently plan her or his next steps (e.g., chains of consecutive actions with each action building on the results of the previous one). 4. self management: the problem solver may have to face time pressure, stress, and frustration as well as conflicts between his inner values. she or he has to manage these non-cognitive affordances by either changing the system or his own behaviors and habits."
"the k-means and crossover aco methods can be implemented to solve mtsp. k-means is applied to divide the cities that each salesman will visit. then, the crossover aco is applied on every cluster from k-means results to get the optimal travel route from each salesman. in this paper, the algorithms are applied on 3 datasets from tsplib, they are att48, pr76, and rat99 with the number of salesmen are 2, 3, 4, and 8. the implementation results show the city selection as depot can affect the total distance of the salesmen tours. the computational time required to complete mtsp using k-means and crossover aco will increase as the number of salesmen and number of cities. the resulting mtsp solution using the k-means and crossover aco methods has smaller value than the k-means and aco methods."
"in this paper, mtsp will be solved using combination of k-means and crossover aco based on latah [cit] . k-means is one of clustering method, which divide a set of data into a set number of clusters. ant colony optimization (aco) is an optimization algorithm developed by prof. [cit] s. this algorithm is inspired from ant colonies social behavior in finding the shortest path from their nest to a food source [cit] . in aco, pheromone is an important aspect that plays role for ants to choose the path to be traversed. when the intensity of the pheromone on a path is too high compared to other paths, the path tends to always be chosen by the ant or the path will continue to be exploited in the selection of path while the other path will be rarely passed. to improve the exploration of ants in search of the path, pheromone crossover is implemented so that the exploitation and exploration of ants in choosing the path will be more balanced [cit] . crossover is an operator of the genetic algorithm [cit], which involves two parental chromosomes to obtain a descendant (child) chromosome."
"after reviewing some of the most important fields of research on cps, and based on the definition given above, we are now going to summarize the interdisciplinary findings in a process theory of cps, concluding with a short outlook for upcoming research. cps can be understood as the process of solving problems that have to be considered \"complex\" (i.e., containing many highly interrelated elements). for instance, every scientist, who wants to describe, explain, and predict a complex system by means of her or his hypotheses (containing a parsimonious but viable subset of all variables possibly relevant) might be facing a complex problem. a mayor of a city as well as a manager of an organization or a policy maker trying to get rid of climate change, each may be considered as having a complex problem to cope with. trying to make a modern computer do what it is supposed to can turn out to be a complex problem as well as changing certain settings of an unknown mobile phone device. the process of cps usually consists of different phases: (1) knowledge acquisition and (2) goal-oriented knowledge application [cit] . usually a problem solver switches between these phases in a complex way:"
"1. human problem solving starts with constructing an internal representation of the external problem statement, a \"problem space\" (i.e., a set of possible states of the problem, given the initial state, the applicable operators, and certain goal states). which operators can be considered applicable might be different for problem solvers of different expertise and intelligence [cit] . 2. given an internal representation of the problem, a method for reaching the current goal is being searched for. general searching algorithms (like \"hill-climbing\", or \"means-end-analysis\") are distinguished from more domain specific methods (like \"take the hammer to get the nail into the wall\") 3. using a method can change the external problem as well as the internal representation. of course, changes in the environment or the consequences of a method may lead to new (sub-)problems or new possible solutions. methods also can be aborted when metacognitive processes do interfere. when a method does not lead to a goal state, (1) another method can be tried, (2) the internal representation may be changed, i.e., the problem may be reformulated, or (3) the attempt of solving the problem may be aborted."
understanding how networks of interconnected neurons can learn to solve tasks using only information that is spatially and temporally local to individual neurons is an active area of research in both neuroscience and machine learning [cit] . prior research along these lines have proposed and studied specific local learning rules [cit] or approximated existing non-local algorithms (such as backpropagation) with more biologically plausible variants [cit] .
"this is very similar to the optimization problem we derived in the previous case, with the only difference being that the covariances are replaced by cross expectation or correlation. thus, finding the two angle bisector hyperplanes is the same as finding two vectorsw a andw b in d+1 such that the cross expectation of the projection of class c − points on these vectors is maximized while keeping the cross expectation of the projection of class c + points on these vectors at zero. again,"
"our approach is to directly optimize a synaptic weight update rule using a meta-objective that maximizes performance on a suite of semi-supervised learning tasks. additionally, we design our learning rule to be biologically plausible: it is neuron local, meaning updates to weights depend only on the corresponding pre-and post-synaptic units; it further does not use weight tying between feedforward and feedback passes during training [cit] . these design features enable the update rule to be applied to train models with different depths or widths, and to be applied to new datasets. in contrast to previous work [cit], our approach is more biologically plausible, simpler, and tackles semi-supervised rather than unsupervised learning. we show that meta-learned biologically plausible learning rules for semi-supervised learning can outperform supervised learning on 2d toy tasks, and have the ability to generalize to unseen tasks."
"the rest of this paper is organized as follows: 1 we describe our algorithm in section ii. section iii presents some analysis that brings out some properties of the angle bisectors of the clustering hyperplanes. based on these results, we argue that our angle bisectors are a good choice for split rule at a node while learning the decision tree. experimental results are given in section iv. we conclude this paper in section v."
"in this paper, we present a new decision tree learning algorithm, which is based on the idea of capturing, to some extent, the geometric structure of the underlying class regions. for this, we borrow ideas from some recent variants of the support vector machine (svm) method, which are quite good at capturing the (linear) geometric structure of the data."
"we also note here that neither of our angle bisectors scores high on any impurity based measure; if we use either of these hyperplane as the split rule at the root, both child nodes would contain roughly equal number of patterns of each class. this example is only for explaining the motivation behind our approach. not all classification problems have such a nice symmetric structure in class regions. however, in most problems, our approach seems to be able to capture the geometric structure well, as seen from the results in section iv."
"simulation results in figure 3 show that when using ifdd, the cache hit is higher than the other strategies. we also observe that, regardless of lce or ifdd or popular strategy, the cache hit with the increasing cache capacity increases significantly. however, the effect of the increase under ifdd is more significant. this suggests that the ifdd strategy is more sensitive to the increase in cache capacity. [cit] found that the content should be cached in the network core routers, but [cit] came to the opposite conclusion. our simulation results indicate that the cache hit is very sensitive to the degree of dispersion of interest, especially in the ifdd. with the increase in the locality of interest, replicas are more centralized towards the edge of the network when ifdd is applied. with the α reduction, interests tend to be scattered. in this case, the probability that the content is cached to core routers increases and the caching strategy will gradually fail."
"the main purpose of in-network caching is to improve the reusability of the content, reduce the processing delay of the consumer request, and save the network bandwidth. a typical way is to save bandwidth overhead but with caching costs. cdn uses the centralized control method to collect the data, calculate the popularity of content, and then coordinate the data distribution on each router. this method cannot be applied to ccn because ccn routers are further away from each other than cdn. therefore, the overhead of using this method may offset the benefits of in-network caching in ccn."
− is the number of points falling in set s t r . we choosew 3 orw 4 to be the split rule for s t based on which of the two gives lesser value of the gini index.
"the feature of locality was ignored in caching strategies on cdn [cit], p2p [cit], web caching system [cit] and social network (sn) [cit] . these strategies usually only considered the content popularity. the existing caching strategies in ccn also only exploited the content popularity without the consideration of the locality. [cit] discussed that the locality is hardly observable even within culturally homogeneous regions, but it could be observed partly in large systems serving different linguistic/cultural communities and even in limited geographical regions."
"3) we carry out simulations for evaluating ifdd on ndnsim platform by varying kinds of system parameters. we found that both the network topology itself and the interest distribution have a considerable impact on network performance. the simulation results suggest that an effective network cache allocation mechanism should consider various system parameters, such as application types and network topology."
"based on these factors, the existing caching strategies could be divided into two categories: the collaborative approach and the non-collaborative approach. the collaborative approach usually adopts the network characteristics as the design factors and formulates cache allocation into a global optimal/suboptimal problem. this approach pursues the overall optimal/suboptimal caching performance usually with higher decision cost. therefore, some researchers thought that this overhead may offset the benefits of network caching [cit] . the non-collaborative approach is mainly based on the simple random caching method or based on statistical method to independently make caching decision. in general, simple random approaches, while with low overhead, may result in a large amount of redundancy in the cache. thus, they may not be suitable for highly dynamic ccn environments."
"consider the 2-d classification problem shown in fig. 1, where the two classes are not linearly separable. the hyperplane learned at the root node using oc1, which is an oblique decision tree algorithm that uses the impurity measure of gini index, is shown in fig. 1(a) . as can be seen, although this hyperplane promotes the (average) purity of child nodes, it does not really simplify the classification problem; it does not capture the symmetric distribution of class regions in this problem. fig. 1(b) shows the two clustering hyperplanes for the two classes and the two angle bisectors, obtained through our algorithm, at the root node on this problem. as can be seen, choosing any of the angle bisectors as the hyperplane at the root node to split the data results into linearly separable classification problems at both child nodes. thus, we see here that our idea of using angle bisectors of two clustering hyperplanes actually captures the right geometry of the classification problem. this is the reason we call our approach \"geometric decision tree (gdt).\""
"in a small-scale network, the content is less and the distance of routers is relatively closer. in this case the caching strategy cannot play an effective role. when the network size becomes larger and the distance between the consumer and the provider increases, the network cache is far from satisfying the needs of the content. then the caching decision strategy gradually highlights the advantages. in the selected four topologies, the number of nodes is 278 (topo-1221), 162 (topo-1755), 920 (topo-3697), 624 (topo-7018) respectively. among them, topo-1755 and topo-3697 are similar. they both have long request paths and smaller node degrees. but topo-1221 and topo-7018 have higher node degrees."
"in this paper, we have presented a new algorithm for learning oblique decision trees. the novelty is in learning hyperplanes that captures the geometric structure of the class regions. at each node, we have found the two clustering hyperplanes and chosen one of the angle bisectors as the split rule. we have presented some analysis to derive the optimization problem for which the angle bisectors are the solution. based on this, we argued that our method of choosing the hyperplane at each node is sound. through extensive empirical studies, we showed that the method performs better than the other decision tree approaches in terms of accuracy, size of the tree, and time. we have also shown that the classifier obtained with gdt is as good as that with svm, whereas it is faster than svm. thus, overall, the algorithm presented here is a good and novel classification method."
"(ii) the cache in ccn is more common, dynamic and diverse than in cdn. by common, we mean that caching, the basic network function in ccn, should support all kinds of applications and all users, unlike cdns only for certain applications and specific users. therefore, the caching methods proposed in both cdn and similar networks are not fully applicable to ccn."
"meanwhile, many attack models have been found in ccn. among of them, pollution attack is a most important one that is a paradigm of distributed denial of service (ddos). this kind attack works mainly by sending invalid or illegal requests to occupy component resources, such as pit and cs, leading to the failure of these components. when cs is attacked, attackers can make the router cache filled with the content that is not consistent with the goal of the caching strategy. under this attack, the caching strategy may fail. therefore, the network performance is compromised."
"in our simulation, the attack starts at the 20 th second from the beginning of the simulation and lasts 10 seconds. these malicious nodes send requests at a higher rate twice than normal nodes. that is, these contents are more ''popular'' so that they have a higher caching potential. malicious providers respond to these requests with useless contents. these contents are more likely to occupy the cache to achieve the goal of the attack."
"t he decision tree is a well-known and widely used method for classification. the popularity of the decision tree is because of its simplicity and easy interpretability as a classification rule. in a decision tree classifier, each nonleaf node is associated with a so-called split rule or a decision function, which is a function of the feature vector and is often binary valued. each leaf node in the tree is associated with a class label. to classify a feature vector using a decision tree, at every nonleaf node that we encounter (starting with the root node), we branch to one of the children of that node based on the value assumed by the split rule of that node on the given feature vector. this process follows a path in the tree, and when we reach a leaf, the class label of the leaf is what is assigned to that feature vector. in this paper, we address the problem of learning an oblique decision tree, given a set of labeled training samples. we present a novel algorithm that attempts to build the tree by capturing the geometric structure of the class regions."
"most existing strategies in traditional networks adopted the simple random way to determine whether the content should be cached. these strategies include leave copy everywhere (lce), leave copy down (lcd), move copy down (mcd), leave copy probability (lcp), randomly copy one (rc one), probabilistic caching (prob cache) etc. lce is the default strategy which is available on the ndnsim platform. in this strategy, replicas of the content are cached on all routers in the path during the transfer of content from producers to consumers."
"this simulation assumes that there exists a certain percentage of malicious consumers and producers. the locations of these nodes are completely random. the proportion of the total number of malicious nodes accounted for 10%. in the absence of the attack, these nodes are not different from the normal nodes."
"now, let us assume that we have enough samples from both classes, so that we can assume that the empirical averages are close to the expectations. we can rewrite the objective function in the optimization problem given by (11) as"
"3, where d is the dimension of the feature space. on the other hand, svm solves a quadratic program whose time complexity is o(n k ), where k is between 2 and 3 and n is the number of points. thus, in general, when the number of points is large compared to the dimension of the feature space, gdt learns the classifier faster than svm."
"for the inner loop, we use a batch size of 10 and train for 50 steps. we compare supervised learning from 10 labeled examples (using backprop) against semi-supervised learning using the same labeled examples and unlimited unlabeled examples using our learned update rule (fig. 2, left) . we see that the learned update rule has learned to leverage unlabeled examples, achieving higher generalization accuracy. in figure 2 (right), the decision boundaries resulting from applying the learned update rule are nonlinear and better aligned to the underlying data distribution."
"semi-supervised learning (ssl) refers to a class of approaches that use unlabeled data to improve supervised learning. one approach to ssl starts by learning an unsupervised representation using unlabeled data, followed by fine tuning the model on a small labeled dataset [cit] . another common approach to ssl uses unlabeled data during training by adding additional loss terms, for example via generative modeling [cit], entropy minimization [cit], small perturbations [cit], or creating labels for nearby unsupervised examples [cit] . our meta-training procedure consists of a learning loop (applying the learning rule to update the weights of a network), which itself is wrapped in a meta-learning loop (fig. 1, left) . the training procedure involves iteratively improving the learning rule in the meta-learning loop, where each iteration involves training and evaluation of the learning rule on a sampled task. in a single iteration of meta-training, we sample data from a semi-supervised learning task (discussed below), initialize a base network with random weights, and perform a fixed number of learning iterations where the learning rule is applied to update the base network weights. the inner model (the model the learned update rule is training) is a 1-hidden layer fully connected neural network in all experiments, with feed-forward weights w . we introduce two additional sets of weightsw andw in addition to the feed forward weights w (fig. 1, right) . these two sets of auxiliary weights help propagate error top-down in the network (w ) and communicate lateral signals between neurons in the same layer (w ), and are exclusively used for learning and not prediction. these are loosely inspired by the numerous feedback and recurrent connections present in cortex. all three sets of weights are initialized at random and updated using the learned update rule."
"our work here follows a different thread, which uses meta-learning to learn a local learning rule. [cit] s [cit] . despite differences in scale (for example, these early learning rules had just 7 or 16 meta-parameters [cit] ), the overall setup is largely similar: write down a parameterized learning rule that is a function of neuron local variables, specify a set of training task(s), and finally optimize the learning rule for those tasks using either 1st (gradient descent) or 0th order optimization methods (simulated annealing, genetic algorithms). concurrent work along these lines uses multilayered fully connected networks to parameterize the learning rule [cit] . a distinct line of research does not focus on biological plausibility, that is, they use meta-learning for learning non-local learning algorithms [cit] ."
"instead of computing the gradient of the parameters with respect to the loss (as would be done in backprop), we query the learned update rule for updates to the inner-parameters. specifically, we compute updates one layer at a time from the top of the network down. for each neuron i, we use the forward activation (z), reverse signal (z obtained fromw ), and lateral signal (z obtained fromw ) to query the outer model and compute weight updates ∆w, ∆w, and ∆w for pre-and post-synaptic neurons j and i:"
"in figure 1, we assume that consumers connected to r1, r2, r4, r6 send same requests i (a) to the provider that is connected to r11. c(a) has a higher access probability on routers along the path r5 to r11 . if we use lce as the caching strategy, c(a) will be cached in each router along the path r5 to r11. furthermore, even if the popularity based strategy is used, content c(a) will be cached on most routers along the path from r5 to r11 since it has higher popularity than other contents. obviously, this method causes a lot of redundancy. especially for some routers, it is not entirely reasonable. for example, for router r7 and r5, if the content c(a) is more popular than the others, and consumers of the content are mostly at downstream routers of the path, the content should be cached on these routers. in this case, r5 is the best caching location because if we do not consider the other metrics, r5 will satisfy most of the r1, r2, r4, r6 requests. in the same way, r3 is the best router if only r1 and r2 request content c(a). in addition, taking into account the content popularity and the probability of request, if r1 and r2 have higher probability to request the content than the others, r5 does not need to cache the content though r4 and r6 are also possible to request c(a)."
"as earlier, we assume that there are enough examples from both the classes, so that the empirical averages can be replaced by expectations. then, as in the earlier case, we can rewrite the optimization problem given by (13) as"
"in an extreme case, when the distribution of interest is completely free of the locality, the requests follow the uniform distribution on the network. now, the effectiveness of the algorithm is equivalent to that of the popularity-based strategy. on the other hand, when the locality is more obvious, the benefits of the algorithm will gradually increase."
"case 3: ifdd strategy: if attackers want to attack, at least two requirements are met: (1) attackers frequently request the fake content, making it popular, and (2) the geographic location of attackers is sufficiently extensive so that the interest has sufficient degree of dispersion. indeed, if the attacker has reached these two requirements, our strategy may fail. however, the situation is better than expected. the reason is that the locality on each router is not consistent. actually, there may be only a small number of routers affected by attacks. in fact, it is difficult for an attacker to want the valid content with high popularity and low locality on most routers at the same time. if an attacker wants to meet this requirement, he must work closely with many of the consumers scattered across the network. if the attacker has such capacity, the entire ccn architecture under the network caching mechanism will face a great security challenge. for such scenario, it is necessary to re-examine the severity of this attack."
"ccn is similar to peer-to-peer (p2p) networks, web caching systems, and content distribution networks (cdn) from the viewpoint of caching design. p2p and cdn caching mechanisms are usually only for a single type of application services, and the content provider is responsible for content deployment. but ccn, as a more underlying network paradigm, provides caching services for a variety of applications and requires data to be transferred at line-speed. that is, ccn caching design and resource management are more complex, compared to p2p and cdn."
"decision trees can be broadly classified into two types, i.e., axis parallel and oblique [cit] . in an axis-parallel decision tree, the split rule at each node is a function of only one of the components of the feature vector. axis-parallel decision trees are particularly attractive when all features are nominal; in such cases, we can have a nonbinary tree where, at each node, we test one feature value, and the node can have as many children as the values assumed by that feature [cit] . however, in more general situations, we have to approximate even arbitrary linear segments in the class boundary with many axis-parallel pieces; hence, the size of the resulting tree becomes large. the oblique decision trees, on the other hand, use a decision function that depends on a linear combination of all feature components. thus, an oblique decision tree is a binary tree where we associate a hyperplane with each node. to classify a pattern, we follow a path in the tree by taking the left or right child at each node based on which side of the hyperplane (of that node) the feature vector falls in. oblique decision trees represent the class boundary as a general piecewise linear surface. oblique decision trees are more versatile (and hence are more popular) when features are real valued."
"the output δ from the outer model is a learned top down error signal, and θ denotes the metaparameters that parameterize the learning rule. see the appendix a for a detailed description for how weight updates, and reverse and lateral signals, are computed. to generate our preliminary results, we train a fully connected neural network with one hidden layer. we train and evaluate semi-supervised learning rules on toy classification tasks. we train on tasks formed by re-sampling labeled examples from the two-moons dataset."
"there is no doubt that if the attacker has such ability, our strategy cannot resist pollution attack. fortunately, it is very difficult for an attacker to have such ability, especially in the third factor d(g). there are significant differences in the complexity of pollution under different caching strategies. we will discuss in the following three different cases:"
"ifdd applies madm [cit] to determine their weights. madm is usually used to perform multi-criteria decision making, having the following advantages: (1) the weight of decision attributes can be determined objectively, (2) the dimensions of each attribute can be different, and (3) allowing each node to participate in decision-making properties or the ratio between them is not the same. [cit] made a comparative analysis of some concrete algorithms of madm and discussed the characteristics of these algorithms. here, we present the concrete algorithm of madm, which is used in ifdd."
"we use an example to illustrate how the locality is involved in caching decisions. in this example, the working principle of ifdd is compared with lce and the other popularity-based strategy."
"as a new network paradigm, content-centric networking (ccn) [cit] has gained significant attention in the past few years. benefiting from various attractive advantages (e.g. the name routing and the in-network caching), ccn is considered as a potential substitute for current tcp/ip, especially for mobile scenarios such as mobile ad-hoc networking (manet) and vehicular ad-hoc networking (vanet), etc. it is regarded as the most competitive network architecture for the future internet."
"even if the topology is not a tree or hierarchical structure, a good routing algorithm should maintain the stability of routing and avoid routing floating. interests generated by a particular community/region are different from that of the other communities [cit] . that is, the interest requests are clearly imbalanced in geographical distribution. all interfaces on the router can clearly see this feature. in our approach, the popularity of the content is obtained in this direction by counting the interest of the same prefix on an interface. further, if these interests are mainly sent by a community/region, cv x (a) of the interest prefix will be very different from those sent by the relatively dispersed consumers."
"the major contributions are summarized as follows: 1) we study a pollution attack in the scenario where popularity-based caching strategy is deployed. this attack may seriously damage the caching advantage and greatly increase the processing delay of consumer requests. the simulation results show that the system under the existing popularity-based cache allocation mechanisms, including simple random strategy (e.g. lce [cit] ), is vulnerable to this attack and then these mechanisms could not work effectively."
"obviously, the best way is to cache those contents that are most likely to be requested by the other consumers in the future. but it is hard to predict what content will be requested in the future. the popularity and the locality have been used to decide what kind of content to be cached in cdn (e.g. youtube [cit] ). however, it is difficult to implement these cdn solutions directly in ccn. there are two major reasons as follows: (i) the ccn network has no centralized control mechanism to calculate the popularity and the locality."
"from table ii, we see that the average accuracy of gdt is better than all the other decision tree algorithms, except for the wine, votes, and heart data sets, where lddt has the same or better average accuracy. in terms of the confidence interval of the average accuracy, the performance of gdt is comparable to the best of other decision tree algorithms on the breast cancer, bupa liver, magic, heart, votes, and wine data sets. on the remaining eight data sets, the performance of gdt is significantly better than all the other decision tree approaches. thus, overall, in terms of accuracy, the performance of the gdt is quiet good. in majority of the cases, gdt generates trees with smaller depth with lesser number of leaves, compared with other decision tree approaches. this supports the idea that our algorithm better exploits the geometric structure of the data set while generating decision trees."
"note that the centrality degree of the router (degree x ), the capacity of the caching store (capacity x ) and the other factors can also be added to make decision. here, we only use cv x (a) and p x (a) as the attributes since ifdd is designed to be a bootstrap and without the aid of the other routers. we will consider these factors in our future research."
"the simulation results in figure 6 show that under different topologies, the cache hit is directly related to the concentration of interest. however, the cache hit in both topo-1221 and topo-7018 is significantly better than in topo-1755 and topo-3697. the main reason is that the node degree is higher in topo-1221 and topo-7018."
"the caching strategy is closely related to various factors. these factors can be divided into two categories. one is related to the dynamic characteristics of the network, such as quality of service (qos), router cache capacity, network topology and so on. the other is related to the characteristics of interests."
"this subsection aims to investigate the impact of interest distribution on strategies, that is, the impact of cache size and caching strategy, on cache hits under different parameters of the interest distribution model."
"although the individual router cache hits may be reduced due to the decrease in the number of replicas, in general significantly higher than lce and popular. when α is increased, ifdd will have a higher cache hits than the others. the simulation results demonstrate the effectiveness of our algorithm."
"in the gdt algorithm described in section ii, 1 is a parameter. if more than (1 − 1 ) fraction of the points fall into the majority class, then we declare that node as a leaf node and assign the class label of the majority class to that node. as we increase 1, chances of any node to become a leaf node will increase. this leads to smaller sized decision trees, and the learning time also decreases. however, the accuracy will suffer."
"future work includes the analysis of the network cache attack models. in addition, we plan to investigate the potential relationship between the cache replacement and the caching decision strategy in ccn for better caching performance. note that this paper only investigates the effect of the traditional cache replacement strategies, which were not designed for ccn. in the future work, we plan to explore whether we could design an appropriate cache replacement mechanism which could cooperate with cache allocation mechanism to improve ccn performance. note that madm is a very basic tool to solve the problem under question. this paper exploited a concrete implementation of madm in ifdd. future work will investigate the effect of different concrete algorithms of madm on the effectiveness of ifdd."
"t, where e 50 is a 50- dimensional table ii comparison results between geometric decision tree and other decision tree approaches vector, whose elements are all 1. now, the points are labeled as follows:"
"from the performance viewpoint, the core of cache allocation decision in ccn is \"caching the specific content to the appropriate location\". the solution to this problem faces two challenges:"
"the approaches for learning oblique decision trees can be classified into two broad categories. in one set of approaches, the structure of the tree is fixed beforehand, and we try to learn the optimal tree with this fixed structure. this methodology has been adopted by several researchers, and different optimization algorithms have been proposed [cit] . the problem with these approaches is that they are applicable only in situations where we know the structure of the tree a priori, which is often not the case. the other class of approaches learns the tree in a topdown manner. top-down approaches have been more popular because of their versatility."
"this section first presents simulation setup. then ifdd is compared with the other strategies, i.e. lce and naive popularity-based strategy under various system parameters. the considered parameters include interest distribution, replacing strategy and network topology. in the following, lru is used as the default replacement mechanism. the metrics considered include cache hit ratios and application request processing delay. we also evaluate ifdd's capability in being resistant to pollution attack."
"we implement the algorithm on ndnsim platform [cit], which is an open experiment environment based on ns3 [cit] . the operating system used is ubuntu-12.04-lts. four real network topologies [cit] are used in our simulation, shown in figure 2 (a)(b)(c)(d) and indexed as 1755, 2914, 3967 and 7018, respectively. they cover the typical topologies in terms of the number of routers and the topology structure."
"in this section, we present empirical results to show the effectiveness of our decision tree learning algorithm. we test the performance of our algorithm on several synthetic and real data sets. we compare our approach with oc1 [cit] and cart-lc [cit], which are among the best state-of-art oblique decision tree algorithms. we also compare our approach with the recently proposed linear-discriminant-analysis-based decision tree (lddt) [cit] and with the svm classifier, which is among the best generic classifiers today. we also compare our approach with gepsvm [cit] on binary classification problems. the experimental comparisons are presented on four synthetic data sets and ten \"real\" data sets from the uci ml repository [cit] ."
"t would be zero because columns of q span the null space of g. now, the eigenvector corresponding to the largest eigenvalue ofh is selected as the desired vector (clustering hyperplane)."
"this subsection presents how to compute the popularity and the locality to be used in ifdd. the symbols and notations to be used latter are given in table 1. the major steps for a ccn router to process interest/ content are as follows: when the router r x receives the interest i (a) at the interface rf i x, the prefix a is firstly recorded in pit. then r x searches pit and judges whether prefix a does exist or not. if a does not exist, r x creates a pit entry for i (a) and adds the incoming interface to pit entry, then forwards i (a) based on fib. if a exists, the router adds the incoming interface into the pit entry for interest aggregation. following this process, when a duplicate interest is received, the router simply discards or aggregates it."
"in this section, we present some analysis of our algorithm. we consider only the binary classification problem. we prove some interesting properties of the angle bisector hyperplanes to indicate why angle bisectors may be a good choice (in a decision tree) for the split rule at a node."
"for a two-class classification problem, the multisurface proximal svm (gepsvm) algorithm [cit] finds two clustering hyperplanes, i.e., one for each class. each hyperplane is close to patterns of one class while being far from patterns of the other class. then, new patterns are classified based on the nearness to the hyperplanes. in problems where one pair of hyperplanes like this does not give sufficient accuracy, mangasarian and wild [cit] suggests the idea of using the kernel trick of (effectively) learning the pair of hyperplanes in a high-dimensional space to which the patterns are transformed. motivated by gepsvm, we derive our decision tree approach as follows: at each node of the tree, we find the clustering hyperplanes as in gepsvm. after finding these hyperplanes, we choose the split rule at this node as the angle bisectors of the two hyperplanes. then, we split the data based on the angle bisector and recursively learn the left and right subtrees of this node. since, in general, there will be two angle bisectors, we select that which is better based on an impurity measure. thus, the algorithm combines the ideas of linear tendencies in data and purity of nodes to find better decision trees. we also present some analysis to bring out some interesting properties of our angle bisectors that can explain why this may be a good technique to learn decision trees."
"2) we propose a lightweight non-collaborative cache allocation approach (ifdd). lightweight means that when ifdd strategy is deployed, not only lower communication overhead is produced but also small calculation overhead is required at routers. the first small overhead is because no cooperation is required among routers. the latter one is because the calculation is completed at each router in the linear polynomial time. to the best our knowledge, ifdd is the first caching approach which integrates the popularity and the locality in making caching decision. moreover, ifdd is the first caching approach which could not only enhance caching performance, but also defend against the pollution attack. note that this paper evaluates performance in terms of request processing delay and cache hit ratio."
"a ] are kept constant to ensure that the solutions of the optimization problem are bounded. once again, we feel that the preceding discussion shows that the angle bisectors are a good choice as the split rule at a node in the decision tree."
"we further show that the learning rule generalizes from a two moons task to random two-gaussian tasks that it never experienced during outer-training. on this new semi-supervised learning task, it continues to outperform the supervised baseline. note that the learned update rule solves the task in a qualitatively different way than backpropagation, and despite doing better on test accuracy performs dramatically worse on training loss."
"in terms of the time taken to learn the classifier, gdt is faster than svm on majority of the cases. at every node of the tree, we are solving a generalized eigenvalue problem that takes time on the order of (d + 1)"
"we first explain our method by considering a two-class problem. given the set of training patterns at a node, we first find two hyperplanes, i.e., one for each class. each hyperplane is such that it is closest to all patterns of one class and is farthest from all patterns of the other class. we call these hyperplanes as the clustering hyperplanes (for the two classes). because of the way they are defined, these clustering hyperplanes capture the dominant linear tendencies in the examples of each class that are useful for discriminating between the classes. hence, a hyperplane that passes in between them could be good for splitting the feature space. thus, we take the hyperplane that bisects the angle between the clustering hyperplanes as the split rule at this node. since, in general, there would be two angle bisectors, we choose the bisector that is better, based on an impurity measure, i.e., the gini index. if the two clustering hyperplanes happen to be parallel to each other, then we take a hyperplane midway between the two as the split rule. before presenting the full algorithm, we illustrate it through an example."
"case 1: lce or similar strategies: when lce or a similar strategy is deployed, attackers could attack successfully with only certain s(δt). the ability of s(δt) enables attackers to send a lot of fake interest to the network. if there are some malicious providers cooperating the attack, they will respond to this interest with fake content. the content will invade and occupy cs along the path from the consumer to the provider. therefore, in this case it is easy to pollute the cache."
"the existing studies [cit] indicated that caching performance is affected by a variety of factors, including router cache capacity, content/interest distribution, network topology and caching strategy. in the past years, various caching mechanisms [cit] were proposed. most of them [cit] were based the content popularity. here, the popularity refers to the number of times of requesting the content at a router per unit time. for a specific content at a specific router in ccn, the corresponding popularity can be obtained by counting the total number of requests for this content on this router. in the rest of the paper, unless otherwise specified, the popularity of a content is called the popularity. when popularity-based cache allocation mechanisms are deployed, the higher the content popularity, the higher caching priority the content."
"this paper explored the combination of content popularity and locality to design a lightweight non-collaborative cache allocation approach. the proposed approach could significantly improve caching performance in terms of decreasing dramatically request processing delay and increasing cache hit ratio. meanwhile, it also could defend against pollution attacks from malicious nodes. the simulation results verify the effectiveness and efficiency of ifdd."
"the performance of any top-down decision tree algorithm depends on the measure used to rate different hyperplanes at each node. the issue of having a suitable algorithm to find the hyperplane that optimizes the chosen rating function is also important. for example, for all impurity measures, the optimization is difficult because finding the gradient of the impurity function with respect to the parameters of the hyperplane is not possible. motivated by these considerations, here, we propose a new criterion function to assess the suitability of a hyperplane at a node that can capture the geometric structure of the class regions. for our criterion function, the optimization problem can also be solved more easily."
"some researchers pointed out that the ccn topology is different from traditional networks. however, most ccn studies assume that the ccn topology is a hierarchical tree structure, and if not, it can be converted to a tree structure. tree topology can simplify the network design. for example, eliminating the loop and then the broadcast storm is avoided. in addition, this makes some spanning-tree algorithms widely used in the network, such as minimum spanning tree (mst) and shortest spanning tree (spt). in ccn, one of the advantages of a tree topology is that it can be used to determine the interface of interest/content source. if consumers (e.g. communities) are connected to one gateway, they will send all requests to router r x through the same interface in the tree topology."
"the complete algorithm for learning the decision tree is given as follows: at any given node, given the set of patterns s t, we find the two clustering hyperplanes (by solving the generalized eigenvalue value problem) and choose one of the two angle bisectors, based on the gini index, as the hyperplane to be associated with this node. we then use this hyperplane to split s t into two sets, i.e., those that go into the left and right child nodes of this node. we then recursively do the same at the two child nodes. the recursion stops when the set of patterns at a node are such that the fraction of patterns belonging to the minority class of this set are below a user-specified threshold or the depth of the tree reaches a prespecified maximum limit."
"as mentioned previously, one of the advantages of our approach is its ability in being immune to the cache pollution attack. we design a simulation scenario to verify the effectiveness of our approach in defending against the pollution attack."
"a problem with all impurity measures is that they depend only on the number of (training) patterns of different classes on either side of the hyperplane. thus, if we change the class regions without changing the effective areas of class regions on either side of a hyperplane, the impurity measure of the hyperplane will not change. thus, the impurity measures do not really capture the geometric structure of class regions."
"case 2: popularity-based strategy: when popularity based strategy is used, same as in case 1, they only have enough s(δt). however, in this case, the demand for this ability is much higher than in case 1. malicious consumers must send enough invalid interests to achieve a successful attack. the frequency of these requests must be high enough to make the content more popular. this case is more difficult than in case 1. however, it also could be executed effectively if attackers have the ability of n (g)."
"studies [cit] indicated that there is the significant difference in consumers' content interests among some large and heterogeneous communities owing to consumers' community attributes and geographic locations. that is, the probability of requesting a given content may vary significantly from region to region. we use locality to denote the feature of this difference in this paper, namely, the degree of locality dispersion. the higher the locality, the larger the difference in the number of times of requesting this content per unit time among the interfaces of this router."
"it is well known that the traffic model is vital for simulation. our investigation indicates that there is no public data set for ccn simulation. many researchers realized the problem and proposed some traffic models either theoretically or through experiments. [cit] studied the impact of the popularity and the locality under different cache replacement strategies in content caching systems, and introduced a traffic model named snm. snm is different from early traditional caching traffic models. it is a more general and flexible model which can be extended for different requirements of popular caching strategies. their work provided effective tools for building simulation scenarios. snm is an important reference model when we design our simulation traffic model."
the rest of the paper is organized as follows. section ii presents the related work about popularity-based caching strategy and presents the existing locality studies. section iii presents the details of ifdd mechanism. section iv describes the simulation results to evaluate the performance of our design on ndnsim platform. section v concludes the paper and discusses future work.
") symmetric positive semidefinite matrix. g is strictly positive definite when matrix a has full column rank. similarly, matrix h is also a positive semidefinite matrix, and it is strictly positive definite when matrix b has full column rank. the problems given by (1) and (2) are standard optimization problems, and their solutions essentially involve solving the following generalized eigenvalue problem [cit] :"
", where e n t + is an n t + -dimensional column vector 3 of ones. now, d + (w, b) can be further simplified as"
"simulation results: we now discuss the performance of gdt in comparison with other approaches on different data sets. the results provided are based on ten repetitions of tenfold cross validation. we show the average values and standard deviation (computed over the ten repetitions). table ii shows the comparison results of gdt with other decision tree approaches. in the table, we show the average and standard deviation 5 for the accuracy, size, and depth of tree and the time taken for each of the algorithms on each of the problems. we can intuitively take the confidence interval of the estimated accuracy of an algorithm to be one standard deviation on either side of the average. then, we can say that, on a problem, one algorithm has significantly better accuracy than another if the confidence interval for the accuracy of the first is completely to the right of that of the second."
"theorem 3: if the sample mean of two classes are the same, then the clustering hyperplane found by solving optimization problems (1) and (2) will pass through the common mean."
"2) the performance difference of the three replacement strategies is not obvious. as the default replacement strategy in ndnsim, lru does not show a more obvious performance than a simple fifo. in other words, the effect of the replacement strategy on caching performance is much smaller than the allocation strategy. it seems that the replacement strategy designed for the traditional caching mechanism may not be suitable for the ccn, and we may need to design a new replacement strategy for ccn. our experiment results confirm that the design of the caching strategy is very meaningful for ccn in-network caching mechanism. we also repeat the simulations of section iv.b but under lfu. figure 5 shows the results, indicating that there is less performance difference between lfu and lru. 3) for ifdd and popular, the cache hit is higher under lfu than under the other two replacement strategies. but for lce, the effect of a replacement mechanism is less. one possible reason is that lce is not related to the content popularity. when the router caches a content according to a decision algorithm, this content is not necessarily the most popular. therefore, if the replacement strategy is lfu-like, the content may soon be replaced, making the allocation strategy less effective. this suggests that there may be a close relationship between the cache allocation strategy and the replacement strategy. it may be better to design the cache replacement strategy and the allocation strategy together for much better network performance in term of cache hit ratio. we leave such investigation and design for future work."
"2) we use the information entropy method to get the weight of attributes. the column vector of nsd x is (a 1, a 2 ) . then, the entropy of a 1 and a 2 for attribute cv x and p x will be"
"to understand the robustness of our algorithm with respect to this parameter, we show, in fig. 3, variation in crossvalidation accuracy and the average number of leaves with 1 . the range of values of 1 is chosen to be 0.05-0. 35 . we see that the cross-validation accuracy does not change too much with 1 . however, with increasing 1, the average number of leaves decrease. thus, even though the tree size decreases with 1, the cross-validation accuracy remains in a small interval. this happens because, for most of the points, the decision is governed by nodes closer to the root node. few remaining examples, which are tough to classify, lead the decision tree to grow further. however, as the value of 1 increases, only nodes containing these tough-to-classify points become leaf nodes. from the results in fig. 3, we can say that 1 in the range of 0.1-0.3 would be appropriate for all data sets."
"this subsection presents the ifdd algorithm and the concrete algorithm of madm used in ifdd. the caching decision strategy can be briefly described in three steps: (1) building statistic table according to the received interest, (2) calculating the fitness (cv x (a), p x (a)) of content based on stat x (a), and (3) making caching decision using madm algorithm with (cv x (a), p x (a))."
"this paper explores how to combine the popularity and the locality to design cache allocation mechanisms. a novel cache allocation algorithm, named ifdd (interestinterface dynamic degree) based cache allocation, is proposed. in ifdd, the popularity and the locality are both calculated at each router according to the counting of content interests. there is difference in the calculation of these two features. the popularity is obtained only by counting the interests of the content at the router. but the computation of locality still needs counting the content interests at each interface of this router. more details are given in algorithm1 and algorithm2 of section iii. with its own calculated popularity and locality, every router could makes caching decision independently with multiple attribute decision making (madm) algorithm [cit] ."
"note that the strategy proposed in this paper is mainly used to defend against those attacks which aim for popularitybased caching strategies. in this attack model, malicious consumers send a lot of requests for non-popular contents, leading to that the cache is depleted by non-popular contents, and eventually leading to the caching strategy failure. our simulation results validate the existence of this attack and verify that our approach has a certain immunity to this attack."
"in figure 2, blue nodes represent content providers which are randomly selected. black nodes are gateway nodes and gray nodes are trunk nodes. consumers are at the edge of network, and small dots are used to represent them. the simulation compares the cache hit ratio among the four topologies respectively."
"is mean of the prefix a value on r x . cv x (a) could reflect the degree of equilibrium distribution of all interests with prefix a for the content c(a) on r x . in ifdd, the lower cv x (a), the higher the locality. therefore, content c(a) with lower cv x (a) is more suitable for caching on r x ."
"where the value of β can be chosen, so that it is consistent with our scaling of w 1 and w 2 . now, the parameters of the two angle bisectors can be written as ("
"1) different allocation strategies show significant differences in performance under the same capacity. ifdd produces the best cache hits, and the cache capacity does have a critical impact on the cache hits. the cache hit increases when the cache capacity is increasing. this suggests that ifdd is more sensitive to cache capacity in terms of the cache hit, which is exactly what we are pursuing in the caching design."
"where α 1 and α 2 are some scalars. this means that both clustering hyperplanes are parallel to each other and (w 3, b 3 ) is such that w 3 ∝ σ −1 (μ + − μ − ). this is the same as the fisher linear discriminant, thus proving the theorem."
"in ccn architecture, a router maintains three main components: pend interest table ( pit), content store (cs) and forward information base (fib). pit is used to record the unsatisfied interest prefix and the incoming interface index. it could provide a reference interface to the returned content. cs is a key component to cache the returned content and to satisfy the later interest request with the same prefix. the router forwards interests according to fib information, which is calculated by the routing algorithm."
"in this section, we first discuss the challenges in designing a caching decision approach. then we detail how ifdd achieves effective cache allocation and why it has immunity to pollution attacks."
"as is easy to see, in our method, the optimization problem of finding the best hyperplane at each node is solved exactly rather than by relying on a search technique. the clustering hyperplanes are obtained by solving the eigenvalue problem. after that, to find the hyperplane at the node, we need to compare only two hyperplanes based on the gini index."
"this subsection aims to investigate the effect of cache replacement on the performance of caching allocation mechanisms. three common cache replacement strategies are considered: lru (least recently used), lfu (least frequently used) and fifo (first-in-first-out). evaluations are carried out under different cache capacities. figure 4 shows the results. we observe that:"
