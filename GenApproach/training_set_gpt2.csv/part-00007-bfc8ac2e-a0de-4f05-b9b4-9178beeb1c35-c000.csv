text
"comprehensive framework support (cfs) refers to the completeness of the existing approach. if an ao uml approach has cfs, this means that the approach represents aspects in all uml diagrams."
"the scientific world journal expected outputs (artefacts) of each of the steps. the outputs from preceding steps are then input to the subsequent steps. often oo uml diagrams are seen as just a random \"bag of tools. \" there is no preset order to their use; the same is applicable for ao. in other words, developers only draw the diagrams they need for their specific application, and only when they need them. however, the design process could be made easier if developers were able to refer to a comprehensive set of steps."
"in this section earlier, we highlighted the difficulties of conducting an evaluation in this research field and explained the reasoning for the researcher's decision to conduct a workshop for a ten-person focus group consisting of industry professionals who have been involved in some aspect orientation projects. figure 17 shows the workshop conduction general process. in this workshop, the researcher (ii) explains how the other two selected approaches work;"
"the analysis of the participants' responses to q5, which are presented in table 13 and figure 19, confirms (h6). table 14 summarizes the above results."
"4.1. phase 1: theoretical study. in this phase, many books, journals, online articles, and proceedings relevant to this research topic are reviewed. in this review, many aspectoriented (ao) approaches and tools are screened and all aspect-oriented design modelling (aodm) approaches are reviewed. the main outputs of this phase are the problem statement and the literature summary. the outputs of this phase are considered as inputs to the next phase."
"modelling the case studies. in this process, the researcher models the case studies by using the proposed aspectual uml modelling approach and by using two other selected approaches. there are several approaches that could be used to model aspects. in fact, as mentioned in slr, there are 14 established aspect-oriented (ao) uml modelling approaches. however, due to research constraints, it is not possible to compare the proposed aspectual uml approach against all of them, so just two of them are compared with the proposed approach. the researcher uses the two most effective approaches, the composite pattern approach [cit] and the uml-based ao design notation for aspectj approach [cit] ."
"the main topics with which one has to be familiar in this field include uml [cit], uml extensions [cit], aspect-oriented design modelling (aodm) [cit], aop, and aspectj language constructs [cit] ."
"the problem of virtual machine placement can be formulated as a variant of multidimensional bin packing problem, which is combinatorial np-hard problem. for this problem, several heuristics have been developed, such as ffd(first fit decreasing), bff(best first fit), which can quickly provide sub-optimal solution."
"the majority of existing approaches focus on the abilities of uml extensions to model aspects in aosd, which basically depends on using stereotypes, tagged values, and constraints (i.e., a lightweight uml extension). however, there are no set standards for extension modelling that have yet been agreed on by the aom community [cit] . therefore this section proposes a comprehensive set of recommended aspectual uml design modelling notations for aosd. the researcher illustrates, shows, models, visualizes, and documents each aspectual uml modelling view based on the proposed aspectual uml modelling approach. the approach employs two uml extension abilities: the uml profile extension and the uml metamodel extension. in other words, the proposed aspectual uml modelling approach is based on mixed-mode uml modelling as shown in figure 4 ."
"there are many case studies that could be used, but for this work the following two case studies are selected: (1) the european ornithological trust survey system (eotss) and (2) the scicom contact centre learning and development management system (sccldms). the selection criteria used to choose the two case studies are as follows. enables the researcher to present them comprehensively to the focus group during the workshop."
"as mentioned above, based on the results of the slr, a total of 14 ao modelling approaches are relevant to the research questions. in this part of the study, these approaches are compared against the selected criteria briefly identified in section 2.2. the discussion in this section is presented in a tabular format with explanatory text. the approaches are given in the rows and the criteria in the columns of the (table 2) . table 2 illustrates the comparison between the selected approaches based on this criterion and its subcriteria. one of the most interesting findings is that none of the proposed approaches used uml 2.4 and none of them focus on all the uml diagrams; rather, the majority focus on the class diagram (structural modelling)."
"this section describes the six criteria used to compare the ao uml modelling approaches as well as their respective subcriteria. the six key criteria are illustrated in figure 2 . the motivation behind the selection of these criteria was to reveal the knowledge gaps and identify and show how this study aimed to fill those gaps. in other words, identification of these criteria provided a basis for a comprehensive analysis of existing ao uml approaches, which allowed comparison of different ao uml approaches in greater detail. a brief description of each criterion is given below."
"advanced separation of concerns tends to separate crosscutting concerns such as logging and security from the core concerns of the system by using aspect-oriented programming (aop). it provides the implementation solution for the code scattering and tangling issues as a part of aosd. aspect-oriented programming overcomes the problem of code spreading over the core concerns-the issue encountered in object orientation when it implements the crosscutting concerns-by using a new implementation modularity unit called the \"aspect. \" aspectj is a type of aop that has grown in popularity in the industrial environment and in academic research [cit], which has consequently increased the interest in investigating aspect orientation for other stages of the sdlc."
"a salient feature of cloud computing is to provide on-demand resource allocation strategy for applications in order to handle dynamic workloads. as part of future work, we are planning to study how to apply reinforcement learning and virtual machine live migration technologies to intelligently deal with dynamic peak workloads."
we have proposed some semantic rules/relationships for all aspectual diagrams (behavioural and structural); these relocations/rules are there to maintain consistency and to make sure that oo semantic correctness is maintained. table 11 shows a sample of these ao semantic rules for aspectual class diagram.
"to support the proposed approach, a proof-of-concept prototype was implemented based on the umlet open-source tool. the motivation for proposing a novel aspectual uml to support the aspectj approach can be found in the four intended main benefits."
"a join point is a well-defined point in the execution of a program. there are many types of join point [cit] . the proposed acd can represent all the join points to model the join point signature and type, as shown in figure 20 . figure 20 illustrates an aspect called security. it does not show all the details of the aspect but depicts the representation of the join points. from the figure, it can be seen that even though the aspect defines the crosscutting behaviour generally, it also defines the types of join point that cut across the points in a program's execution. the proposed modelling shows these join points in the early stage of the design process. the benefit of showing the join points and their types early on is that the programmers will have a more complete picture of the nature of the join point that they will expose and the crosscutting effect that will take place, and this knowledge will make the development process easier for them."
"the researcher therefore conducted a systematic literature review (slr) which focused on identifying, appraising, selecting, and synthesizing all the available high-quality research on state-of-the-art approaches on aodm that use uml. the slr is a process that consists of reviewing, extracting, and evaluating and then analysing and interpreting the studies that are relevant to the three research questions of this research. (rq1) how do the existing approaches that model aspects (crosscutting concerns) use uml? (rq2) what are the objectives of having a comprehensive approach that extends uml to support aom in the early stage of aosd? (rq3) how beneficial is it to have a comprehensive approach that extends uml to support aom based on aspectj in the early stage of aosd? most research starts with a literature review. however, unless a literature review is thorough and fair, it is of little scientific value [cit] . hence, this review aims to synthesize existing work in a manner that is not only fair but seen to be fair."
"recently, several prototypes and methods have been proposed to address problems of virtual machine management by making a trade-off between applications performance, energy consumption and migration costs."
"the complexity and size of systems have grown substantially in recent years. this has led to the manifestation of new concerns. these new concerns crosscut other concerns and core classes in the system. the implementation solution for these crosscutting concerns is scattered throughout the components of the system, which results in the problem of code scattering and tangling. code scattering and tangling make the components of the system hard to reuse, comprehend, and maintain. these drawbacks have led to the development and adoption of aspect-oriented (ao) concepts to handle the crosscutting concerns in all stages of the software development life cycle (sdlc). this approach is described as aspect-oriented software development (aosd) [cit] . as a result, the concept of soc has been enhanced to generate a new concept known as advanced separation of concerns (asoc) [cit] . however, the development of an improved approach to model and represent these crosscutting concerns is still critical to reduce or eliminate tangled code."
"illustration (rei) takes into consideration the applicability and reality of the demonstrated example used in the approach. the rei could be a used example (ue), which indicates that the example has been used before by other approaches. it could also have complexity (c), which indicates that example is complex rather than trivial."
"it should be noted that the proposed approach does not pay attention to very detailed matters concerning pointcuts such as the accurate syntax of pointcut definitions, the pointcut signature, and the matching and type of patterns. in the context of this study, the researcher is interested in preserving the soc and does not want to interfere with the concerns of the other stages of the software life cycle."
(ii) quality. case studies must be of good quality and nontrivial in nature. these case studies have not been used as an example in the context of aspect orientation. both of the selected case studies meet the quality requirement.
"the ao uml modelling approaches can be classified into two general categories. the first category consists of approaches that construct/build uml profiles. the construction of a uml profile extension is usually called a lightweight extension because it does not involve the implementation of any new uml metamodel elements. the uml profile extension approaches utilize uml constraints, tagged values, and stereotypes to model different domains. thus the uml profile extension technique depends on the flexibility and extendibility of the standard uml [cit] . the second category consists of approaches that extend the uml metamodel, which are considered to be heavyweight extensions because they involve developing a new uml metamodel to represent aspects and their crosscutting nature [cit] ."
"auml defined semantic rules as the enforcement of the checking of all the aspectual relations and notations to ensure that these aspectual notations match the semantics of aspect orientation. these rules are derived from the semantic rules of the omg uml 4.1 standard [cit] . the aspectual uml approach proposes new aspectual uml notations and relationships, so it is critical to identify and understand the effects of the semantic changes of these new notations and relationships on the existing uml notations in order to maintain the semantic correctness of both object-oriented (oo) uml and aspect-oriented (ao) uml. for instance, the researcher proposes a new relation called \"crosscutting. \" the semantic rule specifies that this relation can only be drawn between one aspect and one class. in other words, if the designer attempts to draw this relation between two classes or between two aspects, then the proposed aspectual uml modelling approach will not allow this based on the semantic rules proposed for this approach, as shown in figure 15 ."
"in this phase, the principles and suggestions for a possible framework are consolidated in one place. after obtaining the problem statement and the literature summary, the issues found in the previous works are reviewed and consolidated. the initial process of developing the aspectual uml approach starts in this phase. different principles such as aspectj support, completeness of the approach in terms of covering all aspects, use of both types of uml extensions, and designing semantic rules to define the relationships between the ao and oo notations are investigated."
"the aspectual uml approach and the prototype auml tool are implemented in this final phase. in this phase, both the aspectual uml approach and the auml tool are evaluated. different kinds of evaluation techniques are used. to evaluate the proposed approach, three different qualitative methods are used: (1) a focus group-based evaluation is used to evaluate the applicability and correctness of the proposed approach, (2) a \"good design\" criteria-based evaluation is used to evaluate the modularity of the approach and (3) an aspectjbased evaluation. finally, the proposed tool is evaluated using black box and white box testing. these phases are elaborated as well in table 3 ."
"the third example of the aspectual behavioural diagrams is the aspectual interaction overview diagram (aiod). an interaction overview diagram provides a general idea of the flow of control, where the nodes of the flow are interactions or interaction uses. it is different from the ad because the nodes are the interactions or interaction use. the interaction overview diagram describes the interactions where some messages and lifelines might be hidden. it links a few diagrams that have already been drawn to represent a specific functionality. in its current form, the interaction overview diagram cannot be utilized to represent aspects [cit] . to address this issue, the proposed aspectual modelling approach includes new aspectual modelling notations to represent aspects in an aiod. table 10 represents all the new modelling notations."
"this semantic rule means that this is an \"oo notation. \" this means that the sa should not attempt to use oo relations such as dependency and inheritance as a relation with an aspect at any end. the oo notation cannot be connected to ao notations."
the proposed approach provides aspectual uml modelling steps. does the proposed approach provide a better means of modelling aspects through the use of the proposed aspectual uml modelling steps?
"the researcher has presented a summary of the work undertaken in this study. the study contributes to the field by proposing a suitable uml extension for the design of ao systems to support aspectj. the proposed approach is divided into two main parts. the first part concerns the aspectual design model, which the researcher developed in order to be able to visualize, draw, and model crosscutting concerns. the second part is the amm, which consists of some recommended steps that developers can follow when modelling crosscutting concerns using the proposed approach. another contribution consists of the development of some aspectual semantic rules to control the structure of the model, which the researcher has devised to ensure that the modelling of the aspects to support aspectj maintains and conveys meaningful and correct semantics when aspects interact with each other as well as when they interact with the core classes and other oo notations. lastly, the researcher has also attempted to develop an approach and tool that can be used to generate aspectj pseudocode."
respondents with a good level of knowledge of the aspect orientation concept. the process of conducting the focus group evaluation is shown in figure 16 . the researcher faced a lot of challenge to find the right knowledgeable respondents; finally ten respondents were found.
"the previous sections discussed the slr and the research methodology. in that review, the researcher identified the current state of play with regards to research on aspectoriented uml design modelling (audm) that supports aspectj. based on that review, the researcher proposes an extension to the published work by introducing an aspectual uml modelling approach to support aspectj. the proposed comprehensive approach introduces a new audm notation to model crosscutting concerns (aspects) to support the aspectj programming constructsmapped with aspectj. it is intended to act as a complete reference guide to developers and researchers and it aims to provide an aspectual modelling methodology (amm) that can be followed in aosd. the proposed audm notation is divided into two types: aspectual uml structural diagrams that cover all uml structural diagrams, where a notation to represent aspectj constructs is proposed for each diagram, and aspectual uml behavioural diagrams that cover all uml behavioural diagrams, where a notation to represent aspectj constructs is proposed for each diagram. this approach attempts to generate files in an xml svg portable format which could help in the export of the drawings. additionally, this approach attempts to generate aspectj pseudocode out of the drawings."
"tool support (ts) is self-explanatory and has the following subcriteria: modelling support (ms), code generation (cg), which means generating aspectj pseudocode, and model kernel extraction (mke), which refers to extracting and saving the model kernel into a portable file format such as scalable vector graphics (svg) xml."
(h6) the proposed approach provides aspectual semantic rules that offer a better means of representing the semantics of the crosscutting concerns compared to the other ao approaches.
"the remainder of this paper is organized as follows. section 2 describes related work. then, section 3 formulates virtual machine placement problem and describes in detail constraint programming-based virtual machines placement algorithm. evaluation results are presented in section 4. finally,section 5 concludes this paper and presents future work."
"the scientific world journal (dabb). the dabb caters for the needs of environmental and wildlife preservation organizations and committees throughout europe. the core functionalities of the eotss are to (1) record observations, (2) analyse observed data, (3) transfer data to a central collation system, and (4) transfer the configured data from the central system to the end users via various media."
"(1) heavyweight uml: [cit], (2) lightweight uml: [cit], (3) mixed-mode uml (i.e., the combination of heavyweight and lightweight uml extensions): [cit], (4) standard uml: [cit] ."
"(iii) the proposed modelling approach is to support a specific aop language (aspectj), so it might be worthwhile to investigate how to extend the proposed modelling approach to be language independent and to cover other aop languages such as jasco, spring, and aspect werkz."
"in the final process of the focus group-based evaluation, the participants' comments, answers, decisions, and ratings collected in process 5 were analysed to determine whether each of the six hypotheses identified in process 3 could be confirmed, inconclusive, or unconfirmed. table 13 shows which question(s) were used to determine the result for each of the hypotheses."
"as an example, figure 11 illustrates the aiod for the security and the traceability aspects. the aiod shows how the two aspects interact in the context of the interaction overview. umlet 12. 0. there are two main reasons for developing the auml tool: (1) the majority of the existing approaches do not develop their own tool to prove their proposed method and (2) the tool is used in this study to prove the applicability of the proposed theoretical approach. there are some interesting features of this tool, which help in enhancing the contribution of this research: (1) the auml helps in generating xml svg files out of the aspectual drawings; (2) the auml implements the semantic rules between the new proposed aspectual notations and the existing oo notations and relations to maintain correct semantics and to not break the uml omg standards; (3) the proposed auml tool is able to generate aspectj pseudocode from the uml drawings. these add-ins are added and integrated in umlet. this pseudocode generator is built using the vb. [cit] framework. the umlet tool generates the svg xml files using the built-in xml generator as illustrated in"
"the screening process consisted of four steps; the flow of the steps is illustrated in figure 1 . of crosscutting concerns in the early stage of the sdlc. many research studies have developed different kinds of modelling approaches such as theme/uml [cit], subjectoriented programming [cit], cocompose [cit], aspect at design time (adt) [cit], and ao uml diagram modelling. the latter (ao uml diagram modelling) is the concern of this research."
"the aspect is the element that contains and encapsulates all the constructs. however, this does not necessarily mean that all these constructs (join point, pointcut, advice, and introduction) appear in one aspect. the content of the aspect basically depends on the crosscutting nature of that aspect. the proposed approach provides a modelling notation to represent aspects. the aspect notation in the acd gives a name and a separate section to each one of the aspectj constructs, where applicable. figure 23 illustrates the representation of an aspect in the acd. figure 23 illustrates how an aspect is represented by using the notations of the acd. from the figure, it can be seen that the aspect represents all the other crosscutting constructs (advice, pointcut, etc.). however, it should be noted that the proposed aspect representation does not address all the issues relating to the implementation of an aspect, such as aspect declaration, instantiation, and aspect privilege as this would interfere with the implementation part of the system developer. the aspect can be modelled by using different kinds of aspectual uml diagrams."
"to conclude, it is clear to the researcher that aodm has a very wide scope in terms of application and that there will always be new developments in this technology and room for researchers to innovate and add to existing knowledge. a number of suggestions regarding ways to improve the proposed approach are briefly outlined below."
"the ls comprises the following subcriteria that are related to the specifications of the modelling language: (i) the uml modelling language version (umlmlv), that is, the edition of uml that has been used; (ii) the extension mechanism (em), that is, whether it is a uml profile extension (lightweight) or a uml metamodel extension (heavyweight); (iii) language purpose (lp), that is, whether an approach is a general or a specific domain model; (iv) platform dependency (pd), that is, whether an approach is specific to a selected platform or is suitable for crossplatform use; (v) diagram type (dt), that is, which types of uml diagram, structural and/or behavioural diagrams, are utilized by an approach; (vi) modelling process (mp) [cit], that is, whether an approach has clearly stated processes for the modelling steps; (vii) traceability (t), that is, whether the approach has the ability to track and trace the concern from the early stage of the software life cycle to the later stage; and (viii) adaptability (a), that is, whether the approach has the capability of being extended to model a complex system."
the scientific world journal the scientific world journal the scientific world journal this aspectual reference notation is used when a few message communications represent a pointcut.
"the third example of the aspectual structural diagrams is the aspectual composite structural diagram (acsd). composite structure diagrams are used to explore runtime instances of interconnected instances collaborating over communications links to meet some common tasks. these diagrams are usually used to show the internal structure of a classifier and the classifier interaction with the environments through ports. in the proposed approach, this kind of diagram is called an acsd because some new aspectual notations have been added. table 7 describes the proposed aspectual notations for the ascd."
"in this study, the goals of the focus group evaluation are to (1) investigate the applicability and validity of the proposed aspectual uml approach and (2) draw on the expertise of the focus group to identify any omissions in the approach. there are two main reasons for selecting a focus-based method of evaluation for this research. first, a focus group can provide much more detailed information than other available methods. second, some other evaluation methods such as the survey would not result in a significantly useful evaluation because the aspect orientation concept is not well known in the industry. therefore, the researcher would not be able to find this notation is used when a few message communications represent a pointcut."
"the advice construct defines crosscutting behaviour and this piece of code runs at every join point picked by a specific pointcut. specifically, the code that runs depends on the type of advice. aspectj defines three types of advice, which are modelled by the researcher. these types determine how the advice interacts with the join point: \"after\" advice runs after the join point, \"before\" advice runs before the join point, and \"around\" advice runs to replace the join point. \"after\" advice is divided into two more types: after returning and after throwing [cit] . the proposed approach provides a modelling notation for all these advice constructs. figure 22 illustrates these kinds of advice modelled in the acd. figure 22 illustrates a sample aspect. it does not show all the details of the aspect but depicts the advice modelling applied in the acd. examples of the types of advice are shown in figure 22 in the advice section. also, there is no fixed format and/or signature for the advice, so the researcher has provided a flexible advice section, which gives programmers the ability to model any advice necessary. the benefits of modelling these types of advice are almost the same as for join points and pointcuts in that software developers can gain an overview of the effect of crosscutting concerns, so they know what types of main advice have to be implemented and this makes the developer's task a bit easier."
"framework implementation. in this phase, the research moves to another level and the implementation of the conceptual framework identified in the previous phase starts. designing the aspectual uml modelling approach to support aspects involves designing aspectual notations that support aspect constructs, designing the semantic rules between the notations, and designing the aspectual modelling steps and the aspectual uml (auml) tool based on umlet, which is an open-source uml tool. in this phase, aspectj templates are reviewed in order to propose a method to generate aspectj pseudocode."
the eotss is a real project that was undertaken by a nonprofit organization. the requirement text was obtained and analysed by the researcher to determine and model the concerns and the crosscutting concerns (aspects). the eotss was used to set up a digital atlas of breeding birds
"the last step (distribute the questionnaire) is one of the most important steps of the focus group workshop. the questionnaire has been designed carefully to help answer the six hypotheses (formulated in process 4), answer the research questions, and achieve the objectives of the thesis."
"the proposed approach provides aspectual semantic rules that offer a better means of representing the semantics of the crosscutting concerns compared to the other ao approaches. the analysis of the participants' responses to q3 and q10, which are presented in table 13 and figure 19, confirms (h4)."
"relates to the maturity of the proposed approaches. as outlined above, many approaches have been proposed, but this study focuses on those that are well established and that have gained a good reputation among researchers [cit] . the subcriteria of mi are divided into modelling examples (me), application in real-world projects (arwp), and usability (u)."
"this research study uses design science methodology. the dsm is a set of synthetic and analytical techniques and perspectives that can be used when performing research. it involves the creation of new knowledge through the design of novel or innovative artefacts (things or processes that have or can have material existence) and analysis of the use and/or performance of such artefacts along with reflection and abstraction. these artefacts include, among others, algorithms (e.g., for information retrieval), human/computer interfaces, and system design methodologies or languages. design science researchers work in many disciplines and fields, notably engineering and computer science, and they use a variety of approaches, methods, and techniques [cit] . figure 3 illustrates the problem statement within the context of a conceptual or theoretical framework. this type of description contributes to a research report in two key ways because it identifies the research variables and clarifies the relationships among those variables."
"the researcher identified 468 articles and books that are related to the research questions posed in this study. the researcher extracted all the directly related studies by removing duplication and redundancies and by reading the abstract and the full text of the articles. as a result, the researcher found 73 primary studies related to the topic investigated in this research. a breakdown of the study counts for the research questions at each stage of the screening process is shown in table 1 ."
"as an emerging computing paradigm, cloud computing can provide users with on demand it services and elastic computing platforms. many institutes and companies recently focus on cloud computing technology. some notable companies have designed and set up their cloud computing platforms respectively, such as amazon ec2, ibm blue cloud, microsoft windows azure, and salesforce sales force. these cloud computing platforms offer customers storage, computing power, deploying environments and it services at different layers. the development of cloud computing abilities is closely related to supports of underlying datacenters, which provide a powerful parallel computing power and massive data storage for cloud computing."
"to summarise these processes the author explains the proposed aspectual uml approach to the participants; the different kinds of aspects are explained and various topics are covered such as the reasons for developing a new approach, the types of new notations created and how they work, what improvements have been made, and the purpose of the aspectual modelling methodology. the author as well explains how the other two selected approaches work, explains the two selected case studies and how they were modelled using the three approaches, shows the participants the drawings of each case study generated by each approach, gives the participants a small case study to try to model using the proposed aspectual uml approach and gives them the option to try the proposed aspectual universal modelling language (auml) tool as well, and lastly distributes a small questionnaire designed based on six hypotheses proposed and asks the participants to complete it and return it back by the end of the session."
"show the structure of a system and its parts on different abstraction and implementation levels and how they are related to each other. the elements in a structure diagram represent the meaningful concepts of a system, which may include abstract, real-world, and implementation concepts. figure 5 shows the aspectual structure diagrams that were used to represent crosscutting concerns (aspects) in the proposed comprehensive modelling approach."
the aspect notation consists of the following four main sections: (i) the join point section indicates the join point signature and type; (ii) the pointcut section indicates the pointcut signature; (iii) the advice section shows the code that has to be injected at the pointcut; and (iv) the aspect static crosscutting section illustrates the static crosscutting.
"in this paper, we focus on the problem of virtual machine placement appeared in initial plan stage of datacenters management and implement virtual machine placement algorithm exploiting constraint programming approach in cloudsim. the objective is to cut datacenters operating costs and improve resource utilization by reducing the number of physical machines and using virtualization technology. in order to achieve this objective, we formulate the virtual machine placement problem as csp, and then choose choco to solve this problem. we experimentally evaluate our approach and compare it with the built-in virtual machine placement algorithm in cloudsim and ffd algorithm. experimental results show that constraint programming-based virtual machine placement algorithm can efficiently reduce the number of physical servers compared with ffd algorithm and cloudsim built-in placement algorithm, although suffers from relatively long search times."
-c1: the total of resource requests of virtual machines sharing the same physical machine are smaller or equal to resource capacity of physical machine hosting them.
"software systems have become more complicated because their size has grown dramatically as new distributed system modules and system components have been developed and, in addition, system users have become more diverse. the existence of such complex systems has led to the manifestation of new crosscutting concerns (aspects) and these new concerns crosscut other concerns and core classes. many modelling techniques have been proposed to model these huge systems. one of the most well-known techniques is object orientation."
the proposed approach provides aspectual uml design modelling notation to support aspectj does the proposed approach provide aspectual uml design modelling notations that offer a better means of capturing and representing crosscutting concerns?
"cloudsim is developed for simulation of cloud computing platform and support system modeling of datacenters, resource management and task scheduling. in cloudsim, the task of virtual machine placement is mainly completed by vmallocationpolicysimple class which selects a physical machine with the most number of available process units to create virtual machine iteratively. the virtual machine placement policy will use up all physical machine as long as the number of virtual machines is greater than the number of physical machines in datacenter, and this motivated us to design a virtual machine placement policy to reduce the number of physical machines that can host all virtual machines."
"object-oriented modelling has many advantages and benefits that have been outlined in the literature and its usability in modelling software systems has been proved [cit] . however, oo modelling has weaknesses in terms of modelling aspects for the following three reasons: (1) it attempts to model an aspect by treating it as an object, which is semantically incorrect; (2) it does not have the modelling notations and mechanism to capture the constructs of the aspect; and (3) it does not maintain a high level of consistency between the various software life cycle stages [cit] . to date, there is still no consensus in the aom community on a standard set of extension modelling notations [cit], but the aom community is motivated to investigate and find a solution for the above mentioned issues."
"furthermore, this research attempts to propose an approach to model aspects because there is a lack of standard design notation in aom to design an ao program [cit] . aspects are responsible for producing spread and tangle code representation that is spread across and tangled throughout the various software stages; hence an approach that can model and represent these crosscutting concerns effectively is vital."
"this notation is used when a few message communications represent \"around\" advice. this notation can be used to add any text or comments. \"precedence\" is an aspect-aspect relation this semantic rule means that \"precedence\" is an aspect-aspect relationship. this means that the user (system analyst (sa)) should not attempt to use the \"precedence\" relationship with a class at any end. the \"precedence\" relationship takes place only between aspects, when both have to eject a piece of code at the same join point. so the \"precedence\" relation sets which of the two aspects executes the code first."
"as an example, figure 9 gives the asd for a tracing aspect. it shows how the aspectual sequential interactions take place. there is one aspect called tracemyclasses. the circle represents the advice location and type. if there is a piece of code (advice) to be injected before the join point then the before advice circle is provided, whereas if there is a piece of code (advice) to be injected after the join point then the after advice circle is added."
(h4) the proposed approach provides a better means of modelling aspects through the use of the proposed aspectual modelling steps. is the proposed approach more comprehensive than the other ao approaches?
the proposed aspectual uml approach provides aspectual semantic rules to control the structure of the model. does the proposed approach provide aspectual semantic rules that offer a better means of representing the semantics of the crosscutting concerns compared to the other ao approaches?
"server virtualization technology is an effective approach to improve the efficiency of resources and save energy while provides performance guarantees for applications. using server virtualization technology, application runtime environment is encapsulated in a vm(virtual machine), and one or more virtual machines host on a pm(physical machine). virtualization technology can provide performance isolation between vms resided on the same pm, and avoid performance degradation caused by greedy or malicious applications . moreover, it is helpful to improve resource utilization and save costs by multiplexing of datacenters resources across applications. furthermore, virtualization technology enables vm live migration to handle dynamic workloads and meet application performance requirements."
"constraint programming has been widely used in a variety of domains such as production planning, scheduling, timetabling and product configuration. the basic idea of constraint programming is that user formulates a real-world problem as a csp(constraint satisfaction problem) and then a general purpose constraint solver calculates solution for it. formally, a csp is defined by a triplet (v ariables, domains, constraints), where constraints are the set of constraints being responsible for pruning the search space."
"the general conceptual framework of this research consists of four phases. the first is the theoretical phase, which consists of a review of the literature (lr) on the relevant topics, issues, and problems of this research. the second is the principles and framework suggestion phase, which puts forward the proposed bottom-up methodology and the research framework. this includes aspectj mapping from programming level to design and analysis level to model aspects using aspectual uml diagrams. in the third phase, the proposed methods and concepts are designed the scientific world journal 7 and implemented. in the fourth and final evaluation phase, several evaluations are performed including a focus groupbased evaluation to validate the applicability of the proposed approach and tool. also, the modularity of the proposed approach is evaluated using a \"good design\" criteria-based evaluation approach. the proposed concept is then evaluated by implementing and assessing a prototype tool that carries out the proposed theoretical part of this research work."
"\"crosscutting\" is an aspect-class relation this semantic rule means that \"crosscutting\" is an aspect-class relationship. this means that the sa should not attempt to use it with no class at any end. the \"crosscutting\" relationship takes place between an aspect and a class to show that there is crosscutting."
(iii) real project. case studies have to be real projects. the eotss was designed by the researcher as part of the researcher's master's degree while the sccldms was designed by the researcher for the researcher's previous employer.
"behavioural diagrams present the dynamic aspects of the functionality of a system, but existing uml behaviour diagrams are unable to represent crosscutting concerns and their constructs. figure 8 shows the aspectual behaviour diagrams that were considered in the proposed comprehensive approach. the researcher proposes some new extensions of the notations for these diagrams to represent crosscutting."
(iii) explains the two selected case studies and how they were modelled using the three approaches and shows the participants the drawings of each case study generated by each approach; (iv) gives the participants a small case study to try to model using the proposed aspectual uml approach and gives them the option to try the proposed aspectual universal modelling language (auml) tool as well;
"that was the case for the acd. the proposed approach gives programmers the ability to model the advice and its types by using the other aspectual uml behavioural and structural diagrams as well. finally, it should be noted that the proposed approach does not pay attention to all the detailed matters concerning advice such as the accurate signature, modifiers, and precedence of the advice types because the researcher cares about preserving the soc and does not want to interfere in the implementation stage."
"evaluation is a process of collecting and assessing the evidence against specific criteria for functionality and assurance. it can result in a measure of trust that indicates how well a system meets particular criteria [cit] . this section presents the qualitative evaluation methods used to assess the proposed approach. the overall aims of this evaluation process are (1) to test that the proposed approach has appropriate functionality, (2) to address whether the proposed aspectual uml approach offers a better means to handle aspect modelling compared to the other aom approaches, and (3) to confirm that the proposed aspectual uml approach provides a better means for modelling crosscutting concerns. various studies (e.g., [cit] ) were reviewed to identify which evaluation methods would be best suited to perform this assessment. as a result, it was decided to use the following qualitative evaluation methods: (1) a focus group-based evaluation [cit], (2) a \"good design\" criteria-based evaluation (the researcher consulted a linkedin group for researchers working in the field of aspect orientation called the \"aspect-oriented software development group, \" which advised using this method along with other evaluation methods. furthermore, [cit] defined these \"good design\" criteria), and (3) an aspectj support-based evaluation (it is a kind of think-aloud method proposed by prof. omar aldawud (aom expert)). here, the researcher presents only general steps of the focus group evaluation method conduction."
"however, adjusting the configuration of virtual machine and performing live migration manually obviously does not meet the needs of datacenters management. we need automate and intelligent approaches to address virtual machine management issue. management of virtual machine consist of two phases: initial plan and runtime management. in initial plan stage, given a set of virtual machines and resource requirements of each virtual machine, as well as a set of physical servers, we need map each virtual machine onto physical machines. the goal of initial plan stage is to minimize the number of physical servers that host all virtual machines. in runtime management stage, we need to adjust configuration of virtual machines or perform live migration of virtual machine according to dynamic workloads. the purpose of runtime management is to minimize the number of virtual machines migration considered migration costs, while meet application slas. in this paper, we focus on the problem of virtual machine placement appeared in initialization plan stage of datacenters management with the goal of minimizing the number of physical machines. we first formulate the problem of virtual machine placement as a variant of multi-dimensions bin packing problem, and then exploit constraint solver to solve this problem with the objective of minimizing number of physical machines. finally, we implement virtual machines placement algorithm using constraint programming technique in cloudsim [cit] . experimental comparison with other heuristic methods verifies the effectiveness of constraint programming-based approach."
"by conducting an slr following the above detailed steps, the researcher found that there are 14 relevant and main studies relevant to the topic addressed in this paper. based on an analysis of these 14 studies, the researcher also categorized the existing uml modelling approaches into the following four types:"
"discussion. six hypotheses were formulated in this research. to validate these hypotheses, a questionnaire was distributed to the participants by the end of the focus group. the results are shown in figure 19 ."
"the \"name/:\" notation is used to indicate the name of the aspectual reference notation if needed. after that, the xml file is used as an input to the aspectj pseudocode generator, which is a parser as shown in figure 13 . this means that it searches and parses the xml files for special kinds of tags to generate a pseudocode; the generator gives the programmer the option to extract and copy the pseudocode. it also has the option to generate it in txt file format. the software programmer then needs to work on the pseudocode generated to convert it to real code. this process is illustrated in figure 14 . the proposed aumt is a standalone tool for modelling and representing crosscutting concerns (aspects) and their crosscutting nature using all the proposed aspectual uml diagrams. the proposed tool supports aspectj and provides a standalone environment for software designers which supports them in capturing, modelling, documenting, and representing aspects and their constructs to produce correct software design and analysis artefacts, which are inputs in the later software development stages. this is crucial because, without such a tool, the erroneous representation of crosscutting concerns will result in inaccurate output, which will eventually lead to a lot of challenges in the later stages of development."
"operating costs of datacenters is rapidly increasing with scaling of datacenters. however, datacenters resources are seriously underutilized. energy consumptions of it devices in servers clusters is the largest contributor to operating costs. [cit] the total electricity consumptions of datacenters in us cost of about $4.5 billion. [cit] . however, statistics in sports, e-commerce, and financial domains show the average server utilization varies between 11% and 50% [cit] . the leading reason of underutilization is that administrator allocates hardware resources for hosting applications according to peak requirements of applications, although peak workloads of applications may not appear frequently. furthermore, workloads of applications are dynamic, as well as there are stringent requirements for applications performance, such as applications throughput, response time, and cloud computing platform must meet applications sla(service level agreement). it is a big challenge for cloud datacenters to reduce operating costs and improve resource utilization while meet applications slas."
"in order for the researcher to formulate accurate hypotheses, the researcher has to list all aspectual uml approach features, map them to questions, and then convert the questions into hypotheses, as shown in figure 18 . table 12 summarizes the mapping between the features, questions, and hypotheses."
"as stated above, uml is one of the most well-known oo modelling languages in the industry [cit] . it has been built to support the concepts of object orientation. taking into consideration that aspect orientation is an extension to object orientation [cit], it appears to be logical to investigate whether it would be possible to extend uml to support aspect orientation. indeed, many approaches have been proposed in this regard. the majority of these existing approaches depend on a uml lightweight extension mechanism to support aspect modelling. according to the uml 2.4 infrastructure, the profile mechanism (lightweight) is not a first-class extension mechanism, where it does not allow modification to the existing metamodel; rather, the intention of profiles is to give a straightforward mechanism for adapting an existing metamodel. however, the first-class extensibility is handled through metamodel extension, where there are no restrictions on what one is allowed to do with a metamodel: one can add and remove metaclasses and relationships as one finds it necessary. of course, it is then possible to impose methodology restrictions that one is not allowed to modify the existing metamodels but only extend them [cit] ."
"this semantic rule means that \"aspect crosscutting\" is an aspect-aspect relationship. this means that the sa should not attempt to use the \"aspect crosscutting\" relationship with a class at any end. the \"aspect crosscutting\" relationship takes place when two aspects are cut across each other."
"combining the definition of virtual machine placement problem and the basic idea of constraint programming, the problem of virtual machine placement can by modeled by csp in the following way: requests of virtual machines hosted on pm i . -solutionn um denotes the number of physical machines used in the solution for virtual machine placement problem."
"after formulating the virtual machine placement problem as csp, we now choose a constraint solver to solve the problem. constraint programming is often realized in imperative programming by software library, such as geocode, jacop and choco. we choose choco as our constraint solver that is compatible with cloudsim. the algorithm for constraint programming-based virtual machine placement as follows:"
"since the modelling stage is critical, aspect-oriented modelling (aom) has become a hot topic in research circles. aspect-oriented modelling focuses on the process of identifying, analysing, managing, and representing crosscutting concerns in aosd. at the same time, the uml is the most well-known and widely used modelling language in the industry, so it is not surprising that there is also interest in investigating uml support for aom."
"development management system. the sccldms is a real project that was developed by the researcher for a previous employer, scicom, as part of the nokia contact center project. the specification was obtained from the company's training managers, human resource managers, and on-floor operation managers. the sccldms includes the necessary features to meet the objectives of an organization such as keeping records, analysing data, and preparing reports to aid effective planning and decision making, which all are carried out online. this case study is chosen for two main reasons: (1) the researcher is quite familiar with the nature of the project and the environment which facilitates better and more accurate case study modelling and (2) it is a middle-sized case study."
"from table 3, we can find that the performance of ffd algorithm is optimal and valid that ffd algorithm can provide a fast and but often non-optimal solution. when the ratio of number virtual machines to number physical machines our experiments conclude that constraint programming-based virtual machine placement algorithm can find better solution than ffd algorithm and cloudsim built-in placement algorithm with the goal of minimizing the number of physical machines hosting all requested virtual machine within timeout. although constraint programming approach suffers from relatively long running times, we argue that time performance of virtual machine placement algorithm based on constraint programming is also acceptable because initial configuration of virtual machines is a task of planning and normally takes a long time. in practice, it is needed to trade-off between minimizing the number of physical machines and performance."
(iv) familiarity. case studies have to be familiar to the researcher so that the researcher can analyse them critically and then design and model them appropriately by using the proposed aspectual uml approach. these two case studies meet this criterion because the researcher was directly involved in them.
"object orientation has the potential to enhance software modularity and reusability by using polymorphism and inheritance mechanisms. however, it lacks the ability to address the separation of concerns (soc) concept [cit] . concerns are represented as classes and objects when analysing, designing, and programming object orientation. however, complex systems usually have \"crosscutting concerns\" which, as the term implies, cut across functional modules and components, thereby increasing their interdependencies, which leads to a reduction in their modularity and the generation of scattered and tangled code in the implementation solution. in the object orientation paradigm, these crosscutting concerns, which are also known as aspects, are not efficiently represented in object orientation."
"the term  denotes target stiffness, which defines how much will the robot deviate from the commanded position when we apply a disturbance force f . this target stiffness is a design parameter, that we will set and keep constant for the entire task. it enables to set the controller parameters k from the estimated environment stiffness as"
"medical information systems (mis) and information technology (it) form the infrastructure of telemedicine that incorporates computer to store, process and exchange medical information remotely [cit] . during transmission, digital medical images and confidential patients' information should be protected from attackers. therefore, different techniques have been presented to provide protection of medical images and electronic patient records (epr) [cit] . these methods are based either on cryptography or steganography techniques. cryptography methods use encoding to hide the meaning of the message, but they cannot hide its existence [cit] . however, the transmission of encoded text certainly stimulate intruders' attention, whom may try to decode it. steganography on the other hand is the art of concealing the existence of the secret message within other media, such as text, image, audio and video without drawing attention of unintended users [cit] . combining steganography and cryptography aim to make it difficult for attackers to extract the sensitive information [cit] . this type of integration does not embed readable text, but embeds encrypted text to add another layer of security."
"niques can be applied to virtually any nmt system. according to the preliminary results of the manual evaluation, the final translation quality is comparable to or even better than the quality of human references."
"the three reported automatic metrics are: casesensitive (cased) bleu, case-insensitive (uncased) bleu and a character-level metric chrf2 (popovi, 2015) . we compute all the three metrics with sacrebleu [cit] . the reported cased and uncased variants of bleu differ also in the tokenization. the cased variant uses the default (ascii-only) for better comparability with the results at http://matrix.statmt.org. the uncased variant uses the international tokenization, which has higher correlation with humans (machek [cit] and noticed that in many cases the human reference translation is actually worse than our transformer output. thus the results of bleu (or any other automatic metric comparing similarity with references) may be misleading. table 3 the reports results of all english [cit], according to both automatic and manual evaluation. for the automatic evaluation, we use the same three metrics as in the previous section (just with wmt18 instead of wmt17). for the manual evaluation, we report the reference-based direct assessment (refda) scores, provided by the wmt organizers."
"our research focuses on pbd of assembly operations, where the robot passes from an unrestricted movement to the movement in a contact with the environment. for the stable operation, the robot should fulfill position and force constraints arising from the environment [cit] . the impedance control is generally accepted paradigm for providing intrinsically safe robot actions while interacting with the environment [cit] . however, it requires at least an approximate model of the environment in order to appropriately set the control parameters. in order to accomplish an assembly operation, we have to define robot poses, forces and torques and determine the required impedance parameters. during the kinesthetic guidance, forces and torques can be accurately obtained by reading robot sensors, while the determination of the impedance parameters is more challenging. the problem with kinesthetic guidance is how to accurately demonstrate the required task since the operator has to manually guide gravity compensated robot arm to perform the desired task while avoiding singularities, joint limits and resolving kinematic redundancies. even with multiple demonstrations, it is not trivial to accurately define complicated motion parameters at the required speed. some of the above-mentioned problems can be solved from multiple visual observations of humans performing the desired task. after capturing the human motion, the robot builds the model of the task and optimizes its motion taking into account the joint limits, singular configurations, and kinematic redundancies [cit] . however, the problem of how to model the task with the desired accuracy required for the assembly operations and how to capture interaction forces and torques beside the task impedance parameters remains. by joining both approaches, we can benefit from advantages of both worlds, which are: the ability to globally model and optimize the task from visual observations and the ability to precisely capture the motion and force/torque parameters during the kinesthetic guidance. this joined approach was applied in the work of lee and ott [cit], where the robot incrementally refines the initially demonstrated motion by kinesthetic guidance using the concept of the refinement motion tube [cit] . the motion tube determines the range of allowed deviations from the nominal path within which the operator can modify the path. from previous demonstrations, they calculated the covariance matrix of the spatial part of the nominal trajectory and associated it with the robot controller impedance parameters. the gaussian mixture model (gmm) and the gaussian process regression (gpr) was used to decouple the spatial part from the temporal part of the trajectory. the idea of the refinement tube was used also for the bi-manual human-robot cooperation scheme, where the dynamic motion primitives (dmp) were used to encode the nominal trajectory [cit] . motion refinement tube was defined regarding the frenet-serret (fs) frame and the decoupling from the synchronization between the spatial and temporal part of the trajectory was accomplished by projecting the actual velocity to the fs frame and adjusting the dmp speed scale factor."
"the paper is organized into 6 sections. in the next section, we briefly describe learning from observation (lfo) framework, where we generate nominal bi-manual trajectory from multiple demonstrations and encode it with dmps. our approach to the refinement of the nominal trajectory with kinesthetic guidance is presented in section iii. in section iv we tackle few issues related to the bi-manual execution of assembly tasks. the whole framework was experimentally verified in a generic bi-manual assembly task, described in section v. final conclusions and possible future extensions are discussed in section vi."
1. identification of roi and roni: the otsu's method is utilized to identify the threshold value that reduces the difference of the black and white pixels.
"supp fig 12: profiles of top genes identified by psupertime in 2i-treated mef cells. psupertime applied to 3600 cells over labels d0 to d8 under dox condition; 3600 over labels d9 to ipscs under 2i condition [cit] . 20 genes with highest absolute coefficients, plotted against psupertime pseudotime. x-axis is the values from projections of each cell by psupertime. y-axis is smoothed, z-scored log pseudocounts for each cell. colours indicate ordered labels. black line is smoothed curve as fit by geom smooth in the r package ggplot2 [cit] . fig 13), however psupertime is better able to classify the cells than lasso (supp 426 fig 14) . these results could be expected: treating the sequential labels as integers to be 427 regressed against, as in lasso, is optimizing for correlation rather than separation, while the 428 thresholds between labels give psupertime additional flexibility as a classifier. taken together,"
"the psupertime procedure results in a set of coefficients for all input genes (many of which 261 will be zero) that can be used to project each cell onto a pseudotime axis, and a set of cutoffs"
"this issue can be addressed by calculating the psupertime ordering, and reviewing all genes 524 that have high correlations with the genes identified by psupertime. alternatively, a trivial 525 extension to psupertime would allow training with a combination of l1 and l2 penalties (the 526 elastic net), resulting in a compromise between sparsity and prediction performance."
"assuming that the estimated environment stiffness is below the desired target stiffness. if this is not the case, eq. 34 results in negative gains k; we set the k a,i, k r,i to a small positive value making the robot very complaint. according to this simple method, the robot will be compliant in the directions constrained by the environment and vice versa, stiff in unconstrained directions. evaluation, we selected a generic assembly task, composed of two peg-in-hole (pih) operations, one in relative and the other in absolute coordinates. the robot had to assemble two parts of the vacuum cleaner tube and release it on the pedestal. the initial task demonstration was performed by a human, where the joint angles of both human arms were tracked with kinect-2. joint angles were mapped to the cartesian coordinates and further to the absolute and relative task of the bi-manual setup, as explained in section ii. as the kinect tracker is not capable to track the demonstrator's motion with the precision required for the assembly operation, the demonstrated policy needed additional refinement."
"we assume that the robot is interacting with the environment only through the manipulated object. our goal is to determine the effective compliance, which unifies tool and environment compliances, expressed in absolute and relative coordinates of a bi-manual robot, as illustrated in fig. 3 . in this work we assume a simple model of the effective positional compliance [cit]"
"we found out that the czech monolingual data set [cit] contains many english sentences. those sentences were either kept untranslated or paraphrased when preparing the synthetic data with backtranslation. thus the synthetic data included many english-english sentence pairs. consequently, the synth-trained models had a higher probability of keeping a sentence untranslated."
"the perturbations discussed here correspond to potential mislabelling of the data, which robustness of psupertime to perturbations of labels. rows correspond to measures of psupertime's performance: classification error is the proportion of labels correctly assigned by psupertime; cross-entropy is a measure of confidence in predictions, which is high when a correct label is predicted with high probability; proportion of non-zero genes indicates what proportion of the input genes for which psupertime identified non-zero coefficients. both classification error and cross-entropy are shown for test data, i.e. cells which were not used to train psupertime. the line range shows interquartile range, dots show minimum and maximum values observed, both over 20 random runs. the x-axis corresponds to the datasets detailed in table 1. ing structure, in which progress along the biological process is accompanied by a bimodal (or dppa5a, tmem176b in supp fig 12) . population which is related to the sequential labels is not expected to affect the performance of 382 psupertime."
"for example as input to dimensionality reduction algorithms, resulting in a low-dimensional 465 embedding based only on genes specific to the biological process in question."
"psupertime is applicable to many of the increasing number of single cell rna-seq studies 528 being generated. it shows consistently better performance than benchmark methods, due to 529 use of sequential labels as input. the conceptual novelty of identifying genes via ordinal logistic 530 regression both permits genes relevant to processes annotated with sequential labels to be 531 identified, and suggests new ways of using these labels to understand the genes involved in such 532 processes."
"exchange of medical information between hospitals and medical clinics placed in different areas is a habitual practice. therefore, numerous methods have been introduced to provide secure storage and transmission using information hiding techniques."
"to separate the cover image into two regions, a threshold value is defined to differentiate between roi and roni i.e., the pixel is related to roi, if ). this will be followed by concatenating the hh1, lh1 and hl1 sub-bands in vertical order. in order to improve the embedding capacity and to detect the edge regions accurately, the absolute value is applied to the high sub-bands to generate positive coefficients. also, the original coefficient sign (t sign ) is preserved in a temporary matrix to ensure a good quality of the stego image. 5. identification of edges: according to the fact that human eyes can bear more modifications in edge regions compared to uniform regions, we attempt to improve the stego image quality through embedding the message bits into edge regions. this will be achieved through an efficient edge detection scheme that uses overlapping blocks to identify and classify image blocks according to their edge strength, where more bits can be hidden into the strong edges and less bits into the smooth ones. moreover, the aim of using overlapping blocks is to enhance the embedding capacity by reducing the number of unused pixels. the implementation of this process is explained in the following steps:"
"the quality of neural machine translation (nmt) depends heavily on the amount and quality of the training parallel sentences as well as on various training tricks, which are sometimes surprisingly simple and effective."
"we used a third approach, termed concat regime, where the authentic and synthetic parallel data are simply concatenated (without shuffling). we observed that this regime leads to improvements in translation quality relative to both mixed and fine-tuned regimes, especially when checkpoint averaging is used."
"(b) image edges are classified into four directions. therefore for each block, the magnitude value for each direction is computed using eq. 1. (c) the final magnitude value (e) of each block is the maximum of the four values. the secret data (m ) is embedded into the four shaded pixels (p 22, p 23, p 32, and p 33 )."
"fast deployment of robot programs is one of the challenges of contemporary robotics, which addresses both robots in production lines as well as the service and humanoid robots in our home environments [cit] . the ultimate goal is to enable an inexperienced operator to efficiently generate complex robot actions in a human-friendly way. one of the most successful paradigms towards this goal is the programming by demonstration (pbd), referred to also as the imitation learning [cit] . it involves many aspects beyond a simple copying of human demonstrated motions, such as context understanding, generalization to different contexts, capturing and interpretation of interaction forces and compliance, etc. [cit] . due to its simplicity, predictability, and ability to learn difficult dynamic tasks in reasonable amounts of time, this is a promising route to equip robots with necessary functionalities for performing a large variety of tasks, ranging from the classical industrial production tasks to the applications in households, stores, hospitals, etc. this is why pbd humanoid & cognitive robotics lab, department of automatics, biocybernetics and robotics, joef stefan institute, jamova 39, 1000 ljubljana, slovenia, 1 robolab, faculty of electrical engineering, traka 25, 1000 ljubljana, slovenia, (bojan.nemec, leon.zlajpah,ales.ude)@ijs.si, (sebastjan.slajpah, jozica.piskur)@robo.fe.uni-lj.si is still one of the most intense research topics in the field of robotics."
"for playback, we switched to the bi-manual impedance control, as described in section iv. we set the target stiffness parameter  to 1000n/m and limited the minimal robot stiffness to 100n/m for all positional coordinates. in this experiment, we adjusted only the positional part of the robot stiffness in relative and absolute coordinates according to (34). initial environment stiffness was set to zero. therefore, the robot was initially stiff in all coordinates, which assured the precise tracking of the demonstrated trajectory. during the pih in relative coordinates, the robot lowered the stiffness to the minimal stiffness in x and z relative coordinates, while the stiffness along the axis of insertion y remained at a relatively high value of approx. 750n/m. we set also the initial values of the w a matrix to 0, which means, that the robot executed only the relative part of the task, while the redundancy resolution minimized kinetic energy during the motion. therefore, the actual robot motion in absolute coordinates was different from the one demonstrated. intuitively, we increased the weights of w a prior an absolute force was detected, indicating that the robot interacts with the environment and that the absolute coordinates become important. the robot performed a smooth transition from the part of the task, where only the relative coordinates were relevant to the part of the task, where the robot precisely tracked also the absolute coordinates. this can be seen from the measured joint velocities of both robots, plotted in fig. 8 . from fig. 6 we can see that in most parts of the trajectory we could increase the velocity of the task execution. it was necessary to decreased it only for the peg insertion in order to increase the reliability of the whole assembly task."
"to minimize the computational cost, edge detection method is applied only once using a threshold of value 4. then, edge blocks are classified into five groups according to the magnitude value (e) as shown in table 1, where sharper edges, such as g 5 and g 4, have a higher priority than the weaker edges, such as g 1 and g 2 . 6. the xor operation: n-bits are embedded in each roni edge coefficient, where the value of n is determined by the magnitude value (e) of each block. the algorithm begins with g 5 coefficients by utilizing three bits from each coefficient to embed the secret message. if g 5 coefficients are not enough for embedding, then g 4 coefficients are utilized (three bits from each coefficient). if the message is not fully embedded, then move to the lower priority groups (g 3 then g 2 and finally g 1 ). the xor operations are applied to embed two secret bits m 1 and m 2 into three bits p 1, p 2, and p 3 of the roni coefficients as follows:"
"we performed comparisons between psupertime and other unsupervised methods on the the design of many single cell studies results in sequential groups of cells (see for example 503 reviews on development [cit] and aging [cit] ). psupertime has been developed for single cell 504 rna-seq data, however it could in principle also be applied to other single cell data such as 505 mass cytometry [cit] . we have shown good performance for psupertime for single cell rna-seq 506 data, even though we would not a priori expect that a biological process would correspond to a 507 linear combination of gene expressions. this may be the result of the high dimensionality of the 508 dataset, which provides a large set of features with which to approximate a non-linear process."
"impedance control is very appropriate for the assembly operations, as it provides stable operations in contact and not contact motions and transitions between the two [cit] . here, we briefly present impedance control for a bi-manual robot, which provides tracking of the commanded control variables obtained by the proposed kinestetic guidance. commanded"
"to restrict the analysis to relevant genes and denoise the data, psupertime first applies pre-197 processing to the log transcripts per million (tpm) values. specifically, psupertime first re-198 stricts to highly variable genes, as defined in the scran package in r, i.e. genes that show above the expected variance relative to genes with similar mean expression [cit] . in the case where the number of input variables is high relative to number of observations 231 and may include many uninformative variables, as is common in single cell rna-seq, it can be 232 helpful to introduce sparsity (i.e. to increase the number of zero coefficients). psupertime uses"
"as a classifier, psupertime can be first trained on cells with one set of condition labels, then 512 used to project new cells onto the associated process. we showed this by applying psuper-513 time to a time course of ipscs allowed to differentiate, and used this to classify ipscs kept in 514 pluripotency-maintaining serum (supp fig 9) . this illustrates potential uses for psupertime psupertime uses l1 regularization to obtain a small set of reported genes. however, this 520 may result in exclusion of other relevant genes: where there are multiple highly correlated genes 521 that are predictive of the sequential labels, l1 regularization will tend to result in only one of 522 these genes being reported, and produce give zero coefficients for other correlated genes [cit] ."
"where k 21 and  21 are the axis and angle that realize the rotation q 1 to q 2 . our framework applies the control in the task coordinates. this is necessary because we have to provide the desired robot compliance in task coordinates, which is more appropriate for performing precise assembly operations. therefore, the singularity and joint limit avoidance will be performed in real time during the task execution, exploiting the kinematic redundancy of bi-manual robot. however, it is very important to choose an appropriate initial robot configuration, which assures that the local optimization will result in a feasible trajectory. the appropriate initial robot configuration is determined iteratively by a global optimization procedure [cit] ."
the nonlinear forcing terms f p (x) and f o (x) are formed in such a way that the response of the second-order differential equation system (6) - (10) can approximate any smooth point-to-point trajectory from the initial position p p p 0 and orientation q q q 0 to the final position g g g p and orientation g g g o . they are defined as linear combinations of radial basis functions (rbfs)
"during the assembly process, the robot should automatically adjust its compliance according to the task constraints gathered during pbd. let consider a simple one-dimensional case as shown in fig. 4 . from the force equilibrium we can fig. 4 . a simple model of the robot interacting with the environment. f is the disturbance force acting at the robot end effector,x is the displacement due to this force, and k and p are the robots and the environment stiffness, respectively."
the number of non-zero genes required to achieve a given level of classification error (i.e. spurious genes where there is no structure to the data.
"l1 regularization to do this [cit] . our approach is based on that in the r package glmnetcr [cit], 234 which reformulates the data and associated likelihood functions into one single regression model,"
"we deleted phrases repeated more than twice (immediately following each other); we kept just the first occurrence. we considered phrases of one up to four words. with the training-data filtering described in section 3, less than 1% sentences needed this post-processing."
"the last phase of learning is actual speed learning. once we are satisfied with the trajectory, we push the robot from the start to the trajectory end with the desired speed, capture (x) and calculate weights v j, which encode (x) as a linear combination of the rbfs (16) . note that nothing restricts this motion to be only in the direction of the nominal trajectory. we can freely demonstrate such (x) which goes forward and back along the nominal trajectory. in this way, we can efficiently program quasi-periodic motions and encode tasks like sawing, polishing, wiping, material tooling, etc. with a minimal number of parameters."
"the thresholds indicate the points along the psupertime axis at which the probability of 272 label membership is equal for labels before the cutoff, and after the cutoff. the distances 273 between thresholds, namely the size of transcriptional difference between successive labels, is 274 not assumed to be constant, and is learned by psupertime. distances between thresholds 275 therefore indicate dissimilarity between adjacent labels, and thresholds which are close together 276 suggest labels which are transcriptionally difficult to distinguish."
"data derived from mass cytometry is lower-dimensional, and therefore has lower flexibility of 510 marker choice. here, a non-linear model may be necessary to obtain good performance."
"based on these observations, we prepared a cztuned model and a noncz-tuned model. both models were trained in the same way, they differ only in the number of training steps. for the cztuned model, we selected a checkpoint with the best performance on wmt13-cz [cit], which was at 774k steps. similarly, for the noncz-tuned model, we selected the checkpoint with the best performance on wmt13-noncz, which was at 788k steps. note that both the models were trained jointly in one experiment, just selecting checkpoints at two different moments."
"where () denotes dot product and k 1, k 2 are positive scalars, which scale the velocity of the motion along the nominal trajectory. if we apply this  to the set of equations (6-10), the robot moves in the tube along the nominal trajectory, as illustrated in fig. 2 . the axes of the ellipse, which define the refinement tube, depend only on applied force/torque and the robot impedance along the normal n and binormal b axes of the frennet-serret frame. new robot positions and orientations have to be sampled at exactly the same phase as the nominal trajectory and saved as new modified trajectory. in the work of lee and ott [cit] and in our previous work [cit] the robot impedance was calculated according to the covariance matrix that describes the expected deviations around the nominal trajectory. in this work, we relax this requirement and keep impedance (defined with the robot control gains) fixed during the kinestetic guidance. it might happen that this fixed impedance does not allow to modify the trajectory for the desired displacement. we override this limitation by letting the user modify the nominal trajectory in multiple passes. one can simply push the robot back and at the next change of the motion direction, new nominal"
"to take advantage of the fast performance of the glmnet package [cit] . the model originally 236 implemented in glmnetcr is the continuation ratio likelihood; we have extended this approach to implement the proportional odds likelihood, as this model is more appropriate for assessing 238 an entire biological process."
"where w and h are the width and height of the cover image respectively, and c ij and s ij are the gray values of pixel (i, j) of the cover and stego images respectively. the weighted peak signal-to-noise ratio (wp sn r) is an alternative image quality measurement. it improves the classical p sn r as shown in eq. 4."
"we tested the extent to which each pseudotime method could correctly order the cells by 310 calculating measures of correlation between the learned pseudotime, and the sequential labels."
"in the paper, we presented an effective framework for the fast deployment of robot assembly tasks. it is particularly effective for bi-manual learning. however, it can be efficiently used also for single arm tasks, as substantially decrease the time for preparing a new robot application with less experienced robot operators. therefore, we see possible applications of the proposed learning framework in lowbatches sme production lines and for humanoid and service robots performing in our home environments. the framework exploits the impedance properties of the robot manipulator. it can be adapted also to the traditional stiff robot arms equipped with a force/torque sensor mounted in the robot wrist. however, the performance would be degraded while interacting with the stiff environment. future work involves testing in more complex assembly tasks and implementation of the proposed framework for the full-sized humanoid robot talos."
"the above procedure describes the task refinement for a single robot arm. for the dual-arm implementation, we have to assure that both robots are using a common speed scaling factor (x). this means, that when we move one robot along the nominal trajectory, the other has moved as well. the trajectory adjustment is performed regarding the nominal trajectory of each robot independently and not in absolute and relative coordinates because this is more intuitive for the user. task coordinates of each robot are obtained from the absolute and relative coordinates by inverting relations (1) (2) (3) (4) . note that this transformation is unique and always exits. after calculating the update according to (19) (20) (21) for each robot arm independently, we calculate the new nominal absolute and relative trajectory at phase x using (1 -4) . in this way, we obtain a very intuitive way of changing absolute and relative part of the task. if we move just one robot or when we move both robots in opposite directions, we modify relative position coordinates. when we move both robot arms in the same direction, we modify absolute positions. similarly, we modify the absolute and relative orientations. please note that this refers to the moving of robot orthogonal to the tangential vector t, since movement along the t immediately changes the phase (10) ."
"task constraints relate to the forces acting on each robot as a consequence of a bi-manual operation and interaction with the environment. we assume that the forces and torques can be measured with a force/torque sensor, usually mounted on the robot wrist. force readings, mapped to the tool center point of the robot and expressed in the common base, are denoted asf 1,m 1,f 2,m 2 for the first and the second robot, respectively. the corresponding forces and torques expressed in the absolute and relative coordinates are"
"to overcome some of the drawbacks of the existing methods (i.e., the low embedding rates, low visual quality, underflow/overflow problem and protect roi from any modifications), this paper proposes a new information hiding method in the iwt domain to embed the secret data in the edge regions of the high frequency sub-bands."
"to complete the embedding process, the two secert bits m 1 and m 2 are compared to the calculated bits k 1 and k 2 . there are four cases for this comparison:"
"in this work, we apply previously presented framework to robot learning and we propose modifications, that allow an efficient and simple pbd of assembly tasks. we demonstrate the benefits of the proposed approach on a representative bi-manual assembly operation."
"we first restricted to highly variable genes, using the default settings for psupertime. where the order of labels is randomized, the performance of psupertime is reduced, but 359 for some datasets this reduction was small. this may indicate that within the large number of 360 highly variable genes used as input (between  800 and  2900; see table 1 ), there are sufficient 361 genes to recapitulate a given order of a relatively small number of labels, for a relatively small 362 number of observations."
"in this paper, we propose a new medical image steganography for hiding the epr into the coefficients of the integer wavelet transform (iwt) high sub-bands to achieve high level of imperceptibility and security. the embedding process is accomplished in eight steps: (i) separate the cover image into two regions, roi and roni, (ii) reconstruct roni pixels into a square matrix, (iii) histogram modification is performed on roni matrix, (iv) the modified roni is transformed using 2d iwt to get four sub-bands, (v) implement the detection of edge locations in high frequency sub-bands, (vi) apply an xor operation, which minimizes the degradation caused by embedding the secret message, (vii) apply inverse iwt to recover roni pixels, and (viii) combine the roni to get the stego image. our proposed algorithm utilizes the visual characteristic of overlapping blocks of pixels such that it achieves better capacity and maintains high visual quality."
"in czech, as a pro-drop language, it is common to omit personal pronouns in subject positions. usually, the information about gender and number of the subject is encoded in the verb inflection, but present-tense verbs have the same form for the feminine and masculine gender. for example, \"nen doma\" can mean either \"she is not home\" or \"he is not home\". when translating such sentences from czech to english, we must use the context of neighboring sentences in a given document, in order to disambiguate the gender and select the correct translation. however, our transformer system (similarly to most current nmt systems) translates each sentence independently of other sentences. we observed that in practice it always prefers the masculine gender if the information about gender could not be deduced from the source sentence."
"kendall's  considers pairs of points, and calculates the proportion of pairs in which the rank 312 ordering within the pair is the same across both possible rankings."
"restricting to the genes identified as relevant by psupertime results in an improvement 467 in the results of dimensionality reduction algorithms (pca and umap [cit] ), with respect to 468 continuous ordering of the sequential labels ( supp fig 19) . with the exception of the human we have shown that where sequential labels are available, psupertime is able to to identify genes 484 whose expression profiles correspond to this order. it does this even in the presence of substantial 485 unrelated variation, and does so better than benchmark unsupervised methods. psupertime is 486 conceptually simple, and its simplicity allows for several avenues for future development."
"note that the probability given here is cumulative, and that to calculate the probability of an 246 individual label, we have to calculate the difference between successive labels. this results in 247 the following unpenalized likelihood:"
"the least significant bit (lsb) method is the most common steganography methodology [cit] . it is capable of embedding large amount of secret data in a cover image, however it is vulnerable to statistical attacks [cit] ."
"the rest of this paper is organized as follows. the related work is described in section 2. details of the proposed method are presented in section 3. section 4 presents the experimental results, and the conclusion is given in section 5."
"psupertime requires two inputs: (1) a matrix of log read counts from single cell rna-seq, 164 where rows correspond to genes and columns correspond to cells; and (2) a set of labels for the 165 cells, with a defined sequence for the labels (for example, a set of cells could have labels day1, 166 day3, day1, day2, day3 ). (note that not all cells need to be labelled: psupertime can also be 167 run on a labelled subset.) psupertime then identifies a set of ordering coefficients,  i, one for 168 each gene (fig 1a) . multiplication by this vector of coefficients converts the matrix of log gene the process which generated the sequential labels."
"each training (steps 2, 3 and 4) took eight days on eight gpus. [cit] (step 1) took about two weeks and with our transformer models (steps 2 and 3) took about five days. the final model trained in step 4 is +0.83 bleu better than the model trained in step 2 [cit] (cf. table 2 )."
. (11) maps the quaternion describing the rotation between the goal and current pose to the rotation error vector. its inverse transformation is defined as
"in this paper, a new and efficient medical image steganography is presented to hide the epr data into the integer wavelet coefficients of roni. it incorporates edge detection technique using overlapping blocks to identify and hide the secret data in the sharp regions of the image. the motivation behind using overlapping blocks is to improve the embedding payload by reducing the number of unused pixels. the difference between the cover and stego images is reduced by applying an xor operation, which changes one bit at most to embed two secret bits into three pixel bits. the experimental results demonstrate improvements in the embedding capacity, and imperceptibility compared to one of the existing epr hiding methods."
"where  denotes angular velocity, calculated from two subsequent quaternions. subscript (.) n denotes nominal trajectory, obtained with lfo as described in section ii. now, define the direction of the translational motion as the tangential axis of the frenet-serret frame [cit], (see fig. 2"
"as a future work, we would like to assess the relative improvement of each of the five techniques based on manual evaluation (because automatic single-reference evaluation is not reliable when the mt quality is near to the quality of reference translations)."
"these results suggest that psupertime is the appropriate statistical model in terms of both 430 ordering the labels according to the sequence, and accurately labelling the cells. table 1 . colours indicate the benchmark pseudotime inference approaches described in subsection 3.5."
"where free parameters w i,p, w i,o determine the shape of the position and the orientation trajectory. centers c i of rbfs with widths h i are evenly distributed along the trajectory. the temporal scaling function (x) determines variations form the demonstrated speed profile and allows to specify non-uniform speed changes along the demonstrated trajectory. similarly to the forcing terms (13) and (14), it is encoded as a linear combination of rbfs"
"evidence theory provides a representation of uncertainty which is different but related to that given by the traditional probability theory. uncertainty is indeed represented by two measures of likelihood denoted by belief and plausibility. the belief provides a lower bound on the likelihood, and the plausibility provides an upper bound on the likelihood. for a given uncertain input variable, the continuous pdf defined in probability theory is associated to a cumulative distribution function (cdf) or complementary cumulative distribution function (ccdf). ccdf is a single and continuous function. in evidence theory ccdf is replaced by a range (which includes ccfd) between two curves, the complementary cumulative belief function (ccbf) and the complementary cumulative plausibility function (ccpf). basically evidence theory provides a range of uncertainty related to the ccdf. or, in other words, it provides an uncertain bound on the description of a given uncertain variable. the pdf determined following a fully probability approach is replaced with a much simpler representation of uncertainty based assigning likelihood in intervals. this basic measurement, is denoted by basic probability assignment (bpa). in this section the ncrit's pdf shown in fig. 2 rbdo problems based on evidence theory can be formulated in different ways [cit] . typically, as a multi-objective problem such as for this case it would be:"
"the first uncertainty-based design optimization approach, based on probability uncertainty quantification, improves the pseudodeterministic approach by considering uncertainty in a more rigorous fashion though probability distributions, although not sufficient information might be available for certain uncertain variables. the second uncertainty-based design optimization approach, based on evidence theory uncertainty quantification, further improves the previous approach by properly modeling uncertainty based on the actual amount of information available, avoiding assuming probability distributions when not enough information are available. it should be noted that the design optimization problems shown below are intentionally not equivalent (i.e, they do not have the same objectives), and they were devised to show how the optimized airfoil shapes are influenced by different probabilistic optimization formulations."
"future work will investigate the design of airfoils under more complex design scenarios, and assess and improve the uncertainty propagation algorithms in terms of computational cost."
"in recent years, a number of papers on wind turbine airfoil and rotor design optimization have dealt with uncertainty in a more rigorous way [cit] . a common approach to formulate uncertainty is through the classic probability theory, representing uncertain parameters by means of probability distributions. this approach is reasonable if enough information are available to define the distributions. however, this probability approach is weak when little or no information is available on the uncertain parameters [cit] . based on the level of knowledge, uncertainty is classified in two categories: aleatory uncertainty and epistemic uncertainty. aleatory uncertainty is related to the intrinsic variability of the physical system, and experimental data are normally available to define it. epistemic uncertainty is instead related to the lack of knowledge or incomplete information, typically tied to the analysis models. hence, since sufficient data is available, aleatory uncertainty can be represented mathematically through a probability distribution function, and therefore the classic probability theory is a sound approach. on the other side representing epistemic uncertainty by a probability distribution is questionable because there are not enough elements to choose a function over another, and different methods need to be used. one of the approach developed to deal with epistemic uncertainty is the evidence (dempster-shafer) theory [cit] ."
"clean conditions is constrained to be lower or equal to 2.07. the maximum lift is constrained to avoid too large aerodynamic loads in case of sudden increase of the angle of attack over the outboard part of the blade, potentially resulting in structural issues."
"optimizing the airfoil efficiency over an operative range of angles of attack, rather than a single one, allows the airfoil's performance to be less sensitive to variations of the angle of attack due to, for example, sudden variations of the wind which cannot be followed by the rotor controller."
"this paper has presented three approaches to optimize wind turbine airfoils in an uncertain design scenario. uncertainty has been related to the definition of the xfoil's ncrit value: a parameter we have considered affected by both aleatory and epistemic uncertainty. the first optimization approach, denoted by pseudo-deterministic, resembles the common practice to treat uncertainty in a rather simplistic way. the pseudo-deterministic airfoil has been optimized by maximizing the average of its efficiency at the upper and lower bounds of the ncrit's variability range. the second and the third approaches have taken into account uncertainty in a more rigorous way. one through probability theory and the other by means of evidence theory. the optimization using probability theory characterized ncrit's uncertainty through a distribution function, and aimed at maximizing the mean efficiency by constraining its standard deviation. the optimization using evidence theory instead defined uncertainty with less information through likelihood intervals, and aimed at maximizing the reliability of the airfoil efficiency. the resulting airfoils have shown different characteristics based on the proposed optimization approaches. the pseudo-deterministic airfoil, once evaluated in the probabilistic scenario, has achieved the largest mean value, but highest standard deviation. the uncertainty-based design optimization approaches have allowed us to achieve more robust airfoils, limiting the efficiency's standard deviation with small penalty in term of its mean. the largest robustness achieved by these airfoils is mainly tied to a larger relative thickness, which in turn has reduced efficiency at high ncrit values and increased performance at low ncrit values. the largest thickness indeed has allowed these airfoils to have a more gentle curvature on the aft part of the pressure side (than that of the pseudo-deterministic one), which delays transition under low ncrit conditions. between the two uncertainty-based approaches, the most promising one appears to be the one using evidence theory as it has led to an airfoil having also more reliable performance."
"the drag coefficient behavior can be further investigated by looking at the pressure geometries of the optimized airfoils. figure 5 . airfoils' lift-to-drag ratios evaluated at the lower and upper bounds of the ncrit's variability range. distribution and the skin friction coefficient depicted in figures 8 and 9 (both generated at the design angle of attack). under high ncrit conditions, the pseudo-deterministic airfoil minimizes drag by maximizing the extension of laminar flow on the pressure sides. this is achieved with a rather small curvature, which limits the pressure gradient, delaying transition. under low ncrit conditions, the onset of the pseudo-deterministic airfoil's transition is largely anticipated, increasing the skin friction drag due to a greater amount of turbulent flow. the figure 6 . lift coefficient. two airfoils optimized with uncertainty-based approaches limit this situation through a larger relative thickness which allows for a larger curvature in the aft part of the pressure side. this geometry induces a less steep pressure gradient delaying transition, and having more laminar flow that that of the pseudo-deterministic airfoil. under these conditions, despite the earlier occurrence of transition (larger skin friction drag due to larger amount of turbulent flow), the ev. airfoil has lower overall skin friction drag than that of the pr. airfoil due to the lower relative thickness, which reduces the airfoil's cross-section (and hence reduces the skin friction drag)."
"for this work, the adopted formulation aimed at determining only one point of the pareto front, i.e. an airfoil shape that maximizes the belief to deliver an efficiency (eq. 1), which is at least 190 (we defined this arbitrary based on the pseudo-deterministic optimization). basically this approach aims to increase our confidence that the airfoil deliver at least an efficiency of 190. this approach differs that the probability one in the sense that here we ask for more performance reliability (to have efficiency at least 190) rather than a general robustness."
"where f det is the objective function of the pseudo-deterministic problem expressed by eq. 3, t is the airfoil relative thickness, while c l,des, s and c l,max are respectively the design lift coefficient, stall margin and maximum lift coefficient. b l and b u are respectively the lower and upper bounds of the design variables' variability ranges."
"the aerodynamic design of wind turbine airfoils is largely affected by uncertainty. the sources of this uncertainty are of different nature, and they are mainly related to the highly stochastic nature of the wind, to manufacturing errors, to leading edge erosion, to the limited fidelity of the aerodynamic models that are used to perform the simulations, and to these models' parameters setup. based on the current industrial practice, the aerodynamic design of wind turbine airfoils is performed deterministically or, in the best cases, following arbitrary and rather unmethodical engineering approaches to include the effect of the different sources of uncertainty. when designing wind turbine airfoils, the inadequate treatment of uncertainty may lead to inefficient airfoils, having poor aerodynamic performance or not meeting the design requirements under off-design conditions. properly quantifying and including uncertainty in the aerodynamic design of wind turbine airfoils allows designers to achieve more robust and reliable airfoils, improving the overall performance of wind turbines and reducing the cost of wind energy."
"the pseudo-deterministic optimization and the two rbdo processes based on probability and evidence theories, all solved with the multi-population adaptive inflationary differential evolution algorithm (mp-aidea) [cit], determined the three different airfoils depicted in figure 4, denoted respectively by det., pr., and ev.. the top part of table 2 compares these three solutions based on the pseudo-deterministic objective function and constraints (eq. 2). as expected, the pseudo-deterministic solution scores the highest pseudo-deterministic objective function, as this airfoil was indeed optimize to maximize this function. it is also noted that the ev. airfoil achieves the highest value of the efficiency at the lowest ncrit. this is also an expected result as this airfoil was optimized to increase the reliability of efficiency at values around 190. the bottom part of table 2 evaluates the optimized airfoils based on the mean and standard deviation of the efficiency. the pseudo-deterministic airfoil achieves the highest mean but also the largest standard deviation. the other two airfoils achieve around 25% lower standard deviation at the cost of around 1.4% reduction in the mean value. 8.8 6.7 6.2 table 2 . comparison of the optimized airfoils' pseudo-deterministic objective function and constraints (top part), and the mean and standard deviation the airfoils' efficiency (bottom part). figure 3 shows the ccdf, ccbf and ccpf for the three optimum airfoils. these curves represent the probability that a selected airfoil delivers at least a given efficiency. for example, the probability that the three optimized airfoils deliver an efficiency which is at least (or larger than) 190 is around 95%. by looking at these curves one can notice the fundamental difference between the probability and the evidence theory uncertainty quantification. in the probabilistic approach, the characterization of the input uncertainty (i.e., ncrit) through a continuous distribution function resulted in a continuous ccdf (of the output f ). following the evidence approach, we instead characterize ncrit's uncertainty through discontinuous intervals, and this resulted in discontinuous f 's ccbf and ccpf. the region between ccbf and ccpf represent the uncertainty in the probabilistic representation of the uncertain output function. as mentioned above, the ev. airfoil has been optimized to maximize the ccbf for values of the efficiency larger than 190. the result is an airfoil which is indeed more reliable than the other two airfoils for value of f larger than 190 (ccbf more on the right)."
"in this optimization, we enforced the probability that each constraint was verified to 1 (i.e., constraints always satisfied). mean, standard deviation and constraints in eq. 4 were obtained through monte carlo sampling based on the ncrit's pdf in 2"
"the aim of the paper is twofold. the first goal is to present and compare three reliabilitybased design optimization (rbdo) approaches to include uncertainty in wind turbine airfoil design (rbdo here indicates the probabilistic treatment of both objective function and constraints). to this aim, we have devised a simple probabilistic design scenario in which uncertainty incorporates an aleatory component, related to the inflow characteristics, and an epistemic source, related to the definition of a key parameter in the airfoil aerodynamic model, namely xfoil's ncrit [cit] . the three approaches proposed by this paper deal with uncertainty in different way. the first approach considers uncertainty in a simplistic way, somehow resembling the current industrial practice. the second one uses classical probability theory to model both aleatory and epistemic uncertainty, although not enough data is available to confidently define a distribution function for the epistemic component. the third approach models epistemic uncertainty by means of the evidence theory. the second goal of the paper is on the one hand to understand how different airfoil shapes can be achieved by including a proper treatment of uncertainty, and on the other hand to understand how different probabilistic optimization objectives affect the airfoils' shape and the aerodynamic performance."
"the goal of the pseudo-deterministic design optimization was to maximize the mean of the efficiency (eq. 1) evaluated at two ncrit values, namely at 0.5 and 9.5. these are respectively the lower and upper bounds of the ncrit's variability range. this optimization was denoted by \"pseudo-deterministic\" because the objective function and constraints were valuated through two deterministic calculations. the pseudo-deterministic optimization was formulated as follows:"
"2.3. rbdo based on probability theory uncertainty quantification as seen above, the pseudo-deterministic optimization considered only the lower and upper bounds of ncrit's variable range. this approach neglects the actual probability or likelihood of ncrit to assume values between these two extreme bounds. the approach presented in this section characterizes uncertainty in a more rigorous way by assigning ncrit a continuous probability distribution function (pdf). this pdf represents the probability of ncrit resulting from the chain the aleatory uncertainty (dl) and the epistemic one (ncrit), both quantified through continuous pdfs. as seen above, dl was lognormally distributed whereas the epistemic uncertainty related to ncrit was modeled with a uniform distribution spanning a given range (not shown here) of ncrit values depending on the value of dl. the combination of these two sources of uncertainty resulted in the ncrit's pdf depicted in fig. 2 . various formulations can be adopted to define rbdo problems based on probability theory [cit] . this paper follows a rbdo approach aiming to maximize the mean of efficiency (eq. 1) by enforcing its standard deviation lower or equal to 7 (we enforced this value once we determined the standard deviation of the pseudo-deterministic optimum, which we aimed to reduce). through this approach we aimed at designing an airfoil to have the highest mean efficiency, but at the same time we limited the efficiency sensitivity to ncrit variations (i.e., we ask for a certain performance robustness). this design optimization approach is formulated as follows:"
this paper aims to compare the airfoils designed by means of three different optimization approaches: a pseudo-deterministic optimization approach and two different uncertaintybased design optimization approaches: one based on probability uncertainty quantification and the other based on evidence theory uncertainty quantification. the pseudo-deterministic optimization resembles the common practice approach in which airfoil design is performed considering uncertainty in a rather simplistic way.
"in conclusion, we have found that the particular architecture of the xeon phi, with threads working synchronously in each core, is not particularly suitable for softwarebased speculative execution."
"the atlas framework [cit] enhances openmp with a new clause to allow the speculative use of variables inside a program. the use of this tool is simple: a programmer only needs to include the list of speculative variables in the predefined speculative clause of al parallel for directive. the compilation and runtime system automatically transforms the code to a version capable of running in parallel while preserving sequential semantics. to do so, the system augments all accesses to speculative variables, adapting them to the functions of the atlas runtime library [cit], that ensures sequential consistency. in this work, we have modified the runtime library so as to adapt it to particularities of the intel xeon phi coprocessor. however, our aim was doing as few modifications as possible. a more in-depth adaptation would require a deep modification of the library implementation, that is out of the scope of this paper."
"to test the performance of the atlas tls runtime, we have used three different real-world benchmarks, together with a synthetic one. the real-world applications include the 2-dimensional convex hull problem (2d-hull) [cit], the delaunay triangulation problem [cit], and a c implementation of the tree benchmark [cit] . the synthetic benchmark is the fast [cit] benchmark."
"we have also used a synthetic benchmark called fast [cit], which presents almost no dependences between iterations, and which was designed to test the efficiency of the speculative scheduling mechanism, with few iterations"
"intel xeon phi [3, 11, 20 ] [cit] . it is called coprocessor because, although it can run a linux operating system by itself, it should be placed aside another processor to work properly. although first impressions might suggest a number of similarities, it is not an accelerator such as gpus. whereas the intel xeon phi cores are more similar to classical complete cpus, the gpus thread scheduling hardware is different. as the reader may know, gpus have a hierarchical hardware architecture, so they should be programmed with a hierarchical thread structure in mind [cit], that uses the concept of threads, blocks, and grids. 2 furthermore, intel xeon phi coprocessors do not use the grid, and groups of threads in the same way, and also the memory latency hiding mechanisms are different. this issue hinders easy code migrations to the latter kind of accelerators, and requires and in-depth understanding of special programming models as cuda [cit], or opencl [cit] . on the other hand, the xeon phi coprocessor is able to use all standard parallel programming models such as openmp [cit], posix threads, mpi [cit], or even opencl. thus, using this new coprocessor only requires a minimum learning curve, assuming that the programmer knows at least one of these common parallel programming models."
"currently, physical limitations of single core chips are inducing a quick development of multicore architectures. one of the most recent approaches is the intel xeon phi tm [cit], a coprocessor with more than 60 cores able to execute both offloaded and native codes. nonetheless, due to its own novelty, this coprocessor has not yet been extensively tested with non-regular parallel codes. the dissemination of experimental results under these conditions would be really useful to test the behavior and capabilities of this computing resource."
"the tree problem [cit], unlike the previous two applications, does not suffer from dependence violations, but it is still not parallelizable at compile time because the compiler is not able to ensure that there are no data dependencies. compilers also find hurdles in several sum and maximum reductions contained in the code. we have run this benchmark with a 4096-point input set."
"in this work we have evaluated the behavior of the xeon phi coprocessor in the context of software-only, thread-level speculation (tls), a parallel technique that optimistically executes in parallel sequential codes without a prior dependence analysis. intel xeon phi coprocessors are one of the state-of-the-art architecture that aims to execute parallel codes. our experimental results show that the particular memory architecture of the xeon phi leads to better scalability with regards to speculative execution, with better relative speedups than those obtained using a conventional, shared-memory architecture. however, the relatively low computing power of its computational units when specific vectorization and simd instructions are not exploited, indicates that further development of new specific techniques for this platform is needed to make it competitive for the application of speculative parallelization comparing with high-end processors or conventional shared-memory systems. this situation is likely to change with the arrival of the new generation of xeon-phi platforms, that incorporates more competitive processors. we plan to extend this work to this new generation, with a more detailed profile analysis, as soon as it becomes available."
"the degradation model of biodegradable polymer materials simulates the degradation process of polymer materials by computer and the simulation results should be consistent with the experimental data. therefore, the objective function of the optimization is setting as:"
"all threads had exclusive access to the processors during the execution of the experiments, and we used wall-clock times in our measurements without taking into account times spent in data transfer. we have used icc (icc) 15.0.2 for all applications in both platforms. although we know that icc may be not the most appropriate compiler for an amd platform, xeon phi offloaded codes can only be compiled with icc, and we preferred to use the same compiler in all the experiments. times shown in the following sections represent the time spent in the execution of the parallelized loop for each application. to better assess the scalability offered by the xeon phi, the time required for data offloading has not been taken into account in the measurements."
"the 2d-hull problem solves the computation of the convex hull (smallest enclosing polygon) of a set of points in the plane. we have parallelized [cit] 's implementation. the algorithm starts with the triangle composed by the first three points and adds points in an incremental way. if the point lies inside the current solution, it will be discarded. otherwise, the new convex hull is computed. note that any change to the solution found so far generates a dependence violation, because other successor threads may have used the old enclosing polygon to process the points assigned to them. the probability of a dependence violation in the 2d-hull algorithm depends on the shape of the input set. therefore, we have used three different, ten-million-point input sets to run this benchmark. the kuzmin input set follows a gauss-kuzmin distribution, with a higher density of points around the center of the distribution space, which leads to very few dependence violations, since points far from the center are very scarce. the two other input sets, square and disc, cause more dependence violations than kuzmin, with their points uniformly distributed inside a square and a disc, respectively. the square input set leads to an enclosing polygon with fewer edges than the disc input set, thus generating fewer dependence violations."
"despite the poor performance delivered, we consider that the xeon phi coprocessor may still help in the speculative execution of loops thanks to their comparatively big number of threads. our future work include the combination of software-based tls techniques with other solutions, such as value prediction, or the use of helper threads."
"this paper takes visual c++ 6.0 as a tool to develop the design of optimal program in the windows platform completely. furthermore, the program is used to solve optimization parameters of the degradation equations' optimization model. when using particle swarm optimization algorithm to optimize the model, the initial solution is completely randomly generated. table 1 demonstrates parameters' values and values of objective function (fitness value) solved by using particle swarm optimization algorithm during the 21 times iterative process. it is visible that the optimal parameters are obtained at the 20th iteration. subsequently, the optimal parameters are brought into (1) and (2) to solve the polymer molecular weight * e () ci and then the results are contrasted."
"we will now explore some enhancements which might possibly improve the performance of tls on intel xeon phi coprocessors. we will center our discussion on applying ideas belonging to the classical hardware approaches to manage speculation in multicore processors. therefore, the implementation of these ideas would need changes in the xeon phi architecture."
"in order to handle the speculative parallelization of a loop, all variables have to be classified as private, shared, or \"speculative\". 1 all reads to a speculative variable are replaced at compile time with a function that recovers the most up-to-date value for this variable. in a similar way, all writes to a speculative variable are replaced with a function that not only performs the write operation, but also ensures that no thread executing a subsequent iteration has already consumed an outdated value of this variable. tls is useful when executing codes that present scarce dependence violations at runtime. otherwise, costs associated to check for correctness, stop and retry executions, and commitments, make this technique inefficient."
"the rest of this paper is structured as follows: sect. 2 describes the main characteristics of the xeon phi coprocessor. section 3 describes the software-based, tls framework used to test the coprocessor. section 4 describes both the experimental environment and the benchmarks used. section 5 shows some experimental results in terms of performance measured in a shared-memory system without coprocessor, and in a xeon phi coprocessor. section 6 summarizes some works that helps to put into perspective our contribution. finally, sect. 7 concludes this paper."
"in this paper, we abstract a suitable mathematical optimization model based on an overall consideration of the actual principles in degradation of polymer materials and various factors. the optimization scheme for parameters' determination in degradation equations is provided by particle swarm optimization. it avoids the inadequacy of artificial determining process, improves the safety and reliability of parameters, identifies more reasonable parameter values and makes simulation results of experimental model become better."
"it is interesting to point out that each thread only writes on its local version copy data structure, so no critical sections are needed to protect them. the only critical section used protects the sliding window data structure, because, without it, a thread could overwrite another thread's state."
"the contribution of this paper is to test the performance of a state-of-the-art tls runtime library using an intel xeon phi. this coprocessor has a big number of parallel threads, therefore, it is interesting to measure its behavior with a shared-memory technique such as tls, when data is permanently shared among threads. we believe that tls is also a good problem to test such hardware architecture, because the xeon phi platform allows the different threads to follow different execution paths (contrary to gpus), so the use of a xeon phi platform can be viewed as a natural environment for tls. our experimental results show that the benchmarks considered scale well when running them on a xeon phi coprocessor. however, our results also confirm that, due to the irregular nature of the target applications for tls techniques, and the modest computing capabilities of each individual core when vectorized and simd instructions are not fully exploited, execution times are much higher than those gauged in conventional shared-memory systems. this limit the usability of the current genera-tion of xeon phi platform for irregular, not-easily-vectorized applications, although we expect that this situation will change with the new generation of the xeon phi platform, that will incorporate out-of-order, more powerful processors."
"the goal of this work is to test the xeon phi coprocessor in off-loading mode to speculatively execute in parallel different, well-known benchmarks. in this way, the atlas runtime library was adjusted to offload the execution of the parallel loop to the xeon phi coprocessor, without further optimizations such as vectorization, one of the most important features of the xeon phi. in any case, this feature is not very useful for our benchmarks, mainly composed of irregular code."
"by above knowable, these partial differential equations are non-dimensionalised giving two normalised parameters which control the interplay between the hydrolysis reaction and the monomer diffusion. these formulas demonstrate the weight of biodegradable polymer molecular changes with degradation time. there are three parameters in the model still need to be determined by manual setting, namely 1 k, 0 d and a, which are referred to the degradation rate, the diffusion coefficient and the diffusion factor."
"in order to manage versioning and detect dependence violations on speculative variables, all accesses to speculative variables are replaced at compile time with a function that manages the structures described above. reading a speculative variable implies to obtain the most updated value of this variable, in order to avoid, as much as possible, dependence violations. for the same reason, write operations are also replaced with a function that, in addition to storing the value in an intermediate place, checks if any successor thread (a thread which executes a chunk of subsequent iterations) has used an outdated version of this variable. in this case, the thread should discard the data calculated so far during the execution of the current chunk of iterations and restart it. when doing so, the thread will forward the correct value of the variable. as can be inferred, while a load or store operation of a scalar datum only requires to perform a single memory access, the transformation of this operation in a speculative load or store requires to replace the single memory access to a function call that performs all the required actions. this implies that the time consumed by the speculative load or store operation can be easily two or three orders of magnitude higher than the original one."
"intel xeon phi coprocessors have up to 61 cores at 1090 mhz, interconnected by a high-speed bidirectional ring. each core is enhanced with four hardware threads (up to 244 threads per coprocessor), and with a 512-kb l2 cache. l2 cache levels are shared by all cores. furthermore, in addition to 64-bit x86 instructions, cores offer 512-bit wide simd vectors, intended to speed-up the execution of regular code through vectorization. the coprocessor is generally connected to the host system via the pci express bus, and supports up to 8 gb gddr5 memory. figure 1 briefly describes the architecture of the intel xeon phi."
"particle swarm optimization is used in this paper to establish optimization model of the degradation equations. we optimize the parameters of the polymer materials' degradation optimization model using the particle swarm optimization algorithm and compare the optimized calculation with pan's calculation. experiment results show that absolute errors of predicted values obtained from optimal parameters by using the particle swarm optimization algorithm is 0.229391, which is lower than the model before optimization about 49.93%. it shows that the particle swarm optimization algorithm achieves the optimization purposes, which makes up shortcomings caused by manual setting parameters, improves the reliability of the parameters, makes designed optimization plans more reasonable, reduces the calculation error and advances the accuracy of the experiment."
"in this paper we use a xeon phi coprocessor to run irregular applications that were speculatively parallelized, with the help of a software-only, speculative parallelization library. thread-level speculation (tls) [cit], also called speculative parallelization (sp) [cit] or optimistic parallelism [cit] tries to extract parallelism of loops that can not be considered fully parallel at compile time. tls optimistically assumes that dependence violations will not occur, launching the parallel execution of the loop. a hardware or software monitor ensures the correctness of that assumption. if a dependence violation is detected, offending threads are stopped and re-started in order. after solving the issue, the optimistic, parallel execution is allowed to continue. the target of tls systems are usually for loops. other loops can be considered as well, but as long as their number of iterations can not be so easily predicted, the applicability of tls solutions is limited by scheduling problems."
"another possible improvement might be the addition of a new cache, based on the trace cache [cit] . this proposal stored traces (dynamic sequences of instructions stored in the hardware) at runtime, and instructions were executed in parallel, while dependences were speculated with the use of predictors."
"particle swarm optimization (pso algorithm) [cit] . the algorithm regards each individual of colony as a particle possessing flight direction and speed in the d-dimensional searching space. in the iteration process, each particle continuously makes statistics for optimal values of itself and the colony to adjust its flight direction and speed. in this process, the individual gradually migrates to the better region and eventually makes colony search to the problem's optimal solution. the migration of particles could be performed by the following equations:"
"native execution: the intel xeon phi coprocessor is capable of running a linux operating system. it is possible to log into the xeon phi from the host processor using ssh, through a mic0 network interface, added to the kernel by a module provided by intel, and use it natively. thus, it allows the execution of the typical linux-based commands as well as our own programs. offload extensions from the host: intel defined a set of pragmas and keywords to be used in parallel codes in order to execute them in coprocessors. a programmer only needs to declare the region which should be executed in a coprocessor. inside this region, any kind of function can be used. for example, in the case of openmp, a single pragma defined as #pragma offload target mic should be used, where mic represents the identifier of the target xeon phi coprocessor. in addition, we should point out the variables that will be used in the coprocessor, declaring their use with the clauses in(), out(), or inout(). the use of variables with dynamic size requires to explicitly declare the size, e.g. in(a:length(n)). these variables will be copied from the host to the device, and/or vice versa, depending on their usage."
"a different approach like the used in the i-acoma architecture [cit] may work as well. they used a binary annotator that added some notes into executable files to detect possible dependences, that were managed at runtime with a special module called memory disambiguation table. another source of ideas for improvements is the threaded multi-path execution [cit] approach, that was focused on prediction techniques. this proposal executed all possible branches of a loop, whilst there were enough resources, a situation that is likely to occur in xeon phi coprocessors."
"although the use of a xeon phi coprocessor to execute software-based, tls codes is not competitive, the xeon phi architecture might be useful when combining tls solutions with other existing techniques such as value prediction or helper threads. in this way, some of the available threads could be used to help tls execution, reducing dependence violations and thus improving performance."
"at present the study of the biodegradation process of medical polymer materials is focused on chemical hydrolysis of the polymer backbone. this phase includes four stages: (a) water absorption, (b) fracture of the ester bonds, (c) diffusion of water-soluble oligomer and (d) dissolution of fragments. first is water absorption, small water molecules contacting the surface of polymer and diffusing into the ester bonds or around hydrophilic groups. under the influence of acid or alkali medium, ester bonds in macromolecular main chain generate free hydrolysis and convert the long chains into shorter fragments. then it causes the sample's number average molecular weight decreases slowly and loses its mechanical strength. when molecular weight is small enough to the water-soluble limit value, the overall structure starts to be deformed and loss in weight and the sample dissolves subsequently, generating soluble degradation products."
"we have used two different platforms to compare the scalability of the speculative execution of our benchmarks. the first one is heracles, a 64-processor server, equipped with four 16-core amd opteron 6376 processors at 2.3 ghz and 256 gb of ram, which runs centos 7 linux. the second one is chimera, a server equipped with two intel xeon e5-2620 v2 processors with six cores each, 32 gb of ram, and a xeon phi 3120a coprocessor with 6 gb of ram running at 1.1 ghz. the system also runs centos 7 linux."
"cintra and llanos [cit] developed another scheme mainly based on an aggressive sliding window, with checks for data dependence violations on speculative stores that reduced synchronization constraints, and with fine-tuned data structures."
"biodegradable medical polymer materials, typically poly (lactide) (pla) poly (glycolide) (pga of various form) and their copolymers are currently being used in the human body as fixation devices in the form of screws, pins, meshes, etc., to protect the healing of fractured bones. these polymers have different degradation rates and mechanical properties. but now the determination of parameters in degradation equations is still depended on artificial implementation. it possesses shortcomings such as great difficulty in work, poor rationality, low efficiency and heavy burden. as improving the parameters' reliability of the degradation equations becomes increasingly prominent, people have been trying to search for a reliable and effective method to solve this problem."
"most of the reviewed approaches might be useful to test the performance of the xeon phi. as we saw in previous section, we have used the atlas framework [cit] . our results suggest that the use of other software-only tls solutions may lead to similar conclusions."
"it is very important to understand that there is not a fixed association between threads and slots. whenever a thread is assigned a new chunk of iterations, it is also assigned a slot to work in, that is located at the right of the most-speculative slot. this allows to maintain an order relationship among the chunks being executed. in addition to its state, each slot points to a data structure that holds the version copies of the data being speculatively accessed. figure 2 represents a loop with three speculative variables. at a given moment, the thread executing the non-speculative chunk has speculatively accessed variables a and b. each row of the version copy data structure keeps the information needed to manage the access to a different speculative variable. the first column indicates the address of the original variable, known as the reference copy. the second one indicates the data size. the third one indicates the address of the local copy of this variable associated to this window slot. finally, the fourth column indicates the state associated to this local copy. once accessed by a thread, the version copies of the speculative data can be in three different states: exposed loaded, indicating that the thread has forwarded its value from a predecessor or from the main copy; modified, indicating that the thread has written to that variable without having consumed its original value; and exposed loaded and updated, where a thread has first forwarded the value for a variable and has later modified it. figure 2 represents a situation where the thread working in slot 1 has performed a speculative load from variable a (obtaining its value from the reference copy) and a speculative store to variable b. regarding a, the figure shows that the thread working in slot 3 has forwarded its value. with respect to variable b, the information in the figure shows that b has been overwritten both by threads working in slots 1 and 3 without taking into account its prior value (since both version are in modified state). when the commitment of the data generated by these threads take place, variable b will be first overwritten by the version copy produced by the non-speculative thread. after finishing this commit operation, the non-spec pointer advances one position, and when the thread located in slot 2 finishes, it will overwritten again the variable b with the new value."
"the purpose to design this parameter optimization model is to make the values of * e () ci calculated by the degradation model more close to the experiment result e () c i through optimizing the three parameters. it makes up shortcomings such as great difficulty in work, poor rationality, low efficiency and heavy burden, which are caused by manual setting parameters."
"the partial commit operation is exclusively carried out by the non-speculative thread. every time a thread should check if its data have to be committed or discarded, it first checks if it has not been squashed and if is the non-speculative thread. if the thread is speculative, the slot is left, since it will be committed later by the non-spec thread."
"this experiment considers only the rigid case (scheduling mdp) and the oracle setting. the goal is to show that rl is a good candidate for providing responsiveness and to focus on the fair-share performance. we compare the performance of our method with a baseline one, fifo scheduling. the same input files, created from the parameters described in table 1, have been used for both methods."
"these results are quite good. the rl-method clearly outperforms fifo: the delay is divided by more than 8 when there are 20% of interactive jobs, and by nearly 20 for the 50% case. in fact, this improvement holds, with rather similar values, for both the interactive class and the batch class. one can suspect that favoring interactive jobs results in nearly starving some batch ones. in fact, the standard deviation and the maximum are also reduced by the rl method, which proves that this is not the case."
"the second result is that switching from the unrealistic oracle setting to a very simple estimation method degrades the performance only marginally. the explanation is the bimodality of the distribution of the execution times shown in fig. 2 . these two features, mix of interactive and batch, and bimodality, are not specific to our sample; it can be generalized as shown by the discussion of the one-year egee workload in section 2.1."
"finally, it may be useful to motivate the choice of the data-set from egee. both the grid observatory [cit] and the grid workloads archive (gwa) [cit] provide traces, albeit with different logic and goals. the available traces from the gwa are mostly oriented towards parallel workloads and/or co-allocation, which are not directly covered by our state-action model. conversely, in the egee trace, the dependencies (in any) have been resolved before enqueuing, thus the model does apply. furthermore, the analysis presented on the gwa site for the das-2 trace indicates that the workload characteristics would not offer much optimization opportunities, as system is somehow lightly loaded (average 10%, maximum 39%), and with an overwhelming majority of short or very short jobs (90% percentile below 10 min)."
"we developed a simulation framework to evaluate the performance of rl-based resource allocation and scheduling. this section presents the simulation methodology, the workloads, and the experiments."
"a complete model of the grid would include a detailed description of each queue and of all the resources. this would be both inadequate to the mdp framework and unrealistic: the dimension of the state space would become very large. instead, the state is represented by a the following five real-valued variables:"
"in the first mdp each waiting job is a potential action to be chosen by the scheduler. thus, the action space is the set of waiting jobs. defining the action space as the set of waiting jobs implies that the scheduler will always select a job (if any) when a resource becomes available. thus, the scheduler is work-conserving. on the other hand, if dependencies exist amongst jobs (e.g. if they are part of a workflow, or if co-allocation is required), they should be resolved before the jobs are enqueued, as all queued jobs are assumed to be eligible for running. more precisely, a job is represented by two discrete and one real-valued variables:"
"we analyze here more than one year of egee production under glite [cit], the egee middleware. the data are provided by the real time monitor tool [cit] and include more than 17 million production jobs belonging to 114 virtual organizations (vos). jobs launched by operations management for testing service availability have been removed, thus the results faithfully describe user activity. figure 1(left) shows the distribution of execution times. the striking feature is the importance of short jobs. all requests aggregated, the 70% percentile is approximately 900 sec. these data also support our claim in the introduction that all scientific communities need responsiveness. this is obvious for the biomedical community (biomed vo), with more than 80% of short jobs. however, even the atlas vo (the largest hep community) features more than 50% of short jobs. figure 1 (right) shows the cumulative probability distribution of v, the dimensionless relative overhead; v is the ratio of the time spent in queue to the execution time of a job; the time spent in the middleware stack is not included. the fact that 40% of the short jobs experience a tenfold slowdown due to queuing delays alone indicates clearly the unresponsiveness of the system. thus minimizing the relative overhead will be target of the supervisors described in the paper."
"between exploitation (using the knowledge gained so far) and exploration (looking for potentially better actions), with probability 1  we select an action drawn randomly from among all the available actions. this is the so-called -greedy strategy where the parameter determines the exploration-exploitation trade-off."
"some vos may ask for less than their share. without greedy allocation, the previous rule leads to resource underutilization, a highly undesirable property. this classical problem has been addressed in the framework of network allocation as well as for processor allocation [cit] ), with the objective of fair excess allocation: if excess resources do exist, they should be proportionally allocated to the active requests. these methods could be adapted to our framework, by dynamically adjusting the w k as a function of the actual requests. however, with greedy allocation, there is no risk of resource underutilization (as far as there is enough overall work). on the other hand, the excess resource can be advantageously exploited by favoring the user utility in the short term. thus we keep the fairness utility as defined in eq. 2."
"the quality of the optimization performed by the rl is measured by the distribution of the target indicator which is the responsiveness utility w. even if w can be satisfactorily optimized, it remains to be proved that it correctly captures the users' expectations regarding quality of service. the user experience is dominated by the wall-clock queuing time which is also reported. considering fair-share, we report the difference between the fair-share achieved by the baseline scheduler (fifo for the synthetic workload, and native for the egee workload, which is the state of the art in the domain) and the fair-share of our scheduler computed following eq. 2. utilization, when relevant, is reported directly as computed by eq. 3."
"in our research, we seek to develop resource provisioning models and operational systems that reconcile these two usage scenarios in the context of e-science. motivated by this general goal, this paper focuses on the specific problem of supporting workloads that combine requests for quasiimmediate allocation of computational resources for a limited time period (responsive requests) and requests for computational resources whenever available (best-ef fort requests), on moderately elastic sites."
"-we describe a formalization of the supervision problem as a continuous action-state space, multi-objective reinforcement learning problem, under realistic hypotheses. -we explore implementations of the reinforcement learning framework integrating those high level goals and explain the role of elastic allocation. -we show experimentally that our rl-based supervisor achieves responsiveness without degrading utilization, as measured by several metrics related to user and administrator satisfaction."
"turning to the comparison with the native scheduler, table 3 and fig. 6(left) show that the rl scheduler improves massively the native scheduler for all the jobs (both interactive and batch). for the interactive case, only 53% of the jobs reach a 0.9 w in the native scheduler, versus more than 80% in the rl scheduler. a lesser but still significant improvement is reached for the batch jobs (64% vs. 77%). the batch case exemplifies once again the potential of improvement related to switching from a hard-coded priority system relying on separate queues and manual setting of complex parameters to a model-free framework. the superior performance of interactive jobs proves that the responsiveness utility was indeed a good optimization target."
"in some configurations, the optimization for interactive jobs is defeated, as revealed both by the threshold in the distribution of the responsiveness utility and unacceptable delays suffered by 5% of the jobs. indeed, from table 4, some interactive jobs actually experience very large delays (although in lower number than in the rigid mdp), up to more to 10 4 sec, which explains the relatively large values of the mean. since even the reference setting (ora-1) keeps too many (slightly less than 4%) of the jobs above 2 min, the limits might be in the method itself. more precisely, the alternative is that either the exploration/exploitation parameter is to be increased, or the impact of the hypothesis that the machine pool can evolve only at a relatively low frequency (15 min of simulated time). this parameter may limit the capacity to adapt the resources to the bursts in the service request process (fig. 2) . the over-provisioning mentioned before could be attributed to the same cause. it would this be worth to experiment with a null delay, even if this setting is unrealistic. a complete simulation is not possible, due to the prohibitive cost of solving the continuous approximation problem (training the neural network). however, experiments on limited segments indicate that the limit is actually in the method: the models learnt in the burst phases has sufficiently long-term impact to limit the adaptation before a new burst occurs."
"the exploitation model of e-science infrastructures (high performance computing centers, and grids) is dominated by the best-effort scenario, with high job throughput as the primary performance metric. the sites' batch schedulers are responsible for resource allocation, with a preliminary step of matchmaking in large grids. batch schedulers efficiently serve complex fairshare patterns, at the expense of complex manual configuration. although, in principle, they support advance reservations, this mechanism is rarely enabled, both because of the associated utilization problems [cit] caused by the need to block best effort jobs in order to reserve slots for the future reservation, and also because the advance reservation simply does not match the request for unplanned access. the batch schedulers also provide basic facilities for immediate execution, e.g. in the pbs scheduler. in a previous work [cit], we showed that these capabilities can be combined as an enabling mechanism for virtual reservations (vres), which provides responsiveness without jeopardizing utilization. practical experience in a supportive environment proved that the complexities of the implied configuration severely limits the applicability of this scheme. autonomic computing comes into play here in that (1) high-level goals such as responsiveness and fair-share should be easily tunable by system administrators, and (2) the underlying scheduling mechanism should be able to self-adapt to the uncertainties in the environment and the trends in usage."
"the cumulative distribution function of the waiting time is shown on fig. 3(left) for the interactive class. an important result is that the delay is now acceptable for human interaction: in the worst case (20% of interactive jobs), 90% do not wait more than 2 min."
"we implemented two different markov decision processes. the scheduling mdp solves the problem of job scheduling under the hypothesis of a fixed amount of computing resources with the objective of minimizing the overhead (as defined in section 2.1) and maintaining a predefined fairshare amongst groups of users. in the elastic provisioning mdp we consider that the amount of computing resources may vary within reasonable bounds; we then ask the mdp to also make decisions about resource provisioning. the objective in this second mdp is to minimize overhead and maximize utilization. for simplicity, the fair-share constraint was dropped in this mdp, but we plan to integrate it in further work."
"the decision problem associated to elastic computing extends the scheduling framework to adjusting the number of computing resources available for maximizing the utilization of the resources. thus, there are two actions in this case: a job (to be scheduled) and a request for a specific number of processors (cores in the present setting) to be used for the next period of time. this work sticks to the grid model where jobs are not virtualized. without virtualization, the range of adjustments is constrained: as a running job cannot be suspended (e.g. for going back in a queue), the number of resources must always be larger or equal to the number of running jobs. in order to create a realistic model, we added two constraints. first, the extension/contraction cannot be assumed to be instantaneous. thus, the decisions concerning the size of the resource pool (number of idle machines in the state space) are taken at a fixed frequency. second, the administrative structure responsible for a pool (equivalent of the site administrators in the rigid case) will be required to guarantee some basic level of service in terms of a minimum number of available cores. thus the number of cores will have a nonnull lower bound. both parameters, frequency and lower bounds, are constants of the model."
"the binary attribute (batch/interactive) could be extended to more classes. however, in the rl framework, a meaningful finer class segmentation should define a differentiated set of rewards (utility functions), for which we have no convincing examples."
"in the continuous approximation problem, learning the target q function can be considered as an optimization problem. however an exploration-exploitation tradeoff must be ensured during the learning process to correctly sample the expectation reward function. in the case of continuous representation of the state/action space, several algorithms based on gradient descent and residual minimization have shown good efficiency and robustness in synthetic and real applications [cit] . one of the advantages of the esn is the simplicity of the learning algorithm: a linear regression [cit] over the output weights is used to learn the target function."
"in all simulations, the first and last 500 jobs were dropped from the result, in order to avoid the bias in the results introduced by the ramp-up and draining phases: the small number of jobs in these phases is not representative of the continuous process of arrival and departures."
"our work defines the utility functions ab initio. in the service level agreement (sla) framework, they could be externally imposed. slas including universal terms (variables), and analytical combinations of them [cit] exactly meet the requirement for expressing the high level objectives of users and administrators."
"is this model realistic? the vo associated with the job is a mandatory feature in large scale grids systems, and is available along the whole lifecycle of the job. the interactive/batch attribute describes an input tag requesting higher quality of service. qualitatively, a user should tag a job as interactive when it is urgent, meaning that it should not wait in queue . of course, the counterpart is that such jobs should not last long, and will be killed otherwise. if the choice between batch and interactive quality of service is proposed by the grid environment, the knowledge of this attribute is a realistic assumption: the interactive/batch tag will be known for each job before the execution, and since the user has a strong interest to correctly specify it, we can trust it. the interactive attribute has no immediate relation with the glite interactive job type, but is inspired from to the sdj attribute."
"the statistics of the waiting time are summarized in table 2 . the first column gives the fraction of interactive jobs in the workload, as in table 1 ."
"this section describes the reinforcement learning model of the supervision problem. for the sake of completeness, the first section briefly recalls the basics of markov decision process (mdp) and rl."
"as explained before, a reinforcement learning formalization needs to define states, actions, and rewards for a given problem. we propose a set of variables describing states and actions to allow the formulation of the grid scheduling problem and the resource provisioning problems as continuous action-state space reinforcement learning problems. the set of variables describing the state of the system is the same in the two mdp's but they differ in the set of actions and the definition of the reward."
"this paper proposes an approach that uses reinforcement learning (rl) as a unified resource provisioning and scheduling (resource allocation) mechanism. in the following, this double function (provisioning and scheduling) will be called supervision, and the corresponding software entity the supervisor. the flexibility of an rl-based system allows us to model the state of the resources, the jobs to be scheduled, and the high-level objectives of the various grid actors. rl-based scheduling can seamlessly adapt its decisions to changes in the distributions of inter-arrival time, quality of service requirements, and resource availability. moreover, it requires minimal prior knowledge about the target environment including user requests and infrastructure."
"-the total workload of running jobs; -the time before a resource becomes available; -the backlog, that is, the amount of work corresponding to queued jobs; -the number of idle machines; -the proportion of jobs of each vo in the queues."
"jensen introduced time utilities functions, and models the scheduling objective as the maximization of the aggregated utility (utility accrual, ua) over time, for single processor [cit] and later for multi-unit [cit] real-time scheduling. the ua paradigm is a key concept in that it formalize the goal of optimizing the productivity of the system over time. the advances in rl, and more specifically in temporal difference learning (tdm), have relatively recently allowed harnessing the ua paradigm with implicit prediction of the future."
"in a very complex optimization landscape, running the modified sarsa algorithm with an untrained esn would lead to extremely bad decisions in the beginning. this would adversely impact the performance both because of the actual scheduling of the first jobs and because of a poor initial approximation of the value function. to overcome this initialization issue, the rl system is pre-trained off-line with an early deadline first policy. after a few learning sweeps using the collected rewards, the network can start to take its own decisions and to be optimized using real rewards."
"this paper shows that the combination of rl and esn can address an issue typical of the new challenges in machine learning: devising an efficient policy for a large and noisy problem where no approximate model is available. the problem at hand also exemplifies a real-world situation where traditional, configuration-based solutions reach their limits, and calls for autonomic methods. the scope of the work presented here covers two real grid scheduling situations. in the matchmaking case, the grid workload is first dispatched to distributed sites, where actual scheduling happens; we have shown that rl is a good candidate for this level. the method is directly applicable to overlay or traditional schedulers, which feature a centralized job pool."
"we perform a discrete event simulation of the complete life-cycle of jobs. the simulator is written in matlab, which greatly helped rapid prototyping of the learning components. the events are submission, dispatch, and termination. the submission of a job adds an entry onto a shared queue: although our simulator can manage multiple queues, one of the goals of this work is to show that model-free methods are more effective; the state of the art batch schedulers rely heavily on multiple queues and prioritization amongst queues based on configuration files."
-the type of the job (batch/interactive); -the vo of the user who submitted the job; -the execution time of the job (the time to complete the job without any queuing or management overhead).
"with extensive monitoring facilities already in place, the egee grid offers an unprecedented opportunity to observe and gain understanding of new computing practices of e-science. considering it has tens of thousands of cpu's, petabytes of storage, an extensive coverage of scientific communities, and the perspective of sustainable development, egee provides a good approximation of the current needs of e-science. the following analysis will give empirical evidence of two facts: (1) short jobs are the \"dark matter\" of e-science, and (2) improving responsiveness is required."
"our future work will follow two avenues. the first one will integrate a more refined model of the switching delays, based on realistic hypothesis of future grid-over-clouds deployments. the second one will explore more aggressive methods for favoring interactive jobs when the rl-based supervision appears to be lagging behind."
"the most important event is the termination of a job which causes the manager to select a new job to run in all cases. in realistic schedulers (and in our implementation), the selection process is at worst on the scale of milliseconds, thus each termination is an opportunity to select a job. in the elastic case, a termination event might also lead to extending or contracting the resource pool. the pool size is constrained not to drop below 30 cores, and to be stable for at least 15 min."
"we develop a general reinforcement learning framework with models for classes of jobs (currently two, best-effort availability and responsiveness), for objective functions, and for the infrastructure. furthermore we state that introducing a moderate level of elasticity in the resource provisioning is critical to ensure that both classes can coexist with a high level of user satisfaction. we considerably extend our previous work [cit] in the same area by first getting rid of unrealistic hypotheses about perfect knowledge of the computation characteristics, second by introducing elasticity as a key performance factor, and finally by exploiting recent advances [cit] in approximating the value function through recurrent neural networks. this work has been developed in the framework of the flagship eu grid infrastructure egee (enabling grid for e-science) [cit] both for the grid model, and for the experimental data."
"the extended timescale of the trace offers the opportunity to test the capacity of the rl-based supervisor to adapt to changing conditions. the large value of the standard deviation in fig. 2 is a first indicator of high variability. the graph in fig. 2 shows the process of service requests. the service request is the average of the requests for cpu time over a given time interval (here 10,000 sec). obviously, the service request is a bursty process: for instance, the peak at bin 300 amounts to 12 days of work for the 81 cores. however, the overall utilization remains moderate, at 0.5623."
"according to the sarsa algorithm (algorithm 1), the selected job is, with high probability, the one that maximizes the value function, which is the expected long-term discounted reward (see below for the definition of the rewards and the computation of the expectation), or with low probability, a random one ( -greedy strategy)."
"beyond the vres example, the lack of responsiveness, and more generally the need for flexible prioritization, together with the independent problem of fault management presently lead to an increasing usage of overlay schedulers exploiting placeholder jobs in egee and in teragrid as well. (for an in-depth presentation of this strategy, see [cit] )."
"is this model realistic? any job management system provides the last two descriptors at any time. the first three descriptors are related to the execution time of jobs, thus will be discussed after the definition of the job model."
"in this experiment, the fair share configuration is 4 vos, with respective target weights 0.7, 0.2, 0.05 and 0.05. the schedule is feasible, meaning that the actual proportions of work in the overall synthetic workload are the same as the target ones. besides, inside each class of jobs (interactive and batch), the proportions are also close to the target."
"amongst the vos present in the trace, only six contributed significantly. the target vector is [0.53, 0.02, 0.17, 0.08, 0.01, 0.16, 0.03]; the last share corresponds to the aggregation of the small vos. in the segment considered in the workload, the fairness utility of the native scheduler is nearly constant (after the ramp-up phase) at 0.7."
an interesting byproduct of this development is the consensus of the community on what level of responsiveness could reasonably be expected. some middleware penalty is the unavoidable counterpart of a large scale system; a typical delay of 2 min is thus considered acceptable.
"as a consequence, the grid-cloud convergence is actively experimented, mainly in the iaas (infrastructure as a service) paradigm. quoting the stratuslab initiative [cit] \"using cloud technologies for resource provisioning would enhance failover and redundancy solutions, provide elastic sites able to expand available resources, and permit machine migration for f lexible load balancing.\" discussing the grid-cloud convergence is beyond the scope of this paper; for an in-depth comparison see [cit] . a key point is that the sociology and economics of the e-science make it difficult that user computations could be embarked individually as images to draw from an undifferentiated resource pool. a more likely path is that the middleware stack will be virtualized and deployed onto resources leased by scientific institutions."
"with these definitions, all the rewards are in the [cit] range, thus on the same scale. in both problems, the actual reward is a linear combination of two of the three rewards. in the scheduling problem, the reward is defined as"
"finally, we compare our results with the result of the native pbs scheduler, as recorded in the trace, labelled native in the figures and tables."
"in this work, we compare two sub-models. the oracle model assumes perfect knowledge of the execution time of a job when placed in the queue. the estimated model estimates the execution time by its median, separately for the batch and interactive jobs, along an extended time window in the past. although utterly unrealistic (except in very specific cases), the oracle model provides an upper bound on the quality of the rl-based scheduling, and resource provisioning as well. the estimated model is fully realistic: historical data are readily available online from job management systems. the motivation for choosing a very crude estimator is discussed in section 7. one of the goals of this work is precisely to show that it is efficient."
"this example illustrates how 3dbionotes can be used to analyse external variants. we have collected the variants of the g protein subunit beta (gnb1) [cit] (see supplementary table s3 ). gnb1 protein modulates transmembrane signalling pathways controlled by g protein-coupled receptors. we have requested the ppi network information for the gnb1 protein (uniprot accession p62873) and attached the collected variants to 3dbionotes. when the variants are mapped to the ppi network, many of them appear affecting the binding sites between gnb1 and other g proteins. moreover, the contingency analysis identified that the co-occurrence between many of the gnb1-biding sites and the submitted variants was statistically significant (see supplementary fig. s8 ). consequently, mutations of the gnb1binding sites may affect the interaction with other g proteins and, thus, some of the cell signalling pathways involving g proteins."
"to achieve efficient spectrum access for 5g networks, various influential factors must be considered in the design, such as the number of devices and the bandwidth requirement. therefore, 5g networks have to have a much higher degree of flexibility and scalability than those of former generations. the ufmc, which is an attractive waveform for 5g networks, is vulnerable due to the issues described above. thus, we propose an interference cancelation scheme for ufmc systems to solve this problem and present the scheme in detail. figure 1 shows the ufmc system model and our proposed interference cancelation scheme. compared with standard ofdm systems, the entire band of this model with n subcarriers is divided into m subbands, which correspond to m pieces of equipment. each subband can be allocated to either one piece of equipment or physical resource block (prb) in lte, and each piece of equipment occupies a different amount of consecutive subcarriers determined by its service type [cit] . additionally, the subband sidelobe level can be significantly suppressed by using a bandpass filter (bpf). however, filtering has some negative effects on a certain number of subcarriers, especially on the edges of the subband. thus, the proposed scheme is shown in fig. 1 . the process of modulation-demodulation shown in fig. 1, including the transmitter and receiver, is as follows. at the transmitter, the modulation control unit uses subcarrier modulation strategy to generate interference cancelation and data subcarriers to reduce the interference; then, by means of an n-point inverse discrete fourier transform (idft) converter, the frequency-domain subband signal x i (k) is converted into a time-domain signal x i (n), with output length n. after the idft operation on each subband, the signal passes to the bpf with length l, so the length of a ufmc symbol becomes n + l  1 because of the convolution process. both the doppler effect due to moving equipment and local oscillator misalignment between transceivers have to be considered to model the carrier frequency offset (cfo), and the transmitted signal of the ufmc is generated by summing all filtered subband signals. from the view of the receiver, a 2n-point discrete fourier transform (dft) is performed after appending zeros, and a subband allocation unit is used to estimate the symbols in individual subbands."
"three treatment conditions adopted analytic phonics. letter-sound correspondences were taught using the words in textbooks, and word decoding was taught during oral text reading [cit] study were stories featuring rhyming pairs. reading activities included rhyme detection, word family, and spelling patterns. flash card activities were implemented for students to practice recognizing sight words in all the three conditions."
"now, each data center's replication cost is calculated by considering the location of a replica anywhere in the path from the node itself towards the origin server or root. the replication cost, cost(v, rd), for each data center node v is calculated based on the condition that the replica is located at some distance towards the root. as mentioned below, the optimum position of a replica is also calculated for each case. once the cost vectors for all the children of a data center are calculated, the cost vector for the data center itself is calculated."
"to gradually illustrate the relationship between n-point sequence x i (k) and y i (k) and separate the desired signal part from the interference part in eq. (5), we first deriv"
"whether the instruction is effective on the following outcomes in the ascending order of level of cognitive processing, phonological awareness and letter knowledge, phonological decoding (non-word reading), word reading (lexical access and pronunciation), and reading comprehension?"
"in this example we have analysed the genomic variants associated to kras human protein (uniprot accession p01116). kras protein is a gtpase that acts as a signalling switch in many transduction pathways including cell proliferation. the active state of kras occurs when the protein is bound to gtp. in this state, the protein recruits and activates other growth factors and cell signalling receptors. upon gtp hydrolysis and conversion to gdp, kras is inactivated. kras mutations are known to be involved in different diseases such as multiple cancer types or neurofibromatosis [cit] . we used 3dbionotes to analyse the co-occurrence of kras variants associated to diseases with the different biochemical annotations. the main reason was to check whether those variants occur in particular regions of kras or randomly distributed. supplementary figure s4 and supplementary tables s1 and s2 display the analysis panel of 3dbionotes and clearly show that many of those variants occur in the 'nucleotide binding site' annotated regions. kras acts as on/off switch in many processes and its active or inactive form depends on the interaction with gtp or gdp, respectively. then, mutations affecting kras gtp/gdp-binding sites may affect its activation and therefore, many cell regulatory processes."
"compared to ofdm, ufmc systems have greater robustness against cfo because of the introduced filters. however, our current work shows that the carriers on the two edges of the subband are influenced by the filter, which leads to degradation of system performance. therefore, we need an interference suppression scheme to decrease the sensitivity of internal carriers to the filter."
"equation (27) has the same assumption as that of eq. (26), that is, the desired signal is on subcarrier \"0\". however, to analyze the effect of the filter in the subband, we place the desired signal in the middle subband. then, eq. (27) becomes"
"one of the main prospective scenarios of 5g networks is machine-type communications (mtc) [cit], where the devices are generally one order of magnitude larger than human communication users. these devices and their corresponding traffic will generate pieces of spectrum that will be a primary challenge of 5g networks [cit] . therefore, 5g networks have to support high bit rate traffic with high spectral efficiency. the well-known orthogonal frequency division multiplexing (ofdm) is widely applied in multiuser systems because of its robustness and easy implementation based on fast fourier transform (fft) algorithms. nevertheless, the predicted application scenarios of 5g networks present challenges where the ofdm can be applied in only a limited way, for example, the sporadic communication of mtc devices in the internet of things (iot), which makes it difficult to maintain the orthogonality among subcarriers in the strict synchronization *correspondence: [cit] @126.com 1 school of electronic and engineering, beijing university of posts and telecommunications, 100876 beijing, china full list of author information is available at the end of the article process [cit] . the ofdm symbol with cyclic prefix (cp) presented low spectral efficiency when used to solve the low latency requirements in tactile internet applications [cit] . additionally, the high oobe of ofdm represented a challenge for random and dynamic spectrum access systems [cit] . these problems make ofdm vulnerable when solving frequency misalignments in multiuser scenarios, and the system is affected seriously by intercarrier interference (ici)."
"for the performance evaluation of our proposed replication algorithm we have leveraged the use of a java based simulator program. our hierarchical cloud structure consists of four tiers having each tier with data centers. each data center has five children and thus the total number of data centers becomes 155 including the users in the lowest tier. requests for data come from the users only. uniform distribution is used to model the available link bandwidth with the range [0.622, 2.5] (gbps). according to the same distribution, data center storage capacities are also modeled. the simulation experiments use 2500 data files where the size of each data file is 10 gb. hence, the total data size becomes nearly twenty-five tera byte (tb). to measure the efficacy of our system, we utilized five diverse storage settings of data centers which are created in accordance to the relative storage capacity of data centers. the relative storage capacity (rsc) is determined by a percentage of total storage size of all data centers compared to the overall data size in the system. for our experiments, we use rscs ranging from 13% to 75%. the use of relative storage capacity is justified by the fact that it affects the decision to create replicas on data centers in contrast to their absolute storage capacities. different storage settings as discussed above are shown in fig. 3 . upon requests for data from the users, each data center replica server tries to meet the request. however, the number of access requests served by each replica server is limited by its workload constraint. in our experiments, six different workload configurations according uniform distribution are used as shown in fig. 4 . simulation experiments are done by submitting 50 different jobs each one having a fixed probability of being submitted."
"to date, a number of replication strategies have been adopted in many application areas such as cloud storage, large data storage, data grids, distributed systems and so on. these replication strategies are mainly divided into static and dynamic categories. the number of replicas created is fixed in static categories. as mentioned already, gfs [cit] and hdfs [cit] are adopting this strategy. however, this technique is lacking flexibility even though replica management is straightforward. most of the current research work is focusing on dynamic replica placement strategies where the number and location of replicas created can vary depending on the application requirements."
"ten studies compared phonological-based approach with an alternative approach of explicit reading instruction. comparison conditions in six studies [cit] adopted the whole word approach to teach students to read, which is an approach that a word is taught to students through repetitive encounters without phonological decoding. letter names were taught along with or prior to the whole word recognition in eight conditions. in four conditions, a word was repeatedly shown to students in various demonstrations and forms provided by the teachers, e.g., in different sentences and pictures [cit] . word repetition was conducted via out-of-context drills in the same format in two studies. the students were shown a word in flashcards repeatedly and asked to repeat its pronunciation and meaning in their first language after the teacher [cit] . two studies adopted mnemonics as a strategy to facilitate students' rote memorization of the association among a word's pronunciation, print form, and meaning [cit] . writing and copying exercises were featured in the whole word approach in one study . three studies described the comparison conditions as the whole word approach, but did not give implementation details [cit] ."
"a number of replica placement algorithms are also available in other areas of interest such as distributed and mobile databases and p2p networks. as a whole, these replication strategies are not directly applicable to the target environment in our paper and thus we did not present them in the scope of this paper. in this paper, we present a fully distributed approach to data replication which aims at using a multi-objective model in cloud that seeks near optimal solution by minimizing total replication cost and by balancing the trade-offs among the stated objectives such as qos requirements from applications, workload of replica data center nodes, consistency of created replicas."
"as mentioned above, whenever a datacenter server receives a request for data it serves the request locally if data is present in the server. alternatively, the request is served by any other closest replica server that holds the requested data. in our data cloud architecture, each user carries an aggregated count, which represents the total number of accesses for a particular data item over a time interval. this aggregated count determines the weighted communication cost for a user requesting the targeted data item. it is worthwhile to note that we have excluded the communication cost among the users and their directly associated datacenters as this does not affect the replica placement decision. the overall cost for data access is given as follows:"
"the sample size of the studies ranges from 40 to 1,030. eight studies were conducted with kindergarten or grade 1 students whose experience of learning english language was less than 1 year [cit] . three studies involved grade 3 students [cit], and three studies involved grade 5 students [cit] . one study was conducted with students of mixed grades from grade 1 to 6 [cit] . two studies were conducted with students identified as english struggling readers [cit] . students' experience of learning english out of school was taken into account in four studies. one study recruited participants who had no out-of-school experience [cit], and the rest three studies showed equal distribution across two groups on this variable [cit] . none of the studies reported the proficiency level of oral and written abilities in students' native languages."
"five comparisons were made on letter recognition. two of them showed significant effects [cit] . the effect size ranges from 0.01 to 0.44 with a median of 0.30. the two significant effects are with size of 0.29 and 0.27, respectively. one comparison was made on letter-sound correspondence [cit] . the result showed extremely large effect, because the students in the control group showed knowledge of letter-sound correspondence in neither pre-nor posttests."
"proof. as stated earlier, when the children of a data center node complete the calculation of replication costs, the node itself starts to calculate the cost functions. in lemma 1, we verified that replicas are allocated optimally in a leaf node for all replica distance values. thus, we can infer that the non-leaf data center nodes one hop up from the bottom of t contain children (leaf nodes) whose calculated replication costs are optimal. now, it remains to show that rpcc places replicas optimally in the sub-tree with a root being any internal data center, v, considering each possible value of d. first, when the value of d equals to 0, we observe by considering equations (7) and (8) that the replication cost associated with sub-tree rooted at v is the least of the following two scenarios: 1) v itself contains a replica. the cost becomes the aggregate of the costs of the sub-trees having root as v's children and the value of d equals to 1 and moreover storage and consistency maintenance cost at v."
"phonological-based instruction in efl classes is attracting more and more attention of researchers, school administrators, and teachers. efl students do not have english-language environments to develop literacy skills spontaneously. thus, the foundational skills, such as phonemic awareness and alphabetic principles, need to be explicitly taught so that students are prepared to learn to read in english [cit] . government-endorsed english curriculums in many efl countries include phonics or pa as an instructional component, such as malaysia [cit] and taiwan [cit] . phonologicalbased instruction, phonics instruction in particular, has become a trend in english classes. qualitative studies have found that english teachers in efl countries have the belief that the spelling rules (phonics) are essential in teaching english to young children [cit] . for example, english teachers in hong kong reported that they found phonics instruction effective on their students' spelling and reading performances in early grades [cit] ."
"these two equations show that the received desired signals in these regions are disturbed by the even carriers, and the coefficient of x i (d) becomes an important factor in determining the strength of the interference. thus, the previous interference coefficient in eq. (9) becomes fig. 2 bandwidth allocation of the proposed scheme for ufmc. this figure illustrates the bandwidth allocation of the three parts in one subband for the proposed scheme and the remaining received signal in a i2, which contains unmixed data carriers, is expressed as (14) then, the whole received signal can be written as"
"although at the beginning efl students rely less on phonological decoding to identify words than their english-speaking counterparts, there is evidence suggesting that phonological decoding facilitates vocabulary acquisition of efl students. [cit] found that chinese-speaking efl students better associated a novel word's auditory form with its semantic referent when the word was presented in print form. the print effect was larger for the efl students with better phonological awareness. this suggests that phonological processing enabled phonological decoding to form bonds between pronunciation and print, thus enhancing sight vocabulary learning. therefore, explicit instruction on english alphabetic principles and phonological awareness may be beneficial for efl students; it may boost their development of word decoding skills and further facilitate sight vocabulary acquisition."
"the presentation of the instructional outcomes follows the ascending order of level of cognitive processing of reading. rhyme detection, phonemic awareness, letter naming, letter-sound knowledge, and non-word reading are skills and knowledge that underpin the reading process; real word recognition, including pronunciation and lexical access are real-time activities of word reading; text comprehension requires higher level processing beyond word recognition."
"the number of studies investigating the effectiveness of phonological-based instruction among efl students is drastically small compared to studies with ells in english-dominant societies and those with english native speakers. studies on this topic with rigorous research design are highly needed. quasiexperimental design is found to be the dominant method. studies that employed the design of randomized controlled trial are scarce. in educational research, class and student both can be the unit of randomization. the match between the unit of randomization and the unit of analysis is important, because the mismatch could inflate the effect of the instruction considerably [cit] . future studies on this topic which assign classes of students should account for variance at both class and student level."
"the rest of this paper is organized as follows. first, a summary of related work is given in section 2. then, we present a modified system model for ufmc systems and its corresponding interference cancelation method in section 3. furthermore, section 4 analyzes the simulated ber performance based on the proposed scheme of ufmc systems under the condition of multiuser access. we also compare the performance with those of conventional ufmc, guard band (gb) ofdm, and standard ofdm. finally, the conclusions are presented in section 5."
"phonological-based instruction, namely phonological awareness instruction (pa) and phonics instruction, has shown to be effective on early literacy skills among young children in western countries. children who learn english as a foreign language (efl) learn to read english differently from children in english-dominant societies. effectiveness of the instruction in the efl context is much less investigated. the present study systematically reviewed 15 [cit], on the topic of the effectiveness of phonological-based instruction in the efl context. study characteristics and instructional features were described, and effect sizes were calculated. phonological-based instruction was consistently found to be effective among primary school efl students on reading underlying skills, including phonemic awareness and non-word reading. the median value of the effect size was moderate. in contrast, the effectiveness on word recognition (lexical access and pronunciation) and reading comprehension were inconsistent across studies. the median value of the effect size on word reading was small. this pattern suggests a limitation of the phonological-based instruction, which is the difficulty of transferring the phonological underlying outcomes to real reading. we found that most studies, although meeting the minimum standards of evidence for effectiveness, suffer from methodological flaws; thus, they are potentially biased. therefore, the positive effects reported in this study should be interpreted with caution. the implication for practice of this study is that including phonological-based instruction in the current english curriculum may be beneficial for young efl students, thus they can better learn to phonologically decode english words. but not enough evidence has been found to support the instructional effectiveness on real word recognition and reading comprehension. future research on this topic with rigorous design is needed so that strong causal inference can be made. the findings of this study provide novel insights into foreign language education of english for young learners."
"is the ici coefficient between the kth and dth subcarriers in the ith subband under the assumption that the kth subcarrier is the desired signal and the dth subcarrier is the interference. in other words, eq. (8) shows that the received signal has been distorted by the existence of interference from other subcarriers. we focus on the effects of cfo and the filter using an additive white gaussian noise (awgn) channel so that the sequence s i (k  d) is defined as the interference coefficient to explain the interference degree between the kth and dth subcarriers in the ith subband. its influence on the system is denoted as"
"the two authors of this article independently assessed the quality of each study and selected studies that met the above criteria. then they compared notes and reached agreement. this yielded 15 studies that qualified for the current study. these studies were further coded on study characteristics, instructional context and features in both treatment and comparison conditions, and their reported statistics were extracted to calculate effect sizes."
the application supports the submission of custom data in such a way that users can analyse their own genomic variants or other annotations and compare them with 3dbionotes integrated data. the submitted information is fully integrated and the different visualization and analysis tools can be used to display and process the external data.
"we now devise the replica placement problem in a largescale cloud computing environment. in practice, the replica placement problem can be regarded as an optimization problem:"
"besides the whole word approach, reading instruction in one comparison condition focused on text reading. the teacher explained the meaning of the text sentence by sentence and asked questions to check for comprehension [cit] ."
"phonological-based instruction focuses on aurally analyzing words at the phonemic level and mapping linguistic units to print so that students can eventually learn to read. phonological-based instruction includes two types of instructional programs, phonics instruction and phonemic awareness instruction (pa)."
"the characteristic dimensions include authors and year of publishing, region and native language, study design, study type, sample size, the number of classes involved, the number of schools involved, students' grade and ability level, schools' social economic status (ses), and english-learning context, including language medium of daily instruction and the number of english lessons a week."
"as expected, the efl students in the review studies had limited exposure to english language. the primary input of english efl students received was from english classes at school. the duration of english exposure was less than 5 h a week as indicated in the reviewed studies, which was much more restricted compared to students from english-speaking areas. moreover, english reading materials students were exposed to were very limited. new words were usually learned through explanation from their teachers and drill practices, rather than through authentic reading. in countries where english is one of the official languages, the students in the reviewed studies did not have sufficient exposure to the oral and written language; their situation of learning english was similar to students in a completely foreign context."
"it should be noted that none of the studies took phonics/ phonological awareness training as the only english program. the instruction in the studies reviewed is meant to be supplementary to daily english classes, and it is most effective when delivered regularly and discretely. the primary goal of early foreign language education should be language comprehension and communication [cit] . learning english phonological skills and alphabetic knowledge cannot replace whole language teaching."
"in order to find which protein regions are statistically affected by the genomic variants associated to a particular disease, fisher's exact test between the different annotations and the variants associated to the different diseases is computed. the main objective is to find co-occurrence of protein residues between the different structural or biochemical annotations and the variants associated to diseases. for example, most cancer related variants of the kras human protein map on its nucleotide binding region (see section 3.1 and supplementary section s3)."
"in the future, we envision to implement our proposed data replication technique in a physical cloud infrastructure. besides, we plan to extend our replication technique to deal with the bandwidth constraints imposed on the network links."
"a new type of query to request information for a set of proteins is now available. moreover, a panel to visualize ppi networks using a graph-based representation has been integrated. this panel displays the physical binding between proteins when the information for a multimeric entry is requested or the ppis that have been experimentally observed when a given set of proteins is submitted. in the first case, the contacts are computed using a distance threshold of 6  between heavy atoms. for the second case, ppi data is collected from interactome3d [cit] . moreover, the network panel can display annotations at network level using a similar approach as dsysmap [cit] (see supplementary section s4)."
"despite the importance of the findings in this review, we do think that this study has the following limitations. first, many of the selected studies are not of the best quality. thus causal inference in regards to the instructional effectiveness cannot be drawn. second, the number of studies reviewed here is relatively small. one reason is that we only included studies published in english. more studies on this topic may be published in other languages to make it more accessible to the native language speakers. last, some factors important to instructional effectiveness are not included in this review. for example, the training and proficiency of teachers were not analyzed, which could influence the fidelity of instructions. in addition, the performance of students was assessed only at immediate posttest tests. the instructional effectiveness at delayed posttests was not examined."
"all studies except for two were conducted in classroom setting. the class size in the 12 studies ranges from 12 to 50 with a median of 25. [cit] developed a computer program to deliver the pa instruction at individual level. [cit] conducted a study with struggling readers and the instruction was delivered in groups of eight. the instruction reviewed was delivered regularly. each instructional session lasts from 20 min to 1 h. the instruction was provided for different length of time, from 1 week to one academic year. the accumulation of the instruction ranges from 120 min to 128 h with a median of 560 min."
"since most of the studies reviewed could be potentially biased, the positive effects reported should be interpreted with great caution. thus, the implications provided here are only suggestive. phonologicalbased instruction may be effective in improving phonological decoding abilities among efl learners at primary school level. therefore, allocating resources to this type of instruction may be beneficial."
"given the issues and trends stated above, in this paper, we investigate replica management problem in cloud computing environments to support big data applications from a holistic view. to this end, we provide cost-effective replication of large amount of geographically distributed data into the cloud to meet the quality of service (qos) requirements of dataintensive (big data) applications while ensuring that the workload among the replica data centers is balanced. targeting a typical cloud platform that encompasses disparate data centers, we formulate cost-minimizing data replication problem, and present an effective distributed algorithm that optimizes the choice of energy efficient data centers into the cloud for allocating replicas taking into account the consistency among replicas due to update propagation. thus, we build up a multi-objective optimization approach for replica management in cloud that seeks near optimal solution by balancing the trade-offs among the stated issues. hence, we make the following contributions:"
"an interference cancelation method is proposed for ufmc systems in this paper to further decrease the interference in one subband. this method, which is based on the ici cancelation method used in ofdm systems, can be flexibly configured according to the specific bandwidth requirements of multiusers in 5g networks. the ici cancelation method performs much better than the method used in standard ofdm systems [cit] . however, the bandwidth efficiency is reduced several fold owing to the redundant modulation in the entire band. based on the analysis of filter interference, we find that the subcarriers on the edges of a subband are greatly influenced by the existence of a transition zone, especially in low-cost devices with low-order filters. this phenomenon inspires us to modulate the subcarriers on the edges with the ici cancelation method. then, an interference cancelation method is proposed for ufmc systems to restrain the edge interference and prevent significant reductions in spectral efficiency."
"we also compared the proposed scheme with the standard and gb ofdm systems. the simulation results showed that the standard ofdm system had the worst performance because of the serious isbi, while the proposed ufmc outperformed the gb ofdm under the condition of the same spectral efficiency."
"learning to read in english is challenging for efl students. exposure to oral and written english is limited in most efl contexts [cit] . thus, the development of english oral language and literacy skills of efl students is constrained. english literacy instruction for this group of students is important but far from being evidence-based. this paper presents a systematic review of experimental and quasi-experimental studies on phonological-based instruction in efl context."
"although all the above schemes provide better performance than that of conventional systems for ici, isbi, and isi reduction, researchers have not considered the effect of the filter in the subband, which also results in system performance degradation due to partial loss of information. therefore, we concentrate on this situation and propose an interference cancelation scheme to further improve system performance."
"the storage resources used in the system is vital to grid providers. since storages are relatively cheaper, we can come to a trade-off in case improvements in job execution times and network bandwidth consumption are achieved. fig. 10 shows the storage usage (y-axis) as a function of varying workload (x-axis) for all algorithms with a relative storage capacity of 75% and 17.5%. rpcc shows moderate storage usage compared to greedy add and greedy remove algorithms in all cases. when data centers' capacities in terms of workload decreases, the number of replicas created increases (fig. 6 ) and accordingly storage overhead increases in most cases but by varying amounts."
"it is worth noting that the treatment conditions in four studies [cit] adopted jolly phonics, a commercially available teaching program developed in uk [cit] . it is a synthetic phonics program consisting of 42 units including 26 letter-and-sound corresponding and 16 digraph rules. multiple components were featured in the program including phonemic awareness activities, word decoding, dictation, and decodable text practice. this program used stories, songs, and body gestures to create mnemonics for students to remember letter-sound correspondences and engage students in learning."
"the remainder of the paper is presented as follows. qosaware replica placement techniques are reviewed in section 2. section 3 presents the system design and architecture. the proposed replication algorithm is provided in section 4. sections 5 and 6 present simulation methods and obtained results, respectively. we conclude in section 7 with directions of future work."
"where c i (n) is the time-domain frequency-offset expression of the ith subband with the same length as t i (n), and * denotes the linear convolution operator. in the frequency domain, i (k) is the 2n-point dft of c i (n) and can be presented as"
"first, studies must be focused on evaluating the effects of phonological-based instruction. the instruction can be related to either phonological awareness or phonics which might include the component of phonological awareness. studies that examined instruction of other types such as the international phonetic alphabet were excluded. in addition, instruction in the control condition should not include a phonological component. studies that compared effectiveness of two phonological skillbased programs were discarded. second, studies were included if they were conducted in the context where english is learned as a foreign language. studies with ell students in native english countries were removed. third, the participants of the studies must be at the primary grade level ranging from kindergarten to grade 6. studies with secondary school students and adults were excluded."
"the reviewed studies consistently showed positive instructional effects on reading underlying skills including phonemic awareness and phonological decoding. since some of the studies were methodologically flawed, the findings only weakly suggest the effectiveness of phonological-based instruction with primary school efl students, in comparison to oral language teaching and whole word reading approach."
"the effect of the number of aes on ber performance is analyzed in experiment (c), and the corresponding results are presented in fig. 7 . the parameters are the same as those of experiment (a). the figure shows that the proposed scheme has better performance than that of the gb ofdm for the same  and number of aes, and it also outperforms the conventional ufmc system under conditions of different  and number of aes. these results are demonstrated by the ber value presented in the figure when eb/n0 is 20 db. the proposed scheme improves 7 and 2.5 db compared with conventional ufmc and gb ofdm, respectively, under the condition of four aes. for eight aes, the improvements are 3 and 1.8 db. additionally, the proposed scheme outperforms the others even when the number of aes is increased, although the ber performance degrades under these conditions."
"the words chosen for the word-reading task in five studies were supposed to be familiar to the students [cit] . they were compiled from the english text books and were simple words without complex morphological or orthographical structures. two studies chose words that were novel to the students, and the words were taught in the treatment instruction [cit] . two studies chose standardized measures developed for assessing reading ability of english native speakers [cit] . [cit] study; [cit] study."
"eventually, the demodulation control unit adopts a similar strategy as that of the modulation block to complete the signal estimations for both the interference cancelation and data subcarriers. a mathematical analysis of the above process is presented in the following."
"however, the english learning situation of young learners in the two circles did not differ much in regards to the studies involved in the analysis. [cit], where english was the medium of instruction. the number of english lessons students took each week ranged from two to eight, with a median of three. each lesson lasted from 30 to 40 min. this is very limited exposure compared to their counterparts in englishdominant context."
"and f i (n), respectively, and e(k) is the n-point representation of(k). in eq. (8), the first term represents the desired signal, where c i (0) takes its maximum given no frequency offset. the second term indicates the interference components, where the sequence"
"by filtering through bpf, the output signal t i (n) is the result of discrete linear convolution between the filter impulse response f i (n) and the time-domain signal x i (n). as previously mentioned, f i (n) has length l, and t i (n) has length n + l  1. therefore, the formula of ufmc symbol y(n), in consideration of cfo, is expressed as"
"next-generation sequencing has flooded many databases with biomedical data where single-nucleotide variations are associated with phenotypes or diseases [cit] . this information comprises collections of variant-disease pairs that can be used to infer which genomic variations might be involved in a particular disease. however, changes on the biochemical or structural features of the affected amino acids (if applicable) can be more informative in order to understand the causes of diseases. for that reason, some of the existent resources compiling variant-disease knowledge also annotates protein residues with biochemical features displaying what properties could be affected [cit] ."
"although the quantitative difference of instructional effect was shown on word reading between english native children and efl children, studies with the two populations revealed a similar pattern that the instruction is more effective on phonological decoding than on word recognition [cit],b) . the pattern was repeated in all the reviewed studies which reported results on both outcomes."
"in this work, we present a new version of 3dbionotes [cit] where different analysis tools and viewers have been integrated to find how genomic variants may affect the different protein residues. 3dbionotes is a web framework that integrates biological annotations and structural information of proteins from multiple sources (see supplementary section s1). in this version the application computes fisher's exact test in order to find what biochemical or structural features are statistically affected by the variants associated to a particular disease. moreover, a new panel displays those annotated regions where the co-occurrence between protein features and variants are statistically enriched. in addition, a gene annotation viewer has been fully integrated to display protein features at gene level. also, a protein-protein interaction (ppi) viewer has been included in such a way that the different annotations can be displayed at network level. finally, and as an additional tool, a new type of query, request by set of proteins, has been implemented to explore and analyse ppi networks. moreover, custom annotations, including variants, can be submitted and analysed with the biological features integrated in the application."
"the outcomes of word recognition were measured by the task of pronouncing word items that were of high frequency and simple to decode in 10 studies. seven out of the 10 studies showed significantly positive effects. the insignificant effects found in two of the three studies were attributed to lack of oral support and shortened length. the significant effects were dominantly small. in contrast, studies with english-native-speaking children found that phonological-based instruction had moderate to large effect on word recognition [cit],b) ."
"evaluating instructional effectiveness of phonologicalbased instruction in the efl context is complex. besides factors that are generally considered to have influence on instructional effectiveness regardless of cultural context such as school ses and nature of comparison, factors that specific to the efl context should also be considered and thoroughly reported in future studies. for example, literacy experience and reading abilities in students' native language are important for learning a foreign language, but were reported in none of the reviewed studies. future studies that directly investigate the moderating effects of the above mentioned factors are also needed."
"then, we consider the interference in only one subband because the isbi from other subbands is suppressed sufficiently by filters. to simplify the analytical model, all the signals in odd subcarriers are ignored. according to eqs. (5), (6), and (7), we separate the desired signal from the received symbols and obtain the n-point received signal of the ith subband as"
"there are a number of critical issues that need to be addressed to achieve big data replication in cloud storage: i) determining the degree of data replicas that should be created in the cloud to meet reasonable system and application requirements. this is an important issue for further research. since excessive replication would increase the replica maintenance and storage cost creating too many or fixed replicas are not a good choice. ii) distribution of these replicas to achieve higher efficiency and better system load balancing. iii) maintaining consistency among replicas due to replica update or delete. these three correlated problems are jointly referred to as the replica management problem. in addition to that, some data replication strategies in the cloud consider energy efficiency issue to optimize the energy consumed by the data centers."
"in this paper, we studied the data replication problem in data cloud considering the qos requirements from users to support big data applications. aiming to put forward a multiobjective solution to the replication problem, user qos constraints in terms of distance to replica data center servers and workload constraints of replica servers are considered. first, we formulate the replica placement problem as a dynamic programming problem. second, we propose a novel distributed replica placement algorithm (rpcc) for a multitier cloud platform so as to avoid the limitations usually found www.ijacsa.thesai.org in centralized algorithms such as scalability, reliability, and performance bottlenecks. performance analysis of the proposed algorithm was done in terms of job execution time, mean bandwidth usage, storage resource utilization, total number of replicas that are created during a simulation period, and satisfaction rates of users. the simulation results showed that rpcc can considerably reduce job execution times which include data access time while incurring modest bandwidth and storage costs compared to two other algorithms. these results are obtained by utilizing a variety of storage and workload setting of data center servers and data access patterns with a degree of temporal locality and randomness."
"comparison conditions in two studies focused on oral/aural language development without introducing print [cit] . activities included watching videos, listening to stories, conversing, singing, and chanting."
"in this review, we examined studies investigating effectiveness of phonological-based instruction with efl students at primary school level. after screening literature based on our criteria, 15 studies were included in this review. we described the study characteristics and the instructional features in the treatment and comparison conditions, and calculated effect size on a variety of english literacy skills for each study."
"given the set of replica data center servers r and the set of users requesting the data file (f) q, the overall communication cost ( ) is computed as follows:"
"the treatment instruction was described by the following features, delivering personnel, group size, intensity, instructional content, and adapting strategies. the coding scheme of instructional content was adapted from a summary of instructional components commonly seen in phonological-based programs of different approaches [cit] we also summarized each study on the adapting strategies adopted in the treatment condition to tailor the instruction to young efl students. purposes of the strategies include enabling active learning, creating comprehensive input, facilitating transfer from knowledge of first language, developing oral language, and facilitating memorization. two researchers independently coded the above mentioned aspects for each study. we compared notes and solved all the differences. hundred percent agreement has been reached."
"although the studies reviewed met the minimum standards of evidence, most of them had flawed design and the findings rendered weak evidence for effectiveness [cit] . the flawed design could lead to biased results. first, the positive effects on the outcomes might not be solely attributed to the instruction in 12 studies due to the lack of group randomization. second, the effects might be inflated in 12 studies, because the clustering of participants was not taken into account. third, other factors that affect comparisons identified in this study include variation of control conditions and outcome measures. the comparison in which the control condition contains a reading component may yield a smaller effect on literacy skills than the comparison where the control condition focuses on oral/aural language without exposure to print. instructional outcomes measured by the items which were simple and aligned to the instruction may yield larger instructional effects than the ones measured by the standardized items which were difficult and were not directly addressed in the instruction. last, some other sources of bias cannot be assessed because of the absence of required descriptive information, e.g., students' proficiency level in english or in their native language."
"despite the limitation and variation of study designs, we identified some patterns consistent across the studies, which are discussed below together with suggestions and implications for future research and educational practice."
"on the other hand, some researchers argue that early acquisition of sight vocabulary depends on rudimentary phonological awareness and large exposure to print, rather than refined phonemic awareness and proficient knowledge of letter-sound correspondence [cit] . systematic instruction on alphabetic principles should be based on students' knowledge of sight words and certain level oral language proficiency [cit] ."
"coding scheme for this study is shown in table 1 . the creation of the coding scheme was iterative. during the first reading, each study was described in general dimensions adopted in previous studies [cit] . then, we created categories for the dimensions so that all the studies were covered. the two authors independently coded the studies and compared notes. then, the dimensions and categories that were ambiguously defined and led to different interpretations were removed or corrected. the third round of the coding yielded the following scheme."
"to avoid significant reductions in spectral efficiency, a new interference cancelation scheme is proposed by introducing an ici cancelation scheme into ufmc systems. based on our analysis of carriers in the affected region of the filter, we find that the greater the distance to the subband edge is, the weaker the interference of the filter. therefore, we concentrate on the internal interference of the filter for each subband. here, each subband is regarded as a protected object, and the interference cancelation subcarriers are inserted in pairs on the two edges. a diagram of the process in shown in fig. 2 ."
"we analyze the mse performance by increasing eb/n0 in the final experiment. here, the pilot signal is inserted in the middle subband for both the proposed ufmc and gb ofdm. the corresponding results are shown in fig. 8 . the proposed ufmc outperforms the other systems; however, the result of the conventional ufmc is similar to that of the proposed ufmc when eb/n0 is less than 5 db because of the high noise power. in conclusion, the proposed ufmc provides better performance than those of the three other systems."
"our multi-objective data replication technique is designed based on the hdfs (hadoop distributed file system) architecture and it is assumed that different cloud computing datacenters are placed in different geographical locations (fig. 1 ). there is a three tier topology in hdfs architecture which consists of only one namenode and a number of datanodes arranged within multiple racks. the primary task of namenode is to administer the file system namespace and to maintain a virtual map of how different data blocks are associated with different datanodes. in hadoop, datanodes are meant for the execution of applications."
proof. we provide the proof for our generalized replication problem with the targeted cloud tree topology. the proof is carried out based on induction where lemma 1 is the induction base and lemma 2 is the induction step.
"substantial amount of evidence has suggested that phonemic awareness and knowledge of alphabetic principles are important to learning to read in alphabetical languages at the beginning learning stage [cit] for a review). instruction targeting at phonological skills and letter knowledge is effective in young learners on word level reading, regardless of whether english is their first language (national reading panel (u.s.), & national institute of child health and human development (u.s.), 2000; [cit] . findings from quantitative meta-analysis studies showed that phonological-based instruction has moderate to large effects on english literacy skills [cit],b) ."
"the examination of the characteristics of the studies and selecting less biased studies for further analysis, summarizing features of the treatment and comparison instruction, the calculation of effect size of the instruction in each study on the outcomes."
"which approach is the most effective one? while this issue is debated extensively in english-speaking countries [cit], it is less addressed in the efl context. analytic phonics approach focuses on phonetically analyzing words which are already familiar to students. thus, a basic level of sight vocabulary is required in this method. in contrast, prior knowledge of literacy is not required in synthetic phonics because students are taught to sound out novel words. for this reason, synthetic phonics approach may be more feasible in the efl context, especially for students in lower grades who have little or no prior experience with english. however, results of some studies reviewed here do not support the hypothesis. [cit] analyzed the effect of analytic phonics featuring activities of sight words and analogy phonics, and the results showed large effects in phonemic awareness and sentence comprehension. [cit] conducted a study with grade 3 students in taiwan comparing analytic and synthetic phonics and found that the two approaches were equally effective when systematically delivered. therefore, there is not enough evidence to draw any conclusion at this point."
"in this figure, we divide each subband into three carrier blocks. the middle position is allocated to the data carriers, and the interference cancelation carriers are placed on the two edges. each block occupies variable bandwidth to meet the flexible requirements for 5g networks because of the diversity of the access equipment (ae) and filter type."
"however, transferring the instructional outcomes to improvement of word recognition is a challenge. theories suggest that instruction on semantic and grammatical information and large language/print input is essential for improving the ability of word recognition. synthetic and analytic phonics both provide methods that incorporate meaning in the instruction of decoding skills, such as using decodable text and word family analysis."
"one important distinction between efl and english-nativespeaking students in terms of word reading is that the strength of association between the formal (spelling and pronunciation specification) and semantic information of words [cit] . according to the stage theory of lexical acquisition [cit], formal information is weakly linked to semantic information at the initial stage of lexical acquisition due to the constrained input students receive in efl context. when sounding out a high-frequency word, access to meaning is usually assumed for an english-native-speaking student. however, the lexical access is less likely guaranteed for an efl student. for efl students, learning to crack the code of print-and-pronunciation correspondence does not guarantee the access to lexical-semantic information. the words students know the meaning of are likely to be less than what they can pronounce. therefore, decoding (pronunciation-print association) and lexical access (print-meaning association) are treated as separate outcomes of word reading in the present study."
"students were taught to recognize a word as a whole without breaking it into smaller parts of letter groups. a word is acquired through repeated encounters. specifically, the whole word approach was coded in the following aspects: (a) whether explicit instruction on letter knowledge was included, (b) whether writing exercise was involved, (c) how the repetition was delivered, through authentic reading, multiple demonstrations and examples provided by the teachers, or through out-of-context drills."
"the two studies that measured lexical access in word recognition both showed insignificant results. the words chosen for assessment were directly taught in the instruction. this suggests that the instruction focused on skills of phonological decoding was not effective on lexical retrieval of words via the print form. this contradicts the findings with native english-speaking children, which found that phonological decoding enhanced sight word vocabulary and students with good phonological skills learned sight words faster and more accurately [cit] ."
"to build sight vocabulary, orthographic mapping of large quantity of words needs to be attained [cit] . orthographic mapping is letter-sound formation that bonds the pronunciation and spelling of a word. orthographic mapping is acquired by phonological decoding, which refers to in-depth analysis of the relation between the pronunciation and spelling of the word. the unit of analysis could be syllables, phonemes, rimes, or morphemes [cit] for a review)."
"lately, cloud computing has become an attractive and mainstream solution for data storage, processing, and distribution [cit] . it provides on-demand and elastic computing and data storage resources without the large initial investments usually required for the deployment of traditional data centers. cloud computing facilitates the delivery of storage and computing resources as services without any restriction on the location [cit] . it offers three types of architecture for service delivery such as saas (software as a service), paas (platforms as a service) and iaas (infrastructure as a service) [cit] . end users can use the software applications hosted by the cloud providers in a saas architecture. in case of iaas architecture, virtualized storage and computing resources are provided by the cloud providers. customers of iaas can then access these resources and other services to complete the application stack such as to create virtual machines and to deploy operating systems and middleware. as for paas architecture, the cloud providers allow users to develop, run and customize applications while the providers host them on their own hardware infrastructures."
"the three-tier cloud computing architecture as shown in fig. 1 maintain a number of databases among which the central database (central db) is stationed in the highest layer. this database stores entire datasets that are accessed by the applications residing in the cloud. the other type of databases hosted by data centers is primarily used to improve data access efficiency and they are called datacenter database (datacenter db). these databases are intended for copying the most often accessed data items from central db. furthermore, each rack is equipped with a rack-level database (rack db) which replicates data from datacenter databases."
"a few schemes have been proposed to mitigate the interference caused by time/frequency synchronization error in ufmc systems. one study [cit] presented a novel filter optimization technique with both low complexity and high throughput to reduce inter-subband interference (isbi). two methods were applied: spectrum shaping with low complexity and carrier insertion between two filters. the filter optimization method provided a better signal-to-interference ratio (sir) than that of the original method, improving the robustness against isbi."
"we also define a cost vector to be used in calculating replication cost with respect to a particular data center node in the cloud hierarchy. let v be a data center node and there is a sub-tree rooted at v. now, let be the cost for replication contributed by the above sub-tree where rd denotes the replica distance from v towards the root data center. so, if this distance is zero (i.e., the replica is in v itself) the replication cost will include the communication cost for all descendants of v, the consistency maintenance cost and the storage cost at v. moreover, if this distance is greater than zero (i.e., the replica is located at any data center sitting on the path from v towards the root), the replication cost will include the communication cost for all the descendants of v only."
"phonological-based instruction was compared with the status quo of english education in three studies [cit] . the students received regular english curriculum, which included various instructional components of both reading and oral language development. all the curricula were without systematic and explicit teaching of phonemic awareness and phonics. incidental teaching of english alphabetic knowledge and phonological awareness was mentioned in two control conditions [cit] ."
"twelve comparisons were conducted on english real word reading. word decoding, the association between print and pronunciation, was measured in 10 comparisons by the same task which was to read aloud a list of words untimed. lexical access, the association between print and meaning, was measured in two studies. one measure was word-picture matching [cit] and the other was native-language translation [cit] ."
"six out of the eight phonics programs focused on synthetic phonics which is typically organized by grapheme-phonemecorrespondence (gpc) rules in the sequence of english alphabet followed by the units of diagraphs and consonant blends. students practiced unitizing specific set of gpc rules to read and spell words. [cit] adopted simplified version of synthetic phonics which only included letter-sound knowledge and phonemic awareness without extending to complicated spelling patterns. synthetic phonics was taught in one study to enhance vocabulary acquisition [cit] . specifically, letter-sound correspondences related to the target words were taught to the students. thus, they utilized the knowledge to sound out the novel words."
"additionally, [cit] established a multiservice framework based on a subband-filtered multicarrier system to analyze the desired signal, intersymbol interference (isi), ici, isbi, and noise. inter-serviceband interference cancelation algorithms were also proposed by precoding the information symbols at the transmitter. in this process, a certain gb was inserted between different types of services to mitigate the interference."
"a replica manager module in the namenode performs analysis of data accesses periodically and determines which data items should be replicated and in which data nodes. the goal is to improve data access efficiency and balance workload of the data centers by spreading data replicas from central database to appropriate data centers down the hierarchy. replica updates are only transferred from the central database to the databases in the data centers in the lower tiers. in addition to replica managers (as illustrated in fig. 2 ), a scheduling broker and data centers constitute the system of could data service. the system is managed centrally by the scheduling broker whereas the locations of the replicas in different data centers are stored in replica managers."
"the phonological reading skills tap the process of aurally breaking a word into smaller units and applying the sound-printconversion rules to sound it out, which was directly addressed in the phonological-based instruction as shown by the analysis of the treatment instructions. the positive instructional effects imply that explicitly teaching of decoding skills in english may be independent from english oral language experience and proficiency. a certain level of oral language proficiency and sight vocabulary may not be the prerequisite for learning english alphabetic principles and phonological awareness. further studies are needed to test this hypothesis."
"meanwhile, the average power of the interference signal is calculated under the assumption that the transmitted data x i (k) have a mean of zero and are statistically independent. the average power can be represented as"
"this pattern suggests the limitation of the instruction regardless of context, which is the difficulty of transferring the phonological skills and letter knowledge to real word reading. word recognition requires more than phonological processing. explicit instruction on phonemic awareness and letter-sound conversion rules is not enough. explicit instruction on applying the skills and knowledge to decode a novel word is also needed [cit] ."
"the treatment instructions reviewed were mostly in the realm of synthetic phonics, which focuses on the explicit instruction of alphabetic principles and applying the knowledge to sounding out novel words. phonemic awareness was also commonly included as a component in synthetic phonics programs, or as an intact program which could serve as the precursor to synthetic phonics. activities such as sight word recognition and word family analysis, which were often featured in analytic phonics, were rarely implemented in the studies reviewed. therefore, the effectiveness of phonological-based instruction demonstrated in this review is more reflective of the effectiveness of synthetic phonics instruction than that of other approaches."
"for most of the d values and (b) the total number of interference signals is reduced to half because we include only even terms in the summation in eqs. (11) and (12) . consequently, the interference signals in eq. (15) an interference cancelation demodulation scheme, corresponding with the modulation strategy, is used to further reduce the interference. in the modulation process, each signal on the k + 1th subcarrier (k denotes an even number) is multiplied by 1 and summed with that on the kth subcarrier. thus, in the demodulation, the desired signal in a i1 or a i3 is determined by the difference between y i (k) and y i (k + 1), and it can be derived as"
"moreover, the assessment of english word reading of efl students should be multi-facet rather than relying solely on word pronunciation. not only can print-pronunciation association not guarantee lexical access but the task also produces unreliable results because scoring pronunciation is influenced by many factors such as scorers' background [cit] . in the future, both lexical access and pronunciation of words should be measured, so that the findings can be more reliable and valid."
"where the signals of bothx i (k) andf i (k) with period 2n are 2n-point dfts of x i (n) and f i (n), respectively, and e (k) is an additive noise sample of subcarrier k."
"moreover, the effectiveness of phonological-based instruction may be constrained by the limited exposure to oral and written english in the efl context. semantic and syntactic information of words are not the focus of phonological-based instruction and are often gained from large exposure to print and oral language. semantic and syntactic information of a word is also essential for word recognition. according to the lexical quality hypothesis, successful retrieval of words during reading and spelling depends on high-quality representation of the word [cit] . a high-quality word is represented with specified spelling (orthographic information), meaning (semantic information), as well as pronunciation at phonemic level. lack of information in any of the three aspects will result in low-quality representation, thus leading to unsuccessful retrieval. students are more proficient in reading words familiar to them despite the complicated orthographic and phonological structure [cit] ."
"the request for data block (read and update) goes from an hdfs application to the namenode which examines the prestored mapping for data blocks to datanodes to find an appropriate datanode to process the request. each rack is equipped with an ethernet switch for facilitating communication between the data nodes within the rack. for inter-rack communication, aggregated ethernet switches are used. thus, a logical network topology based on tree structure is built with different switches which use the prevalent communication protocol called spanning tree protocol (stp). to this end, as shown in fig. 1, core network, aggregation network and access network layers constitute the interconnect network in this scenario."
"introduction english has an alphabetic writing system, which means the print represents speech largely at phonemic level. therefore, phonological decoding is greatly involved in learning to read in english. phonological-based instruction, which focuses on explicit teaching of phonological analysis of words and lettersound correspondences, is shown to be effective in improving literacy outcomes at early stage [cit],b) . whether this approach is effective with children who learn english as a foreign language (efl) has not been substantially investigated yet."
"phonics instruction focuses on explicit and direct teaching of alphabetic principles and grapheme-phoneme corresponding rules, and of applying the knowledge to word-and text-level reading. pa focuses on teaching phonological skills, such as rhyming, identifying, segmenting, and blending phoneme sounds. there is overlap between phonemic awareness instruction and phonics instruction [cit] b) . both of them may include the component of grapheme-phoneme correspondence of 26 english letters. phonics instruction goes beyond teaching letter-sound knowledge to more complex spelling rules such as digraphs and diphthongs. phoneme awareness instruction focuses on training students to manipulate speech sounds without the presence of written letters, and word level reading and spelling are important outcomes of phonics instruction. phoneme awareness instruction can serve as a precursor to systematic phonics instruction [cit] ) ."
"in this paper, we proposed an interference cancelation scheme to mitigate the effects of both the filter and cfo by introducing an ici self-cancelation scheme into the ufmc system to flexibly allocate the bandwidth in terms of the different requirements for 5g networks. to reduce the interference, our main focus is on the internal interference of the filter. each subband was regarded as a protected object, and the interference cancelation subcarriers were inserted in pairs on the two edges. this proposed method avoids the significant reduction in spectral efficiency in the current system. in addition, the filter interference was reduced to further improve the system performance. the corresponding simulation results showed that the proposed scheme had better performance than that of the conventional ufmc because the filter interference on the edges was effectively suppressed."
"where, sc u,f denotes the cost of storage of a file with type f in the data center u and the total cost for storage for different types (f) of files will be:"
"the findings discussed above cannot be directly generalized to the efl context, because efl students are in a completely different situation compared to ell learners in english-speaking countries. efl students are from neither language minority groups nor struggling readers who fell behind english-native-speaking counterparts. efl students learn english as a school subject in their native countries. they have extensive print exposure of their first language and many of them have started to receive formal literacy instruction in their first language before they learn to read in english."
"meanwhile, some concerns have been raised about adopting phonological-based instruction in efl classrooms in early grades. the primary goal of english education at early stage is oral language development. some people are concerned that phonics instruction focuses too much on identifying letters and words and introducing it too early could cause negligence of conversational skills. for example, some english teachers took phonics instruction as an easy way out, because they were not confident in conversing in english themselves [cit] ). research reviews with english-language learners (ell) in english-dominant societies several literature reviews have been conducted on the topic of effectiveness of phonological-based instruction with the ell, who learn english as their second language in english-dominant societies [cit] . national reading panel reported that ell students generally respond to phonological-based instruction as well as english-native-speaking students [cit] . a synthetic review was conducted on studies with ell students who were struggling readers [cit] . the findings showed that the interventions, which included phonologicalbased instruction as one of the instructional components, had moderate to large effects on word reading. [cit] reviewed intervention studies with spanish-speaking children, who were struggling with english reading, and found that comprehensive programs with phonics and phonemic awareness instruction included had large effects on reading comprehension. however, the sole effect of the phonological components was not investigated in these studies. [cit] synthesized studies with ell students in the response to intervention setting and found that most of the studies were conducted at tier 2 level, where the instruction is targeted at struggling readers and is delivered relatively intensively and in small group. in contrast, studies investigating the instructional effects in general educational setting are scarce."
"due to the benefits offered by cloud computing, organizations are now moving to the cloud. some of the top cloud providers such as amazon s3 [cit], google cloud platfom [cit], app icloud 1, ibm cloud 2, microsoft azure 3, and dropbox 4 provide cloud services to thousands of millions of users by means of geographically dispersed data centers across the globe. as such, end users are relieved of purchasing traditional expensive hardware (data centers) to process huge amount of data. consequently, growing interests are shown in an effort to develop data-intensive applications that access massive amount data sets. to name a few, smart city applications and healthcare information systems (hiss) are struggling with data bombardments that need immediate solutions to process these data for knowledge acquisition. these data-intensive applications demand for large-scale computing and storage resources and recent cloud computing advancement suits to meet these challenges. lately, popular big data applications such as facebook, twitter, and human genome project 5, are making most use of computing framework such as mapreduce and hadoop [cit] for petabytescale data processing to extract insight."
"whenever a user needs to access data, it sends the request to the closest data center server along with the qos constraints. a user (v) specifies his/her qos requirement, q(v), by means of an upper bound on some criteria. for example, a user may request for a data item within a specific time or from a specific distance in terms of network hops. the replication strategy then confirms that there is a data center server within this q(v) to satisfy the request. if the request is not satisfied due to the absence of a data center server or any other reason the requirement is said to be violated."
"reading is to make meaning out of print. according the simple view of reading, two cognitive components are involved in the reading processes, comprehension and word decoding [cit] . comprehension in reading is underpinned by listening comprehension and develops with oral language proficiency. decoding is to access a word's meaning from its print form. to become a successful reader, one has to decode effortlessly so that most of the cognitive resources can be dedicated to comprehension. the set of words one can recognize effortlessly from memory without further breaking it down to smaller unit is called sight vocabulary [cit] ."
"we coded skills and knowledge measured in the assessment. skills and knowledge that underlie word reading included rhyme awareness, phonemic awareness, letter knowledge, and phonological decoding (non-word reading); tasks of word level reading included measures of print-pronunciation association and print-meaning association. reading comprehension, which requires higher level of processing, was also included as an outcome."
"the idea that phonological decoding is necessary for sight reading is also advocated in lexical quality hypothesis [cit] . according to the hypothesis, a word retrieved reliably and efficiently by sight is the one represented with good quality, meaning that the word is represented with redundancy and specificity in terms of semantics, phonology, and orthography. phoneme-grapheme mapping of a word is redundant if the pronunciation and print form of the word are separately specified in the representation. the redundant cues of phoneme-grapheme correspondence can confirm the connection among a word's spelling, pronunciation, and meaning by avoiding the confusion with words similarly spelled or pronounced. for example, one who memorizes the word \"president\" by rote and does not decode it phonologically may have difficulties in distinguishing it from words that are visually similar such as \"present, \" \"precedent, \" and \"precious. \""
"while data replication in distributed cloud storage is addressed in the literature, majority of the current techniques do not consider different costs and benefits of replication from a comprehensive perspective. most of them provide emphasis on high availability, fast response and high efficiency. even though critical, these average performance measures do not tackle the quality requirements demanded by various dataintensive applications. the performance of these algorithms gets better as the number of replicas increases. however, the increased number of replicas incurs higher degree of replication cost associated with storage and energy. hence, the goal should be to minimize the required number of replicas to avoid high replica creation and maintenance cost. accordingly, a good replica management strategy should be designed to balance a variety of tradeoffs. moreover, as in the case of existing replication strategies, the increased number of replicas is cost prohibitive due to consistency management."
"where, p(v) is the parent of node v and t v is the sub-tree rooted at node v in the update distribution tree. the communication link (v, p(v) ) participates in the update multicast process if . hence, the cost related to consistency maintenance is determined by aggregating the data transfer costs over the communication links (v, p(v)), when"
"in addition, the signal in a i2, which does not include the interference cancelation carriers, is the same as in eq. (14), that is,"
"the job execution times using relative storage capacity of 17.5% are shown in fig. 7 for the same data access patterns as before. generally, rpcc exhibits shorter job times compared to greedy add and greedy remove. however, the use of more constrained storage size results in only a meager benefit for rpcc in terms of job times in most cases. furthermore, job times for all three algorithms in this case got an increase by varying amount compared to the case when relative storage capacity of 75% is used. fig. 8 shows satisfaction rates of users for all methods using relative storage capacity of both 17.5% and 75%. mostly, rpcc outperforms the other two algorithms. particularly, the performance gain of rpcc over greedy add and greedy remove is more when the storage capacity of replica data centers (17.5% relative capacity) is restricted. nevertheless, satisfaction rates of users decrease in case of constrained storage space of replica data centers regardless of quality requirements from users and the patterns used for data access as shown in fig. 8 ."
"the range and the median of effect sizes on each outcome were reported. meta-analysis synthesizing multiple studies for a presentative mean effect size was not conducted, because the studies reviewed varied in both the features of the instructions and the characteristics of the studies. moreover, the analysis of the study design showed that most of the studies were potentially biased, thus the synthesized effect size could be misleading."
"the performance of these big data applications on cloud depends largely on the reliability, availability and efficient access to the data centers [cit] . to address these issues data can be replicated in various locations in the system where applications are executed [cit] . this means that data partitions can be replicated across different data centers in the cloud. replication not only improves data availability and access latency but also improves system load balancing. many existing cloud systems adopt static replication strategies by replicating data in some physical locations without considering the issue of replication cost and terrestrial diversity. for instance, gfs (google file system) [cit], amazon s3 [cit], and hdfs (hadoop distributed file system) [cit] implement a replication strategy where three copies of data www.ijacsa.thesai.org are created at one time to increase reliability of target data. this replication strategy would result in increased (twice) replication cost which will adversely affect the effectiveness of the cloud system."
"equation (6) indicates that the odd subcarriers contain part of the signal energy and the interference, which comes from other subcarriers because of the 2n-point dft. additionally, the 2n-point received sequence has the same conditions. by combining 2n-point signal (k) with the relationship between n-point dft and 2n-point dft, we obtain the expression for n-point received signal"
"our replication strategy was evaluated using the performance metrics which include job execution time, mean bandwidth use, storage utilization, number of replicas created, and rate of satisfaction for users. job execution time refers to the overall time needed to execute the whole set of jobs and also takes into account the data access time. replica maintenance algorithm. the bandwidth usage for a data transfer is the data size times the aggregate costs of the data transfer route. the average cost for bandwidth usage is calculated by dividing the overall bandwidth usage by total data access counts. storage consumption is the proportion of the data center storage occupied by the replicas in the system. finally, user satisfaction rate represents number of users whose qos constraints are met compared to the total number of users who requested for data access with some qos constraints. the target is to reduce total job execution time and minimize average bandwidth and storage consumption while maximizing the user satisfaction rate."
"adapting strategies were adopted in the majority of the reviewed studies. some strategies were aimed at engaging students and promoting active learning, such as playing games, while others were used to facilitate memorization like mnemonics and telling stories. all these strategies are helpful for young children, regardless if their first language is english or not. meanwhile, introducing skills and knowledge of students' native language was not seen as an adapting strategy in the reviewed studies. when starting to learn english, efl students are usually well developed in speaking their native language and have been receiving the native literacy instruction formally and intensively. studies found that knowledge in their first language can be transferred to english learning; the instruction facilitating the transfer is effective on english literacy outcomes [cit] tested the instructional method of comparing the writing system of kanada (students' first language) with english alphabetic in synthetic phonics instruction and found large effects favoring this method."
"four studies investigated the instructional effect on reading comprehension, two studies on passage comprehension [cit] and two studies on sentence comprehension [cit] . three studies found significant effects favoring phonics instruction with effect size of 2.35, 1.05, and 0.50, respectively [cit] ."
the storage cost ( ) is determined as follows. let be the cost of storage associated with a file stored at replica server (data center) . suppose is replicated over sites. the total cost for storage for f is 
"this section first presents our base distributed heuristic technique of multi-objective replica placement for cloud computing environment (rpcc). the proposed qos-aware strategy will then be devised by extending the base algorithm we demonstrate that our multi-objective rpcc can be designed as a dynamic programming problem and a distributed technique can be adopted to find its solution in a hierarchical cloud topology. in essence, the proposed technique breaks the overall reapplication problem into various sub-problems and advocates solving these subproblems separately and combining the solutions to achieve the overall solution. using the data access cost function, each data center node in the hierarchical cloud topology can determine the cost for creating a replica locally and transferring a replica from another data center up in the hierarchy as well. in rpcc, each data center needs to decide (based on the cost) whether it should create a replica locally or fetch the data from any replica server up in the hierarchy. a parent data center node accomplishes this by accumulating the results provided by its children. in reality, this is a bottom-up approach which begins from the lowest tier users and stops at tier-0. on the other hand, replica placement is done starting from the tier-0 and stopping at the lowest tier users (based on the previously calculated results). the details of the technique is discussed below."
"another factor that might contribute to the difficulty of transferring the phonological skills to word recognition is that phonological awareness and letter-sound knowledge might not be the most dominant skill underlying english word recognition for efl students. vocabulary and oral language proficiency are also significant predicators of word level reading for young efl students, as important as phonological awareness . in addition, the dominant method of literacy instruction students receive also influences the underpinning skills and knowledge of word recognition. phonological awareness and letter-sound knowledge are more important to word reading in the context where synthetic phonics is the major literacy teaching approach [cit] . if the whole word approach is the dominant teaching method of reading in english and in students' native language, visual memory capacity might be more important than phonological decoding for word recognition [cit] ."
"the ability of phonological decoding has massive power in kicking start the self-teaching mechanism for learning new vocabulary through independent reading [cit] . phonological decoding is enabled by two underlying abilities. one is phonological awareness, which refers to the ability of detecting and manipulating linguistic sounds in speech such as segmenting, deleting, and blending [cit] . the other is the knowledge of letter-sound correspondence. the two skills are the most robust predictors of subsequent reading performance after oral proficiency has been controlled [cit] ."
"the comprehensive literature search yielded 17 comparisons out of the 15 studies. two studies included two comparisons. only the ones with larger effect size were included. five studies are unpublished dissertations and the other 10 studies were published in peer-reviewed journals. we described characteristics of each study and the treatment instruction, followed by the report of the effectiveness on each outcome (see details related to each study in the supplementary material)."
"in any case, we suspect that these standard error formulas, for all their flaws, should give a better sense of uncertainty than what is obtained using the current standard approach for comparing differences of deviances to a  2 distribution, a practice that is derived for gaussian linear models or asymptotically and, in any case, only applies to nested models."
"as noted above, the distribution of the importance weights used in loo may have a long right tail. [cit] to fit a generalized pareto distribution to the tail (20% largest importance ratios). by examining the shape parameter k of the fitted pareto distribution, we are able to obtain sample based estimates of the existence of the moments [cit] ). [cit] to be used routinely with is-loo for any model with a factorizing likelihood. [cit] show that when estimating the leave-one-out predictive density, the central limit theorem holds if the distribution of the weights has finite variance. these results can be extended via the generalized central limit theorem for stable distributions. thus, even if the variance of the importance weight distribution is infinite, if the mean exists then the accuracy of the estimate improves as additional posterior draws are obtained."
"we exploit ontology and the principle of divide and conquer to construct the navigation situation understanding model of a mass, and divide the situation of mass navigation into scenes based on the international regulations for preventing collisions at sea (colregs)."
linear regression imputation is a very general technique for dealing with missing values in time series analysis. linear regression imputation uses the available data (observed data) to estimate the missing values by using a linear model:
"and similarly for waic and k-fold cross-validation. the effective numbers of parameters, p loo and p waic, are also sums of independent terms so we can compute their standard errors in the same way. these standard errors come from considering the n data points as a sample from a larger population or, equivalently, as independent realizations of an error model. [cit] which uses both between and within-chain information and is implemented in stan. in practice we expect monte carlo standard errors to not be so interesting because we would hope to have enough simulations that the computations are stable, but it could make sense to look at them just to check that they are low enough to be negligible compared to sampling error (which scales like 1/n rather than 1/s)."
"ombination of features since each distance between features was calculated using a different standard, prior to combining these features, we e distances using a gaussian [cit] . this method ensures that the distance is normalized between 0 and 1. after the normalized distances, we used a linear combination of"
"that said, quick estimates of out-of-sample prediction error can be valuable for summarizing and comparing models, as can be seen from the popularity of aic and dic. for bayesian models, we prefer psis-loo and k-fold cross-validation to those approximations which are based on point estimation."
"although drl-based mass navigation behavioral decision-making and path planning in an uncertain environment were realized, the algorithm iteration speed was too slow in the whole experiment, the total iteration time was up to 14 min, and it was trapped in local iterations many times. affects the applicability and credibility of behavioral decisions. therefore, there was a need to improve the drl-based behavioral decision-making algorithm. therefore, this section describes the addition of an artificial potential field (apf) to improve the drl and establish an autonomous navigation decision-making-based apf-drl. to this end, increasing the gravitational potential field was the initial q value of drl, avoiding the huge number of calculations in the complex environment, and effectively preventing the mass from falling into the concave trap in the environment and speeding up the iteration speed of the algorithm."
"according to the designed algorithm, first, the mass is combined with colregs to divide the navigation situation into the individual sub-scenarios. second, the system takes the perceived environmental state information as an input of the current value network and then generates an action based on the current policy through training. it then performs an action to obtain the empirical data and store them in the playback memory unit. finally, the empirical data are used to update the value function and model parameters until the error is the smallest and the cumulative return value is at a maximum. figure 5 shows the main flowchart for the high-level driving decisions for mass."
"as the data can be divided in many ways into k groups it introduces additional variance in the estimates, which is also evident from our experiments. this variance can be reduced by repeating k-fold-cv several times with different permutations in the data division, but this will further increase the computational cost."
"the significance of a good data imputation process is especially important in the field of medicine where discovery and imputation of missing values can help to identify abnormal conditions and reduce incorrect diagnosis [cit] . hence, interest has risen considerably in this and associated fields where it is important to effectively model and analyze multivariate time series data. therefore to examine the performance of the proposed algorithm for handling a real-world missing data problem, a case study involving electro-cardio gram (ecg) signals was accomplished. an ecg dataset without missing values was obtained from the physionet website (http://www.physionet.org/ physiobank/data base/ptbdb). then two datasets were created by randomly removing data elements. a 10% missing completely at random (mcar) and a 20% mcar dataset was created. the initial physionet dataset included 290 patients with 549 records (aged between 17 and 87, mean 57.2; 209 men, mean age 55.5, and 81 women, mean age 61.6; ages were not recorded for 1 female and 14 male subjects). each subject is represented by one to five records. there are no subjects numbered 124, 132, 134, or 161. each record includes 15 simultaneously measured signals: the conventional 12 leads (i, ii, iii, avr, avl, av f, v1, v2, v3, v4, v5, v6) together with three frank lead ecgs (vx, vy, vz). each signal is digitized at 1000 samples per second, with 16-bit resolution over a range of 16.384 mv. on special request to the contributors of the database, recordings may be available at sampling rates up to 10khz. more detailed discussion can be found [cit] . the diagnostic classes of the patients are divided into nine types; this case study considered a 12-lead ecg signals for two diagnostic classes: myocardial infarction and healthy control for two patients. two cases of mcar missing data mechanism with two different percentages 10% and 20% were generated. table 1 shows the values of the four model order selection criteria. as can be seen each test indicates that the model with lag two has the highest priority. tables 2 and 3 a c c e p t e d m a n u s c r i p t show the recovering accuracy for the missing data in the heart rate signal using different imputation methods, under two cases of missing data mechanisms, i.e., 10% and 20% mcar, respectively. in both cases, the proposed method var-im gives better results comparing with the other methods."
"for dic, there is a similar variance-based computation of the number of parameters that is notoriously unreliable, but the waic version is more stable because it computes the variance separately for each data point and then takes the sum; the summing yields stability."
"(2) safety: in the deep q-learning algorithm model, the unknown environment is divided into a state space, which is divided into a safe state area and an obstacle area. the system should be selected in the local area without the obstacle to sailing. an action search strategy and \"early, clear, big-amplitude\" is used to avoid obstacles. thus, in the reward function, the penalty value is added to the behavior close to the obstacle, and the reward value is increased: where n obs is the number of obstructions that the mass needs to avoid in the present state of the ship,  is the symbol \"or\", and (x obs, y obs ) is the obstacle position. z 0 is the safe encounter distance of the ship."
"large estimates for the tail shape parameter indicate that the full posterior is not a good importance sampling approximation for the desired leave-one-out posterior, and thus the observation is surprising. the original model used the white blood cell count directly as a predictor, and it would be natural to use its logarithm instead. figure 10 shows the distribution of the estimated tail shapes k and estimation errors compared to loo in 100 independent stan runs for this modified model. both the tail shape values and errors are now smaller."
"the computed estimates elpd loo and elpd waic are each defined as the sum of n independent components so it is trivial to compute their standard errors by computing the standard deviation of the n components and multiplying by  n. for example, define"
"to escape the effects of the discretisation of the color space which is intrinsic to the use of histograms, another approach has been used successfully in several image retrieval systems, his approach involves calculating a weighted sum of the average and the variance and for each color channel, for providing a ] the authors showed that the methods of using statistical moments goes faster and give better results is the value of the pixel j for channel i (i: hsv), n is the number of pixels in the image, these moments are defined"
"1. fit the generalized pareto distribution to the 20% largest importance ratios r s as computed in (6). the computation is done separately for each held-out data point i. in simulation experiments with thousands and tens of thousands of draws, we have found that the fit is not sensitive to the specific cutoff value (for a consistent estimation, the proportion of the samples above the cutoff should get smaller when the number of draws increases)."
"we next consider some approaches for assessing the uncertainty of cross-validation and waic estimates of prediction error. we present these methods in a separate section rather than in our main development because, as discussed below, the diagnostics can be difficult to interpret when the sample size is small."
"in this paper, the autonomous navigation decision-making algorithm for a mass based on the drl algorithm is proposed. the model was established based on the four basic elements of deep qlearning. however, the simulation results were not satisfactory, and the easy-to-fall-into local in summary, by comparing the convergence and decision-making effects of apf-drl and drl algorithms from multiple aspects, this study finds that the performance of the models and algorithms based on an artificial potential field to improve deep reinforcement learning was better."
"finally, these methods all have limitations. the concern with waic is that formula (12) is an asymptotic expression for the bias of lpd for estimating out-of-sample prediction error and is only an approximation for finite samples. cross-validation (whether calculated directly by re-fitting the model to several different data subsets, or approximated using importance sampling as we did for loo) has a different problem in that it relies on inference from a smaller subset of the data being close to inference from the full dataset, an assumption that is typically but not always true."
process of calculation of our descriptor [cit] will be presented in the (fig. 5) . we see that the new method is more efficient than other methods mentioned above.
"we demonstrate the practical use of loo in model comparison using the radon example from section 4.6. model a is the multilevel linear model discussed in section 4.6 and model b is the same model but without the county-level uranium predictor. that is, at the county-level model b has"
"(2) safety: in the deep q-learning algorithm model, the unknown environment is divided into a state space, which is divided into a safe state area and an obstacle area. the system should be selected in the local area without the obstacle to sailing. an action search strategy and \"early, clear, big-amplitude\" is used to avoid obstacles. thus, in the reward function, the penalty value is added to the behavior close to the obstacle, and the reward value is increased:"
"both the environmental state set and motion state in the deep q-learning algorithm are limited, and the actual mass transportation process is a continuous systematic event. thus, this study generalizes the activation function as a nonlinear hybrid function:"
"to escape the effects of the discretisation of the color space which is intrinsic to the use of histograms, another approach has been used successfully in several image retrieval systems, it is the statistical moments of colors. this approach involves calculating a weighted sum of the average and the variance and the third moment for each color channel, for providing a unique number used to index."
"hand for illustrative purposes only) has a smaller rmse for small and medium scale values as the variance is reduced, but the price is increased bias at larger scale values."
"the standard error (23) and the corresponding formula for se ( elpd waic ) have two difficulties when the sample size is low. first, the n terms are not strictly independent because they are all computed from the same set of posterior simulations  s . this is a generic issue when evaluating the standard error of any cross-validated estimate. second, the terms in any of these expressions can come from highly skewed distributions, so the second moment might not give a good summary of uncertainty. both of these problems should subside as n becomes large. for small n, one could instead compute nonparametric error estimates using a bayesian bootstrap on the computed log-likelihood values corresponding to the n data points [cit] ."
"ontology is a philosophy concept, which studies the nature of existence. ontology can be classified into four types: domain, general, application, and representation [cit] . a complete marine transportation system is a closed-loop feedback system consisting of \"human-ship-sea-environment.\""
"when the tail of the weight distribution is long, a direct use of importance sampling is sensitive to one or few largest values. by fitting a generalized pareto distribution to the upper tail of the importance weights, we smooth these values. the procedure goes as follows:"
"where, l(b; x 0, x m ) is the likelihood function, b is the parameter vector, b n+1 is the estimate of the model parameters, x 0 is observed data, x m is the missing data. in the m-step, the model parameters can be calculated using (2) to maximize completedata log likelihood function from the e-step:"
"the quantity of images produced and archived every day is growing. to exploit these archives, mechanisms to indexing and retrieval stored images are essential. this requires choosing on the one hand, an appropriate representation of the database images through meaningful and reliable visual primitives able to describe the contents of the database and, on the other hand, an efficient indexing structure which takes place in the relatively short time during which the user expects the system response. several of image retrieval systems are now available in the market, some of these systems are commercialized with a demonstration on the web as qbic [cit], and others are still in an experimental phase as blobworld [cit] . we outline the main application areas of content-based image retrieval such as the field of biometrics fingerprint recognition or eye color, they are based on specific algorithms for determining these models, we also find handwriting recognition, pattern recognition, medical imaging .., to render the images in the database by the similarity. generally, the criteria we use to describe an image are the low-level visual descriptors, also called feature vectors, such as color, texture, and shape. these are called low-level descriptors because these are closer to a signal (since image is a signal). these are widely used in current systems because they are easy to implement. this work described in its sections a new method of color images retrieval, and it is organized as follows. we make an overview on the technique of generating 2-d histogram in hsv space in the second and third section, we present then in the fourth section the difference between histograms and histobins. the fifth section focuses on statistical moments. we end this paper by giving some results and then a conclusion."
"the drl algorithm had no prior knowledge of the environment, and all state value functions v(s) in the initial state were equal or completely random. each step of action a was produced in a random state, i.e., the markov decision process (mdp) environment state transition probability was equal. for the track decision problem, the return value r was only be changed when the destination was reached or obstacles are encountered. the sparsity of the reward function resulted in an initially low decision efficiency and numerous iterations. particularly for large-scale unknown environments, there was a large amount of invalid iterative search space."
"in the previous example, we used exact loo as the gold standard. in this section, we generate simulated data from the same statistical model and compare predictive performance on independent test data. [cit] . comparing the error, bias and variance of the various approximations, we find that psis-loo offers the best balance."
"we shall discuss figure 2c in a moment, but first consider figure 3, which shows the rmse of the approximation methods and the lpd of observed data decomposed into bias and standard deviation. all methods (except the lpd of observed data) have small biases and variances with small population distribution scales. bias corrected exact loo has practically zero bias for all scale values but the highest variance. when the scale increases the loo approximations eventually fail and bias increases. as the approximations start to fail, there is a certain region where implicit shrinkage towards the lpd of observed data decelerates the increase in rmse as the variance is reduced, even if the bias continues to grow."
"in this section, the abovementioned mass autonomous navigation decision-making module collects the ship's own information and environmental status through the sensing layer as input for deep reinforcement learning. through the system self-learning, the best navigation strategy is finally decided, which makes the cumulative return of mass in the self-learning process the largest. once the system is trained, mass will automatically navigate to avoid obstacles and reach the destination under the command of the autonomous navigation decision-making level."
the loo r package provides the functions loo() and waic() for efficiently computing psis-loo and waic for fitted bayesian models using the methods described in this paper.
code for waic. for waic the code is analogous and the objects returned have the same structure (except there are no pareto k estimates). the compare() function can also be used to estimate the difference in waic between two models:
"on the basis of the above description, we are developing a new approach for image retrieval. in the first step, we use the space color hsv to construct the histogram where each pixel is represented by its hue or its intensity."
"obtaining good, reliable, and complete data for a research study is often taken for granted, however, without good data; the results of a research project will be incorrect and could lead to significant errors in model development. for various reasons the obtained data may be corrupted with missing, incorrect, or distorted values. these anomalies may occur during or after the data collection process. the problem of how to deal with corrupted data has been a significant problem throughout many research fields for many years. data imputation is the process of replacing missing, abnormal and distorted values of dataset. many techniques of imputing missing data have been developed as it constitutes a central part of data mining and analysis [cit] . for this study, two of the traditional and modern methods were selected as baseline comparisons to the proposed new algorithm. these are list wise deletion, linear regression imputation, marss package and em algorithm."
"the first three terms of the expansion of waic match the expansion of loo, and the rest of the terms match the expansion of lpd. [cit] argues that, asymptotically, the latter terms have negligible contribution and thus asymptotic equivalence with loo is obtained. [cit], and demonstrated also in section 4. if the higher order terms are not negligible, then waic is biased towards lpd. to reduce this bias it is possible to compute additional series terms, but computing higher moments using a finite posterior sample increases the variance of the estimate and, based on our experiments, it is more difficult to control the bias-variance tradeoff than in psis-loo. waic's larger bias compared to loo is also demonstrated by in the case of gaussian processes with distributional posterior approximations. in the experiments we also demonstrate the we can use truncated is-loo with heavy truncation to obtain similar bias towards lpd and similar estimate variance as in waic."
"learning from the decision-making of the officer on the voyage, this research proposes a hierarchical progressive navigation decision-making system, which mainly includes two sub-modules: a scene division module and a navigation action generation module. the main contributions of this paper are as follows:"
the remaining sections of the paper are organized as follows. related works are presented in section 2. the scene division module is presented in section 3. the autonomous navigation decision-making module is presented in section 4. the simulation results and algorithm improvement are presented in section 5. the paper is concluded in section 6.
"high estimates of the tail shape parameter k indicate that the full posterior is not a good importance sampling approximation to the desired leave-one-out posterior, and thus the observation is surprising according to the model. it is natural to consider an alternative model. we tried replacing the normal observation model with a student-t to make the model more robust for the possible outlier. figure 6 shows the distribution of the estimated tail shapesk and estimation errors for psis-loo compared to loo in 100 independent stan runs for the student-t linear regression model. the estimated tail shapes and the errors in computing this component of loo are smaller than with gaussian model."
"start sampling from random state s 0 and randomly select action. sampling is terminated at t cycles or the mass collides. the resulting sample set is s. each input in s must be included: (1) current states s t, (2) action a, (3) return r, (4) the next state after the action s t+1, and (5) the termination condition output: weights parameter  * for drl require: : a small positive number representing the allowed smallest convergence tolerance; s: the state set; p(s, r s, a) : the transition probability from current state and action to next state and reward; : the discount factor;"
"the vector autoregressive model (var) is commonly used model for the analysis of multivariate time series. in many applications where the variables of interest are linearly each related to each other the var model has shown to be a good choice for representing and predicting the behaviour of dynamic multivariate time series [cit] . it primarily provides good forecasts as compared to models from univariate time series and many other models. because the var model can make conditions on the prediction paths of specified time series within the model itself, the forecasts from this model are relatively easy to derive [cit] . in addition to time series analysis and prediction, the var model is additionally utilized for causality inference and strategy investigation of the multiple time series. in causality analysis, specific hypotheses of the causality of the time series under analysis are assumed, and the subsequent causal effects of each time series are outlined. this chapter concentrates on the use of the var model to analyse stationary multiple time series datasets with missing data."
further research needs to be done to evaluate the performance in model comparison of (24) and the corresponding standard error formula for loo. [cit] .
"aiming at the problem of local path planning and collision avoidance decision-making, a method of autonomous navigation decision-making for masss based on deep reinforcement learning is proposed, in which the reward function of multi-objective optimization is designed, which consists of safety and approaching target points."
"starting point coordinate a and the target point coordinate b were determined. a gravitational potential field with the target point as the potential field center in the initialization state value function was established. the v(s) table was initialized according to the position of the target point as prior environmental information, and the initialized v(s) value was set as larger than or equal to 0."
"in this section, it is shown that the effectiveness of the autonomous navigation decision-making algorithm for the mass based on deep reinforcement learning was verified by the case study. this experiment built a two-dimensional (2d) simulation environment based on python and pygame. specifically, the numpy library and sys, random, and math modules were used for the simulation. in the 2d coordinate system, each coordinate point corresponded to a state of the mass, and each state could be mapped to each element of the environmental state set. in the simulation environment model, there were two state values for each coordinate point, which were 1 and 0, where 1 represented the navigable area, which is shown as the sea-blue area in the environmental model, and 0 represented the obstacle area, which is shown as a brown and dark gray area in the environmental model. in accordance with the simulation environment model in figure 6, the 2d map of the state of the simulation environment was simulated, and the obstacles, such as the ship, breakwater, and shore bank were simulated in the environmental model. for the mass, the location information for these obstacles was uncertain."
"for autonomous navigation decision-making based drl in complex environments, the action state space is large and iteration speed is slow. when the gravitational field of target point was added as the potential field center to improve the drl algorithm, the autonomous navigation decision-making of unmanned ships tends to the target point more quickly and iteratively, and the navigation strategies given in each state are directional, whereas a random strategy ensures that it does not fall into a local optimal solution. the dynamic and experimental parameters of the ship were the same as those in section 5.1."
"in most cases we recommend partitioning the data into subsets by randomly permuting the observations and then systemically dividing them into k subgroups. if the subjects are exchangeable, that is, the order does not contain information, then there is no need for random selection, but if the order does contain information, e.g. in survival studies the later patients have shorter follow-ups, then randomization is useful. in some cases it may be useful to stratify to obtain better balance among groups. [cit] for further discussion of these points."
"the deep q-learning algorithm based on the markov decision process could obtain the optimal path through the trial and error algorithm, but its convergence speed was still slower and the number of iterations was large. the first intended improvement involves enhancing the adaptive ability of drl such that a small number of iterations can be used to learn the correct navigation behavior with only a small number of samples."
(1) approach to the target point: the search behavior of the autonomous navigation decisionmaking made in an uncertain environment should bring the mass closer to the target point. a value close to the incentive function will choose the reward; otherwise it will be punished:
"for dynamic systems the auto-regression process depends on the past values of the targeted data point, if the time series includes missing values, means that there is past values missed and the auto-regression cannot be applied in (15) . in this case the traditional approaches such as list wise will not work, because ignoring of missing values will effect on the property of dynamic system. to start up the estimation process correctly,"
"in the reinforcement learning system of a mass, the activation function plays an important role in evaluating the effectiveness of the behavioral decision-making and safety of obstacle avoidance. it has a search-oriented role. the goal of reinforcement learning is to obtain the search strategy that gives the highest return value in the driverless process. the reward function consists of safety, comfort, and arrival target points. when designing the reward function, the following elements should be maximally considered [cit] ."
"it is extremely important to effectively handle multivariate data anomalies that contain missing values. this is especially true for medical data, which could involve great number of critical health diagnostic variables. the proposed var-im method provides improvements to speed and accuracy for imputing missing values of multivariate time series datasets. it outperforms the commonly used methods such as list wise deletion, linear regression imputation, marss and em algorithms. from the results of the case study, the var-im method provides an effective alternative for the imputation of missing values in multivariate time series. while the other proposed traditional and modern methods become less effective with the increase of the proportion of missing data, var-im shows less deterioration in performance with increasing percentages of missing entries. in addition, the var-im method is more robust than the other proposed techniques when applied to the data types discussed in the case study, and performed better on static and noisy data. there are some limitations of the proposed method. firstly, this study only considered the scenario in which data was missing completely at random (mcar), that is, the cause of the missing data was independent of both the observed and missing values. a less stringent assumption of missing data mechanism missing at random (mar) may be more realistic in practice. mar refers to the case in which missingness is related to the observed values, but not to the missing values themselves. secondly, the validity of var-im approach requires that the time series should be stationary. finally, the percentage of missing data has significant impact on most missing data analysis methods, var-im does not have the priority to be used if the percentage of missing data is quite low (say less 10%). despite these limitations, var-im provides an important alternative to existing methods for handling missing data in multivariate time series. further extension of the method to include other types of methods will be considered in other future work. his research interests concern application of missing data analysis, generalised linear models, clustering, pattern recognition, regression, gaussian processes, system identification; polynomial models, var modelling, model selection; kernel density estimation and em algorithm."
(1) approach to the target point: the search behavior of the autonomous navigation decision-making made in an uncertain environment should bring the mass closer to the target point. a value close to the incentive function will choose the reward; otherwise it will be punished:
"the scene division mainly organizes and describes the multi-source heterogeneous information in the driving scene with the prolog language [cit] . this research uses the ontology theory and the principle of divide and conquer to divide navigation environment into entities and attributes. entity classes are used to describe the objects of different attributes, including chart entity, obstacle entity, ego ship, and environment. attribute classes are used to describe the semantic properties of an object, including position attributes and orientation attributes."
"the experiment set the initial position (128, 416) and target point (685, 36) of the mass. in the early stages of the experimental iterations, the mass collided with the obstacle at different time steps, and there was no collision after the initial iteration in the experiment. the system maneuvered the mass back to the previous navigation state and re-decided the navigation path planning strategy. compared with the experiment in section 5.1, as shown in figure 8a, in the initial iteration, the mass fell into a local iteration. as shown in figure 8b, after 100 iterations, the system first planned an effective navigation path, and the collision obstacle phenomenon occurred multiple times in the process, but the path of the experiment with the same iteration step was shorter than that in section 5.1. as shown in figure 8c,d, compared with the section 5.1 iteration steps, the collision phenomenon was reduced and the path fluctuation was significantly slowed down. as displayed in figure 8e,f, at the 1500th [cit] iterations. the final authenticated publication is available online in reference [cit] ."
"when the filter is stable and the input series u t is stationary with cross-covariance matrices  u (p), (5) is a stationary process [cit] . the cross-covariance matrices of the stationary process y t are then given by"
"since each distance between features was calculated using a different standard, prior to combining these features, we normalize the distances using a gaussian [11 ensures that the distance is normalized between 0 and 1. after the normalized distances, we used a linear combination of distances."
"for example, as we demonstrated in section 4.1, in a hierarchical model with only one data point per group, psis-loo and waic can dramatically understate prediction accuracy. another setting where loo (and cross-validation more generally) can fail is in models with weak priors and sparse data. for example, consider logistic regression with flat priors on the coefficients and data that happen to be so close to separation that the removal of a single data point can induce separation and thus infinite parameter estimates. in this case the loo estimate of average prediction accuracy will be zero (that is, elpd isloo will be ) if it is calculated to full precision, even though predictions of future data from the actual fitted model will have bounded loss. such problems should not arise asymptotically with a fixed model and increasing sample size but can occur with actual finite data, especially in settings where models are increasing in complexity and are insufficiently constrained."
"list wise deletion is among the simplest techniques for imputing missing data. specifically, in this technique, all measured values at a specific time point, are ignored if one of the variables has a missing value for that specific measurement. because this method removes the data with missing values, it decreases the number of variables and the length of sequences resulting in a reduced sample size. in dynamic modelling where all values are important for estimating the current values, the list wise deletion approach can significantly affect the autoregressive model estimation. although even with these weaknesses, this approach is still being used for missing data analysis due to its simplicity. in some mainstream statistical programming such as r and sas, this method is the most popular one for dealing with missing values, especially when analysing time series. however, there is no obvious indication that list wise deletion is adequate for handling missing data involving multivariate time series modelling [cit] ."
"pareto smoothed importance sampling. we can improve the loo estimate using pareto smoothed importance sampling (psis; [cit] ), which applies a smoothing procedure to the importance weights. we briefly review the motivation and steps of psis here, before moving on to focus on the goals of using and evaluating predictive information criteria."
"in this paper, the autonomous navigation decision-making algorithm for a mass based on the drl algorithm is proposed. the model was established based on the four basic elements of deep q-learning. however, the simulation results were not satisfactory, and the easy-to-fall-into local iterations and slow iteration convergence speed problems were apparent. to solve this problem, we added a gravitational potential field centered on the target point, and established an autonomous navigation decision-making algorithm of the mass based on apf-drl. in the initial stage of the interaction with the environment, the mass had little knowledge of the environmental status information, and there were collisions and large fluctuations in the navigation path planning. as the number of iterations increased, the mass accumulated learning experience, completed the adaptation to the environment, and finally successfully planned the path and reached the target point. in future research, the algorithm still needs significant improvements:"
"the moment i  of the first order is the average color of the image. the second moment  image. more variance of colors is big, more the picture is contrasted. the moment of the third order amount of light in an image. an image with a positive dissymmetry coefficient tends to appear darker and brighter than a similar image with a lower coefficient of dissymmetry."
"however, few experts currently apply deep reinforcement learning to mass intelligent navigation. taking advantage of these, this paper uses deep reinforcement learning to solve the problem of autonomous navigation decision-making for a mass. learning from the decision-making of the officer on the voyage, this research proposes a hierarchical progressive navigation decision-making system, which mainly includes two sub-modules: a scene division module and an autonomous navigation action generation module."
"the order of the model p k is updated until the differencea pk  a p(k+n) is less than , where  is a prescribed small value [cit] ."
"as measured by root mean squared error, psis consistently performs well. in general, when is-loo has problems it is because of the high variance of the raw importance weights, while tis-loo and waic have problems because of bias. table 3 shows a replication using 16,000 stan draws for each example. the results are similar results and psis-loo is able to improve the most given additional draws."
"on the basis of the threshold function (equation (2)), it determines an intermediate image (fig 1 (b) ): for values low saturation, a color can be approximated with a gray value specified by the intensity level, while for higher saturation, the color can be approximated by its hue value. then, we build two histograms color 2-d for intermediate images (fig 1 (c) and 1(d)) as follows: for each block of pixels 3 x 3 of two intermediate images, we consider the central pixel which can be a component of intensity (fig 2(a) ) or hue component (fig 2 (c) ), according to the equation (2), and calculate the maximum value (max) and minimum value (min) in the component matching (h or v) in the original image (fig 2(b) and 2 (d) ). while max is the maximum hue or intensity in the 3 x 3 block, except four-neighbor pixels and min is the minimum hue or intensity in the four-neighbor. finally, we will obtain a 2-d histogram in the hsv space (fig 3(c) ). it consists of two separate histograms. the first represents the pixels associated with the values of hue (fig 3 (a) ) and the second with the intensities (fig 3 (b) )."
"we illustrate with six simple examples: [cit] to illustrate the estimation of the variance of the weight distribution, and one example of a multilevel regression from our earlier applied research. for each example we used the stan default of 4 chains run for 1000 warmup and 1000 post-warmup iterations, yielding a total of 4000 saved simulation draws. with gibbs sampling or random-walk metropolis, 4000 is not a large number of simulation draws. the algorithm used by stan is hamiltonian monte carlo with no-u-turn-sampling [cit], which is much more efficient, and 1000 is already more than sufficient in many real-world settings. [cit] . we performed 100 independent replications of all experiments to obtain estimates of variation. for the exact loo results and convergence plots we run longer chains to obtain a total of 100,000 draws (except for the radon example which is much slower to run)."
"after fitting a bayesian model we often want to measure its predictive accuracy, for its own sake or for purposes of model comparison, selection, or averaging [cit] . cross-validation and information criteria are two approaches to estimating out-of-sample predictive accuracy using within-sample fits [cit] . in this article we consider computations using the log-likelihood evaluated at the usual posterior simulations of the parameters. computation time for the predictive accuracy measures should be negligible compared to the cost of fitting the model and obtaining posterior draws in the first place."
"when comparing two fitted models, we can estimate the difference in their expected predictive accuracy by the difference in elpd loo or elpd waic (multiplied by 2, if desired, to be on the deviance scale). to compute the standard error of this difference we can use a paired estimate to take advantage of the fact that the same set of n data points is being used to fit both models."
"although drl-based mass navigation behavioral decision-making and path planning in an uncertain environment were realized, the algorithm iteration speed was too slow in the whole experiment, the total iteration time was up to 14 min, and it was trapped in local iterations many times. affects the applicability and credibility of behavioral decisions. therefore, there was a need to improve the drl-based behavioral decision-making algorithm. therefore, this section describes the"
"for k-fold cross-validation, if the subjects are exchangeable, that is, the order does not contain information, then there is no need for random selection. if the order does contain information, e.g. in survival studies the later patients have shorter follow-ups, then randomization is often useful."
"we have defined the log-likelihood as a vector log lik in the generated quantities block so that the individual terms will be saved by stan. 6 it would seem desirable to compute the terms of the log-likelihood directly without requiring the repetition of code, perhaps by flagging the appropriate lines in the model or by identifying the log likelihood as those lines in the model that are defined relative to the data. but there are so many ways of writing any model in stan-anything goes as long as it produces the correct log posterior density, up to any arbitrary constant-that we cannot see any general way at this time for computing loo and waic without repeating the likelihood part of the code. the good news is that the additional computations are relatively cheap: sitting as they do in the generated quantities block (rather than in the transformed parameters and model blocks), the expressions for the terms of the log posterior need only be computed once per saved iteration rather than once per hmc leapfrog step, and no gradient calculations are required."
"in the reinforcement learning system of a mass, the activation function plays an important role in evaluating the effectiveness of the behavioral decision-making and safety of obstacle avoidance. it has a search-oriented role. the goal of reinforcement learning is to obtain the search strategy that gives the highest return value in the driverless process. the reward function consists of safety, comfort, and arrival target points. when designing the reward function, the following elements should be maximally considered [cit] ."
"the entity class is categorized into four sub-entity classes: chart entity, obstacle entity, mass entity (egoship), and environmental information. chart entity includes point entity, line entity, and area entity, where point entity refers to navigational aids and line entity refer to reporting lines. the area entity includes seapart, channel, boundary, narrow channel, divider, anchorage, junction, and segment. obstacle entities include static obstacle and dynamic obstacle, where static obstacle entities are divided into rocks, wreck, and boundary; and dynamic obstacle entities include ships (vessel), floating ice, and marine animal. a mass entity (egoship) is used to describe its own state information. environmental information includes height, sounding, nature of the seabed, visibility, and disturbance."
"in this section, it is shown that the effectiveness of the autonomous navigation decision-making algorithm for the mass based on deep reinforcement learning was verified by the case study. this experiment built a two-dimensional (2d) simulation environment based on python and pygame. specifically, the numpy library and sys, random, and math modules were used for the simulation. in the 2d coordinate system, each coordinate point corresponded to a state of the mass, and each state could be mapped to each element of the environmental state set. in the simulation environment model, there were two state values for each coordinate point, which were 1 and 0, where 1 represented the navigable area, which is shown as the sea-blue area in the environmental model, and 0 represented the obstacle area, which is shown as a brown and dark gray area in the environmental model. in accordance with the simulation environment model in figure 6, the 2d map of the state of the simulation environment was simulated, and the obstacles, such as the ship, breakwater, and shore bank were simulated in the environmental model. for the mass, the location information for these obstacles was uncertain."
"the initial values are required; the simple way to determine these initial values is using a simple traditional method such as linear interpolation. we will denote this by expressing x t as (x tmiss, x tobs ), where x tmiss denotes the multivariate data set with missing values, and x tobs represents the multivariate data set with replacing missing values by initial values (imputed by interpolation technique)."
"t one of the fundamental objectives of multivariate time series analysis of y t is to fit the data to a model and demonstrate the dynamic relationships among univariate time series. the selection of each time series model, included in y t depends on the dynamic interrelationships between these time series variables which are affected directly by time lags between the data points for each time series. the multivariate time series data set y t is stationary time series if at arbitrary time intervals t 1, t 2, ., t k the probability distributions of the component time series variables y t1, y t2, ., y tk and y t1p, y t2p, ., y tkp are the same, where k is the number of the measured values p represents the lag. that means cross time intervals t 1, t 2, ., t k throughout the stationary multivariate time series, has a random probability distribution of the observed data points with respect to the time lags. consequently, any stationary multivariate time series should have the same mean value m at any time intervals where:"
"we can also compare models in their leave-one-out errors, point by point. we illustrate with an analysis of a survey of residents from a small area in bangladesh that was affected by arsenic in drinking water. respondents with elevated arsenic levels in their wells were asked if they were interested in getting water from a neighbor's well, and a series of models were fit to predict this binary response given various information about the households [cit] ."
"recently, marine accidents have been frequently caused by human factors. based on the statistics from the european maritime safety agency (emsa), [cit], there were 3301 casualties and accidents at sea, with 61 deaths, 1018 injuries, and 122 investigations initiated. in these cases, human error behavior represented 58% of the accidents and 70% of the accidents were related to shipboard operations. in addition, the combination of collision (23.2%), contact (16.3%), and grounding/stranding (16.6%) shows that navigational casualties represent 56.1% of all casualties with ships [cit] . the important purpose of maritime autonomous surface ships (masss) research is to reduce the incidence of marine traffic accidents and ensure safe navigation. therefore, safe driving and safe automatic navigation have become urgent problems in the navigation field. future shipping systems will rely less and less on 1. first, the molded dimension of a mass is larger. research on the key technologies of usvs pays more attention to motion control. however, for masss, navigation brains that can make autonomous navigation decisions are needed more. 2. second, the navigation situation of a mass is complex and changeable, and its maneuverability is slow to respond. therefore, it is necessary to combine scene division with adaptive autonomous navigation decision-making in order to achieve safe decision-making for local autonomous navigation."
(2) which can lead to a characteristic component of both parties: the values of hue between 0 and 2 quantified after a transformation and a set of quantified intensity values.
an artificial potential field is added to alleviate the problem of easy-to-fall-into local iterations and slow iterations of autonomous navigation decision-making algorithms based on deep reinforcement learning.
simulation results based on python and pygame show that the artificial potential field-deep reinforcement learning (apf-drl) method has better performances than the drl method in both autonomous navigation decision-making and algorithm iteration efficiency.
the multivariate auto-regressive state space (marss) [cit] as the first complete package for handling missing data in multivariate time series data [cit] . marss incorporates an expectation-maximization (em) algorithm. it is an r package employing a special formula of vector autoregressive state-space models to fit multivariate time series with missing data via an em algorithm. a marss model has the following matrix structure:
2. stabilize the importance ratios by replacing the m largest ratios by the expected values of the order statistics of the fitted generalized pareto distribution
"as before, these calculations should be most useful when n is large, because then non-normality of the distribution is not such an issue when estimating the uncertainty of these sums."
"the figure 7a, in the initial iteration, the mass could not determine the temptation area in the simulation environment and fell into the \"trap\" sea area in the simulation port pool. as shown in figure 7b, after 100 iterations, the system gradually planned the effective navigation path, but the collision obstacle phenomenon occurred many times in the process, and the planning navigation path fluctuated significantly. as shown in figure 7c,d, after 200 to 500 iterations, respectively, the collision phenomenon was gradually reduced, and the planning path fluctuation slowed down. as shown in figure 7e, all the obstacles were effectively avoided after iterating 1000 times and the planned path fluctuations were weak and gradually stabilized. as shown in figure 7f, [cit] th iteration, the probability of a random search was the smallest, and the system navigated the final fixed path through the decision system to reach the target point."
"the mathematical nature of reinforcement learning can be regarded as a markov decision process (mdp) in discrete time. a markov decision process is defined using the following five-tuple, ( s, a, a p, a r,  ). s represents the finite state space in which the mass is located. a represents the behavioral decision space of mass, i.e., a collection of all the behavioral spaces of the mass in any state, such as left rudder, right rudder, acceleration, and deceleration.  (0,1) is the discount factor of the stimulus, and the discounting at the next moment is determined according to a factor [cit] . figure 4 displays the schematic of the autonomous navigation decision-making of the mass based on deep reinforcement learning. in the memory pool, the current state of the observed mass is taken as the input of the neural network. the q value table of the action that can be performed in the current state is the output, and the behavioral strategy corresponding to the maximum q value is learned through training."
"although the implementation is not automatic when writing custom stan programs, we can create implementations that are automatic for users of our new rstanarm r package [cit] . rstanarm provides a high-level interface to stan that enables the user to specify many of the most common applied bayesian regression models using standard r modeling syntax (e.g. like that of glm). the models are then estimated using stan's algorithms and the results are returned to the user in a form similar to the fitted model objects to which r users are accustomed. for the models implemented in rstanarm, we have preprogrammed many tasks, including computing and saving the pointwise predictive measures and importance ratios which we use to compute waic and psis-loo. the loo method for rstanarm models requires no additional programming from the user after fitting a model, as we can compute all of the needed quantities internally from the contents of the fitted model object and then pass them to the functions in the loo package. examples of using loo with rstanarm can be found in the rstanarm vignettes, and we also provide an example in appendix a.3 of this paper."
"this paper has focused on the practicalities of implementing loo, waic, and k-fold cross-validation within a bayesian simulation environment, in particular the coding of the log-likelihood in the model, the computations of the information measures, and the stabilization of weights to enable an approximation of loo without requiring refitting the model. some difficulties persist, however. as discussed above, any predictive accuracy measure involves two definitions: (1) the choice of what part of the model to label as \"the likelihood,\" which is directly connected to which potential replications are being considered for out-of-sample prediction; and (2) the factorization of the likelihood into \"data points,\" which is reflected in the later calculations of expected log predictive density."
"to test and to implement our descriptor for color image retrieval, we develop the following the moment of the third order i s characterizes the amount of light in an image. an image with a positive nds to appear darker and brighter than a similar image with a lower coefficient of dissymmetry."
"if the goal is robust estimation of predictive performance, then exact loo is the best general choice because the error is limited even in the case of weak priors. of the approximations, psis-loo offers the best balance as well as diagnostics for identifying when it is likely failing."
"two sets of experimental data were extracted, and the performance of the two navigation decision-making algorithms were compared and analyzed from the aspects of the number of local iterations, large fluctuation iterations (waves more than 300 times), collision rate, optimal decision iteration number, and optimal decision iteration time. as presented in table 2, compared with the drl algorithm, the decision algorithm that added the artificial potential field to improve the deep reinforcement learning had fewer local iterations. moreover, the number of fluctuations and the trial and error rate were reduced. the simulation result shows that the apf-drl algorithm had a fast convergence rate. two sets of experimental data were extracted, and the performance of the two navigation decision-making algorithms were compared and analyzed from the aspects of the number of local iterations, large fluctuation iterations (waves more than 300 times), collision rate, optimal decision iteration number, and optimal decision iteration time. as presented in table 2, compared with the drl algorithm, the decision algorithm that added the artificial potential field to improve the deep reinforcement learning had fewer local iterations. moreover, the number of fluctuations and the trial and error rate were reduced. the simulation result shows that the apf-drl algorithm had a fast convergence rate. in addition, the step size of each iteration of the two sets of experiments and iterative trends were visually analyzed. figure 10 displays the comparison of the iterative step size scatter distribution of the two sets of experiments. the color of the scatter in the graph represents the path length of the decision. the drl-distance was up to 4000, and the apf-drl algorithm had a maximum iteration step size of 3000, which was much smaller than 4000. furthermore, the distribution trend of the two sets of scattering points exhibited that the scatter points were almost concentrated on the drl experimental iteration surface, which indicated that the improved algorithm had a shorter distance for each iteration. thus, the experimental iterations done by the new procedure were better. in addition, the step size of each iteration of the two sets of experiments and iterative trends were visually analyzed. figure 10 displays the comparison of the iterative step size scatter distribution of the two sets of experiments. the color of the scatter in the graph represents the path length of the decision. the drl-distance was up to 4000, and the apf-drl algorithm had a maximum iteration step size of 3000, which was much smaller than 4000. furthermore, the distribution trend of the two sets of scattering points exhibited that the scatter points were almost concentrated on the drl experimental iteration surface, which indicated that the improved algorithm had a shorter distance for each iteration. thus, the experimental iterations done by the new procedure were better. figure 11 shows the experimental collision avoidance parallel diagram of the two algorithms. the three parallel axes in the figure, from left to right, were drl avoidances, apf-drl avoidances, and epochs. the heavier the color, the worse the algorithm learning effect and the slower the convergence. for the apf-drl algorithm, the avoidance presents a regular distribution, where as the number of iterations increased, the number of collisions decreased, and the self-learning success rate of the algorithm improved. on the contrary, drl's collision avoidance iteration diagram is rather figure 11 shows the experimental collision avoidance parallel diagram of the two algorithms. the three parallel axes in the figure, from left to right, were drl avoidances, apf-drl avoidances, and epochs. the heavier the color, the worse the algorithm learning effect and the slower the convergence. for the apf-drl algorithm, the avoidance presents a regular distribution, where as the number of iterations increased, the number of collisions decreased, and the self-learning success rate of the algorithm improved. on the contrary, drl's collision avoidance iteration diagram is rather messy. it shows that the algorithm's self-learning ability was unstable. figure 10 . experimental iteration step scatter plot. figure 11 shows the experimental collision avoidance parallel diagram of the two algorithms. the three parallel axes in the figure, from left to right, were drl avoidances, apf-drl avoidances, and epochs. the heavier the color, the worse the algorithm learning effect and the slower the convergence. for the apf-drl algorithm, the avoidance presents a regular distribution, where as the number of iterations increased, the number of collisions decreased, and the self-learning success rate of the algorithm improved. on the contrary, drl's collision avoidance iteration diagram is rather messy. it shows that the algorithm's self-learning ability was unstable. in summary, by comparing the convergence and decision-making effects of apf-drl and drl algorithms from multiple aspects, this study finds that the performance of the models and algorithms based on an artificial potential field to improve deep reinforcement learning was better."
"the ventricular depolarization of the heart can be represented by three nodes on the heart electrical wave displayed within ecg signal. these are the q, r and s nodes, known as the qrs complex. the amplitude of qrs represents the polarization and depolarization of the ventricular. the qrs duration is the required time for the signal to pass through the ventricular myocardium [cit] . the normality of qrs complex is measured by its duration (time interval). a normal duration of the qrs complex is between 0.08 and 0.10 seconds. an intermediate qrs complex has an interval between 0.10 and 0.12 seconds. while an abnormal qrs time interval is more than 0.12 seconds. important qrs properties include rise level (lr), fall level (l f ), rise duration (t r), and fall duration (t f ). these factors represent the quality of a qrs wave in terms of specifying the ventricular depolarization. the rise and fall levels represent length of edges of r peak on the right and left hand side respectively, the rise and fall durations are the required time to move from the q peak to r peak and from r peak to s peak respectively [cit] ."
"the performance of the var-im method is evaluated by comparing the effectiveness of missing data imputation on qrs wave properties, in both cases of missing data (10% and 20% mcar) and complete data. furthermore, the efficiency of missing data imputation is considered in the filtering processing. figure 2 shows the qrs complex rise level, fall level, rise time and fall time in the case of complete data. in comparison, figures 3-5 show various results with respect to the case of 10% mcar. the four proposed techniques with var-im methods were applied to solve the missing data problem here. clearly, most approaches generated obviously different results: for example, the list wise deletion could not achieve any improvement in all features; it gave results similar to the case of missing data. on the other hand, all the other methods gave acceptable results, for some features such as lr, l f, t r and t f, but the var-im methods still has the highest priority to be the best methods to recover the missing values, which is similar to the real data especially the qrs peaks locations. table. 4 summarizes the results of the effectiveness of missing data imputation of the four methods for the qrs wave properties. when the percentage of the missing data increases from 10% to 20%, the proposed var-im method gives the best results among the five methods. table 5 summarizes the results generated from the recovered data using the five methods, as well as a comparison with that generated from the complete data. as can be noted in both cases of missing data (mcar 10% and 20%) the marss package and em algorithm have similar results. the reason may be that the marss depends mainly on em algorithm in estimating an marss model."
"exact cross-validation requires re-fitting the model with different training sets. approximate leave-one-out cross-validation (loo) can be computed easily using importance sampling (is; [cit] but the resulting estimate is noisy, as the variance of the importance weights can be large or even infinite [cit] . here we propose to use pareto smoothed importance sampling (psis), a new approach that provides a more accurate and reliable estimate by fitting a pareto distribution to the upper tail of the distribution of the importance weights. psis allows us to compute loo using importance weights that would otherwise be unstable."
"3. to guarantee finite variance of the estimate, truncate each vector of weights at s 3/4w i, wher w i is the average of the s smoothed weights corresponding to the distribution holding out data point i. finally, label these truncated weights as w s i ."
"the expectation-maximization (em) algorithm is an iterative algorithm for parameter estimation using maximum likelihood parameter values when the information (e.g. measurements) of some variables are incomplete [cit] . the em algorithm is achieved through two basic steps: estimation step (aka e-step) which replaces missing values by estimated values, and the maximization step (aka m-step) which estimates the parameters. these two steps alternately iterate until convergence [cit] . the conditional expectations of missing data in observed series and estimates of model parameters in the e-step are calculated by:"
"the proposed histogram is based on a window 3 x 3 and not on the intensity of the pixel only. this approach overcomes the disadvantage of the classical histogram which lies in the fact that it ignores the spatial distribution of pixels in the image. iv. histogram and histobin a technique widely used for color is the histogram intersection. with this method, first we must compute the image histogram (histogram of an image can be presented by a vector in which each component is a number of color pixels corresponding to the index). then a histobin will be created from the histogram as follows: each bin (hole) in the histobin is the sum of a few neighbouring elements of the histogram. the number of neighbours is determined by the number of the bin histobin. let n the number of bins for each component, the histobin has 3n bins in total. the number of neighbours is 256 / n. one can see that the histobin is more compact than the histogram. in our system we calculate the histobin for color component in hsv space. having had the histobin image, the distance between two images is the distance between two histobins. this distance can be calcul following formula [cit]"
here we show how to fit the model for the radon example from section 4.6 and carry out psis-loo using the rstanarm and loo packages. after fitting the models we can pass the fitted model objects modela and modelb directly to rstanarm's loo method and it will call the necessary functions from the loo package internally.
"some choices of replication can seem natural for a particular dataset but less so in other comparable settings. for example, the 8 schools data are available only at the school level and so it seems natural to treat the school-level estimates as data. but if the original data had been available, we would surely have defined the likelihood based on the individual students' test scores. it is an awkward feature of predictive error measures that they might be determined based on computational convenience or data availability rather than fundamental features of the problem. to put it another way, we are assessing the fit of the model to the particular data at hand."
the safe encounter distance of the ship is related to the size of the ship (length). a large-sized ship will have a long required safety distance.
"the proposed algorithm for imputing missing data into a multivariate time series dataset is to use a vector autoregressiveimputation (var-im) method combined with an em algorithm together with a prediction error minimization (pem) algorithm. the method based on a combination of these algorithms can significantly improve the imputation performance for dealing with missing data problem. specifically, in the first step, the traditional linear interpolation estimate is made for an initial guess of the missing data. then a var(p) model is estimated by selecting the best lag value p. finally, the parameters of the var(p) model are estimated by alternatively using em and pem algorithms resulting in an improved value for the data imputation. basically, the alternation of the two algorithms between imputing missing data and estimating models, improves the model performance by applying pem algorithm in a way similar to the em algorithm. the flow chart for the proposed var-im algorithm is shown in figure 1 ."
"deep reinforcement learning is a combination of deep learning and reinforcement learning. in this paper, q-learning was combined with a neural network to establish the autonomous navigation decision-making model based on deep q-learning. the deep q-learning algorithm uses an empirical playback algorithm whose basic core idea is to remember the historical information that the algorithm performs in this environment. in practice, the number of environmental states and behavioral states is extremely large and it is necessary to adopt a neural network for generalization. therefore, lstm was selected as the q network. the core concepts of lstm are cell state and \"gate\" structure. cell state is equivalent to the path of information transmission such that information can be transmitted in sequence. this can be thought of as the \"memory\" of the network. lstm network has three control gates: forget gate, input gate, and output gate. the forget gate determines which environmental states and behavioral states from the last cell state to continue to pass through the current cell. the input gate controls whether a new datum could flow into the memory and updates the cell state. the output gate decides which part of the q value to be exported as output [cit] . the three control gates weaken the short-term memory effect and regulate the predicted q value corresponding to multiple autonomous navigation actions in the current state."
"state-space models are models that use state variables to describe a system by a set of differential or difference equations. state variables can be reconstructed from the measured inputoutput data, but the variables themselves are not measured during an experiment. a state-space models can be estimated using in either time and frequency domains. in this paper, the discrete-time state-space model is used to present the multivariate time series data set, having the following structure [cit] :"
"the strategy functions and value functions in the drl were represented by deep neural networks, where the networks were poorly interpretable. this unexplained the security problem, which is unacceptable in unmanned cargo ship transportation. the second intended improvement is to improve the interpretability of the model. 3."
"in the actual voyage, the navigation behavior of the unmanned cargo ship had a complex continuity. in this simulation experiment, only a simple generalization was performed to divide the navigation behavior of the mass into the eight navigation actions. as such, the third intended improvement direction is to increase the ability of the model to predict and \"imagine.\""
"the relationship between the mass and the obstacles can be divided into binary relationships: mass and static obstacles (abbreviated as es), and mass and dynamic obstacles (abbreviated as ed). in the azimuthal relationship, the abbreviations are as follows: ho is the head-on encounter, ot is the overtaking encounter, and cr is the crossing encounter. the mass ontology model relationship attribute is presented in table 1 . the scene ontology model corresponding to the relationship property of the ontology model of mass in table 1 is established. figure 2 shows the ontology conceptual model of the mass navigation scene. combining the colregs, the navigation scenes of are divided into the ho sub-scenario, the cr sub-scenario, the ot subscenario, and the mixed sub-scenarios. however, this paper mainly analyses the scene division between mass and dynamic obstacle. so six scenes is become hasfronted, hasfrontlefted, hasfrontrighted, hasbehinded, hasbehindlefted, hasbehindrighted. , only including cr. in summary, the visual display of the six scenes is shown in figure 3 combining the colregs, the navigation scenes of egoship  staticobstacle and egoship  dynamicobstacle are divided into six scenes: hasfront, hasfrontleft, hasfrontright, hasbehind, hasbehindleft, hasbehindright. then, the scenes corresponding to egoship  dynamicobstacle are divided into the ho sub-scenario, the cr sub-scenario, the ot sub-scenario, and the mixed sub-scenarios. however, this paper mainly analyses the scene division between mass and dynamic obstacle. so six scenes is become hasfronted, hasfrontlefted, hasfrontrighted, hasbehinded, hasbehindlefted, hasbehindrighted. in summary, the visual display of the six scenes is shown in figure 3 ."
"deep reinforcement learning is a combination of deep learning and reinforcement learning. in this paper, q-learning was combined with a neural network to establish the autonomous navigation decision-making model based on deep q-learning. the deep q-learning algorithm uses an empirical playback algorithm whose basic core idea is to remember the historical information that the algorithm performs in this environment. in practice, the number of environmental states and behavioral states is extremely large and it is necessary to adopt a neural network for generalization. therefore, lstm was selected as the q network. the core concepts of lstm are cell state and \"gate\" structure. cell state is equivalent to the path of information transmission such that information can be transmitted in sequence. this can be thought of as the \"memory\" of the network. lstm network has three control gates: forget gate, input gate, and output gate. the forget gate determines which environmental states and behavioral states from the last cell state to continue to pass through the current cell. the input gate controls whether a new datum could flow into the memory and updates the cell state. the output gate decides which part of the q value to be exported as output [cit] . the three control gates weaken the short-term memory effect and regulate the predicted q value corresponding to multiple autonomous navigation actions in the current state."
"the autonomous navigation decision-making system of a mass plays the role of the \"navigation brain.\" the problem to be solved is to determine the best navigation strategy based on environmental information. at present, related works mainly focus on the ship's intelligent collision avoidance algorithms in specific environments."
"throughout the literature, many imputation methods for missing data have been proposed. the methods fall primarily into two broad classifications: traditional and modern techniques. traditional techniques such as simple deletion, averaging, or regression estimation are limited but still used in many cases. on the other hand, modern approaches such as multiple imputation (mi) and maximum likelihood (ml) routines, have proved superior and are have gained favour. in fact modern data imputation algorithms that use these approaches are very prevalent and can be easily administered in standard statistical packages such as statistical package for social sciences (spss) and multivariate autoregressive state-space (marss or even standalone applications such as norm. [cit] . the mi approach first imputes multiple data sets from random samples of the population using techniques such as bootstrapping [cit] or data augmentation [cit] . then, using rubins rules, the results from the imputed data sets are combined [cit] . the ml technique for handling missing data is becoming commonplace in microcomputer packages. specifically, ml algorithms are currently available in many existing software packages (e.g. em algo-rithm) [cit] . when conducted properly, both ml and mi techniques enable researchers to make valid statistical inferences when data are missing at random [cit] . however, these techniques either have limitations or are difficult to carry out for dynamic systems modelling [cit] . for example, many dynamic models involve autoregressive variables and the output is normally a linear or nonlinear combination of a lagged variable. the estimation of autoregressive models requires that the data be fully observed. with the existence of missing values, this is not possible, rendering it impossible to estimate the model. furthermore, these methods often lead to bias in the estimates. in this paper, a new method is proposed for missing data imputation in multivariate time series datasets. the new algorithm utilizes a vector autoregressive model (var) to handle missing data by combining the prediction error minimization (pem) [cit] with an em algorithm. the new algorithm is called a vector autoregressive imputation method (var-im). a description of the algorithm is presented and a case study was accomplished using the var-im. the case study involved electrocardiogram waves that contain multivariate time series data. also the advantages and limitations of the proposed method are analysed. finally a simulation study of the proposed algorithm is compared to traditional and modern imputation methods."
"owing to these differences, the autonomous navigation decision-making system is the core of a mass, and its effectiveness directly determines the safety and reliability of navigation, playing a role similar to the human \"brain.\" during the voyage, the thinking and decision-making process is very complex. after clarifying the destinations that need to be reached and obtaining the global driving route, it is necessary to generate a reasonable, safe, and efficient abstract navigation action (such as acceleration, deceleration, and steering) based on the dynamic environmental situation around the ship. the \"brain\" needs to rapidly and accurately reason based on multi-source heterogeneous information such as the traffic rule knowledge, driving experience knowledge, and chart information stored in its memory unit. in this paper, we only train navigation strategies by learning relative distance and relative position data. we assumed the following perception principles."
"hua-liang wei (bsc, msc, phd) is a senior lecturer in the department of automatic control and systems engineering, university of sheffield. his recent research interests include system identification; data mining and data analytics; narmax methodology and its applications; statistical digital signal processing; machine learning; spatio-temporal system modelling; wavelets and neural networks; non-stationary(time varying) process modelling; forecasting of stochastic and dynamical processes; generalized regression analysis; linear and nonlinear optimisation; multidisciplinary applications in medicine and biomedicine, medical informatics, space weather, environmental, economical and financial systems, and social sciences.."
here we describe how we extract the features from various data sources. these feature extraction methods can serve as a general pipeline for any urban-scale metagenomics study.
"interconnection features (f c ): we first describe how we construct the subway system network. each subway station is denoted as a node and each interaction between two stations is drawn an edge. the weight of edge(i, j) is computed by the minimum number of stops from station i to station j. we also consider the case of express trains and if there exist express trains directly connecting two stations, we assign 1 as the weight to that edge."
"for each label i, let o i be the predicted score of metamlann. given the score from the other model m as o m i, we conduct a linear hybrid prediction for ensemble as follows:"
"the framework of metamlann is shown in fig. 2 . it contains two major components and one model: the blue component for learning and the red component for inference, together with the metamlann model. in the following subsections, we introduce how metamlann is constructed, explain the regularization framework, discuss how feature extraction has been done to train metamlann, and present the ensemble model. our general framework. starting from the map, we simulate the inference task by splitting the samples into the training set (blue dots) and test set (red dots). we use metaphlan2 [cit] to obtain the microbial distribution profiles from the raw sequencing data. we first extract and integrate features for both training and test data. we also construct the evolutionary (phylogenetic) microbial similarity matrix, using the 16s rrna of the bacteria as a regularizer. then, we feed the training data's features and the similarity matrix into metamlann, which will perform microbial inference based on the features of test dataset. our model can also be integrated with other classification models trained with same features as an ensemble model"
"upon obtaining the predicted microbial distribution vector y * i for given location i from the blocks, each vector is regularized by feeding y * i into eq. 5, where  refers to the predicted vector y * i and  i,  j refers to microbe i and microbe j at this location, respectively."
"we assess the performance of our classifier in several ways. while accuracy is the simplest and the most straightforward measure, it is biased toward classes with a larger sample size. instead, we report precision, recall, and f1 score as our evaluation metrics. these metrics are defined as:"
metagenomics studies the genomic content obtained from a human body site or an environment with a goal of understanding microbial diversity. the microorganisms in our environment are greatly associated with human health and disease.
"in addition, each sample is also associated with meta information, including the latitude and longitude showing where the sample was collected, and surface materials. all these information are indispensable for the enrichment of feature generation."
where y i and o i are the ground truth label and the predicted score for sample i. the graph laplacian regularizer can represent any pairwise relationships between parameters. here we discuss how to use the evolutionary similarities as priors and the corresponding laplacian regularizers to incorporate structured domain knowledge. the laplacian matrix l is firstly obtained by constructing the pairwise evolutionary similarity matrix (p) of different microbes.
"as feature extraction is crucial for inferring microbial communities in a complicated urban system with heterogeneous data sources, we first demonstrate the effectiveness of our feature construction. recall that we have three groups of features: subway station features (f s ), interconnection features (f c ), and surface material features (f m ). as shown in table 6, a random forest model is used to compare the performance of individual features and their combinations. overall, the complete features set have the best performance in precision, f1 score, and ranking loss. note that we intentionally choose to use random forest instead of our model, metamlann, to conduct experiments. this is to demonstrate that our feature extraction techniques are beneficial in general to the microbial community inference problem without favoring our model."
"given two genus g a and g b as sets of species, the similarity score between the pair of genus can be computed as: (9) where sp a and sp b are the species of g a and g b, respectively."
"to alleviate the lack of training data, in addition to the regularization, we also propose to construct an ensemble of metamlann with any other model that needs fewer training samples."
"all the models mentioned above either cannot address the complicated environmental conditions or handle the intricate relationships between microbial compositions and the urban environment. in our recent work [cit], we propose metamlann (metagenomic multi label artifical neural network), a neural network based and supervised learning model to predict the microbial community for city-scale metagenomics. metamlann is built on the widely-used feed-forward neural network model. but unlike the conventional feed-forward neural network model that predicts each label individually, it leverages an extra shared structure to capture the dependencies among different labels (microbes). to begin with, we train metamlann using a state-of-the-art network embedding technique to integrate features constructed from different data sources. next, we leverage manifold regularization to extend our model. our model is robust to the sparse samples with limited labeled data by incorporating the domain knowledge. to further improve our model, we also introduce an ensemble model, metamlann+, which can outperform each individual model by leveraging the diversified information from metamlann and different classification models with the strong signal. to our best knowledge, our work is the initial attempt to predict the microbial community for urban metagenomics by using the neural network model. in this paper, we extend our previous work by presenting detailed theoretical foundations and additional statistical analyses."
"profiling city-scale microbial diversity is important for urban long-term disease surveillance and health manage- higher precision, recall, f1 score, and lower ranking loss indicate better performance. bold entries indicate best performance among different methods ment. the great efforts to collect dna samples in densely populated cities still cannot meet the challenge to obtain the metagenomic profiles at fine-grained geo-spatial resolutions. to address this issue, we first define the task of inferring microbial community for city-scale metagenomics as a multi-label classification problem. we then propose metamlann, a neural network based approach to infer microbial communities of unsampled locations given the information from multiple data sources in the urban environment, including subway line information, sampling materials, and microbial compositions in sparsely sampled locations. the model captures the interactions between microbes and the urban environment by a shared hidden layer, and fuses the heterogeneous urban transit information with embedding for feature extraction. additionally, by incorporating signals from other strong models, the ensemble technique metamlann+ further improves the performance of the model. extensive experiments demonstrate the effectiveness of our approach. in this work, we mainly focus on new york and boston subway stations due to the limitation of data availability. in the future, with more cities being sampled, we plan to extend our model to the regional scale to solve the inter-city metagenomic inference problem."
"to capture the underlying microbiota structure, we construct a pairwise similarity matrix to represent the evolutionary relationship between two species. we retrieve the 16s ribosomal rna sequence for bacteria and archaea, 5s ribosomal rna for eukaryotes, and the whole dna sequences for viruses from the ncbi [cit] and the silva [cit] database. we perform sequence alignments to compute the pairwise similarity within each kingdom. we normalize the similarity values to the range of 0 to 1 and we assign 0 to their similarity for cross-kingdom species pairs. finally, we take the mean of all species' similarity scores under that level and aggregate them as the new score for each genus pairs (eq. 9). in this way, we can obtain the similarity matrix between genus level."
"for this work, we extract the following features: subway station information, inter-station connections, and sampling surface materials. all features are concatenated into a feature vector for each sample and are used to train metamlann."
"upon obtaining the station network, we apply the network embedding algorithm node2vec [cit] . each node is embedded into a low dimensional vector based on the generated network."
"to demonstrate the effectiveness of metamlann, we conduct comprehensive experiments by using both the new york and boston datasets. in this section, we will discuss the experiment setup, evaluation metrics, baselines and results."
"from the above equations, the two parameters  i and  i are enforced to be similar, which can be incorporated into the cost function. the regularized cost function is defined as:"
"to address these challenges, we formulate the prediction of microbial communities at unsampled locations as a multi-label classification (mlc) task. based on a set of heterogeneous features extracted from the urban environment, we aim to predict the presence or absence of a list of microbes at a nearby location. for mlc, each location is considered as an instance and each label represents a microbe."
"where t is the number of training epochs; n is the number of training examples;  is the set of parameters in the model. to demonstrate the convergence of the proposed algorithm, we plot the values of the loss function over different optimization epochs in fig. 4 . finally, the heterogeneous neural network model f  :"
"the average performance of each method from these three folds is reported. in addition, we also justify the effectiveness of our feature construction by comparing the performance of individual features and their combination with the same classifier."
"the taxonomy here is referred as the identification, naming, and classification of organisms. we choose to use the evolutionary similarity as the domain knowledge, which is then fed into our regularizer. this is because taxonomy is often informed by the evolutionary relationships among different microbes (i.e., phylogenetic). to incorporate such microbial similarity, many regularization techniques can be used. we choose one of the most popular techniques, graph laplacian regularizer, to build our regularization frameworks [cit] ."
"we use k-fold cross-validation for all experiments. setting the value of k to be three, we randomly and equally split the data into three non-overlapping subsets. each subset has a chance to train the model and to test the model."
"in our microbial community inference case, we extract feature vectors of n samples and represent them as x. the microbe index created from known locations is used as y, where the order of microbes is preserved."
"since different class labels have to be predicted simultaneously [cit], mlc is suitable for solving the microbes inference problem, with their dependencies exploited at the same time. these properties reflect the nature of microbial communities."
"in the field of urban computing, statistical models like regression trees have been applied to do real-time air quality prediction. for example, in u-air [cit], the authors inferred the fine-grained air quality in a city by using a semi-supervised learning approach. the model was able to predict air quality at non-monitored stations based on the air quality data reported by existing monitor stations. the spatial classifier for their model was based on an artificial neural network (ann). however, this model only estimated a single value (i.e. the air quality index) for each location, so it was also inadequate to address the mlc task we formulated."
"as seen in fig. 5, with the level of taxonomy becoming more specific, the performances of all methods decrease due to the increase of complexity. against all competitors, metamlann and metamlann+ idw constantly achieve the highest f1 score and the lowest ranking loss across all taxonomic levels. the advantage of metamlann becomes more obvious with a finer granularity of taxonomic level."
"note that it is possible one location is associated with multiple lines or no lines. for the multiple lines' case, there will be more than one v i equal to 1. for the case of no line, we will simply remove such location since we focus on the inference at stations. therefore, all locations will be associated with a subway line feature as a vector."
"human microbiome studies are already rich enough to uncover the microbial diversity within the human body [cit] . environmental metagenomics, though falling behind in the past years, has also become increasingly important due to the increasing awareness of its impacts on public health, especially in densely populated urban areas transportation system, which described microbial communities across multiple surface types. however, collecting, sequencing, and analyzing the metagenomics data at every station cost them a great amount of money and time. given that, our study focuses on developing a model to automatically predict the microbial communities for unsampled locations."
"for each sample in the new york dataset and samples subjected to shotgun metagenomic sequencing in the boston dataset, we conduct the following preprocessing steps:"
"it is observed that many species are seriously underrepresented (i.e. appearing at only one location) for the abundance at all levels. we choose to focus on the genuslevel abundance to alleviate the issues including underrepresented microbes, missing species-level taxonomy, and very similar microbial species."
"as we formalize the inference problem as a multi-label classification (mlc) problem, we adopt several widely used mlc algorithms as the baseline methods, including inverse distance weighting (idw) interpolation, k nearest neighbor (knn) [cit], support vector machine (svm) [cit], random forest [cit], and neural network [cit] ."
"in the field of metagenomics, several computational models, such as biomico [cit] and nmf [cit] have been developed to infer microbial community structures. to estimate the composition of each sample given the abundance profile, biomico uses the supervised bayesian model while nmf leverages the matrix factorization."
"we use the new york subway station data and the boston subway station data from the mta and mbta website respectively to construct the subway line features. they contain geographic locations, subway station names, and subway lines that pass each station. we also obtain the turnstile data of mta and mbta to count the busyness of all stations. the detailed descriptions can be found in table 2 ."
"to further demonstrate the generality of our model, we compare the performance of metamlann with other aforementioned baselines under different taxonomic levels from phylum to species. we ignore kingdom level due to few numbers of classes."
"for example, there are in total 25 different subway lines in new york, thus we create a binary vector of size 25, each element in the vector indicates whether this line will pass this location or not. for example, for station l, the subway line feature vector is defined as"
"neural networks tend to suffer from limited training examples. however, with only a few instances of each label, it is challenging to train metamlann. one potential solution to compensate for the data sparsity is to incorporate prior knowledge. inspired by the general observation that evolutionary relationships are expected to be associated with patterns of community composition [cit], we presume that the groups of microbes tend to co-occur in the same community when they are closely related to each other in the taxonomy."
"the regularizer can preserve the local geometrical structure of a parameter vector  with length i. according to the definition, we observe that l has the following property that makes it suitable for regularization. given the trace operator tr():"
"we use stochastic gradient descent (sgd) [cit] to efficiently optimize the cost function in eq. 4. we randomly sample a location i and a unit from y i to compute b i for each individual block. we randomly sample a location i and a unit from all the classes among y 1 and y m to capture the global properties shared by all microbes for the shared block b share,. the updating rules for different variables w and b can be derived by taking the derivatives of the above objective function and applying sgd. training our model is efficient with sgd and back-propagation. more specifically, the time complexity of training our"
"it has been shown that the number of riders is positively correlated with the amount of dna collected in a station [cit] . therefore, we also retrieve the public mta data with the turnstiles usage information at each station. the corresponding node vector is then weighted by the average number of riders within dna collection date at each station."
"using the combined features, tables 4 and 5 show the performance of metamlann and other aforementioned baselines on new york and boston datasets, respectively. as discussed in experimental settings, we focus on the genus level inference. we observe that metamlann and metamlann+, outperform all baselines on f1 score and ranking loss."
"second, the formation and transmission of microbial communities are also affected by the material type of surfaces where the samples are collected [cit] . lastly, within each community, the genetic properties of each individual fig. 1 there are three groups of subway stations based on the hierarchical clustering of the microbial community abundance in each location. we set the number of clusters to be three and use the pearson correlation as the distance metric. we observe that the east river is a clear boundary that separates the three districts: manhattan (blue dots), brooklyn (yellow marks), and the roosevelt island (one red dot at top right) microorganisms and the correlation between individual microorganisms also contribute to the complexity. considering the mixed effects from various factors, a simple model for each station along the same subway line should not be enough."
"given the cost function j(; x, y), we seek for a parameter vector  which minimizes it. j(; x, y) measures the difference of given targets y and predictions of the network. here, we choose cross-entropy [cit] as our cost function: (4) where y i and o i are the ground truth and the predicted scores for label i, respectively. the sigmoid activation"
"the correlation value measures the similarity of the tissue compositions between a pair of brain regions. when a patient is affected by mci, the correlation values of a particular brain region with another region will be potentially affected, due possibly to the factors such as tissue atrophy. these correlation changes will be finally captured by classifiers and used for mci prediction. an early work was presented in a conference [cit] . it is worth noting that by computing the pairwise correlation between rois, our approach provides a second order measurement of the roi volumes, in contrast to the conventional approaches that only employ first order volumetric measurement. as higher order measurements, our new features may be more descriptive, but also more sensitive to noise. for instance, the influence of a small roi registration error may be exaggerated by the proposed network features, which may reduce the discrimination power of the features. to overcome this problem, a hierarchy of multiresolution rois is used to increase the robustness of classification. effectively, the correlations are considered at different scales of regions, thus providing different levels of noise suppression and discriminative information, which can be sieved by a feature selection mechanism as discussed below for guiding the classification. additionally, we consider the correlations both within and between different resolution scales. this is because the optimal scale is often not known a priori. we will demonstrate the effectiveness of the proposed approach with empirical evidence. in this study, we consider a fully-connected anatomical network, features extracted from which will form a space with intractably high dimensionality. as a remedy, a supervised dimensionality reduction method is employed to embed the original network features into a new feature space with a much lower dimensionality."
-does integrity verification indeed work in a robust manner ? -what is the impact of the embedded template watermark on the recognition performance using the template for matching only ? -can a combination of template watermark and template result in more robustness in an actual matching process ?
"to be an essentially discriminative network feature, the two associated rois may satisfy one of the two following conditions: n one roi shows significant difference between the mci group and the normal control group, while the other roi is relatively constant with respect to the disease. therefore the correlation between these two rois varies over the two groups in comparison. n both rois change with the disease, but their change speeds are different over two different groups."
"compared with volumetric features, the dimensionality of our proposed network features is even much higher. to address this problem, we propose to use both feature selection and feature embedding to efficiently reduce the feature dimensionality. the reason is two-fold. firstly, feature selection alone may still give rise to many informative features for the classification. for example, suppose that only 10 rois really contribute to the discrimination. the dimension of volumetric features may be maximally reduced to 10 if the feature selection method is effective. however, the number of the corresponding network features that model the pairwise interactions of the 10 rois might be up to 45. this possible number is only computed for the layer l 4 . if considering about the interactions of rois between different hierarchical layers, this number will be further increased. secondly, feature embedding based on the original high dimensional features may not be able to accurately estimate the underlying data structure due to the existence of too many noisy features. also, a large amount of features will greatly burden the computation of embedding."
the adni data were previously collected across 50 research sites. study subjects gave written informed consent at the time of enrollment for imaging and genetic sample collection and completed questionnaires approved by each participating sites institutional review board (irb). more information about the adni investigators is given in acknowledgement.
"in the rest of this section, our proposed classification scheme is explained in detail. firstly, in section ''problem on identifying discriminative features'', we justify the necessity of incorporating both feature selection and feature embedding into the dimensionality reduction module in fig. 5 . then a brief introduction about the partial least square analysis is given in section ''partial least square analysis'', which is the key technique used in our classification scheme. pls integrates the dimensionality reduction process (step 1 * 3 in fig. 5 ) and the classification process (step 4 in fig. 5 ) by considering classification labels when seeking a low dimensional embedding space. it also integrates feature selection (step 2 in fig. 5 ) and feature embedding (step 3 in fig. 5 ) into the same framework to optimize the selection performance. finally, we summarize how pls is used to facilitate the classification in our case step by step in section ''summary of the proposed classification scheme''."
"although t1-weighted mri, as a diagnostic tool, is relatively well studied, it continues to receive the attention of researchers due to its easy access in clinical settings, compared with task-based functional imaging [cit] . commonly used measurements can be categorized into three groups: regional brain volumes [cit], cortical thickness [cit] and hippocampal volume and shape [cit] . volumetric measurements can be further divided into two groups according to feature types: voxel-based features [cit] or region-based features [cit] . in this paper, we focus on regionbased volumetric measurements of the whole brain for the following reasons. firstly, the abnormalities caused by the disease involved in our study are not restricted to the cortex, because, as shown by pathological studies [cit], ad related atrophy begins in the medial temporal lobe (mtl), which includes some subcortical structures such as the hippocampus and the amygdala. secondly, a whole brain analysis not restricted to the hippocampus is preferred, because early-stage ad pathology is not confined to the hippocampus. also affected are the entorhinal cortex, the amygdala, the limbic system, and the neocortical areas. as has been pointed out in several studies [cit], although the analysis of the earliest-affected structures, such as the hippocampus and the entorhinal cortex, can increase the sensitivity of mci prediction, the inclusion of the later-affected temporal neocortex may increase the prediction specificity, and hence improve the overall classification accuracy [cit] . thirdly, we focus on region-based volumetric features because voxel-based features are highly redundant [cit], which may affect their discrimination power."
"please note that, as mci patients are highly heterogeneous, the comparison of the absolute classification accuracy with the existing works in the literature is meaningless. therefore in our study, we evaluate the improvement of our proposed approach over the conventional volumetric features by comparisons on the same data set with the same experiment configuration. furthermore, to investigate the generalization of the proposed method, we conduct experiments repetitively on different random partitions of training and test data sets with different partition ratios. the average classification accuracy estimated in this way tends to be more conservative than the traditional leave-one-out approach. more discussions are given below."
"as a second topic, we investigate iris recognition performance using the template extracted from the watermarked sample (w1) and the extracted template wm (w2), and compare the behavior to the \"original\" results using templates extracted from the original sample data (without embedded wm, w0). for this purpose, we compare roc curves of the three cases with and without attacks (i.e. jpeg compression, noise insertion, and mean filtering) conducted against the sample data."
"there has been a lot of work done during the last years proposing watermarking techniques to enhance biometric systems security in some way (see [cit] for our recent survey on the topic). major application scenarios include biometric watermarking (where biometric templates are embedded as \"message\" as opposed to classical copyright information), sample data replay prevention (by robustly watermarking once acquired sample data), covert biometric data communication (by steganographic techniques), and employing wm is a means of tightly coupled transport of sample data and embedded (template or general purpose authentication) data for multibiometric or two-factor authentication schemes, respectively."
"as first topic, we investigate integrity verification under conditions which require robustness properties. as \"attacks\" against the sample data with embedded wm, we consider mean filtering, noise addition, and jpeg compression. as a first scenario s1 (restricted to the verification scenario), comparison between extracted template wm and database (db) template is covered. we consider the case that 5 different templates are stored in the database out of which a single database template is generated by majority coding like explained before in the case of the template wm. the first thing to note is that even without attack, ber is clearly above zero. for s2 this effect is solely due to the influence the embedded wm has on the extracted template -obviously the wm changes the sample in a way that about 10% of the bits are altered. for s1 the differences are higher which is clear since the db template is constructed from several distinct templates. we have to consider that a typical decision threshold value for the iris recognition system in use is at a ber in [0.3, 0.35]. when taking this into account, the extent of template similarity is of course enough to decide on proven sample integrity. for both s1 and s2, adding noise and applying jpeg compression with quality set to 100 (q100) does not change the ber. when decreasing jpeg quality to 98, ber starts to increase slightly. the situation changes drastically when applying jpeg q95 and mean filtering: ber is up to 0.4 -0.5 which means that integrity cannot be verified successfully. we realize that integrity verification in our technique is indeed robust against moderate jpeg compression and noise. on the other hand, mean filtering and jpeg compression at quality 95% destroys the template wm and indicates modification. the distribution of incorrect bits can be used to differentiate between malicious attacks (where an accumulation of incorrect bits can be observed in certain regions) and significant global distortions like compression where incorrect bits are spread across the entire data. s1 and s2 can be combined into a single integrity verification scheme. the idea is to combine the single templates extracted from the watermark and the template extracted from the watermarked sample into a weighted \"fused template\": in our example, we use 4 copies of the template and the embedded number of templates from the template wm in a majority voting scheme to generate the fused template. table 2 shows the corresponding ber when comparing the fused template to the db template. it can be clearly seen that while the ber without attack and applying moderate attacks is higher as compared to s2, we get much better robustness against jpeg q95 and even mean filtering. with the fusing strategy, robustness even against those two types of attacks can be obtained. of course, the fusion scheme does only make sense in a biometric system in verification mode, since integrity verification is done against templates stored in the template database."
"in step 3, using the features selected in step 2, a new pls model is trained to find an embedding space which best preserves the discrimination of features. the embedding is performed by projecting the feature vectors in the matrix x onto the new weight vectors w~(w 1,w 2,   ,w p ) learned by pls analysis. in other words, the representation of each subject changes from a row in the feature matrix x to a row in the latent matrix t. the feature dimensionality is therefore reduced from d to p (p%d)."
"without requiring any new information in addition to the baseline t1-weighted images, the proposed approach improves the prediction accuracy of mci from 80:83% (of conventional volumetric features) to 84:35% (of hierarchical network features), evaluated by data sets randomly drawn from the adni dataset [cit] . our study shows that this improvement comes from the use of the network features obtained from hierarchical brain networks. to investigate the generalizability of the proposed approach, experiments are conducted repetitively based on different random partitions of training and test data sets with different partition ratios. the average classification accuracy estimated in this way tends to be more conservative than the conventional leave-oneout approach. additionally, although the proposed approach can be easily generalized to incorporate regional similarity measurements other than pearson correlation, the experimental results reinforce the choice of pearson correlation for our application, compared with some commonly used similarity metrics."
"in this paper, we have presented how hierarchical anatomical brain networks based on t1-weighted mri can be used to model brain regional correlation. features extracted from these networks are employed to improve the prediction of mci from the conventional volumetric measures. the experiments show that, without requiring new sources of information, the improvement brought forth by our proposed approach is statistically significant compared with conventional volumetric measurements. both the network features and the hierarchical structure contribute to the improvement. moreover, the selected network features provide us a new perspective of inspecting the discriminative regions of the dementia by revealing the relationship of two rois, which is different from the conventional approaches. the flexibility to generalize our proposed method has been demonstrated by different distance metrics tested in our experiment."
"the overview of the proposed method is illustrated in fig. 1 . each brain image is parcellated in multi-resolution according to hierarchically predefined rois. the local volumes of gm, wm, and csf are then measured within these rois and used to construct an anatomical brain network. each node of the network represents an roi, and each edge represents the correlation of local tissue volumes between two rois. the edge values (the correlations) are concatenated to form the feature vectors for use in classification. this gives rise to a large amount of features. for a robust classification, both feature selection and feature embedding algorithms are used to remove many noisy, irrelevant, and redundant features. only essentially discriminative features are kept to train our classifier that can be well generalized to predict previously unseen subjects. in the following, the description of the proposed method is divided into three parts: hierarchical roi construction (section ''hierarchical roi construction''), feature extraction (section ''feature extraction''), and classification (section ''classification'')."
"with the roi hierarchy defined above, an anatomical brain network can be constructed for each subject, from which informative features are extracted for classification. for each brain network, its nodes correspond to the brain rois, and its undirected edges correspond to the interactions between two rois. there are two types of nodes in our model ( fig. 2-left) : the simple roi in the bottommost layer l 4, and the compound roi in the other layers. similarly, we have two types of edges, each modelling within-layer and between-layer roi interactions, respectively ( fig. 3-right) ."
"partial least square analysis. pls models the relations between the predictive variables (the features x) and the target variables (the labels y) by means of latent variables. it is often compared to pca that only models the eigenstructure of x without considering the relationship between x and y. pls maximizes the covariance of the projections of x and y to latent structures, as well as the individual variance of x and y. this method has advantages on data set where the size of the samples is much smaller than the size of the features."
"when comparing this approach to previous techniques proposed in literature, we notice the following differences / advantages: as opposed to techniques employing robust template embedding watermarking (e.g. as proposed for enabling tightly coupled transport of sample and template data of different modalities), the proposed scheme can ensure sample data integrity. the importance of this property has been recently demonstrated [cit] in an attack against robust embedding schemes used in the multibiometric and two-factor authentication scenarios. as opposed to techniques employing arbitrary (semi-)fragile watermarks for integrity protection (instead of the template watermark used here), the template watermark data can be used to provide a more robust matching process after data integrity has been assured."
"n to demonstrate the benefit purely from the hierarchy, we compare the classification performance of the single-layer network features in method ii and the four-layer network features in method i. the advantage of the four-layer structure is statistically significant over the single-layer. moreover, the result that method i statistically outperforms method iii indicates the necessity of using the cross-layer edges in the network."
"in our study, we conduct two kinds of comparisons, that is, to compare the discrimination power of the network and the volumetric features, and to compare the performance of different classifiers for the network features. the discussion of the classification results are given at the end of this section."
"the employed iris recognition system is libor masek's matlab implementation 3 of a 1-d version of the daugman iris recognition algorithm. first, this algorithm segments the eye image into the iris and the remainder of the image. iris image texture is mapped to polar coordinates resulting in a rectangular patch which is denoted \"polar image\". for feature extraction, a row-wise convolution with a complex log-gabor filter is performed on the polar image pixels. the phase angle of the resulting complex value for each pixel is discretized into 2 bits. the 2 bit of phase information are used to generate a binary code. after extracting the features of the iris, considering translation, rotations, and disturbed regions in the iris (a noise mask is generated), the algorithm outputs the similarity score by giving the hamming distance between two extracted templates."
"the euclidean distance and the l1-norm distance measure the linear relationship between a pair of roi nodes. no parameter needs to be set. the kernel based distance provides a non-linear measurement of roi feature similarity. the parameter s is set, by cross-validation, to be 0:2 times the average euclidean distance between roi pairs. based on the 20 random training and test partitions as in section ''comparison of features'', the average classification accuracies are reported in table 6 . for comparison, the accuracies of our network approach using pearson correlation, and the conventional volumetric approach are also repeated in the table. in addition, the test accuracies over different numbers of training samples for different metrics are plotted in fig. 10 . it can be seen that, pearson correlation yields the best performance, followed by the kernel based distance. these two distances give significant improvement over the conventional volumetric approach, whereas the euclidean and the l1-norm distances do not. the importance of the choice of the metric is quite visible: only when a proper metric is selected, the network construction may bring useful information compared with the conventional volumetric approach."
"where d is the number of features, p is the number of the latent vectors as defined above, w jk is the j-th element in the vector w k, and r k is the regression weight for the k-th latent variable, that is, r k~u t k t k . about 60*80 features with the top vip scores are selected for feature embedding in the next step."
"alzheimer's disease (ad) is a progressive and eventually fatal disease of the brain, characterized by memory failure and degeneration of other cognitive functions. pathology may begin long before the patient experiences any symptom and often lead to structural changes of brain anatomies. with the aid of medical imaging techniques, it is now possible to study in vivo the relationship between brain structural changes and the mental disorder, providing a diagnosis tool for early detection of ad. current studies focus on mci (mild cognitive impairment), a transitional state between normal aging and ad. these subjects suffer from memory impairment that is greater than expected for their age, but retain general cognitive functions to maintain daily living. identifying mci subjects is important, especially for those that will eventually convert to ad (referred to as progressive-mci, or in short p-mci), because they may benefit from therapies that could slow down the disease progression."
"number of rois matrices of correlation matrices computed above. moreover, before computing the correlation of the volumetric features f i and f j, we employ a normalization step by subtracting f f from f i, where f f is the mean volume (in gm, wm, and csf) of different rois belonging to the same subject. by centerizing features in this way, we can obtain a better classification accuracy."
"as aforementioned, our network features differ from the conventional volumetric features in two aspects: i) the network features model the regional interactions; ii) the network features are obtained from a four-layer hierarchy of brain atlases. the contributions of these two aspects are investigated separately. to test the advantages of using regional interactions over local volumes, we compare the network and the volumetric features on the same hierarchical structure (either single-layer or four-layer). to test the advantages of using the hierarchical network structure, we compare network features obtained from different layers (the bottommost layer and all four layers) in the hierarchy. moreover, we compare the networks with and without the cross-layer connections to further explore the function of the hierarchial structure. in summary, five methods are tested in the experiment: n method i is the proposed method in this paper, using the fourlayer hierarchical network features. the results are summarized in table 3 . the classification accuracy in table 3 is averaged across the 20 randomly partitioned training and test groups. a paired t-test is conducted between method i and the other four methods respectively, to demonstrate the advantage of our proposed method. the t-value and the corresponding p-value of the paired t-test are also reported. it can be seen from table 3 that method i is always statistically better (the significance level 0:05) than any of the other four methods. in addition to comparing the average accuracies in table 3, the classification accuracies are also compared on each of the 20 training-test groups between the four-layer network features (method i) and the conventional volume features (method iv) in fig. 6, and between the four-layer network features (method i) and the single-layer network features (method ii) in fig. 7 ."
"and the between-layer interaction is computed as for a given number of training samples, the classification accuracy is averaged over 20 training/test groups randomly partitioned from our data set using this number of training samples. the scheme configurations are shown in table 4 . doi:10.1371/journal.pone.0021935.g009 table 5 . selected discriminative features. where d 4 is a general metric that measures the relationship between two rois in the bottommost layer l 4 . the definitions of other symbols remain the same. if pearson correlation is used, these two equations become identical to (1) and (2) . it can be seen that, for a different metric, the hierarchy can be left intact and only the regional interactions in the bottommost layer need to be recomputed."
"the selected features are different for the twenty randomly partitioned training and test groups used in section ''comparison of features''. table 5 shows the most discriminative features selected by more than half of the training and test groups. it can be clearly seen that hippocampus remains the most discriminative roi in differentiating the normal controls and mci patients. table 5 is separated into two parts. on the upper portion of the table, the two rois of a network feature may be both associated with the mci diagnosis, such as hippocampus, entorhinal cortex, uncus, fornix, globus palladus, cingulate etc, as reported in the literature [cit] . a typical example is the correlation between hippocampus and ventricle. it is known that the enlargement of ventricle is a biomarker for the diagnosis of the ad [cit] . however, different from the hippocampus volume loss that often occurs at the very early stage of the dementia, the ventricle enlargement often appears in the middle and late stages. therefore, the progression pattern of disease in these two regions is different. their correlation is thus selected as the discriminative feature. on the lower portion of the table, the first roi is associated with the disease, while the second roi is not. for example, it has been reported that the anterior and posterior limbs of internal capsule and the occipital lobe white matter are not significantly different between mci patients and normal controls in a dti study [cit] ."
"it is worth noting that the influence of different ratios of training-test partitions on the classification result is often ignored in many existing works. one possible reason is that a leave-oneout validation is used when the size of the data is small. this often leads to the use of more than 90% data for training, which tends to produce a more optimistic result compared with using other lower ratios of training data."
"in this work we consider the application scenario where the aim of wm is to ensure the integrity and authenticity of the sample data acquisition and transmission process. during data acquisition, the sensor (i.e. camera) embeds a watermark into the acquired sample image before transmitting it to the feature extraction module. the feature extraction module only proceeds with its tasks if the wm can be extracted correctly (which means that (a) the data has not been tampered with and (b) the origin of the data is the correct sensor)."
"the determination of the region of interest (roi) is the key for region-based analysis methods. once rois have been determined either by pre-definition [cit] or by adaptive parcellation [cit], the mean tissue densities of gray matter (gm), white matter (wm) and cerebrospinal fluid (csf) in each roi are usually used as features for classification. disease-induced brain structural changes may occur not at isolated spots, but in several inter-related regions. therefore, for a more accurate characterization of the pathology, feature correlation between rois has to be taken into account. measurement of such correlations may provide potential biomarkers associated with the pathology, and hence is of great research interest. however, for most existing approaches, the dependencies among features are not explicitly modelled in the feature extraction procedure, but only implicitly considered by some classifiers, such as the support vector machines (svms), during the classification process. for example, a linear svm classifier models the dependency (inner product) of feature vectors between two subjects, instead of the interaction of two rois (via volumetric features) of a specific subject. these implicitly encoded feature dependencies become more difficult to interpret when a nonlinear svm classifier is used. based on this observation, we propose in this paper a new type of features derived from regional volumetric measurements, by taking into account the pairwise roi interactions within a subject directly. to achieve this, each roi is first characterized by a vector that consists of the volumetric ratios of gm, wm and csf in this roi. then, the interaction between two rois within the same subject is computed as pearson correlation of the corresponding volumetric elements. this gives us an anatomical brain network, with each node denoting an roi and each edge characterizing the pairwise connection."
"before introducing our proposed approach, it is worth highlighting the advantages of the hierarchical brain networkbased approach over the conventional volume-based approaches. firstly, as mentioned above, our proposed method utilizes a second-order volumetric measurement that is more descriptive than the conventional first-order volumetric measurement. secondly, compared with the conventional volumetric measurements that only consider local volume changes, our proposed hierarchical brain network considers global information by pairing rois that may be spatially far away. thirdly, our proposed method seamlessly incorporates both local volume features and global network features for the classification by introducing a whole-brain roi at the top of the hierarchy. by correlating with the whole-brain roi, each roi can provide a first order measurement of local volume. fourthly, although our current approach uses pearson correlation, it can be easily generalized to any other metrics that are capable of measuring the similarity between features of roi pairs. fifthly, the proposed method involves only linear methods, leading to easy interpretations of the classification results. finally, for the first time, we investigate the relative speeds of disease progression in different regions, providing a different pathological perspective complementary to spatial atrophy patterns."
"n to demonstrate the benefit purely from using the regional interactions, the same atlases in the hierarchy are applied to volumetric features as in method v. it can be seen from table 3 that the hierarchical structure does not improve the discrimination of the single-layer volumetric features in method iv. moreover, the benefit of using regional interactions can also be shown by the better result of the single-layer network features in method ii than the single-layer volumetric features in method iv."
"in the context of biometrics, we propose to embed template data as semi-fragile wm information instead of general purpose image descriptors as used in classical semi-fragile wm schemes [cit] . this is sensible since on the one hand template data are of course image dependent data and therefore are able to prevent wm copy attacks or similar. on the other hand, in case of tampering or other significant data manipulations, the aim is not to reconstruct the sample data at first hand, but to be able to generate template data from the sample data required for matching. so data for reconstructing the sample data is suggested to be replaced by data for directly generating template data. in the following, we describe the wm embedding and extraction processes:"
n our proposed hierarchical network features in method i outperform the conventional volumetric features in method iv. the advantage may come from using both regional interactions and the hierarchical structure.
"as the baseline system, we employ the fragile watermarking scheme as developed by yeung et. al and investigated in the context of fingerprint recognition [cit] . for this algorithm, the watermark embedded is binary and padded to the size of the host image. subsequently, the wm is embedded into each pixel according to some key information. as a consequence, the wm capacity is 89600, 76800, and 30000 bits for casiav3, mmu, and ubiris, respectively."
"the advantages of pls for our network features over some commonly used unsupervised and supervised nonlinear methods, such as laplacian eigenmap embedding and kernel fisher discriminant analysis (kfda), have been evidently shown in our experiment in section ''comparison of classifiers''."
"the classification performance of our proposed classification scheme is compared with other six possible schemes shown in table 4 . to simplify the description, our proposed scheme is denoted as p1, while the other six schemes in comparison are denoted as p2*p7. to keep consistent with p1, each of the six schemes p2*p7 is also divided into four steps: rough feature selection, refined feature selection, feature embedding and classification, corresponding to step 1*step 4 in p1. please note that the first step, rough feature selection, is the same for all schemes p1*p7. in this step, the discriminative features are selected by their correlations with respect to the classification labels. from the second step onwards, different schemes utilize different configurations of strategies, as shown in the second column of table 4 ."
"in this paper we focus on protecting the transmission of sample data from the sensor to the feature extraction module employing a specific semi-fragile watermarking technique. in particular, we propose to embed biometric template data instead of general purpose watermark information which can then be used in the matching process in addition to checking integrity. in section 2, we introduce the template-embedding based semi-fragile watermarking approach and discuss its properties. section 3 presents experiments where the proposed concept is evaluated in the context of iris recognition using a variant of a well known watermark embedding scheme. section 4 concludes the paper."
"in our network design, each edge represents the correlation or the ''similarity'' between a pair of roi nodes. pearson correlation is just one of the possible similarity measurements. by viewing pearson correlation as an inverse distance, it is straightforward to include other commonly used distance metrics, e.g., the euclidean distance, the l1-norm distance, and the kernel based distance, for measuring the feature similarity between roi pairs. by virtue of separating the computation of the hierarchy and the regional interactions, our proposed method can be easily generalized to other metrics with merely a slight revision of (1) and (2) as follows. the within-layer interaction is computed as"
"in step 1, the discriminative power of a feature is measured by its relevance to classification. the relevance is computed by pearson correlation between each original feature and the classification label. the larger the absolute value of the correlation, the more discriminative the feature. features with correlation values lower than a threshold are filtered out."
"in short, either feature selection or feature embedding alone may not be sufficient to identify the discriminative network features with respect to classification. therefore, a dimensionality reduction process is proposed, which couples feature selection and feature embedding via partial least square (pls) analysis [cit] . as a supervised learning method, pls considers about the information in the classification labels and thus achieves a better discrimination than many of the commonly used unsupervised methods, for example, principal components analysis (pca) and the laplacian eigenmap. as the key technique used in our classification scheme, a brief introduction about pls is given to make our paper self-contained."
"in step 4, after pls embedding, a small number of features in the new space are able to capture the majority of the class discrimination. this greatly reduces the complexity of relationships between data. therefore, these features are used to train a linear svm for predicting mci patients and normal controls. in our case, a linear svm can achieve better or at least comparable classification accuracies as a non-linear svm."
"1. from the acquired sample data, a template is extracted. 2. the template is embedded into the sample data employing a semi-fragile embedding technique (this template is referred to as \"template watermark\" subsequently). 3. the data is sent to the feature extraction and matching module. 4. at the feature extraction module, the template watermark template is extracted, and is compared to the template extracted from the sample (denoted simply as \"template\" in the following). in this way, the integrity of the transmitted sample data is ensured when there is sufficient correspondence between the two templates. in case of a biometric system operating in verification mode the template watermark can also be compared to the template in the database corresponding to the claimed identity (denoted \"database template\" in the following). 5. finally, in case the integrity of the data has been proven, the watermark template and the template are used in the matching process, granting access if the similarity to the database template(s) is high enough."
"note that each network feature characterizes the relationship between two rois, instead of an individual roi as in the conventional approaches. therefore, for the first time, we study the relative progression speed of the disease in different rois of the same subject, which eliminates the impact of personal variations. on the contrary, the conventional methods study the absolute progression speeds of rois among different subjects. normalizing subjects by the whole brain volume in conventional methods may not completely remove the personal variations."
") is the kronecker product of the i-th row in m l 1 and the j-th row in m l 2 . now, let us consider the correlation matrix for two layers l l and l 4 . it can be simply computed as:"
"the situation changes when the attacks get more severe. as shown in figs. 1.c and 2.c, under jpeg compression with q95 w2 is the worst option now since the robustness of the wm is not sufficient any more. while for the casiav3 data w0 and w1 are close (so the impact of the wm is negligible), for ubiris the impact of the wm is quite significant (which can be explained by the fact that the ubiris data is of already quite low quality without any further degradation, the additional wm complicates template extraction). for mean filtering the result for w2 is even worse as shown in figs. 3.a and 3.b, no recognition can be performed at all with the extracted template wm after this attack. finally, the strategy of combining w1 and w2 into a fused template for integrity verification (results given in table 2 ) can also be applied for matching. fig. 3 shows examples where the roc behavior of w2 can be significantly improved by using this approach. in particular, in the case of mean filtering the fused template can be used for recognition purposes as shown in figs. 3.a and 3.b."
"since this technique is a fragile wm scheme, no robustness against any image manipulations can be expected of course. however, the usually smaller size of biometric templates can be exploited to embed the template in redundant manner, i.e. we embed the template several times. after the extraction process, all template watermarks are used in a majority voting scheme which constructs a \"master\" template watermark. we expect to result in higher robustness as compared to the original algorithm due to redundant embedding leading to an overall quasi semi-fragile wm scheme for the watermark templates. in our implementation, the iris code consists of 9600 bits, therefore, we can embed 9, 8, and 3 templates into images from the casiav3, mmu, and ubiris databases, respectively."
"firstly, we compare the efficacy of different features with respect to classification. the data set is randomly partitioned into 20 training and test groups with 75 samples for training and 75 samples for test. for a fair comparison, our proposed classification process is applied similarly to both the volumetric and the network features."
"it is noticed that different ratios of training and test partitions may lead to a variation in the classification accuracy. to reflect the influence of this factor, we test seven different numbers of training samples, occupying 50% to 80% of the total data size. for each number of training samples, 20 training and test groups are randomly generated and the average classification accuracy is summarized in fig. 8 . when 150 training samples are used, the test accuracy in fig. 8 corresponds to the classification accuracy of 85:07% obtained by method i in table 3 . in general, the classification accuracy goes up slightly when the number of the training samples increases. this is not surprising because the larger the number of training samples, the more the learned information. it can be seen that the network features show a consistent improvement in classification accuracy of approximately 3% in all cases, compared to those by using the conventional volumetric features. averaged across different numbers of training samples, the classification accuracy becomes 84:35% for the network features, and 80:83% for the volumetric features, which represents an overall classification performance of these two different features. a paired t-test is performed on the seven different ratios of training-test partitions using both features. the obtained pvalue of 0:000024 indicates that the improvement of the network features over the volumetric features is statistically significant."
"the brain network may be quite complicated. for instance, fig. 2 (b) partially shows the network connections between rois in the layers of l 2, l 3 and l 4, respectively. to determine informative features from the network, the computation of roi interactions is initially conducted on the bottommost layer l 4, and then propagated to other layers effectively via a membership matrix that indicates the relationship of rois from different layers. the process is detailed as follows."
"since a hierarchical fully-connected brain network is used in our study, the dimensionality of the network features is very high: originally more than 10,000 features for each subject. to address this issue, in this paper, we propose a classification scheme to efficiently learn discriminative information from this large amount of network features. the scheme involves feature dimensionality reduction and classification. the overview of the whole process is given in fig. 5 . as shown, we use both a two-step feature selection (step 1 and step 2 in fig. 5 ) and a feature embedding (step 3 in fig. 5 ) algorithms to efficiently reduce the dimensionality of features. this gives rise to a small number of discriminative features that can be well separated by a linear classifier. in particular, the features of the training subjects are first selected according to their relevance with respect to the clinic labels. this step reduces the original more than 10,000 features to about 200*300 features. then in the second step, about 60*80 features are further selected based on their predictive power in a partial least square (pls) model [cit] . after the two-step feature selection, another pls model is trained to embed the selected 60*80 features into a low dimensional space that maintains their discriminative power. after feature selection and feature embedding, each subject is represented by only 4 to 5 features. these features are fed into a linear svm classifier for differentiating mci patients and normal controls (step 4 in fig. 5 )."
"the classification results are summarized in fig. 9 and table 4 . please note that the classification accuracy at each number of training samples in fig. 9 is an average over 20 random training and test partitions as mentioned in section ''comparison of features''. also, the overall classification accuracy in table 4 is an average of accuracies at different numbers of training samples in fig. 9 . the best overall classification accuracy of 84:35% is obtained by our proposed scheme p1: vip selection + pls embedding + a linear svm. this is slightly better than p2, where a nonlinear svm is used. it can be seen that the classification schemes with pls embedding (p1*p4) achieve an overall accuracy above 84%, better than those without pls embedding (p5*p7). the supervised embedding methods, i.e., pls (p1*p4) and kfda (p7), perform better than the unsupervised laplacian eigenmap embedding (p6). moreover, pls embedding (p1*p4) preserves more discrimination than the nonlinear supervised embedding of kfda (p7). although the proposed scheme p1 achieves the best classification performance, the difference between p1*p4 is not significant. this may indicate that the discriminative dimensionality reduction by pls embedding plays a more important role than the classifier type in improving classification performance. after pls embedding, the data complexity is greatly reduced and the intrinsic relationship underlying the data becomes more evident, therefore allowing even simple classifiers to achieve performance comparable to more sophisticated classifiers. although the difference between p1*p4 is not significant, p1 is still preferred over p2 and p4 because the linear svm employed in p1 is much faster than the nonlinear svm employed in p2 and p4. p1 is also preferred over p3, because the vip selection employed in p1, while yielding improvement over p3, does not increase the computational cost substantially."
"in this study, 125 normal control subjects and 100 p-mci subjects are taken from the adni dataset. each subject is rescanned and re-evaluated every six months for up to 36 months. the p-mci subjects are those who developed probable ad after the baseline scanning. the diagnosis of ad is made according to the nincds/adrda criteria [cit] for probable ad. the demographic and clinical information of all the selected subjects are summarized in table 1 ."
"the msra dataset [cit] contains 76k frames from nine different subjects with 17 different gestures. this dataset was captured with intels creative interactive gesture camera [cit] and has 21 annotated joints, including 1 palm joint and four joints for each finger as shown in figure 4(a) . following the most commonly used protocol [cit], we used a leave-one-subject-out cross-validation strategy for evaluation on this dataset."
"like the finger branches, the hand-part-specific feature f 0 for the palm branch is first extracted. as mentioned at the beginning of this section, the palm is inflexible and more stable than fingers. thus, the palm joint j 0 is directly estimated by applying a series of fc layers to the palm feature f 0 ."
"rnns learn a hidden representation for each time step of sequential data by considering both the current and previous information. thanks to their ability to memorize and abstract the sequential information over time, rnn has achieved great success in sequential data modelings such as natural language processing (nlp) and speech recognition. formally, the hidden state and the output feature at the current time step, t, can be respectively obtained by"
"to design a practical architecture for 3d hand pose estimation, we considered the articulated structure of the hand and proposed an efficient regression network, namely termed hcrnn. the proposed hcrnn has a hierarchical architecture, where six separate branches are trained to estimate the position of each local part of the hand: the palm and five fingers. in each finger branch, we adopted an rnn to model spatial dependencies between the sequential-joints of the finger. in addition, hcrnn is built on a 2d cnn that directly takes 2d depth images as inputs, making it more efficient than 3d cnn-based methods. the experimental results showed that the proposed hcrnn not only achieves competitive performance compared with state-ofthe-art methods but also has a highly efficient running speed of 240 fps on a single gpu."
"the inference speed is also an important factor for the practical application of 3d hand pose estimation. table 3 compares the inference speed of conventional and the proposed methods on a single gpu. while top-ranked methods using 3d inputs have a higher inference time owing to the time-consuming 3d convolution operation or data conversion procedure, our method has a faster inference speed owing to its efficiency of 2d cnn-based architecture. the proposed hcrnn is ranked in 2nd place among the compared methods behind feedback. based on the aforementioned results, it can be seen that our proposed hcrnn not only achieves competitive performance compared with state-of-the-art methods but also is very efficient, having a high frame rate, which shows the applicability to real-time applications."
"3.1. overall network architecture figure 2 illustrates the overall architecture of the proposed 3d hand pose estimation methods. the proposed network mainly consists of two parts: an encoder that transforms an input depth image onto the abstracted feature space; a joint regression sub-networks (subnets) that are composed of six branches corresponding to five fingers and a palm. the input depth image is firstly fed into an encoder for low-level feature extraction. then, the regression sub-nets take the obtained feature map from the encoder as an input and predict the 3d pose of a palm and fingers."
"where w h, u h, and b h are the parameters for the hidden state and w y and b y are the parameters for the current output. this recurrent structure allows the rnn to convey the information in the past time steps to the current prediction process. however, as the time gap between information grows, the basic rnn cannot preserve temporal memories and faces the problem of long-term dependencies due to the vanishing gradient [cit] ."
"with in-plane rotations, but we did not use them because we applied online data augmentation during training, as described in section 3.5. this dataset has 16 annotated joints, including 1 palm joint and 3 joints for each finger, as shown in figure 4 (c)."
"the icvl dataset [cit] was captured with an intel realsense camera. in this dataset, there are 22k frames from 10 different subjects for training and 1.5k images for testing. the training set includes an additional 300k augmented frames table 2 : comparison of the proposed method with state-ofthe-art methods on three 3d hand pose datasets. mean error indicates the average 3d distance error."
"the nyu dataset [cit] was captured with three microsoft kinects. it contains 72k training and 8k testing images from three different views. the training set was collected from one subject, while the testing set was collected from two subjects. according to the evaluation protocol that most previous works follow, we used only a frontal view and a subset of 14 annotated joints which is depicted in figure 4(b) for both training and testing."
"test speed (fps) input v2v-posenet [cit] 3.5 3d point-to-point [cit] 41.8 3d handpointnet [cit] 48 3d 3d densenet [cit] 126 3d 3d cnn [cit] 215 3d deepprior++ [cit] 30 2d generalized [cit] 40 2d crossingnets [cit] 90.9 2d crossinfonet [cit] 124.5 2d feedback [cit] 400 2d hcrnn (ours) 240 2d table 3 : comparison of inference speed with state-of-the art methods. the inference speed is measured with a single gpu. ure 7. as can be seen, our method outperforms state-ofthe-art methods with 2d inputs on all the three datasets. as compared with the methods using 3d inputs, our method performs better than 3dcnn [cit], shpr-net [cit], hand-pointnet [cit], and 3d densene [cit] and achieves comparable performance with point-to-point [cit] on the icvl and msra datasets. on the nyu dataset, the results of the proposed method are worse than those of v2v-posenet [cit] but are better in terms of percentage of success frame rates when the error threshold is larger than 30mm. on the msra dataset, following the evaluation protocol of prior works [cit], we also measured the mean joint error over various viewpoint angles. as shown in figure 8, our method can obtain promising results from large yaw and pitch angles, which demonstrates the robustness of our pro-posed method to viewpoint changes, which is a challenging problem in hand pose estimation. the qualitative results of our method on three datasets are shown in figure 9 . it can be seen that our method can accurately estimate 3d hand joint locations on the three datasets."
and j (n) k and j (n) k are the ground truth and estimated 3d coordinates of nth joint from the kth branch. the smooth 1 loss is proven to be more robust to outliers than the 2 loss and is widely used in regression problems such as detection and classification tasks [cit] .
"accurate 3d hand pose estimation has received considerable attention regarding a wide range of applications, such as virtual/augmented reality and human-computer interaction [cit] . as commercial depth cameras have been released and become more common, depth-based hand pose estimation methods have attracted significant research interest in recent decades [cit] . nevertheless, it is still a challenging problem to accurately estimate 3d hand pose, because of the low quality of depth images, large variations in hand orientations, high joint flexibility, and severe self-occlusion."
"we also evaluated the effect of the rnn-based regression on finger joint estimation. we designed another network architecture, where the rnn-based regression block is replaced with two fc layers, as shown in figure 5(b) . the fc-layer-based network directly regresses all joints of the finger from the input finger features rather than sequentially estimating finger joints along the kinematic chain. as shown in figure 6, the proposed rnn-based network architecture achieves a better result than the fc-layer-based network with direct regression and reduces the mean 3d distance error (mm) by 0.37 (from 6.95 to 6.58). these experiments confirm that the proposed rnn-based sequential regression can effectively estimate the spatially related finger joints along the kinematic chain."
"to evaluate the performance of the different 3d hand pose estimation methods, we used two metrics. the first metric is the average 3d distance error between the ground truth and predicted 3d position for each joint. the second one is the percentage of succeeded frames whose errors for all joints are within a threshold."
"we compared the proposed network on three public 3d hand pose datasets with the most recently proposed methods using 2d depth maps as an input, including disco [cit], deepprior [cit], its improved version deep-prior++ [cit], feedback [cit], multi-view cnns [cit], ren-4x6x6 [cit], ren-9x6x6 [cit], pose-ren [cit], generalized [cit], global2local [cit], crossingnets [cit], hbe [cit], and crossinfonet [cit], as well as methods using 3d inputs, including 3d cnn [cit], shpr-net [cit], 3d densenet [cit], handpointnet [cit], point-to-point [cit], and v2v-posenet [cit] . the average 3d distance error per joint and percentage of success frames over different error thresholds are respectively shown in table 2 and fig-figure 9 : qualitative results on the three public datasets. left: icvl dataset. center: nyu dataset. right: msra dataset. the ground truth is shown as red lines, and the prediction is shown as green lines."
sub-network (subnet) of (8) by using a recurrent model in which the hidden layer containing the information of previous links controls the current estimation as follows:
"although recent state-of-the-art 3d hand pose estimation methods mostly adopt discriminative learning-based methods as deep learning technology advances, model-based methods still have their own advantages [cit] . as considering the joint connection over the finger, the movements of different joints of a finger are highly related to each other. with a finger root, i.e. metacarpophalangeal (mcp), as the base position, each finger is composed of sequentialjoints, which can be represented as a kinematic chain. using the kinematic structure of a hand, the model-based methods can constrain the solution space of the hand joint positions. to take the advantages of both the model-and discriminative-learning-based methods, we estimate a nth joint of the kth finger, j (n) k, by using the feature sequence for the previous links as follows:"
flat routing protocols in ad hoc networks adopt a flat addressing scheme which means all nodes participating in the routing process play an equal role. flat routing protocols may generally be classified into two main categories.
"suppose that, for a communication between a source and a destination node, there are routes and the number of nodes in the th route, called route, is (as shown in figure 5 ). in e2aodvv2, each th node in route, has a battery power (bp ) level quantified as 16 different values (from 0 to 15)."
"(ii) concentrated traffic with overloaded nodes: traffic destinations will be predefined (all traffic in the network goes toward those sink nodes). this variant is shown by \"concentrated\" in table 6 ."
mapreduce is a programming model for expressing distributed computations on huge amounts of data and an execution framework for large-scale data processing on clusters of product servers. it was initially developed by google and built on well-known principles in parallel and distributed processing.
"the metric for energy consumption balancing in a network consisting of nodes is as discussed in section 5.2.1. as a sample, the value of when network consists of 12 nodes (i.e., 12 ) is marked in figure 10 per different protocols for the distributed traffic mode and similarly in figure 11 for the concentrated traffic mode. also, table 6 summarizes these sample values for 12 . furthermore, the value of decreases as the network size increases, demonstrating the scalability of the protocols as shown in figures 10 and 11 for distributed and concentrated traffic modes, respectively."
"the first experiment that we describe is done on the popular victoria-park dataset. it was acquired with a car equipped with a laser range finder and odometer. point landmarks are the trees in the park and are observed through laser scans. a feature extraction algorithm reports the x  y location of the detected trees, in the laser frame. we will refer to this sensor model as the \"cartesian\" dataset. from this dataset, we constructed a bearing only dataset, where we replaced the cartesian observations of a landmark with the corresponding bearing measurement. as shown in figure 4a, the noise in the odometry is relatively high."
"usually has a smoother behavior than the vector differenc z k  z k . in eq. 4 the difference is first computed in the original non-euclidean domain, and then it is converted to a vector form. this difference is supposed to be small, thus its minimal form is far from singularities. conversely, the vector difference first computes the minimal representation of values potentially far from the origin (and thus close to the singularities), and then computes the difference. this can result in high values of the error norm that come from little displacements in the orientation."
"during this exercise the students become familiar with ecg and ppg signals and ptt. ecg and ppg are simultaneously sensed (like in fig 1.), amplified, pre-filtered and modulated. modulated signal, fig. 5 (up), is acquired by sound card in rate 44.1ksamples/sec and filtered in parallel branches by ideal band bass filters, 2khz30hz and 4khz30hz, in order to separate carriers with ppg and ecg. thereafter, the separated carriers are passed through demodulator block based on hilbert transformation. the demodulated signals consists noise and should be re-filtered by low pass filters with border frequencies of 10hz (ppg) and 30hz (ecg). the filtered ecg and ppg signals are plotted together or separately, fig. 5 (down)."
"it is undisputed that optical flow estimation is one of focus areas in the research of computer vision, image and video processing. after remarkable work by horn and schunck [cit] and lucas and kanade [cit], the accuracy and robustness of flow filed computation have improved significantly via numerous publications in the last three decades. recently, various techniques of optical flow estimation have found broad application in human posture recognition and tracking [cit], video object segmentation and foreground prediction [cit], online video stream abstraction [cit], wireless video communications [cit], image and video aesthetic quality assessment [cit], and many other fields."
"in a large class of problems, e.g., slam or sfm, the variables in the factor graph represent spatial entities that are either poses of the moving sensor (laser range finders, cameras, etc) or positions of the observed entities (landmarks, scans, local maps and so on). the factors correspond to the measurements that depend on either single agent positions (like gps, magnetic field, or attitude), temporally subsequent agent positions (like odometry, velocity, or acceleration), or they arise by the observation of a map element from a certain position of the observer. usually, each measurement involves a set of variables that are spatially close. this results in a local connectivity of the graph: since variables are related to spatial entities and only variables that are within a certain range between each other are connected."
"(ii) hop-by-hop routing protocols: like ad hoc ondemand distance vector (aodv) [cit], it uses the on-demand mechanism of route discovery and route maintenance from dsr and also a mechanism for the hop-by-hop routing and sequence number. per each destination, aodv creates a routing table, while dsr uses node cache to maintain routing information."
"the most commonly available tools for laboratory examples typically have the sensors which acquire the observed signals and transfer them to a computer using the data acquisition systems in form of daq cards or external portable devices. the daq cards make the system bulky and expensive. the portable devices are better solution since they use standard ports like serial (com), parallel (lpt) or usb. both approaches require special software drivers, which are usually not freely available and differ for one manufacturer to the other. on the other hand, the most of today desktop or laptop computers do not even have the parallel or serial ports, while the programming of usb port is not trivial task and some standard computation software like matlab do not support this interface yet."
"for a quantitative evaluation, the average angle error (aae) and average endpoint error (aee) of the flow field were computed to show performance of the proposed method. the volume 6, 2018 formulas of the aae and aee are:"
"here, x * denotes the homogeneous matrix of the transformation x * . intuitively, the measurement between two poses is the relative movement between one and the other. the measurement between a landmark and a pose is the projection of the landmark in the frame of the observing pose. however, the above choices are not unique and in the general case, the user can select any measurement function that is invertible in x i . we selected these measurement functions to compute our condensed factors because the experiments demonstrated that they behave better than other models. once we know which particular measurement function to select for each separator, we need to determine the factors connecting the origin and the separators:"
"from ecg signal after implementation of qrs detectors they extract the vector of inter beat intervals (ibi) which gives the time distance between two successive r peaks, fig. 8 (up) . ibis are associated with variation in heart rhythm and hrv which has a close relationship to sympathetic and parasympathetic nervous systems and is an important index to assess and monitor cardiovascular diseases or symptoms such as arrhythmia, coronary artery diseases, myocardial infarction, diseases, hypertension, etc [cit] . hrv can be studied by different methods, but two are the most common: time domain to determine the deviation of successive r-r intervals; and spectral domain to determine the power spectral density of definitive frequency components of the ecg. the first step in the hrv analysis is the expression of successive r-r intervals in the record as a function of the time or the heartbeat number in the record, fig. 8 (up) . the second step is the conversion of the product of the first step, which is a tachogram, to hrv power spectrum by using fft, fig. 8 (down) . the final step is the determination of the power spectrum density of certain frequency ranges in the hrv spectrum. as an example lp is the power density number for the low frequency range (0.04-0.15hz) that is generated mainly by sympathetic activity. hp is the power of the high frequency range (0.15-0.40hz) and is derived from vagal activity which is modulated by respiration. their ratio is a good indicator of symphatetic-vagal balance. the students notice and discuss lp and hp concentrations from diagrams like in fig. 8 (down) ."
"m-commerce history, present information and essential metrics to evaluate usefulness of mcommerce are described. it focuses on easy m-commerce ideas to make mobile sales and on how to build website mobile friendly [cit] ."
"(iii) network lifetime. network lifetime is another important metric that reflect the load balancing capability of the routing protocol; the bigger the lifetime, the more effective the energy balancing capability. to compare the lifetime of the proposed protocol with other protocols, we take two parameters into consideration: (1) the first failure time ft first (the time that the first node in the network ran out of battery power) and (2) last failure time ft last ."
"the bearing-only dataset cannot be solved by direct approaches when run either in batch mode or incrementally (fig. 4e), due to the high nonlinearities in the sensor model. conversely, our approach succeeds (fig. 4f) ."
"the dynamic topology in ad hoc networks implies that some nodes may relay more traffic than others, mainly because of their location in the network; these hot spots will consume their energy reserves sooner than the others. unbalanced battery power consumption in the network nodes can leads to early node failure rate a, network partitioning, and to a reduction in the route reliability. traffic concentration on these nodes may increase radio jamming, delay, and packet loss. also, since the majority of the network traffic can potentially pass these nodes, they can become an important target for attackers."
"(iv) node failure level. finally the last performance metric that we use is the node failure level, which determines the capability of e2aodvv2 in keeping nodes alive for longer durations. the node failure level, for a time window, is the percentage of nodes in the topology that have failed due to a depleted battery. this value is calculated as follows:"
"in early studies, various modifications of the objective function were aimed to improve the performance of flow field computation under the influence of brightness variation, image noise and complex scene of initial evaluation sequences. for data term, the brightness constancy is unreliable due to changes in illumination. several higherorder constancy assumptions have been presented, such as gradient constancy [cit], hessian constancy and laplacian constancy [cit] . the gradient constancy has been an indispensable supplement of the data term in many past works [cit], because the second-order constancy is more sensitive to image noise. moreover, since global models are more sensitive to image noise and local strategies yield sparse flow field, a combination of global and local methods has been used to overcome the obvious defects [cit] . the l1 norm has been a consentient approach to preserve the flow discontinuities by applying a penalty function to the constancy assumptions [cit], since quadratic formulation of the data term is suffering from outliers."
"in presence of gaussian sensor noise, inferencing in a factor graph corresponds to solving a nonlinear least-squares problem. typically this is done by forming a linear system around the current state, solving, and iterating. one promising technique for solving the linear system is preconditioned conjugate gradient (pcg), which was used by konolige [cit] as well as montemerlo and thrun [cit] as an efficient solver for large sparse pose constraint systems."
"that depend on the type of the separators. in our slam examples we have two types of variables: the landmark poses that are represented as vectors in  2 or  3 and the robot poses that belong to either se2 or se3. the origin of the local maps is always selected to be a robot pose. the \"virtual sensors\" in our case are:"
3. local filter construction: bloom filter is constructed on the key. this filter is called local as they are created only the intermediate results in a tasktracker. here we get the map output of local filter. 4 . global filter merging: all map output of local filter is given to jobtracker which will construct the global filter.
"to maintain a consistent evaluation standard, we also measured the flow field error using aae and aee instead of the kitti benchmark. the experimental results on kitti datasets are given in table 4. according to statistical results of errors as shown in table 4, we found that errors increased compared to those of the middlebury and mpisintel datasets, due to probably great challenges provided by the kitti data. however, the guided filtering operation improved the accuracy of the flow field estimation significantly. for example, the nnf+gf method offered a remarkable reduction in both aae and aee compared to the original nnf model, particularly for frames of 000000 and 000036."
"wsn is a key enabling technology for next-generation networks, having significant application in connecting a multitude of wireless tiny sensors or being able to operate in a more advanced mode by locally connecting different \"things\" of different capabilities, such as personal device assistance (pdas), mobiles, and laptops, making then an ideal solution for next-generation networks. wsns technology has a profound effect on our everyday life due to its inherent features such as being ubiquitous in nature and offering an inexpensive alternative to traditional networks."
pulse transmit time (ptt) is the time between the heartbeat and the arrival of the blood pressure wave to a peripheral site. to calculate it we need simultaneously to acquire ecg and ppg signals. then the ptt is calculated as a time interval between r peak of the ekg and a point corresponding to maximum or minimum of ppg signal.
"as shown in fig. 9, although flow fields of each couple of the basic and developed models look roughly close. volume 6, 2018 there are indeed numerous emendations in the estimated flow fields of improved models compared to presentations of original approaches. through visual inspection, we identified differences of image edges or motion boundaries between the flow fields of the baseline and improved methods as indicated using inserted blocks and circles. as can be seen, there are significant improvements in the marked regions for the developed flow fields since the over-segmentation or blurring of image edges and motion boundaries that appeared in the original flow fields actually disappeared."
"during the entire simulation time, in a communication between a pair of source and sink nodes, there are streams of packets, with each stream consisting of packets. we studied the average jitter of all streams of data in the network."
"if a good initial guessx of the parameters is known, a numerical solution of eq. 2 can be obtained by the gaussnewton (gn) or levenberg-marquardt (lm) algorithms. in the remainder, we will refer to these approaches as direct methods. in this section we review two common tricks that make least-squares minimization more robust when the measurements z k or the state variables x k span over noneuclidean manifold spaces."
"for the second variant, that targets concentrated traffic with overloaded nodes, we chose nodes, amongst all network nodes ( ), as destination nodes (sink nodes) for the generated traffic in the network. the number of is set to 20% of, which due to the many-to-one traffic pattern, creates overload conditions at destination nodes. simulation results show that, in such scenarios, the standard deviation behaviour is similar to the previous variant but with the worst performance ( figure 11 ). however, as in the previous scenario, all three protocols improve in terms of performance as the number of nodes in topology increases ( figure 11 ). as both figures 10 and 11 show, the proposed protocol e2aodvv2 reaches a higher performance in terms of battery power efficiency, balancing, and scalability in contrast to the baselines."
"to this end, we replace the factor graph of each local map with another and simpler one whose solution approximates the original local solution. in constructing this factor graph, we utilize smooth sensor models that can fully qualify each variable within the local map with respect to its origin. we construct the reduced problem by considering the variables in the origin along with all the shared variables of a local map. we then add a factor between each shared variable and the origin, as illustrated in figure 3d . this factor is computed from a measurement function that depends on the type of the variable: we should obviously describe differently measurements of a robot pose and measurements of a landmark. the mean and the information matrix of the measurement are computed by projecting the marginal covariance of a state variable in the measurement space via the unscented transform [cit] . this step is discussed in detail in section v-b."
"in the last decade, several evaluation benchmarks such as middlebury, mpi-sintel and kitti brought new challenges in terms of illumination changes, large displacements, motion discontinuities and occlusions. how to minimize objective function has been a major task in this line of research. it has been proven that coarse-to-fine estimation is an effective way in dealing with large displacements; and incremental warping at each pyramid layer can minimize a non-convex function [cit] . in addition, texture-and structure decomposition was used to reduce the influence of image shadow caused by lighting changes [cit] . furthermore, some fast computational schemes were achieved near real-time performance by using parallel-computing gpu [cit] . besides, due to the great success of convolutional neural network modeling in recent years, some works have investigated the use of convolutional neural network in computation of the flow field [cit] . however, those methods cannot be applied to real world data where the ground truth is not easily accessible."
"1. job submission: when user does login, the calculation of recommendations for active user starts. the users in datasets are getting divided amongst the available tasktracker. tasktracker contains all the essential information required for map phase."
"the function that determines the precedence of route is given by routeprio( ). this function depends on the energy and the traffic parameters of route that are calculated by (6) and (7), respectively:"
"(i) distributed traffic amongst nodes: traffic sources and destinations will be chosen randomly in time. this variant is shown by \"distributed\" in table 6 ."
"in a communication between a source node ( ) and a destination node ( ), originates a route message (rm), called route request (rreq) message, and broadcasts this to all of its neighbours. these rreqs are then flooded in the network until they reach . an intermediate node which does not know the route to should forward the rreq to its neighbours. the intermediate node will also drop the repeated request for the same destination and will not forward them anymore, as shown by figure 2 . rm has several fields such as current node address, next node address, hoplimit (the default value is 10 based on [cit] ), target (destination node address if it is an rreq message), and the origin (source node address if it is an rreq message) [cit] . by tracing the rreq traversed path (called accumulated path), each node, such as intermediate or destination node, which receives an rreq message, can extract routing information. figure 2 shows the rreq phase in aodvv2 routing protocol for a sample network. figure 3, when rreqs finally reach the destination node, another rm, which is called route reply (rrep), will be originated by the destination node. the intermediate nodes which have, in their routing table, an entire route towards that destination node also can immediately reply to the rreq originated by the source node (known as gratitude reply and it is an optional feature of aodvv2). both the rreq and the rrep have the same uniform structure."
"this procedure converts the factor graph of a local map, which can have an arbitrary topology into a star topology. the center of the star is the origin of the local map and the other nodes are the shared variables. the non-shared variables are not considered in this stage, since they do not directly concur to determine a global alignment. the shared variables are connected to the origin by factors that are generated based on the solution of the local map. we call these factors condensed factors, since they summarize the relationship between a variable and the origin of the local map by considering all the measurements when optimizing the local map."
we then compute  i by inverting the covariance matrix reconstructed from the projected sigma points. the procedure outlined above allows us to determine the new factors used to describe a local map in a compact manner at a higher level of abstraction. the new factors are computed after considering the solution of a full portion of the problem and model the relationships between the origin of a local map and the separators.
"as an edge-preserving filtering, a well-designed guided filter can avoid blurring image edges. furthermore, a guided filter is based on a local linear model and its computational time consumption is independent of the filter size. the guided filtering may have many applications such as image fusion [cit], stereo matching [cit], image enhancement [cit] and others [cit] . in this paper, the guided filtering is first applied in optimizing computation of flow field."
"2.1. ad hoc routing protocols category. many routing algorithms have been proposed for ad hoc networks in the literature. these routing protocols can be divided into several categories based on various criteria. in this section, we briefly review these categories and provide one sample from each category."
"the paper describes the development of a variety of classical biomedical experimental exercises by using standard tools such as amplifiers, modulators, microcontrollers, microphone input and matlab software. signal acquisition is performed by sound card through microphone input which is a very convenient way. multiple signals are transmitted through one cable using frequency multiplex technique. the exercises are developed by the fusion of the knowledge from various relevant disciplines and are intended to introduce students to fundamental concepts of biomedical experimentation, from the instrumentation and data acquisition requirements to subsequent data analysis techniques. a case study emphasizes the processing of ppg and ecg signals, but the same principle can be applied to other signals. additionally, the approach can be successfully used in research purposes where virtual instruments should be use as a cheap and effective replacement for expensive instruments."
"in this paper, we discuss an approach to determine a good initial guess for least-squares problems arising from slam or sfm. the key idea of our approach is to partition the input factor graph into small locally connected sub-graphs that represent sub-problems. these sub-problems can be solved robustly and efficiently, but their solutions cannot be combined together in a straightforward manner. to this end, from each partial solution we construct a simple factor graph that constrains the relative positions of the variables in the solution. these sub-graphs represent an convex approximation of the original ones which exhibit a larger convergence basin. the factors in these graphs incorporate the information contained in the sub-graph from which they were generated, g. grisetti and r. kmmerle are with the university of freiburg. g. grisetti is also with sapienza, university of rome. kai ni is with microsoft. fig. 1 . a robot equipped with a stereo camera is simulated in a manhattan world. the trajectory is drawn in blue whereas the features are depicted in orange. left: the initial guess is computed by composing the odometry of the robot. middle: running a standard iterative levenberg-marquardt algorithm yields a sub-optimal estimate. right: our approach converges to the global minimum."
"(i) source node: when a source node wants to send an rreq, it initializes the values of the energy and the traffic fields (they will be set to zero)."
"this section illustrates the overall architecture of our implementation. hadoop is used in our approach, an open-source implementation of the mapreduce framework. in hadoop, we have master node and worker node. the master node is called jobtracker and the worker node is called tasktracker. figure 2 shows the overall execution flow of join function on the datasets in our implementation. when the user login to the system by entering username and password, the following sequence of steps is carried out."
"(v) jitter. in a data transmission between a pair of source and sink nodes, jitter is the variation in the time between packets arriving at the source node. let us assume that, at time, the source node sends packet and the sink node receives it at time . the jitter of packet is calculated as follows:"
a problem in this form can be effectively represented by a factor graph. a factor graph is a bipartite graph where each node represents either a variable node x i or a factor node f k between a subset x k of state variables involved in the k th constraint. a factor is connected by edges to all nodes in the subset x k . figure 2 shows an example of a mapping between a factor graph and an objective function.
"a way to circumvent this limitation is to use low frequency signal to modulate a carrier in the audio range; capture the modulated signal using a soundcard; and then demodulate in software [cit] . matlab provides ideal solution for such scenario since it has functions for easy control of sound cards as well as functions for demodulation, filtering and powerful signal processing, data analyze and visualization."
"specifically, we began with an overview of the development of optical flow computation in the past three decades and a discussion on limitations of the current approaches. we then described the general formulation of the guided filtering and specific scheme of the guided filtering for optical flow with the framework of coarse-to-fine computation. furthermore, we modified some typical and state-of-the-art flow field estimation methods such as hs+nl, classic+nl, nnf and nnf+eac by applying the proposed guided filtering operation to baseline models. finally, we conducted comparative experiments using test sequences of the middlebury, mpi-sintel and kitti databases to examine potential benefits of the guided filtering. the results of comparisons between the original and developed models showed that the proposed guided filtering with optimizing scheme can improve both the accuracy and robustness of the flow field estimation."
"the rest of this paper is organized as follows. section 2 explains related work in mobile commerce, mapreduce, joins in mapreduce and bloom filter. section 3 introduces user-based collaborative filtering recommendation algorithm on hadoop platform. section 4 describes the proposed architecture. section 5 shows experiment results and examination. finally section 6 concludes the paper."
"we utilized as the baseline models several typical and state-of-the-art approaches including hs+nl [cit], classic+nl [cit], nnf [cit] and nnf+eac [cit] . the hs+nl model is developed by applying median filter and coarse-to-fine scheme to the original horn and schunck optical flow method. the classic+nl model is a nonlocal tv-l1 optical flow method by using median filter and coarse-to-fine scheme to solve the total variational objective function with l1 norm. the nnf model employed an approximate nearest neighbor field to correct the initial optical flow for variational optical flow computation, and the nnf+eac model is presented by applying edge-aware constraints to variational optical flow energy function to preserving image and motion edges. to demonstrate benefits of the guided filtering in developing variational optical flow estimation, we applied the guided filtering scheme to each basic method, named hs+nl+gf, classic+nl+gf, nnf+gf and nnf+eac+gf, respectively. specifically, we utilized the guided filtering as an additional operation after the median filtering during the coarse-to-fine computation of flow field as shown in fig. 4 . fig. 9 shows flow fields of all couples of the basic and improved methods performed on the middlebury data. we didn't look for the best method among the baseline and development approaches above-mentioned, but did a comparison between the basic model and corresponding development program to reveal significant benefits of the proposed guided filtering operation for flow field computation."
"in figure 5, if the queue size of the interface of node located in route is aq and the maximum queue size is mq, then the traffic parameter of node in e2aodvv2 (i.e., tp ) will be calculated based on"
"where rx is rating of user x on item s and ry is rating of user y on item s, sxy indicates the items that user x and y co-evaluated."
"this part describes how collaborative filtering algorithm can be implemented within the mapreduce framework. it is difficult to directly use mapreduce model in computation process of collaborative filtering algorithm. the recommendation process for each user is summarized in the map function i.e. while making recommendation, we will save user id in text files which serves as input to the map function. the mapreduce framework defines few mapper to handle the user id files. the algorithm is partitioned into three stages as data segmenting stage, map stage and reduce stage."
of the measurement can be computed by a sensor model h k (x k ). the uncertainty of the measurement is modeled by its information matrix
"however, most of current power-aware approaches lead to some common drawbacks, such as increased delay and increase in the number of required control messages that should be created by the routing protocol to deliver users' data packets (called normalized routing load), limited scalability, among others."
"as it can be seen from the table, the larger the problem becomes, the harder is for lm to converge, while our method always finds the correct minimum. also the non-linearity of the sensor has a great effect on the convergence. in 2d, when we use a bearing only sensor and the number of constraints, increases lm fails. in 3d using a cartesian sensor leads to the correct solution for standard approaches, while the same approach fails when using a depth or a disparity model, the former being better than the latter. our method succeeds in all cases."
"the inherent benefits of aodvv2 due to standardized control packets following ietf uniform packet formats and the capability to support ipv6 suggest that this protocol will have a strong legacy in ad hoc networks. therefore, if we are to pursue energy efficient operation for routing in ad hoc networks, then exploring aodvv2 as the fundamental building block can be potentially a springboard for promoting significant energy savings in the network. in this section, we describe our proposed approach to increase the energy efficiency of aodvv2."
"where (u e, v e ) t denotes the estimated flow field, (u g, v g ) t denotes the ground truth, and n is the total number of image pixels."
"aodvv2 is a successor to the aodv that is being developed by ietf manet; prior to the 26th revision [cit], aodvv2 was called dynamic manet on-demand (dymo)."
"a mapreduce cluster consists of one master node and a number of worker nodes [cit] . the master at regular intervals communicates with each worker to test their status and manage their performance. when a mapreduce job is acceptted, the master divides the input data and generates map tasks for the input splits, and reduce tasks. the master allocates each task to idle workers. a map worker accepts the input split and implements map task given by the user. a reduce worker reads the intermediate pairs from all map workers and accomplish reduce task. when all tasks are complete, the mapreduce job is finished."
we validated our approach on real-world data and performed extensive statistical tests on simulated data. on all datasets we compare our approach with the levenbergmarquardt (lm) implementation in the g 2 o package. all results have been validated by both visual inspection and comparing the errors of the final solution. real-world experiments provide evidence on the real applicability of the results. while the ground truth of simulated datasets allow us to characterize the behavior of the approaches in a more detailed way.
"for a quantitative comparison, table 2 summarizes results of aae and aee of the estimated flow fields of middlebury data. it is obvious that both the averaged and individual errors of the improved methods decreased compared to the baseline methods, especially in some typical sequences with detailed edges and complex motion boundaries such as grove3, urban3 and venus."
"in general, the visual and quantitative comparisons between the developed and basic methods with the middlebury data indicate that the guided filtering operation is able of improving accuracy and robustness of the optical flow estimation due to probably its valuable characters of edge-preserving."
"during student exercises, due to its simplicity, easy handling and speed, the ecg signal is taken from fingertips (see fig 1) in two leads, ground free configuration. it can also be done in chest version of lead 2 or ra-la-rl. the ppg signal is taken from finger."
"to demonstrate benefits of the proposed guided filtering, we employed the classic and classic+nl methods [cit] as the baseline models. the classic model was implemented by using coarse-to-fine scheme to solve the classical formulation of optical flow. the classic+nl model was presented by adding median filtering to the classic model. furthermore, we applied the guided filtering scheme to the classic and classic+nl models respectively, named classic+gf and classic+nl+gf. several evaluation experiments were conducted to test the classic model with different filtering choices by using grove3 and venus sequences of the middlebury training set. table 1 shows the results of aae and aee of the various models. on one hand, the comparison between classic model and classic+gf model as well as between classic+nl model and classic+nl+gf model showed that the proposed guided filtering scheme is able to improve the computation accuracy of optical flow due to edge-preserving. on the other hand, the guided filtering showed weaker capability of removing outliers than median filtering because the proposed guided filtering is a linear scheme. to improve the robustness of optical flow and to preserve the edges and boundaries, we employ the median filtering as a pre-filtering operation. the presented combination of median filtering and guided filtering is capable of providing robustness and preserving boundaries of optical flow estimation."
"a mapreduce program consists of two functions: map and reduce. the map function accepts a set of records from input files in the form of simple key-value pairs and constructs a set of intermediate key-value pairs. the values in the intermediate pairs are automatically collected by key and sent to the reduce function. the grouping consists of sort and merge processes. the reduce function accepts an intermediate key and a set of values related to the key, and finally it generates output key-value pairs [cit] ."
"in map stage, the hadoop framework decides whether to define mapper to handle the user id files. the hadoop framework defines a new mapper, if sufficient resources are available. initially the ratings matrix between the consumers and products is constructed by setup function of mapper. then it gets the line number as the input key and equivalent user id as value. then it computes similarity between the user and other users. the last step is to determine the user's nearest neighbours based on similarity values and compute his expected rating on products. these ratings are sorted and stored in recommend list. the user id and its associated recommend list is given to the reduce phase."
"we presented a novel approach for optimizing factor graphs obtained from slam or sfm problems. the algorithm is robust to noisy initial guesses and highly nonlinear sensor models. the key idea is to construct an approximation of the original problem having a larger convergence basin by computing condensed measurements from partial solutions, to determine a good initial guess. our approach can solve problems that cannot be handled by other state-of-the-art methods. in the future, we plan to exploit the divide-andconquer strategy of our method to take advantage of modern parallel cpu and gpu architectures."
"for the first variant, distributed traffic amongst nodes, all protocols show a similar performance ( figure 10 ): increasing the number of nodes leads to improved energy balancing (lower standard deviation). this can be explained due to the chosen traffic model since multiple traffic source/destinations are chosen randomly throughout the simulation, and thus more nodes will participate in the routing process."
"in the presence of devices having a highly nonlinear model, like a bearing only sensor or a monocular camera, and in absence of a good initial guess, direct methods can fail. observing the same scene with more informative sensors that are, for instance, capable of detecting also the range of a landmark or the depth of a point increases the chances of success for direct methods. thus, direct approaches applied to slam-like problems are sensitive to the sensor model: the used sensor model has a great impact on the profile of the error function, and thus on the chances of finding the global minimum. clearly, the sensor model is a characteristic of the problem and cannot be arbitrarily changed. however, once we have a consistent local solution for a portion of the problem, we can formulate another problem having a similar solution that uses less non-linear sensor models. it is in general convenient to use in this stage sensor models that have the smoothest possible error profile and that allow to observe the highest possible number of dimensions of the involved state variables."
"another limitation of the guided filtering is that its free parameters must be set suitably because impertinent values of the free parameters may reduce the benefit of the guided filtering. in our experiments, the free parameters of guided filter were selected by running the hs+nl+gf with different choices of the parameters on sequences of rubberwhale and grove3 from middlebury database. nevertheless, fixed values of the free parameters may not be the best choices for other modified optical flow models. the self-adaptive parameters need to be considered to address the issue."
the job in collaborative filtering is to guess the usefulness of product to a particular user which is based on a database of user votes. collaborative filtering algorithms guess ranking of a target item for target user with help of grouping of the ranking of the neighbours (similar users) that are known to item under consideration.
"a key enabler in ad hoc networks is a routing approach that needs to be robust and low in complexity to support endto-end connectivity in highly dynamic operating environments. however, the effect of node/user mobility, dynamic topologies, frequent link breakages in the communication path, limitation on nodes resources such as battery power, and lack of central point such as base stations or servers means that routing in ad hoc networks can be a very challenging issue. beside these aforementioned issues, there are also several qualities of service (qos) metrics that should be considered in a communication, such as data throughput, delay, energy efficiency, traffic balancing, and the protocol overhead. furthermore, due to the distributed and cooperated nature of ad hoc networks, attributes such as the number, 2 international journal of distributed sensor networks velocity, and the mobility pattern of mobile nodes may affect the qos metrics provided by the routing protocol."
"during this exercise the students observe the ecg and ppg signals in frequency domain. the matlab fast fourier transform (fft) functions are applied to modulated and demodulated signals, fig. 6 . first diagram show the spectrum of mixed modulated ppg and ecg signals. two peaks on 2khz and 4khz correspond to the carrier frequencies. around carriers are frequency components produced by the modulation signals. the students note that the spectrum around ecg carrier is wider and \"richer\" in context of harmonics than one around ppg. after the separation, demodulation and low pass filtering, the fft is applied to both ecg and ppg signals. as it can be seen, the position of the dominant peak in frequency spectrums corresponds to the hr in hz or beats/min. the spectrum of ppg is less scattered than ecg and is more convenient for hr extraction. the students measure the hr at different conditions, relaxed and during physical exercise and make a graph of its trend."
"with the guidance image g, the guided filtering assumes that the output image o of the filter is a linear transformation corresponding to the guidance image in a local window [cit] :"
bloom filters used in mapreduce will help to reduce the intermediate results in map phase which in turn speed up the overall process of recommendation. this paper shows how mapreduce can be used to parallelize collaborative filtering. it also presents an architecture to enhance the join performance using bloom filters in the mapreduce framework. we propose to apply this concept to recommender system for mobile commerce.
"as mentioned before, critbat( ) identifies the number of nodes in route which have reached to a critical battery level and hence should be avoided from packet relaying functions (otherwise, the route is likely to break down). a large value for critbat( ) triggers an alert that this route has several nodes with a critical battery level. consequently, ( ) can be revised to reflect also the negative impact of these nodes in route . the final ( ) of a route in e2aodvv2 will be calculated based on (6) . when a destination node receives an rreq, it can calculate, via the energy field of the rreq:"
"the error function depends on the (known) measurement function h i () and on the unknown measurement z i . since the error should be 0 at the current solution of the local map, the measurement vector at the equilibrium is:"
"most commonly used algorithm for personalized recommendation in commercial recommendation system is collaborative filtering (cf). the main problem of cf is its scalability means the calculation cost of cf would be high if the dataset is very large. mapreduce has been widely used for large scale calculation job. it is used to process massive amount of data in a reasonable amount of time. the main advantages of mapreduce are straightforward programming interface and high scalability along with refined failure management. but mapreduce has some restrictions while doing join operation on large scale datasets. the main difficulty in join processing in mapreduce is that complete dataset should be processed and sent to nodes. this creates bottleneck in performance particularly when only small fraction of data is applicable for the join. in this paper, we implement bloom join algorithm on the hadoop, an open-source execution of mapreduce framework, to enhance the join performance. with this method, we can avoid redundant records and decrease communication overhead."
"several problems in autonomous robotics and computer vision, like simultaneous localization and mapping (slam) or structure from motion (sfm), require the solution of a least-squares problem that exhibits a strong locality, which results in the sparse structure of the corresponding factor graph. variables in these problems are correlated when they are temporally or spatially close. the temporal locality is the consequence of the sequential data acquisition, while the spatial locality derives from the limited range of the sensors."
"by displaying ecg and ppg signals in the same window, and using built-in \"cursor function\" the ptt can be measured as distance between r peak and local minimum of ppg signal, fig. 5 . there are mathematical relations between ptt and systolic and diastolic pressures (sbp, dbp) that gives us a possibility for cuffless blood pressure monitoring [cit] ."
"processing the whole cartesian dataset with direct approaches does not give the correct solution (fig. 4b), which can however be obtained by running the direct approaches incrementally, after inserting every 50 sequential odometry measurements (fig. 4c) . however, our approach always finds the correct solution (fig. 4d) . the solution is the same as the one computed by the direct approaches run incrementally."
"the traffic field of rreq has two cells: tottra and maxtra. the first one is the summation of all traffic parameters in route, as defined by"
"since the amount of information in e-commerce & mobile commerce increases, there is need to filter irrelevant information & discover helpful contents & reliable sources. recommender system is the standard tool which gives advice about items, products, information or services users might be fond of. recommendation systems generate a ranked list of items on which a user might be interested. recommendation systems are constructed for movies, books, communities, news, articles etc. they are intelligent applications to assist users in a decision-making process where they want to choose one item amongst a potentially overwhelming set of alternative products or services [cit] . recommender systems are personalized information filtering technology used to either predict whether a particular user will like a particular item or to identify a set of n items that will be of interest to a certain user. it is not necessary that a review is equally useful to all users. the review system allows users to evaluate a review's support by giving a score that ranges from \"not helpful\" to \"most helpful\". if a particular review is read by all users & found helpful then it can be assumed that new user might appreciate it. controversial reviews are the reviews that have a variety of conflicting rating (ranking). controversial review has both passionate followers and motivated enemy without clear majority in either group. the recommender system uses information from user profile & interaction to tell possible items of concern. it is useful to approximate the degree to which specific user will like a specific product. the recommender systems are useful in predicting the helpfulness of controversial reviews [cit] . recommender systems are a powerful new technology for extracting additional value for a business from its user databases. these systems help users to find items they want to buy from a industry. recommender systems benefit users by helping them to discover items they like. they help the business by generating more sales. recommender systems are fastly becoming a crucial tool in e-commerce on the web [cit] ."
"the remainder of this paper is organized as follows. in section ii, an overview of the previous work is provided. section iii introduces the formulation of guided filtering. section iv describes the proposed guided filtering schema for flow field optimization. the experimental results and discussions are presented in section v. finally, section vi concludes the project."
"the proposed guided filtering scheme is able to preserve the image edges and motion boundaries of optical flow as shown in the evaluation experiments. however, it has some potential limitations. for instance, the guided filter has a volume 6, 2018 common limitation that it may exhibit artifacts boundaries near weak edges since the guided filter transferring the local structure information in the guidance image to the target one directly. to address the issue that the guidance flow field may have false edges in some regions near the motion occlusions, the mutual-structure based guided filtering [cit] may enhance the capability of edge-persevering of optical flow near occlusions."
"the local filter is constructed on the time interval as months 3, 6,9,12 etc. then we get the map output of the time interval as we will consider only those reviews of given time interval from specified date. all map output of the local filters is given to job tracker which will construct global filter which will be derived from the movie i.e. how many rating for considered movies. finally we get reduce operation output. the number of intermediate /output records is constructed from the interval in months (i.e. 3, 6,9,12 etc), map output from reviews and map output from movies. finally join of reviews and movies will give the reduce output. in this implementation, map phase is terminated early as the intermediate results are decreased by the bloom filters."
"(a) proactive protocols. this type of protocols attempts to find and maintain consistent, up-to-date routes between all source-destination pairs regardless of the use or need of such routes. therefore, each node maintains one or more tables to store routing information (table driven protocols). proactive protocols require periodic control messages to maintain routes up to date for each node. routing techniques are either link-state or distance vector (or a mixture of both) [cit] ."
". the advantage of using the  operator instead of the more common + is that  takes care of handling the singularities and the error function in x has a smoother profile. clearly, in case the measurements or the state variables are euclidean, the  and the  become a regular vector addition and subtraction."
"in 2d we used point landmarks and simulated both a cartesian sensor, similar to the one used in the victoria park experiment and a bearing-only sensor. in 3d we used an ideal cartesian sensor capable of measuring the position of a landmark in the reference frame of the observer (this can model a 3d laser), a depth sensor, which measures the \"depth\" of points in the image plane (this can model rgbd cameras like the kinect) and finally we used a disparity sensor suitable to model stereo cameras. table i shows the characteristic of the different dataset in terms of number of poses, landmarks and factors in the graph and sensors used to perceive the landmarks. for each dataset we report the value of the error f () at the minimum found by the algorithms for the different datasets. f id is the theoretical minimum value obtained by running lm using the ground truth as initial guess. f init represents the error of the initial guess derived from the odometry. f lm is the error of the solution obtained by running 100 iterations of the lm algorithm in the g 2 o package and f cond using our approach to determine the initial guess and then running 10 iterations of standard lm. we marked in bold the cases of wrong convergence. the reader might observe that in some cases our approach did not reach the absolute minimum, but this is due to the fact that we limited the lm iterations to 10. we verified that running 100 iterations of lm results in reaching the theoretical minimum in our simulated experiments. additionally, we observed a substantial speedup in using the condensed measurements. for a large problem of 5001 poses computing the solution with lm takes approximately 18 minutes against 9 minutes of running the condensed approach plus 10 iterations of lm. in total the generation of the condensed factors and the solution of the sparse problem took less than 4 minutes on a core 2 duo 2.6 ghz using one single core. the column \"# cond\" reports the number of condensed factors in the global sparse problem, while the column f sparse reports the initial and final error of the optimization of this sparse problem. the significant reduction of the error is possible because the constructed problem is more convex than the input one. for problems having a very small size, no condensed factors are generated. this is the case of the 2d dataset with 11 pose variables, where the solution is computed with standard lm."
"thus we refer to them as condensed measurements. to find the global layout of the partial solutions in the space, we find the minimum of the union of all translated sub-graphs. as this translated problem is more convex, we have increased the chance of finding the correct minimum, which represents the initial guess. our method can be seen as a generalization of other divide-and-conquer methods like hog-man [cit] or t-sam [cit] . compared to hog-man our approach allows to deal with factor graphs whose variables are not only robot poses, but arbitrary elements like landmarks and system parameters. t-sam is orthogonal to our approach since it mostly focuses on the solution of the linear sub-problem and does not address the non-linear aspects in the general case. we tested our approach on a wide set of real-world and simulated experiments acquired with different 2d and 3d sensors. furthermore, we performed a statistical comparative analysis. our approach succeeded in solving problems where other state-of-the-art methods failed. our system is built as an extension to g 2 o [cit], an open source generic factor graph optimization package. thus, it can be straightforwardly applied to all the instances of problems handled by g 2 o with minimal effort. figure 1 shows a motivating example of our approach where we simulated a robot with a stereo camera driving in a manhattan maze. a standard iterative levenberg-marquardt algorithm converges to a local minimum. however, our approach estimates the correct solution."
"in this section, we began testing guided filtering using middlebury data since the middlebury database is a standard benchmark [cit] for optical flow. fig. 8 displays the reference frames and ground truths of the middlebury data."
"similar to the previous case, here again there are two different traffic models: distributed traffic amongst nodes ( figure 12 ) and concentrated traffic with some overloaded nodes (figure 13 ). as discussed in section 5.2.1, we use the first failure time ft first and the last failure time ft last as the network lifetime metrics. the values of ft first and ft last are marked in figures 12 and 13 and also summarized in table 6 . average jitter (ms) figure 14 : jitter versus number of nodes."
"in recent years, it is noteworthy that the median filtering heuristic acts as a prerequisite tool in the most of variational optical flow methods, since it can significantly improve the accuracy and robustness of almost all methods, even the original hs formulation. however, there is clearly a limitation that the median filtering may generate oversegmentation or blurring at image edges and motion boundaries caused by large displacement, motion occlusion, or complex scene. fig. 1 indicates the performance of a classical median filtering when processing sequences with complex scenes and occlusions. the results reveal that the image and motion edges in the computed flow fields are obviously blurred compared to the ground truths. to tackle the issue of over-segmentation and blurring at image and motion edges in flow field, we propose a guided filtering scheme for flow field computation. this given program is planned as an additional operation during the coarse-to-fine computational process. next, we apply the guided filtering to some typical and state-of-the-art approaches of flow filed estimation, and employ the evaluation sequences of middlebury, mpi-sintel and kitti databases to show benefits of the guided filter for edge-preserving."
"we generated a set of synthetic 2d and 3d datasets by simulating a robot moving in a grid world and sensing point landmarks in its neighborhood. in all cases, we created a synthetic world by placing a set of landmarks in the environment and letting the robot move for increasingly long trajectories along a simulated manhattan world. the same synthetic world was used to create different datasets: one for each sensor setup. thus, in these experiments we tested: different trajectory lengths and different sensor modalities. figure 6 shows the ground truth of two synthetic datasets used in our experiments."
"a route with a lower traffic metric cost has a higher priority in the routing process of e2aodvv2. nevertheless, a route may have a low overall traffic metric even if one of its nodes is overloaded with traffic, thus creating a bottleneck, and should be potentially avoided. the traffic parameter of route i is shown by ( ) and is calculated by"
"(a) balancing energy consumption and scalability. for this metric, simulation results are presented in terms of the capability of our approach to balance battery power (energy) consumption. the traffic model is the one presented in table 5 with the following variants."
"most of current routing protocols, proposed in manet group of ietf [cit], consider the path length metric when choosing the best route between a source ( ) and a destination node ( ). however, this approach may, in most cases, minimize the end-to-end delay in a communication between the source and the destination node, but it may not be adept to handle other qos metrics such as energy efficiency and load balancing, because they do not consider the node residual energy as a criterion in the route selection process [cit] ."
"mapreduce's join algorithms are generally categorized into two classes: map-side joins and reduce-side joins [cit] . map-side join algorithms are more well-organized than reduce-side joins as only final result of the join is constructed by them in map phase. for map-merge join, two input datasets need to be separated and sorted using join key. whereas reduce-side join algorithms are more common, still they are not competent as the whole input records need to be transferred from map workers to reduce workers [cit] ."
future work will extend the proposed approach in this paper for energy efficiency and load balancing to include other qos metrics such as delay and data throughput by varying the topology area and velocity of nodes. another metric that could be considered in the future works to obtain the required power for transmitting the data between two nodes is the signal to noise ratio (snr). a high snr value for a link between two neighbor nodes shows that we should find an alternative energy efficient link and eventually a path for the data transmission between a pair of source and sink nodes. this approach may also lead to the need for a crosslayer design [cit] for our protocol.
"more recently, dellaert and colleagues suggested a system called  sam [cit] implemented using sparse direct linear solvers [cit] introduced a variant of this called isam that is able to update the linear matrix associated with the nonlinear least-squares problem. [cit] showed how to construct the linear matrix efficiently by exploiting the typical sparse structure of the linear system. however, the latter approach is restricted to 2d pose-graphs."
"however, the sound card filters are of band pass nature with 20hz-20khz range, both on the input and the output. most commonly, down border frequency varies from 50hz-200hz depending on the quality of the device. it means that the dc and slow varying signals cannot be faithfully reproduced by direct coupling to the microphone input. the most of the information with the physiological signal, like ecg and ppg, are just below 20hz and these signals should be considered as low frequency or even dc."
"reflection model which considers both the direct path and a ground reflection path between two mobile nodes. also all nodes in the network move based on the random way point mobility model. in this movement model, a mobile node starts its travel from a random location inside the table 4 : the energy and traffic features of all paths in the network of figure 9 . topology area after pausing for a certain period of time (called \"pause time\"). after the initial pause time, the mobile node chooses another random location inside the topology area and moves toward this new location by a speed that is uniformly distributed between a predefined minimum and maximum speed. upon arrival, the mobile node pauses again for the pause time and repeats the previous process again till the simulation time is expired [cit] . the simulation results presented in this paper were obtained using the ns-2 simulator [cit] . traffic sources are cbr (constant bit rate) and the packet sending rate at the source nodes is 8 packets per second. table 5 presents a summary of the parameters for the simulated scenarios (movement and traffic files)."
"in the kitti test datasets, error of estimated flow field is measured by gathering statistics of pixels with an endpoint error greater than 3. fig. 12 shows the error images and the statistical results of flow error, respectively. in spite of the kitti data is quite different with the middlebury and mpi-sintel data due to the motions is often dominated by the camera. the modified approaches with guided filtering performed a superior result of the flow error compared to the original methods by using the kitti benchmark."
"a common way to solve these least-squares problems is exploiting iterative methods like gauss-newton or levenberg-marquardt, which progressively refine an initial guess until convergence. if this guess is out of the convergence basin of the algorithms, the iterative optimization may not converge to the global minimum. the convergence basin, in turn depends on the shape of the sensor models used to construct the factors of the least-squares problem. the more irregular/non-linear these error functions are, the tighter the convergence basin is."
is an error function that computes the distance vector between the prediction k and a real measurement z k . this error is 0 when the prediction obtained by mapping the states x k to the measurements is equal to the real measurement:
"microphone input (mic) is the only interface built in almost all computers, regardless they are desktop or laptop, and even can be found in mobile phones or personal digital assistants (pdas). an alternate data transfer method using this type of interface proves out to be very convenient and effective since the integrated sound cards allow acquisition process at high resolution (up to 24bits) and adjustable sampling rate (up to 44khz) behind built-in amplifiers, analog switchers and filters. additionally, the sound cards can be easily controlled from a broad range of standard applications."
ppg is a non-invasive method to detect cardiovascular pulse wave that propagates through the body using a light source and a detector. simply we graphically illustrate the blood circulation through our finger or ear lobe.
"here we illustrate a portion of a slam problem with unknown parameters represented as a factor graph. the round nodes represent state variables, while the square nodes represent the factors. x 0:n denotes the robot poses, and x k denotes the unknown calibration parameters of a sensor on the robot. the factors are depicted as black squares and arise either from odometry measurements z u 0:n or from environment measurements z l ij which relate pairs of robot locations x i and x j and calibration parameters x k ."
"once we \"condensed\" the local maps, we assemble an approximation of the original global factor graph by combining all the newly computed factor graphs into a new sparser factor graph, whose solution is a global configuration of the origins and of the shared variables. this is illustrated in figure 3e . furthermore, since the sensor models are smoother than the original ones, the new problem will have a larger convergence basin and direct approaches are likely to work. having found an approximated solution for the origins of the local maps and of the shared variables, we can determine a good initial guess by arranging the local maps computed at the beginning of the procedure accordingly, as shown in figure 3f . at this point, an optimization which considers the original factors can further refine the approximated solution."
"in the second experiment, we describe the results of processing a 3d dataset acquired at the freiburg university campus with a mobile robot equipped with a bumblebee stereo camera. from each frame, we extracted visual features along with disparity and constructed one large bundle adjustment problem enriched with odometry information. the results of the experiments are illustrated in figure 5 . for this dataset, we considered two initial guesses: one obtained by optimizing the pose graph constructed by densely matching pair-wise observations (thus pretty accurate), and one based only on the wheel odometry. we processed this dataset with direct methods both batch and incrementally. direct approaches always succeeded in finding the optimal solution when initialized with the good guess, while they failed in all cases starting from the bad guess. our approach succeeded in creating the local maps and determining a good initial alignment."
"where  is a regulatory parameter with a constant value near zero. using a linear regression model, the linear coefficients a k and b k will be determined as shown below:"
"wen liu received the ph.d. [cit] . he is currently a professor with the department of physical therapy and rehabilitation science, the university of kansas. his current research interests include medical image processing and analysis, motor learning, and postural control."
"the energy field of rreq in e2aodvv2 has three cells: totbat, minbat, and critbat. totbat is the summation of the total battery power level of all nodes of route, as shown by (1) . the minbat cell in the energy field of rreq shows the minimum value of bp for all nodes in route, whilst critbat shows the number of nodes which have a bp less than cbpl in route :"
"providing a concrete routing solution that is energy efficient, but on the other hand able to deliver adequate qos, is challenging, especially in a highly mobile environment. usually each protocol tries to focus only on one or some of these qos metrics. different approaches reactive and proactive have been explored, and each has distinctive advantages."
"where is the number of nodes in route . when a destination node receives an rreq, it can calculate via the traffic field of the rreq."
"heart rate (hr) is the number of heartbeats per unit of time, typically expressed as beats per minute (bpm). the measurement of hr is used by medical professionals to assist in the diagnosis and tracking of medical conditions. it is also used by individuals who are interested in monitoring their heart. the rr interval is the inverse of the hr."
"next-generation wireless sensor networks, constituting distributed autonomous wireless sensors and nodes, can benefit international journal of distributed sensor networks from ad hoc networking technology. however a key requirement for this technology that should be satisfied is an energy efficient and load balanced routing protocol. this paper proposes a new energy efficient and traffic balancing routing protocol based on the well-known ietf aodvv2 protocol, a popular approach for routing in ad hoc networks. the protocol uses a standard generalized manet packet/message format. the e2aodvv2 has been enhanced with battery power efficiency and balancing capability that can detect the nodes that reach a critical battery level in the network and switch the route in order to avoid network fragmentation and to achieve a higher network lifetime. the same behaviour is also fulfilled when bottlenecks are detected in a specific route in terms of traffic load. these additional functionalities are achieved without the need to create new disruptive approach in the protocol messages. e2aodvv2 achieves a higher performance with respect to energy consumption balancing, scalability, network lifetime, and the percentage of failed nodes in comparison to well-known baseline protocols such aodv and dsr and a jitter value very close to the aodv. as shown by the simulation results, the e2aodvv2 routing protocol specifically outperforms in scenarios where the load of the network is not balanced. moreover, the proposed approach can be said to be technology agnostic in that it can be applied to many other reactive ad hoc protocols."
"having calculated all elements of the el, we can now calculate the standard deviation of for a network consisting of nodes (i.e., el ). the value of el will be our energy balancing metric for comparing different protocols; the smaller the el, the more effective the energy balancing capability. (ii) scalability. another interesting metric is the scalability of the proposed protocol in terms of load balancing, that is, if the proposed protocol can balance the load of the network when we increase the number of network nodes (i.e., in our settings). for this purpose we change the number of nodes (the network size) to a maximum number (which is 20 in our setting, based on table 5 ) and calculate the related el for each network size."
"(iii) destination nodes: when a destination node receives rreqs from different routes, by extracting the required information from rreq, it can calculate the route priority for all these rreqs and finally the route which has the best route priority will be chosen. intermediate nodes). it finally reaches the destination node which is responsible for running the precedence function, finding the best route and initializing the rrep. therefore, via this distributed mechanism, the nodes in e2aodvv2 can cooperatively balance the load in the network, in terms of traffic and battery power consumption."
"as the accuracy and robustness of flow field estimation often face challenges of illumination change and large displacement, the coarse-to-fine computation with median filtering has been a requisite for recent optical flow computation approaches [cit] . however, either median filtering or coarse-to-fine computation may cause blurring and oversegmentation of image or motion boundaries. to overcome the limitation, we design the guided filtering to be a postpositive operation of optical flow estimation at each layer of image pyramid. and increment du k, dv k t computed at the current layer. the median filtering is employed to remove outliers in the flow field immediately because the median filter has the remarkable ability of removing outliers in flow field. we then perform the proposed guided filtering to manage the output flow field of median filtering in order to preserve image edges and motion boundaries. finally, the filtered flow field is up-sampled and used as an initialization for computing increment of optical flow at next larger pyramid layer. the iterative scheme is repeated until the pyramid layer of original resolution image is achieved."
"to qualify the factors in eq. 11 we still have to compute the information matrix  i . since the origin node is fixed, its covariance matrix is zero. thus, only the marginal covariance of x i contributes in determining  i . the procedure outlined in the previous section gives us the covariance matrices of the increments x i . hence, we need to remap them through the error function. to this end, we rewrite the error function, highlighting the contribution of the increments:"
"additionally, we summarized the running times of the evaluation methods in table 5, which indicate that the proposed guided filtering scheme may increase the time consumption of the developed model about 5%  10% compared to those of the baseline method. however, the guided filter is able to improve both the accuracy and robustness of the flow field estimation, especially with the benefits of preserving the image edges and motion boundaries."
"our approach partitions the original problem into small chunks based on the trajectory of the vehicle. this is illustrated by the dotted-dashed line in fig 3a. each of these chunks form a small factor graph describing a portion of the problem. because of the local connectivity, each factor will capture a small contiguous portion of the environment that can be seen as a local map. these local maps interact with each other through variables belonging to more than a single local map. these shared variables are illustrated in red."
"for this network, the original aodvv2 chooses the shortest path (i.e., the first path s-c-i-d); however, the new proposed protocol, e2aodvv2, will choose the second path (i.e., s-a-b-f-h-d), which is a stronger alternative in terms of energy efficiency, and even traffic load which is discussed later on."
"in e2aodvv2, when a destination node receives several rreqs from different routes, as in figure 5, it runs a route selection process to determine the best route in terms of energy and traffic parameters."
"in this section, we illustrate our approach based on the three slam features highlighted above. we provide a graphical explanation through figure 3 that shows how our approach works on a simple landmark-based slam problem. in this case, we restricted ourselves to a factor graph involving only binary factors and in the figure we highlight only the nodes that represent variables. the binary factors are represented by the edges, while triangles denote robot poses and circles landmark locations."
"collaborative filtering algorithm is commonly used recommendation method in business-related recommendation system which improves the performance recommendation process. the main drawback of collaborative filtering is its scalability means when the size dataset is very huge, the cost of calculation would be very high. cloud computing tries to solve this problem of high calculation task [cit] . collaborative filtering algorithm has assumptions: people have similar likings & concern which are steady. it is possible to guess their option from their past preferences. collaborative filtering algorithms acquire users' history summary in the form of a ratings matrix consisting of rating given by user to every item. the next step is to compute the similarity between users & discover their nearby neighbours. the most commonly used similarity measure is the pearson correlation coefficient which is standard for cf."
"results. this section presents the simulation results for the comparison between two reactive protocols aodv and dsr that are considered baselines in this work, against our proposed e2aodvv2 protocol."
"as these results show, e2aodvv2 has always better value for both ft first and ft last . also, as expected, the number of failing nodes increases with simulation time. when nodes use e2aodvv2, their lifetime gets extended due to the energy balancing capabilities of e2aodvv2 and shows a lower node failure level (figure 12 ). this feature is even more pronounced when traffic is concentrated in a few nodes ( figure 13 )."
"direct methods iterate the linearization in eq. 7, the solution in eq. 8, and the update step in eq. 9, updating the damping factor and executing backup steps to achieve monotonic convergence."
"where is the number of nodes in route, totbat( ) is the total battery power level of all nodes in route, as presented in (1), and initialbat is the initial battery power level of a node (which is preset to the same value for all nodes). in some scenarios, a route may have a few nodes with a very low energy level, but a high overall energy level; this route should be avoided due to these bottleneck nodes. therefore, the negative impact of minbat( ) (which is the minimum battery power level of route ) should be applied to ( ), as given by"
"however, we can choose a trade-off between energy efficiency and traffic balancing, but this function is flexible and can be customized by giving a higher weight to the energy or traffic parameter."
"for short trajectories the open loop estimate obtained by using incremental approaches (wheel odometry, visual odometry, integration of the accelerometers) is sufficient to obtain a good solution despite using less informative measures (like bearing only data). thus, direct approaches work well on small size problems."
"in data segmenting stage, the user id is separated into various files in which every row has a user id. these files serve as input to the map stage. this stage needs to fulfil two rules. the first rule is that instead of repeatedly define the mapper, the maximum amount of the execution time should be used up in calculation procedure. the second rule is that each mapper should have same end time."
"similarly, we can use the  operator to write down the equation of the error under a small perturbation of the state variables around a linearization pointx as:"
"since these sub-graphs are small, we can obtain a reasonable solution for each of these problems by using a direct method. in absence of global measurements, these local maps are free to float in the space. thus, to determine a unique solution, we need to \"fix\" some variables. in the remainder of this section, we will refer to these variables as the origin (gauge) of the local maps and are illustrated in dark blue in fig. 3c . having a solution for a local map means that we know a gaussian approximation of each variable within the local map, relative to its origin. this is true regardless the sensors that have been used to determine this solution. as an example we can obtain the x  y location of landmarks in a slam problem even if our robot is equipped with bearingonly sensors, because multiple observations are fused by the slam algorithm to obtain the solution. this step is described in detail in section v-a."
"the laboratory toolset we propose and consists of both, hardware and so final appearance, now in the process and full use, is shown in fig 1. students begin the experimenta designing sensing amplifiers an experimental board. they use stan amplifiers (for ppg) and integrated amplifiers (for ecg"
"t represent the mean and variance of input flow filed in the filtering window, respectively. for the issue that a pixel may belong to different filtering windows, the modified guided filtering formulation for flow field computation can be expressed as:"
"(c) jitter. jitter has several sources such as network congestion, route changes, and delaying packets in the buffer queues of the intermediate nodes in a source-sink communication session. usually a jitter buffer is used at sink nodes to counteract the jitter. jitter is a measurement for the quality of the communication; a small jitter indicates a high quality communication by a low latency. as figure 14 shows, the performance of the proposed protocol is better than dsr and very close to the original aodv. this can be explained due to the more complicated algorithm of e2aodvv2 for choosing a route which leads to a higher delay."
"this work is structured as follows: section 2 presents the literature review and related works, section 3 describes the current implementation of the aodvv2 protocol, section 4 introduces the new proposed e2aodvv2 routing protocol, section 5 presents the analysis and simulation results, and finally section 6 presents conclusions and future works."
"phase. an intermediate node may originate an rm, called an route error (rerr), in the two main scenarios. in the first case, the intermediate node does not have a valid route for the destination of a received data packet and consequently the packet is"
"hard qos routing guarantee in ad hoc networks is a nondeterministic polynomial time-(np-) complete problem [cit] . therefore, the current works focus on providing a soft qos routing guarantee for ad hoc networks as a more realistic solution. in this paper, we consider battery power consumption and traffic balancing as a measurement of efficiency."
"ming li received the ph.d. [cit] . he is currently a professor with the school of information engineering, nanchang hangkong university, china. his current research interests include image processing and artificial intelligence."
"recently, the kitti database [cit] has being increasingly popular for evaluating the robustness of the latest flow field computation methods since the dataset was produced by using a moving vehicle. to demonstrate advantages of the proposed guided filtering program for dealing with difficult scenes, we applied all pairs of the basic and modified models on some typical sequences of kitti database. fig. 11 displays reference frames, ground truths and the estimated flow fields of the kitti data, respectively. the revised models with guided filtering operation showed the competitive results compared to original approaches. however, the kitti datasets are actually difficult for the existing optical flow estimation approaches due to challenges of illumination change, large displacement, motion occlusion and complex scene."
"(1) figure 6 shows a network based on the previous sample that has a simpler topology. in this example, there are 8 nodes. the battery power level and the traffic parameter of all these nodes are given in table 1 (the traffic parameter will be introduced later in this paper). based on table 1, the energy and the traffic parameters of all paths of the network in figure 6 are calculated and given by table 2 ."
"the students are introduced with steps of pantompkins algorithm. also, as a part of their homework they work on different algorithms, like ecg beat detection using filter banks, proposed by v. afonso, w. tompkins at all [cit] . initially, they test efficiency of the existing algorithms, off-line, by mit-bih database; and after that in real time using vi. example of real-time qrs detection applying ecg filter banks is illustrated in fig. 7 . during qrs detection, the students note the effects of the algorithmic steps; low and high pass filtering, derivation, moving averaging, adaptive thresholds etc. they, also, clearly recognize the problem of qrs appropriations in real time conditions."
"additionally, we want these local maps to admit unique solutions, which means that the resulting linear system of eq. 8 should be fully determined, once we fix the vertex in the origin. in general this is a challenging problem, however, since we assume that we have odometry, we can construct a solvable sub-graph by selecting a contiguous segment of the robot trajectory (around five meters in our experiments). subsequently, we consider all landmarks that have been seen from within the trajectory portion. depending on the type of sensors, the landmarks can be fully observable or not. for instance, to determine the position of a landmark observed with a bearing only sensor we need two observations from two different robot positions. if we are able to measure also the range, a single observation is sufficient. based on the odometry guess we attempt to determine the position of all observed landmarks. the landmarks whose position cannot be determined from within the local map are discarded together with their measurements. in this way, we obtain a set of factors that lead to a fully specified problem. note that certain landmarks can be discarded from all local maps, since their position cannot be determined in any of them. this results in ignoring some of the information when approximating the initial guess. however, this information will be recovered in the final refinement stage of the algorithm where we employ a direct method on the original factors starting from the initial guess computed by our approach."
"in the presented work we propose the laboratory toolset for physiological measurements based on suggested approach, which we believe can be widely applied and accepted from students researchers."
"the jobtracker gives the map tasks to idle tasktrackers. a map tasktracker accepts the input split for the task, transfers it to key/value pairs, and then executes the map function for the input pairs."
"in this section, we show our experiment result. we implement user based collaborative filtering algorithm on hadoop platform. five computers hadoop cluster was constructed. all experiments were run on this cluster which has one jobtracker (namenode) and remaining four tasktrackers (datanodes). each computer has 4gb ram & intel 2.5ghz cpu and operating system ubuntu 10.10. the software used for the experiments includes hadoop mapreduce framework, java jdk 1.6. the additional hardware required are the mobile device (android 3.0 & above) and switch. in experiments, the netflix data set is used. the netflix dataset consists of around 17770 movies and 4, 80,189 users. the main aim of our cf algorithm is to compare the runtime between standalone and hadoop platform so accuracy is not focussed. we have taken 4 copies of subdatasets as our experimental datasets which include 100 users, 200users, 500 users and 1000 users. similarly, datanode number is considered as 2 nodes, 3 nodes and 5 nodes."
"the procedure outlined above requires to first partition the input into small sub-graphs that lead to local maps. since the local maps need to be merged in a later step, we should prevent the information stored in the original problem from being used multiple times. the relations between variables are modeled by the factors of the graph. hence, it is sufficient to partition the factors of the input problem into different local maps. conversely, the variables can be replicated; a variable that appears in more than one local map becomes a shared variable, that is a vertex separator of the original factor graph."
"the heart is a hollow, muscular organ which through a coordinated muscle contraction generates the force to circulate blood throughout the body. each beat of our heart is triggered by an electrical impulse from special sinus node cells in the right upper chamber of our heart. the electrical impulse travels to other parts of the heart and causes the heart to contract. an ecg records these electrical signals. looking at the ecg you can see that there is a recurrent pattern in this signal. every characteristic of this pattern has certain features and a certain physiological meaning."
"the energy parameter of route, called ( ), indicatesthe priority of the route, in terms of the battery power level as presented by"
"once we have a solution for the local maps, we seek for a global alignment that satisfies all the equality constraints induced by the shared variables. these constraints are depicted with dotted lines in figures 3b, 3c, and 3d. approaching this problem by initializing the original factor graph with the local solutions can fail, because these local solutions will be destroyed during the global optimization. we then reduce the problem of determining a global initial guess to finding a global alignment of the local maps and of the shared variables, while attempting to preserve the structures of the local maps computed before."
"similarly the reduce phase is also terminated early as number of intermediate records are decreased, so the number of input records to work with in reduce function are decreased. it is essential to decide the suitable dimension of the bloom filter. if the dimension of bloom filter is small, it will be unable to filter out redundant records. similarly, if the dimension of filters is bulky, it will create large overhead to design and communicate filters."
"wsns are more specific use-cases for ad hoc wireless networks that today are autonomous in nature and support flexible topologies to deliver fast and everywhere connectivity. a self-configuring wsn, constituting distributed autonomous wireless nodes, can benefit from a multihopping ad hoc approach, in which nodes operate not only as a transceiver but also as a router and forward packets to other nodes in the network which may not be within direct transmission range of each other. therefore, by operating in \"ad hoc\" mode, packet travelling from a source node toward a destination node may pass through multiple nodes to reach its destination."
"bagging. the bagging classifier [cit] creates various data subsets (by selecting instances with normal and renewable distribution) from main training data using the bootstrap method [cit] . each generated data set is applied to a classifier called a base classifier and finally the final output is obtained by voting from all the base classifiers. similar to boosting classifiers, bagging classifiers are considered combinatorial classifiers, except that in boosting the base classifiers are ordered hierarchically and training of the classifier (weight setting) in each stage is done using the output of the previous stage classifier. in the bagging method, however, the base classifiers conduct the learning process in a parallel manner [cit] . bagging's robustness against noise is higher than boosting and application of this method is recommended when data are different from one another [cit] . using nonlinear and unstable classifiers such as neural networks and dt as base classifiers can improve the efficiency and performance of the bagging algorithm [cit] . the dt algorithm is an efficient and suitable method for generating comprehensible knowledge structures and it is used in a wide range of classification problems due to its high flexibility. however, in most cases, the low classification accuracy and extensive calcul ations makes this method not a very ideal algorithm for classification problems."
"other sections of this paper are structured as follows: section 2 provides a brief introduction and explanation of the structure of the most basic and combinatorial classifiers as well as evolutionary algorithms. in section 3 the practical results of the application of the classifiers and the evolutionary algorithms analyzed in the previous section will be evaluated and compared for designing a bmi-based system as well as introducing the databases of the used eeg signals. finally, section 4 contains our conclusions."
"as the problem in (18) is highly nonlinear, it is solved numerically using the interior point method. the optimized solution is denoted as p ."
"without loss of generality, each packet in the secondary or primary network is considered successfully transmitted if and only if the transmission time from the source to the intended receiver is less than a given timeout threshold. as a consequence, the event in which a packet is transmitted successfully is defined by"
"in order to implement an evolutionary-based bmi system, evolutionary algorithms were used in the feature selection and classifier design stages. according to the literature review, the three single-objective algorithms ga, de and pso, as well as the multi-objective nsga-ii algorithm, were the most employed algorithms in studies on bmi systems. considering the multi-objective nature of feature selection process, the nsga-ii algorithm was used for selection of optimal features. the employed classifier was a multilayer artificial neural network, in which the weight coefficients were determined through evolutionary algorithms. weight coefficients were determined using single-objective algorithms such as the de, pso, hs, iwo, bbo and tlbo algorithms and bi-objective algorithms such as the nsga-ii, nshs, nsiwo, nsbbo and nstlbo algorithms, as well as some of their improved versions. in order to employ the de algorithm, five mutation strategies, namely de-1, de-2, de-3, de-4 and de-5, were used. considering the two datasets, de-4 demonstrated the best performance. in the case of the bbo algorithm, linear (bbo-1), third-and fifthorder quadratic (bbo-2 and bbo-3), and sinusoidal (bbo-4) migration models were used. according to the results, the sinusoidal model outperformed other migration models. regarding the hs algorithm, a simple hs algorithm was compared to one of its improved variants known as ghs. in all single-and biobjective cases as well as for both datasets, ghs and nsghs algorithms achieved higher accuracies compared to hs and nshs. finally, as an overall conclusion, it can be stated that iwo and nsiwo evolutionary algorithms, independent of the employed data type, achieved the best performances, followed by tlbo and nstlbo as the second best algorithms. finally, a summary of past articles in the field of eeg-based bmi systems are presented in appendix b (tables b1-b5)."
"we have assumed that all channels are constant for the duration of a packet transmission but may change independently thereafter. as a consequence, interarrival time of packets at the sr buffer is an identically and independently distributed (i.i.d.) rv with a general distribution, and so are the packet transmission times. therefore, the traffic to the sr buffer can be modeled as a gi/g/1 queueing system [cit] ."
"in brain-machine interface (bmi)-based systems [cit], users are capable of interacting and communicating with their surrounding environment by analyzing and processing their brain activities without making any movements [cit] . a bmi-based system consists of four parts: (1) signal acquisition (2) signal processing (3) observation classification (4) control interface [cit] . bmi studies include a wide range of topics such as screen pointer control [cit], control and navigation of wheelchairs and robots [cit], and, most recently, neuro-marketing [cit] ."
"where f is the frequency offset between the waveforms sent from two adjacent transmit antennas which is fixed to k/t to attain approximate orthogonality [cit], where k is a positive integer. when the operating frequency is"
"in this section, waveform design for the scnr maximization problem is introduced for cognitive mimo radar systems. the waveforms are constructed as lfm signals where the starting frequencies and bandwidths for each of the lfm signals will be optimized. the optimization problem with the constraints of the allowable range of operating frequency and total transmit energy is presented. the method to solve the optimization problem is given subsequently."
"optimization algorithms can be categorized into two major groups: classic optimization algorithms and evolutionary and swarm intelligence-based optimization algorithms. as their main advantages over classic optimization algorithms, evolutionary algorithms are self-regulatory, robust to noise and sudden changes in parameters, applicable to non-continuous, non-differentiable problems, easily implemented and require no complex knowledge of mathematical topics [cit] . evolutionary algorithms can be categorized into several types, including genetic algorithms (gas), evolutionary strategies (ess), genetic programming (gp), evolutionary programming (ep), swarm intelligence (si) and other algorithms inspired by nature. these algorithms are among the most important tools for solving optimization problems in various fields of science such as engineering, economics, and medicine."
"as shown in figures figures 7-10, migration model 4 (bbo-4) achieved more favorable results on both datasets and in both single-and bi-objective cases. also, according to the results, the single-objective pso algorithm and the biobjective nsga-ii algorithm achieved average performances. as an overall conclusion, it can be stated that the iwo and nsiwo evolutionary algorithms, irrespective of the employed data type, achieved the best performances, followed by tlbo and nstlbo as the second best algorithms."
c ijk x i x j x k (6b) where c i s are the coefficients of the entire structure and their best values are selected using the mean square error (mse) minimization method [cit] .
"for brevity, we discuss only the scenario where the pu-tx 1 transmit power fig. 4 . impact of the pu-tx 2 transmit power in region ii, i.e., p (p, 2), on the throughput of the ccrn for identical channel mean power, i.e.,"
"hop: similar to the first hop, in the second hop, the sr uses the adaptive transmit power policy given in (30) to transmit packets to the su-rx. the timeout probability for a packet transmitted in the second hop can be derived as"
"], then the average transmission rate of the sr is equal to the average arrival rate of packets at the sr buffer. hence, the end-to-end throughput can be determined as"
"lemma 1: let a, b, c, and d be positive constants. further, let x 1 and x 2 be independent and exponentially distributed rvs with mean  1 and  2, respectively. then, rv z defined as (20) has a cdf and a pdf, respectively, given by"
"here, performance analysis for the considered system model is presented. we first consider adaptive transmit power policies for the su-tx and sr. specifically, the cdf, pdf, and moments of the packet transmission time for the first and second hops are derived. finally, performance in terms of end-to-end throughput, end-to-end transmission time, and stable condition for the sr operation are presented."
"in the above equations, z is the result in proportion with w weights, p is the estimated probability in each stage and n is the number of observations [cit] ."
"one of the most commonly used non-invasive methods is the use of eeg signals. in this method, electrodes [cit] are attached to the skull of the living organism for signal acquisition. the most important advantages of this method include safety, low cost, ease of use, high temporal resolution and applicability in real-time systems [cit] . as for the major disadvantages of this method, low spatial resolution, contamination of the signal with a variety of artifacts and noise, and low snr can be mentioned. based on the employed signal type, eeg-based bmi systems can be classified into the following four categories: (1) smr-bmi, (2) erp-bmi [cit], (3) slow cortical potential-bmi (scp-bmi) [cit] and (4) steady state visual evoked potential-bmi (ssvep-bmi) [cit] . the second category itself comprises several important subcategories such as p300 evoked potential, visual n100 and n200 [cit] . the major distinction between the above mentioned categorizations lies in the physical characteristics of signals, stimulation type and procedure, and ultimately, location and time of signal acquisition. the general block diagram of a bmi system is shown in figure 1."
"finally, case 6 results in the worst performance compared with the other considered cases. this can be explained by the fact that the sr and su-rx suffer from strong interference caused by the pu-tx 1 and pu-tx 2, respectively. therefore, there may be many packets in both hops of the ccrn being timed out, which leads to low end-to-end throughput."
"to overcome these problems, ensemble models, such as dtbased bagging are used as the base classifier [cit] . this is called random forest [cit] . the random forest algorithm generates each tree independently from the other. the two parameters affecting the performance of random forests include the number and the depth of the trees. although random forest and bagging algorithms are relatively unknown in the bmi field [cit], the application of these classifiers in the recent years indicates their relative success both in accuracy [112, [cit] and the time of decision making [cit] compared to"
"this database includes three-class motor imagery-based eeg signals associated with dataset v of bci competition-iii [cit] . the data are gathered from three normal people in a four-stage non-feedback sampling with frequency of 512 hz by placing 32 electrodes based on ordering system of 10-20 [cit] of the idiap research institute [cit] . in these experiments, instructions are divided into three major categories: (1) imagination of repetitive self-paced left hand movements (label 2), (2) imagination of repetitive self-paced right hand movements (label 3), and (3) generation of words beginning with the same random letter (label 7). the instructions are announced randomly and running time of each order is 15 s. the implementation of each stage takes 4 min and the break time between each stage is 5-10 min. all four stages for each person were completed in one day."
"it is noted that the su-tx must control its transmit power p (s, 1) to satisfy the timeout probability constraint  (p, 1) th of pu-rx 1 given in (4) . in addition, the su-tx transmit power is limited in practice; hence, it may be constrained by a peak transmit power p (s, 1) pk"
"considering the multi-objective nature of a feature selection process, the nsga-ii algorithm was used for the selection of optimal features. the input data comprised two categories: (1) 25% of the total observations in the second session for the second subject regarding the first database (qusmr) with a dimension of 96, which was used in subsequent procedures, and (2)"
"in this section, the evolutionary algorithms reviewed in section 2.2 and also some of their enhanced models were compared and assessed. these algorithms were used in two stages, i.e. feature selection and classifier design. the employed classifier was a multilayer artificial neural network, where the weight coefficients were determined through evolutionary algorithms."
"in order to design an evolutionary-based neural network classifier, the following two datasets were used: (1) qusmr dataset, and (2) 25% of the total observations of the third subject in the second database (qup300). the employed neural network included one hidden layer, in which five neurons were considered. weight coefficients were determined by singleobjective algorithms de, pso, hs, iwo, bbo and tlbo, by bi-objective algorithms nsga-ii, nshs, nsiwo, nsbbo, and nstlbo, as well as some improved models of these pareto representation of the number of optimal features based on the classifier error for the qusmr dataset, as well as subject1, subject3, subject7 and subject8, regarding the second database."
"proof: because the interarrival time of packets is i.i.d. with a general distribution, the transmission time of packets is also i.i.d. the packet traffic at the sr can be modeled as a gi/g/1 queueing system. using the stable condition given in (19), we can deduce the desired stable condition for the sr buffer as"
"in this equation,  iter is the standard deviation of round iter,  final is the final standard deviation, and n is the value of nonlinear modulation. the right value for selecting n is recommended to be 3 [cit] .  initial,  final and n are the three parameters affecting the performance of the iwo algorithm. a descending change of the dispersion standard deviation ( iter ) from  initial to  final with the velocity of n creates a good balance between the two concepts of exploration and exploitation which ultimately results in the successful optimal global search around the local optimums in this algorithm [cit] . improved versions of this algorithm have been introduced for optimization [cit] and classification [cit] problems in recent years."
"as a cognitive mimo radar system has the ability of learning, the prior knowledge of the environment state can be obtained from previous measurements. using a specific environment database which contains the statistics of the target and clutter impulse responses and noise, the statistics of the target return x t and the clutter return x t can be derived. next, we discuss the statistics of the target impulse response h t [ n], the clutter impulse response h c [ n], and the noise z."
"in this paper, we have analyzed the performance of a ccrn with a buffered relay. in particular, we have assumed that packets in the primary network and secondary network are subject to timeout constraints. in addition, we have assumed that the communication channels and interference channels undergo rayleigh fading. the transmit power of the su-tx and sr is subject to both the peak transmit power constraint and the timeout probability constraint of the pus. on this basis, adaptive transmit power policies for the su-tx and the sr have been investigated. the cdf, pdf, timeout probability, and moments for packet transmission time in each hop have been derived. moreover, by employing the gi/g/1 queueing model, the performance analysis in terms of the end-to-end throughput, end-to-end transmission time, and stable condition for the sr operation has been provided for the considered system. numerical results have been presented to quantify the impact of pu-tx transmit power and channel mean power of the interference links on the performance of the ccrn. using the same approach as given in [37, (14) ] for (67), we finally obtain the cdf of z as in (63)"
". accordingly, the timeout probability constraint of the primary network may not be violated. thus, the su-tx and the sr can increase their transmit power to reduce the packet transmission time. as such, the number of packets that suffer a timeout in the secondary network may be decreased; hence, the end-to-end throughput of the ccrn is increased. this special case of having poor interference links can be related, for example, to the scenario where operating frequencies are switched between the two hops of the secondary network to reduce the mutual interference between the primary and secondary networks, as well as to increase the system performance."
"eeg signals [cit] have a frequency range of 1-100 hz and 10-100 volt [cit] . these signals are nonlinear, non-gaussian, uncorrelated and random, which are considered among their most important characteristics. various methods have been proposed for extracting features from these signals. these methods can generally be categorized into two groups: linear methods, such as independent component analysis (ica), fast fourier transform (fft), eigenvector, autoregressive (ar), wavelet transform (wt), wavelet packet decomposition (wpd), power spectral density (psd); and nonlinear methods, such as correlation dimension (cd), hurst exponent (h), largest lyapunov exponent (lle), different entropies, higher order spectra (hos) and fractal dimension (fd) [cit] . the feature extraction output or sometimes the raw signal (often in p300-based systems) is considered as the input to the classifier, which is one of the most important components of a bmi-based system. in recent years, various methods were introduced to improve the performance of bmi-based systems. the evolutionary algorithms are among the most commonly used methods of this kind."
"where accu100%, accu50% and accu25% represent the classification accuracy for 100%, 50% and 25% of data volume, respectively. the lower the sdv values, the higher the stability of classifiers over data volume changes. the mean validation accuracies of classifiers for the three subjects regarding motor imagery data with respect to the data volume, and the mean validation accuracies of classifiers for the four subjects regarding p300 data with respect to data volume are presented in tables 2-4. the performance of each classifier is assessed merely based on 100% of data volume. concerning the base classifiers and according to the results, lda and svm classifiers employing the kernel method were able to outperform their linear versions when dealing with motor imagery-based signals. however, the use of the kernel approach in these classifiers results in decreased validation when dealing with p300 signals. therefore, in the case of database 1, lda and svm classifiers were implemented using quadratic and polynomial-order 100 kernels, whereas a no kernel (linear) was employed in the case of database 2. as previously mentioned in section 2.1.8, the k-nn classifier presents varying performances on different data sets. for instance, in this study, although k-nn performs as the best classifier with respect to motor imagery-based signals, this was not the case for p300 signals. according to the data in database 1, the most appropriate value of k for the k-nn classifier for all subjects and sessions was 3, while in the case of database 2, k assumed different values depending on the subjects. note that the results are presented in detail in appendix a (tables a1-a21). the designed mlp classifier for both databases included one hidden layer in which the appropriate number of neurons was obtained 10 and 5 through trial and error for databases 1 and 2, respectively. the number of centers and codebooks in the rbf-nn and lvq classifiers for database 1 were considered 600 and 60, respectively. however, in the case of database 2, these values varied depending on the subject. all the employed lvq classifiers were of type lvq1 with a learning rate of 0.01. second-order polynomials were used to design the pnn classifier. according to three boosting classifiers, namely, adaboost, logitboostand gentleboost, were assessed. in the case of both databases, the learning rate was 0.1 for all classifiers, with dt being the base learner. 1000 learners were considered for motor imagerybased signals of the first database. according to the results of table 3, gentleboost and adaboost classifiers achieved the maximum and minimum validation accuracies of 87.4543% and 84.9967%, respectively. in the case of p300 signals of the second database, the most appropriate number of learners for each classifier was different depending on the subject. regarding these signals, logitboost and gentleboost achieved the maximum and minimum validation accuracies of 87.3150% and 86.6450%, respectively. moreover, in the case of both databases, adaboost (database 1: 2.9297% and database 2: 0.5858%) and logitboost (database 1: 4.1389% and database 2: 0.91741%) scored the minimum and maximum sdv values, respectively. the results of boosting classifiers are shown in table 3."
the idea of genetic algorithm is built on the two principles of crossover and mutation. the weakness of this algorithm is its inefficiency in multi-objective problems. to solve multi-objective problems using a genetic algorithm one can use the classic (the sum of weighted functions) in which the objective functions will turn into a single-objective function with the help of the assigned coefficients.
"the advantages of multiple-input multiple-output (mimo) radar have drawn considerable attention in the last decade [cit] . mimo radar systems employ multiple antennas on both the transmit and receive sides. the antennas can be either co-located or widely separated. geometry gains can be obtained for the former since the antennas are located in several different directions with respect to a target, while waveform gains can be produced for the latter by sending different waveforms with different antennas."
"the lda algorithm is also used for dimension reduction and feature selection [cit] . the most important features of this classifier which make it more applicable in real-time bmi systems are its simplicity and low level of computation, resulting in its stability [cit] . however, its performance is reduced in the presence of noise and outliers. it also suffers from dimensionality when used in problems with few training data. improved versions of this classifier such as fisher lda (flda) [cit] and different types of regularized lda (rlda), such as bayesian lda (blda) [cit], stepwise lda (slda) [cit], automatic stop-swlda (asswlda) [cit] and shrinkage lda (sklda) [cit] have been proposed to overcome these problems [cit] . one of the main problems of lda when working with eeg signals is its linearity [cit] . the idea of using a kernel is one of the recommended solutions. the lda classifier and its improved versions are used in bmi problems as the base classifier [cit] as well as the decision-maker in optimized filter design [cit], and in selection of optimized channels and electrodes [52, [cit] ."
". rbfnns [cit] are strong approximators which use only limited information. using only one hidden layer with a sufficient number of neurons, these networks are able to approximate any continuous function with any degree of accuracy. the basic structure of an rbf-nn consists of three layers. no process is performed in the input layer. the second or the hidden layer provides a nonlinear mapping between the input space and a space of usually larger dimensions, which plays an important role in converting non-linear patterns to linear separable patterns. finally, the third layer generates the total weight with an input. when an rbf-nn is used for classification, a hard limiter or a sigmoid function can be applied on output neurons. in general, the major characteristic of this network is the processing operation in the hidden layer."
"where tpr and tnr represent true positive rate and true negative rate, respectively. the number of population for all single-and bi-objective algorithms was 200, [cit] and 1500, respectively. different strategies on mutation operator have been presented to employ the de algorithm [cit] . in this study, five typically and widely used strategies were employed. table 7 illustrates these strategies."
"then, both of the optimized waveform parameter vector p opt and receiving impulse response vector h r,opt are achieved. the optimized waveform s opt can be obtained by plugging p opt into (11)."
"different methods have been put forward in order to increase the accuracy and improve the performance of the nb algorithm [cit] which resulted in introduction of algorithms such as the tree-augmented naive bayesian (tan) and bayesian network-augmented naive bayesian (ban) [cit] . despite accuracy and efficiency of nb classifiers in classification of motor imagery eeg signals since they do not need much training data [cit], these classifiers are less frequently used in bmi-related problems [cit] ."
"(24) where seed min and seed max are the minimum and maximum number of seeds, f min and f max are the minimum and maximum amounts of fitness function in the population, and f i and seed i are the fitness function and the number of seeds in the ith plant, respectively. it is recommended that the maximum number of seeds is selected between three and five and the minimum is selected to be zero [cit] . the produced seeds are distributed randomly in the problem space. the distribution function is a normal distribution with zero mean and standard deviation of  which is descending from an initial value of  initial to a final value of  final in each iteration. the following equation specifies the standard deviation in each iteration."
"this study mainly aimed to present a comprehensive theoretical-experimental survey in the classification methods for figure 10 . demonstration of test accuracy and confidence interval of bi-objective ea-based neural networks for the qup300 dataset. bmi-based systems and their optimization approaches using evolutionary algorithms. the first part of this paper reviewed a wide range of different types of base and combinatorial classifiers as well as evolutionary algorithms. the second part compared and evaluated those classifiers and evolutionary algorithms in a practical manner. in a practical implementation, the database provided by bci competition iii, specifically the features of the extracted three-class power spectral density associated with that databse, as well as the two-class p300 signals provided by bci group of epfl university were used."
"when the receiver impulse response h r is fixed, the problem in (12) can be recast to (6) and h c in (9), as described in the following lemma."
"where r is a random vector between zero and one, x teacher is the best member of the population in iteration i which tries for the mean of the class to tend towards his own, mean is a vector including the mean of the class results, and t f is the learning factor which is assigned to 1 or 2 randomly with the same probability. in fact, if the learning factor is 2, the distance from the mean increases and the learning velocity and conv ergence rate accelerates [cit] . application of tlbo in pattern recognition has received much attention in recent years. reference [cit] presents a new method for improving feature selection algorithm based on rough set theory using the tlbo algorithm. moreover, [cit] address the training of a feedforward neural network, a polynomial neural network and a specific type of high order-nn [cit] called a pi-sigma neural network [cit] with the help of evolutionary algorithms of tlbo, pso, de and ga. based on the results, in all cases, neural network algorithms trained by tlbo created better results. one of the most important advantages of the tlbo algorithm is the high convergence rate [cit] . this feature turns into a shortcoming in high-dimensional problems causing premature convergence. in order to solve the problem of premature convergence as well as improving exploration and exploitation, improved versions of this algorithm have been introduced. in [216, [cit], the teacher learning sectors and the learning rate selection are changed in order to improve the performance of the standard tlbo. a section is also added to the algorithm called self study. if the number of students whose fitness is below average is high, then the teacher should make more effort to improve the class. in this case, the class is divided into several sections and each section is assigned to a teacher. a high t f increases the conv ergence rate and reduces the exploration and vice versa [cit] . that is why t f is determined comparatively according to the problem."
"is used to process the receiver signal r at the receive end [cit], as shown in figure 1 . thus, the scnr at the output of the receive filter is defined as"
"relation r + 1 2+cd [cit] was used in all bi-objective algorithms such as nsiwo, nstlbo and nsghs, which, according to their base algorithms, require the best or worst member of their population to be determined. r and cd represent the rank, i.e. the front number and crowding distance of each member in the population, respectively. the smaller the obtained value, the better the respective member, and vice versa. in the case of single-objective and bi-objective algorithms, the training error was considered the primary objective, while for the latter, the secondary objective is defined according to the following equation: demonstration of test accuracy and confidence interval of single-objective ea-based neural networks for the qup300 dataset. table 8 . bbo migration models."
"two main groups of the base and combinatorial classifiers were analyzed and two types of combinatorial classifiers, i.e. bagging and boosting, were considered for practical implementation. the bagging classifiers were implemented using five base classifiers lda, dt, lr, svm and k-nn. in contrast to the bagg-knn, bagg-svm, bagg-lda and bagg-lr classifiers, only bagg-dt (random forest) classifier demonstrated a significant validation accuracy among bagging classifiers compared to their base classifiers. the reason may be the insensitivity of base classifiers such as k-nn and svm to perturbation of the training data. these classifiers are referred to as stable classifiers [cit] . the experimental results of this study verifies the belief that unstable and weak classifiers such as dt achieve considerably better performance compared to stable classifiers when being run using the bootstrap method [cit] . among bagging classifiers, bagg-lr and bag-knn with sdv values of 1.9999% and 0.0815%, were the least sensitive classifiers against changes in data volume in the case of motor imagery as well as p300 signals, respectively. moreover, by averaging the sdv values of both databases, the bagg-lr classifier achieved the best stability to data volume. regarding boosting classifiers, three classifiers were implemented: adaboost, logitboost, and gentleboost. overcoming the weaknesses such as sensitivity to noise and outliers in adaboost classifier can be considered one of the major objectives in design of classifiers such as gentleboost and logitboost [cit] . while the latter two have been of less interest in bmi-based systems compared to adaboost, according to the results obtained in this study, gentleboost and logitboost achieved the highest validation accuracies. adaboost demonstrated the minimum sensitivity against data volume with sdv values of 2.9297% and 0.5858% for motor imagery-based and p300 signals, respectively. according to the literature review on base classifiers, lda, svm and mlp can be regarded as the most widely used classifiers in bmi systems as well as evolutionary-based bmi systems. in the present study, 10 base classifiers, namely dt, lda, svm, lvq, nb, mlp, lr, k-nn, rbf-nn and pnn, were investigated and implemented. despite their high computation complexities, pnn and lvq classifiers did not provide desirable performances. according to the exper imental results, occurrence probability of overfitting in these classifiers, specifically in pnn, is greater than that of mlp. selecting an appropriate number of codebooks for lvq as well as the number of neurons in the hidden layer for pnn are two major factors affecting the success of these classifiers. with a validation accuracy of 57.4804%, lvq is the least accurate classifier for the database of motor imagery-based signals. the dt classifier achieved the lowest validation accuracy of 76.9279% in the case of p300 signals. according to the results obtained from both databases, the application of combinatorial methods such as boosting and bagging on the dt classifier is recommended in order to improve its performance. the k-nn classifier demonstrated varying performances depending on the input data. in this study, k-nn performed as the best base classifier with a validation accuracy of 99.9673% when dealing with motor imagery signals. however, this was not the case in p300 signals. lda and svm were recognized as the best base classifiers achieving high accuracies in both cases of motor imagery and p300 signals. in the former, svm and lda achieved accuracies of 98.2026% and 99.0980%, respectively, while in the latter, these values were 88.3508% and 89.0904%, respectively. in the case of smr signals, lda and svm classifiers employing kernel approach outperformed their linear counter parts. however, the use of the kernel approach in these classifiers reduced their validation accuracy in the case of p300 signals, which can be accounted for by the complex nature of the three-class learning model of the first database compared to the two-class learning model of the second database. by calculating the average sdvs of both databases, nb classification was shown to achieve the best stability against the changes in data volume."
"the rest of the paper is organized as follows. in section 2, the signal model of the cognitive mimo radar is introduced. in section 3, the lfm-based waveform design for limited maximum allowable frequency band and total transmit energy is presented, and the algorithm for solving the optimization problem is given. in section 4, we show the superior scnr performance of our designed waveforms over the fs lfm signals through numerical examples. the effects of the number of transmit antennas are also analyzed. finally conclusions are drawn in section 5."
"classifiers can be considered as one of the most important topics in regard to bmi-based systems. the output of this part has a major impact on the success or failure of the entire system. in general, a bmi-based system can be considered as a pattern recognition system in which the features obtained from brain signals are used as a pattern [cit] . in most studies, classification accuracy is used as the only criterion to evaluate a classifier. however, other criteria such as sensitivity, specificity, discriminant power, youden's index and computational time are also used to evaluate the performance of a classifier [cit] . we will review a number of classifiers which are used in most studies on bmi-based systems in the following."
"given the results in section iii-b and c, we are now in the position to derive performance measures for the ccrn such as the stable condition for the sr operation, end-toend throughput, and end-to-end average transmission time as follows."
"generally, bmi-based systems can be classified into three categories: partially invasive [cit], invasive [cit] and non-invasive. non-invasive bmi systems can further be categorized into various types based on the technique used, including electroencephalography (eeg), magnetoencephalography (meg), functional magnetic resonance imaging (fmri), near-infrared spectroscopy (nirs) and positron emission tomography (pet) [cit] . the use of eeg-based bmi systems are more common due to their lower risks compared with pet, lower costs compared with pet, fmri, and meg, high temporal resolution compared with pet, fmri and nirs, and high data transmission rate compared with event-related optical signal (eros) and nirs [cit] . they have also been developed more than other systems in recent years."
"algorithms. for all implementations, 70% of the data were considered as the training and the remaining 30% as the test data. all the employed bi-objective algorithms were implemented on single-objective base algorithms through the application of non-dominated sorting and crowding distance concepts."
"the bmi is considered as the communication path to control systems and interactions between the brain and its surrounding environment. according to the classic definition, the main objective of designing such systems is to provide a non-muscular communication path for individuals suffering from such diseases as amyotrophic lateral sclerosis (als), brain stem attacks, brain-spinal damage and cerebral palsy [cit] . in their new definitions, such systems are used for processing brain signals and determining different states of the mind in healthy individuals throughout a given task so that the system and user may adapt and the system performance may be improved [cit] . therefore, the following three general categories are presented for classification of bmi-based systems according to this definition. [cit] : (1) active bmi: brain signals are consciously and directly obtained without the application of any external stimulation; (2) reactive bmi: brain signals are consciously and directly obtained through the application of an external stimulation; (3) passive bmi: the users are not required to control their brain activities as brain activities are extracted and processed during the assigned tasks. the research on passive bmi has recently grown and attracted more attention [cit] . another highly important and common categorization of bmi-based systems, addressed in numerous books and articles, is based on the methods of eeg signal acquisition, according to which bmi systems are classified into three categories, namely partially invasive, invasive and non-invasive. in partially invasive and invasive systems, intracranial electrodes are located out of the gray cortex and inside the gray cortex in the brain, respectively. good signal-to-noise ratio (snr), high amplitude as well as less sensitivity to noise and artifacts are some of the most important features of these two methods. however, they are rarely used on human beings due to high risk and ethical issues [cit] . electrocorticography (ecog) is considered a partially invasive method. compared to other methods, non-invasive methods have been utilized more in recent years, since they impose less risk than partially invasive and invasive methods. non-invasive methods of signal acquisition can be categorized into two groups of exogenous and endogenous procedures [cit] . in exogenous procedures, the user can only send commands to the system if an external stimulation has occurred. in endogenous procedures the user is able to arbitrarily apply mental and cognitive commands to control equipment. therefore, in endogenous procedures the user can send the commands in whatever way they desire without needing an external stimulus."
"in the following sections, in order to further understand the mechanisms and efficiency levels, all aforementioned methods in sections 2.1 and 2.2 are pragmatically compared and assessed based on two relatively widely used bmi systems, namely, smr-bmi and erps-bmi. the employed databases are initially introduced. then, the results of employing the base and combinatorial classifiers are presented and compared. ultimately, all the reviewed evolutionary algorithms and some of their improved models are compared in terms of their capabilities in feature selection and the design of classifiers (all implementations are conducted using matlab software)."
"from (2), the statistic of the clutter return vector x c is determined by the statistic of the clutter impulse response vector h c, as described in lemma 1. http://asp.eurasipjournals.com/content/2014/1/89"
"the primary network and the secondary network can cause strong interference to each other due to high channel mean power of the interference links, i.e.,"
"the sr transmit power p (s, 2) is also restricted by the timeout probability constraint of the pu-rx 2 given in (8) and peak transmit power p (s, 2) pk as"
"finally, five bagging classifiers, bagg-dt, bagg-knn, bagg-lda, bagg-lr, and bagg-svm, were analyzed. the number of bootstraps in all bagging classifiers was considered to be 100. as demonstrated in table 4, among the five aforementioned bagging classifiers, merely bagging-dt (random forest) was able to score a significant validation accuracy compared to its base classifier (dt). the improvements for the motor imagery-based signals and the p300 signals are, respectively, 16.7091% and 8.25%, while no significant changes were observed in other classifiers. the reason may be attributed to the appropriate interactions of unstable and weak classifiers such as dt with bagging classifiers. moreover, bagg-lr with 1.9999% on the first database tables 5 and 6 compare the mean equality probability of classifiers' validation accuracy in pairs considering the database; the closer the probability to one, the higher the mean equality probability of the two classifiers' validation accuracy. based on the results, in both databases, the mean of validation accuracy for gentleboost-logitboost, adaboost-mlp, and lda-svm classifiers are significantly equal. also, the probability value of bagging classifiers with their stable base classifiers is equal to 1, except for bagg-dt (in this article, significance level is 0.05)."
"the task of the su-tx is to select a transmit power level such that it can exploit the licensed spectrum of region i as much as possible but does not cause harmful interference to the pu-rx 1 . given the related constraints in (4) and (11), an adaptive transmit power policy for the su-tx is derived as follows."
"proof of lemma. see ( s wang, q he, z he, rs blum, waveform design for mimo over-the-horizon radar detection, submitted to ieee transactions on aerospace and electronic systems)."
consider a cognitive mimo radar equipped with m transmit antennas and l receive antennas. each of the transmitted waveforms is assumed to be narrowband. the discrete-time waveform transmitted by the mth transmit antenna is denoted by
"a packet is considered being successfully transmitted if its transmission time t (p, 1), given in (1), is below a predefined timeout threshold t p out . in other words, the probability of unsuccessful packet reception for pu-rx 1, known as timeout probability p (p, 1) out, can be formulated as"
"in the second hop, the sr also adjusts its power to forward the packets in its buffer to the su-rx, where packets are served in first-come-first-serve order. it is noted that the buffer length of commercial devices is often very large nowadays. this fact can be adopted to assume that the length of the sr buffer is infinite. similar to the first hop, the time it takes to transmit a packet in the second hop is expressed as"
"we can now apply similar manipulations as in the first hop to derive the pdf and the moments of t the first and second moments of t (s, 2) are obtained by"
"here, we derive the timeout probability, the pdf of packet transmission time, and the first and second moments of packet transmission time from the su-tx to the sr."
"the iwo algorithm [cit] works based on the natural characteristics of weeds such as seed production, growth and struggle for survival in a colony of weeds. the steps that should be taken in order to simulate the behavior of weed can be summarized as the following: determining a limited number of initial seeds randomly in the problem space; producing new seeds considering the value of the fitness function of each plant; distributing produced seeds in the search space randomly, continuing the aforementioned operations until the highest number of weeds are obtained and after removal of plants with the lowest fitness function; and finally going on with this cycle until reaching a pre-specified stopping criterion [cit] . the amount of seeds produced for plant is calculated by the following equation [cit] ."
"motivated by all of the above, in this paper, we study the performance of a ccrn with the sr operating in full-duplex mode. more specifically, we consider the case that the sr receives packets from the su-tx, decodes, and then forwards them to the su-rx over orthogonal channels. to assure the desired performance of the pu, the su-tx and sr must control their transmit power to meet both the timeout probability constraint of the pus and the peak transmit power constraint of the sus. given these settings, the performance analysis for the considered system is developed. main contributions in this paper are summarized as follows."
"the de algorithm [cit] by storn and price as a powerful and fast method for solving optimization problems in complex search spaces. unlike other evolutionary algorithms which use random search, de's searching method is based largely on greedy search [cit] . in the de algorithm, there are three factors of mutation, crossover and selection, and three control parameters of size of the population, scale factor and the crossover probability. in terms of convergence, the de algorithm is not sensitive to the param eters but based on the results, right selection of parameters leads to more optimum answers [cit] ."
"the main goal of the lda algorithm is to use a hyperplane to separate classes and find directions in the feature space such that the intraclass variance of features increases and the inter-class variance decreases. in a more scientific language, lda aims to find the transition matrix (z) so that the inter-class scatter matrix (s inter ) is minimized and the intra-class scatter matrix (s intera ) is maximized [cit] ."
"the single-objective evolutionary algorithms perform well, however, real-world problems are multi-objective in which objectives are related, or in some cases, are not. thus, using multi-objective evolutionary algorithms is very popular. [cit], the results of which show a relatively good performance compared to other algorithms. some of these algorithms are the strength pareto evolutionary algorithm (spea) [cit], the improved strength pareto evolutionary algorithm (spea ii) [cit], the pareto-archived evolution strategy (paes) [cit], the pareto envelope-based selection algorithm (pesa) [cit], region-based selection in evolutionary multi-objective optimization (pesa ii) [cit] and non-dominated sorting ga-ii (nsga-ii) [cit] . most of these algorithms use an elitism-based strategy [cit] . the application of evolutionary algorithms in bmi-based systems can be categorized into four major groups of filter design [57, 58, [cit], feature selection [61, [cit], channel selection and optimal electrodes [52, 58-60, 175, 176], parameter selection and optimal classifier structure [9, 68, 69, [cit], and finally improving the performance of control tools [cit] ."
"this method has two fundamental weaknesses. firstly, it cannot search the entire problem space and, secondly, it needs high accuracy for specifying the range of the coefficients. the multi-objective optimization algorithms only identify a set of responses rather than finding a single one, called pareto front, none of which have absolute superiority over the others. for example, a multi-objective optimization algorithm as a second edition of a multi-objective genetic algorithm with non-dominated sorting was introduced by kalyanmoy [cit] . this algorithm is created by adding two new operators of non-dominated sorting and crowding distance to the common genetic algorithm. the former sorts the members of the population based on the concept of non-dominated sorting, while the latter preserves the variability of the responses [cit] . the nsga-ii algorithm is mostly used for feature selection [cit], efficient electrode selection [cit] and optimization of the filter [cit] in bmi-based systems. the lda classifier is used as the decisionmaker of the evaluation function in most of these studies due to its simplicity and high classification speed [cit] ."
"where t s out is the timeout threshold for the secondary network. applying (21)- (32), we obtain a closed-form expression for the timeout probability p (s, 1) out in the first hop as"
"in this section, the data of all four sessions for all three subjects from database 1 were used. 3400 observations were randomly selected in each session as the training data to estimate cross-validation accuracy. moreover, from database 2 two pairs of disabled and healthy subjects, i.e. subject1 and subject3, and subject7 and subject8, respectively, were randomly considered. 2400 observations for subject1 and 2500 observations for subjects 3, 7 and 8 were randomly selected as the training data to estimate cross-validation accuracy. note that the observations were selected from among all sessions and trials conducted for each subject (in database 2). generally, the data of each session for each subject from database 1, and the data of all sessions combined for each subject, from database 2, were applied separately on each classifier. the training data were divided into 10 equally sized folds in order to estimate cross-validation accuracy (cva). regarding the base classifiers, dt, k-nn, lda, lr, lvq-nn, mlp, nb, pnn, rbf-nn and svm and regarding the combinatorial classifiers, three boosting classifiers, adaboost, gentleboost and logitboost, and ultimately, five bagging classifiers, bagging-dt (random forest), bagging-knn, bagging-lda, bagging-lr and bagging-svm, were used. furthermore, in order to investigate the stability of the classifiers to various data volumes (sdv), the data used for each classifier was considered in three different volumes of 100%, 50% and 25% of the total observations. the last rows in tables 2-4 demonstrate the stability of respective classifiers over data volume changes. the calcul ation procedure for sdv is expressed through the following equation:"
"bagg-svm 0.00 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.65 1.00 0.00 0.00 0.00 0.00 0.00 0.06 0.00 1 bagg-lr 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 bagg-lda 0.00 0.48 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.00 0.00 0.00 0.00 0.46 1.00 bagg-knn 0.00 1.00 0.41 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 bagg-dt 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 logitboost 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 gentleboost 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 adaboost 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 svm 0.00 0.00 0.37 0.00 0.00 0.00 0.00 0.00 0.21 1.00 rbf 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 pnn 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 nb 0.45 0.00 0.00 0.00 0.00 0.00 1.00 mlp 0.00 0.00 0.00 0.00 0.00 1.00 lvq 0.00 0.00 0.00 0.00 1.00 lr 0.00 0.00 0.00 1.00 lda 0.00 0.43 1.00 k-nn 0.00 1.00 dt 1.00 p300 signals regarding the second database with a dimension of 256, which was used in section 3.2. in the case of qusmr data, the lda classifier with a quadratic kernel and, in the case of the second database, linear lda were used as the decision maker. regarding nsga-ii, the initial population included 400 individuals, where the maximum number of iterations was 1000. the probability of crossover and mutation were 0.7 and 0.2, respectively. moreover, the objectives were considered as: (1) the number of selected features, and (2) the classifier error. 70% of the data were considered as the training data and the remaining 30% as the test data. a pareto chart of the optimal features with respect to the classification error for different subjects and datasets is demonstrated in figure 6 . according to the results, in the case of the second database, as previously mentioned in section 3.1.2, the number of optimal features for selected subjects, that is, subject1, subject3, subject7 and subject8 were 37, 43, 48 and 34, respectively. moreover, three features were selected as the optimal features in the case of the qusmr dataset (as the data of database 1 have high classification accuracy, the least number of optimal features were used in order to challenge the evo lutionary algorithms against these data)."
"where p ( f 1, f 2, ..., f n ) can be ignored due to being a constant value for all classes."
"in the first hop, the su-tx transmits packets to the sr by using the power-allocation policy given in (28) . as such, the probability p (s, 1) out that a packet is dropped due to timeout can be expressed as"
"in this paper, the waveforms are proposed to be constructed by a group of lfm signals with undetermined starting frequencies, bandwidths, and transmit energies. the waveform parameters and the receiver impulse response are jointly designed by maximizing the scnr performance of a cognitive mimo radar system under the constraints of allowable range of operating frequency and total transmit energy. the algorithm for solving the waveform optimization problem was presented. we showed through numerical examples that the systems using the proposed waveforms have superior scnr performance than the systems using the fs lfm signals."
"in the first hop, the su-tx regulates its transmit power to send a packet of length l s bits to the sr. as the sr receives the packet successfully, it is stored in the buffer at the sr. then, the sr feeds back a short ackowledgement (ack) without delay to the su-tx. upon the reception of the ack, the su-tx transmits the next packet. the time consumed to send a packet in the first hop is given by"
"in other words, to send a packet to the sr, the su-tx should control its power to jointly meet the timeout probability constraint of the pu-rx 1 given in (4) and the peak transmit power constraint of the su-tx given in (11) ."
"a neural network is a framework with a network structure composed of several layers of neurons. this model is able to estimate any continuous function given that it has sufficient layers and neurons [cit] . the neural network which is trained by backpropagation or generalized delta rule is called an mlp network [cit] . the most important part of the implementation of a neural network is determining the number of neurons, hidden layers and activation functions [cit] . in general, training a backpropagation neural network (bp-nn) consists of three basic steps including: (1) forward feeding of training pattern input, (2) backpropagation of the related error, and (3) adjusting weights. in the first phase, the inputs are sent to neurons of the hidden layer using random weights and their value is calculated using different activation functions. after outputs are obtained, each unit compares its calculated output with the target value so that units errors are determined. then, using specific patterns, the weights can be updated by distributing the calculated errors on the previous layers. this cycle continues until suitable weights are obtained [cit] . due to nonlinearity and the non-static characteristics of eeg signals, mlp networks are frequently used in bmi-based systems [65, [cit] . the major disadvantage of this classifier is overfitting upon encountering a complex learning model provided with few training data [cit] . this causes overtraining in the network and consequently results in overfitting."
"2) adaptive transmit power for the sr: similar to the derivation for the first hop, the adaptive transmit power for the sr in the second hop is derived as follows."
"proof: see appendix b. due to the impairments caused by the wireless channel, a packet may be transmitted successfully or unsuccessfully. as such, the moments of packet transmission time should be calculated based on the packet transmission time with and without being timed out [cit] . therefore, the moments of t (s, 1) can be obtained by using the law of total expectation as follows:"
"in recent years, several review papers [12, 14, 16, 19, [cit] have been published on bmi-based systems. these papers have explored many topics including introduction and history of various bmi systems [cit], signal processing methods [16, 19, [cit], classifiers [cit] and control interface [cit] . these studies generally lack the examination and evaluation of optimized bmi-based systems using powerful methods such as evolutionary algorithms and swarm intelligence. therefore, the main focus of this paper is to introduce the different types of classifiers and evolutionary algorithms used in bmi-based systems. the paper is divided into two main parts. in the first part, a wide range of different types of base classifiers such as decision tree (dt), k-nearest neighbor (k-nn), linear discriminant analysis (lda), logistic regression (lr), learning vector quantization (lvq), multi-layer perceptron (mlp), naive bayes (nb), polynomial neural network (pnn), radial basis function neural network (rbf-nn) and support vector machine (svm) as as well as combinatorial classifiers including boosting classifiers (adaboost, gentleboost and logitboost), bagging classifiers (bagg-dt, bagg-knn, bagg-lda, bagg-lr and bagg-svm) and evolutionary algorithms such as deferential evolution (de), particle swarm optim ization (pso), harmony search (hs), invasive weed optim ization (iwo), biogeography-based optimization (bbo), teaching learning based optimization (tlbo) and non-dominated sorting genetic algorithm-ii (nsga-ii) are investigated. in the second part, these classifiers and the evo lutionary algorithms are assessed and compared based on two types of relatively widely used bmi systems, i.e. sensory motor rhythm-bmi (smr-bmi) and event related potentials-bmi (erps-bmi). moreover, in the second part, some of the evolutionary algorithms as well as bi-objective algorithms which are based on hs, iwo, bbo and tlbo, are exper imentally assessed and compared."
where (t) is the time-dependent learning coefficient (usually between zero and one) and c represents a class. the input vector to the algorithm is classified according to the most similar codebook vector. the similarity criterion is usually euclidean
"in this paper, the waveforms are designed for the cognitive mimo radar system. since the detection probability is a nondecreasing function of the scnr under the log-likelihood ratio test [cit], we employ the scnr maximization as the objective of the optimization problem to improve the detection performance. lfm signals are employed as the transmit waveforms. unlike the fs lfm signals usually adopted in mimo radars [cit], where each of the transmit lfm signals has identical bandwidth and transmit energy and equally spaced starting frequencies, we propose to construct the transmit waveforms as a set of lfm signals whose starting frequencies, bandwidths, and energies can be different and are to be optimized. the prior information about the target and clutter obtained by the cognitive process is used for the waveform optimization. the constraints of the total transmit energy and the allowable range of operating frequency impose restrictions on our optimization problem."
"where x j i is the instance i of the class j,  j is the mean vector of members of the class j,  is the average of all the data, n j is the number of instances of the class j and c the number of classes."
information visualization principles [cit] are used to codify displays and organise subcategories of interactions with data. displays are described by a pseudo-code to facilitate data manipulation. [cit] who states that users should be provided with the ability to: -obtain an overview of the data to get a broad understanding of a phenomena -zoom into areas of specific interest and filter out unwanted data -ask for a specific type of detailed information -retrace previous steps (retrace history) -compare and relate information
"although this means display instances in this case might be very similar to the ones requested for overviews, more in depth information can be provided through data superimposition and data density increase (e.g. dense tables and superimposed histograms). information could be complemented by 2d contour plots or surface views displayed as small multiples providing a performance summary of the impact of design variables potentially related to design actions in space."
"another feature of this model is that lists, especially list of metrics and list of displays, are not supposed to be exhaustive but to be constantly enriched by software developers based on further research and interactions with designers. new additions could vary from less conventional types of displays (kinematic, haptic, etc.) up to comprehensive metrics which could couple performance with other types building design metrics (e.g. proportion systems, ergonomics, rules of construction assemblage, etc.). before release to the users, every new addition should be assessed in terms of preferred combinations as illustrated in this work."
"this is assuming the demand for each core is the same. after obtaining the new routing probability p * isr from the algorithms, we get the original p ir by"
"the purpose of the data model described in this paper is therefore to structure and represent simulation output data through a database / data management system. however, exploring simulation output data relevant to design decision making is seen independently of proposing a simulation output data interface. it is essential that this exploration happens prior to the development of an interface as any interface should focus on different user experiences in interacting with data and machines rather than on the data itself."
"once the system has found a matching template, it could recall a specific script to run simulations and the necessary ancillary tools (e.g. optimization routines) and/or procedures (e.g. automatic elimination parametric tests) to generate the data to answer the question automatically. the 20 questions developed are already a list of potential variations for the template question. they could be all hand-coded individually, simplifying the question answering system to focus only on identifying the different types of design actions for the different types of questions listed. the design of this kind input interface and the details related to the question / answering system are a problem of software implementation and beyond the scope of this study."
"to assess if my solution is applicable to real-world ebss, i selected eight ebss from the test suite which have been used in evaluating prior research [cit] . while all subject systems are implemented in java, they are from different app domains (e.g., game, simulator, and chat system), of different sizes (5k-247ik sloc), and use different underlying mechanisms (e.g., jms [cit], prism-mw [cit], and rebeca [cit] ) for event communication. since the list of sensitive apis and trust boundaries were not provided for those systems, i have used the configuration that every 'getter' or 'setter' method was a sensitive method and every component belonged to different trust boundaries. according to the well-known sensitive api list for android [cit], 81% of sensitive methods are eight getters or setters (getter: 97%, setters: 65%), which implies that getters and setters are more likely to be sensitive to security attacks compared to other methods. however, it is important to note that this does not necessarily meant that all getters and setters are always sensitive methods. among the eight subject systems, my prototype flagged 25 vulnerable event communication channels in three systems (dradel: 12, ers: 11, klax: 2). on average, the precision of result was 85.67% (dradel: 75%, ers: 82%, klax: 100%). every false positive was caused by the prototype's inaccuracy in identifying control-flows between sensitive methods and event interfaces. for those three systems, xanitizer reported 83 security warnings such as \"may expose internal representation by returning reference to mutable object\" and \"io stream resource leaks\" (dradel: 6, ers: 62, klax: 15). however only seven of them (8.43%) were related to the vulnerabilities that expose the system to event attacks. owasp orizon and sonarqube returned 13 (dradel: 9, ers: 1, klax: 3) and 95 (dradel: 17, ers: 73, klax: 5) implementation bugs, respectively, indicated as \"empty catch detected\" and \"found potential dangerous keyword\". but none of them were related to the vulnerabilities that expose the system to event attacks. those three tools also did not return any such vulnerability from the other five subject systems. although my prototype outperformed the three tools in this evaluation, it is to be noted that they detected additional types of vulnerabilities my prototype does not target."
"the framework paper outlined the need for a conceptual data model to be developed. this current paper explores in detail how the conceptual data model for presenting simulation information for design decision making (dashed box in figure 1 ) was generated and how it can become operational. it specifically focuses on the type and relationship among data as well as representation systems building designers need to make decisions. this conceptual data model does not focus on data management or on proposing a database structure. it is a starting point for constructing a database / database management system in which entities, their attributes and relationships are described without using a formal language and independently of any choice of database technology."
"the aim of this paper is to describe a conceptual data model from which dynamic thermal simulation information for building design decision making may be generated. providing such information to the building designer is a challenge that has been addressed in the past by the design of new software and interface/outputs, mainly from an engineering or project management perspective. in this paper we follow an approach based on considering primarily the needs of the user. this focus on the user is inspired by the practice of interaction design [cit] and represents a new approach toward the problem of enabling a wider range of design professionals to make use of simulation software in the design of low energy buildings."
"the patterns are based on the assumption that event communication within the same trust boundary is intended access, but event communication across the boundaries can be unintended access from a malicious component. specifically, in case of the pattern (1), c3 c2 can be spoofing. for the pattern (2), c1 c3 can be interception or eavesdropping. for the pattern (3) and (4), c1 c2 c3 can be confused deputy or collusion. if a given efg contains event communication channels that match any of these patterns, the corresponding channel(s) to vulcp (i.e., a set for vulnerable event communication channels) are returned. finally, all the identified event communication channels in vulcf and vulcp are returned. while the channels belonging to both sets can be considered as the most vulnerable, other ones also need to be inspected and protected in order to minimize the threats of event attacks in a target ebs. www.ijacsa.thesai.org iv. evaluation i have implemented the prototype of my solution as a stand-alone java app which combines approximately 2,000 newly written sloc with the off-the-shelf tools, eos [cit] and soot [cit] . eos is used in the extraction phase to extract pet and cet from target ebs. soot is used to generate cgs and cfgs of the components within a target ebs. the prototype was empirically evaluated in terms of its accuracy, applicability, and performance in detecting vulnerabilities from a target ebs's byte-code."
definitions: 'types of data displays' are a class which describes or defines the different ways of representing useful information for design decision making. an indicative notation system in the form of a pseudo-code is developed. this provides a synthetic and clear description of each display instance facilitating their manipulation in the conceptual data model structure as well as their interpretation by computer programmers.
"data evidence: information from the data set was organised into the different types of interaction with data instances reported in section 3. overviews and different types of zoom were used to gather insights about how designers query information relevant to design decision making. overviews provide data summaries (figure 6a ). zooms into different time frames are generally used to increase understanding about a specific type of behaviour (figure 6b ). zoom into different building locations, faade orientations and construction assemblages are instrumental to design decisions. they are generally displayed using performance metric instances represented on top of plans, elevations and sections (figure 6c) ."
"the different methods, principles and approaches of this research together with the data they used or generated are summarised in figure 4 . details involved in defining each class of the conceptual data model and the list of data which belongs to them are explained in section 4. pairwise comparisons are used to explore appropriate combinations of relevant data for design decision making. pairwise comparisons are a common analysis method used in the social science, psychology and artificial intelligence to undertake comparative judgement between pairs of data [cit] . in computer science, they are also used to undertake internal validation of software development. in this conceptual data model, they illustrate if a combination is preferred or not."
"since existing test benchmarks for web apps neither target ebss nor event attacks, i have created a test benchmark for evaluating security analysis techniques for ebss. to minimize internal threats to the validity of results, i asked graduate students at usc to build a set of apps that implement event attacks based on the published literature [cit] . they built 20 distinct event-based apps by using two representative types of mom platforms (10 apps for each): (1) java message service [cit], the widely adopted java-oriented middleware; and (2) prism-mw [cit], a research-off-the-shelf middleware platform for distributed software systems. every app was designed to contain a malicious component that had the sole purpose of launching an event attack. the benchmark also comprises five \"trick\" apps containing vulnerable but unreachable components, whose identification would be a false warning. this yielded a total of 25 event-based apps containing 20 vulnerable event communication channels."
"i also tested my prototype on the event-based apps comprising different numbers of components. i created four distinct apps by adding different numbers of components (i.e., 25, 50, 75, 100, respectively) to an app randomly selected from my benchmark. to check the prototype's best-case performance overhead, each of the added components is designed to have a minimized architecture-containing one method for communicating with at most two other components (55 sloc)-which would induce the shortest analysis time while connected with other components. the size of the apps spanned 2.8k-7k sloc. none of the added components are involved in the vulnerable event communication channels so that they can be pruned in reduction phase. then i measured the analysis time for each app both \"with\" and \"without\" the reduction phase. the result (see fig. 4) indicates that as the number of added components increased, the difference of analysis time between \"with\" and \"without\" reduction phase also increased. this result confirms that my solution minimizes the potential overheads in its analysis by introducing the pruning operation. considering the fact that the added components are designed to have a minimized architecture, the effectiveness of pruning will drastically increase in the case of large-scale ebss comprising a number of components with higher complexity."
"data evidence: the data set included mainly metrics used to describe overall building performance (e.g. heating and cooling demands, temperatures, etc.). a second common set of metrics is used to understand causes behind this performance in attempt to gain insights on where to act in the building to improve its behaviour (e.g. heat balance breakdowns). more specific metric instances are used to assess specific design intents (e.g. shading and"
"event-based systems (ebss) implemented by using mom platforms are widely used. they are implemented in various types of systems such as web apps or soa-based systems by using different types of mom platforms such as prism-mw [cit], java message service [cit], and siena [cit] . ebss have become popular because of its high flexibility, scalability, and adaptability. these advantages are enabled by its reliance on implicit invocation and implicit concurrency. specifically, in ebss, components may not know the consumers of the events they publish, nor do they necessarily know the producers of events they consume. however, this communication mechanism is based on non-determinism in event processing, which can introduce inherent security vulnerabilities into a system referred to as event attacks. for example, developers may build ebss by utilizing externally developed components that contain malicious code, and users may use those ebss comprising malicious components. for those cases, malicious components can launch unintended behaviors through event communication, such as eavesdropping on events to steal sensitive information or exploiting the information in events to hijack the system's functionalities."
"following these criteria, only display instances considered most appropriate for the different types of interaction with data are marked as preferred. this means fields not filled with an '' might be prone to data cluttering compromising the speed and effectiveness of results interpretation. fields filled with 'small multiples' indicate multiple displays of the assigned type. preferences for combining these two classes are illustrated in table 6 and discussed below:"
"-temperatures and comfort indices mainly involve quantifying phenomena at a time instant. they can be represented either connected to this time instant when appropriate (e.g. 2d graphs for temperatures) or summarised using statistics (e.g. tables, histograms, box plots, etc.). results can be plotted in 2d surface views to highlight where potential problems could be expected. -'transmitted solar radiation', 'shading on floor plan in %' and 'daylight illuminance' are location based metric instances and should preferably be displayed through 2d contour plots and 2d surface views. the latter two metrics can also be summarised using histograms or tables and displayed in relation to time instants in carpet plots. 'transmitted solar radiation' can also be summarised in tables and displayed in relation to time instants using 2d line graphs. 'shaded surfaces' are generally represented in 2d surface views to better convey the geometric representation of a shading pattern zooms should also be enabled at a metric instance level so that users could query for example the heating and cooling portions of thermal energy delivered to the spaces, discriminate uncomfortable hours due to overheating and under-heating in comfort indices, etc. zooms of this type were not explored in detail in table 7 to avoid information overload. 6. discussion 6.1 operation of the model operation of the model involves linking the design questions asked to the data produced as output (figure 2). as there are a limited number of questions it would be possible to supply these on one or more on-screen menu's to be manually selected. figure 2 in theory also provides a template to interpret design questions which could be hand-coded on a natural language type of interface. a question / answering system would need to be developed to recognize a design input question as an instance of the template:"
"generating answers to these questions would involve structuring modelling and simulations assumptions as well as writing scripts to identify patterns in results (e.g. identify discomfort, hours of overheating, flag zones in which it would happen and help the user to improve performance by displaying the causes of the problem, etc.) many of these types of patterns could be automatically identified and reported to the users in a simplified way, through integrated performance metrics, simple text format (e. presenting answers to these questions would involve developing a database/database management system to enable manual, semi-automatic and totally automatic searches in preferred combinations of the 6 aforementioned pairwise comparisons (figure 8 ). the search would start by automatically identifying the 'type of analysis' in a question to eliminate all but one specific column from table 4 (comparison 1). metrics and different types on interaction with data would eliminate most but a few fields in table 5 (comparison 2). the selection of metrics could be done in three different ways as, already outlined in section 3 and suggested in figure 8 . types of interaction with data should be provided preferably at an overview level first (as suggested by information visualization literature and in interview with designers 17 ). the selection of different types of zooms could either be done manually or automatically and the interface should allow both ways to happen."
"to overcome aforementioned challenges and the shortcomings of the existing approaches, i designed a technique that automatically detects target ebs's vulnerabilities that expose the system to event attacks. my solution statically inspects target ebs in order to identify security vulnerabilities that expose the system to event attacks. it performs vulnerable-flow analysis and pattern matching on event communication channels between components. my technique is distinguished from prior works because (1) it detects potential risks of event attack in ebss more accurately than existing techniques, (2) it supports multiple types of mom platform, and (3) it enables a scalable analysis of ebss comprising a large number of components and methods. this paper makes the following contributions: (1) i proposed a novel technique that identifies security vulnerabilities from multiple types of ebss; (2) i developed a prototype tool that implements the proposed technique; (3) i provided the results of evaluations that involve real-world ebss and comparable techniques. section 2 illustrates event attacks in ebss, which motivate my research. section 3 details my approach and section 4 presents the evaluations of my technique. a discussion of related work is provided in section 5, and my conclusions are presented in section 6."
"previous initiatives which explored output data to inform design decision making mainly focused on expanding the scope of representation systems to describe and compare building performance. simple examples of these can be found in most 'user friendly' simulation software to date [cit] . in these tools, some performance metrics can be displayed on top of 2d and 3d views designers are used to manipulate (plans, section, elevation, perspectives, etc.). these initiatives also include the development of integrated thermal performance metrics (e.g. comfort, hours of overheating, etc.) and more elaborate types of 3d representation systems (examples of virtual reality images, movies, etc. [cit] ."
"requirements and site constraints, provide clear visual guidance to explore building form in the early design stages (figure 3a) . however, the same can rarely be said when more elaborate optimization routines are applied to produce design advice. in these cases, even though users are generally provided with a pareto front graph to query on best design alternatives (figure 3b ), queries tend not to be displayed in a user friendly format [cit] for a review of optimisation and building performance analysis). this means the user needs to postprocess information that comes from optimization routines into something (s)he can understand to then query the content of this information."
"the model was validated and tested throughout its construction by using a set of different methodological approaches to extract concrete and relevant data for design decision making from designers themselves. validation in this case happens in a different way and can be summarised by the three following stages / steps: -participatory action research (par): contrarily to other methods, in which the researcher proposes a solution to a problem and test this solution with his/her potential beneficiaries, in par the beneficiaries themselves propose concrete solutions to their own problems. this being the case, testing what is proposed by beneficiaries with beneficiaries themselves becomes redundant, especially when the sample comprises more than a 100 participants. -elimination and filtering: quality assurance in terms of the physics contained in the solutions produced by designers was guaranteed by checking proposals in terms of them fitting or violating dynamic thermal modelling principles. solutions which violated these principles were eliminated and/or adapted to fit them. -pairwise comparisons: these comparisons were used to 'fine tune' quality assurance procedures by examining every combination of types of analysis, types of metrics, types of interaction with data and types of data display in pairs. they were also used to manage information association and are a common procedure used to do internal validation of software development."
definition: 'types of analyses' are a class which describes and defines how building designers would use dynamic thermal simulation tools to inform or assess design decisions. they are important procedures or algorithms to extract design advice or undertake performance queries in bps output data. controlling different types of analysis is seen as the most important aspect of integrating bps tools throughout the building design process 10 . table 1 provides a list of the five types of analysis instances which belong to this class together with the purpose in using each of these analysis instances to inform design decision making.
"materials of the situation'. this means designers gradually discover the problem while attempting to propose solutions to it. a key aspect of this process is that it necessarily involves experiments. these experiments can be of the following three types: (i) exploratory experiments, in which action is undertaken only to see what follows; (ii) move-testing experiments, used to assess moves depending on the changes produced and whether the designer likes the changes produced; and (iii) hypothesis-testing experiments, used to discriminate among competing alternatives generally not used to reach a final solution but to constantly reframe the problem through a new hypothesis to be tested."
"ways to display comparisons with benchmarks, notional buildings, regulatory targets and other design options were extensively explored in the simulation literature [cit], 1999b, 1999c, [cit] to cite a few). examples provided by these authors range from multi-criteria evaluation strategies to complex output interfaces with highlights to facilitate data interpretation. they generally focus on comparing different models and/or different performance metrics for a single model. the way comparisons are structured is appropriate to describe behaviour against targets but not very useful to describe behaviour of different design alternatives. when assessing different design alternatives, designers need to be reminded in a clear and straightforward way which design parameters were changed and by how much, in order for these changes to be associated with changes in building behaviour."
figure 3 -a: visual guidance to explore building form produced from simple generative forms [cit] vs. b: pareto graphs resultant from optimization studies [cit] .
"as shown in this example, since event attacks appear to be ordinary event interactions, existing malware inspection techniques, especially the techniques that rely on signaturebased detection [cit], may not be able to detect event attacks. moreover, since publishing and consuming events can be processed via ambiguous interfaces, existing flow-analysis techniques will be unable to accurately analyze implicit invocation between components. furthermore, since routing event is performed in an invisible and non-deterministic way, it is difficult to expect when and where the event attacks are actually launched."
"-comfort and energy related metric instances are relevant to be displayed in all types of interaction with data. at an overview level, they are useful to quantify and benchmark overall building behaviour. at a zoom level, they improve understanding on when, where and potentially why performance is happening. -the time and space dependency of shading/solar related metrics make them more appropriate to be displayed preferably when zooming into data. -cost related metrics are suitable to be displayed at an overview level but could also be displayed at zoom level in fine tuning, resolving conflicting design objectives or whatever other analogous circumstance."
"each analysis instance reported in the dataset was critically assessed using dynamic thermal modelling principles to ensure the dynamic, systemic, non-linear and stochastic nature of building thermal physics phenomena would be preserved. analysis instances to explain building behaviour based on simplified methods, (e to explain causes of a specific building behaviour or performance results sensitivity analysis"
"display instances from the data set were identified, classified and had indicative pseudo-code assigned to specify their content. in this pseudo-code a display instance is defined by a name followed by its attributes. the names for each display instance come from the literature of information visualization [cit] . the respective attributes are listed in table 3 together with examples of how important information can be highlighted. the list of highlights is far from exhaustive and does not include interactive highlights (such as brushing, etc.). table 3 only informs how things can be highlighted -not what information can be highlighted. the way information can be highlighted depends on the display instance used to represent it, whereas the type of information to be highlighted depends on the aims behind an analysis. table ( 5. exploring pairwise comparisons of the conceptual data model this section examines the preferred combinations of each of the 6 pairwise comparisons of the conceptual data model. preferred combinations are not exhaustive and should in theory be customizable. preferred combinations in each comparison are described following the sequence of operations outlined in figure 5 . principles underlying each combination are outlined together with comments for combinations that should not be allowed. while many of the combinations presented in tables 4-7 are allowed in theory, they would be limited in practice by the simulation software being used, and therefore also provide a means by which any particular software can be assessed as to its functionality in terms of provision of outputs."
"lighting metrics are also used to assess if a desired type of atmosphere is achieved in some of the internal spaces). metrics instances in the dataset can be grouped into: comfort related metrics (air temperatures, daylight factors and illuminance levels), solar related metrics (incident solar radiation on windows and data related to shading) and energy related metrics (heating and cooling demands, heating / cooling degree hours, electric energy consumption and heat balance breakdowns)."
"as it is totally custom-based, the conceptual data model proposed can be expanded and further developed to include different simulation software users as this could be easily managed in a database / database management system environment. a range of users could thus gain access to the power and accuracy of complex simulation tools, which thereby could facilitate the design of low energy and low carbon buildings."
"in this approach, the first priority was to identify and make available a full and exhaustive range of meaningful simulation outputs for building designers, rather than having the software manufacturer decide for them on a reduced set of representations. having this full range potentially available opens up the possibility of different designers / users being able to choose how they wish to analyse performance and view / interact with results. this full range of possibilities was initially explored through a thematic analysis on building designers' work. dynamic thermal modelling and information visualization principles were then applied to further organise simulation output information. a filtering system was added to reduce what could be a long list of output data. this filtering system started by analysing pairwise combinations of simulation output data to exclude those which are irrelevant or not allowed. a second layer of filtering is applied when designers ask a question from which aims, analysis processes and potentially design actions and metrics are extracted to narrow down the visualization choices once again."
"the participatory methods used to produce the conceptual data model also provide its validation, as the model emerges from the identified needs of the user. a further stage of validation will only become relevant when (and if) the conceptual model is developed and implemented into a working system."
"i ran the three tools on my test benchmark and measured their (1) precision, i.e., identified vulnerabilities that were actually vulnerable to event attacks, and (2) recall, i.e., the ratio of identified vulnerabilities to all those exposed to event attacks. my prototype detected vulnerable event communication channels with 100% precision and recall, correctly ignoring all \"trick\" cases. however, other tools (i.e., xanitizer, owasp orizon, and sonarqube) were unable to find any of the vulnerabilities related to event attacks from the benchmark. specifically, xanitizer did not return any vulnerability. while owasp orizon and sonarqube reported some security warnings (e.g., potential dangerous keyword in the method), they are not directly related to the vulnerabilities caused by event attacks. this is primarily because these three tools neither target event attacks nor support inter-component flow analysis."
"simulation outputs need to be 'in tune' with these experiments. they need to provide answers to the different 'what if' situations generated within these experiments. the framework extracted from these 'what if' situations questions about performance. it also proposed a structure to set up specific questions about performance, so that these questions can be embedded in sequences of moves directed by reflection in action ( figure 2 ). since only five aims 2 and five analysis processes 3 were identified and confirmed in a survey and interviews with building designers, around 20 standard questions were developed [cit] for a full list of questions). examples of questions are: \"how sensitive is this building to [design action]? \"how does this building perform with [design action]? designers are expected to be able to select which standard question(s) and (sets) of design action(s) best fit the design experiment they are undertaking. examples of design actions include: different types of shading devices, different glazing ratios, a specific type of external wall panel system, etc."
"a thematic analysis is applied on this empirical data sample. thematic analysis, a common research method from the social sciences, consists of investigating recurrent themes in a dataset so that a phenomenon can be described [cit] . a thematic analysis should not be confused with a statistical analysis. it comprises identifying and recording recurrent themes from all the data in a dataset so the conceptual data model can present all relevant possibilities regardless of how frequently they are used."
future studies can focus on building a runtime-access controller which controls runtime event communication based on the statically-analyzed vulnerabilities. also i can apply a visualization technique which can display the identified vulnerabilities between components in order to help engineer's understanding.
"-descriptive analyses can be represented using the vast majority of display instances and provide more possibilities for different metrics to be compared without cluttering. -elimination parametric tests, sensitivity tests and comparative analysis can be represented using similar display instances as they all involve comparing different models. comparisons can be emphasized through data grouping (e.g. grouped bar charts), data superimposition (e.g. superimposed histograms or line graphs) and/or by increasing data density (e.g. dense tables). -data from sensitivity tests can be summarised through special display instances called tornado charts."
"discussion. the revenue, cpu utilization and the 95th percentile of the response time of the sessions are shown in figure 8 and 9. for the 4 classes case, the observation is mostly consistent with that of the 2 classes case. the optimization program returns the best revenue among all the policies. and the revenue is around 27% higher than others. besides, the optimization algorithm has a more balanced response time compared to the heuristic because it assigns all the users of session 2, 3, 4 to vm1 which makes the response time higher for requests served by that vm instance. for the demand estimated using the ci algorithm, we can see that vm4 has larger demand than other instances which we believe is because vm4 has the heaviest load of the 4 vms."
"in general, users want to avoid having to understand and deal with the complexities involved in generating information they use in their everyday activities: they want this information to be readily available. building designers are no different. they do not want to deal with simulation output post-processing to be able to use this information for design decision making."
"the weighted round robin is a load balancing policy offered by many popular cloud providers. however, there is a lack of effective mechanisms to decide the weight assigned to each server to achieve an overall optimal revenue of the system. in this paper, we experimentally explore the relation between probabilistic routing and weighted round robin policies. from the experiment a similar behavior is found between these two, which makes it possible to assign the weights according to the routing probability estimated from queueing theoretic heuristic and optimization algorithms in the literature. the algorithms described also support multi-class workload which could be in the future, we are planing to use opt and the heuristic as part of a self-adapting load balancing mechanism. we plan to change the routing decision adaptively at run time and possibly apply a load-dependent algorithm. in addition, it would be interesting to perform a systematic comparison between the optimization algorithm based weighted round robin policy and other policies such as the lc and s policies."
"these examples of outputs illustrate that while researchers and developers continue to propose and integrate new representation systems, comparative displays, parametric tests and different types of analysis algorithms to existing tools, there is a lack of a comprehensive overview or system that collects all these proposals and explores more general ways of ordering information. therefore, a framework to post-process and shape simulation information for building designers to use was proposed by the authors. this current paper builds on this framework and explores the construction of a conceptual data model to be transformed into a database / data management system of meaningful simulation outputs to design decision making. once developed, this database / data management system, potentially accessible through a user friendly interface, would guide designers to query simulation output data while undertaking design experiments. the database of outputs is not seen as exhaustive but could accept new additions, especially in data metrics and displays, following new research developments in these areas. this approach to structuring simulation software output information using a database / data management system can in theory be extended to any user, who could include building engineers and consultants needing different types of analysis and results on which to base design decisions."
"types of interaction with data should be explicitly organised to facilitate data query, minimise visual noise and reducing the \"cognitive load by removing unnecessary information from displays\" [cit] . further empirical studies would be necessary to conclude if precise definitions of overviews and zooms can be generalised or if they need to be addressed on a case-by-case basis (e.g. would looking at energy use in a specific time frame, for instance the summer period, be considered an overview or zoom into time? the answer to this might potentially depend on the type of project, personal preferences, etc.). proposing 'progressive disclosure' 15 would also involve investigations to indicate whether these can be generalised or need to be addressed in a case-by-case basis."
"the number of choices can be therefore limited by each different user considering their specific needs, rather than by finding a stereotypical user and assuming what he/she wants. possibilities of data display will always be reduced, depending on the question asked and if a customised and/or learning systems is in place. in case a learning system is in place, it is expected that options will reduce according to an increase in the number of queries. this is because, the larger the number of examples the system has stored, the more it can learn with them and reduce the number of display options presented to the user."
"while event-based communication model enables highly decoupled, scalable, and easy-to-evolve systems, the nondeterminism in event processing can be exploited by event attacks. existing solutions for general software systems cannot be directly applied to resolve event attacks because they do not support event-based communication model. furthermore, existing security solutions targeting ebss do not appropriately resolve event attacks or suffer from inaccuracy in detecting event attacks."
"the conceptual data model, like the framework, emerged form a process of participatory action research and thematic analysis of design work produced by 140 novice designers. all types of analysis, metrics, interaction with data and data displays were extracted from the 140 design journals. principles of information visualization and dynamic thermal modelling were used to filter and quality assure these entity classes. associations between pairs of entity classes were explored based on pairwise comparison used to identify appropriate and inappropriate combinations of relevant data for design decision making."
"enabling machine learning techniques to be implemented so that the database/database management system could learn from each user what are his/her preferred outputs. supervised learning could be used in this case to store and analyse user specific preferences, presenting a reduced number of visualization options every time a new query is made. the context of a question would need to be represented as a feature vector (e.g. encoding what kind of simulation the user has performed) together with a record what aspects of the output the user has previously wanted to see. these would enable the system to learn with the users what would be the most appropriate metrics and visualization options to each different type of query, potentially reaching a point in which choices are no longer presented if not specifically requested."
"during our evaluation of the algorithms, we notice some experimental issues that needed to be addressed to run the experimental comparison. the most challenging problem is how to adapt the algorithms to multi-core instances."
"data evidence: the empirical dataset is rich in information display instances especially to connect performance information with design parameters (figure 7 ). representation systems can be of two types: (i) location based, in which performance metrics are displayed on top of commonly used building design displays (e.g. plans, sections, elevations, etc.); (ii) abstract, in which performance is displayed in a non-spatial way through graphs, tables, text, etc. in the first case, the aim is to inform where a specific parameter or performance result would occur or which specific building design element is mainly responsible for causing specific resultant behaviour. the second case seems to be more useful when highlight strategies are adopted to help interpret information (e.g. ranks, ranges and differences between two or more options or between an option and a target)."
"the need to organise and better present relevant thermal simulation outputs for design decision making is constantly highlighted in the literature but is expressed in practical terms in many software initiatives in a disarticulated and disjointed format. the conceptual data model is a meaningful resource for software developers to structure output interfaces because it deals with information only, independent of a specific database or database management system format. this gives developers freedom to choose how they wish to design a database/database management system which best fit their different software structures."
to inform on the sensitivity of the model to changing a single parameter to inform on the sensitivity of the model to changing 'n' parameters optimization to inform on the best performance for the optimum combination of a group of pre-defined parameters table 1 -types of analysis to be included in the conceptual data model [cit]
"based on information from comparisons 1 and 2, preferred combinations of the remaining 4 pairwise comparisons could then be automatically identified. results to be reported to designers would only include combinations which are marked as 'preferred' in all pairwise comparisons. designers would be provided with a list of relevant displays to represent a selection of few metrics in specific types of interaction with data to choose how to best answer their design question. additional information about manual, semi-automatic and automatic types of data selection illustrated in figure 8 provide an extra filtering system to reduce the amount of choices to be presented to designers. the model should also enable combinations of different types of data selection to be customised (e.g. a practice could wish to always output an overview of the minimum rate of return on investment as the first metric to inform or assess design decisions). customization could be set up by users and/or automated via the use of a machine learning system. supervised learning could be used in this case to store and analyse user specific preferences, presenting a reduced number of visualization options every time a new query is made. the context of a question would need to be represented as a feature vector (e.g. encoding what kind of simulation the user has performed) together with a record what aspects of the output the user has previously wanted to see. these would enable the system to learn with the users what would be the most appropriate metrics and visualization options to each different type of query, potentially reaching a point in which choices are no longer presented if not specifically requested. the design and implementation of this supervised learning system and the development of this database/ database management system are again a problem of software implementation and beyond the scope of this study."
"the classes of the conceptual data model are discussed in this section. [cit] but are described here in terms of the data model. a definition for each class is provided followed by an explanation about how it is defined based on information from the dataset, principles of dynamic thermal modelling and information visualization. recommended lists of instances for each class are provided based on the empirical data set in combination with information from the literature and bps software output interfaces. these lists of instances are not supposed to be exhaustive."
data evidence: information from the data set reports mainly descriptive and comparative types of analysis instances. many comparisons focus on understanding the contribution of each of the heat balance component in the overall building behaviour. they are used to understand causes of building performance and provide some information on where to act to improve it. comparisons with targets and standards and comparisons among different design alternatives are also common. elimination parametric is used to illustrate the influence of internal gains in overall heating and cooling demands. sensitivity tests are sometimes undertaken to experiment with window areas and window material properties. optimization routines are sometimes used to explore shading device form.
"extensive discussions about appropriate descriptions of the building designer's 'modus operandi' can be found in the building design literature 1 . one of the most famous descriptions is provided by schon (1984 schon (, 1988 schon ( [cit] . according to schon, designers solve problems by 'reflecting in action' through 'a conversation with the 1 [cit] to cite a few."
overview zooms time location / orientation parameters potentially related to design actions tables    dense tables    bar chart     grouped bar chart    stacked bar chart     arrow diagram     pie chart     2d line graph 16     2d superimposed line graph   histogram    superimposed histogram    tornado chart     multiple tornado chart    multi-metric tornado chart     2d contour plot    or small multiples 2d pareto front graph   2d surface view   small multiples carpet plot  box plot   multiple box plot  
"these are modified bar charts with ranked data categories listed vertically generally used to illustrate the relative importance of each variable of a sensitivity test. -optimization data are generally summarised through pareto front graphs which could be a useful interface to further query optimization results. the term \"after zoom\" in table 4 refers to potential types of displays to be presented after querying the pareto front graphs. zooming into a specific point of the pareto graph could lead to display instances suitable to represent descriptions. zooming into a specific region of the pareto graph could lead to display instances suitable to represent comparisons. histograms, used to illustrate how often values for each specific design parameter being optimised were used in optimisation tests, could assist in identifying the most important contributors to building performance. -all analysis instances, with the exception of descriptive ones, when displayed as 2d contour plots or as 2d surface views, could be represented as small multiples (i.e. showing multiple 2d displays, one for each different model being compared). table 4 -comparison 1: types of analysis and types of data display."
"they could be open to being customized by each different practice depending on the building typologies they generally deal with, types of contracts undertaken, specific ways they organise design teams, etc. 9 section 5 is therefore intended to illustrate how pairwise comparisons are used to relate the four classes of the conceptual data model for 'generic' types of low energy design problems. figure 5 [cit] . the analysis process in a question narrows down the search in preferred combinations in comparison 1. types of display not appropriate to respond to the type of analysis in the question are automatically eliminated from a future list of choices. aims and actions can be used to infer a selection of 'metrics' and 'types of interaction with data' potentially available to answer the question. the dashed arrows ( figure 5 ) indicate a separate study would be necessary to determine if this selection could be at least partially automated. in case this selection should be manual, it would be necessary to determine the best user interface and database / database management system to present this information for designers to select."
"here, we study the relation between pr and wrr. two different classes of users are used. the detailed composition of the sessions set in ofbench is shown in table iii . the number of users n r is set to 40 for each user and the think time between requests and sessions is set to 2 seconds. each experiment lasts one hour. the probabilities for both algorithms are shown in table v and vi. for the wrr policy, since haproxy does not support fractional numbers to be used as weights, we round the ratio of the probabilities returned from heuristic and optimization algorithms to integers. since vm1, vm2 and vm3 are all installed in a m1.medium instance and we found similar demands for all the requests, we average the demands taken from the three vms so that two ofbiz running on the same m1.medium instance have identical demands. this is because small differences in the demands can result in macroscopic differences in the load balancing decisions of the heuristic in particular. however if the vms have the same instance class but with different hardware, the demands would be quite different even if the instance class is the same."
"overviews and zooms, become instances of the conceptual data model class of 'types of interaction with data' whereas the remaining three types of interaction with data are embedded in the conceptual data model structure."
"definition: 'types of interaction with data' are a class which describes and defines possibilities involved in and afforded by manipulating thermal simulation post-processed output information. the instances defined for this class are the following: overviews, zoom into different time frames, zoom into different location / orientation 14 and zoom into parameters potentially related to design actions."
"the rest of this paper is organized as follows. section ii gives background of load balancing in the cloud. section iii explains the load balancing algorithms for probabilistic routing. in section iv, we evaluate different load balancing algorithms, and compare probabilistic routing and weighted round robin"
"-all analysis instances should enable interaction with data at an overview level to convey data summaries. specifically in the case of elimination parametric tests, main causes of building behaviour could be provided at an overview level reporting the following variables: internal gains, ventilation losses and gains, solar gains, fabric conduction losses or gains and fabric storage. -all analysis instances should also enable interaction with data at all zoom levels to improve understanding on when and where performance is happening as well as on what is causing it. specifically in the case of elimination parametric tests, causes of building behaviour could be provided by zooming into parameters potentially related to design actions reporting: (i) usage related variables (people, artificial lighting, equipment and ventilation loses or gains) or (ii) building related variable losses or gains (window conduction, wall conduction, roof conduction, floor conduction, window mass, wall mass, roof mass, floor mass, solar, infiltration). this same type of zoom should also be enabled in optimizations if designers wish to use optimization results to explore which design parameters are the most important contributors to building performance."
"preferences for combining these two classes are examined considering principles of information visualization. in these principles, excellence in data display follows from communicating complex ideas with clarity, precision and efficiency [cit] . representation systems should avoid data distortion, encourage comparisons, provide coherence to large data sets and display the data such that the substance of it, what it represents, is brought into focus rather than means and methods behind representations. comparisons should be controlled to a small number of displays for many variables (5 being a good number) and multiple small views of states in a single variable (small multiples) [cit] ."
"to minimize the risk of event attacks, this paper presented a novel vulnerability detection technique for ebss that are implemented by using mom platforms. my technique statically analyzes vulnerabilities by examining intercomponent flows and event communication patterns. it improves upon existing techniques in detecting vulnerabilities that expose the system to event attacks from a given ebs, while supporting multiple types of mom platforms and increasing the coverage, accuracy, and scalability of vulnerability detection. my empirical evaluation demonstrates that my technique is more accurate in identifying vulnerable event communication channels from 33 ebss compared to the state-of-the-art vulnerability detection techniques for web apps."
"both the heuristic and optimization algorithms are defined using queueing models with single-server queues. this modeling abstraction corresponds to assuming that servers have a single core. however, nowadays servers and vms normally have multiple cores. in order to adapt the algorithms for multi-core instances, we propose two methods. one is to divide the demand by the number of cores while applying the algorithms. in particular, we use the demand"
"-overviews should convey data summaries and broad indications of performance with clarity. the preferred display instances for these should deal with caution with data density and data superimposition (fields related to superimposed and dense types of displays were not filled with a ''). -zooms into different time frames (seasonal, monthly, typical days, etc.) are preferred to be displayed using instances which emphasize when performance needs to be improved. performance profiles are suitable to be illustrated using line graphs. performance data aggregated over a specific time frame are suitable to be illustrated through bar charts or tables. -zooms into different types of location / orientation could be provided directly through spatial representation systems (2d surface views, contour plots, etc.). they could also be provided through abstract display instances, having at least one nominal variable to represent location/orientation (bar charts, arrow diagrams, etc.). -zooming into design parameters potentially related to different design actions are generally presented by display instances which emphasize comparing performance data summaries for different models."
"pairwise comparisons are used to ensure that all possibilities of how the output is constructed have been considered, as opposed to simply assuming that the user will be satisfied for example with a list of figures or one type of chart. therefore, the conceptual data model is intended to enable software developers to strike a balance between providing too much and not enough information for design decision making. examples are provided to illustrate and discuss the potential and capabilities of the model. the full design and implementation of a database/ database management system is outside the scope of this study as is the interface to enable building designers to manage it."
"definition: 'types of metrics' are a class which describes and defines the different quantities associated with building behaviour relevant to design decision making. these quantities need to be capable of being represented as time series, summarised and aggregated as appropriate enabling designers to see when exactly heating, cooling and artificial lighting are needed through structured searches for overheating and under heating patterns at typical and peak days [cit] 13 . table 2 provides a list of the metric instances which belong to this class."
"this issue seems to be addressed by some initiatives which explore the integration of parametric tests to existing simulation tools. as in parametric tests the focus lie on understanding the consequences that changing design parameters have into simulation results, comparisons are sometimes displayed mainly linked with these changes [cit], to cite a few). however, when this is the case, information seems to be quite restricted in terms of how users can navigate through output data. researchers provide generally one or two representation systems they believe are the most appropriate ones to display this kind of information. they tend not to query their suitability in terms of the way users interact with data and derive meaning from it."
preferences for combining these two classes of the conceptual data model are not reported in a table because any metric instance can be used in any analysis instance.
"cost related metric instances were excluded from this combination. these instances require the application of financial value techniques and classical investment analysis methods to be processed and have an appropriate display instance attributed to them, which are beyond the scope of this study. the following principles of what is preferable in this pairwise comparison are outlined based on information from table 7 : -'thermal energy delivered to the space' and 'energy use at the meter' can be represented using the majority of display instances listed. 2d contour plots should only be available for displaying these metrics if energy results enable simulations to be undertaken at a sub-zone level. -'time exceeding glare index', 'working hours operating in a passive mode' and 'working hours not requiring artificial lighting' are similar metric instances. they are counts of the number of times a phenomenon occurs as expected and can be represented using most display instances depending on the level of data interaction required. as these metrics are not time dependant, they are not appropriate to be represented by line graphs or carpet plots."
"in this section, i will present a simplified example of event attack which can be launched on event-based web apps. fig. 1 and 2 illustrate eavesdropping attack. an app app1 follows event-based communication model and is implemented by using java message service [cit], a java mom platform for message-based communication between components. app1 is corrupted to contain an unintended component mal (in fig. 2 ) so that event attacks can be launched. fig. 1 and 2 show where app1's vulnerability resides. in this app, all events are published through -customtopic."
"comparisons were numbered according to the sequence of operation indicated in figure 5 . preferred combinations discussed in detail in section 5 come mainly from the dataset, polished by information from the literature on bps software. however, preferred combinations reported in section 5 should not be seen as exhaustive and could be further developed / refined and even made specific to each different design practice."
"once metrics and types of interaction with data are chosen, a search in preferred combinations in comparison 2 can be undertaken. comparisons 1 and 2 would provide all the necessary constraints to automate searches in the remaining comparisons. the aim of the search is to output a narrow list of types of display to represent the answer to the question. choices of displays should preferably be provided rather than a single display option."
"where w r is the revenue weight for users of class r and x r is the corresponding mean throughput. we similarly define x ir to be the throughput of the users of class r at vm i. our model assumes that vm operate in parallel, therefore"
"discussion. the revenue, cpu utilization and the response time of the sessions are shown in figure 3, 4 and 5 . from the figures, we can see that the pr policy has a trend similar to wrr. in particular, regarding the revenue, it can be noticed that wrr has larger revenue than the pr policy for all the tests. a larger total throughput also contributes to higher cpu utilization for the wrr case than pr according to little's law [cit] . we think the reason behind this is that wrr is a more balanced and stable policy for distributing requests while pr relies on a probability distribution which may incur oscillating load on the instances. overall, the optimization algorithm returns the best revenue for the pr and wrr policies. the heuristic also returns good results compared to other tests and its response time is quite balanced across servers. we also show the per minute average response time and the number of outstanding requests for the opt policy in figure 6 and 7. from the figure we can see that the response time remains stable and for each instance the response time is very similar. pr shows a small peak in the response time due to a peak of requests in figure 6a . in summary, the results suggest that the probability returned by the probabilistic algorithms is well suited to assign the weights of wrr."
"enabling the manual selection of specific elements of the conceptual data model (e.g. the selection of a single metric) so that the list of visualization options can be narrower (ii) enabling the designer, or his/her consultant, to select and save preferred outputs such that these are always made available when specific preferred combinations are selected (e.g. a practice could wish to always output an overview of the minimum rate of return on investment as the first metric to inform or assess any design decision) (iii)"
"existing system analysis techniques neither focus on event attacks nor correctly detect vulnerabilities across components [cit] . specifically, existing vulnerable-flow analysis techniques do not support implicit invocation between components and are not scalable to analyzing systems comprising large numbers of components [cit] . while a large body of research has studied detecting vulnerabilities that expose android apps to event attacks [cit], they cannot be directly applied to other types of ebss, because android uses its system-specific communication model, apis, and component life-cycles. thus a generalized solution is required to protect other types of ebss."
where c i is the number of cores for server i. another approach is to assume the multi-core machine is represented by several single core machines while computing the routing probability. then we sum the probabilities of the one core machines to get the original routing probability. we set demands to be
"the system resembles a closed queueing network. the jobs in this closed network are requests from the users to the target servers and think time represents the waiting time between successive jobs. the performance of the load balancing system is here evaluated by a revenue, which can be defined in many ways, e.g. the mean throughput or the mean response time of the requests. here we use the throughput of the system since we are assuming a closed topology and therefore throughput maximization and response time minimization are equivalent. we define m to be the number of servers and r to be the number of users classes. the notion of class may be defined in different ways, we here interpret it as a set of requests that requires a specific performance. this is the case, for instance, of systems with differentiated slas, where users can be of different classes (e.g., gold, silver, bronze) and each class experiences different quality of service from the server. the revenue  for each server is defined as the weighted sum of the throughputs of each class of users, which could be defined as:"
"the load balancing system assumed in this paper contains a load balancer and several target servers. the users send requests to the load balancer, which then dispatches the requests to the target servers following a static policy. the target servers are assumed to be arranged in a parallel topology, which means that once the server finishes processing the requests, it will return the result directly to the user. another assumption is that there is a fixed number of jobs running in the system, which represents typical admission control (i.e., concurrency limits) in servers."
"3. analysis and methods this work starts by using a participatory action research approach. in this approach designers are invited to propose what they think are appropriate building thermal physics information for building design decision making. the advantages of using this type of approach is that beneficiaries themselves propose a solution to their own problems eliminating the needs for further tests. examples of meaningful information for design decision making, from a designer's viewpoint, are extracted from a sample of 140 design journals. these journals narrate all steps used to solve a design problem which included thermal comfort, energy efficiency and the testing of passive design strategies (a summary of one of these journals is presented in annex 1). the data set is limited to the design of an office building envelope in which heat balance calculations were undertaken using simplified methods. hand calculations were used, instead of any kind of software, to prevent any bias by existing user interfaces to interfere with proposals. those sorts of calculations were also seen as an efficient mechanism to facilitate knowledge transfer of building thermal physics concepts to designers."
"11 breakdowns from bps are difficult to interpret especially when designers want to know where to act on the fabric and/or relate fabric and solar radiation to improve building behaviour. identifying the main contributors in the air heat balance breakdown could be initially informative. however, tracing information further in the inside and outside surface heat balance breakdowns is not an easy task -if at all possible. 12 even though elimination parametric can be considered a sub-case or special type of sensitivity analysis, the authors decide to treat it separately in this conceptual data model because it can be examined as a special case of analysis prone to automation 13 [cit] would enable designers to get a broader understanding of when energy is needed without being overloaded by large amounts of time series graphs with potentially minimal and/or meaningless extra information to 'be digested'."
"this evaluation targeted vulnerability detection tools for web apps, because they fall under a particular type of ebs which can be implemented by using mom platforms. among the state-of-the-art static analysis tools for detecting security vulnerabilities in web apps, three tools were executable while supporting java-based systems: xanitizer [cit], owasp orizon [cit], and sonarqube [cit] . i evaluated my prototype's accuracy in identifying vulnerable event communication channels by comparing its results against those three tools."
"load balancing services are increasingly offered by many cloud providers [cit] . requests are dispatched by the load balancer to end servers following certain load balancing policies. these policies normally aim at minimizing the imbalance between different servers to improve system throughput or to reduce response time. among commonly available load balancing polices, round robin is the most common one supported by major cloud providers [cit] . considering the heterogeneous resources available in the cloud, the weighted round robin policy, also offered by many cloud offerings, places more load to the servers with higher weight using a policy similar to round robin. this is much more suitable than round robin for cloud deployments to assign requests according to the capacity of the end servers. however, a challenging problem is how to decide the weights assigned to the servers. a simple intuitive way is to set the weights according to the computing power of the servers, but this ignores the fact that there could be different kinds of requests with different processing needs. these simple intuitive approaches do not guarantee an optimal performance for the system."
"these experiments are not controlled (not allowing phenomena to be isolated or variables to be separated). more importantly, these experiments are generally used to transform the situation from 'what it is' to something the designer likes better [cit] . this means the design process is a constant work in progress which only stops when designers decide this is the case (figure 1 )."
"the paper is a follow on to a previously published paper in this journal [cit] which proposed and described a framework within which thermal simulation post-processed information meaningful to building design decision making may be generated. the framework explored what information is relevant to designers and how it can be generated. it did not address in detail how to manage and use data representation and data display systems meaningful to design decision making. the current paper describes in detail a conceptual data model to address these issues. this conceptual model is a high-level description of the entity classes and the associations between pairs of these classes, which together order the data to effectively communicate simulation results to building designers. conceptual data models are used in computer science to organise information prior to the development of database/database management systems. the framework and conceptual data model are developed by considering building designers as the ultimate simulation tool users either directly or indirectly when supported by consultants, and are therefore developed to fit the building designer's 'modus operandi'."
further studies would be necessary to explore potential useful combinations of different zoom instances. are these more efficiently managed when directly combined (e.g. when zooming in time and location/orientation happen simultaneously) or are they are better managed if undertaken in sequence (e.g. zooming in time first and from there proceed to zooming into location/ orientation)? further explorations of combining different zooms could be used to refine preferred combinations and provide a user defined structure to request details on demand. table 6 -comparison 5: types of interaction with data and types of data display
"7. conclusions and future work this paper proposed a conceptual data model to present meaningful dynamic thermal simulation information for design decision making. it explained how the model was generated and how it could become operational, followed by examples of its applications to practical problems. rather than following a conventional statistical analysis on user preferences which would not cope with the idiosyncrasies of the design problems, different types of clients, different types of design practices, etc.; the authors proposed a totally custom-based approach."
"require: we have deployed each component of the system on amazon ec2. the configuration of different instances we use is shown in table ii . each experiment lasts for one hour. the number of clients, the think time as well as the workload differ for each experiment. all the instances are in the ireland eu-west-1a region."
"15 \"a strategy for managing information complexity in which only necessary or required information is displayed at any given time\" [cit] figure 6 -examples from types of interaction with data found in the dataset"
"18 bb101 specifies maximum environmental/operative temperatures allowed for schools in the uk: maximum of 120hs above 26c, no hours above 32 c and 0 hours where mean 'ti'-mean 'to' is greater than 5c. even though this metric is not directly listed in table 2 (section 4.2), this table is not supposed to be exhaustive. a series of metrics related to specific performance targets could be further included there to cover most of the current legislation and building regulations. having simulation output data information meaningful to design decision making in a hierarchical data structure and within lists, facilitates choices and the retracing of previous steps in querying results. it also facilitates setting up interfaces in which users can customise their own preferences either through the use of supervised machine learning techniques and/or by manually saving them to be retrieved in different projects. the hierarchical structure also facilitates the request for 'details on demand' to be further explored through simultaneous and/ or sequential zooms, as discussed in section 5.5."
"this successive data querying and filtering also involves reviewing the information generated from simplified methods. if this information is to be produced by dynamic thermal simulation tools, it should comply with the dynamic, systemic, non-linear and stochastic nature of building thermal physics phenomena. this compliance is achieved by using dynamic thermal modelling principles to revise and adapt metrics and analysis methods used in the data sample. metrics or quantities used to measure building behaviour are changed (e.g. air temperatures are replaced by environmental / operative temperatures, metrics related to simplified heat balance breakdowns are eliminated, etc.). appropriate analysis methods to post-process bps data into a format which match design aims are proposed in replacement of the simplified ones found in the data sample 8 (e.g. simplified heat balance breakdown results are replaced by elimination parametric tests to explain main causes of building behaviour, etc.)."
"some of these metric instances are adjusted to be more precise in delivering the information requested (e.g. air temperatures are replaced by environmental/ operative temperatures to provide a better indication of comfort; heating, cooling and lighting energy consumption are replaced by heating, cooling and lighting energy consumed per fuel type to account for other sources of energy supply). metric instances related to heat balance breakdowns are eliminated (see discussion in section 4.1). new metric instances related to comfort and passive building behaviour are introduced (e.g. pmv and working hours operating without hvac respectively). a series of metric instances related to cost are suggested [cit], to cite a few)."
"reduction-to identify vulnerable event communication channels, both inter-and intra-component flows are considered by combining the extracted event types with each component's control-flow graph (cfg). however considering a large-scale ebs, it may not be scalable to generate and traverse every component's cfg. to address this, we build an event flow graph (efg), which provides a macro perspective of target ebs (see fig. 3 ), and examines the efg in order to prune the components that are unnecessary for subsequent analyses."
"our method is an instance of learning from demonstration [cit], where the robot learns a policy based on examples. a significant amount of work in this area [cit] has applied function approximation techniques to learn the policy from examples provided by a human or another robot. 1 we adopt the same principle, and use an extension to online learning with kernel machines [cit] as the function approximator."
"we test our central hypothesis -that our method enables novice users to customize prior retargetting functions to good outcomes -in a user study. the results confirm the hypothesis, and point the way to future improvements. one of our users had a particularly hard time providing good examples: although he did improve a bad prior in one condition, the examples he gave to a good prior in a different condition made this prior drastically worse. our analysis revealed that a major problem was the lack of visual feedback on his input: much like an actor needs a mirror to verify his pose, a user needs to verify his input before providing an example. overall, we believe that the ability to customize retargetting functions online is key to a better teleoperation experience. we are excited about pursuing this research further, by developing better ways of giving feedback to the users, making the effects of examples more intuitive, and exploring methods for generalizing customized retargetting functions to new tasks."
"proprioception was not the only factor contributing to this user's poor training performance. fig.8 shows the difference between this user and second user, with better training performance: the latter explores more parts of the space, including configurations close to those necessary for the grasping task that served as a test after training, which puts him at an advantage. we see this as a side effect of not having told users what the task was ahead of time, in order to prevent them from training the priors only for the test task: a user that missed the task-related areas of the space performed worse on the task, stressing the importance of task-specific retargetting functions."
"first, large changes have different semantics than small changes when training the retargetting function. usually, large changes need to be propagated wider through the space (i.e. with a very wide kernel), because they are meant to change the prior in a fundamental way -the prior is in some way offset by a considerable amount from the desired mapping. small changes, however, should affect the space only locally (i.e. with a narrow kernel), because they usually represent local corrections."
"we introduced a method for enabling users to customize the motion retargetting function that maps their input onto the robot, by providing example correspondences during teleoperation. we validated its usability in a user study, which also pointed to a possible danger of the setup: poor proprioception can result in poor examples. in future work, we plan to explore the design of an interface that provides more feedback to the users as they give examples, and analyze long-term use effects. we are excited about a future where robots learn to better interface with their users, and believe that this work gets one step closer to that ideal."
"robot learns new mapping as the margin increases, we are more and more confident that the difference between u p  t and n p is greater than the negative margin. the same figure also depicts a typical non-inferiority test, where the lower bound for the 95% confidence interval must be greater than the negative margin. fig. 7 . the user's poor self-perception leads to poor examples. this is a focus side-effect: focusing on fixing a particular problem sometimes introduces undesired additional offsets. on the left, the approximate intended example, and on the right the provided example, which adds a large offset to the roll. timing results were analogous, and fig.6 plots the means for both success and time over the four conditions. we attribute the slight mean decrease in success rate of n p  t to noise."
"finally, we want to demonstrate that the trained unnatural prior does not perform worse than the natural prior and the trained natural prior. we define \"does not perform worse than\" using the concept of \"non-inferiority\" [cit] : a distribution is non-inferior to another if their difference is significantly greater than a negative margin. h3. u p  t is non-inferior to n p and n p  t ."
"in the teleoperation community, unlike in animation, motion retargetting happens in real-time, and across the entire space rather than along a pre-set trajectory. while there are many interfaces for teleoperation, especially for robot manipulators [cit], the retargetting function is usually designed a-priori and cannot be changed. some robots, however, such as the davinci from intuitive surgical [cit], allow users to change the retargetting function online, albeit only by a constant (or global) offset, equal throughout the input space. the surgeon operating the davinci can use a clutch to stop teleoperation and reset himself to a comfortable position [cit], thus offsetting the origin of the retargetting function. our approach extends this idea to variable offsets: the user can modify different parts of the space in different ways, and can reach any (smooth) retargetting function."
"rbf-least-squares uses a fixed kernel width w for all examples. the changes propagate in the same way to the rest of the space, independently of the nature of the change, as in fig.3(left) . however, two aspects of our problem make this undesirable: the semantics of changes to the prior, and constraints on the maximum acceleration of the robot."
"2) online learning: given our representation, we want to find the function that best fits the user examples: a minimization of a loss function over the space of representations. we choose a typical loss function composed of the sum squared error between the user examples and the prediction, subject to regularization:"
"2) subject allocation: we chose a within-subjects design, where each of our 6 users (all students at carnegie mellon, all familiar with herb but not with the teleoperation interface) was assigned to all four conditions, in order to enable paired comparisons among the conditions. we first had an introductory phase, where the users interacted with the natural prior n p . we then tasked the users with training both priors. we instructed them to train the functions for general purposes, thinking about any task they might want to achieve with the robot, and stop when they are satisfied with the retargetting function they reached. after training, we moved the robot in front of a table and tasked the users with testing each of the four functions (the two untrained and the two trained) on a simple manipulation task -grasping a bottle on the table. to avoid ordering effects, we randomized the order of the conditions. each user attempted each task three times, in order to reduce the variance in the data."
"with every new example (x i, q i ), the method discounts previous data and adds an additional term to the function, which places an offset at x i proportional to the desired change and propagates to the rest of the space according to the kernel width corresponding to w. therefore, the algorithm satisfies all of the requirements from section ii-a, and scales linearly with the number of examples."
"when teleoperating a robot (or an animated character), we rely on a retargetting function -a function that maps the user's input onto the robot. this function must be different for different robots, because of their variation in kinematics [cit] . similarly, it must be different for different users, because of their variation in preferences: not every user will want the same retargetting function, much like how not every user likes the same graphical interface [cit] . finally, the retargetting function must be different for different tasks, e.g. pushing the buttons on the microwave panel requires more finesse than waving an arm in greeting, and even at different configurations for the same task, e.g. grasping an object to the far left, near joint limits, is more challenging than grasping it in the middle of the workspace. in this work, we explore the idea of enabling users to customize the retargetting function. our method can be used to adapt any prior function to the three factors stated above: the kinematics of a particular robot, the user's preferences, or the specific task at hand."
"therefore, changes should be incorporated online, and in real-time. 2) smooth: the function o(x) needs to be smooth, as there can be no sudden shifts that cause the robot to move abruptly while the user is barely changing pose. 3) propagation decay: different areas of the input space require different offsets. therefore, examples typically have local support and should decay as we move away from them in the input space to allow different alterations in different areas."
"3) dependent variables: for each condition, we measured the success rate and the time to completion for the grasping attempts. potential failures included knocking over the target object, colliding with the table, or being unable to complete the task within 200 seconds."
"the largest outlier in our anova was a user who performed much worse on the trained natural prior, n p  t, than on the untrained prior n p . this user commented during the study that proprioception was an issue (\"it's really difficult sometimes because i am not good at perceiving myself\"). indeed, poor perception of the input causes users to provide poor examples, especially when focusing on fixing a particular problem in the prior and introducing undesired offsets in other dimensions. an example is in fig.7 : the user was fixing a problem with the elbow angle, but introduces a large offset in the shoulder roll with his example."
"therefore, we have two additional requirements: larger changes must propagate further, and the maximum curvature has to be bound. we resolve these two requirements by keeping the curvature c at the center of the kernels constant. fig.3 shows that this enables us to enforce the first requirement: larger changes do propagate further. with respect to the curvature requirement, we assume that examples are sparse enough that the curvature at the center of the kernel is negligibly affected by other examples. then, the maximum curvature of the offset function o(x) occurs at an example point x i, and is therefore c. as a result, the maximum curvature of f (x) will only depend on c, and can be selected to satisfy the desired bound."
"although n p is a natural retargetting function in general, the kinematic differences between a human and herb make particular tasks (such as grasping an object on the far left of the robot) particularly difficult. our method enables the online adaptation of n p to such tasks: fig.9 shows that an expert user can provide an example that makes the task area easier to reach. due to propagation decay, the example does not have undesired effects on other areas of the space (unlike a global offset technique, see fig.10 ): trained retargetting functions must be more than global offset versions of their priors. while an expert user was not able to succeed within 200 seconds with n p, it only took 27 seconds to provide the example and complete the task. this is in no way reflective of the performance of a novice user with the system -it is merely a vote of confidence in what the training method enables an experienced user to accomplish."
"we designed a study to decide whether users would be able to repair a poor retargetting prior using our system. we used the herb platform [cit] for our experiments. herb comprises of two barrett wam arms mounted on top of a segway rmp 200 platform. the wam arm joints are roughly anthropomorphic, with a total of 7dofs made up of a 3dof shoulder, a 1dof elbow, and a 3dof wrist. herb's joint limits, as well as its link lengths and proportion, differ from those of an average human."
"our empirical evaluation is focused on the question \"does our method enable novice users to successfully train prior retargetting functions?\". we answer this in a study that tests the methods's usability, where we ask users familiar with the robot, but not with the teleoperation interface, to train a bad prior. we verify that the trained prior's performance reaches the level of a good retargetting function, showing that the training was successful. aside from improving bad priors, our method also enables adaptation of good priors to particular tasks. testing this particular aspect with novice users remains a future work goal. however, we show an example from an expert user on a difficult task, where the time taken to train the prior and complete the task is much lower that attempting the task without training."
"second, safety and hardware constraints suggest a bound on the acceleration of the robot. this is equivalent to bounding the curvature of the retargetting function."
"in this section, we outline the requirements of this training process and recap an online learning algorithm from the machine learning literature that meets these requirements. finally, we present a novel modification to the algorithm that incorporates domain knowledge to approximate retargetting functions with fewer examples."
"the heuristic is based on iterated local search (ils) [cit] . this method combines iteratively a local search with a perturbation phase. the local search heuristic inserts new visits to a route, one by one. for each visit i that can be inserted, the cheapest insertion time (shif t i ) is determined. for each of these visits the heuristic calculates a ratio, which relates the tourist score of the poi (the estimated satisfaction for a given tourist) to the time required to visit it. among them, the heuristic selects the one with the highest ratio for insertion. this process is repeated until no more pois can be inserted. a perturbation phase removes consecutive pois from a route. after the removal, the heuristic shifts all visits following the removed visits towards the beginning of the route as much as possible, in order to avoid unnecessary waiting. the perturbation procedure and the local search heuristic are executed until a termination criterion is met. the heuristic returns the incumbent solution as the result."
"jackknife process can be viewed as a special instance of k-fold when k is n-1, where n is the number of samples. while the jackknife method is recognized as the least arbitrary that outputs unique results on the given benchmark dataset [cit], the k-fold method offers an advantage whereby all instances or observations in the dataset can be used in both the training and test phases."
"nowadays, the generation of personalized tourist routes that take into account the profile of the tourist and up-to-date attraction information is a time consuming task. during the pre-trip stage, tourists have to spend their time collecting information about the destination in order to start planning the stay. this is a difficult task because they have to choose the pois they are going to visit and estimate visits duration and traveling times."
"we have modeled the tourist planning problem, integrating public transportation, as the time dependent team orienteering problem with time windows (tdtoptw). we have designed a heuristic able to solve it in real-time, precalculating the average travel times between each pair of pois in a preprocessing step. moreover, the system includes the basic functionalities required to customize the generated route. they allow to move, add and remove visits in a route."
"the dataset for glycation was obtained from publically available and widely used cplm database [cit] (available http://cplm.biocuckoo.org/) that was curated from comprehensive clinical and in vitro studies [cit] . the benchmark dataset we retrieved was filtered for redundant sequences with a threshold of 30% for pairwise sequence identity. the final dataset consisted of 1753 lysine sites in total found in 55 proteins. among them, 235 lysines are glycated and 1518 are non-glycated sites. the primary sequences used to build glystruct are included in supplement as the additional file 1."
"we have evaluated the performance of our approach against test instances for the city of san sebastian. results are able to obtain routes in real time (worst case of 0.2 seconds for a 2 day 8 hour per day route) in a scenario with 50 pois, 26 public transportation lines and 467 stops, and are comparable to the best results available on the literature [cit] . moreover, most available examples are not able to generate routes in near real-time and the times they require to obtain a solution for comparable test instances are an order of magnitude higher. [cit] ."
"future works consists on extending the system to more cities with a different public transport network topology. the next one consists on integrating an advanced recommendation system in a wholly functional pet. finally, we would like to insert social network capabilities, allowing to store, share and add travel experiences to better help tourists on the destination."
"we compared our results to the state of the art of bioinformatics study on glycation gly-pseacc [cit], which was the only predictor that had the webserver available for testing our dataset."
"the effectiveness of any classifier is measured using cross-validation methods. the three most widely used cross-validation schemes across the literature are independent dataset, k-fold and jackknife [cit] . since the dataset for glycation in the curated protein sequences is limited, it was not practical to obtain additional data to run independent test validation."
"vansteenwegen [cit] and souffriau [cit] present an extensive review of existing personalized electronic tourist guides (pets). however, none of these guides uses advanced heuristics to solve the planning problem nor integrates the use of public transportation. although dynamic tour guide [cit] generated and recalculated routes in real-time (less than five seconds), the routing algorithm was very simple and it had important restrictions: it could only create proper solutions for one day routes and a small number of pois. a pet called p-tour [cit] applied a genetic algorithm to calculate routes. nevertheless, tourists had to manually enter the pois they wanted to visit with their details (visiting time, duration and tourist score). moreover, the system needed nearly ten seconds to obtain a route for just twelve pois. again, no public transportation was taken into account. [cit] described a multiagent based system for planning tourist visits. although they mentioned transportation, they did not integrate selection and routing nor give hints or details about the implementation or performance. [cit] presented rose, a mobile application assisting pedestrians to find preferred events and comfortable public transport connections composed by three main services: recommendation, route generation and navigation. they identified the route planning problem to solve and they described it as a multiple destination recommendation with public transportation and time windows. this is the same problem we propose. however, they did not find any algorithm able to solve it."
"mathew's correlation coefficient (mcc) metric essentially measures the quality of classification for a classifier. this metric varies between -1 (total misclassification), 0 (no better than random prediction), and 1 (perfect prediction of test instances)."
"otherwise, if the real travel time is larger than the average one, we arrive later to the visit. if this causes a visit to become infeasible, we remove it from the route and we update the route, moving the rest of the visits towards the beginning."
"with an exception of glynn and pregly, all other state of the art methods including glystruct have obtained data from cplm database. however, there is a significant difference in datasets attributed to regular updates to databanks, the inconsistencies in the selection of primary sequence identity threshold by various authors, and filtering techniques employed to the negative instances of the dataset before training the classifier. nonetheless, we made comparison with the published results of those methods, which we could not verify through standard means of webservers or codes. the glypre method published high specificity of 0.9078 but recorded average sensitivity of 0.5747 compared to 0.7013 achieved by glystruct. the accuracy and mcc for glypre were reported to be marginally higher at 0.7968 and 0.52 respectively compared to 0.7562 and 0.51 respectively for glystruct. furthermore, iprotgly-ss published high sensitivity of 0.9238. however, it recorded lower specificity of 0.6009 compared to 0.7989 by glystruct. all comparisons are made for 10-fold cross validation which tend to produce best results."
"-recommendation. combining the tourist information of the destination with the tourist profile, the system has to create a list of recommended pois. as a result, pois will have a different score and visit duration for different tourists. interested readers will find a deep review on tourist recommendation systems on the recent paper from kabassi [cit] . the system we present in this paper focuses on the last two functionalities. first (section 4) it applies an advanced or algorithm to generate personalized tourist routes that take advantage of the existing public transportation network. once a personalized route is generated, it gives to the tourists the opportunity to customize it (section 5)."
"finally, once we find a final solution, we run a repair procedure introducing the real travel times between the pois. starting from the first poi of the route, we compare the average and the real travel time, taking into account the real leave time. if the real travel time is smaller than the average one, the travel time is adapted by advancing the arrive time to the visit towards the beginning of the route as much as possible. the waiting time and the leave time of the visit are also updated."
"once the system has generated a personalized route that maximizes the tourist satisfaction, the tourist can decide to change this route to better fit his/her own interests. although this customization step is voluntary, it greatly increases the flexibility of the system."
"to predict glycation sites with high accuracy and to address the shortcoming of those previous studies, we introduce a new machine learning method called glystruct to predict glycation of lysines. to develop glystruct predictor, we incorporated structural information extracted from the predicted local structure of protein sequences as our input feature set and employed support vector machine (svm) as a classifier [cit] . our achieved results demonstrate that glystruct is capable of predicting both, the glycated and non-glycated lysine residues better than previously proposed method found in the literature for this task. using glystruct, we achieved 0.7013, 0.7989, 0.7562, and 0.5065 for sensitivity, specificity, accuracy and mathew's correlation coefficient, respectively for the 10-fold cross validation."
"in clinical methods, ptms are identified in wet labs by observing this modification using methods such as mass spectrometry and immunofluorescence, and stored in online databanks such as dbptm, cplm and plmd [1, [cit] . despite ptm being an important area for morbidity detection and genetics, clinical approaches face great limitation due to the plethora of protein sequences in existence in data repositories [cit], high costs, and time-consuming process of biochemical experimentations in wet-labs [cit] . hence, data scientists have been exhorted to actively pursue the development of computational tools to provide cost-effective solutions [3, [cit] . this has led to an evolution of data mining in medicine, especially in the area of proteomics [cit] . a concerted international effort has seen large dataset being actively developed to study and predict site-specific protein modification [cit] ."
"local backbone angles refer to the torsion angles between neighbouring amino acids that provide backbone conformations (local structure) of a polypeptide. they complement the information provided by asa and the secondary structure predictions (sspre) [cit] of amino acids. the predicted backbone torsion angles, , , , , represent the interaction of local amino acid along the protein backbone [cit] as shown in fig. 1 [cit] .  and  demonstrate the torsion angles among the molecules inside one single amino acid with respect the neighboring molecules. on the other hand,  and  demonstrates torsion angles between alpha carbons (c) in neighboring amino acids [cit] . in fact,  determines torsion angles between three neighboring c and c i  1  c i  c i + 1 while  determines the torsion angles between two neighboring c and c i  c i + 1 . while secondary structure provides the general elucidation around sections of peptide, local backbone angles provide elaboration of structure within the locality of ptm points, the latter being lysine residue in this case."
"glycation endproducts (ages). with aging, ages accumulate and alters the tissue protein structure, function and turnover. if untreated, ages can lead to chronic complications of diabetes mellitus and neurodegenerative changes such as alzheimer's disease and amyotrophic lateral sclerosis [cit] . moreover, correlations have been established between levels of ages and diabetes with its related complications [7, 20, [cit] in aging homo sapiens. glycation being a non-enzymatic reaction presents a great challenge in detection due to the motifs having greater levels of entropy compared to other ptms. conversely, enzymatic reaction is characterized by a more specific reaction and often has more biased sequence motif [cit] ."
accuracy (acc) metric is measured as the total number of both glycated and non-glycated lysine residues correctly classified over the total number of test instances (n). this metric also takes on values between 0 (totally inaccurate) and a 1 (totally accurate).
"the customization is based on six basic operations a pet should provide. the difficulty of these operations is directly related to the public transportation network. as the leave time from the previous poi changes, the travel time to the next poi does too. thus, all the affected traveling times have to be recalculated. this process varies in function of the executed operation:"
"-walk 2 minutes from hotel nh to bus stop avda tolosa 5 -wait 5 minutes for bus number 5 -take the bus and travel 12 minutes until bus stop buen pastor cathedral -walk 1 minute to buen pastor cathedral apart from the functionalities presented on the previous sections, once the customization process is finished, a tourist can access the details of the whole route. besides depicting all pois on a map, the system provides full details for each visit. as an additional functionality, tourists can generate and download a pdf of the whole route. finally, tourists can navigate the destination using a map based interface. we show all pois on a map and tourists can choose to access details about any poi."
"a personalized electronic tourist guide (pet) should perform at least the same task fulfilled by the lto, on a hand-held device, providing three main functionalities: recommendation, route generation and customization. a pet should select the most interesting pois for tourists (recommendation); generate routes in real-time maximizing tourists' satisfaction taking into account different restrictions, attractions' attributes (opening and closing times, visit duration, entrance fee, etc.) and traveling times (route generation); and allow further adaptations to the route (customization)."
the complete calculation of the average travel time matrix for an instance with 50 pois takes around 90 minutes on a pc intel core 2 quad with 2.40 ghz processors and 2 gb ram. the average travel times are stored in a database for a later fast retrieval.
"furthermore, public transportation information should be integrated, since that is identified as one of the most appreciated functionalities of a pet [cit] . in this paper we integrate public transportation and present a prototype that focuses on the route generation and customization."
"to build our predictor model, benchmark dataset was curated from the online databanks. following the standard methodology in bioinformatics [cit], the dataset was then formulated to make it suitable for training classifiers and an appropriate cross-validation scheme was used to objectively evaluate the accuracy of the predictor."
"the desktop client offers two main functionalities: generation of personalized tourist routes and their customization. besides, it allows to navigate through the destination, to load previously customized routes, to update user's profile and to update the tourist score and visit duration."
"the model we propose is the time dependent toptw (tdtoptw) [cit], which combines the previous models and makes possible to integrate one or more public transportation networks to travel between pois. the travel time required to move from one poi to the next, will vary according to the transportation mode (walking or public transportation) and the leave time. to the authors' understanding, there is no algorithm able to solve the tdtoptw."
"the mobile client offers the same functionality, except for the customization. it has been taken out in order to improve the user friendliness of the prototype. the interface has been adapted to use with touch screens. figure 3(a) shows the main screen with its button enlarged. figure 3 (b) shows a map with a personalized route and the details about one visit. due to the reduced screen size, on the mobile client the details are shown on a new window."
"we have designed and implemented a client server architecture. the client consists on a thick client executed on a web browser. the server is constituted by a database (mysql), and an application server (apache tomcat). the client is a web client based on google web toolkit (gwt). gwt is a framework from google allowing to develop a web application on java. gwt's compiler transforms the java code on the corresponding html and javascript code compatible with main browsers (including iphone's and android's browsers)."
"to evaluate our glystruct predictor, we carried out k-fold cross validation for 6-, 8-and 10-folds and jackknife test which is a common practice [cit] ."
"we obtained promising results from the glystruct predictor presented in table 1 . for statistical stability, we took an average of 50 runs for each cross-validation fold. we obtained the highest sensitivity 0.7059 for 8-fold cross-validation while other folds recorded marginally lower sensitivity within 1 %. we also achieved high specificity at 0.7989 for 10-fold with a deviation of half percent for other folds. the best values of accuracy and mcc were 0.7562, and 0.5065 respectively (both in 10-fold). the 6-fold results yielded slightly lower than other folds with 0.6984, 0.795, 0.7528 and 0.4983 for sensitivity, specificity, accuracy and mcc, respectively. the aucs were 0.7935, 0.7927 and 0.7839 (fig. 4), for 10-, 8-and 6-folds, respectively. mathew's correlation coefficient (mcc) is around 0.5 for each fold indicating that the predictor performance is promising for glycation prediction. jackknife procedure yielded highest sensitivity of 0.7404 and, specificity, accuracy, and mcc were 0.7793, 0.7622 and 0.5186 respectively."
"there was a notable increase in the sensitivity of 0.6845 for gly-pseaac method with our dataset from their reported value of 0.5748 for 10-fold. we anticipate that most of the protein sequences we tested on their webserver may have been used in training their model primarily because of the limited datasets available publically in databanks. in addition, the gly-pseaac server has been tuned to a threshold probability of 0.35 allowing higher misclassification of negative samples leading to very high fall out or false positive rate averaging 32% for the three k-fold validation schemes. high false positive rate may have a serious bearing on the clinical significance in terms of better morbidity detection. in contrast, the specificity of gly-pseaac for 10-fold was reduced to 0.6745 from the reported 0.8017 and mcc was also slightly lower on our dataset (0.3587 compared to their reported 0.38). the accuracy was also slightly lower (0.6784) compared to their reported results (0.6812). in order to show the significance of the achieved results for glystruct, pairwise t-test was conducted. the p-values obtained were 0.025, 0.019, 0.025 for 10-, 8-and 6-folds respectively. these p-values are less than 0.05, which demonstrates that improvement on performance by glystruct is significant compared to glypseaac. significance of contribution and the false discovery rates were also tested for each feature used. all features were found to be significant contributors to the results obtained. the aforementioned test results are included in additional file 1."
"we have followed the best practices promoted by google [cit], which can be summarized as the adoption of the following elements: dependency injection, model-view-presenter (mvp) pattern, bus event and browser history. on the next subsections we briefly introduce both backends of the prototype."
"-add a visit. when a tourist wants to add a new visit to a route, there are two possibilities to determine the insertion position. in the first one, the system finds the best insertion point, minimizing the total time (travel plus wait plus visit times). the second possibility consists on the tourist choosing the insertion order within a day. in any case, the system has to update the order of the following visits. the system has to insert the new visit and update the travel and wait times starting from the previous visit. -remove a visit. in this case the system has to remove a visit. thus, all the following visits of the day are moved towards the beginning. starting from the visit preceding the removed one, traveling and waiting times have to be recalculated until the end of the day is reached. -move a visit towards the beginning of the route. when a tourist wants to visit a poi earlier than scheduled, the system swaps this visit and the previous one. starting from the one before the previous visit, traveling and waiting times have to be recalculated until the end of the day is reached. obviously, this move can be repeated to schedule the visit even earlier."
"the dataset retrieved by gly-pseaac authors from cplm database is larger than glynn and pregly, which consisted 223 positive and 446 negative samples filtered from 72 protein sequences with 40% pairwise sequence identity. their dataset is slightly different (by approximately 5% for positive samples) from the glystruct dataset of 235 positive and 303 negative samples from 55 proteins obtained after filtering with a threshold of 30% pairwise sequence identity. therefore, to compare the performance of gly-pseaac webserver, we uploaded our dataset manually to the gly-pseaac webserver by creating a fasta file format. the performance results we obtained from the webserver are presented together with the glystruct performance in table 1 ."
"visit a poi later than scheduled, the system swaps this visit and the next one. starting from the previous visit, traveling and waiting times have to be recalculated until the end of the day is reached. -move a visit to the previous/next day. when a tourist wants to move a visit to the previous/next day, first the system has to remove the visit from the actual day. once this operation is completed, the system finds the best insertion position (smallest required extra time) for the poi on the previous/next day. -customization exception. once a tourist starts customizing a route, it is possible that a violation of some of the restrictions occurs. as the customization is a hand-made process realized by a tourist, in these cases the system shows an alert asking for confirmation before committing the change."
"selecting a visit, more information about it and its poi (arrival time, visit duration, leave time) can be seen. moreover, tourists can start customizing the route using five different buttons (figure 2(b) ). four of the buttons represent a direction: up, to move a visit towards the beginning of a route; down, to move a visit towards the end of a route; left, to move a visit to the previous day; and right, to move a visit to the next day. and finally, the last one, the one with a minus character label, allows to remove the visit. finally, a button with a plus character allows to add new visit. tourists have to choose whether they want to add the new visit at a certain order, at the end of the day, or at the optimal order (the one minimizing total route time). once the customization process is over, the tourists can access a detailed summary of the route and generate a downloadable pdf."
"the k-fold cross-validation procedure is carried out by first partitioning the total benchmark dataset into k roughly equal folds. then one fold is held as a test set and the remaining k  1 folds are used to train the classifier and a model is constructed. using the constructed model and the test dataset that was held out, all prediction metrics are computed. this procedure is repeated k times as per the fold number chosen to obtain the average of the performance metrics."
accessible surface area (asa) provides an estimate surface area of a particular amino acid reachable by a solvent situated in the protein's three-dimensional configuration [cit] . the predicted values of asa for individual amino acids hence provides essential information of how it locally interacts with other amino acids to build global protein structure.
"we have modeled the problem of generating personalized tourist routes with public transportation as a tdtoptw. after analyzing different approaches, we finally designed a heuristic able to solve it in real-time [cit], based on a fast heuristic for the toptw [cit] . the inclusion of public transportation makes the process of building a solution much more complex: each distance calculation between two pois becomes a time dependent shortest path problem (tdsp). a small increase in the planned leave time from one poi can cause a significant increase in the arrival time to the next poi. for instance, when a tourist just misses the bus and has to wait for the next one or walk to the next poi. thus, in order to handle the difficulty of the tdtoptw, we apply a hybrid approach combining two different heuristics. each of them focuses on a different aspect of the problem."
"the secondary structure features reveal intrinsic information regarding the characteristics of a protein sequence. in this study, we considered three attributes that formulate the local structure of protein namely, the secondary structure, local backbone torsion angles, and accessible surface area (asa). the prediction of those attributes was carried out using the spider2 toolbox [cit] . the spider2 toolbox demonstrated promising result predicting these attributes compared to other methods found in the literature for predicting secondary structure [cit], backbone angles [cit], and accessible surface area [cit] of amino acids. predicted results using spider2 has been used in different studies and demonstrated promising results [cit] . the following describes the features integrated in this work:"
"finally, we have shown a prototype which generates and customizes routes in real-time. this prototype has a desktop and a mobile frontend and has been successfully evaluated against test instances. [cit] in the city of san sebastian in order to evaluate the quality of the generated personalized tourist routes from the tourists' point of view."
"although examples are present of finding the shortest path in public transportation networks or time dependent networks, they all focus on solving individual queries. [cit] provide extensive information about the earliest arrival problem (eap). [cit] focused on finding the best departure time to minimize the total travel time on time dependent graphs. a large number of public transportation providers implement these algorithms. for example, the public transportation service provider of san sebastian has a best path finding application to move between two points (http://www.dbus.es)."
"the true positive rate or sensitivity is an important performance indicator of the ability of the classifier to predict the glycated lysine residues correctly. the metric varies between 0, (that is classifier is totally inaccurate) and 1 (signifying the classifier is totally accurate). hence the higher the true positive rate, the better the classifier performance is at detecting the glycated lysine residue. sensitivity is given by"
"with this input data, the system generates a personalized tourist route in real time (less than 5 seconds). this route takes advantage of the public transportation network of the city, shortening the traveling times and increasing the mobility of tourists. regarding the traveling information, the travel between the previous poi and the actual poi is divided in transportation links. each transportation link stores details about how the traveling is done. if a tourist goes on foot the distance between pois and the estimated travel times are stored. otherwise, for each public transportation involved, the traveling distance from/to the previous/actual poi/stop are stored, containing details about the public transportation line, the estimated waiting and traveling times."
"where tp (true positive) denotes the number of correctly identified glycated instances from the test set, and fn (false negative) denotes the number of incorrectly classified glycated sites."
"in svm algorithm (eq. 3), the margin between hyperplanes needs to be minimized, which represent boundaries between classes (of glycated and non-glycated lysines). if the boundaries are non-linear, kernels functions are used [cit] . the kernel functions can be non-linear such as radial basis function (rbf), polynomial and sigmoid. in this work, we designed our glystruct predictor using svm with a polynomial kernel"
"when tourists are at a destination, this task is often done by the staff of the local tourist organizations (ltos). the staff determines the profile of the tourists (cultural, romantic, family, etc.) and their restrictions (time, money, etc.). combining this information with knowledge about local attractions (location, price, timetable, etc.), the lto suggests a personalized route for the tourist. finally, they make small changes to this personalized route based on the feedback from the tourist."
the true negative rate or specificity is the ability of the classifier to identify negative (non-glycated) instances. this metric also has a range between a value of 0 (totally incorrect) and a 1 (totally correct) in classifying the non-glycated lysine residues. tn (true negative) denotes the number of non-glycated instances identified and fp (false positive) denote the non-glycated sites identified as glycated.
"svm works by establishing an optimal hyperplane between classes and extends to patterns that are not linearly separable by using kernel functions. if the dimensionality of feature vectors is very high, then dimensionality reduction techniques can be employed before svm application [cit] ."
"the prototype has been initialized with data about the city of san sebastian. san sebastian is a spanish tourist city located near the french border. it has around 200.000 inhabitants and 50 pois. most tourists visit the city by combining public transportation with short walks. the public transportation network includes 467 bus stops, 26 lines and more than 65.000 direct bus connections between stops. the prototype has two different frontends. the first one is targeted for desktop interaction on the pre-trip stage. the second one is targeting mobile devices with a touch friendly user interface. in relation to the application's interaction with the user, first of all, tourists have to enter their restrictions. moreover, the system has to know their profile in order to assign the correct tourist score to each poi. although there are advanced approaches available on the literature [cit], this step is out of the scope of this paper. thus, we have followed a simple approach, having four different profiles (nature, culture, gastronomy, leisure). for each poi and profile, the system administrator has to assign the estimated tourist score (0-100) and the estimated visit duration. tourists have the opportunity to overwrite these scores. they can update both the tourist score and the visit duration. besides, they can mark some pois as must-seen/must-avoid poi. the system automatically assigns these pois a tourist score of 100/0."
in section 2 we review the state of the art regarding pets and operations research (or) algorithms. in section 3 we give an overview of the general functionalities of a pet we are interested in. in section 4 we focus on the route generation algorithm. in section 5 we introduce the customization functionalities. in section 6 we show our prototype. finally in section 8 we summarize the conclusions and some further work.
"we have presented an intelligent routing system able to generate and customize personalized tourist routes in real-time and taking into account public transportation. although the whole process is divided in three steps, we have focused only on the last two: route generation and route customization."
"overall, our predictor glystruct, using only structural features of peptides and svm as a classifier produced consistent results (averaged out with 50 runs of cross-validation for each fold) in all the metrics and for all folds. it was better performing than the comparator method, gly-pseaac. with other state of the art the prime motivation to develop a prediction model for glycation is to for clinical support in timely diagnosis of morbidity and cellular conditions in a cost-effective manner. however, for prediction of ptm like glycation, we need to be mindful of the fact that while sensitivity is highly desired to identify the glycation process, making a false positive prediction can lead to potentially lethal situation. in such cases of false positive prediction, the medical professional may administer medication which would lead to further lowering of blood glucose concentration causing an induced hypoglycemia which can be fatal if not managed well [cit] . the prediction model we developed has a low false positive rate (or high specificity) that can be instrumental in avoiding the induced hypoglycemia situation."
"as figure 1 illustrates, the process is not necessarily unidirectional between the separate stages, and ultimately the entire process may involve elements of iteration in order to build confidence in the results from the discovery process."
"gaussian processes are a popular model that can be used for both classification and regression tasks. they are a probabilistic model, and so one of their great strengths is in quantifying uncertainty. that is, not only will the model provide a prediction, but it can also provide a measure of confidence in the prediction. [cit] exploited this aspect of gaussian processes to propose a new audiogram estimation technique that can significantly reduce the time required to measure an audiogram. one practical application in audiology is that the uncertainty measure can be useful in detecting outliers within a data set: the outlier represents an (highly) unexpected setting or operation that may warrant further investigation as to the cause."
"during the training process, we use 10 000 images from database [cit] for training and 3 466 images for validation. but for deep learning methods, the amount of data has a significant impact on the performance. but to the best of our knowledge, only a few databases provide annotations of eye centers, which are not enough to support the fcn network. therefore, we use a data augmentation method to augment the available training images to improve the model performance on validation data. after splitting, the training set is augmented via affine transformation including rotation (+/-30 degrees) and scaling (0.75-1.25) and horizontal flipping to increase size of the training set."
"the data collected can be noisy or anomalous in a multitude of ways. for example, values in some part of the data set may be missing. often, records with missing values are simply dropped for a specific analysis. however, missing values can sometimes be imputed. [cit] provide a survey of such techniques. records that are either obviously wrong or unreliable may need to be filtered out before a full analysis can begin. these are two theoretical examples of the iteration required within the preprocessing stage of kdd. practical examples of missing, wrong, or unreliable data come from consideration of measurements of an audiogram. even with a complete audiogram, that is, no missing values, one may doubt the veracity (the fourth ''v'' described earlier) of some or all of the recorded values. this could be for several reasons, such as a nonorganic hearing loss, operator error, or the audiogram values being deliberately adjusted from their true value for purposes best known to the clinician. a second source of doubt could arise in the velocity (the second ''v'' described earlier) recorded in a data set. successive visits of a device to a clinic may prompt a repeat measure of the audiogram, thereby creating a time series of audiograms. there is an implicit assumption that this time series has been generated from or by the same wearer. large changes recorded between successive visits may have a valid explanation, such as a fluctuating hearing loss. however, the pattern of changes may also indicate that the device has been loaned temporarily at different times to multiple wearers. with this interpretation, a time series does not represent the experience of a unique wearer of the aid, could lead to error-filled analyses, and so may become a candidate for either separate handling or complete removal."
"classification is a supervised learning task. in contrast to clustering, meaningful groupings of the data are known a priori and are provided as labels. the task is to be able to example of subgroup discovery. in this example, the data are clustered into two clusters in both modality x and y. the two clusters of modality x are shown on the left of the figure with one cluster containing crosses and the other circles. the two clusters of modality y are shown in the center of the figure with one cluster containing blue points and the other red points. we consider the subgroup made from c x and c y (marked on figure) . the right of the figure shows the contingency table produced and the p value from a 2 test associated with the table. from this, we would conclude that the subgroup formed by c x and c y is interesting, and there is a dependence between the two modalities."
"the training procedure for eye center localization is similar to the one training fcn for sematic segmentation, which uses the images and labels of each pixels as input. we use heatmaps as the labels, which are generated by using landmarks of eye centers. landmarks of eye centers are encoded using the gaussian kernel to generate heatmaps at the provided location of the eye center landmarks. each eye center landmark has its own heatmap and allows the network to distinguish between two points more easily. the eye center localization network training is formulated as a per-pixel regression problem based on the ground-truth segmentation masks. formally, the objective function can be represented as the following formula:"
"data mining is the discovery and extraction of patterns and knowledge from large or complex data sets. this covers a wide variety of tasks including grouping or clustering, discovery of dependencies, and detection of anomalous examples within the data. with easier accessibility to larger amounts of data, there has been a greater focus on how to effectively make use of that data and adopt tools and processes that systematically provide new insight, where possible, into the relationships between data. this paradigm shift in data generation and availability has been termed big data, which is often characterized by the five vs [cit] :"
we take the effect size to be the ratio of the joint probability of clusters c x and c y and the product of the marginal probabilities of c x and c y . the marginal probability can be expressed in terms of the joint probability as follows:
"the structure of each tree in the forest is defined by the results generated from a set of training data via ''bagging.'' bagging (or bootstrap aggregating) is a procedure of sampling with replacement from the training set. in addition, the candidate dimensions for splitting at each node are a random subset of the total set of dimensions. the generation of each tree in the forest by the use of bagging and random subsets of candidate dimensions decreases the statistical dependence between trees, thereby improving the estimate of the final, aggregated, decision from the forest."
"for a task such as classification or regression, the objective is the predictive power of the learned model on new instances of data, which prompts several questions: (a) where does a newly acquired data set fit into the patterns from historic data sets and, if it does not, (b) does the model need updating, and finally, (c) how does that affect our decisions on patient management? a model that does not generalize to being able to obtain sensible predictions from new data, but models only the training data, is called ''overfitted'' and is comparatively useless."
"eye center localization has been an interesting topic in the field of computer vision in recent years. there are many factors that can affect performance of the eye center localization such as the significant variability situation of eye appearance from different illumination, shape, color and viewing angles. a good eye center localization system must be accurate and robust to these factors. early works tackle such difficulties using specialized devices like infrared cameras or head-mounted devices. this kind of devices is very popular in commercial areas since they could apply infrared illumination to localize the eye centers through corneal reflections. in that case, these devices could obtain a high accurate eye center location. however, it has some limitations in applications such as the high cost devices and the uncomfortable wearing experience. compared with these specialized devices, the approaches which directly localizing key point positions of eye center through computer vision and image processing techniques are more efficient since they only need a low-cost webcam instead of specific hardware devices and can be easily implemented. this method is often used as an alternative approach of infrared illumination in terms of the high accuracy and robustness."
"2) we regard the problem of eye center localization as a special semantic segmentation problem, which is a novel and important solution regarding the key and future directions for this area of research."
"clustering is the task of grouping observations (or instances) into groups known as clusters, given a training set of data containing observations. the goal is that instances in the same cluster should be more similar to each other than to instances in other clusters. unlike with classification, no labels are provided beforehand."
"in the future, we will explore the use of synthetic data as an alternative solution to this problem [cit] . and a deeper and complex network architecture and a more efficient strategy for the transformation of heatmaps will be explored to improve the performance though at the cost of more computation."
"because these probabilities are not known a priori, they are estimated from the data. the estimated effect size e is given by n times the ratio of the number of examples in both cluster c x and c y to the product of the number of examples in each cluster separately."
"another important consideration in evaluating the algorithm for eye center localization is its computational complexity. the computational complexity is measured by average processing time for each input image. we have conducted a comparison in the processing time of locating the eye centers on bioid database. we train a network model of the proposed method first through a desktop pc. and then deploy it on a standard laptop with an intel core i5 at 2.50ghz processor and 16gb of ram memory for eye center localization. the comparison of our method and other methods in average processing time is shown in table iv . in our experiment, the proposed method is more efficient and faster than all other state-of-the-art methods, taking 5ms per image on average. this shows that our proposed method is suitable for real time applications and embedded systems."
"clustering is a common task in exploratory data mining. it is an unsupervised learning task to identify meaningful groupings of the data into classes that are not known beforehand (''a priori'' or ''prior'') but instead are learned from the data. with clustering, points in the data set are grouped such that points within a given group are more similar to each other, in some sense, than points outside of the group. such exploratory data mining is useful in our context as it may help uncover common profiles of hearing or lifestyle. audiograms, for instance, can be clustered so that audiograms of similar shape are assigned into the same group [cit] . summary statistics of the learned clusters can often give a more informative, high-level, view about the composition of the users, and possibly even etiologies. in addition, such clustering may promote selection of a particular device tailored to better fit common profiles, for example, reverse slope when compared with presbyacusis audiograms. with a newto-market device, as the associated database grew, such a selection could be verified by comparing outcome measures across devices. the concept of a ''good'' (as in sensible and robust) grouping can vary quite considerably, and so there are numerous clustering algorithms in the data mining literature. a simple, yet often effective, workhorse for clustering is k-means [cit], pp. 21-33) . k-means is a method to find a number, k, of clusters such that the sum of the variance of all of the clusters is minimized. this is done with respect to some metric space, which is normally euclidean. an equation defining the objective function of k-means clustering is given in the supplementary material."
"e ye center localization refers to localizing the centers of human's pupil on given face images. locating these centers means that we could establish correspondence between two eyes of the person and the focused targets, which has been proven to be useful for computer vision and human computer interaction tasks such as eye gaze estimation and eye tracking. eye center localization is the first step towards eye gaze tracking and estimation in images and video [cit] . during the process of eye gaze estimation and tracking, we need to determine the precise pixel location of important key points of the eye center for a single given rgb image. moreover, achieving accurate eye center localization is useful for higher level tasks [cit] such as human attention control, driver monitoring system and sentiment analysis, and also serves as a fundamental tool in fields such as human computer interaction and animation."
"we have presented an overview of the field of data mining where we have 1. used the ''five vs'' as an outline framework for data properties that are necessary in order to support meaningful analyses; 2. used the (iterative) structure of the kdd process as a template for supporting the acquisition of knowledge from the analyses; and 3. illustrated methods for the tasks of classification, regression, clustering, and subgroup discovery within big data sets."
"to further demonstrate the overall performance of our method, we also use the minimum normalized error and the average normalized error to evaluate the performance to give an upper bound and an average error. the minimum normalized error and the average normalized error replace the maximum function in (6) with the minimum and average function respectively. in table i, we can find that accuracy of almost all errors are 100%, indicating a reliable accuracy for eye center localization."
"to demonstrate the potential value from data mining to the field of audiology, we follow up this overview with an application of the described techniques to a large hearing aid manufacturer's data set [cit] ."
"dimension is a synonym for an attribute or feature. an example entry, or instance, in the data set will be described by a set of dimensions. examples of dimensions are height, gender, and age, or a measure of absolute threshold at a single frequency."
"the kdd process used to acquire knowledge from data involves the following steps: 1. selection: selecting a data set with a subset of variables or data samples. selection is guided by prior domain knowledge and end-user goals. 2. preprocessing: cleaning the data set; this includes removing outliers or noise and handling missing data. 3. transformation: the data are further transformed into a form more useful for the data mining task; this can include reducing the number of feature variables to the most relevant, or projecting the features to a more useful space, such as a logarithmic rather than a linear scale. 4. data mining: applying the appropriate task and method to the data; tasks include classification, regression, clustering, and subgroup discovery. 5. interpretation/evaluation: task-dependent evaluation of the patterns learned via data mining; domain knowledge is used to assess whether these patterns make sense with respect to the domain to avoid spurious results."
"finally, according to the landmarks of eye centers, we transform these images to a heatmap as inputs of the network. note that the successful use of the network of sematic segmentation on eye center localization heavily depends on the generation of heatmap. we transform each landmark to a single heatmap using gaussian kernel. for the eye center localization problem, there are two landmarks (left and right eye center). this means that we need to generate 2 heatmaps for each eye image, which can be interpreted as a grayscale image in the range [cit] . the ground-truth landmark coordinates are set to white and the other position as black. in other words, a black heatmaps indicates that some landmarks are not recorded, so all pixels on this heatmap are set to 0. we use two formulas based on the gaussian kernel to generate heatmaps of eye center landmarks:"
"in this section, we introduce the proposed network architecture. we use the vgg16-fcn [cit] architecture as a basis for developing our eye center localization network. classical cnn uses the convolutional layers to extract local features in an image. on the top of convolutional layers, the fully connected layers use the inner product operation to integrate high-level local feature maps into a single feature vector to predict the label of each image. therefore, it is not able to predict the label for each pixel. recently, the trend has shifted towards using fcn to solve the dense prediction of each pixel. fcn is a special type of cnn, which replaces all fully connected layers with the convolutional layers and adds additional upsampling or deconvolution layers."
"here is a brief introduction of the structure of this paper. in section ii, we describe the related work on eye center localization and fully convolutional network. in section iii, we describe the methodology about our proposed network. we show results of experiment on the public dataset to evaluate the performance of our proposed method and other existing methods in section iv. finally, sections v and vi are the general discussion and conclusion."
"in this section, we first introduce the database including the training and test set, experimental settings and the evaluation metric. we have also compared with other existing state-ofthe-art methods on the public database including bioid [cit] and gi4e [cit] ."
"the decision that splits the data at each node is chosen to optimize some measure; a common measure used is the ''gini impurity.'' given a random relabeling of the data, where the labels are sampled from the distribution given by the proportion of labels at the node, the gini impurity is a measure of how often the random label would mismatch the true label. the smaller the value of the gini impurity, the better the decision separates the data with respect to the labels. it is zero when there is only one category in the node. an equation defining the gini impurity is given in the supplementary material. the training data are partitioned or clustered by the learned decision tree, with each leaf representing a partition or cluster. random forests are robust to the scaling of data, and so transformations of the data can be less important than in other methods. [cit] . an implementation of random forests is provided by the scikit-learn python library [cit] ."
"as errors between the predicted heatmap and the ground truth data. usually, the mean squared error (mse) loss function is used for this kind of problems. however, research shows that the use of an asymmetric weighted loss can improve the performance when the data is unbalanced [cit] . since the number of eye centers and non-center pixels are imbalanced, we compute a weighted mse in our experiment. given an image i with a size of, we can get ground truth heatmap using the gaussian kernel, and the network predicts a heatmap . during the training procedure, the variant of mse loss function of the proposed network is thus given by"
"where p is the index of the pixel, is the ground truth heatmap which represents ground truth label of the pixel, and is the predicted heatmap which indicates estimated label predicted by the fully convolutional network with parameters . the network parameters  are updated by using rmsprop optimizer. and is the loss function. during the training stage, all parameters  are learned and updated via minimizing a loss function, which are computed"
"that is, the clusters found in c x and c y contain the entire data set. let c x 2 c x be a cluster in the modality x, and let c y 2 c y be a cluster in the modality y. all examples that belong to both c x and c y form a subgroup. in audiology, this could be a subgroup of hearing devices, or human wearers, because devices are (usually) assumed to collect data when attached to a wearer (hence the need for preprocessing to identify and remove most of the cases of ''aid left switched on and sitting in a box''). we assume a subgroup is interesting if the size of a subgroup is larger than would be expected if the clustering c x was independent of c y . this can be checked using a statistical test where we accept or reject the hypothesis with a given, where an appropriate statistical test is the 2 test. in this method, because we are performing multiple hypothesis tests, we need to correct for the possibility of false positives. a simple approach to do this is via the bonferroni correction [cit], where the confidence level required to denote significance is scaled by the number of tests performed. we can further refine the search for interesting patterns by ensuring that the estimate of the ''effect size,'' and the size of the subgroup, both exceed a certain threshold so as to eliminate effects of low relevance or remote chance of occurrence."
the performance of a model should therefore be evaluated on data that are separate to the data used to train or update the model. the estimated performance of the model based on the training data will be overconfident because the model can be adapted to fit the seen data specifically and hence may overfit the data. this is analogous to providing a student with the answers ahead of an exam so they can learn them by rote and expect their exam results to provide an unbiased indicator of the student's knowledge on the general subject.
"in this article, we present an overview of a process called knowledge discovery in databases (kdd; [cit] that describes a series of steps to be taken in the general process of converting data into knowledge. a series of sections expand on the salient points of each step. the ''preprocessing'' section briefly discusses data cleaning; then, the ''data mining techniques: examples'' section introduces common data mining techniques and shows an example method for each. these techniques include clustering, classification, and regression; within each, we give an example of how the method may be of relevance to the field of audiology. [cit], and many others, point out, data mining is not a panacea; there is potential for many spurious patterns to be returned by the tools, and so care must be taken in the interpretation of results in order to provide their proper context. the ''interpretation/evaluation'' section therefore discusses the generation of value from the data mining. any statistically significant relationships that have been revealed between data members need to be sifted by an expert in the field in order to sort the spurious from those that generate insight."
"the success of deep learning methods for various computer vision tasks in recent years motivates us to investigate it in the task of eye center localization [cit] . traditional methods have recently been reshaped by emerging deep learning techniques, which are the main driver behind an explosive rise in performance across many computer vision tasks [cit] . fully convolutional network (fcn) has been proved to be successful not only in object semantic segmentation tasks, but also in other applications such as image classification or object detection. however, deep learning has rarely been mentioned and used for eye center localization. therefore, in this paper, we introduce a novel end-to-end and pixels-topixels method for the eye center localization via fcn."
"these patterns are where the data ''clusters'' due to the similarities between the group members. there may be many, or few, clusters detected, depending on the selection criteria, such as requiring that a cluster stands sufficiently far above the noise to merit attention. when there are many clusters selected, the differences between clusters may appear small to the human observer. in our companion paper [cit], we chose an arbitrary number of clusters, usually five as a ''proof-ofconcept'' such that the patterns in the outputs of the analyses are more obvious to the reader. there may be many more, or even fewer, in real-world data sets."
"in the testing stage, performance is measured with the maximum normalized error [cit], which is the standard evaluation metric for eye center localization. it indicates the accuracy and reliability of each algorithm by calculating the maximum error from the worst estimations of both eyes. the detection error is measured as"
"although transformation can take many forms, such as arithmetic manipulation of data values, a general aim in its use could be to ensure that any one data dimension/ feature does not dominate, thereby introducing a bias to the results. arithmetic transformations are regularly used in the field of audiology: the decibel represents a logarithmic scaling of sound pressure level, while standard audiogram test frequencies are at spacings of one octave, a ratio transformation. the reason for doing so is that the spacing of the scale units are chosen to correspond approximately to a similar degree of perceptual distance, across a wide range of the scale (1,000,000 to 1 in pressure, and 1,000 to 1 in frequency). in data mining, a common transformation is to scale the data features so that, within each data type, statistically, they have a mean of zero and variance of unity, using what is known as the z-score transform."
"data mining may offer great promise at finding novel and complex relationships within data sets, but because of the size of the data sets, and the number of comparisons made during mining, many of these may be spurious. beside statistical confidence, expert interpretation and validation will always be required in order to provide context and to extract potential value from the findings. unexpected findings, if they can lead to the generation of rational hypotheses, may prompt new areas of targeted research."
"classification is the task of predicting the label or category of a new observation (from a set of labels or categories), given a training set of data containing observations (or instances) whose labels are already known."
"modalities are often highly structured, such as with the clinical audiogram. this is usually recorded at octave frequencies between 250 and 8000 hz, as well as at 3000 and 6000 hz, a total of eight values, to form a multidimensional object (commonly with high correlation of values at adjacent frequencies). a second example of a modality would be that, although the sound levels of the environments in which the aid was used form a continuous range, the logging of aid operation may be quantized into a fixed number of levels by grouping a range of levels, such as in steps of 1, 2, or 5 db. in comparison, unstructured modalities can be generated from open-ended data such as patient reports, where the dimensions of data are more flexible."
"the method proposed in this paper is inspired by both the semantic segmentation task and fcn, which regard eye center localization as a special semantic segmentation task. therefore, we design a shallow fcn network with a large kernel convolutional block to overcome the limitations of previous works for eye center localization. it is a feasible and high-efficiency solution for eye center localization, which leads to high performance outperforming many existing stateof-the-art methods."
"in this paper, we propose an accurate and robust network architecture for eye center localization via a shallow fcn with a large kernel convolutional block. the key idea is regarding the eye center localization as a special semantic segmentation problem, which leads to the transformation of heatmaps of eye center positions. in the preprocessing stage, we first use the gaussian kernel to generate heatmaps of eye center landmarks, which are then used to train the network. in the testing stage, we transform the heatmaps generated by the network to coordinates to evaluate the performance. our experimental results on testing database show that the proposed approach outperforms the state-of-the-art methods. we understand that more training dataset will potentially improve the performance."
"outline pseudocode for the procedure is given in the supplementary material in algorithm 1. a simple example is shown in figure 2 where the ''interestingness'' is that cluster c x contains only crosses, and cluster c y contains only blue data points."
"domain is a high-level modality, where the concept is broader in nature. for example, a person's lifestyle may be described in a given domain, and their hearing status may be described in another. each domain can be measured by multiple dimensions/features that may be grouped into multiple modalities."
"eye center localization: localizing the eye center is a critical requirement for eye gaze estimation and eye tracking and has attracted a huge interest in recent years. existing works for eye center localization can be roughly divided into three categories: 1) appearance-based methods, 2) modelbased methods, and 3) hybrid methods. early works tackle this problem mainly using appearance-based methods, which use priori eye knowledge about appearance information such as the color, circle structure and other geometric characteristics of the eye to localize the eye center [cit] ."
"modality is a set of related dimensions/features that describe a single object or concept. for example, a clinical audiogram is typically specified by thresholds at eight different frequencies. when the dimensions together describe a single concept, such as an audiogram, we term this a modality."
"the major contributions of this work are as follows: 1) we design a fully convolutional network (fcn) with a shallow structure and a large kernel convolutional block to accurately locate the eye center, which well balances the performance and the computational costs."
"in this paper, the proposed fcn approach shows a significant improvement for eye center localization, which is more robust and accurate on the bioid and gi4e database. it can maintain enough accuracy especially for images with visible pupils, open eyes, no strong reflection and no occlusion. we notice that several issues still need to be discussed. first, there is still a space to improve performance. the proposed fcn is a shallow and simplified version in terms of the architecture. due to limited training databases which provide annotations of eye centers and limited computational resources, it is only possible to design a shallow network rather than a deep one. therefore, adding more training data or synthetic data and using deeper networks such as hourglass networks could potentially improve the performance for eye center localization. but at the same time the training time and complexity will also increase. we thus need to find a balance between performance and efficiency. second, despite the mse loss could have a minimum value, it cannot guarantee that the improvement of mse loss could lead to the improvement of results for localizing eye centers. this is because the mse loss function is used directly for optimizing the metric for the whole heatmaps instead of the eye centers. and we need to further transform the predicted heatmap to coordinates. third, the generation and transformation of heatmaps is crucial to the proposed approach. we use the gaussian kernel to generate heatmaps during the process of preprocessing and employ the weighted average method to transform generated heatmaps to coordinates in the testing stage. though both gaussian kernel and weighted average method work well in this case, more effective methods or strategies could lead to a better performance. the proposed method cannot handle perfectly for cases such as closed eyes, occlusion from glasses or hair and affection from shadows. as shown in fig. 7, the position of eye centers can still be improved."
"regression is the task of predicting the continuous response to an input variable, given a set of training data containing observations whose continuous response is already known. this prediction of a continuous response is as opposed to classification where solely a discrete label or category is predicted. subgroup discovery is the task of finding a subset of instances in a data set for which some relationship or dependency holds. this is as opposed to classification, regression, and clustering that provide some prediction or description of the whole data set."
6) r 1 (k) represents the received preamble compensated using schmidl's algorithm if offset is within the range of 1 and is given as in (14)    2
"hence, it is required to reduce the frequency errors to a small fraction of the subcarrier spacing. these offsets are considered constant for simulation purposes as the oscillators drift with temperature, supply voltage, load and the other slowly changing environmental parameters. the variations in the cfo due to doppler effects are also considered to be slow in comparison to simulation time. in practical system scenarios, the frequency offset can be many multiples of the subcarrier spacing due to the use of consumer-grade oscillators in the receiver. therefore, a wide frequency estimation range enables greater flexibility in terms of reducing the cost of ofdm receivers to mass-market consumers. various techniques have been proposed in the literature for the frequency synchronization in ofdm."
"the incoming input binary streams are first mapped into constellation points according to any of the digital modulation schemes such as qpsk/qam. in qpsk (quadrature phase shift keying) modulation, the incoming binary bits are combined in the form of two bits and are mapped into constellation point. after mapping into constellation points, the incoming serial bits are converted into parallel bits transmitting n ofdm samples at a time. the ofdm signal is generated using n subcarriers. the total bandwidth is divided into 64 sub channels. the n constellation points are modulated using n subcarriers whose carrier frequencies are orthogonal in nature. the modulation is similar to taking inverse discrete/fast fourier transform (idft/ifft) operation. the output of n point (ifft) block is the ofdm signal. now the n ofdm signal samples are combined and then transmitted i.e., the parallel samples are now converted into serial sequence and then it is transmitted. the ofdm baseband signal at the transmitter is expressed as in (1)"
kasami sequence is generated with period k and the same sequence is repeated in the next half of the duration. the preamble structure is defined with kasami sequence of period eight and offset is estimated using (6) .
"orthogonal frequency division multiplexing (ofdm) is a digital multi-carrier modulation technique that has become an increasingly popular scheme in modern digital communications. it is the attractive technique for high speed wireless communications. it is robust against frequency-selective fading in a multipath channel eliminating the need for complex time-domain equalization. several wireless communication systems adopt ofdm as their modulation technique such as wireless local area networks (wlan), wireless fidelity wifi, mobile worldwide interoperability for microwave access (mobile wimax), 3 rd generation partnership project long term evolution (3gpp lte), digital audio broadcasting (dab), digital video broadcasting (dvb), digital video broadcasting-terrestrial transmission systems (dvb-t), digital video broadcasting-handhelds (dvb-h), digital video broadcasting-satellite services to handhelds(dvb-sh) wireless standards. especially, mobile digital broadcasting system has attracted considerable attention which provides not only tv and radio services but also data and multimedia services to mobile phone and portable devices. the disadvantages of ofdm system are peak to average power ratio (papr), carrier frequency offset (cfo) and timing offset (to)."
"the method using zc sequence as preamble for frequency offset estimator enlarges the range of estimation to 30 of subcarrier spacing for ofdm based wlan system. the accuracy of estimation has improved when compared to other methods. compared to other data aided techniques simulated in this paper like schmidl, minn, ren for cfo estimation, this method gives better accuracy in the estimation of frequency offset."
"if the cfo is less than or equal to one subcarrier spacing, the first procedure alone is needed. but if the cfo is greater than one, the two procedures are carried out separately to estimate the total carrier frequency offset. figure 2 gives a comparison of the estimation ranges of the different methods using preamble structure defined by schmidl, minn, ren and also by using kasami and zc sequence as preambles. from the simulation results, it is found that preamble based on zc sequence gives accuracy and the estimation range is much larger than that of the others. the performance of proposed cfo estimator is evaluated in awgn and different channel environment according to the hiperlan specifications using the table. 2."
"where c k is the complex modulated symbol on the k th sub-carrier generated by the dft of x u (n) and mapped to constellation points. n is the size of ifft and k is the index of samples. these preamble samples are transmitted as rf signal after a parallel to serial conversion along with the data symbols. 4) in the receiver side, the timing offset is modelled as a delay and the frequency offset as a phase distortion of the received data in the time domain, so, the n th received sample is represented as given in (11)"
"here, model a, corresponds to a typical office environment for nlos conditions and 50ns average rms delay spread. model b, corresponds to typical large open space and office environments for nlos conditions and 100ns average rms delay spread. model c, corresponds to a typical large open space environment for nlos conditions and 150 ns average rms delay spread. model d is the same as model c but for los conditions. a 10 db spike at zero delay has been added resulting in a rms delay spread of about 140ns. model e, corresponds to a typical large open space environment for nlos conditions and 250 ns average rms delay spread."
"this paper is based on preamble-aided methods that can be applied to both burst-mode and continuous ofdm applications. the organization of the paper is as follows. in section i, the ofdm system model and importance of frequency offset estimation is described. in section ii, the frequency offset estimation of previous methods is explained. the algorithm for frequency offset estimation using zc sequence is given in section iii. simulation results and discussions are presented in section iv. finally, in section v, conclusions are drawn."
the basic principle behind frequency estimation is correlation function (cf) within the preamble denoted as p(d) to obtain maximum value. this is the notification of the arrival of preamble.
"x i 's are the samples of the preamble in the time domain which satisfies the condition in (9) / 2, 0,..., / 2 1"
ofdm is very sensitive to carrier frequency offsets in the received signal due to doppler shifts or instabilities in the local oscillator (lo) and results in a loss of subcarrier orthogonality leading to inter carrier interference (ici).
"figure 1: graphs of idealized attribute types based on kano quality categories. [cit] reviewed the research on the application of kano's model over the last two decades, in which they found that 21 of the 28 studies used the original kano questionnaire to classify attributes of quality. kano's approach requires compilation of a questionnaire with a list of functional and dysfunctional questions for each attribute to observe the distribution of customer views. for example, customers are first asked how they would feel if a particular attribute were present or fulfilled (possible answers are a) satisfied, b) it should be that way, c) i am indifferent, d) i can live with it, or e) dissatisfied) and then how they would feel if that attribute were not present or unfulfilled (with the same possible answers). response to both questions determines the nature of attribute according to kano's model. existing empirical studies have found this approach too complex and difficult to implement in real-world situations [cit] ."
"the key parts of the questionnaire focused on measuring the cios' satisfaction with the application of sdm in individual development disciplines and net benefits of it projects. they were measured on a 7-point likert scale ranging from 1 (strongly disagree) to 7 (strongly agree). the net benefits were defined in accordance with delonemclean model of is success. the sdm was measured on the level of disciplines that were defined based on the well-established rational unified process [cit] ) and included requirements acquisition, system design and architecture, coding and integration, testing, and deployment. the agreement statements about the net benefits of sdm disciplines for the deployed solution were analyzed and assigned to kano quality categories."
"besides the original kano questionnaire, several other approaches for classifying quality attributes were proposed [cit], such as penalty-reward contrast analysis (prca), importance grid analysis (iga), direct classification method, and the moderated regression analysis. practically most useful are regression methods such as prca, which introduce dummy variables to model nonlinear relationships between attributes and quality [cit] . these methods are theoretically unjustified and the resulting coefficients are not easy to comprehend, especially in the realistic circumstances with a presence of noise in questionnaires [cit] . a practical advantage of the regression methods is that they directly analyze attribute-level customer satisfaction, which is much easier to collect in surveys than the list of functional and dysfunctional questions proposed by kano. in machine learning and data mining, the quality of attributes (also called features, independent variables, predictors, or input variables) is an important research question tackled in tasks such as attribute evaluation, attribute subset selection, and attribute ranking. several measures have been proposed, which mostly estimate the quality of attributes through their predictive power concerning the response variable (also called dependent variable or class variable, in our case this is the satisfaction). [cit] provide an overview of classical attribute selection approaches. recently, the research in this area is focused on specialized measures, for example for specifics of bioinformatics [cit] or big data [cit] )."
"the satisfaction with sdm application in system design and architecture discipline in terms of kano quality varies considerably between cios. this is not surprising as enterprises use different sdm approaches in performing this discipline. even when using similar sdm approaches different enterprises often focus on different categories of kano quality. while formal modeling of software architecture can be a well-established and required approach for some enterprises (i.e. a must-be quality), it might be seen as an interesting but not required approach for other enterprises (i.e. an attractive quality). consequentially, ordeval detected no specific kano quality (figs. 3 and 4) . nevertheless, relieff (fig. 3) clearly shows that cio satisfaction with the discipline importantly positively associates with the net benefits of it projects."
"the ordeval and relieff methods proved useful for our analysis as they enabled a better understanding of kano quality of individual sdm disciplines and their associations with net benefits of it projects. according to our empirical study, the application of sdm in development processes disciplines, in general, has a strong impact on net benefits of it projects with testing providing the highest net benefits. results show that enterprises should be especially cautious when altering testing and deployment i.e. must-be quality disciplines. changes to these mustbe quality disciplines can significantly disrupt the established routines and cause great dissatisfaction but are unlikely to significantly increase satisfaction. the opposite holds for requirements acquisition i.e. an attractivequality discipline where enterprises are encouraged to experiment. recent innovations in the field can significantly increase the satisfaction but are less likely to cause dissatisfaction. finally, coding and integration discipline is considered to be a one-dimensional kano quality. the management can expect that increasing its quality will result in a stable growth of benefits. the identification of these associations provides important new insights for better management of software development disciplines."
"in fig. 3 we present the overall impact of each attribute on the overall satisfaction with the it project taking into account possible attribute interdependencies (relieff). the results show that application of sdm is importantly associated with the net benefits of it projects in all disciplines (with the exception of project management). based on our survey, testing discipline has the strongest association with the net benefits of it projects, while coding and integration discipline is ranked second. interestingly, cios satisfaction with the application of sdm in project management discipline does not positively associate with net benefits of it projects."
"as the vast majority of other it deployment studies in the literature [cit] this study has also focused on the largest non-financial enterprises in a country. the starting population was the top 1000 [cit] including ibm, microsoft, novartis, goodyear, kpmg, etc. we sent the surveys to cios of studied enterprises and received 113 appropriately completed surveys. [cit] . based on personal and phone communication with the cios involved in the study we found that the relatively low 11.3% response rate was mainly caused by the lack of time needed to fill out the questionnaire. such response rate is typical for mail surveys conducted in enterprises in slovenia [cit] . the questionnaire asked participants for information about the characteristics and outcome of a recently completed important software project they had been involved in, regardless of its size and success. the key characteristics of our sample are as follows."
the relieff analysis shows that cio satisfaction with the application of sdm in coding and integration discipline importantly positively associates with the net benefits of it projects. ordeval further identifies one-dimensional kano quality through the linear-like relation between the quality of sdm application in coding and integration discipline and cio's satisfaction with the discipline. coding and integration discipline is largely defined by sdm approaches that systemize key processes importantly influencing net benefits of it projects. for instance development lag that is a consequence of a continuous evolution of is in large organizations [cit] can be considerably reduced by introducing the practice of continuous integration. such approaches lead to higher level of efficiency in coding and integration discipline [cit] ). this matches our findings regarding the positive contribution of sdm in coding and integration.
"as shown by relieff, the satisfaction of cios with the application of sdm in the deployment discipline positively associates with net benefits of it projects. these benefits probably stem from continuous deployment and may result in shorter time-to-market, continuous feedback, improved release reliability, increased customer satisfaction, and improved developer productivity [cit] . ordeval analysis of the sdm application in the deployment discipline (fig. 8) shows a must-be quality effect. we conclude that the sdm application to the deployment discipline has to be established at least at the basic level since higher levels of sdm application in the deployment discipline do not appear to be beneficial."
"one of the avenues for improving our approach is to integrate it with the quantitative kano models that have been recently developed. in order to address the subjective classification present in the kano model, several quantitative"
"our survey and analysis have certain limitations that should be considered when assessing the strength and generalization of the results. the included enterprises are not a random sample as the top thousand enterprises in slovenia were surveyed. however, this group of enterprises presents a relevant study group due to its importance for the national economy. larger datasets from multiple countries would improve the reliability of the results. additionally, the respondents already knew the project outcome when they participated in our survey. this could have biased the responses especially if the projects were highly successful or highly unsuccessful. in line with similar studies (jrgensen 2016), we tried to request mainly objective information about the project characteristics."
"the remainder of the paper is organized as follows. in section 2, we present the background and related work. we first introduce the importance of quality assessment of software development process and present the kano model in this context. next, we present the relieff and ordeval algorithms that are particularly suitable for attributes measured on an ordered scale as is the case with the data collected in this research. in section 3, we introduce our data and analytical approach supporting the kano model. the empirical evaluation is covered in section 4. in section 5, we discuss the implications of the results and we conclude the paper in section 6."
"fifty-eight percent of the projects had a budget of less than 100,000 eur, 29% between 100,000 eur and 500,000 eur and 12% over 500,000 eur while there was no response from the remaining 1%. on average 13.3 people were involved in the project of which 4.8 were external contractors. fifteen percent of the projects lasted less than 3 months, 27% lasted between 3 and 6 months, 29% lasted between 6 months and a year, 21% lasted more than a year while there was no response from the remaining 8%. twenty-eight percent of the deployed software products were custom solutions, 35% were customized local pre-packaged solutions, while the remaining 37% were customized pre-packaged solutions offered by international vendors. eleven percent of the projects achieved all anticipated net benefits of the deployed solution, 64% partially achieved anticipated net benefits of the deployed solution, while the remaining 25% significantly failed to achieve most of the anticipated net benefits."
"simple attribute evaluation measures like gini index [cit] ) take only dependence between one attribute and satisfaction into account. more advanced measures take into account conditional dependencies between a response variable and several attributes, the best known such measure being relieff (robnik-ikonja & [cit] ) . these attribute quality evaluation measures are concerned with the predictive power of attributes and, similarly to regression approaches, can detect important attributes. however, they do not take into account the specifics of ordered attributes (e.g., ordered attributes/questions in surveys) and cannot provide useful evaluation for each individual value of the attribute."
ordeval method was used to classify sdm disciplines into five kano quality categories. figs. 4-9 show visualizations of ordeval results for each discipline. when interpreting the results we focus primarily on statistically significant outcomes. summarized results of how disciplines map to different kano quality categories are provided in table 1 .
"software development enterprises are under increasing pressure to compete in the global market. for this reason, their software development processes need to facilitate development of complex software products for demanding customers. under these conditions, the software development process has to produce and support software products that fit many consumers, while at the same time achieve economic and design sustainability [cit] . this challenges software development enterprises to select and adapt their software development processes to support such software development projects [cit] ."
"the attribute evaluation with relieff ( fig. 3) shows that application of sdm in requirements acquisition discipline importantly affects the net benefits of it projects. ordeval further identifies the attractive kano quality of sdm in requirements acquisition discipline as having the highest statistically significant influence on cio satisfaction (fig. 4) . these ordeval results might be surprising since requirements acquisition is considered one of the basic building blocks of the software development [cit] with high importance to software project success [cit] . the results can be explained by the fact that recently many new requirement acquisition techniques and approaches have been developed [cit] and are probably perceived as innovations by the cios of the studied enterprises. moreover, in an environment of ever changing customer requirements and technological changes there is a need for continuous reflection to decide on the best course of action [cit] . the newly developed approaches strive to improve communication between customers and development teams and try to gain a better common understanding of the problem domain, which importantly affects cios' satisfaction and it projects net benefits."
"for each reinforcement factor, the ordeval computes confidence intervals which are plotted as box-and-whiskers plots above the obtained reinforcement factors. reinforcement values outside the confidence intervals are statistically significant at 0.05 level. missing values of survey questions are estimated from their class-conditional probabilities."
this paper presents an evaluation approach for software development processes that considers quality as a nonlinear concept. it is based on a popular kano's 2-dimensional model of quality. with kano's model [cit] we can evaluate enterprise's satisfaction with software development process. such evaluation improves software development enterprise's ability to identify software development disciplines with improvement potential and facilitates the development of improvement strategies.
"the methodology returns conditional probabilities called 'reinforcement factors'. these factors approximate the upward and downward reinforcement effect the particular attribute's value has on the satisfaction. for each value of the attribute, we obtain estimates of two conditional probabilities: the probability that the satisfaction value increases given the observed increase in the attribute's value (upward reinforcement), and the probability that the satisfaction value decreases given the observed decrease of the attribute's value (downward reinforcement). to take the context of other attributes into account, the probabilities are computed in a local context, from the most similar instances. the visualization of these factors with box-plots gives information about the role of each attribute, the importance of each value, and the threshold values. to understand the idea of the ordeval algorithm, the attribute should not be treated as a whole. rather, we shall observe the effect a single attribute's value may have."
"generally inconclusive and for a certain group of cios attractive quality fig. 4 shows the statistically significant positive influence of sdm application in requirements acquisition discipline on cio satisfaction with the discipline when attribute value increases from 3 to 4, 4 to 5 and from 6 to 7 and statistically significant negative influence when the attribute value decreases from 4 to 3 and from 7 to 6. the top right and left bars show the strongest effects. this indicates that attractive quality of sdm application in requirements acquisition discipline has a strong statistically significant influence on cio satisfaction with the discipline, however, one-dimensional quality influence is also present. fig. 7 shows the statistically significant influence of sdm application in testing discipline on cio satisfaction with the discipline when attribute value increases from 1 to 2 and from 5 to 6 or decreases from 6 to 5. the strongest effect shown in the bottom right bars indicates the must-be quality of sdm in testing, while the effect shown by the bars in the second row from the top indicates that a certain group of cios perceive sdm in testing as onedimensional quality. fig. 9 shows the statistically significant positive influence of sdm application in project management discipline on cio satisfaction with the discipline when attribute value increases from 6 to 7. ordeval results thus indicate a presence of attractive quality. since relief score does not show that the impact of this attribute is relevant we can conclude that the benefits of project management are on average inconclusive, but a certain group of cios perceives project management as an attractive quality."
"we use two attribute evaluation measures: relieff and ordeval, both implemented in r package corelearn ordeval are probabilities that an increase/decrease in the individual attribute's value will have an impact on the response variable. the intuition behind this approach is to approximate the mental decision process, taking place in each individual respondent, which forms a relationship between the attribute and the response. namely, by statistically measuring a causal effect the change of an attribute's value has on the response value, we can perform probabilistic reasoning about the importance of the attribute's values, the type of the attribute, and determine which values are thresholds for a change of behavior. for each respondent, the ordeval selects the most similar respondents and makes an inference based on the differences between them. for example, to evaluate the effect an increase of certain attribute value would have on the overall satisfaction, the algorithm computes the probability for such an effect from similar respondents with a larger value of that attribute. the overall process is repeated for a large enough number of respondents to get statistically valid results."
"without loss of generality and accuracy, we will ignore the complexity of algorithms 3.3 and 3.4, since they will be performed one time whenever the structuring of the db schema is required. henceforth, we are not concerned with the complexity of the algorithms; rather our concern is with the complexity of the data access time as well as data transmission time before and after partitioning. www.ijacsa.thesai.org assume that the db tables are stored in a column wise schema, where the data belonging to one attribute are stored sequentially. then to retrieve one record in a table, it is required to scan all attributes in the record. given that the number of attributes in a table is n; this leads us to consider the space occupied by the table as a major cost of the system. when multiplied by the overall time required executing a query, or the time required transmitting this table the complexity naturally lends itself to the space time product as the main tuning parameter."
"classifier performances were assessed by computing the balanced accuracy, sensitivity, and specificity with which the test samples were classified. sensitivity was defined as the ability to identify the transition toward hallucination state (trans), whereas specificity evaluated the ability to identify the resting-state activity (off). the balanced accuracy score was defined as the average of the sensitivity and specificity. we also implemented the receiver operating characteristic (roc) curve for each classifier, from which the area under the curve (auc) was computed."
"3) environment: in this experiment, our edge server consisted of a laptop computer with a 2.6 ghz intel core processor, window 7 operating system and 4 gb of ram. as discussed previously, the edge server handles running the network and analyses the pre-processed data. our cloud server was a desktop computer with a 3.6 ghz intel core processor, windows 7 operating system and 8 gb of ram."
"consequently, the main issues in the process of designing our framework are the detecting of accurate features for partitioning (classification and restructuring) the data without losing information, and implementing an efficient restructuring data algorithm. to illustrate our approach, we apply it on an imaginary system in a kidney disease center. we assumed that the edge cloud system is deployed in a chronic kidney disease center to monitor the patient's status and we use data from the uci repository for chronic kidney disease (ckd) [cit] . the paper is organized as follows: section 2 summarizes the edge computing, section 3 presents the related work followed by a description of the proposed framework architecture in section 4. section 5 lists the details of the experiment environment and discusses the obtained results. finally, section 6 concludes the paper."
the first component explained 2.5% of the variance. the second component explained 1.4% of the variance. the third component explained 0.09% of the variance. the fourth component explained 0.05% of the variance. the prediction of mental states based on the scores associated with each component yielded a significant decoding performance:
"the third issue is extracting the subset of features that have an inherent locality structure, in other words, grouping the features that form temporal locality in the same subset. the features that are requested in the same query, construct temporal locality. the locality structure is discussed in detail in the next subsection. in this dissertation, we proposed using the k-mean algorithm [cit] which is modified according to our approach, combined with the features selection wrapper methods which is performed in a new way to cope with our approach as shown in fig. 4 [cit] . this combination provides a machine learning subset system that is used to find a subset of features. a) machine learning subset system: in this dissertation a machine learning subset system was used in establishing a subset of features; it groups features into subsets according to their frequency."
"2) partitioning data scheme: this part is responsible for partitioning the dataset based on features that are selected in the previous part. partitioning aims to classify the data in order to remove unrelated data to reduce the size of data. as well as grouping a set of attributes in a subset, where the subset will be used to create a new table of data with the attributes given in the subset. in essence, the process of partitioning leads to the auto reconstructing of the original schema, which was originally used to represent the data. this part includes the following steps. a) classifying the data as controlling data and knowledge extraction data using feature selection in part 1. machine learning classification algorithm (classifier) with feature selection is used to classify the data instance, this classifier learning in the cloud layer is based on features that were selected from user requirements. then it is used to classify data in the edge layer. ii. the number of attributes ai that is shared between the query qi and qi, is given by the similarity s (q)."
phase 1: determine the initial centroids of the subset (group) by taking high and low frequency (fnai). phase 2: calculate new mean for each cluster; until convergence criteria is met.
"the wrapper is a feature selection method, that evaluates feature subsets by the quality of the performance on a modeling algorithm, which is taken as a black box evaluation [cit] . in our approach, the wrapper will evaluate subsets based on the performance of executing queries on the new tables that are built on the feature subset. the evaluation is repeated for each subset and the subset generation is dependent on the k-means algorithm as shown in fig. 4 ."
"in this paper, efficient data storage and retrieval frameworks are proposed based on both the edge computing and cloud computing techniques. the main challenges in terms of data partitioning and requirements analysis are summarized, and appropriate solutions are also provided. specifically, the data partitioning (classification and restructuring ) framework is provided to support low latency and save network bandwidth based on mapreduce algorithms, wrapper feature selection method and a machine learning proposed subsystem, in addition, a new algorithm for restructuring the data was proposed."
"the edge computing layer lies between cloud and end edge network devices; however, there is no commonly agreedupon framework to capture the functionality of this computing paradigm. recently, ibm and some researchers have proposed a three-layer paradigm as the high-level architectures, this is illustrated in fig. 2 [cit] ."
"the present findings indicated that the two classification algorithms were able to significantly detect the prehallucination patterns in brain activity at rest. crucially, spatial regularization (tv) combined with the elastic net penalty significantly improved the prediction performances (increased auc) and provided more balanced specificity and sensitivity."
"however, when predicting the mental state of subjects based on the spca-tv scores, the decoding accuracy was significant. naturally, the performance was decreased compared to the performance obtained in the supervised part of this article, which was expected as we were losing some information from the compression of the 67,655"
"because the hallucinations were frequently multimodal in the sample of patients recruited for this study, we expected more disparities in the functional patterns associated with their complex hallucinations and the transition toward this state compared with pure auditory experiences. in this context, the significant intersubject decoding performances obtained appeared satisfactory and are promising for future fmribased therapy for drug-resistant hallucinations."
"in the previous two steps, the clusters and data set labels were found for a particular pre-chosen k. in this step, we discuss how wrapper feature selection is used in a new method to choose the number of clusters k."
"compared with the procedure that required the incorporation of information from post-fmri interviews with patients into the labeling process, the proposed machine learning-based method is fully automatic, relying exclusively on the imaging data. moreover, the learned model can be applied in real-time during data acquisition."
"rather than detecting hallucinatory events per se, we aim to help patients become aware of the imminence of this experience based on online detection of fmri signal changes in key networks involved in the ignition of hallucinations. thus, in this study, we specifically focused on the period preceding the occurrence of a hallucination, that is, the few seconds corresponding to the brain's transition from a resting state to a full hallucinatory state. interestingly, previous fmri studies have noted the existence of specific fmri changes prior to hallucinations [cit] ."
"the traditional k-means algorithm consists of two stages. the first stage adjusts k based on the number of groups [cit] . the second stage detects initial centroid randomly from the dataset for each group. in this dissertation, we used k-means with modifying its stages (m_k-means), where the initial centroid is determined based on a features frequency (i.e., taking the higher and lower frequency) and the number of groups k determines based on used a wrapper feature selection method. however, we illustrated the steps of this system as follows:"
"bandwidth is the number of bits per second that a link can send or receive, including all flows. data rate (or data transfer rate) is the volume of data transferred through a connection within one second. the data rate cannot exceed the bandwidth of the connection; data rate is closer to bandwidth. thus, the reducing the required data rate reduced traffic overhead on the network; in other words, a reduced data rate leads to a reduce bandwidth requirement."
"the goal was to characterize the variability within the prehallucination scans. pca can extract the significant mode of variation from highdimensional data. however, its interpretability remains limited. indeed, the components produced by pca are often noisy and exhibit no visually meaningful patterns. nonetheless, our ultimate goal was to understand the variability in the form of intelligible patterns. in this context, we used spca-tv (sparse principal component analysis-total variation), which is an extension of regular pca with l 1, l 2, and tv penalties on the pca loadings, promoting the formation of structured sparse components that are relevant in a neuroscientific scope [cit] . we hypothesized that the principal components extracted with spca-tv could uncover major trends of variability within the prehallucination samples. thus, the principal components might reveal the existence of subgroups of hallucinations, notably according to the sensory modality involved (e.g., vision, audition, etc.). from the 376 samples, we retained the 210 elements corresponding to the prehallucinations samples. we applied spca-tv to these 210 samples and interpreted the resulting principal components."
"taken together, these results confirm that adding a penalty to account for the spatial structure of the brain seems relevant in fmri captures, given that it significantly improves the classifier performance and results in clinically interpretable weight maps."
"in this section, we compare the execution time of the 11 queries in table ii with different size datasets, and examine the outcome for two additional queries in table viii on both the original dataset and our new schema dataset(partitioning dataset) (see table vii ). www.ijacsa.thesai.org table viii shows the execution time of the 11 queries in both our schema dataset and the original dataset. the results show significant differences in the execution times between the two datasets. obviously, our schema dataset requires a shorter execution time than the original dataset because it has a smaller size as well as greater spatial locality of reference. in other words, the network with our framework has low latency and high performance. these differences can be shown by analyzing the bar chart in fig. 8 . furthermore the average execution time for these 11 queries was decreased by 79.841%."
"the first pc mainly included the weights in the precuneus cortex and the posterior cingulate cortex. the posterior cingulate cortex, which is part of the dmn, is associated with auditory hallucinations [cit] . we believe that this component may have captured the visual pathways typically involved in the occurrence of visual hallucinations."
"consequently, as a second classifier, we used logistic regression with 3 types of regularization penalties: ' 1, ' 2, and tv [cit], which was denoted tv-elastic-net (tv-enet). the ' 1 and ' 2 penalties served the purpose of addressing overfitting induced from the mri data's high-intrinsic dimensionality. the tv penalty also regularized the solution, but its main purpose was to take advantage of the spatial 3d structure. together, these penalties enabled the generation of a coherent, parsimonious, and interpretable weight map. moreover, these penalties provided a segmentation of the predictive weight map into spatially contiguous parcels with constant values, which is a highly desirable characteristic in the scope of predictive signature discovery."
"first, the way we access data can help to restructure the data in a way that improves its locality characteristics such that the transaction time is reduced. in this work, we take note of the fact that data which exhibits good locality structure can be accessed in a much better way than data with the poor locality. by locality, we refer to a particular access pattern that can improve the efficiency of local memory (main memory or cache) in any system [cit] . second, the space-time product of a task or a set of tasks is inversely proportional to the throughput of the system [cit] ."
the m -means algorithm as shown in algorithm 2 in uses iterative refinement to produce a final result. the algorithm inputs are the number of groups  and the data set. the data set is a collection of features (data attributes) and frequency for each feature. the algorithm starts with two groups; the first group takes the high value of a feature frequency as a centroid while the second group takes the lower value of a feature frequency as a centroid. the initial value of group centroid(fai) was defined to avoid the oscillation with iterative refinement to produce a final result.
"among the current machine-learning approaches available for fmri analysis, multivoxel pattern analysis (mvpa)-a supervised classification method-is gaining recognition for its potential to accurately discriminate between complex cognitive states [cit] . mvpa seeks to identify significantly reproducible spatial activity patterns differentiated according to mental states. extending these methods to the prediction of the phenomena of transition toward hallucinations should provide better insight into the mechanisms of these subjective experiences. thus, leveraging real-time pattern decoding capabilities and applying them in the case of hallucinations could lay the foundation for potential solutions for affected individuals."
"in this study, we compared two different linear classifiers of binary classification. first, we used a regular linear svm based on ' 2 (ridge) penalty on the coefficients vectors. the role of the ridge penalty was to shrink the coefficients toward zero to control for the variance of the fitted coefficients. however, the svm classifier cannot select significant variables and, rather, tends to produce dense patterns of predictors that are difficult to interpret without arbitrary thresholding. in the context of predictive signature discovery, it is crucial to understand the brain activation patterns that underpin the prediction. we, therefore, sought an approach that selects a reduced number of predictive regions. feature selection methods, such as recursive feature elimination (rfe) [cit], have been used to select a reduced set of predictors [cit] . however, as they are prone to local minima, these ad hoc heuristics tend to be replaced by sparse models based on convex minimization problems that simultaneously optimize the prediction performances while performing the feature selection."
"1) requirements analysis: the requirements analysis issue, as mentioned above, features are selected from user requirements, these requirements can be obtained from a given stream of queries generated over a time period for existing systems or from queries generated by network requirements for a new system. it is the responsibility of the system analysts to extract and detect the features and relationships among them for many large problems."
"2) data transmission complexity: while, in term of data transmission time, for transmitted a where a n is the number of attributes referenced in tk and t is the time required to transmit each attribute."
"however, when the requirements queries' size grows too big, the process of analyzing the requirements queries for detecting the features and their frequency is excessively prohibitive. hence, mapreduce [cit] technique was used to overcome this issue."
"to address this important question, an increasing number of studies have focused on so-called intrinsic connectivity networks (icn) and their potential role in the onset of hallucinations [cit] . icns typically reveal interactions among brain regions when the subject is not engaged in any particular task."
"the objective of this part is to group attributes, which are likely to be referenced together, in a partition or table. the more frequently the attributes are referenced together, the more likely for them to belong to one locality structure. for example, in a monitaring system in a factory, the monitoring devices share the same identifier, also in medical systems the diseases may be chronic or not chronic."
"query stream analysis is used to find the frequency of a given query execution and the frequency of attributes requested within the same query. furthermore, query analysis is used to detect the adjacency of attributes, where adjacency is defined as the attributes referenced within the same query. attribute ai is said to be adjacent to aj: aj (ai,aj) if ai and aj are requested within the same query q k . by default, adjacent attributes form a temporal locality, since they are referenced within the same query and their access to the data storage falls within the same time period. note that adjacent attributes ai and aj may or may not be stored within the same virtual page or in close space proximity."
"such developments may have crucial impacts on the implementation of innovative fmri-based therapy for drug-resistant hallucinations, such as fmri-based neurofeedback [cit] . during fmri-based neurofeedback, brain activity is measured and fed back in real time to the subject to help her/him progressively achieve voluntary control over her/his own neural activity. precisely defining strong a priori strategies for choosing the appropriate target brain area/network(s) for fmri-based protocols appears critical. interestingly, considering the rapid technical developments of fmri techniques and the availability of high-performance computing, the pattern classification approach now appears to be one of the potential strategies for fmri-based neurofeedback sessions."
"finally, the fourth pc included two clusters of opposing signs. in the right hemisphere, there was a large activation cluster that involved the temporo-parietal junction and a deactivation cluster that involved the precuneus cortex and the posterior cingulate gyrus. interestingly, this pc revealed activation of the brain regions involved in auditory hallucination-related processes and in self-other distinction, such as the right temporo-parietal junction [cit], together with a deactivation of key nodes of the dmn, including the posterior cingulate cortex, medial prefrontal cortex, medial temporal cortex, and lateral parietal cortex [cit] . our results appeared fully compatible with recent fmri-capture findings demonstrating that aberrant activations of speech-related areas concomitant with hallucinatory experiences follow complex interactions between icns, such as the dmn and the cen [cit] . a disengagement of the dmn during goal-directed behaviors has been seminally evidenced in the resting-state literature [cit], and similar mechanisms might be involved in hallucinatory occurrences [cit] . such fluctuations in the icns are, thus, thought to be highly involved in the transition from a resting state to an active hallucinatory state."
"2) dataset: in this research, we use data from the uci repository. the data collected from the apollo hospital, india by b. jerlin rubini. the number of instances in the dataset are 400 instances with 25 attributes (including attribute classes), where 250 instances include those who have chronic kidney disease (ckd) and the remaining 150 who did not have chronic kidney disease (notckd) as in table i ."
"some of the techniques that are commonly used to find k are: cross-validation, information criteria, the informationtheoretic jump method, the silhouette method, and the gmeans algorithm. in this dissertation we proposed a new method of wrapper feature selection to find the value of k."
"the study was approved by the local ethical committee (cpp nord-ouest france iv), and written informed consent was obtained for each participant enrolled in the study."
"the population was composed of 37 patients with schizophrenia (dsm-iv-tr criteria, average age 5 35.23 years, 10 females/27 males) who were suffering from very frequent multimodal hallucinations (i.e., more than 10 episodes/h). participants were recruited through the fr2sm network (f ed eration r egionale de recherche en sant e mentale), which groups all the private/public institutions for mental health in the hauts-de-france region (62% of the participants were hospitalized at the time recruited, 38% received outpatient care). this sample presents a partial overlap with previous works from our team [cit] . the clinical characteristics of the recruited subjects are summarized in table 1 . fmri was acquired at rest. participants were asked to lie in the scanner in a state of wakeful rest with their eyes closed. the subjects experienced an average of 5.6 hallucinatory episodes per scan. [cit] and were assigned to one of the following four categories: transition toward hallucinations (trans), ongoing hallucinations (on), no hallucinations (off), and end of hallucinations (end). this labeling task is a nonstraightforward two-steps strategy; the first step is a data-driven analysis of the fmri signal using an ica in the spatial domain. the second step involves the selection of the ica components associated with possible sensory experiences that occurred while scanning. this pipeline is said to be semiautomatic as it combined the following: (a) [cit] and (b) a manual and time-consuming part, with the use of an immediate post-fmri interview conducted with the patient, in which the sensory modalities, number of episodes, and phenomenological features of the experiences were specified."
"however, to fit a sigmoid function to a set of points, two parameters need to be estimated. given the fact we only had a limited set of 8 consecutive prehallucination epi volumes, fitting a sigmoid would have meant leaving only 6 degrees of freedom. given the arguments above and our wish to reach the highest possible level of robustness, we, thus, chose to use a ramp model in these conditions. figure 1a represents the evolution of the signal intensity in one single voxel over the 8 consecutive volumes of a prehallucination period of a subject. in this specific voxel, the signal presents a ramplike increase during the prehallucination period."
"the classifier was able to distinguish the \"trans\" samples from \"off\" samples, with an auc of 65%, a recall mean of 65%, a sensitivity of 68%, and a specificity of 64%."
auditory 32 visual 5 tactile 7 olfactory 2 note. abbreviations: ahrs 5 auditory hallucination rating scale; cgi 5 clinical global impressions scale; eqoz 5 equivalent olanzapine; panss 5 positive and negative syndrome scale.
"classification algorithms may ideally benefit from modalityspecific training on more restrictive datasets of patients hallucinating in just one sensory modality. however, even if this could be easily performed for voice-hearing, this appears quite challenging for other modalities."
"it is important to mention that user requirements can be past referenced behavior for existing systems; the past referenced behavior is obtained from a given stream of queries generated over a time period. while the requirement for building a new system, are obtained from the users' requirements and network analysis, thus a large sequence of queries can be generated by these requirements, which represents the system referencing behavior."
where () is the difference between the feature's frequency and cluster's centroid. let the number of features assignments for each cluster centroid is s i .
"prior to training classifiers, the first step involved computing samples from the fmri signal. the intention was to convert the fmri signal into vectors of features reflecting the pattern of activity across voxels at a point in time. we opted against creating the samples directly from the fmri signal. instead, we created the samples by estimating the activity within each voxel using a linear model. the design of such a model was a crucial part of the learning process. we used a general linear model average number of hallucination episodes per patient 5.6"
"here, we applied both supervised and unsupervised machinelearning methods to an fmri dataset collected during hallucinatory episodes. the goal of this article was twofold: (i) to predict the activation patterns preceding hallucinations using a supervised analysis and (ii) to uncover the variability in these activation patterns during the emergence of hallucinations using unsupervised analysis. the goals of these two analyses appear completely complementary in the context of future fmri-based clinical and therapeutic applications."
"the objective of the proposed method is to maximize the throughput of the system by minimizing the space-time product for the data access (i.e., reduce latency), and data transmission (i.e., saving bandwidth). therefore, the throughput is increased with decreasing space (s)."
"learning with hundreds of samples (376) using high-dimensional data (7 3 10 4 voxels) was associated with a high risk of overfitting in the training subjects, leading to poor performances of the independent subjects. such issues of replicability can be addressed using state-ofthe-art regularized learning algorithms."
the edge server connects to the cloud server through the internet and pre-processes the raw data. we used matlab  [cit] b extreme learning machine (elm) libraries for the classification methods and a k-means function for the clustering method.
"mapreduce a programming model [cit] was inspired from the functional programming model introduced by google, which uses the divide and conquer technique to process large amounts of data. it works in three functions [cit] : the map function, the shuffle function and the reduce function. the mapper stage splits the queries into features by whitespaces, and the output of the mapper is the pair of features with their frequencies known as (key, value) pair, where key is the feature and value is a count of features. then, in the intermediate phase, shuffle sorts the (key, value) pair according to the key and sends it to the reducer. pairs that have the same key go to the same reducer."
"another solution to obtain a limited number of predictors is the use of ' 1 -regularized classifiers (lasso) that produces sparse patterns of predictors by enforcing many voxels to have zero-weights. the combination of ridge and lasso penalties in elasticnet [cit] promotes sparse models while still maintaining the regularization properties of the ' 2 penalty. however, despite the fact that lasso or elasticnet classifiers have often been advocated as leading to more interpretable models, they generally lead to scattered and unstable weight patterns [cit] ."
"2) data classification: the second issue is the classification of accumulated data into control data and knowledge extraction data based on feature extraction from user requirements. thus, we used extreme learning classifier with feature selection to classify our data."
this part is responsible for detecting the locality structure of control data and knowledge extraction data; it relies heavily on user requirements or query stream analysis.
"based on the requirements noted above, as well as from network requirements, we selected the features that classified the data into two groups: ckd, and not ckd. the elm, using a wrapper method for feature selection, was used to classify this data [cit] ."
"the infrastructure of the proposed framework consists of five main components as shown in fig. 1 : cloud-computing layer, edge computing layer, edge device layer, cloud computing queries, and edge device queries."
"the main issues in cc that are faced by edge computing are storage, bandwidth, and real-time data analytics; this work presents a new method for improving edge cloud network's performance through data processing in the edge layer before sending it to the cloud. this is performed by processing the data schema on the edge computing level. basically, the proposed technique of data processing is data partitioning, this procedure involves classifying the data and restructuring the data schema based on feature selection techniques. the objective of data partitioning is to partition the table into smaller tables in a manner, to allow the queries which access feature selection is a data preprocessing strategy based on dimension reduction, it directly selects a subset of relevant features, by removing irrelevant, redundant and noisy features from data; hence, preventing sending unnecessary data to the cloud to reduce the utilization of resources in the cloud (i.e. storage, bandwidth) as well as providing lower latency with data retrieval by only choosing relevant features [cit], feature selection techniques reduce storage and computational costs while avoiding significant loss of information [cit] . feature selection algorithms can be divided into wrapper, filter, and embedded methods. [cit] . wrapper methods generate models with subsets of features and gauge their model performances."
we believe that such fmri-nf based on the tv-enet classifier could reduce the associated distress based on an improvement in the feelings of control and self-efficacy.
"to analyze the brain regions that drive the prediction, we refitted the model on all samples of the dataset and extracted the associated discriminative weight map. this weight map revealed the spatial patterns that best discriminate the two cognitive states (trans and off). the weights revealed the relative contribution of each voxel to the decision function. positive weights indicated a positive contribution toward predicting the trans state, whereas negative weights signaled a positive contribution toward predicting the off state."
"a) data transmission time and storage space: as mentioned above, from user requirements and network requirements, the pre-processing data were separated into two classes: ckd, and not ckd. only the ckd class is relevant."
"the importance of this work is reflected from the main contribution of this paper, which is to develop a data processing framework based on both edge computing and cloud computing, by integrating the functions of data preprocessing that involves classifying, restructuring, storage, and retrieval. therefore, this work focuses on the structure of the stored data as well as the transmitted data."
"the studied cohort contained patients who were suffering from complex multimodal hallucinations. thus, the hallucinations captured during acquisition could have been very heterogeneous not only across subjects but also across occurrences. when evaluating each classifier's performance on the nonauditory hallucinations only, we obtained degraded prediction scores as opposed to the ones obtained with the patients experiencing auditory hallucinations, among other modalities. this finding is to be expected as the learning of the model is conducted on 37 subjects, of whom 32 exhibited auditory experiences. therefore, our predictive model seemed to be more specific to the prediction of auditory hallucinations than any other modalities."
"interestingly, these regions, especially the speech-related brain regions, were previously shown to be involved in hallucinations (uri [cit] ). [cit] . second, the right cluster identified in our study also emphasized the role of the right-sided homologues of the classical speech-related areas (i.e., the right inferior frontal gyrus, right superior temporal, and supramarginal gyrus) in auditory hallucinations, as previously described in the literature. it has been hypothesized that activity in these regions, especially the insula and the right homologue of broca's area, is associated with the occurrence of auditory hallucinations [cit], whereas language production in a natural context predominantly activates left-lateralized frontal and temporal language areas."
"in this paper, we modify the mapreduce algorithm to support our work to find features from user requirements by resetting all values for each key to one as shown in algorithm 1, we reset list (count) number for each feature (attribute) to one, to avoid the duplication of features in one query. the output of this stage is a list that combines the feature with its total frequency where is the location of the attribute."
"in this context, the feasibility of fmri-based neurofeedback relies on robust and reliable classifying performances and on the ability to detect hallucinations sufficiently early to allow the patients the necessary time to modulate their own cerebral activity [cit] ."
"the predictive maps obtained with the svm method were dense and difficult to interpret without arbitrary thresholding. even though the prediction performance was relatively good, a physician will never draw a conclusion from such a black-box model in a clinical setting as presented in figure 2a . understanding the brain activation patterns that drive the prediction is crucial. in addition, the predictive map obtained with tv-enet was considerably more interpretable given that it provided a smooth map composed of two clearly identifiable regions."
"the preprocessing data scheme was partitioned by some processes as shown in the flowchart in fig. 3 which is assigned by a dotted line. these processes and their design issues can be described in the following two parts. first, requirement analysis (feature selection) and second, partitioning data schema."
"with virtually unlimited computing power and storage resources, clouds are conceived to be the perfect platform for large-scale data analytics, while storage efficiencies also provide easy management of different applications. however, with cloud-based applications, most data must be sent to the data centers in the cloud [cit] . during the expansion of these applications, the volume of data increases and a large amount of data from these applications (e.g., iot devices) are moved to the cloud causing network bottlenecks due to bandwidth constraints. as time-sensitive and location-aware applications are developed (e.g., patient monitoring, real-time applications, transportation systems), the remote cloud will fail to fulfill the low-latency requirements of these applications; the round trip delay is too great [cit] ."
"the main functionality of an edge computing layer, when integrated with cloud network, is to improve performance in terms of latency, and network traffic load through processing data. hence, data processing is the main issue in edge computing in cloud computing real-time application networks such as iot, cps, and m2m [cit] ."
"in this step, each data point is assigned to its nearest centroid, based on its frequency fi. more formally, if is the centroid in cluster, then each data point x is assigned to a cluster based on the following:"
"we conclude that changes in dataset size do not adversely affect the results, meaning our proposed framework can achieve its objectives in different networks with different dataset sizes. fig. 10 compares the execution times of the two additional queries in table x against each of the two sizes of dataset. again, the proposed framework has a consistently shorter execution time compared to the original system."
"1) data retrieval complexity: in term of data access time and reduce latency, for a given query (q i ), the space consumed during the query execution equals the space of the tables referenced by q i . for example, assume that q i has the following structure:"
the components of the proposed framework flowchart are discussed; data partitioning (knowledge extraction and control data) and requirements generation are presented from a wider view. the functionalities of each point of the proposed framework are displayed in detail.
"recently many applications seek to improve society by sharing distributed devices and processing their data, such as the internet of things (iot) [cit], cyber-physical systems (cps) [cit], machine to machine (m2m) technologies [cit], industrial internet [cit], and smart cities [cit] ."
"the mapreduce algorithm is used to identify suitable features. we first taught the classifier in the cloud server, and then applied steps one and two in the previous section on our data to define the features that are used in restructuring the data. we also created the database structures necessary for the new tables that are created."
"the third pc revealed a cluster in the frontal gyrus and the anterior insula. these regions are important for speech production, encompassing the well-known broca's area [cit] and are involved in auditory hallucinations [cit] ."
"performance was evaluated through a double cross-validation pipeline. the double cross-validation process consists of two nested cross-validation loops. in the outer (external) loop of the double cross-validation, we employed a leave-onesubject-out pipeline where all subjects except one were referred to as the training data, and the remaining subject was used as test data. the test sets were exclusively used for model assessment, whereas the training sets were used in the inner fivefold cross-validation loop for model fitting and model selection."
"here, we wanted to automate the detection of specific functional patterns preceding hallucination occurrences in participants scanned at rest. first, using supervised analyses, we found evidence of prediction scores with a reliable level of significance. our prediction of the emergence of hallucinations appeared to be accurate and yielded highly interpretable associated weight maps. second, using unsupervised analysis, we characterized the variability of the prehallucinations patterns across both occurrences and subjects in the form of intelligible components."
"the total amount of explained variance was surprisingly low. indeed, the activation maps of the resting-state fmri data preceding hallucinations were very noisy, and only a minor part of its variability could be captured."
in the following we describe the design issues and constraints on the implementation of the framework and present the data workflow also the network throughput calculation.
"the components extracted with the spca-tv method were of great interest from a clinical point of view (figure 3 ). they revealed structured, interpretable patterns of variability within the different prehallucinations periods in our sample. details regarding the clusters present in each principal component are provided in table 5 ."
"in fig. 7 (a) the plotted bar of stored value indicates the amount of stored data is 92 kb for the network for the original dataset and only 6.95 kb for our partitioned dataset. this represents a 92.4% reduction in storage space required. similarly, in fig. 7(b), the plotted bar of stored value indicates the amount of transmission time is 0.007376 s for the original dataset and only 0.000556 s our partitioned dataset. this represents a 92.5% reduction in time required for transmission."
"based on the users' requirements, we generated random queries to apply our approach. specifically, from the users' requirements in the edge layer, 120 queries are generated randomly. in table ii, the 10 edge layer generation queries that had the highest frequency are listed. from the expert and user requirements in the cloud layer, we can extract and select relevant feature subsets. these subsets, shown in table iii, are used to restructure the data passed to the cloud layer."
"here, we demonstrated that supervised classification methods can accurately predict the imminence of a hallucinatory episode. thus, leveraging real-time pattern decoding capabilities and applying them in the case of hallucinations could lay the foundation for alternative solutions for affected patients in the near future, such as fmri-based neurofeedback."
"in this study, we chose to train a classifier to specifically detect periods preceding the occurrence of hallucinations (i.e., \"trans\" periods). as mentioned earlier, several studies have demonstrated that this period is potentially associated with specific brain activations. [cit] demonstrated reduced activity in the left parahippocampal gyrus, the left superior temporal gyrus (stg), the medial frontal gyrus, and the right inferior frontal gyrus (ifg) prior to auditory hallucinations."
"an interesting observation is the execution time of the q5 which is low in both datasets because the temporal locality of the query schema is virtually identical to the spatial locality of both types of data schema. table ix shows the execution times produced using a relatively large dataset. by comparing table ix with table viii, we observe that the proposed framework consistently has the shorter execution time despite the change in data size. fig. 9 illustrates graphically that the difference between execution times persists in spite of the change in the dataset size."
"all analyses were performed in python using the scikit-learn toolbox [cit] and the pylearn-parsimony package (https:// github.com/neurospin/pylearn-parsimony). given the slow, partially manual and interview-intensive nature of the cognitive state labeling pipeline [cit], we constructed an algorithm in parallel to detect a transition-to-hallucination state in a real-time, automated fashion exclusively relying on the imaging data. we focused the analysis on the transition toward a hallucination state (trans) with the intention of distinguishing it from the resting-state activity (off)."
"indeed, traditional svm naturally tends to allocate the \"off\" response, which subsequently leads to a good specificity but to a reduced detection rate (sensitivity) of patterns preceding the occurrence of hallucinations."
"images to the functional images and spatial normalization to the montreal neurological institute (mni) space using dartel based on the segmented t1 scans. we did not perform any spatial smoothing step in the preprocessing pipeline. the mni brain mask was used to restrict voxels considered in the subsequent steps to 67,665 voxels."
"however, there are great challenges to efficiently process, store, and manage the data collected by these applications. since cloud computing (cc) has virtually an unlimited capacity in terms of storage and processing power; hence, cloud computing provides an opportunity to properly approach these tasks by integrating these networks and cloud computing [cit] ."
"(glm) to estimate the activity within each voxel. from each set of consecutive images within a prehallucination state (\"trans\" periods) or \"off\" state, we created one sample. on average, each \"trans\" or \"off\" state lasted for 8 consecutive epi volumes, which appeared sufficient to estimate activity. based on the glm, we regressed the fmri signal time course on a linear ramp function for each set of consecutive volumes."
"modify k-mean algorithm: the k means algorithm is used to establish a subset of features, it groups features into subsets according to their frequency [cit] . traditional k means www.ijacsa.thesai.org algorithm consists of two stages. the first stage adjusts k based on the number of groups. the second stage detects initial centroid randomly from the dataset for each group. in this paper, we used k-means with modifying the second stage (m_k-means), where the initial centroid is determined based on feature's frequency (i.e. taking the higher and lower frequency) as shown in algorithm 2."
"frequently reported networks include the default mode network (dmn), the control executive network (cen), the salience network (sal), and the sensorimotor network (smn) [cit] . numerous studies have asserted that fluctuations in those icns are associated with the onset of hallucination periods. for instance, the emergence of hallucinations correlates with a disengagement of the dmn [cit] . more recently, stochastic effective connectivity analyses revealed complex interactions among hallucination-related networks, dmn, sal, and cen, during the ignition, active phase, and extinction of hallucinatory experiences [cit] . despite significant progress in the field, \"capturing\" the neural correlates of subjective mental events (such as hallucinations) remains a time-consuming task with multiple postprocessing steps and analyses."
"features into 4 scores. however, the fact that we could still significantly distinguish the prehallucination samples from the resting-state samples using those 4 component scores revealed that they made sense and were specifically related to hallucinations. consequently, although the explained variance was low due to the resting-state nature of the data, the components were relevant and captured the cognitive processes involved in the onset of hallucinations."
"it has been shown that the space-time product of a task or a set of tasks is inversely proportional to the throughput of the system [cit] . in other words, if we want to maximize the throughput (x), which is defined as the number of tasks performed within a time period(t), then we have to minimize the space-time cost (y), i.e., the total space (s) consumed by the tasks within the same time period (t). this implies the following relationship."
"before implementing our framework, there are three important design issues that should be addressed, the requirements analysis to selected features, data classification as control data and knowledge extraction, and the extraction of the subset of features to restructure data schema. these issues are discussed for our proposed framework as follows:"
"when using the regular svm classifier, the relevance of the obtained discriminative weight maps was limited (figure 2a) . the whole brain seemed to contribute to the prediction. it is clinically challenging to interpret the weight map. the tv-enet classifier yields a more coherent weight map with two defined stable predictive clusters (figure 2b ). the details of these two clusters are described in table 4 ."
"extensively covered in a wide range of studies in patients suffering from auditory hallucinations (allen, lari, [cit] . beyond location, the functional dynamics of the neural networks involved in auditory hallucinations have also been studied."
"therefore, we proposed to use the benefit of the known structure of brain fmri images to force the solution to adhere to biological priors, producing more plausible, interpretable solutions. indeed, mri data are naturally encoded on a three-dimensional grid. some voxels are neighbors, whereas others are not. the goal is to obtain a predictive pattern that is both sparse (i.e., limited number of non-null weight), but also structured (i.e., organized into clusters). therefore, such structured sparsity can be obtained by combining ' 1, ' 2, and total variation (tv) penalties. such a combination of penalties will enforce the spatial smoothness of the solution while segmenting predictive regions from the background."
"in an attempt to overcome the data processing issue, we construct data processing framework over the edge-cloud network by integrating the function of data partitioning, restructuring, storing and retrieving, to enhance the performance of edge-cloud network in terms of lower latency level and network traffic. the architecture of the proposed framework consists of three main layers as presented in the flowchart shown in fig. 3, these layers are edge device layer, edge computing layer, cloud layer."
"the storage of data is critical to the process of retrieving data in real-time. by extension, it necessarily has a material impact on network latency. the result in table ix shows that best throughput is achieved with our approach when the data storage is considered and takes into account the fact that data which exhibits good locality structure can be accessed more easily than data with poor locality."
"thus, the amount of the processing data was reduced. this conclusion is supported by fig. 6 which shows both the original data and relevant classification data plots. the classification data clearly required a lower amount of storage and analysis. fig. 7(a) compares the amount of data transmitted over the network from the edge layer to the cloud layer for storage. the two bars represent the two datasets: the red bar for original dataset before any partitioning, and the blue bar for the dataset after partitioning (classification and restructuring, as shown in table vi ). clearly the volume of data transmitted over the network for the original dataset is extremely high relative to the volume transmitted over the network for the dataset after partitioning. fig. 7(b) portrays the corresponding transmission times of the two datasets. this result confirms that a network with our framework can reduce network resource requirements (i.e., bandwidth and storage space) and, as a direct consequence, reduce network traffic because of the increased rate of data transmission."
"one of the major limits of such fmri-based therapies remains the accessibility and cost of the equipment. it appears fundamental to develop less complex devices as potential second-line treatments for hallucinations, such as near-infrared spectroscopy (nirs). from this technological transfer perspective, the discriminative maps obtained using the tv-enet classifier also appear advantageous, given that the identified clusters are cortical regions with activity that are easily measured with nirs."
"in our proposed, features are detected using a modify mapreduce algorithm (m-mapreduce) and proposed a novel machine learning subsystem that are applied on user requirements (also called functional specifications in software engineering) [cit] . these requirements reflect expectations for a new or modified product, requirements should be quantifiable, relevant and detailed."
"this section describes the use of the throughput calculation functions in section iv to theoretically evaluate our framework in the proposed system. this calculation involves two elements, provided by equations 5 and 7: the first element is the measurement of data access time, performed by calculating the space-time for demand query. the second element is to calculate the data transmission volume, performed by computing the space-time product for data moving through the network."
"the second pc was composed of one activation cluster in the paracingulate gyrus and the anterior cingulate gyrus and two symmetric bilateral activation clusters in the temporal cortex. this fronto-temporal [cit] . this second pc yielded regions classically involved in inhibition (paracingulate gyrus and anterior cingulate gyrus) [cit] . the severity of auditory hallucinations has been found to be inversely related to the strength of the functional connectivity between the temporal-parietal junction, the anterior cingulate cortex (acc), and the amygdala [cit], and might explain global inhibition impairments in the pathophysiology of hallucinations, which may account for this feature beyond the schizophrenia-spectrum, as in lsd-induced hallucinations, for instance [cit] ."
"for example, assume that query qk is used to select attribute ai and attribute aj from table t1. thus, the attributes ai and aj are adjacent within the query q. ai and aj certainly form a temporal locality, in the sense that both are referenced within the same period. the system will access both a i and a j and return their values as requested. however, a i and a j may be very well located in different memory regions, or more www.ijacsa.thesai.org precisely could be located in two different pages in the virtual space. if q k is referenced n times, then it is only reasonable to have a i and a j stored in close proximity, e.g., in the same page or the same page block, such that when the page or page block is transferred from virtual storage to main memory or cache, then the requested items a i and a j will have been located in main memory already."
"the original dataset of our system has 400 instances and 25 attributes. using our approach, this dataset, after classification and restructuring to establish a knowledge extraction dataset, a new dataset was built containing 250 instances and only two attributes."
"variations in transition-to-hallucination functional patterns from one patient to another (e.g., due to phenomenological differences) and from one occurrence to the next (e.g., depending on the modalities involved) appears to be the potential major shortcomings in developing an effective classifier. indeed, such disparities may inexorably lead to a decrease in decoding performances. therefore, characterizing the variability within the prehallucination patterns across subjects and occurrences is highly desired. principal component analysis (pca) is one such unsupervised method that has been successfully applied in the analysis of the variability of a given dataset. the principal components (pcs) and the associated basis patterns shed light on the intrinsic structures of the variability present in a dataset. this unsupervised approach is complementary to the supervised approach described above, as it can help with interpreting the classification performances."
"to verify the performance of the proposed framework of data partitioning, we assumed there is an edge cloud network deployed in a chronic kidney disease center to monitor the patient's status. we experimented our proposed framework on this network and compared it to the same network, without our framework. we compared between these networks in term of data transmission amount, storage space and queries execution time (data retrieval efficiency). the proposed framework is experimented with different queries and different sizes of data. the utilized dataset, experiment environment, queries generation, and the results with their discussion are given as follows."
"given that most of the patients hallucinated more than once during the scanning session, we had more samples than patients (376 samples created from 37 patients). the samples that we used as inputs to the machine-learning process were the statistical parametric maps associated with the slope coefficients of the regression (see figure 1b as an example of one sample). we obtained a dataset of 376 samples:"
"the classification results are presented in table 2 . classification of rest- since the 37 patients included in this study were suffering from multimodal hallucinations (table 1), we also evaluated the performance of the prediction of the tv-enet model on two subsamples, one of which comprised the 32 subjects suffering from auditory hallucinations, among other modalities, and the other comprised the 5 subjects without any auditory hallucinations (table 3 )."
"while both single and dual tasks tug have shown good psychometric properties, dual task tug with manual or cognitive components have potential to provide information even considering the limiting compensation for healthy older persons [cit], probably because cognitive and motor reserve can influence the gait efficiency [cit] . because single task tug tests have shown limited ability to predict falls among community-dwelling elderly [cit], and since dual task tug presents good psychometric results, highly standardised administration procedures and it is easily applicable considering its low cost, time and space required, many studies have successfully investigate identification of clinical important conditions, such as frailty and disabilities risk in patients with chronic conditions using manual or cognitive dual task tug [cit] ."
"in order to analyse the gait patterns during the tug tests, instead of using just the seconds or the number of steps taken to perform the tests, a third independent researcher (blind) extracted features based on information theory such as entropy in order to measure complexity, and frequency features that tries to analyse periodicity, speed and stability of the acceleration by looking at the peak frequencies and harmonics [cit] ."
"low-pass filter in studies with accelerometers it is common to apply a butterworth low-pass filter in order to suppress noise using bandwidth values such as: 50hz [cit], 20hz [cit] and 5hz [cit] . human gait for healthy adults was found to be well characterised by frequencies up to 15hz for walking, running and jumping [cit], but there is not enough evidence on the elderly specially when doing tasks such as sitting and standing (as in the tug test)."
"the segmentation is described in algorithm 1, which basically computes the mean to be subtracted from the input signal (lines 1-2), applying a rolling median filter in order to reduce variance within each tug (line 3) and then, for each half second, sums the values contained in the processed signal (lines 4-5), then it thresholds the data by using this sum (lines 6-9). finally, each segment is labelled and this result is returned (lines [cit] . this algorithm succeeded to segment all but two signals, for which the segmentation had to be corrected manually."
"we first evaluate the diagnostic's effectiveness on data generated from a variety of different synthetic distributions paired with a variety of different estimators. using simulated data here allows direct knowledge of the ground truth value (p, n), and by selecting different synthetic distributions, we can design settings that pose different challenges to the diagnostic procedure. for each distribution-estimator pair and sample size n considered, we perform multiple independent runs of the diagnostic on independently generated datasets of size n to compute the diagnostic true rate (dtr), the probability that the diagnostic outputs true in that setting. we then evaluate this dtr against the bootstrap's actual performance on datasets of size n; because the underlying data generating distributions here are known, we can also compare to known theoretical expectations of bootstrap consistency."
"future studies could use our methods within a free gait data collection study by first detecting sitting and standing activities, which are present in tug tests, and processing those signals in order to extract the features. also investigating orientation-based features, by considering each axis separately is a matter of future work. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0. q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q 0.0 0."
"in addition the following values are analysed: the area under the roc curve (auc), which can be interpreted as the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance; and the f1-score, which is the harmonic mean between the precision and the sensitivity. we perform also a sensitivity vs specificity analysis, in order to find the optimum probability threshold for the variable."
"this data is routinely processed by data analysts, who issue sql queries which are run over the dataset to compute quantities of interest. for example, a typical query might filter the underlying data based on various attributes and then compute a quantity derived from other attributes (such as those given by conviva1, conviva2, and conviva3 above). this computed quantity might correspond to a standard estimator (e.g., the sample mean or percentile), or it might be the result of an arbitrary computation performed by a user-defined function (udf). we study a set of 268 queries selected randomly from the set of obtainable queries which were issued in a production setting against our dataset; 113 out of these 268 queries include udfs which may perform arbitrary computations rather than simply using standard sql operators."
"because of the strong appeal of the application, several papers report measures and features that can be extracted from inertial sensors -such as gyroscopes and accelerometers -and also laser, cameras and devices with multiple sensors [cit] . in this context the accelerometers are specially valuable due to be small, cheap, easy to wear and with low power consumption when compared to more complex devices. recently, regular triaxial accelerometers were found to be reliable when compared with legacy equipments [cit] . moreover they do not depend on environmental monitoring, minimising problems with privacy, usually caused by cameras and home based sensors. in this study we are interested on the analysis of accelerometer data obtained from a single triaxial sensor to identify the fallers."
"we frame the task of evaluating whether or not the bootstrap is performing satisfactorily in a given setting as a decision problem: for a given estimator, data generating distribution p, and dataset size n, is the bootstrap's output sufficiently likely to be sufficiently near the ground truth value (p, n)? this formulation avoids the difficulty of producing uniformly precise quantifications of the bootstrap's accuracy by requiring only that a decision be rendered based upon some definition of \"sufficiently likely\" and \"sufficiently near the ground truth.\" nonetheless, in developing a diagnostic procedure to address this decision problem, we face the key difficulties of determining the distribution of the bootstrap's outputs on datasets of size n and of obtaining even an approximation to the ground truth value against which to evaluate this distribution."
"the eligibility criteria for the study included participants being 60 years and older; possessing the ability to stand up from a chair with arms without other person's help and to walk independently, without aid device. the exclusion criteria were: amputation and/or use of lower limb prosthesis or other device that modifies the gait pattern; neurological or muscular disease, any condition listed it charlson comorbidity index [cit] and presence of any important risk factor that compromises safety, perceived by the evaluator, such as blood pressure lower than 90/60 or higher than 140/100 mmhg or angina. all the participants were eutrophic and none of them was obese [cit] ."
"in order to compare the diagnosis capability of features found to be significant with the regular functional tests based on tug and dual-task tugs, we performed roc analysis showing the curves in figure 8, and the values of auc, sensitivity (tpr), specificity (1-fpr), and f1-score for each individual and combined variable in table 3 . the values for sensitivity, specificity, and f1-score were computed using the optimal probability cut-off threshold of a sensitivity vs specificity analysis as shown in figure 9 ."
"to circumvent the fact that ground truth values for individual real datasets cannot be obtained, we do not directly apply our diagnostic to these three datasets. rather, we treat the empirical distribution of each dataset as an underlying data generating distribution which is used to generate the datasets used in our experiments. with this setup, our experiments on these real datasets proceed identically to the experiments in section 4 above, but now with data sampled from the aforementioned empirical distributions rather than from synthetic distributions. figure 5 presents the results of our experiments on the conviva data. the color scheme used in these plots is identical to that in figure 3, with the addition of magenta, which indicates cases in which the ground truth computations on datasets of size n deemed the bootstrap to not be performing satisfactorily but the bootstrap is expected theoretically to be consistent (i.e., the dtr should ideally be 0). given that the data generating distributions used in these experiments all have finite support, the bootstrap is expected theoretically to be consistent for all estimators considered except the sample maximum. however, as seen in the righthand plot of figure 5, the bootstrap's finite sample performance is often quite poor even in cases where consistency is expected; in this regard (as well as in other ways), the real data setting of this section differs substantially from the synthetic data setting considered in section 4 above."
"devices that help identify fallers can be used to develop programs and implement actions to prevent these falls and to allow the elderly to live an independent life while reducing the long-term care costs. signals obtained by small wearable sensors are widely studied for this purpose. because those are designed to be comfortable to use, those signals can be acquired in high sampling rates and even for long periods, making them a suitable choice to assess ageing in several applications, including fall risk and faller identification for ageing studies [cit] . albeit these facts, the accuracy is key to enable clinical, public health and epidemiological research uses of the signals."
this work can be extended by building on the mathematical properties of the extended wigmore's chart and by identifying advanced means for assigning the initial probability values.
"there are six main symbols required for the construction of the new model. the five foremost symbols are derived from wigmore's chart in a generalized manner to make them applicable to various domains. the last symbol is to represent the new feature of representing a repeated pattern or inclusive diagram. first, white circles are representing the directly related propositions or goals. second, the black circle is a symbol of the directly related facts. third, white squares stand for the subsidiary propositions. fourth, the black squares, which corresponds to the subsidiary facts. fifth, arrows are showing the flow of relations between propositions and facts. arrows are used to clarify the inference logic or flow in the arguments. finally, the black rectangle illustrates the presence of repetitive pattern or another inclusive diagram. the number inside the rectangle will refer to the final conclusion of the repeated pattern or inclusive diagram. the diagram direction is upwards. every symbol will be accompanied with the probability calculation function, which will calculate the provisional probability of each node based on the probabilities of the precedent directly connected nodes. the black circles and squares will be assigned with initial probability values."
"because one of the contributions of this study is to acquire data from 3 tugs, we need to separate those trials by segmenting the signal before the feature extraction step. although it is possible to consider other segmentation methods, we present a simple algorithm to segment the consecutive tug trials."
"in contrast to prior work, we present here a general bootstrap performance diagnostic which does not target any particular bootstrap failure mode but rather directly and automatically determines whether or not the bootstrap is performing satisfactorily (i.e., providing sufficiently accurate outputs) when applied to a given dataset and estimator. the key difficulty in evaluating the accuracy of the bootstrap's (or any estimator quality assessment procedure's) outputs is the lack of ready availability of even approximate comparisons to ground truth estimate quality. while comparisons to ground truth labels are readily obtained in the case of supervised learning via use of a held-out validation set or cross-validation, comparing to ground truth in the context of estimator quality assessment requires access to the (unknown) sampling distribution of the estimator in question. we surmount this difficulty by constructing a proxy to ground truth for various small sample sizes (smaller than that of our full observed dataset) and comparing the bootstrap's outputs to this proxy, requiring that they converge to the ground truth proxy as the sample size is increased. this approach is enabled by the increasing availability of large datasets and more powerful computational resources. we show via an extensive empirical evaluation, on a variety of estimators and simulated and real data, that the resulting diagnostic is effective in determining-fully automaticallywhether or not the bootstrap is performing satisfactorily in a given setting."
statistical test -mann-whitney u-test is carried out in each feature in order to compare the faller and non-faller groups. the u test was chosen because we do not have information about the distribution of the variables.
"bayesian networks and wigmore's chart have number of practical and valuable features. the integration of some of these features will offer a model with superior capabilities and usage. the offered model will encapsulate several characteristics from both earlier models that do not contrast with each other. the extended model has to capture the properties, which are compatible with each other. this will allow the production of useful model, which facilitates the graphical representation of various tasks."
"thus, we have a total of 40 features, i.e. 10 features extracted from each one of the 4 signals, that compose the final feature vector distance-based features because we are also interested in understanding how the tugs with additional tasks are different from the regular one, we also computed the euclidean distance d j (., .) for each feature j related to the full signal (s) and the first tug (t), to the two other tugs -(m) manual task, (c) cognitive task:"
"the pse measure the complexity of the gait signals (via entropy), which is higher for non-fallers. the wpsp is related to the harmonic components of the gait. it is interesting to note that the first harmonic (the fundamental frequency and amplitude that represents the gait) did not differ among the groups, which is probably due to the fact that all participants are active and non fragile. however, they differ when looking at the second and third harmonics, that are related to higher frequencies. again, non-fallers have second and third harmonics in larger amplitude and frequencies, when compared to fallers."
"in section 2, we formalize our statistical setting and notation. we introduce our diagnostic in full detail in section 3. sections 4 and 5 present the results of our evaluations on simulated and real data, respectively. finally, we conclude in section 6."
"by the nyquist-shannon sampling theorem, if a signal is band-limited by a frequency b, a sampling rate of 2b samples per second is needed in order to perfectly reconstruct the signal [cit] . since the use of an arbitrary low pass filter may hamper the analysis, we use a butterworth filter of 100hz as an anti-aliasing filter for frequencies higher than (200/2)hz, because it has the least attenuation over the desired frequency range [cit] ."
"the bootstrap addresses this problem by estimating the unknown (p, n) via the plug-in approximation (pn, n). although computing (pn, n) exactly is typically intractable, we can obtain an accurate approximation using a simple monte carlo procedure: repeatedly form simulated datasets d * of size n by sampling n points i.i.d. from pn, compute u(d *, pn) for each simulated dataset, form the empirical distribution qn of the computed values of u, and return the desired summary of this distribution. we overload notation somewhat by referring to this final bootstrap output as (qn, n), allowing  to take as its first argument either a data generating distribution or a distribution of u values."
"bayesian network (bns) is a general statistical tool that can be applied to various applications. bns are helpful to assess the weight or the influences of premises, to determine the strong inference links. [cit] bayesian network is a graphical representation tool using symbols, numbers and arrows to enable analysts to reason logically far from doubt. it is an appropriate tool to gather and analyze evidences, in order to produce strong arguments. there are two components to construct bns. first, nodes are representing the noticed evidential facts, propositions and variables. second, arrow that connects between various nodes in the diagram. these arrows indicate the dependency probabilities. the value or the weight of each node is affected by the value of the nodes influencing this node and linked with it. the final conclusion of the network is affected by the probabilities of each proposition and inference. (see figure 2) bayesian network is a method to reason logically and rationally using probabilities. the simplest way to understand the goals of bns is to think of a circumstance you need to \"model a situation in which causality plays a role but where our understanding of what is actually going on is incomplete, so we need to describe things probabilistically\" [cit] . there, bns allow analysts to compute the overall probability of the final conclusion. by, computing the probability of propositions connected directly, then the higher connections, then the higher and so on. the benefits from bns are obvious in the prediction of outcomes in doubtful cases. also, the benefits are apparent in the detection of the causes of certain results. the influencing relations are not decisive but probabilistic; the precise probability is assigned for each node and relation. bns are a directed acyclic graph. bns are constructed from nodes and directed links. arrows that connect various propositions are accompanied with the probabilistic information required to define the probability distribution all over the network. to achieve that, initial probability value should be assigned to the nodes with no earlier nodes. then, calculate the provisional probability for the rest of the nodes and for all possible combinations of nodes and their antecedents. bns permit the computation of the provisional probabilities of every node, bearing in mind that the value of some of the nodes has been specified before that computation took place. the diagrams' direction of bayesian network is downwards. in brief, the strength of the final argument is affected by the probability calculations of the supporting evidences and facts. the connections in the network represent the direct inference probabilities. the structure of the network illustrates the probabilistic dependency between various variables in a case. each node is accompanied with a conditional probabilistic table of that node. the mixture of values for the nodes' ancestors will be provided. [cit] .the main incompatibility between bayesian networks and kaos modeling is the fact that the direction of bns is downwards which contradicts with the deduction process of kaos. however, the probabilities feature is an important aspect to be added to the evaluation process."
"of the remaining 251 queries, the diagnostic deemed the bootstrap to be performing satisfactorily on 224, with 9 false negatives and 7 false positives. thus, on this real dataset with accompanying query workload, the diagnostic exhibited low false negative and false positive rates of 3.6% and 2.8%, respectively."
3. power spectrum peak (psp): computed by finding the three highest values of s. this third feature represents the amplitudes of the fundamental frequencies of the gait:
"several consequences arise with increasing elderly population, among them the intensified possibility of occurrence of falls [cit] . the world health organization defines fall as \"come to inadvertently get in the soil or in other lower level, excluding intentional position changes to lean on furniture, walls or other objects\" [cit] . more than a third of older persons fall at least one time per year [cit] . half of those who fell once are likely to experience other falls in the following months, with physical and functional consequences, such as pain, bone fractures, mobility disability, amputations, institutionalisation and cost increases with health care [cit] . these factors put falls as a publichealth problem of great importance [cit] ."
"this paper proposed an extension of wigmore's chart model, intended for evaluating the inference process among goals in kaos models. additionally, it provided a mechanism to measure the possibility of achieving a parent goal if its sub goals are achieved. both wigmore's chart and bayesian networks were reviewed before an extended wigmore's chart could be proposed. the new model provides a mathematical evaluation of kaos, increasing the chances of constructing the right model. the new model presents a method for producing measurable results of the overall goals."
"wigmore's chart (wc) [cit] to help lawyers. [cit] wigmore's chart acts as a legal reasoning diagramming method. wigmore's chart considered as an argument diagramming techniques to demonstrate the structure of reasoning and inferring for an argument in a legal case. the diagram as a whole identifies the logic, structure and grounds behind the reasoning of arguments in legal cases. wc is a tool which enables the creation of arguments followed by the examination of those arguments, then the recreation of those arguments. wc is valuable in cases surrounded with doubt and uncertainty. in order to create wc, analysts of legal cases must identify the connections in all steps of the arguments. then, the analysts should breakdown the argument into propositions and facts. after that, the analysts should connect these facts and propositions together towards inferring the final conclusion of that argument. the chart method of wigmore has a number of symbols to represent the different types of propositions and evidences. these symbols are connected with arrows to specify the direction, influence and weight of the inference. the final conclusions of the chart illustrate the logical deduction of the propositions and facts that assemble the inference. one of the main characteristics of wc is the production of key lists. the key list contains a list of all propositions, facts, evidences and assumptions, which are used to build the final conclusion of the arguments presented. in addition, inference maps show the gathering and linking process of evidences, this validates the argument construction procedure. the chart direction is upwards from facts to assumptions. the chart contains symbols, numbers and arrows only, but, will be accompanied with a key list clarifying the statement of each proposition or evidence (see figure 1 ). there are five main symbols required for the construction of the chart method of wigmore according to schum [cit] (see figure 3) . wigmore's chat properties can be used to evaluate the deduction process of kaos models. but, the lack of measurable results affects the reliability of the evaluation process of kaos models. as shown in the earlier comparison, the need to combine selected features from these two approaches could prove to be beneficial in terms of producing valuable method to evaluate kaos models, as explained in the next section."
"in this paper, we present results of feature extraction on accelerometer data with the aim of identifying fallers among a group of healthy communitydwelling older than 60 years adults (socially engaged, robust, active, nonobese, with preserved cognition). the fact that the sample is composed by healthy older persons makes it more difficult to identify fallers by using regular functional and screening tests such as the tug time trial [cit] . we study features extracted from accelerometer data collected during three consecutive tug tests, in particular frequency features, and analyse how those features can be used to discriminate between faller and non-fallers. in this study we use only one accelerometer, and three tugs (single and dual-task manual/cognitive). therefore, we extract features considering the whole signal -composed by the acceleration data of 3 consecutive tugs -but also considering individually each tug trial."
"our main contributions are: 2. the study of faller identification problem using: tug single task, dualtask manual tug (tug-m) and dual-task cognitive tug (tug-c), not yet explored in the literature in the context of accelerometer-based faller identification;"
"human gait on healthy adults can be well represented by frequencies up to 15hz for walking, running and jumping [cit] . there is still little evidence about the adequate bandwidth for acquiring accelerometer signals from older adults in activities such as walking, sitting, standing and free movement [cit] . those were already studied for healthy adults but not yet established for elderly gait and fall patterns [cit] ."
": the entropy of the power spectrum of the signal. it represents a measure of energy compaction in transform coding [cit], in our case how much acceleration energy the signal contains."
"there are a series of sequential steps to construct the new model representation. first, the analysts should start by identifying the ultimate goal from the analysis. second, the analysis team should realize and assign the final conclusion of the model usage, the penultimate propositions which support the final conclusion and the middle propositions that support the higher propositions. the previous step could be repeated recursively. third, the analysis team have to define the provisional facts and evidences that support all of the propositions in the chart. this can happen by indicating the scenario behind the construction for or against the goal of the analysis team in this case. fourth, the analysts have to list all key premises and inference links to simplify the construction process. fifth, analysts should commit to the appropriate construction of the model, by using the accurate symbols and right features. numbers will be assigned to each symbol indicating the correct proposition or fact from the key list. finally, after the existence of real arguments, the evaluation process should start. the analysis team should assess the arguments and evidences behind them. the analysts have to assign the initial probability values then calculate the provisional probabilities for the whole diagram. afterwards, the joint probability for the whole diagram must be calculated, according to the probability computations rules. by this, the evaluation process could be emphasized. this will help to generate measurable outcomes to solve various issues. figure 4 presents the usage of the new models symbols. the next section will provide a glimpse about the significant of using our new model."
"we individually evaluate each variable/feature to look for the ones that are able to discriminate fallers from non-fallers, i.e. rejects the null hypothesis of equal means. we also performed fusion of relevant features by using the normalised average. this practice is common in pattern recognition systems in order to take advantage of the complementarity of different variables. the combination using the average is often called early fusion [cit] ."
"this research was supported in part by nsf cise expeditions award ccf-1139158 and darpa xdata award fa8750-12-2-0331, and gifts from amazon web services, google, sap, blue goji, cisco, clearstory data, cloudera, ericsson, facebook, general electric, hortonworks, huawei, intel, microsoft, netapp, oracle, quanta, samsung, splunk, vmware, and yahoo!. ameet talwalkar was supported by nsf award no. 1122732."
"the timed up and go test (tug) is widely used in both clinical and epidemiological studies; since the time spent to complete the test is often correlated to functional mobility and associated with a past history of falls [cit] . the tug is also used to assess the risk of falls and to select interventions for older individuals according to the updated guidelines of the american and british geriatric societies for the prevention of falls [cit] . for brazilian older adults, the 12.47 seconds cut-off point is adopted as a predictive value for fall [cit] ."
"upon the publication of the paper, the raw collected data and the code used to produce the results will be available at a public repository. upon publication of the paper a release will be built with documentation and the anonymised dataset containing the signals and extracted features, generating a doi that will allow public access of data and code. the code will allow researchers to extract the features described in this paper in their own data, as well as reproduce our analyses."
"the suggested model extracts most of its properties from the chart method of wigmore with the inclusion of one property of bayesian networks and other external aspects. the model has to include additional aspects in order to address the gaps, which are not fulfilled completely by bns and wc. the new model has several features. first, it enables both top down and bottom up approaches, in order to facilitate the generation of models starting from the basic premises or starting from the desired conclusion. second, the new model allows the production of measurable results to provide more accurate and reliable representation, through the introduction of probabilistic calculations. the third feature states that the new model should be extendable to be applicable to various domains. this is related to the notations of the models and the observation of contextual knowledge. fourth, the new model eases the creation of representation supporting the desired goal, and against the desired goals. finally, the new model will eliminate the complexity and ambiguity raised from representing the multilayered nature of cases or similar repetitive patterns. this multilayered nature could cause high complexity as stated by hepler \"if all these features are represented in one diagram, the result can be messy and hard to interpret\" [cit] . another cause of complexity is the reappearance of a similar pattern of evidences and relations between facts and propositions, within the same case or in similar cases. and it would be \"wasteful to model these all individually\" [cit] . it allows any network to contain an instance of another network without showing the detailed structure until requested. moreover, it authorizes the creation of general networks that contains repeated patterns of evidences and relations, which can be reused after few amendments to customize the structure to the current case. this feature can be represented in the diagram as a special symbol. this model aims to simplify the creation of probabilistic graphical models and to convert the presentation into more efficient and understandable form."
"for ease of exposition, we assume below that  is realvalued, though the proposed methodology can be straightforwardly generalized (e.g., to contexts in which  produces elements of a vector space)."
"these results are important because, although non-fallers in average completed the tugs faster than fallers (see table 1 ), there is no significance between the means of the groups. we believe that the sequence of activities in a tug test (standing, walking, turning, walking, sitting) carries a richer composition of frequencies, which we believe was captured by pse and wpsp and the distance-based features."
"we finally evaluate the diagnostic's effectiveness on a larger real dataset and accompanying real-world analytical workload derived from a sql-based ad-hoc querying system at conviva, inc. [cit] . this dataset is 1.7tb in size and contains approximately 0.5 billion records extracted from access logs describing video streams viewed by internet users during a thirty day time span. we treat each record, which has 104 attributes (such as video genre, web browser type, request response time, etc.), as a data point."
"as far as we know there is no previous study including the items 2-3 described above, i.e. that compares gait features using tug variations for the purpose of elderly faller's identification using a single accelerometer sensor, independently of step detection and investigating adequate sampling rates. also, studies on fall risk and faller identification often do not release the dataset for reproducibility in those studies, and their sample are seldom stratified with respect to gender and age, often not providing information on functional mobility of the subjects [cit] . we believe that making this dataset available will allow for a faster advance of the field. finally, as we show in the results, the identification of fallers among a sample of health elderly is challenging and conventional functional tests fail to provide a threshold for screening."
"a single triaxial accelerometer sensor (analog devices adxl362) was used to acquire the signal using a datalogger set at a sampling rate of 200hz. each participant was ask to wear the sensor using an elastic belt around his/her waist (in front of the mass centre). in fig 3 the overall pipeline is shown: the two first steps (green boxes) are related to the data acquisition; the following two (orange boxes) to the signal pre-processing and feature extraction, and highlighted in darker blue shade are the results obtained using the statistical tests and roc curve analysis. the details of each step are given in the following sections."
"kaos is a goal oriented requirements analysis method, developed by university of oregon and university of louvain. kaos stands for knowledge acquisition in automated specification [cit] . the main advantage of kaos over other requirements analysis methods, which are not part of the goal analysis family, is its ability to align requirements to business goals and objectives. this alignment increases the chances that the new development will add value to business. kaos focus on realizing and indicating the business goals, then specifying the requirements that infer to the business goals. \"each goal (except the leaves, the bottom goals) is refined as a collection of sub goals describing how the refined goal can be reached\" [cit] . the structure of the various connected requirements and goals is represented hierarchically in graphical notation in an upwards direction. the top goals are strategic objectives for the business. as low as the diagram level reaches as closer to the low level requirements. the root of the diagram is the ultimate business goals. then, the analysts must identify the penultimate goals followed by the lower goals and so on. the previous step is recurring until the analysts reach the basic goals. the lower goals are linked with the parent goals through union. the union indicates that the completion of the lower goal successfully will definitely cause the completion of their parent goal. figure 1 shows an example of a simplified kaos model. kaos main focus is on the business requirements, disregarding if this requirement is part of the computer system requirements or not. each goal is accompanied with obstacles and the stakeholders involving in this goal. a limitation of kaos is the lack of any inference evaluation capabilities. the achievement of sub goals does not imply the achievement of their parent goals in all cases. the next section presents a review of two candidate approaches to solve this issue."
"frequency analysis is a widely used tool to extract information from signals that are not clear when looking at the time domain. we use the fast fourier transform in order to analyse the frequency characteristics of the acquired signals, with respect to the different tug tests."
"this study enrolled 41 community-dwelling elderly divided as: 19 fallers and 22 non-fallers volunteer participants' residents in so carlos/ [cit] . the history of falls was determined with a single question: \"have you fallen within the past year?\". data from five participants were lost due to acquisition issues; thus, this study final sample (94% power and 5% error) is 18 fallers and 18 non-fallers, as depicted in fig 1. the study protocol (482.306/2013) and informed consent form received ethics approval from the ufscar ethic committee on human experimentation. the project was advertised at the university of the third age groups, for a population of 468 elderly and the volunteers received a written informed consent concerning conduct of the survey. participation was voluntary and it was explained that the volunteer could leave the study whenever he/she wanted without suffering any loss or consequence. in the open dataset, only the raw accelerometer signal, the gender and the label (faller or non-faller) are available, while the remaining variables are not available in order to assure complete anonymity of participants. all the participants were refer to free physical therapy and gerontology intervention after this study."
"in this study, three variants of tugs were conducted by two trained gerontologists researchers. the regular one we refer as single task or only tug, requiring a participant to stand up from a chair, walk 3 meters, turn, walk back, and sit down, while the time taken to perform the task is recorded in addition the tug dual-task is adopted in two different approaches: tug manual (tug-m), following the same procedure as the regular tug, but carrying a cup filled with water; and tug cognitive (tug-c), in which participants are asked to respond to continuous simple subtraction questions while performing the tug test [cit] ."
"3. the description of features based on tugs and differences between tugs using a single accelerometer sensor, as well as feature fusion methods, resulting in variables that are able to discriminate fallers from non-fallers while conventional functional tests cannot."
"roc analysis -in order to compare how the variables extracted from the different signals are able to provide discrimination between the groups, we performed roc (receiver operation characteristic) analysis [cit] . a roc curve shows the relationship between the true positive rate (tpr) and the false positive rate (fpr). those values are related to the sensitivity (tpr) and the specificity (1-fpr) ."
"a limitation of the study is the sample size (36 participants). in addition, due to the intra-class variability of the data, it is not possible to generalise this result for a broader population. however, the results does indicate that features based on dual-task tugs are able to better discriminate faller and non-fallers, even in a scenario when all standard tests and measures were insufficient to show significant differences. furthermore, both distance-based features and fusion shown to be interesting methods to improve the results. finally, the acquired dataset is available to be used in future investigations."
"in this section, two graphical representation models will be studied as possible methods to evaluate koas models. the features of these approaches will be examined to check the suitability of them to enclose kaos models."
"unfortunately, however, while the bootstrap is relatively automatic in comparison to its classical predecessors, it remains far from being truly automatically usable, as evaluating and ensuring its accuracy is often a challenge even for experts in the methodology. indeed, like any inferential procedure, despite its excellent theoretical properties and frequently excellent empirical performance, the bootstrap is not infallible. for example, it may fail to be consistent in particular settings (i.e., for particular pairs of estimators and data generating distributions) [cit] . while theoretical conditions yielding consistency are well known, they can be non-trivial to verify analytically and provide little useful guidance in the absence of manual analysis. furthermore, even if consistent, the bootstrap may exhibit poor performance on finite samples."
"by using the statistical test, we can assess the capability of a given feature (variable) to reject the null hypothesis of equal means between the faller and non-faller groups. we are interested in features that are able to better discriminate between groups, in contrast with the regular functional tests which had failed to show significant differences. in figure 6 we show the boxplots of each group for each feature (in order to visualise all in the same plot, the values are normalised to the same numeric range). it is possible to observe that the frequency features seems to be informative. however, the statistical test showed differences only for the features extracted from the tug-c, namely the pse and the wpsp2 and wpsp3."
"recall that a query typically filters its input data on one or more attributes before computing its output. to address cases in which queries are highly selective and hence effectively operate on severely reduced quantities of data, we do not consider the 17 out of our 268 queries which filter out more than 99% of the data in any of the diagnostic subsamples; one might consider the diagnostic as simply not being applicable to such queries because the available subsamples are not sufficiently large after the queries apply their filters."
"an example of segmentation is shown in fig 5, in which the regions of the signal outside the grey regions will be ignored in the feature extraction step."
this section will show how the new extended model could be used to evaluate kaos. the extended model will enclose kaos goals and provide a measurable evaluation of the possibility of achieving the final outcome. figure 5 shows a basic kaos model with three goals.
"the table 1 summarizes the demographic and functional mobility characteristics of this study participants: mmse (mini-mental state examination) [cit], fes (falls efficacy scale-brazil) [cit], the three tugs [cit] : (i) single task, (ii) tug-m (dual task manual), (iii) tug-c (dual task cognitive). the tug results are shown in seconds. we also compared the differences in seconds between the different tug tests, computed the average between the tug seconds for each participant, and counted the number of steps during each test. the groups were compared using the t-test for all variables, except for the gender, for which a fisher's exact test was used."
"where x(t), y(t) and z(t) are the accelerometer data acquired from the axis x, y and z, respectively. because we do not assume a fixed position of the sensor, figure 3 : overall picture of the methodology: the triaxial signal is collected and a fusion is performed to keep the methods orientation-independent; then the signal is filtered using butterworth method, and each tug is segmented, and 32 features are extracted; the signal is analysed with respect to the sampling rate, the extracted features are studied using a statistical test, a feature ranking and a classification experiment. the squared sum of each axis allows comparing different outputs regardless the sensor orientation. note that the discrimination between fallers and non-fallers is not trivial by looking only at the signals s(t), as the examples shown in fig 4. therefore, pre-processing and frequency analysis are needed to extract discriminative features, as we will describe in the following sections."
"it then remains to use this ability to evaluate the bootstrap's performance at smaller sample sizes to determine whether or not it is performing satisfactorily at the full sample size n. to that end, we evaluate the bootstrap's performance at multiple smaller sample sizes to determine whether or not the distribution of its outputs is in fact converging to the ground truth as the sample size increases, thereby allowing us to generalize our conclusions regarding performance from smaller to larger sample sizes. indeed, determining whether or not the bootstrap is performing satisfactorily for a single smaller sample size b alone is inadequate for our purposes, as the bootstrap's performance may degrade as sample size increases, so that it fails at sample size n despite appearing to perform sufficiently well at smaller sample size b. conversely, the bootstrap may exhibit mediocre performance for small sample sizes but improve as it is applied to more data."
"as occurred with the tug test seconds in our study, in previous works, speed gait only was also shown to be insufficient to identify fallers [cit], which might indicate the importance of analysing different activities. according to our results, by pre-processing the tug signals and extracting both individual frequencies features (pse, wpsp) and other comparative features, it is possible to obtain a set of features with significant difference between the fallers and non-fallers."
"the main obstacle of the proposed evaluation approach is that it is not always feasible to know and assign the possibility of the inference from the leaves to the parent node. the proposed model suggested the use of a new separate model rather than extending kaos model. this is to avoid adding complexity to kaos models, in addition to the standardization grounds."
"in a survey about fall risk, the authors pointed out the important role of the sitting and standing movements in the continuous monitoring of functional mobility [cit] . indeed, recent studies about fall and technology often apply timed up and go (tug) tests with one or more accelerometer sensors [cit] and also gyroscopes [cit] in order to investigate gait behaviour and falls, often in hospitalised or disabled participants."
"assessment of an estimate's quality-for example, its variability (e.g., in the form of a confidence region), its bias, or its risk-is essential to both its interpretation and use. indeed, such quality assessments underlie a variety of core statistical tasks, such as calibrated inference regarding parameter values, bias correction, and hypothesis testing. beyond simply enabling other statistical methodology, however, estimator quality assessments can also have more direct utility, whether by improving human interpretation of inferential outputs or by allowing more efficient management of data collection and processing resources. for instance, we might seek to collect or process only as much data as is required to yield estimates of some desired quality, thereby avoiding the cost (e.g., in time or money) of collecting or processing more data than is necessary. such an approach in fact constitutes an active line of work in research on large database systems, which seeks to answer queries on massive datasets quickly by only applying them to subsamples of the total available data [cit] . the result of applying a query to only a subsample is in fact an estimate of the query's output if applied to the full dataset, and effective implementation of a system using this technique requires an automatic ability to accurately assess the quality of such estimates for generic queries."
"the alignment of requirements analysis to business goals and objectives is essential for the return of investment of any project. kaos is a goal driven requirements analysis method that defines a goal tree with parent and sub goals. kaos assumes that achieving all sub goals of a parent goal will guide to the achievement of the parent goal. the inferring process in kaos is informal, due to the nature of deduction in kaos, which is based on the assumption that the completion of sub goals leads decisively to the parent goal. however, there is no guarantee that the previous assumption is always valid. the lack of precise assessment for kaos goals requires further consideration. usually, in realty some sub goals does not lead to the parent goal due to some contextual knowledge that was not measured completely in kaos representation. another cause of the uncertainty of goals originates from the possibility of assigning multiple values to one goal rather than only two possible values (true or false), which is the only option taking into account in the current features of kaos. for instance, if one of the sub goals was completed partially, there is no feature to measure the impact of this sub goal to the parent goal."
"the groups faller and non-faller were paired in gender and age to allow comparison. therefore, the groups do not have significant differences considering the demographic characteristics. even so, the fallers display a expected slightly older average age since fall prevalence increases with age [cit] . the groups are also, as expected, more feminine, probably because, as they age, women are more likely to become fallers and to experience negative outcomes from a fall episode than men [cit] . both groups are similar also regarding the functional mobility variables. according to the falls efficacy scale-brazil (fes-i-brazil) [cit], participants from both groups feel little concern with the possibility of falling when carrying out functional activities. moreover, while it is expected that the fallers conduct the tug single task in more than 12.47 seconds [cit], this particular group of fallers equates to non-fallers for tug execution time. even for tug manual and tug cognitive times, both groups can be considered similar. considering only the functional mobility tests would not be possible to discriminate between fallers and non-fallers. this is probably because the participants are involved in regular physical and cognitive activities and can be considered in successful ageing [cit] . in addition, the control variables addressed by the exclusion criteria are related to increases in fall [cit] . therefore, other methods are needed to identify fallers in this scenario."
"this paper takes into account the possibility of failures in achieving the ultimate goals in kaos models. this paper will propose a new graphical representation model, which can absorb kaos models to be represented through it. the new model enables analysts to provide measurable ultimate goals accompanied with probability to give analysts statistical results. these results will facilitate the evaluation process of the whole kaos model. the new model will formalize the inferring process to be mathematically valid."
"modern datasets are growing rapidly in size and are increasingly subjected to diverse, rapidly evolving sets of complex and exploratory queries, often crafted by nonstatisticians. these developments render generic applicapermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components of this work owned by others than the author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. request permissions from permissions@acm.org. kdd'13, august [cit] 2013, chicago, illinois, usa. copyright is held by the owner/author(s). publication rights licensed to acm. acm 978-1-4503-2174-7/13/08 ...$15.00. bility and automation of data analysis methodology particularly desirable, both to allow the statistician to work more efficiently and to allow the non-statistician to correctly and effectively utilize more sophisticated inferential techniques. for example, the development of generic techniques for training classifiers and evaluating their generalization ability has allowed this methodology to spread well beyond the boundaries of the machine learning and statistics research community, to great practical benefit. more generally, estimation techniques for a variety of settings have been rendered generically usable. however, except in some restricted settings, the fundamental inferential problem of assessing the quality of estimates based upon finite data has eluded a highly automated solution."
"we also performed fusion of relevant features by using the normalised average. this practice is common in pattern recognition systems in order to take advantage of the complementarity of different variables. the average method is known for producing predictions with reduced variance, which can potentially improve the results [cit] . this combination is often called early fusion and in this paper it is performed by first normalising each variable to a [cit] range and then averaging all values, obtaining a single combined variable."
"bayesian networks and wigmore's chart have valuable features, which can aid the needed evaluation of kaos models. however, their weakness does not provide a sufficient method for evaluation."
": computed by finding the frequency related to the higher value of s. this feature represents the first harmonics of the gait, which is related to the overall movement speed:"
"the constructed kaos model could be evaluated by transferring the current goals and requirements in this kaos model to the new models' graphical representation. this step is quite simple. the new model is similarly upwards. each goal will be in the same position in the diagram as it was. the direct goals and requirements are represented as white and black circles. all nodes in kaos tree will be represented as circles, the basic requirements with no earlier nodes are black and the goals are white. the accessory goals and requirements correspond to white and black squares. in the new model, analysts will be allowed to represent partially related goals and their requirements as squares, the basic related requirements are black and the related goals are white. unions inside the kaos model can be represented as arrows in the model. the constraints within kaos model can be represented as a rectangle, which can symbolize the contextual knowledge or any compound model that involves repeated pattern or another model structure. figure 6 shows how to enclose the previous kaos model into the new model. then, the initial probability values have to be assigned to the nodes in the diagram. after that, analysts have to calculate and assign the provisional probability to all goals. these provisional probabilities will be produced by computation functions assigned with the inference process, which will calculate the provisional probability of each node based on the probabilities of the precedent directly connected nodes. this function should be following the acknowledged probability rules."
"ideally, we might approximate (p, n) for a given value of n by observing many independent datasets, each of size n. for each dataset, we would compute the corresponding value of u, and the resulting collection of u values would approximate the distribution qn, which would in turn yield a direct approximation of the ground truth value (p, n). furthermore, we could approximate the distribution of bootstrap outputs by simply running the bootstrap on each dataset of size n. unfortunately, however, in practice we only observe a single set of n data points, rendering this approach an unachievable ideal."
"regarding the distance-based features, those measure how the features differ among different tug tests, by computing the distance between values extracted from the signals. we observed that the values showed larger differences between signals for non-fallers when compared to fallers. this indicates hat the different tugs altered more the gait pattern of non-fallers during the tests."
"in fig 7 we also show an example of the power spectra of signals acquired from a non-faller and a faller participant. it can be seen in this example how the distribution of frequencies is more diverse among the signals in the case of the faller, while in the non-faller case the frequency features are more similar."
"in recent decades, the bootstrap [cit] has emerged as a powerful and widely used means of assessing estimator quality, with its popularity due in no small part to its relatively generic applicability. unlike classical methods-which have generally relied upon analytic asymptotic approximations requiring deep analysis of specific classes of estimators in specific settings [cit] -the bootstrap can be straightforwardly applied, via a simple computational mechanism, to a broad range of estimators. since its inception, theoretical work has shown that the bootstrap is broadly consistent [cit] and can be higher-order correct [cit] . as a result, the bootstrap (and its various relatives and extensions) provides perhaps the most promising avenue for obtaining a generically applicable, automated estimator quality assessment capability."
"outputs satisfies the performance criterion defined by c3, -that is, whether or not the  quantile of the absolute relative deviation of bootstrap outputs from (p, n) is less than or equal to c3-determines the ground truth conclusion regarding whether or not the bootstrap is performing satisfactorily in a given setting. to actually evaluate the diagnostic's effectiveness, we then run it on 100 independently generated datasets of size n and estimate the dtr as the fraction of these datasets for which the diagnostic returns true. if the ground truth computations deemed the bootstrap to be performing satisfactorily in a given setting, then the dtr would ideally be 1, and otherwise it would ideally be 0. figure 3 presents our results for all distribution-estimator pairs and both sample sizes n considered. in these plots, dark blue indicates cases in which the ground truth computations on datasets of size n deemed the bootstrap to be performing satisfactorily and the bootstrap is expected theoretically to be consistent (i.e., the dtr should ideally be 1); red indicates cases in which neither of these statements is true (i.e., the dtr should ideally be 0); and light purple indicates cases in which the ground truth computations on datasets of size n deemed the bootstrap to be performing satisfactorily but the bootstrap is not expected theoretically to be consistent (i.e., the dtr should ideally be 1)."
"we next evaluate the diagnostic's effectiveness on real datasets obtained from conviva, inc. [cit], which are routinely subjected to analysis by practitioners. our first set of experiments pairs this real data with the (synthetic) estimators considered in the previous section; we then consider a larger dataset paired with a set of 268 production sql queries."
"(continued from previous page) conclusions: by using a holistic approach, we have been able to understand user needs, behaviours and interactions and give new insights in the definition of effective decision support systems to deal with the complexity of t2d care."
"about the evaluation phase, the first finding is about the difference of studies that were chosen to evaluate the two solutions. given the innovative aspects associated to solution 1, it is difficult to find centres that have available and reliable information to screen for t2d, as well as human and financial resources to perform active searching. for this reason, the evaluation study had an exploratory nature, to understand the reactions of endusers to this kind of innovative software solution. we thus introduced validation cases to simulate actions and interactions between users and units. in this phase, the most important feedback related with t2d screening is that we were introducing a new process, rather than facilitating or supporting the existing practice. this means that users need to learn and understand how this tool can be related with their daily activities. the resulting good levels of user experience versus the not good level of usability suggest that they perceived the good potentials of the tool, but they are worried about the complexity it adds to their working routines."
"in the case of the sus we have used the grading score from sauro and lewis [cit], ranging from f (unsatisfactory) to a+ (absolutely satisfactory):"
"the tools were evaluated at istituti clinico scientifici maugeri hospital of pavia (icsm), italy, for solution 2.1 (population management) and 2.2 (clinical decision the implemented evaluation strategies, while following the general proposed schema, required specific adaptations. in the solution 2.1 the focus was to evaluate the introduction of a new process that was not possible earlier for diabetes patient management. in the solution 2.2, the evaluation of the impact was performed both during clinical activities while monitoring clinicians using the tool and by performing usability testing. in the following, we illustrate the results of the evaluation activities using the log files of the system, focus groups and questionnaires."
"1. semantichealth fp6 5 focused on semantic interoperability issues of electronic health systems and infrastructures and provided a number of relevant definitions, standards, and application domains for semantic interoperability [cit]"
"in a special issue, the journal of clinical oncology has focused on pm in oncology [cit] showing that this new era of medicine offers new perspectives to cure cancer. pm also raises numerous challenges including biobanking, bioinformatics and legal issues [cit] . the intrinsic complexity of cancer and the variety of its forms (each tumor being genetically unique) designate this pathology as a prime target for pm approaches. cancer is a disease caused by the accumulation of mutations occurring in critical genes (oncogenes and tumor-suppressor genes) and resulting in the alteration of key molecular pathways. due to the genetic nature of cancer, the oncology research has largely benefited from the advances in high-throughput genomics technologies in order to decipher the molecular alterations involved in the tumorigenesis on one hand, and to help the clinician to tailor the therapy on the other [cit] . molecular profiling based on genomics information from the tumoral dna and constitutional dna offers new insights into the prediction of the disease progression and the response to treatment for each individual patient. these approaches are based in particular on two dominant concepts: oncogene addiction and synthetic lethality. the first one, oncogene addiction, stipulates that some tumors rely on one particular oncogene for their survival and progression, and inhibiting this gene would therefore stop tumor growth; [cit] . the second one, synthetic lethality, refers to the observation that the inactivation of a pair (or more) of genes might be lethal, whereas individual inactivation of any of these genes would not kill the cell. it offers an opportunity to selectively kill cancer cells, if they already present gene inactivation for one gene of the synthetic lethal pair, by targeting the second gene of the pair. a famous example is the synthetic lethality of brca and parp genes, which is exploited by using parp inhibitors for treating brca deficient breast cancer tumors. both oncogene addiction and synthetic lethality are typical situations where targeted therapy should be the solution of choice."
"the output of this phase is a structured and balanced description of user needs, values and attributes that can be used as specifications for the implementation phase."
"in the context of the shiva trial, the clinical data needed for the mbb are first imported in a dedicated module of the kdi system, named clinicaldb (clinical database, figure 3a ). this step is performed weekly and updates the system by creating the patients recently included in the trial into the kdi core database. at the same time, an anonymous identifier is generated by the system for each new patient. conversion between the different patient identifiers is guaranteed by the kdi core database and is accessible through the bioinfo-portal web application. once available, the raw data generated by the biotechnological platforms are transferred to the bioinformatics platform for analysis (using rsync system). bioinformatics pipelines (mutation and dna copy number pipelines) process each molecular profile and are responsible for raw data storage and traceability within the kdi core database. the summarized results are structured in the bird (biological results database) application. the last step of the data integration workflow is the generation of the bioinformatics reports. two reports are required in the context of the shiva clinical trial at two different time points. a first report is generated by the system after the processing of the dna copy number profile in order to request an ihc validation if needed. a second report is generated by the system and sent to the mbb for the final therapeutic decision. all reports, data and analysis results for each patient are gathered within the kdi modules (kdi core database, clinicaldb, bird). all the information are available under controlled access for any member of the project through the kdi bioinfoportal. in order to supervise the patients' process at each step of the whole bioinformatics workflow, an additional module of the system named the bioinfo-board application (figure 6 ) has been developed. this web application aims to controlling, monitoring and checking the evolution and status of each shiva patient in real-time."
"solution 1 represents a change in terms of organizational and procedural aspects as it triggers an active risk stratification of t2d (in primary care, secondary care or healthcare agencies). the action required to introduce an effective risk stratification tools for t2d regards an active collaboration with the healthcare organizations available to implement this innovation. this involves generating data warehouses specific for this purpose (upon a process of data pooling and quality checks) and the training of healthcare professionals. the personnel involved in the evaluation has provided feedback on how to improve some specific aspects, such as: body mass index (bmi) categories should meet the who recommendations; the decision-making process for risk predictions should be based on the comparisons with what detected by professionals; recommendations should be displayed in checkboxes, so to better select them depending on the risk estimation and the imputed variables; more filtering options should be included (e.g. erectile dysfunction)."
"our study has several limitations: first, we were not able to prospectively evaluate the final prototype of solution 1 in primary care settings and healthcare agencies; to minimize this limitation we involved these types of users in single sessions of usability tests. second, we were not able to finalize the developments of the final prototype for solution 2; this would have allowed us to have even better usability score. these limitations, which are typical of interdisciplinary and international research projects, recall real life situations, when the project management team must take decisions that may positively affect some aspects and negatively influence others. it is important to take consensual decisions and give preference to multiple, multifaceted, stepwise short but focused evaluations, rather than wide but dispersed ones."
"the needs identified during this phase were shared by all the cases and include: the need to increase personalized filtering and stratification functionalities, to provide visualization options, to automatically build and send reports, to include additional contextual information and explanations, and to provide more aggregated and less complex information."
"the comparison of the center population at two different time points. a comparison of the population between the start and the end of the validation period highlighted a significant increase in the number of patients with high cardiovascular risk. this result stimulated clinicians to in-depth inspect how the cardiovascular risk evolved in time, to understand which groups of patients reached the maximum risk level, and to formulate hypotheses on the detected situation."
"(b) mutation analysis pipeline. the sequenced reads are aligned on the human reference genome, and centered on the targeted genomic regions. single nucleotide variations (snvs) and insertion/deletion (indels) are then called. the filtered variations can then be annotated using additional databases in order to lead to a final list of potential druggable variants."
"once requirements were defined, the design process started. in addition to traditional methodologies for software development, the main ideas were drafted via use cases description and functional specifications, which allowed to create and discuss mock-ups with the intended end-users. following up on these discussions, the prototypes were refined from low-level mock-ups to functional mock-ups. the functional mock-ups were tested with human computer interaction (hci) experts first and then with end-users in close to real life situations. both were invited in several rounds to test whether the mock-ups match usability standards, enduser's way of thinking and working. more in detail, heuristic evaluation (he) was used by hci experts to identify usability issues in the existing concepts and prototypes, and to identify common usability issues before performing tests with end-users. evaluations were performed by applying the 10 nielsen heuristics [cit] . in turn, the concept and the first functional prototypes were refined. then, usability testing was performed by end-users to identify improvements in the software prototypes. the usability levels were assessed through two validated questionnaires, one to measure satisfaction as perceived usability, the system usability scale (sus) [cit], the other one to assess user experience, the attrakdiff [cit], in line with the iso9241.210 [cit] ."
"the most important problem in the case of solution 2 were inconsistencies found while using the system. the results of these tests lead to several and important changes on the prototypes. within solution 1, filtering options were added, while interactions and displayed information were made clearer. a new functionality to create reports for the users of the different use cases was added. in the case of solution 2 interactions with the users were simplified by means of aggregated visualization features: traffic lights were added, to capture the attention of the care provider in case a negative trend exists (e.g.: increased glycated haemoglobin (hba1c) level, decreased adherence to drug, etc.)."
"the large heterogeneity of the data that are collected along the healthcare pathway hampered their exchange and their comparison. therefore, it is crucial to describe all the data that are generated with controlled vocabularies also called ontologies. ontologies offer a formal representation of knowledge with definition of the relevant semantic attributes, their hierarchy and their relationship using a well-defined logic. importantly, not only one single ontology can pretend to describe all the knowledge in a field but different ontologies (see 4 ) are necessary to cover different entities of interest such as the gene (gene ontology), the disease (disease ontology) and the sequence (sequence ontology). semantic web standards promoted by world wide web consortium (w3c) make it possible to link knowledge and data together so they can be queried and retrieved. to this aim, the resource description framework (rdf) data format along with sparql query language provide the technical framework to describe, share, interact and query semantic data. while the technical solutions exist to support data exchange and linking, the definition of ontology, their choice and their use in practice for healthcare and biomedical data is still an issue. in order to tackle these challenges and to promote the use of standards and ontologies in the biomedical field, many european initiatives supported by the european community (fp6 and fp7 programs) are involved in the definition and harmonization of standards:"
"we performed two evaluation sessions involving three clinicians and one healthcare policy maker. during these sessions, clinicians were asked to use solution 2.1 and compare snapshots of the mosaic population across different time periods. we considered the population at the beginning and at the end of the validation study [cit] ). the main clinical comments on the center population regarded:"
"about solution 1, we can appreciate from fig. 4 that length of prediction horizon, uncertainty management, usability and model performances are the most important needs. for solution 2.1, model performances and uncertainty managements are important, however costefficient output and optimization of resources are the top ones. in the case of solution 2.2 there is a mix of needs like solution 1 and solution 2.1. usability is highly important for solution 1 and solution 2.2, while has a lower level of importance for solution 2.1. tools availability, multimodality, and compatibility with ehrs are ranked as less important for solution 1 and solution 2.2, and as medium important for solution 2.2."
"during the implementation process, two iterative rounds of development took place, resulting in a low-level mock-up and functional mock-up respectively. the first mock-up was evaluated through heuristic evaluation, while the second was also evaluated through usability tests with real users (their profile is provided in table 1 ). the most relevant usability aspects emerged during the first evaluation were related with the most common usability errors (h1 -visibility of system status, h2 -match between system and the real world and h7 -flexibility and efficiency of use). experts identified the importance of decomposing the process that the tool is supporting into smaller steps, while giving them a clear picture about the whole process, and where she/he is at a given moment (e.g. by using breadcrumbs). they also suggested to inform users on the expected event upon clicking a certain button or link through clear text labels, as icons may be not enough selfexplanatory."
"business core of our web applications has two main objectives: (i) provide structured data for presentation layer, and (ii) make data available for remote and secured access by other applications and technical users. standard services are developed using core functionalities of spring framework (aspect-oriented programming -aop, inversion of control -ioc, javabeans factory). web services are published (server side) and invoked (client side) through apache cxf framework. to respect web services security (ws-security) standards, we use the apache wss4j project provided by cxf (with interceptors chain process) to set up a username token authentication on each web application in the system."
"managers, to predict how many patients are at risk of developing t2d and eventually design intervention strategies at regional level. vc2.s1 -t2d prediction tool for clinicians, to predict how many patients in a healthcare unit are at risk of developing t2d and design interventions at unit level. vc2.s2 -case titration, to validate the system response. this tool is used by a doctor or by a nurse, who has a list of patients at risk and its goal is to help users to validate the risk of t2d suggested by the system. vc2.s3 -t2d detection and parameter estimation tool, estimating the risk to develop t2d within the next 12 years for each patient, based on the automatic estimation performed on the values of clinical and demographic variables that have no value and based on the estimation of oral glucose tolerance test and high density lipoprotein cholesterol."
"the identification of genomic alterations used as biomarkers along with the emergence of molecularly targeted agents (mtas) such as tyrosine-kinase inhibitors have promoted the development of pm in oncology. mtas have proven their efficacy in some cancer subtypes and they provide new opportunities to treat the disease [cit], for a review). the first mta has been trastuzumab, which is a monoclonal antibody targeting the erbb2 receptor. this gene is amplified in 15-20% of patients with breast adenocarcinoma. treating patients with locally advanced disease with trastuzumab for a year decreases by 50% the risk of recurrence [cit] . targeting the bcr/abl fusion gene (i.e., the philadelphia chromosome) with another mta, imatinib, in patients with chronic myelogenous leukemia has dramatically improved their outcome [cit] . braf(v600e) mutation is frequently associated with melanoma, where it seems to play a critical role in the malignancy process and can be effectively treated using vemurafenib [cit] . braf(v600e) mutation has been also identified in multiple forms of advanced cancers such as colorectal or thyroid cancer [cit] . it is generally accepted today that using mta has great potential in the treatment of many types of cancer. around 40 mtas have been approved to date for the treatment of cancer and the development of new inhibitors is in progress. developing new mtas imply also to decipher new biomarkers among the large number of genomic alterations observed in tumors (mutations, amplifications, deletions, translocations, fusions and other structural variants). a large number of genomic alterations are passengers while very few are drivers. a subset of these drivers are actionable, i.e., have significant diagnosis, prognosis, or therapeutic implications in cancer, and a subset may also be druggable, i.e., targets for therapeutic development [cit] . classifying these genomic alterations into actionable and/or druggable is difficult and high-throughput screening techniques might help this classification. the possibility to search within each tumor the actionable/druggable alteration using highthroughput technologies opens the way to pm in the field of oncology."
"data are stored in a relational database using the entity-attributevalue (eav) pattern. this conceptual modeling provides a data model plasticity required to handle the heterogeneity and the scalability of the variables of interest. therefore, with eav modeling, same concepts managed by different projects (with specific requirements by project) can be stored in a unique database without any modification of the data model. mysql has been chosen as database provider for all web applications of the system. complementary solutions such as nosql databases are currently evaluated for particular requirements (ontologies storage, specific queries, etc.)."
"maintaining an efficient bioinformatics workflow in the context of pm is today challenging because of the frequent updates of the computational solutions either installed on the sequencing machine or provided as standalone applications. these frequent updates are mainly due to the rapid evolution of the sequencing and microarray technologies but remain a major issue to ensure the operability of the bioinformatics pipelines and their reproducibility. as a consequence, any update requires that each bioinformatics pipeline is validated to warrant it provides a very high specificity and sensitivity. indeed, any changes in the data format or in the analysis methods can have critical consequences on the downstream analysis and results. moreover, many different methods are currently available to analyze ngs data but no consensus or standard computational tools exist so far. for instance, detecting germline or somatic mutations can be achieved using different bioinformatics algorithms, tools and filters. choosing the most efficient algorithm is not an easy task and a feasibility phase is mandatory to define which algorithms and parameters to apply for a dedicated question."
"in this study, we leveraged on holistic, evidence-based frameworks and initiatives [cit], to understand how risk models for t2d should be embedded in a cdss and allow better management and coordination between healthcare providers and decision makers."
"the prototype was tested in a real clinical and health care environment for a period of 55 days [cit] . once having been trained about the system functionalities, clinicians were asked to use the tool during their daily activities, for the management of the ward and during follow-up visits. a total of 305 sessions were recorded (being a session defined as the continuous time interval that a user has used the tool, considering as the start event the access to the tool, and as the end event the last interaction with the tool, defined as the latest interaction before a non-continuous use of the application took place). the results show that solution 2.2 (patient management during visits) were more extensively used than solution 2.1 (population managements). the 55% of the accesses to the system were related to solution 2.2. table 3 shows statistics regarding the number of sessions per day, their duration, and number of patients visited during follow up visits at the hospital, assessed through the evaluation of solution 2.2. we collected and studied the number of times users accessed the main menu options and the main modules, and for each module the number of times they accessed specific functionalities. specifically, the implemented final prototype of solution 2.2 is structured in three main screens representing risk factors and their evolution in time, clinical variables time series and drug purchase patterns. the access distribution was uniformly distributed among three screens: 30% for the risk factor screen, 32% for the drug exposure, and 38% for the clinical variables screen."
"if many recent publications point out the key role of the bioinformatics for pm today [cit] for a review), clinical trials usually do not detail the complete bioinformatics environment used in practice to assess the quality and the traceability of the generated data. different software platforms such as transmart [cit], g-doc [cit] or the cbio cancer genomics portal [cit] have been recently developed to promote the data sharing and analysis of genomics data in translational research. [cit] reviewed the different solutions available and compared their functionalities. one of the most interesting features of these platforms relies on their analytical functionalities. they provide ready-to-use tools through user-friendly interface offering interesting functionalities for data queries and user analysis. however, these different solutions do not address essential aspects which are offered by our system: first, often they handle a specific type of data; second they do not cover management and traceability of the data in real-time as long as they are generated by the different stakeholders; third they do not provide clinicians with a meaningful digest of the analyses, which they need to take clinical decisions."
"to perform a realistic evaluation of t2d screening tools, we defined two validation cases, one for healthcare agency settings (vc1) and another, organized in three sub-scenarios, for clinical settings (vc2)."
"precision medicine does not only require an efficient informatics infrastructure at the software level but also at the hardware level. indeed, as ngs is now widely used for tumor profiling, data processing relies on an efficient high performance computing (hpc) infrastructure for data storage, transfer, computation and access control. so far, mainly targeted sequencing on a limited panel of genes (e.g., using ion torrent tm pgm with ampliseq tm ) has been used and can be processed with relatively moderate computing resources. however, as sequencing cost keeps on decreasing, whole-exome or even whole-genome might be used soon thus requiring hpc infrastructure. according to moore's law, kryder's law and butter's law, costs are halved every 18, 12, and 9 months for processor, storage and data transfer, respectively [cit] while 5 [cit] period (source 3 ). thus, the difference between biotechnological and informatics capacities grows exponentially. entering the era of big data in cancer research implies a breakthrough at the informatics level. first, the scalability of the infrastrure (input/ouput performance and computing power) is required to allow the management and analysis of ever-growing data. second, bioinformaticians must be trained to the use of low-level programming languages for parallel computing such as message passing interface (mpi), open multiprocessing (openmp), or mapreduce [cit] and to the algorithm analysis. developing these new skills will be essential in order to improve the efficiency of software used in downstream analysis to deliver results as quick as possible to meet deadline expected in the clinical practice. third, the configuration of job scheduler (such as torque/pbs, oge, or slurm) must ensure that resources could be available and allocated to analyze in priority the data needed for decision-making in clinic. this also implies a redundancy of 3 http://www.genome.gov/sequencingcosts/ the hardware components to ensure their availability. resources and new know-how are definitely needed to handle ngs data and pm. importantly, the question of which data and how long the data must be stored is an important issue. we can anticipate that at some point, the storage capacity will be lower than the amount of data generated meaning that data will have to be analyzed on-the-fly to extract the relevant information and reduce the volume."
"as regards solution 2.1, as shown in fig. 6, only the identity hedonic quality is statistically above 3. in the case of solution 2.2, the pragmatic quality, the stimulation and identity hedonic qualities are statistically above 3, while the attractiveness is around the value of 3."
"most of the problems related to older age are linked to chronic diseases [cit] . healthcare systems are challenging the burden of chronic diseases by putting more emphasis on prevention, and by looking for new ways to reorient the provision of care in the light of the day-byday collected data. a paradigm shift in healthcare delivery is required to meet these needs [cit] . individuals should be followed throughout the whole care process; their self-management role and capabilities must be clearly identified, together with the resources and services delivered by the healthcare system in relation to the stage of the disease."
"user satisfaction levels, evaluated through the attrakdiff questionnaires, were found above the minimum threshold of 3, meaning \"almost very good\" for all the 4 subordinate constructs (pragmatic quality, stimulation and identification, and attractiveness), as shown in fig. 7 ."
"all the previous findings were used to develop our solutions during the implementation phase. we defined 3 ucs for solution 1, being performed at multiple levels (e.g.: primary and secondary care, healthcare agencies, citizens, etc.), while solutions 2.1 and 2.2 have been mapped to 2 ucs, focused on primary and secondary care settings. lean canvas helped us to formalize use cases, beneficiaries and end-users. two rounds of he allowed us to reduce common usability errors and focus on improving usability aspects related to the healthcare process we are introducing with our tools. even with a well-defined development plan, it experts can unconsciously underestimate the importance of providing clear and continuous guidance to the end-user of the tool during each moment of the interaction. the improvement of these aspects prior to the conduction of usability tests allowed us to find further improvements that were more related with their real needs. in the case of solution 1, we could clearly identify and design specific moments that are relevant to the active search for t2d: gathering structured input information, running ad hoc risk models, interpreting their results and take actions according to the predicted scenarios. we could understand that the configuration of the tools can be different for each healthcare setting. for example, a public health agency has less information that is usually provided at population level. models in this case should work also when information is not continuously available, and the most relevant action is the connection with primary and secondary care centres that can continue the active search in their settings. in the case of solution 2, even though we could provide users with suitable filtering options and valuable information, they were still not convinced about the effective introduction of our tool. for this reason, for solution 2.2, we introduced attractive visual analytics (using the traffic lights representation) to catch the attention of end-users and make them focus on specific parameters. for solution 2.1, we understood that we needed more efforts to further simplify, aggregate, and synchronize the information coming from different sources (e.g. drug intake patterns, appearance of complications, diet levels, etc.), and we focused on providing end-users with a way to dynamically formulate clinical questions through the tool."
"in the case of t2d care, the uc resembles what identified in the need phase: u2.1-top-down analysis for decision makers, to be used by managers in primary care or secondary care units as a drill down, business intelligence tool at a population level. uc2.2-decision support for clinicians, in this case, trends and patterns should be mined at individual level, using information available in a primary or secondary care unit and support practitioners. more details on the ucs are provided in the additional file 1."
"we can conclude that mosaic t2d tools for solution 1 have good levels of user experience but low levels of perceived usability; they are introducing a breaking and disruptive routine and therefore more work and efforts in terms of user training, and set up of organizational and procedural measures."
"offering a high quality service is most required in the context of a clinical application. the availability of all kdi's component and the reproducibility of the analyses is thus mandatory. to this www.frontiersin.org aim, we have promoted a set of good practices for the software building process. first, the software development phase follows a strict frame with positive technical constraints, and a common methodology known and shared by each data manager and software developer. the configuration management is delegated to a svn repository where all the source codes of kdi system are regularly committed. the unit testing is strongly recommended for all programming languages involved in the system (x-unit) and part of our continuous integration server based on jenkins software. this system allows to check weekly that all the tests parameterized for all applications are successfully passed. this control ensures that the analysis pipelines provide the expected results, identical to a reference analysis which is considered as a gold standard. second, we pay attention to the availability of kdi system. all web applications are monitored with nagios software in order to be able to detect in real-time any disorder on the system and therefore take immediately all necessary actions (log analysis, server restart, update configuration, etc.) to restore initial and nominal state if needed."
"precision medicine relies on a tight connection between many different stakeholders. as the choice of the therapy is based on a combination of different information levels including clinical data, high-throughput profiles (somatic mutations and dna copy number alterations) and ihc data, all this information related to a given patient needs to be gathered in a seamless information system. data integration is definitely required and bioinformatics plays a central role in setting up this infrastructure. to tackle this challenge, we have developed a seamless information system named kdi (knowledge and data integration) described in figure 3 . the kdi system ensures information sharing, cross-software interoperability, automatic data extraction, and secure data transfer. in the context of the shiva clinical trial, high-throughput and ihc data are sent by the different biotechnological platforms to the bioinformatics platform using standardized procedures for transfer and synchronization. data are then integrated into the kdi system within ad-hoc repositories and databases. metadata describing the data are stored in the kdi core database such as the patient identifier, the type of data (e.g., mutation screening, clinical data, dna copy number profile) and the technology used (e.g., affymetrix microarray, ion torrent tm pgm sequencing). each type of data is then processed by dedicated bioinformatics pipelines in order to extract the relevant biological information such as the list of mutations and the list of amplifications/deletions. therefore, the kdi core database acts as a hub allowing referencing all data through the use of web services. the kdi core database knows exhaustively which data is available for a given patient and where the raw and processed data are physically stored. it thus offers the possibility for clinicians to make queries through a web application and to extract the list of available information for a given patient. in addition, the system is also used to manage and perform automatic integrative analysis required for the therapeutic decision."
"similar results are observed also for the sus scale. the overall sus score is 79.32, corresponding to the grade of b. system learnability was scored as 3 (std 1.2 for solution 2.1 and 0,8 for 2.2)."
"precision medicine requires a strong interdisciplinary collaboration between several stakeholders covering a large continuum of expertise ranging from medical, clinical, biological, translational, technical, and biotechnological know-hows. figure 1 illustrates the different practitioners involved in the complex process, describes the data workflow starting from and coming back to the patient in order to tailor the therapy and shows the informatics and bioinformatics infrastructure supporting the workflow. to build the therapeutic decision, the most exhaustive data ranging from clinical to biological, environmental and family information (e.g., description of the tumor histology, list of previous treatments, family history, etc.) needs to be collected along a complex healthcare pathway. as the disease evolves, new experiments such as high-throughput screens (with microarray or ngs technologies for example) or biomarkers detection by immunohistochemistry (ihc) have to be performed to measure relevant biological information required to choose the best therapy. during the process, physicians (including different specialists such as surgeons, pathologists, radiation and medical oncologists, etc.), biologists, pharmacists, bioinformaticians, computational biologists, biostatisticians, informaticians, biobank managers, biotechnological platform managers, clinical research associates, and the technical staff will offer their expertise for the benefit of the patient. different actors and cultures and a variety of miscellaneous constraints, including meeting the deadlines for results delivery, render the application of pm in daily clinical practice extremely challenging. organizational aspects are therefore essential for the success of pm [cit] mentioned the importance of electronic health record (ehr) and clinical decision support (cds) for care delivery due to the acceleration of knowledge discovery and its impact on the increasing number of possible clinical decisions. development in cds is required to handle the large heterogeneity of data and their complexity. the authors also pinpoint the fact that pm strongly depends on our ability to collect, disseminate and process complex information. indeed, every stakeholder produces information during the healthcare pathway at different time points and in different places. the overall information needs to be gathered, integrated and summarized in a digest report to facilitate the therapeutic decision-making."
"data access is supported by the dao (data access object) pattern. by using hibernatedaosupport superclass provided by spring framework, we promote the standardization of database access for all standard queries (findall, findbyid, save, delete). moreover, hibernate mapping through jpa annotations associated with use of hibernate criteria provides a homogeneous frame for this critical layer. database sessions and transactional aspects are also delegated to spring framework."
"from a technical point of view, the kdi system consists of different modules dedicated to the storage, processing, analysis and visualization of each type of data (clinical, biological, microarray, ngs, etc.). high modularity associated with an efficient interoperability makes our system able to retrieve any relevant information. to facilitate the developments of these modules, we have retained a classical n-tiers architecture implemented with the java/j2ee language. the core of each module of the kdi system can be presented as the association of different layers ( figure 3b )."
"we have developed a seamless information system named kdi that fully supports the essential bioinformatics requirements for pm. the system allows management and analysis of clinical information, classical biological data as well as high-throughput molecular profiles. it can deliver in real-time information to be used by the medical and biological staff for therapeutic decision-making. kdi makes it possible to share information and communicate reports and results across numerous stakeholders, representing a large continuum of expertise from medical, clinical, biological, translational, technical and biotechnological know-hows. the system relies on state-of-the-art informatic technologies allowing cross-software interoperability, automatic data extraction, quality control and secure data transfer. kdi has been successfully used in the framework of the shiva clinical trial for more than 18 months. kdi is also currently used for other clinical trials supported by european union consortia covering cancer (raids -rational molecular assessments and innovative drugs selection in cervival cancer) and non-cancer applications (maars -microbes in allergy and autoimmunity related to the skin). this demonstrates the potentiality and flexibility of our system to support pm covering all its requirements ranging from data management, data traceability, data analysis, query, and visualization. the evolution of sequencing technologies has expanded the frontiers of genomics in both biology and clinical environments. the sequencing field will continue to evolve rapidly, offering lower costs and increased speeds. on-going developments in the sequencing technology, such as an ultrafast sequencer like nanopore technology, will improve performance and miniaturization, thus offering new tools to improve prevention, diagnosis, prognosis, choice of the treatment and follow-up for patients in oncology. to promote pm in daily clinical routine, flexible bioinformatics systems like kdi are definitely required for enabling efficient sharing of information in real-time, and rapid data processing needed for therapeutic decisions. kdi also provides the infrastructure for developing and integrating into the clinical decision process new integrative analysis methods with sophisticated mathematical models, representing the multidimensional nature of cancer to propose new biomarkers and to develop new therapies to fight cancer."
"the last step of the bioinformatics workflow is the production a technical report for the mbb. this task is crucial and must be complete and precise on one hand, and summarized on the other to allow a quick decision of the board. to answer this need, a report is generated for each patient. this report first presents the clinical information of the patient and the overall molecular profiles per gene, with the dna copy number alterations, loh status, and number of mutations (figures 5a,b) . this first section provides the mbb with a rapid overview of all detected alterations. if needed, the mbb can also have access to more detailed results, with graphical views of the copy number profiles for each gene, as well as the list of mutations with detailed annotation as previously described (figures 5c,d) . this name-blinded technical report is sent to the members of the mbb for scientific validation and prioritization of the identified molecular abnormalities."
"among chronic diseases, diabetes represents one of the greatest health threats worldwide, with 425 million people affected. the urbanization and rise of western lifestyle are accelerating the diabetes epidemic. ninety percent of patients with diabetes have type 2 diabetes (t2d). the disease can be asymptomatic, slowly evolving in a first phase but then appearing with complications even before its diagnosis [cit] ."
"we used the roadmap to guide the design of our methodology, organized in three main phases, as shown in fig. 1 : user needs, implementation and evaluation. for each phase, the sequence (arrows) of use of the methods (white rectangles) is represented, together with the main intermediate (green boxes) and final (orange boxes) outputs."
"the proposed holistic approach is aimed at helping it innovators in providing solutions for the multiple end-users, stakeholders and healthcare units that should be involved in the continuum of care, early detection and risk stratification of prevalent chronic diseases such as t2d."
"we have found that prediction models for the onset of t2d can be introduced in the clinical practice by understanding the data that they can work with: these models can be built, trained, and validated on data coming from clinical studies, as the current healthcare systems are not (still) producing reliable and longitudinal information on people at risk of t2d. on the other hand, information on people already diagnosed with t2d does exist and it is produced by healthcare information systems, but mainly for administrative and financial purposes: the challenge is how to pool and aggregate information that can be relevant for clinical practitioners. these findings allowed us to focus on the definition of operative solutions and objectives around these two main activities: solution 1, using predictive models built on top of existing clinical studies on t2d onset. solution 2, using data mining techniques on top of existing longitudinal electronic health records data. the execution of multiple focus groups helped us to gather and consolidate user needs for each solution. in some cases, the information collected was even contradictory: for instance, in solution 1, some end-users wanted the system to focus exclusively on identifying false positives, others on true negatives, others were even scared about a tool that could support in finding new cases of t2d. this highlights the perception that the discovery and management of new cases for t2d could introduce a novel routine (that was referred to as active search) in the existing healthcare system."
"nicolas servant and philippe hup coordinated the bioinformatics developments to support the shiva clinical trial. philippe hup and emmanuel barillot coordinated the development for the seamless information system. julien romjon, philippe la rosa, georges lucotte, stphane liva, alban lermine, virginie bernard, nicolas servant managed the data and developed the bioinformatics pipelines for the shiva clinical trial. virginie bernard and bruno zeitouni developed the mutation pipeline for ion torrent tm pgm. pierre gestraud, philippe hup, georges lucotte, and tatiana popova developed the dna copy number pipeline. pierre gestraud and fanny coffin developed the bioinformatics report for the molecular biology board. philippe hup, grme jules-clment, florent yvon, patrick poullet, stphane liva, alban lermine, stphane liva, stuart pook, georges lucotte, philippe la rosa, camille barette, and julien romjon developed the seamless information system kdi. camille barette, franois prud'homme, jean-gabriel dick managed the informatics infrastructure. christophe le tourneau is heading the phase i program as well as the head and neck clinic at the institut curie (paris, france). christophe le tourneau is the principal investigator of the shiva randomized personalized medicine trial. maud kamal is the scientific coordinator of the shiva trial. nicolas servant, philippe hup, emmanuel barillot, pierre gestraud, maud kamal, christophe le tourneau and julien romjon wrote the article."
"www.frontiersin.org sequencing the genome and beyond available ngs techniques expand from sequencing panels based on a couple of genes to whole-exome and whole-genome sequencing. even if the whole-exome and whole-genome sequencing are currently used in cancer research, and can be seen as the future of the clinical investigation, their use in routine clinical practice is much more difficult, mainly because the average depth of coverage is much lower than for targeted genes sequencing complicating mutations detection. however, these applications offer new ways to explore dna copy number and structural variations and can thus be used as an alternative to the current microarray technologies. in addition, the current sequencing capabilities also offer new opportunities to develop gene/transcript expression and epigenomics biomarkers in clinic. for instance, the detection of brca1/brca2 isoforms and their quantification using rna-seq approach would be an interesting complementary approach to mutations screening. in the same way, dna methylation, histone modifications, small non-coding regulatory rnas, or nucleosome remodeling regulate many biological processes involved in tumorigenesis. more recently, evidence that genetic and epigenetic mechanisms are related events in cancer has emerged. alteration in epigenetic mechanisms can lead to somatic mutations, as well as somatic mutations in epigenetic regulators can lead to an altered epigenome [cit] . if drug discovery in cancer epigenetics had been held back due to concern about specificity and toxicity, it remains an active field of investigation [cit], for a review). the application of these new fields in clinic raises the question of combined therapies. combination of targeted therapy with chemotherapy or with other targeted therapies is challenging because of increased toxicity. solutions include the use of lower doses of drugs which might not be relevant if the biologically active dose is not reached and the use of drugs in a sequential manner although the relevance of this approach still needs to be demonstrated. for instance, it is likely that the combination of standard chemotherapy together with drugs against mutated proteins and epigenetics drugs offer synergetic benefits and increase therapeutic efficacy. integrative analysis considering the multidimensional nature of the cancer (genome, proteome, epigenome, kinome, etc.) is therefore a major challenge to unravel the complexity of the disease and identify the most efficient treatments. to this aim, we will have to capitalize on large collection of public datasets such as data from the cancer genome atlas (tcga 8, [cit] ) or international cancer genome consortium (icgc 9 ) and also pathway databases for gene regulatory network, signaling pathway, metabolic pathway, protein-protein interaction network and protein-compound network (e.g., dip, hprd, kegg, reactome to name only a few). the tcga has initiated a pan-cancer analysis project [cit] on the first 12 tumor types profiled by the consortium where the goal is to characterize molecular alterations and their functional impact across tumor type in order to promote the development of new therapies to fight cancer."
"the vision, objectives and high-level solutions were discussed with end-users and stakeholders, to derive user needs, values and expectations. two clinical focus groups with healthcare practitioners from the madrid regional healthcare service, spain, and from the private and public healthcare system of the pavia province, italy, were carried out to discuss solution 1 and solution 2 respectively. a third focus group was carried out with healthcare professionals from the valencia regional healthcare service, spain, to confirm the results of the previous ones. additionally, two scientific focus groups were held, as well as a business focus group oriented to explore the market potential. the full results are detailed in the additional file 1."
"in our work we have used a web based online system, the ahp-os [cit] . the questionnaires have been included as additional files 2, 3 and 4 to this manuscript."
"the main objective of this phase was to analyse the performance of the system in terms of uptake and potential impact. to assess the behaviours of end-users when interacting with the prototype, the following methods were adopted to analyse user interactions, needs and usability aspects:"
"regarding solution 1, experts agreed about the importance to have tools that allow to exclude those persons that are not at risk of developing the disease (i.e. false positives) and therefore reduce the unnecessary activities. the cdss should be linked with existing registries and allow definition of lifestyle interventions to mitigate the detected risks. several participants expressed concerns about the increased workload and additional costs caused by such tools. regarding solution 2, pattern recognition and trends should be used to give an overview about what is happening in a health unit, to improve the efficiency of the provided services and to identify gaps between clinical guidelines and real practice. from a scientific perspective, reluctance to use information from subjective assessments to build the models (e.g. patient diaries) and about fixed time for predictions was clearly expressed. using medications patterns and continuous glucose monitoring as proxy for complications was considered as innovative and promising approach."
"figures 5 and 6 summarize the results of the attrakdiff for solution 1 and 2: our quality criterion consisted in having the confidence interval of the mean value not touching the scale's middle score of 3. the results for solution 1 are above the minimum threshold, meaning that they are \"not frustrating\" end-users. the overall sus score is about 61, corresponding to the d grade. the main issues that were pointed out regarded the need of a technical support to be able to use the system, the lack of confidence in using it, and the necessity to learn too many things before being able to use the system. considering that a score above 80.3 is needed in order to have highly usable products, the usability of the mosaic tools for solution 1 has been perceived as low by primary care and healthcare agency users."
"in the case of t2d screening, three main ucs were defined: uc1.1-risk factors and indicators to be adapted to public health, to be used by local, national and regional healthcare agencies, which usually have demographic information available to perform population screening. uc1.2-risk factors and indicators to be adapted into ambulatory settings, to be used at individual level in primary care by gps. uc1.3-risk factors and indicators to be adapted to citizens for personal use, in this case to be used as a self-screening tool promoted by healthcare systems, professional societies, or patient's organizations."
"perceived usability was 55.36: this value according to sauro&lewis can be interpreted as a grade of a d. the main issues pointed out in this case regarded the need of a technical person to be able to use the system, the perception of the system as cumbersome and the necessity to learn too many things before using the system."
"several useful comments were also made regarding the functionalities of the system. users were interested to (i) refine tuning features for customized filtering and temporal analyses functionalities, (ii) personalize the display of the results and (iii) add further information and statistics to answer specific research questions on the population."
"results of the feasibility part of the project, focused on the first 100 patients were recently published [cit] . among the first 100 patients, diagnostic confirmation and www.frontiersin.org ihc analyses for hormone receptors were performed in 92% of the patients. genomic analyses were performed for 65 patients (68%). dna copy number analyses met quality criteria in all the 65 patients, while a technical problem occurred in 2 patients for mutations analyses. overall, 58 out of the 95 patients (61%) had a complete molecular profile. all patient data were integrated in the kdi system. the median timeframe for the bioinformatics analysis (dna copy number, mutation profiles and mbb report) was 5 days. median timeframe from tumor biopsy/resection to mbb was 26 days [range: 14-42]. to date, eight french cancer centers are participating to the shiva trial, and more than 700 patients were included. all data provided by the different centers are centralized at institut curie using the kdi system, and analyzed in routine."
"the availability of high-throughput technologies dedicated to clinical applications makes it very attractive for cancer centers to use these new tools on a daily basis. however, establishing such a clinical facility is not a trivial task due to the aforementioned complexity of pm framework along with the overwhelming amount of data. indeed, the field of oncology has entered the so-called big data era as the particle physics did several years ago. from the big data 4 v's perspective, data integration issue (i.e., merging heterogeneous data in a seamless information system) in oncology can be formulated as follows: a large volume of patients' data is disseminated across a large variety of databases which increase in size at a huge velocity. in order to extract most of the hidden value from these data we must face challenges at: (i) the technical level to develop a powerful computational architecture (software / hardware), (ii) the organizational and management levels to define the procedures to collect data with highest confidence, quality and traceability, and (iii) the scientific level to create sophisticated mathematical models to predict the evolution of the disease and risks to the patient. obviously, an efficient informatics and bioinformatics architecture is definitely needed to support pm in order to record, manage and analyze all the information collected. the architecture must also permit the query and the easy retrieval of any data that might be useful for therapeutic decision in realtime thus allowing clinicians to propose the tailored therapy to the patient in the shortest delay. therefore, bioinformatics is among the most important bottlenecks towards the routine www.frontiersin.org application of pm and several challenges need to be faced to make it a reality [cit] . first, the development of a seamless information system allowing data integration, data traceability, and knowledge sharing across the different stakeholders is mandatory. second, bioinformatics pipelines need to be developed in order to provide relevant biological information from the high-throughput molecular profiles of the patient. third, the architecture must warrant the reproducibility of the results."
"while there is a clinical consensus on how to manage the disease through drug treatment, screening, self-management, and behavioral change, current challenges involve novel patients' stratification strategies and effective case management, both at population and individual levels. clinical, social, and political coordinated actions, in primary and secondary care, are required to prevent or delay t2d onset and complications. risk prediction models could effectively contribute to support such healthcare interventions and decision-making processes [cit] . however, despite the large number of models being developed and the increased interest in the clinical field, only few ends up being used in the clinical practice. furthermore, there is a lack of consensus and examples regarding how to properly embed them into computerized clinical decision support systems (cdss) addressing proactive search, risk stratification and case management [cit] ."
"in this work we have presented a holistic approach to design, implement and evaluate it solutions to tackle the epidemics of t2d. this approach, applied in the context of the mosaic project, was based on the cehres roadmap, which has been further elaborated to precisely define the sequence and combination of actions and methods to be executed at each step. this mixed method approach allowed to acquire all the necessary information during a short period of time, thus optimally supporting the development process. similarly to relevant literature in cognitive informatics [cit], we have focused on diverse stakeholders to catch different perspectives, relied on multiple methods to make them more reliable and discovered elements, properties and perspectives which are more difficult to extract with traditional methods: if, from the functional and technical side, the requirements can be gathered through traditional software engineering techniques (like use cases development), other techniques coming from human and social sciences were adopted to meet the needs of end-users. it is important to perform usability tests before the final validation, in order not to compromise its success. if the prototype turned out to be not usable enough for many users, the whole evaluation would be jeopardized. the process that led to the development and implementation of the system prototype involved end-users and stakeholders in the whole analysis through multiple and small tests as recommended by nielsen [cit] ."
"the term precision medicine (pm) is also frequently encountered in the literature to denote similar ideas, and generally refers to delivering the right drug at the right time to the right patient, by targeting specifically the molecular events that are responsible for the disease. we will use in this article the terminology pm defined as a customization of healthcare that takes into account individual differences among patients from prevention, diagnosis, prognosis, choice of the treatment and follow-up. pm combines the knowledge of the patient's characteristics with traditional medical records and environmental information to optimize health. pm does not only rely on genomic medicine but also integrates any other relevant information such as non-genomic biological data, clinical data, environmental parameters and the patient's lifestyle."
"the use of high-throughput technology in a clinical context also offers new challenges in the development of cutting edge statistical methods and algorithms dedicated to the field. as an example, the integration of heterogeneous molecular profiles provided by microarrays and sequencing assays could be used to define a patient genotype signature, to improve molecular profile accuracy and to ensure that the generated data come from the same biological samples and patient. the intersection of genotype variations available through the snps arrays technology could thus be intersected with the genotype information extracted from next-generation sequencing. however, this type of quality control requires the sequencing of a large dna region to ensure that a sufficient number of polymorphism is covered. in the same way, the biopsy cellularity can also be estimated using both microarrays and sequencing assays [cit] in order to correlate the tumor purity from both profiles and detect intra-tumor heterogeneity."
"the usefulness of the tool to inspect specific clinical questions. participants highlighted the importance to identify sub cohorts of patients who experienced a specific acute event to inspect possible noncompliance to guidelines. the possibility to analyze disease progression through longitudinal data representation as a function of hospitalizations and disease uptake. they were able to cluster specific groups of subjects while assessing disease complexity, they found this functionality crucial to understand the treated population."
"the solution we have developed to manage the shiva clinical trial provides a first step towards the routine application for pm. however, many challenges still need to be tackled and will require a lot of mutualization and harmonization efforts within the scientific community. the main on-going challenges are listed in what follows."
"to conclude, the usability and learnability scores for solution 2 are quite good. however, it was not possible to make a 3rd round of developments, following the usability recommendations provided after the usability tests and this could explain the slightly reduced score from the tests (where an 81.5 score was obtained) to the final evaluations (79.3)."
"in the case of solution 2, one of the most interesting results is related to the interest of end-users in the possibility of using solution 2.1 functionalities to answer specific clinical questions, related either to specific conditions (e.g. myocardial infarction, poly-medicated patients) or to the evolution of the disease in terms of complexity, which might be relevant to evaluate the specific features of the care center. starting from a selected population, it is possible drilling-down to more detailed information up to individual cases, using solution 2.2."
"presentation layer is based on jsf (java server faces) which is a component oriented framework for building user interfaces for web applications. to enrich the basic component set provided by jsf, we use additional component libraries such as apache trinidad and primefaces. by this systematic approach for each user interface, we aim to build a visual identity, ergonomic, easily usable, for the whole information system. all data available within kdi can be browsed and retrieved from a user-friendly bioinformatics web portal."
"to perform our study we leveraged on a holistic and evidence-based framework, the cehres roadmap [cit], built on a participatory development approach, persuasive design techniques and business modelling [cit] and be used by the project management team (i.e.: the team involved in the design, implementation and evaluation of the technology) as an instrument through which stakeholders can debate to clarify areas that \"would otherwise remain unanswered, unclear, or unknown\", and in which \"technology is not considered as a tool or end in itself, but as a catalyst for innovation\" [cit] . the roadmap consists of five sequential and one iterative retrofitting phases (contextual inquiry, value specification, design, operationalization, summative evaluation, and formative evaluation); for each phase, a list of main research questions, tasks and methods is provided."
"even if end-users expressed the need of more personalization and filtering options, in both cases the usability and the user experience are all above the acceptable thresholds."
"in both solutions the user experience is \"very good\", as it is above 3 and almost above 3.5 for all the dimensions, about 0.5 better than the previous evaluations, as shown in fig. 8 ."
"adopting human-centred design techniques can help to maximize usability, identify design goals, understand unmet needs and unsolved problems [cit], and is central for work domain analysis, design and evaluation of health information system [cit], as it allows understanding the cognitive work performed by practitioners in different contexts, identifying usability problems and reducing errors through task analysis, focus groups and semi-structured interviews [cit] ."
"clinical trials for pm rely so far on a very limited number of biomarkers used for the therapeutic decision [cit] for a review). typically from one up to less than 50 biomarkers are used for pm in currently on-going clinical trials worldwide. moreover, the decision is based on a univariate decision rule meaning that a possible interaction between biomarkers is not considered which certainly explains part of the limited efficacy of targeted therapies even in the presence of their targets. [cit] showed that vemurafenib is highly effective in the treatment of melanoma in patients with braf(v600e) mutation while colon cancer patients harboring the same braf(v600e) mutation have a very limited response to this drug. they found that braf normally exerts a negative feedback regulation of egfr. therefore braf inhibition causes a rapid feedback activation of egfr, which enhances cell proliferation. as melanoma cells express low levels of egfr they are not subject to this feedback activation in contrast to colon cancer. thus, they propose that these patients might benefit from combined therapy consisting of braf and egfr inhibitors. this example highlights the fact that considering interactions between biomarkers and combining different therapies together can dramatically strengthen the efficiency of pm. also it clearly shows that elucidating the reasons behind treatment escape and proposing backup therapeutic strategies would benefit greatly from the knowledge and modeling of the cell regulatory network rewiring. therefore, computational systems biology approaches, based on mathematical models of the cell regulatory network rewiring, are definitely needed to deepen our understanding of the cancer cell and to improve current decision rules. systems biology and systems medicine are two disciplines which open the road to pm. machine learning techniques will also be very useful to develop prediction rules to predict outcome and response to treatment. we can imagine that online machine learning techniques could be used to refine and optimize decision rules as long as new data and knowledge are generated. the key defining characteristic of online learning is that soon after the prediction is made, the true label of the instance is discovered. this information can then be used to refine the prediction hypothesis used by the algorithm. in the case of cancer, every day, for several patients, information is collected: survival, response to therapy, molecular profiles, pathological complete response, etc. this information could be used to retrain the classifier on the available data. in addition to these data-driven approaches, knowledge-based approaches must be developed to capitalize on the large amount of knowledge that is present in the scientific and medical literature to build efficient decision rules. ibm has developed a supercomputer named watson (the name of ibm's founder) able to understand question in natural language and to extract relevant information from the literature. watson supercomputer is currently used at the memorial sloan-kettering (new-york, usa) to help for diagnosis in lung cancer."
"the work has been performed in the context of the mosaic (models and simulation techniques for discovering diabetes influence factors) project, funded by the european commission. the consortium was composed of european institutions and enterprises combining different expertise, such as medicine, epidemiology, biomedical engineering, medical informatics, and medical technology. the goal of the project was to develop new computer models [cit], and implement them in tools to support the detection and prediction of t2d onset and related complications, in different healthcare settings (e.g. hospitals, clinical centres and health agencies). we integrated computer models into dsss for t2d management, detection and prevention and studied how end-users and stakeholders interacted with the system. that is why, in the method section, we inform on how different methodologies and techniques have been selected and combined to gather perspectives from prospective stakeholders and end-users, and to define of contexts and scenarios of use. our findings are presented in terms of user needs, implementation and evaluation aspects, and then discussed with a special attention on the cognitive and behavioural domains."
"we first used the running lean canavas, a business oriented methodology to study new ideas and build successful products [cit], to understand and define from a clinical, economical and customer perspective how validated predictive models for t2d might impact current clinical practices. the most relevant output of this method is a formal definition of the problem, objectives, and solutions. then, we carried out thematic focus groups to determine and describe the key stakeholders, as well as to identify and understand the healthcare improvements and to specify needs and requirements to be covered [cit] from the clinical, scientific and socioeconomic perspective. to further study user needs and transform them into a structured set of specified requirements, the analytic hierarchic process (ahp) was used. ahp is a multi-dimensional, multi-level, and multifactorial decision-making methodology that provides a framework for structuring a decision problem, representing and quantifying its elements, and relating those elements to the overall goals [cit] . this method has been already used in healthcare for user needs elicitation and evaluation framework. users of the ahp start decomposing decision problems into a hierarchy to easily include sub-problems. two experts for each subproblem are needed. once the hierarchy is built, experts evaluate its elements by the means of pairwise comparisons. once evaluations are done, experts discuss and comment the results generated, to gather insights and transform them into requirements and guidance for the development stage."
"from a methodological perspective, the importance of combining human centred design with system engineering and design life cycles have demonstrated to increase reliability, compliance and safety of health information systems [cit] . however, a great number of health information systems are not designed following human centred design guidelines, resulting in low satisfaction, acceptance and abandonment. these systems could be improved by combining different disciplines, ranging from computer to social and behavioural science [cit] . ethnographic techniques can be useful to understand that barriers for effective use of cdss are almost non-technological [cit], analysing stakeholder groups allows to understand that a cdss may be misused or underused if their needs, knowledge and priorities are not taken into account [cit], is more and more relevant in the decision making for the purchase of medical devices and for health technology assessment [cit], and can have a negative or disruptive impact in practitioners routines [cit] ."
"all the ucs were developed and tested with human computer interaction experts and end-users. in the case of uc1.3, it was decided to wait for final and full validation of the predictive models before starting to implement it, as it directly involves patients, and it is not included in this work."
"within solution 2, we faced a different situation: while guidelines and recommended actions on how to manage t2d are available, the challenge is to deliver reliable and meaningful information to daily practitioners and managers. during the first phase, we realized that the offer related to solution 2 should be split into two sub-solutions, the first giving support at a population level (solution 2.1) and the second at an individual level (solution 2.2). once the problem was de-structured and analysed as different sub-problems, we used ahp to recompose the puzzle and have a consolidated view of user needs, objectives and problem. we ranked priorities for all the solutions, and we found that in some cases solution 1 and solution 2.1 had similar priorities, since they both deal with support on risk-stratification decisions. solution 1 and solution 2.2 have in common the importance of not introducing a negative impact on the daily activities of health care professionals. the user satisfaction dimension is extremely important: in the first case because we are introducing an \"extra\" activity (active search of new t2d patients) within the clinical workflow, in the second because we should reduce the burden of gps and specialists in following up a multitude of t2d patients. furthermore, we noticed that hc managers give more importance to have tools available every time and everywhere, while professionals involved in t2d management and screening do prefer an easy to use solution."
"to our knowledge this is one of the first works focused on understanding user needs, behaviours and expectations and corresponding requirements for cdss built on top of predictive models and data mining techniques for chronic diseases. for this reason, we have reported results in the three main phases, user needs, implementation and evaluation."
"though physicians have always considered the individual characteristics of each of their patients, the term personalized medicine appeared recently to account for our new abilities to characterize each person biologically with genomic analysis, and to use this information to guide medical decision-making and deliver the best treatment to each patient. this concept is also referred to as genomic medicine, and other terms such as stratified medicine or targeted medicine are sometimes used interchangeably. a few years ago, the concept of p4 medicine was introduced with the idea of managing the patient's health instead of the patient's disease [cit] . as a matter of fact, the practice of medicine today is mainly reactive, i.e., the physician treats the patient's disease and little is done to prevent the occurrence of the disease. the p4 medicine considers a model of healthcare that is predictive (considering the genetic background of the individual and his/her environment), preventive (adapting lifestyle, taking prophylactic drugs), personalized (tailoring the treatment to the individual's unique features, such as the patient's genetic background, the tumor's genetic and epigenetic landscape, his/her life environment) and participatory (many options about healthcare www.frontiersin.org which require in-depth exchanges between the individual and his/her physician). p4 medicine therefore extends the concept of personalized medicine."
"more specifically, in the first place, we apply a multioutput dnn with 4 common layers and 2-layer private branch for two source task classifications. after 200 training epochs, the accuracies of the two tasks on the test dataset were supposed to be stable enough. then, we added the private branch of the target task and adopted three types of methods mentioned in section 3 respectively. as a standard control, we chose a simple neural network with the same number of architecture, i.e. the same number of layers and neurons, and applied the same learning rate. table 4 illustrates the results of three approaches and the standard control experiment."
"where the  c denotes the parameters in the common layers. it is shown in fig. 1 that a multi-output dnn contains common layers and several branches of private layers. in the structure of the neural network, all the common layers are"
"the structure of the multi-output dnn brings remarkable benefits to eliminating the time and memory cost. still, there're more to explore. in many cases, the demands in a field are not always constant. in the traffic classification, there are often several tasks to discuss. we've applied three types of classical questions in the previous study. but it is unknown that whether the multi-output dnn would be able to handle some new demands. due to the limit of the dataset, it is hard to set new task labels. so, we adopted the 2-phase strategy: firstly, we trained two tasks as source tasks, and then, the third task was raised as target task and be added into the established neural network. we hence evaluate the effectiveness and the accuracy of the target task."
we selected several real-time network traffic traces dataset to evaluate our algorithms. some of the important features of the traces are listed in table 1 .
"treebanks constitute a crucial resource for theoretical linguistic investigations as well as for nlp applications. thus, in the past decades, there has been increasing interest in their construction and both theory-neutral and theory-grounded treebanks have been developed for a great variety of languages. descriptions of available annotated corpora can be found in abeill (2003) and in the proceedings from the annual editions of the international workshop on treebanks and linguistic theories."
"as for the analysis of performance of the knowledge space representing the traffics, the euclidean distance is adopted in later experiments. considering in a multi-dimensional mathematical space, using d ij to denote the normalized distances to the geometrical center of label j for instances with label i, the perplexity is considered to be"
"this article presents research that attempts to increase the degree of automation in the annotation process when constructing a large treebank for spanish (the iula spanish lsp treebank) in the framework of the european project metanet4u (enhancing the european linguistic infrastructure, ga 270893ga). 2 the treebank was developed using the following bootstrapping approach, details of which are presented in sections 3 and 4:"
"obviously, using fully automatic parsing would have been the best solution for speed and consistency, but no statistical parsers for spanish are good enough yet, and when using symbolic parsers, there is no way to separate good parses from incorrect ones. the ensemble method we propose is a way of avoiding monitoring automatic parsing; the error is more than acceptable and recall is expected to be augmented by re-training and the refinement of the different parses."
"as always the case with symbolic grammars, the srg produces several hundreds of analyses for a sentence. the delph-in framework, however, provides a maxentbased ranker that sorts the parses produced by the grammar. although this stochastic ranker cannot be used to select automatically the correct parse without introducing a considerable number of errors (as we will show, it only achieves accuracy of about 61%), it nevertheless allows the annotator to reduce the forest to the n-best trees, typically the 500 top readings. the statistics that form the model of the maxent ranker are gathered from disambiguated parses and can be updated as the number of annotated sentences increases."
"t denotes the set of target tasks. in other words, the common layers should be trained and parameters in both the source tasks and the target tasks need to be optimized using backpropagation. the eq. 7 can also be rewritten similar to eq. 5 by eq. 3. we would refer to this scheme using the abbreviation sct (to train source, common and target layers) in later sections. one way to simplify the above algorithm is to subtract the calculation of previous trained tasks:"
"a common way to illustrate a classifier's oa (overall accuracy) is through the well-known metrics of tp, tn, fp and fn (t and f for true or false, and p and n for volume 7, 2019 positive or negative respectively)."
"in the real-world implementations, adopting techniques like the dnn are quite hard due to the great computation complexity and time cost of the neural networks. but our proposed structure might be a solution. it is supposed to be not only suitable for the centralized data center controllers which are equipped with huge volumes of computational power but be also a potential solution in many scenarios that devices have weak computing power. on the one hand, in the centralized data center, our proposed algorithm is supposed to handle several classification tasks at the same time with one calculation unit and much less cost on either the time or the memory. moreover, a new demand can be tested without influencing the established business and share the common knowledge extracted from a great amount of previous related data. on the other hand, for the weak end devices, using our hierarchical structure, the common learning layers can be placed remotely in the cloud or in some specialized neural network computation units. and then, only a simple cost of calculation for several neurons can the end devices complete the complex and new tasks by connecting to the common layers."
"the less the perplexity is, the better the knowledge space represents the traffics. moreover, we split each the dataset randomly and then adopted the hold-out validation."
"we go through the result spaces for all neurons and select the three of them as delegation. it is illustrated in 3-dimensional views labeling with three tasks as fig. 4 . the fig. 4a shows that the values of different labels are properly represented and clustered separately after some training epochs. as a contrast, fig. 4b demonstrates the knowledge space in a single-task training scenario, where the dnn can extract the knowledge labeling with application types. however, it behaves barely satisfactorily in the other two tasks."
"the platform used for this experiment is: windows 10 operating system running on a laptop equipped with an intel 2.4ghz i5-6300u cpu, 8gb ram, and a custom variant of nvidia geforce gpu with 1gb memory. most of the algorithm implementations in later experiments are completed with the skikit-learn tools [cit] ."
"results for different thresholds (both for the baseline and tree entropy) are shown in table 3 (top). as we can see, setting a high threshold for the baseline, we can select a small subset of 20% of the sentences with precision similar to that achieved by our parse ensemble approach. to select 31% of the sentences (i.e., about the same proportion we obtained with the ensemble approach) we need to set a threshold of 4.5, obtaining a precision of 84%, which is lower than the 90% obtained with the ensemble method."
"where l i refers to the loss function of i th private branch and w i is the weight of the branch. then, the corresponding"
"as the promotion of the hardware capacity and the growing demands in communications, the simple systems of the traditional internet network are gradually replaced with some new management systems, e.g. the sdn (software-defined network) [cit] . a rising computation is implemented in the network. hence, the network traffic classification has become one of the key points to the efficient network traffic management and qos (quality of service) guarantees [cit] ."
"hence, under the condition of eq. 6, a new task in other words, a training set with plenty data is adopted for the training of source tasks in the first place. after the convergence of them, a relative small data set is adpoted for the training of target task. also, three training schemes the same as the one in the last section are proposed."
"we firstly experiment to check if our proposed algorithm is feasible to the multi-task traffic classification. three tasks -duration, flow rate, and flow application classification -are discussed in the following discussion. we implemented five traditional machine learning techniques and three recently proposed techiniques: the knn (k-nearest neighbor), the svm (support vector machine), decision trees, linear regression, the random forest, deep neural networks, maxent [cit], ensenble learning [cit] . also, we normalize the input data before training. we constructed three different single dnns respectively, each with 4 layers and 30 neurons for each layer."
"the grammar applied in parsing is a broad-coverage, open-source spanish grammar implemented in the linguistic knowledge builder (lkb) system [cit], the spanish resource grammar (srg) [cit] )."
"after this introduction, section 2 presents an overview of related work on automatic parse selection, section 3 summarizes the set-up, section 4 presents our experiments and results and, finally, section 5 concludes."
there has been a lot of approaches to the traffic classification. the machine learning tools have been the most promising techniques and they were well researched over the past several decades. there has been a large number of researchers that focus on predicting different flow features. and the studies on the traffic classification were primarily focused several perspectives.
"as far as the dnn is concerned, it keeps very stable performances on all three tasks of different traces. besides, the dnn presents either the best or the second-best results in all the experiments. as the second player on the classification of duration for trace i, the difference between the dnn and the best one is no more than 0.2 percent. therefore, the dnn advances in traffic classifications compared with other machine learning techniques."
"the performance of our parser ensemble approach was measured through precision and recall on the task of selecting those sentences for which the first tree proposed by the maxent model was the correct one. table 2 shows the confusion matrix resulting from the experiment. the row predicted ok counts the number of sentences selected by our ensemble method (malt and maxent delivered parses are identical), and the row predicted nok contains the number of sentences not selected because the parsers disagreed. columns gold present the manual evaluation of a maxent model first ranked parse. from this table, we can compute precision and recall of our sentence selector: 445 sentences were selected out of the 1,428 sentences in the test set (31.2%). precision (number of correctly selected sentences among all the selected sentences) stood at 90.6% (403/445), and recall (number of correctly selected sentences among all the actually correctly ranked first sentences) was 46.6% (403/864)."
the detection of potential elephant flows would avoid some potential network congestions [cit] . and many works focused on the precise classification of multimedia traffic or p2p (peer-to-peer) traffic [cit] accounting for their potential large proportion of all the traffic. the discrimination of specific application layer information of the flows is expected to provide better qos based on different demands of the applications. and the early detection of malicious network traffic is believed to provide an appealing improvement to the network safety [cit] .
"the  * denotes the parameters of final fitting function. as the deviation i is beyond control, we ignore it here and later this section in the equations for simplicity. and the  c and  i refers to the parameters in the common layers and i th to the i th branch of private layers respectively. the training process of multi-output dnn is then slightly different to the simple dnn. a direct method to the eq. 3 is the rrbp (round-robin backpropagation), which is illustrated as algorithm 1. in rrbp, the loss function of each branch is calculated respectively, and for every batch of inputs, each task complete training its private part and the common part in turn. and the other is ppscbp (parallelprivate and sum-common backpropagation). in contrast, ppscbp defines a general loss function"
"as for the multi-output dnn, from the last two columns in table 2, it can be concluded that the accuracies of both 2 training schemes are approximately equal to that of the single structure. the ppscbp has a slightly better accuracy than rrbp in most cases and ties the single dnn generally. the differences are mostly within the limit of the biases of datasets. therefore, the multi-output dnn should share the same behavior on traffic classification as the single dnn. besides, its performance stabilization on different tasks and traces shows the potential ability to handle multi-class problems and thus meets some of our proposals properly. now considering the time cost shown in fig. 3, the training time for variant tasks of different datasets are varying a lot. generally, the rrbp costs a little more than 3 times as the average standard of simple dnns on all three tasks in trace i and trace ii but advances them in entry09 and entry10. ppscbp costs 1 to 1.5 much time as the average one of single structures. considering that the multi-output dnn outputs three results together, we would sum up the time cost of single dnn over three tasks. hence, the rrbp training scheme shares approximately equal time cost with the sum of single structures. on the other hand, ppscbp has advantaged than the simple structure in the convergence speed. moreover, specifically in this experiment, the multi-output dnn with ppscbp can be more than 2 to 3 times faster while saving less neurons' space. and predictably, the more tasks the dnn trains, our proposal structure with ppscbp training scheme would save more time and memory. in later experiments, we would choose the ppscbp to train the other multi-output dnns in later experiments."
"we compared the results of our ensemble method with two parse selection methods based on: (i) a simple probability-based threshold (baseline) and (ii) [cit] . the baseline consisted of selecting sentences for which the ratio between the probabilities of the two highest ranked analyses delivered by the maxent model was over a given threshold. the idea was that a very high ratio would indicate that the parse ranked first had a large advantage over the others, whereas if the ratio was close to 1, both the first and the second analyses would have similar probabilities, indicating lower confidence of the model in the decision. tree entropy takes into account not just the two highest ranked analyses, but all trees proposed by the parser for that sentence. the rationale is that high entropy indicates a scattered probability distribution among possible trees (and thus less certainty of the model in the prediction), whereas low entropy should indicate that one tree (or a few) gets most of the probability mass."
"r fourth, we provided a fully automated chain based on an ensemble method that compared the parse delivered by the dependency parser and the one delivered by the maxent ranker, and then accepted the automatically proposed analysis, but only if both were identical."
"it is noted that the transfer learning has many branches of studies depending on the availability of datasets for source and target tasks. most of the cases are well explored. however, the multi-task classification was rarely noticed for traffic classification problem. besides, some studies have focused on the one-shot learning, where the labeled training set in the target task is quite small. the key point of the one-shot learning is that one can take advantage of knowledge from previously trained data. [cit] mentioned this word at the first time."
"in some cases, a new demand, e.g. a new label classification task, is proposed, but it might be costly to train a new model for solving the problem. by the conventional machine learning methods, there might be faced with difficulties collecting the sufficient data for the target task while abundant data for related tasks were available. we may find it practical to train the new classifier using the data from related tasks or using other tasks' previously trained layers to speed up the training process. such a new task is called the target task d t and the previous trained tasks are referred to the source task d s ."
"we have described research that aims to increase the degree of automation when building annotated corpora. we propose a parser ensemble approach based on full agreement between a maxent model and a dependency parser to select correct linguistic analyses output by an hpsg grammar. this enables a hybrid annotation methodology that combines fully automatic annotation and manual parse selection, which makes the annotation task more efficient while maintaining high accuracy and the high degree of consistency necessary for a useful treebank. our approach is grammar-independent and can be used by any delph-in-style treebank. in the future, we plan to investigate the impact of automatic treebank enlargement on the performance of statistical parsers."
"to evaluate the time cost, we record the training time for each training epoch. as concerned as the dnn, the criterion for the convergence of a training process is thought to be that there's no more than 0.5 percent of increment of the accuracy in 100 later training epochs over test dataset. then, the accuracy and the time cost are recorded and considered as final results. for a multi-output dnn, the time cost is set to equal to the one task which spends the most time to converge."
"in the implementation of oct (to train only common and target layers), one of the ways is to connect the target tasks directly to the original common layers. but this behavior might cause some negative effects to the source tasks."
"finally, the ot takes the least time to converge in most cases. though its achieved accuracies are not as good as the other 2 training schemes and the single structure, this scheme is equipped some special feature that allows testing a new classification without influencing current business. it would be functional in the real industrial environments."
"for past several decades, many theories were proposed to better predict the diverse requirements for bandwidth, latency, and quality of different flows. it is widely recognized that many properties of a flow can be well predicted with a small set of the flow features. among all the approaches, some machine learning tools have progressed promising results using the port number or some statistical features of the the associate editor coordinating the review of this manuscript and approving it for publication was zhanyu ma. flows [cit] . recently, some studies have turned to the deep learning techniques and pleasing scores of accuracy have been obtained [cit] . however, in many studies, different flow classification tasks were usually regarded as different areas and most of the studies were focusing on one specific area."
"then, we would discuss the common knowledge extraction with the multi-output dnn. after training the three tasks using multi-output dnn, the values of the last common layer in the multi-output dnn are recorded. similarly, we record the values of the second-last layer and the corresponding labels for simple dnns as well (the values of dnn is not the de facto common knowledge but just is considered as control groups). table 3 demonstrates the perplexities of the different tasks represented by the common knowledge. it can be read that the perplexities of common knowledge extracted by multioutput dnn are the least in all three tasks. then, each of the single dnn has the second-least perplexity for the corresponding training task but shares relatively higher ones for the other tasks. in addition, it can be drawn that the tasks of duration and flow rate are more similar while that of application is some more different. it is shown that our proposed algorithm is able to extract the common knowledge of the network traffics properly. in this experiment, the common knowledge represents the flows even better than the corresponding knowledge space of single task training."
"in order to compare the first-best trees selected by the maxent selection model and the outputs of the dependency parser, we convert the derivation trees to a dependency format, also illustrated in figure 1 . in this target annotation, lexical elements are linked by asymmetrical dependency relations in which one of the elements is considered the head of the relation and the other one is its dependant. the conversion is a fully automatic and unambiguous process that produces the dependency structure in the conll format [cit] . a deterministic conversion algorithm makes use of the identifiers of the phrase structure rules mentioned previously, in order to identify the heads, dependants, and some dependency types that are directly transferred onto the dependency structure (e.g., subject, specifier, and modifier). the identifiers of the lexical entries, which include the syntactic category of the subcategorized elements, enable the identification of the argument-related dependency functions. 5"
"to address above issues, we compared many machine learning tools and selected the dnn as the basic technique. in order to extract and implement the common knowledge to the training, we propose the multi-output dnn structure which is a reform of the simple dnn. our proposed structure has two phases: the common knowledge is extracted at the first phase by the common layers, and then, the various flow classification tasks are able to be trained and computed at the same time with inter-independent branches of private layers respectively. the common layers are also supposed to be effective for saving the calculation time and memory. each of the classification tasks owns an extended and relative simple branch of private layers for task specialization. besides, with this proposal, we suppose that new related classification tasks might be able to be solved by adding a new branch of private layers connecting to the well-trained common layers."
"for a more realistic experiment, the features or labels for some tasks are hard to obtain. again, we choose the duration, flow rate, and application to be the target task in turn while the other to play the source task. a multi-output dnn with the same structure as the last experiment is adopted. in the first place, we use all the data to train both the duration and flow rate tasks and thus try to obtain the common knowledge. then, 1 percent of total recorded flows were randomly selected for the training of application classification. three different training schemes are tested and compared with each other. also, a single dnn with the same structure is constructed as the standard control. table 4 shows the result of our one-shot experiment."
"for either the training function, a presumption behind this proposal is that training several related classes at a time would update the parameters in the neural network from different directions in the multidimensional space from the perspective of mathematics. hence, it might be helpful for the learning not dropping into local optimums."
"in the results, we can observe that the sct shares approximately equal accuracies to the single dnn with a less time cost over all the tests. this phenomenon would lead to that the common knowledge extracted by previously trained common layers would speed up the training process of a new task with the fair results. besides, this scheme requires much less memory compared to constructing a new dnn. now considering the oct, it achieves slightly better accuracies than the sct while taking even less time as for all the tasks and traces. generally, the single dnn costs as 2 to 4 times as the oct to convergence. this is a remarkable result. still, considering its disadvantage to the sct, it would be a compromise problem."
"under the condition of insufficient data, the dnn performs not very well in some of the cases. accuracies of 70 to 80 percent are obtained in half of our tests. in the contrast, the average accuracy of the multi-output dnn is around 94 percent whichever the training scheme is used. more specifically, the sct performs quite well in the experiment that it not only shows faster speed to converge but also shares higher accuracies than the standard control in most cases. it is noted that a 15 percent difference in the accuracy compared to the single dnn is obtained in some of the experiments. as for the oct, similar to the transfer learning experiments, it exceeds the sct on both the accuracies and saving times. an average 1 to 2 percent of accuracy promotion is obtained for the oct even to the sct. as far as the ot, though its accuracy is slightly lower than other two schemes, it outweighs the single dnn in a big advantage. and it converges much faster than the others. predicting flow rate as the target task and using the entry09, fig. 5 illustrates a typical case of the training process for different algorithms."
"as eq. 9 illustrates, this training scheme adds the new task to the trained neural network but trains only the private layers of the new branch. in such an approach, the common layers and the previous private layers will not be influenced by ot (training only target layers). it meets the demand of testing a new task without influencing the established business. general, the common layers learnt by source tasks would like to be biased towards the source tasks. if the source tasks are diverse enough, the common layers try to learn a neutral traffic representation [cit] . the fig. 2 illustrates a simplified version of the three training schemes."
"r first, we annotated the sentences using the delph-in development framework, in which the annotation process is effected by manually selecting the correct parses from among all the analyses produced by a hand-built symbolic grammar."
"our work was motivated by the fact that the demands will always be changing and that most of the previous works have shown that some features can be used to discriminate different tasks of flows. for instance, the features for classifying either elephant or mice flows and that for identifying the applications of flow, are surprisingly similar [cit] . this alludes to that there might be some common knowledge for different traffic classification tasks and the mapping functions from the selected features to the different flow properties that we need to predict are of close relations. based on the common knowledge, we would like to explore an all-in-one solution, a tool that is supposed to adapt to continuous changes of demands and a tool which has the potential of being applicable for various related tasks of flow classification."
"many data mining and machine learning algorithms make predictions using models which are trained with previously collected labeled or unlabeled dataset. nevertheless, in many real-world applications, the tasks to explore or the distribution of feature spaces are not always constant [cit] . in contrast, the transfer learning allows the training and testing datasets to be of different domains, tasks, or distributions [cit] ."
"the derivation tree is encoded in a nested, parenthesized structure whose elements correspond to the identifiers of grammatical rules and the lexical items used in parsing. phrase structure rules-marked by the suffix ' c' (for 'construction')identify the daughter sequence, separated by a hyphen, and, in headed-phrase constructions, a basic dependency relation between sentence constituents (e.g., subjecthead (sb-hd) and head-complement (hd-cmp)). lexical items are annotated with part-of-speech information according to the eagles tag set for spanish 4 and their lexical entry identifier, and they optionally include a lexical rule identifier. figure 1 shows an example."
"to sum up, adopting the multi-output dnn benefits the process of training when faced with a new traffic classification demand. the sct is a good solution and it achieves our initial proposal in this experiment. ignoring the source tasks, the oct is evaluated to be much efficient and accurate than the traditional dnn approach and other training schemes. besides, the ot strategy can be used as a quick testing tool for new demands."
"in later experiments, three tasks of classification are discussed: the duration, the flow rate, and the application type. for the first two tasks, we divide them into two sets with the median values and label them respectively."
"generally, the average accuracy promotion for the multioutput dnn to the single dnn is around 5.4 percent and the average time is reduced to around 36 percent using our proposed structure. it is considered that the common knowledge from the trained common layers improves the general performances of the training of target task in the one-shot learning experiments. in such scenarios, our proposed structure has apparently advanced the simple dnn quite a lot."
"most previous data mining techniques require a large amount of data. however, it is often difficult to acquire large sets of training examples. the one-shot learning refers to predicting with a very small labeled training set [cit] . in the field of flow classification, the dataset of some label tasks, e.g. the elephant or mice flow, are quite plentiful while requiring others, e.g. information of application layer, are expensive or rarely dated due to historical miss or legal reasons."
"tree entropy exhibits similar behavior, in that a restrictive threshold can select about 15% of sentences with precision over 90%, while setting a threshold such that about 31% of sentences are selected, we obtain precision of about 75%."
"a series of experiments were designed to evaluate our new traffic classifier using a multi-output dnn technique. in these experiments, a typical 3-branch multi-output neural network is implemented, and many different situations are discussed. in the evaluation, we adopt the cross entropy for all the loss function of dnn. the activation function is the sigmoid function. and the batch normalization is implemented in constructing the neural networks in later experiments for improving the accuracy. the common layers consist of four fully-connected layers with 30 neurons separately. in each private branch, two fully-connected layers are implemented (the number of first layer neurons is 30 neurons, the number of output layer neurons equal to the classification numbers)."
"then, we continue to the multi-output dnn. in order to avoid the question that the private layers play too many roles and thus eliminates the validity of the common layers, we adopt a very simple structure for each private branch with only one hidden layer with 30 neurons and one output layer. given that these classifications are not very complex problems, we construct the multi-output dnn with 4 common layers and 30 neurons for each layer. we tested 2 different training techniques of the multi-output dnn that we have discussed in section iii. one is the rrbp (round-robin backpropagation), and the other is ppscbp (parallel-private and sum-common backpropagation). table 2 illustrates the result of the overall accuracies for three tasks of classification with above tools and the 2 schemes for the multi-output dnn in four trace datasets. from the results, we can observe that the knn is mostly a good tool for the application classification. but it doesn't share fair results in other two tasks. the svm performs fairly in trace i and trace ii, but it seems to fail in the entry09 and entry10. decision tree shows a remarkable score of accuracy on the flow rate on trace ii, though its results don't seem well in other experiments. it seems that the simple linear regression is not suitable for this problem. the random forest algorithm is not as stable as the dnn, especially in the experiments using entry09 and entry10. the ensemble learning has stable performance in all the experiment and slightly worse than the proposed algorithm."
"the linguistic analysis produced by the lkb system for each parsed sentence provides, together with a constituent structure and a minimal recursion semantics (mrs) semantic representation [cit] ), a derivation tree, obtained from a complete syntactico-semantic analysis represented in a parse tree with standard hpsg-typed feature structures at each node."
"note that although the baseline has an f 1 score slightly higher than the ensemble, our goal is a high precision filter that can be utilized to select correctly parsed sentences. from this point of view, our approach beats both baselines."
"another way is to sacrefice the advance in saving the memories -to copy all the parameters in the trained common layers and build a new neural network with them. and train the new neural network as usual. pursuing a more clear and simple function, the last method is to ignore the common layers."
"for dependency parsing, we use maltparser [cit] ). to train it, we use manually disambiguated parses among those parses produced by the hpsg grammar, converted to the dependency format we describe earlier."
"for every batch training, the general loss is calculated, and all the private and common layers are updated. algorithm 1 and 2 demonstrates it specifically."
"quantity and quality are two very important objectives when building a treebank, but speed and low labor costs are also required. in addition, guaranteeing consistency, that is, that the same phenomena receive the same annotation through the corpus, is crucial for any of the possible uses of the treebank. the first attempts at treebank projects used manual annotation mainly and devoted many hours of human labor to their construction. human annotation is not only slow and expensive, but it also introduces errors and inconsistencies because of the difficulty and tiring nature of the task. 1 therefore, automating parts of the annotation process aims to leverage effectiveness, producing a larger number of high-quality and consistent analyses in shorter time and using fewer resources."
we select high-quality hpsg analyses using full agreement among a maxent parse selection model and a dependency parser. a comparison between the two is performed on the dependency structures that we obtain converting the parse tree produced by a symbolic grammar to the conll format.
the f i part refers to the mapping function of different branch of private layers respectively and f c denotes the general function of the common layers. the common knowledge c in this structure can be expressed as
"in this paper, we proposed a novel traffic classification structure based on the dnn classifier. firstly, we evaluated the effectiveness and the validation of the multi-output dnn within the field of traffic classification. we adopted three regular tasks: predicting the time duration, the flow rate, and the application type of flows. the ppscbp training scheme is proved to be effective in both reducing time cost and saving memory compared to the simple dnn structure and it shows a higher overall accuracy than other machine learning tools. then we have explored the performance of common knowledge extraction. it is shown that our proposed structure can extract the common knowledge of traffics properly and the knowledge space is less perplexed than all the corresponding single-task dnns. moreover, we evaluate the potential of our proposed algorithm to address new demands. all the three training schemes have achieved addressing the demands of new classification on related tasks at a faster speed in our experiments. besides, in one-shot learning scenes, the multi-output dnn shows remarkable results that it can be applied to acquire an even better accuracy result and a faster training speed with the support of common knowledge."
"in our experiments, we tested the ability of the ensemble approach to select only correct parses. the experiment proceeded as follows: r we compared the outputs of the two models and selected those sentences where both parses produced identical analyses."
5. cms -the ids on cms will send notification to ch when it detects malicious behaviour. it sends warning message with full details about the malicious vehicle that is detected in clustering mode.
"the forward value plays an important role in increasing the detection accuracy in self-driving vehicles. the ids can calculate the forward value of each vehicle; it makes decisions based on the fv. it is calculated from trace file that have been generated from ns-2. the ids considers each vehicle to be malicious when the vehicle does not forward a received packet to the destination after a particular time (t) and the forward value (fv) will be increased by 1 unit. in other words, it will increase by 1 every time abnormal behaviour is observed. the fv is communicated to all neighbour vehicles and they update their stored value with the latest values. the proposed system considers the behaviour of vehicles as normal when the fv is higher than the threshold (e.g. set to 3), otherwise the system will considered abnormal."
"in contrast to the normal situation. the results from the simulation in the frequency domain reveal the distinct deterioration of the whole superbuck system with fault of recessive weakness. the normal operating situation is close to the single change of, with similar am and pm in the frequency domain. however, the recessive weakness generated by both inductance & displays the reduced amplitude margin and phase margin dramatically, which means a weaker and worse system performance. so far, the recessive weakness may bring serious and destructive damages to the system, without timely detection and protection approach. in conclusion, considering the performance and significance of recessive weakness in both time and frequency domain, a new method which focus on the recessive weakness is needed to solve the problem and ensure the normal operation of the power system. to solve against the phenomenon of the analogous character in a time domain and differential character in a frequency domain from recessive weakness, a new method to detect and classify this kind of soft fault in the circuit is proposed later in this paper. in conclusion, considering the performance and significance of recessive weakness in both time and frequency domain, a new method which focus on the recessive weakness is needed to solve the problem and ensure the normal operation of the power system. to solve against the phenomenon of the analogous character in a time domain and differential character in a frequency domain from recessive weakness, a new method to detect and classify this kind of soft fault in the circuit is proposed later in this paper."
"to identify rois from different imaging sessions that correspond to the same cell we performed a multi-step routine: (1) the mean fluorescence images from each day were registered by transforming the coordinates of landmarks present in both images in matlab (2017a) using the fitgeotrans function. the resulting transformation was used to transform the rois from the second imaging session to match the first; all sessions were aligned to the first imaging session. (2) we calculated the distance between all pairs of centroids across the two sessions. for each roi from session 2, we computed the percentage overlap of the 10 cells with the smallest centroid distances from session 1. cells that had more than 1 roi with higher than 20% overlap were manually inspected; the roi that matched the current cell was selected from the overlapping rois or none were selected if it was unclear whether they were the same cell. a good match was determined by considering the percent overlap and the shape of the rois. all other cells were assigned the closest roi as matching. (3) we manually inspected any cells that had duplicate matching rois; again considering the shape and the percent overlap of matching rois, we selected the roi that matched that cell, or we decided that none was a good match. (4) to check for false positive matches, we manually inspected any matches where the centroid distances were greater than the mean + 1 std of all the matches, and any matches that had less than 30% roi overlap; the match was deemed good or not depending on the match criteria. neurons that were not matched to any roi were counted as different or new and assigned a new cell number. this process was repeated for subsequent sessions, registering the imaging field to the first session and comparing the rois to the cumulative rois from previous sessions. a final manual inspection of all the unique rois was performed after all the imaging sessions were registered; rois that overlapped were excluded from the dataset since it was unclear whether they were the same or different cells. examples of tracked cells and aligned rois are shown in s2 fig."
"the value of c ij ranges from 0 to 1, indicating that nodes i and j were never assigned to the same module or were always assigned to the same module, respectively."
"the results from the simulation in the frequency domain reveal the distinct deterioration of the whole superbuck system with fault of recessive weakness. the normal operating situation is close to the single change of l 1, l 2 with similar am and pm in the frequency domain. however, the recessive weakness generated by both inductance l 1 &l 2 displays the reduced amplitude margin and phase margin dramatically, which means a weaker and worse system performance. so far, the recessive weakness may bring serious and destructive damages to the system, without timely detection and protection approach."
"across many scientific disciplines from plant biology [cit] to biogeodynamics [cit] and the study of biodiversity [cit], scientists are faced with the challenge of bridging two or more scales of investigation into the function of complex systems. for example, in evolutionary biology, a key challenge is to bridge physical scales from protein sequences to fitness of organisms and populations [cit], while in the study of cancer progression a key challenge is to map genotype to phenotype [cit] . neuroscience is no exception. ongoing efforts seek to bridge the gap between the connectome and the transcriptome [cit], between brains and social groups [cit], or between large-scale brain regions and small-scale cellular circuitry [cit] . in each case, the development of a formal understanding will depend upon the capacity to build mathematical descriptions and theories across scales. one natural approach to this challenge is to use a formalism that is scale invariant, a characteristic that makes network science particularly appealing. our work in this study is an example of considering tools and conceptual paradigms previously exercised at the large-scale of brain regions, and exercising them at the level of cellular circuitry. we look forward to future efforts explicitly measuring and examining the network architecture of neural systems across both of these scales simultaneously in the same animal, with the goal of better understanding and predicting behavior."
"the routing table provides communication data of any vehicle whether in intra-clustering or inter-clustering of self-driving vehicles. in our proposed system, each vehicle has an ids to sniff, analyse and identify normal / abnormal behaviours. to generate a routing table, we need to add a function in the routing protocol. table 3 shows basic information of routing table generated in ns-2. the ids on each vehicle will extract the following from routing table: vehicle id, time and number of hops to detect wormhole attacks."
"as shown in fig 6, the proposed system has seven parameters as input to cms while it has three outputs: malicious vehicle (sybil / wormhole) and normal vehicle."
"in addition to the flexibility approach, we also used a second method to provide converging evidence of temporal core-periphery structure (fig 3a) [cit] . in this procedure, we calculated the session-averaged connectivity matrix (over the five recording sessions), and based on its organization we algorithmically assign cells (nodes) to a continuously defined core and periphery (see materials and methods for more details). intuitively, core nodes are nodes that maintain strong connections to one another and to the periphery across recording sessions, while peripheral nodes are those whose connections are variable (e.g., observed in only a few recording sessions or absent altogether). the size of the core and the smoothness of the transition from core to periphery are controlled by two free parameters,  and . we systematically explored this parameter space and at each point, we fit the core-periphery model to the session-averaged network to calculated the core quality [cit] . we compare the quality of cores fit to the observed session-averaged matrix against the qualities fit to random matrices generated by a permutation-based null model. this comparison allows us to identify points of interest in is a measure of variability while \"coreness\" is a measure of stability, we find that the two are inversely correlated with one another (red line represents the identity line). (e) cross-subject consistency of optimal parameters for fitting the core-periphery model. for each mouse, we calculated the difference between observed core quality and that of a null model, and we retained the top 10% of those points. these points are depicted at the level of individual mice in panel f. in panel e, we aggregate those values across all mice."
"the inductor is generally constructed of a frame, winding, shielding cover, encapsulates material and others. the cause of the inductance variation includes the change of temperature, the damage of magnets and the rack of fusion. the operating frequency and different material also have great impact on the inductor. with these complicated condition, the trend of inductance is unpredictable [cit] . the inductance changing trend usually has the possibility of both wax and wane. with regard to the variation tendency of the inductance, both situations need to be considered within the verification experiments in this paper."
"the authors would like to acknowledge the support from the school of ocean and earth science in tongji university under the project \"research on the power system of large-scale scientific cabled seafloor observatory networks\". the authors also would hope to express sincere thankfulness to the editors and anonymous reviewers for their valuable opinions and contributions."
"autonomous cars' utilisation of open wireless medium and shared broadcast on wireless channels establishes new security challenges in the external communication systems of these cars. the steps below explain the methodology for proposed security system, namely:"
"in a superbuck converter, if the filter capacitance c 2 is beyond the tolerance range, longer rising time and steady time will cause 50% over rated value. similarly, the condition of 50% under rated value will lead to unexpected filtering effects and larger ripple waves. the loss of capacitance c 1 will cause the larger overshoot and longer steady time, which are the important dynamic indicators of the system. moreover, the increase of c 1 will also cause unsteady voltage and a larger ripple wave. the change of l 1 and l 2 can result in unsteady output voltage and continuous oscillation, which seriously influence the system response character. furthermore, the variation of c 1, l 1 and l 2 will greatly transform the system transfer function (relationship between input and output). without timely detection, recessive weakness will bring out the failure of the whole converter function. the damage occurs to both sides of the input and output units in the circuits."
"all experiments were performed with equal numbers of adult male and female mice (supplier-jackson laboratories; age, 12-22 weeks; weight, 20-36 g; pv-cre mice, strain: b6;129p2-pvalbtm1(cre)arbr/j)."
"the system character aroused by recessive weakness can be also identified by transfer function ( ) in the frequency domain. since the superbuck converter has two capacitors and two inductors, its small signal model is a fourth-order system. the small signal model of the superbuck converter has been derived [cit], indicating the transfer function between output ( ) and control signal ( )."
"wavelet transform is an effective and powerful approach to time-frequency analysis, which is developed from the foundation of overcoming the weakness of fourier transform within a non-stationary signal [cit] . through wavelet transform, some invisible and hidden information in the time domain can be clearly revealed. wavelet transform has been broadly applied in lots of fields, such as signal processing, image processing, pattern recognition and data compression. one of the most important characters of the wavelet transform is the great localization of features in both the time domain and frequency domain, providing the frequency information by the frequency base of the original signal [cit] . the significance of the wavelet transform is through the shift of the mother wavelet function, and then under different scale a to proceed with the transvection with signal x(t)."
"in this manuscript, we used two variants of modularity maximization. first, we studied the network community structure for each recording session independently. for this analysis, we combined modularity maximization with a newly-developed multi-resolution technique that divides the network into communities of different sizes (scales) that are related to one another hierarchically [cit] . this procedure allows us to examine community structure across a range of scales, from large communities to smaller communities that might support more specialized information processing."
"in this paper, we proposed a novel authentication technique to protect the external communication system of autonomous vehicles. it is considered one of the most important security part which must be supported for each wireless communication system. the proposed authentication system has the ability to assist the self-driving vehicles to identify between authorised and unauthorised cars so that these vehicles can communicate with other vehicles and rsus in that radio coverage area. in more details, the authentication process is heavily based on mac number that has been generated from any device on the on board unit such as sensors, gps or lider. the authentication scenario is shown in fig 5 . the security system in this paper assumes that each mobility vehicle has the hash value that extracted from the devices on board unit of self-driving vehicles. the hash value that generated from mac number with salt value that is considered an identifying aspect for each autonomous vehicles on roads."
"according to table 7, we can easily distinguish between two ids. as a result, the ids is proposed in this paper definitely different from fpn-ids."
"stationary signal [cit] . through wavelet transform, some invisible and hidden information in the time domain can be clearly revealed. wavelet transform has been broadly applied in lots of fields, such as signal processing, image processing, pattern recognition and data compression. one of the most important characters of the wavelet transform is the great localization of features in both the time domain and frequency domain, providing the frequency information by the frequency base of the original signal [cit] . the significance of the wavelet transform is through the shift of the mother wavelet function, and then under different scale to proceed with the transvection with signal ( )."
"the detection systems in ids are divided into three types: signature-based, anomaly-based and specification-based detection the anomaly-based ids divided into two further types which are: self-learning and programmed. here, we utilise ids based on data traffic that has been collected from vanets, and the detection is anomaly-based, with a function that can decide what will be the normal behaviour."
"when the recessive weakness occurs in a superbuck converter, the energy space distribution of the output character alters accordingly, which indicates that the variation of output energy contains enough related character information. since the selected mother wavelet and the scale function are mutually orthonormal, then the energy spectrum of each level of the wpd can be acquired, described by perseval's theorem [cit] ."
"if the recessive weakness of the critical component in the circuit occurs with normal output performance then the deteriorative system property may lead to the invalidation of the circuit, or even the failure of the whole function task. on account of the imperceptible and undetectable characteristics of the recessive weakness, it is of great importance to detect and classify the recessive weakness in a timely manner to guarantee and maintain the operation of the power supply system. figure 2 shows the output curve within the same range (10%) of the single change of the inductance l 1, l 2 and the recessive weakness of both inductance l 1 &l 2 (synchronously) in contrast to the normal situation [cit] . the results from the simulation in the time domain show some visible differences of the dynamic performance, such as overshoot, rise time and response time, while because of the existence of the feedback loop of the circuit, the steady output is always adjusted and reaches the steady state. therefore, the steady output voltage is nearly identical without obvious distinction. to the contrary, the ripple wave of the recessive weakness from l 1, l 2 is more similar to the normal situation compared with the single change of l 1 and l 2, which makes it even more difficult to distinguish the recessive weakness in the superbuck converter."
"finally, an additional limitation concerns the measures used to establish the presence or absence of connections between cells. specifically, we constructed networks where nodes represented individual cells and where edges represented the correlation magnitude of fluorescence traces. importantly, correlated activity is not a direct proxy for underlying structural connectivity [cit], and thus a pair of neurons that may not be directly synaptically connected can exhibit correlated activity; rather than reflecting structural connections, functional connectivity provides information about the interactions between neurons due to their function [cit] . moreover, correlated activity also does not represent the coupling matrix that prescribes the temporal evolution of brain activity [cit] . rather, the correlation structure of neural activity represents the product of a dynamical system whose evolution is constrained by structural connections. though correlated activity at the large-scale has proven useful for investigating the functional organization of brain networks [cit], its utility for understanding and characterizing the structure and function of micro-scale networks remains unclear and largely untested [cit] . future studies should both investigate in greater detail the relative advantages of alternative, domain-specific measures of functional connectivity [cit] and the relationship of these measures to other connection modalities [cit] ."
"t 0  dt s : while the switch q is turned off, the diode d is on and the inductor obtains the continuous current through the diode d and load resistor r l . the inductor l 1 charges the capacitor c 1 which causes the linear decline of inductor current. since the end voltage of l 2 is positive, the inductor l 1, the diode d and the load resistor r l constitutes a circuit. the superbuck converter only adds one inductance and capacitance to achieve zero ripple input current and continuous output current. the structure of superbuck converter is shown in figure 1 . and the feedback loop consist of compensator, pulse width modulation (pwm), and power drive. analyze the operating status of the superbuck converter in ccm (continuous conduction model) of one period as follows:"
"second, we note that we have examined correlations in spontaneous activity fluorescence traces, and this approach has the strengths of computational simplicity and ease of interpretation [cit] . however, we acknowledge that correlation-based approaches focus on pairwise functional interactions, and remaining agnostic to underlying structural connectivity as well as to higher-order (non-pairwise) relations between units. it would be interesting in future to consider maximum entropy models as an alternative method to estimate connections between units [cit], both for its sensitivity to underlying structure [cit], and for its ability to assess higher-order interactions [cit] . approaches that could then take advantage of the richer assessment of higher order interactions in these data include emerging tools from algebraic topology [cit], which have already proven relevant for understanding structure-function relationships at both large and small scales in neural systems [cit] ."
"the proposed fault detection and classification algorithm is based on the signature analysis of the sampled operating signals, performed by the wpd and pca techniques to the feature extraction and dimensionality reduction of data while pnn is used for the fault classification [cit] . the flowchart of the proposed algorithm is shown in figure 8 . in the step of data acquisition, the output voltage of the superbuck converter is selected as the sampled signal for fault diagnostic. then, the wpd is used to decompose every signal into a different level, including both low frequency and high frequency information. parseval's theory is also utilized for the feature extraction of each sampled data. the extracted feature vectors are then run through the pca to reduce the dimensionality of the vectors, which is used as the inputs of the pnn classification. the outputs of the neural network indicate the type of the recessive weakness (normal, single, couple, triple circuit components) that may occur in the superbuck converter as the most likely recessive faults."
"the adoption of both authentication and encryption schemes are considered a strong first layer of defence whilst ids can form a second layer of defence against intruders [cit] . intrusion prevention can be utilised to enhance the defence capability of such networks, but these mechanisms often suffer from drawbacks such as high costs of implementation and limited coverage to prevent attacks [cit] . thus the nature of communications in self-driving vehicles requires technologies such as intrusion detection (ids) that are integrated with monitoring and tracking systems to detect any abnormal behaviour."
"ids can log, collect and analyse data that was extracted from vanets. the ids then identifies normal or abnormal behaviour and flags four types of alerts i.e. true positive, true negative, false positive and false negative."
"the ids can detect two most common but serious attacks in vanets: sybil and wormhole attacks. as we know each of these attacks has a different behaviour. thus, each attack has different parameters to detect malicious behaviour. fig 7 shows type parameters of proposed ids."
"intelligent ids is proposed in this paper to detect sybil and wormhole attacks that based on clustering mode. the detection process is heavily based on features that extracted from trace file and routing protocol. these files are generated from network simulator. whereas, fpn-ids is designed to detecting flooding and dropping attacks that targeted control data and sensitive information of vehicles [cit] . the main aspect difference between fpn-ids and clustering-ids is shown in table 5 below. security system in external communication system for self-driving vehicles"
"(1) fast training, the training time is only slightly more than the interval of reading data. (2) no matter how complex the classification problem is, the optimal solution under bayesian criterion can be guaranteed through enough training data."
"the objective of self-driving vehicles is to provide comfort, safety and convenience to road users. in this section, we identify some previous work that is directly and indirect related to our research. many previous works to secure vanets were based on ids; however but many attackers were able to identify weak points that could exploited. hence vanets still suffer from a many security problems [cit] ."
"hierarchical modularity in biological systems is further thought to allow for a decomposability of the system's temporal responses to the environment, with fast processes occurring in small modules at a low level of the hierarchy and slow processes occurring in large modules at a high level of the hierarchy [cit] . prior work at the large-scale has demonstrated the presence of hierarchically modular structure in neural systems specifically, and suggested that large modules support broad cognitive functions while small modules support specialized cognitive functions [cit] . here we extend these prior observations by showing that over short time periods approximately equal to the duration of a recording session, neurons assemble into cohesive modules of varying size, ranging from large, spatially-distributed clusters of weakly coupled neurons to compact, highly correlated ensembles. in theoretical work, it is interesting to note that hierarchical modularity provides an efficient solution to the problem of evolving adaptable systems while minimizing the cost of connections [cit] . this relation between hierarchical modularity and low cost yet efficient information processing in neural systems has also been supported by both theoretical work and analysis of neural data in both c. elegans and human [cit] . when considering our results in this light, it is useful to note that the spatial compactedness of modules suggests that maintaining long-distance correlated activity may be metabolically costly and therefore uncommon. overall, these findings are consistent with those observed in other micro-and macro-scale networks and suggest that the organizational principles of modular architecture and spatially-compact, low-cost clusters may be conserved across spatial scales [cit] ."
"stability of spontaneous, correlated activity in mouse auditory cortex the parameter space: points where the observed core was of greater quality than that of the null model."
"fuzzification technique is utilised in fix one of the common problem of classification which is ambiguity between normal and abnormal behaviours. in more details, sometime border between normal and malicious connection is not sufficient clear [cit] . the extracted features from trace file of ns-2 play direct role on the proposed ids performance [cit] . the detection rate will reduce when the name of class whether normal or malicious is not well separated. in addition, the false alarm also will increase that has direct and negative impact on ids performance. the lightweight of mathematical model of fuzzification is considered a suitable scheme to expose this type of classification problem [cit] . thus, it plays important role in creating clear border within extracted dataset."
the motivation of our work is to design a security system that protects external communications in self-driving and semi-autonomous vehicles. the proposed security system is implemented in seven stages namely:
"3. distance and angle calculation -in this stage, the proposed system can calculate distance and angle degree between vehicles based on values of x-axis and y-axis obtained from gps and applied on eqs 1 and 2."
"the frequency performance of the superbuck circuit with the single increment of, and both of & with 10% are tested as follow. figure 3 shows the bode diagram within the recessive weakness of both inductance & (synchronously) and the single change of the inductance,"
"one of the most important organizational principles of biological neural networks is their organization into cohesive modules [cit] . these modules are thought to support specialized information processing while conferring robustness to perturbations. moreover, converging evidence from micro-and macro-scale network analyses suggest that network modules are also organized hierarchically, with larger modules subtending broader brain function and smaller modules playing more specialized roles [cit] . in this section, we test the hypothesis that networks reconstructed from fluorescence correlations in mouse auditory cortex exhibit hierarchically modular structure."
"in wired networks, we can build an ids on a centralized authority however wireless networks lack such fixed security. this is encouraging researchers to create virtual centralized or semicentralized authority by incorporating clustering [cit] . clustering based tdma architectures provides external communication in self-driving vehicles, whilst offering scalability and fault tolerance resulting in efficient use of vanet resources. fig 2 shows the taxonomy of existing clustering scheme for vanets [cit] ."
"where j stands as the number of the wpd scale. in aim to enhance the features, the difference between energy e f ault caused by recessive weakness and energy e normal in a normal operating situation together derives the feature energy e. the adopted feature vector can be mathematically represented as the following form: the more outstanding the difference between normal and fault energy is, the more related and useful information used for classification are included in the result of the wavelet packet decomposition [cit] ."
"the authors declare no conflict of interest. the funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results"
"remains training data, remains test data. (2) send the normalized training samples to the input layer directly. the fault diagnosis method based on pnn uses the strong nonlinear classification ability of the model to map the fault sample space into the fault mode space, so as to form a diagnosis network system with strong fault tolerance and structure adaptive ability [cit] . compared with the bp network, the main advantages of pnn are:"
tdma is used to provide channel access which sharing medium networks based on split signal between nodes in that zone. it divides the signal between users' networks into different time slots.
"the design of hierarchical ids based on clustering mode enhances the detection rate of proposed ids in the external communication systems by 20.15 compared to coussement [cit] . hence the ids-clustering has a direct and positive impact on the resulting system because of the increase in the detection rate, decrease in the false alarm rate and error rate. the proposed system can be extended to build other ids which can detect other types of attacks such as flooding, black hole and grey hole attacks."
"applying the wpd feature extraction method with 10db mother wavelet within parseval's theory, the detailed energy distributions in different decomposition level of the output signal, including normal condition and recessive weakness are represented in figure 5, respectively. the xaxis is the decomposition level while the y-axis the energy. the distinction between the normal situation and recessive weakness calculated through (10) is shown in figure 6 . through the above analysis, it can be concluded that the application of wpd and parseval's theorem maintains the outstanding features of the invisible fault. in order to achieve less computing time and memory space, pca is an appropriate way to reduce dimensionality instead of using the results of wpd for the classification stage directly. the obtained data after the process of pca are then utilized as the input vectors of pnn for pattern recognition, which is discussed in detail in the following part."
"probabilistic neural network (pnn) is a feed-forward artificial neural network (ann) proposed by specht d. [cit], which is based on the bayes strategy of decision conducting and nonparametric estimators of conditional probability density functions [cit] . the most significant advantage of pnn over other network models is its simple and instantaneous training process using all the samples without learning and its guarantee of asymptotical approaching the bayes' optimal decision surface provided by the smooth and continuous class [cit] . the structure of the pnn is demonstrated in figure 7 . through the above analysis, it can be concluded that the application of wpd and parseval's theorem maintains the outstanding features of the invisible fault. in order to achieve less computing time and memory space, pca is an appropriate way to reduce dimensionality instead of using the results of wpd for the classification stage directly. the obtained data after the process of pca are then utilized as the input vectors of pnn for pattern recognition, which is discussed in detail in the following part."
"tdma is used to control channel access between vehicles by sharing medium communication based on split signal between nodes in that zone. it divides the signal between users' by allocating different time slots. here we design the ids on clustering head (ch) vehicles. the security system uses the tdma cluster-based media access control to secure the external communication for self-driving and semi self-driving cars. to achieve stability and channel utilization, the cluster is needed in vanets. the tdma divides signal into time frames and it divides the time frame into time slots, where each vehicle is associated with time slot in the frame [cit] . fig 4 shows the working of tdma. in addition, tdma can offer fairness in the sharing of communication channels between vehicles without employing any extra infrastructure or virtual leader vehicle [cit] . to design a security system in external communication system for self-driving vehicles robust security system, two important challenges are incorporated: aspects, i.e. fuzzy logic and clustering based tdma."
"the aim of wpd is to develop the representation forms of the analyzed signal f (t) according to the scale and wavelet function [cit] . the wpd deals with not only low frequency but also high frequency and contains possibly more information of the original sampled signal. the procedure of wpd is shown in figure 4, where x(n) is the sampled form of the original signal f (t), h ij (n) l ij (n) represent the high frequency and low frequency decomposition coefficient in each level respectively, i is the decomposition level, j is the array of the decomposition results."
"a measure of distance between vehicles is an important factor in our proposed security system. each self-driving vehicle in clustering mode can calculate distance between itself and other vehicles, based on values of x-axis and y-axis obtained from their gps. the proposed system is based on eqs 1 and 2 to calculate the distance and angle between two vehicles:"
"our proposed system is based on parameters which describe the behaviour of vehicles, classified in either normal or abnormal behaviour. to achieve efficient performance, the proposed ids approach enables us to collect and monitor neighboring vehicles. in other words, we attempt to build a clustering approach that increases the system accuracy and speed, while providing a low false alarm rates in online detection. in order to achieve clustering, we employ a clustering protocol that is based on time division multiple access (tdma) [cit] ."
"a superbuck converter, known as a double inductance buck circuit, can obtain continuous current, small voltage ripple and input output voltages of the same polarity [cit] . it is derived from the buck converter and combines with the input inductor-capacitor filter to achieve the same output gain as the buck circuit. due to the continuity of its input and output currents, a superbuck converter is generally suited to a power factor correction (pfc) and the connection between photovoltaic panels and the battery. furthermore, coupled inductors can realize the requirement of lighter weight and minor volume, so the superbuck converter is widely applied as a power function unit of a satellite [cit] ."
"the system character aroused by recessive weakness can be also identified by transfer function g vd (s) in the frequency domain. since the superbuck converter has two capacitors and two inductors, its small signal model is a fourth-order system. the small signal model of the superbuck converter has been derived [cit], indicating the transfer function between output (v out ) and control signal (d)."
"giving consideration to the vector space, calculating quantity and network repose time, the energy feature vector gained from the wavelet packet decomposition must be further dimension reduced. to reduce the feature dimension, pca is generally applied to the extracted features [cit] ."
"we note that, here, we focus on temporal core-periphery structure, where the core is comprised of nodes that maintain their connection to one another over time and the periphery is made up of nodes whose connections to one another are variable or infrequent [cit] . this definition is distinct from core-periphery structure defined based on topological features alone [cit] . in the case of topological core-periphery structure, the core represents a strongly connected cluster of nodes that is weakly connected to a set of peripheral nodes. the peripheral nodes make few connections to one another."
1. the current cam must differ by at least 4 degrees in value of heading from the previous messages or 2. the current cam must differ by at least 4m in position from the previous message or 3. the current cam must differ by at least 0.5 m/s in speed from the previous message or 4. the current cam must differ by at least 1s in time from the previous message.
"the experiment results of the test samples are summarized in table 3 . the overall test classification accuracy achieves 93.05%, which shows the effectiveness of the proposed method to correctly classify the recessive weakness in the superbuck converter."
"stability of spontaneous, correlated activity in mouse auditory cortex focused on sequences of recording sessions and characterized the stability of modules across those sessions. because the number of recording sessions varied from one mouse to another, we focused on sequences of five recording sessions (the greatest number that was available for all mice). for each mouse, we modeled the thresholded connectivity data from each of these five recording sessions as the layers in a multi-layer network object [cit], and we used multi-layer modularity maximization [cit] to track the fluctuations in modular structure across those five sessions (fig 3a; see materials and methods for more details). the multi-layer modularity maximization approach extends the traditional modularity maximization approach [cit] by incorporating networks from all five sessions into a singular multi-layer network object, and then detecting modules in all layers simultaneously. the main advantage of this procedure is that the same set of module assignments are preserved across all layers, making it possible to track the formation and dissolution of modules over time and to seamlessly compare modules from one recording session to the next. given such a mapping, one can then calculate measures like the local \"network flexibility\" [cit], which indicates how frequently a given node changes its module assignment across layers. past studies have used this flexibility measure to identify temporally stable cores and variable peripheries (clusters of nodes with low and high flexibility, respectively) [cit] . the multi-layer modularity maximization procedure required that we maintain a consistent set of nodes across layers, i.e. a fixed set of cells that were observed on all recording sessions. this conjunction results in a smaller set of cells than the pairwise comparisons utilized in the previous section. the number of cells retained for the multi-layer analysis was 144 (mouse 1), 68 (mouse 2), 67 (mouse 3), and 54 (mouse 4)."
"the specific wavelet composition level (n ls ) to be assumed depends on the sampling frequency f s of the measured original signal. for the sake of allowing the high level signals (concluding both approximation and details) to cover all the regions of frequencies along the localized sideband. the sideband components are generally identified as the harmonic components of the signal, that occur around the both sides of the fundamental frequency components."
"recent studies have begun to build and characterize network models of cellular activity as measured by calcium imaging [cit], and have demonstrated their biological relevance tools from network science [cit] . specifically, we assessed the modularity of the network structure using a commonly applied community detection technique known as modularity maximization [cit] . further, we assessed temporal fluctuations in this modular structure using tools for the analysis of dynamic graphs [cit] . for further details on our methodological approach, see materials and methods."
"2. ns-2 -cms will determine information from other vehicles. they generate a routing table for each vehicle. each vehicle will broadcast 3-10 packets/second [cit] . the cms can extract features like timestamp, vehicle id, gps position and number of hops from routing table and trace file."
"it is clearly noted from figure 9, the proposed method in this paper shows the great robustness to the different sorts of recessive weakness. the designed scheme has been proved and it has the generality for different recessive weakness and operating conditions. furthermore, the recessive weakness is beyond the normal tolerance range of the components and less than the total invalidation of system. recessive weakness can represent the crucial transient process from normal operation to system failure. the meaning of detecting recessive weakness is to find out the abnormal operating situation before the complete invalidation of the circuits. the ordinary error range of inductance and capacitance is close to 5%. the changing range of recessive weakness in this paper is only 5-15%, which can still achieve high classification accuracy with 93.05%. with a larger changing range of components, the classification accuracy can be further improved. the purpose of this paper is to introduce the fault of recessive weakness and propose a useful detection method. figure 9 . the results of the classification with proposed method."
"from table 6, it is easily noticed that the vital role of the ids-clustering in enhancing detection rate of normal behaviour in self-driving vehicles, whereas ids-fpn provides a better detection rate than ids-clustering. in the futur ework, we could for example design clustering fpn to obtain better result on determining normal and abnormal behaviours."
"in the clustering scheme, we install ids on each self-driving vehicle. the role of cms is to collect information of neighbour vehicles in the zone. it is assumed that chs are trusted in external communication of self-driving vehicles. each vehicle uses rules and thresholds to detect abnormal behaviour when identifying a malicious vehicle. whether sybil or a wormhole attack are detected; the vehicle will have sent a message to notify its ch. the ch will block and broadcast the malicious id's to its cms and to other chs. the following are the 7 stages of utilised, and the overall architecture of the proposed security system is shown in fig 6. 1. generate the highway mobility -in this stage, two tools are utilised to generate highway mobility and traffic to simulate the real communication environment of self-driving vehicles (see above). the output files of this stage are considered input files to ns-2 to generate trace file and routing table of normal and abnormal behaviour."
"applying the wpd feature extraction method with 10db mother wavelet within parseval's theory, the detailed energy distributions in different decomposition level of the output signal, including normal condition and recessive weakness are represented in figure 5, respectively. the xaxis is the decomposition level while the y-axis the energy. the distinction between the normal situation and recessive weakness calculated through (10) is shown in figure 6 . the low dimension space has the ability to reflect the most important information from the original data."
"here, an ids is proposed that can detect vehicles that seek to attack via sybil and wormhole attacks to protect the external communication systems of autonomous vehicles. tdma cluster-based media access control is employed in design ids to secure the vanets for vehicles. to achieve stability and maximize channel utilisation, a cluster technique is beneficial for vanets. tdma divides signals into time frames and then into time slots, where each vehicle is associated a time slot in the frame [cit] . the proposed system has the ability to identify and analyse the positions and identifications of vehicles to calculate distance and angle degree based on this cluster-tdma scheme. this can be utilised to determine a malicious vehicle behaviour in the wireless network area, fig 1 shows behaviour of sybil and wormhole attacks in the external communication system of autonomous vehicles."
"apparently, the output character caused by the recessive weakness of the two crucial components simultaneously is approximate to the normal output signal and this inner-system recessive weakness can be easily ignored, considering the output character alone. since the time domain performance of recessive weakness shows hardly no distinguishable difference, the detection of recessive weakness in the frequency domain is considered. apparently, the output character caused by the recessive weakness of the two crucial components simultaneously is approximate to the normal output signal and this inner-system recessive weakness can be easily ignored, considering the output character alone. since the time domain performance of recessive weakness shows hardly no distinguishable difference, the detection of recessive weakness in the frequency domain is considered."
"meanwhile, the electrolytic capacitance is composed of cathode metal, oxide film, and electrolyte. the energy storage is based on the principle of electrostatic storage, the electrochemistry, and the structure of the electrode is in stability. the reason for the ageing problem consists in external stress, auto acceleration, and manufacturer factor and so on. according to the previous research, the capacitance becomes larger instead of smaller under the condition of high temperature and long-time use [cit] . therefore, the numerical value of usually becomes larger gradually with the ageing of the capacitance. only the increment condition of is considered in this paper. the inductor is generally constructed of a frame, winding, shielding cover, encapsulates material and others. the cause of the inductance variation includes the change of temperature, the damage of magnets and the rack of fusion. the operating frequency and different material also have great impact on the inductor. with these complicated condition, the trend of inductance is unpredictable [cit] . the inductance changing trend usually has the possibility of both wax and wane. with regard to the there also exists system uncertainty in the proposed algorithm. some kinds of converters operate in dcm (discontinuous current mode) in the condition of heavy load. thus, the output voltage may be unsteady because of the switch in the operating mode [cit] . the discontinuous voltage influences the uncertainty of the algorithm. besides, the process of generating control command signals produce modeling uncertainties and other external disturbances [cit] . there also present uncertainties in measurement and processes which may lead to false-alarm situations [cit] . the parameter identification errors influence the data-driven fault detection design as well [cit] ."
"meanwhile, the electrolytic capacitance is composed of cathode metal, oxide film, and electrolyte. the energy storage is based on the principle of electrostatic storage, the electrochemistry, and the structure of the electrode is in stability. the reason for the ageing problem consists in external stress, auto acceleration, and manufacturer factor and so on. according to the previous research, the capacitance becomes larger instead of smaller under the condition of high temperature and long-time use [cit] . therefore, the numerical value of c 1 usually becomes larger gradually with the ageing of the capacitance. only the increment condition of c 1 is considered in this paper."
"obviously, recessive weakness is a kind of soft fault, resulted from the synchronous variation of more than two components in a circuit, which is characterized by recessive change in an output curve and actually more deteriorated stability in system. any hard fault of the parameter in the superbuck converter can lead to a dominant change of the output character curve directly and alter the system operating situation. the recessive weakness of more than two parameters concurrently may lead to the wave mutation as a normal situation without apparent change because of the mutual effect of components. the output signal from the recessive weakness is quite analogous, for instance, in the steady state output voltage or the ripple output voltage. the whole supberbuck system character, such as the amplitude margin (am) phase margin (pm), virtually become weaker and unsteady. in consequence, the physical meaning and feature of \"recessive\" is prominent and outstanding."
"probabilistic neural network (pnn) is a feed-forward artificial neural network (ann) proposed by specht d. [cit], which is based on the bayes strategy of decision conducting and nonparametric estimators of conditional probability density functions [cit] . the most significant advantage of pnn over other network models is its simple and instantaneous training process using all the samples without learning and its guarantee of asymptotical approaching the bayes' optimal decision surface provided by the smooth and continuous class [cit] . the structure of the pnn is demonstrated in figure 7 . the fault diagnosis method based on pnn uses the strong nonlinear classification ability of the model to map the fault sample space into the fault mode space, so as to form a diagnosis network system with strong fault tolerance and structure adaptive ability [cit] . compared with the bp network, the main advantages of pnn are:"
"to address this hypothesis, we leverage recent advances in community detection methods [cit] -a collection of algorithms and heuristics that use data-driven approaches to uncover the modular structure of networks. specifically, we use an extension of the popular modularity maximization algorithm [cit] . the standard version of this algorithm defines a module as a group of network nodes whose internal density of connections is maximally greater than what would be expected under a chance model. the extension of this algorithm samples modules over multiple organization scales, ranging from coarse divisions of the network into a few large modules to finer divisions of the network into many small modules. importantly, unlike past applications, this extension also includes built-in null statistical testing capable of rejecting modular structure at different levels of the proposed hierarchy if they were consistent with a null model."
"in the previous section we demonstrated that the correlation pattern of fluorescence traces exhibits modular structure across multiple scales, and that these multi-scale modules unfold as part of a hierarchy. in these analyses, the network's modular structure was derived separately for each recording session. while this approach allowed us to characterize the modular structure on a given day, it tells us little about how those modules fluctuate over the timescales of days or weeks. here, we address this question directly, taking advantage of the longitudinal tracking of cells across multiple recording sessions to assess the temporal consistency of the network's overall organization, as reflected in the full correlation matrix, and in the network's mesoscale organization, as reflected in its modular structure."
"there are several methodological considerations and limitations that are pertinent to the interpretation and generalizability of our results. first, we note that the experimental methods allow us to sample only a subset of neurons within a specific \"slice\" of the auditory cortex. it is likely that most of the neurons that directly target the neurons that we image are not captured by the analysis. therefore, the estimates for the network connectivity should not be taken as an approximation for the actual physical connectivity in the cortical circuit. another important aspect of data collection is that we focus on a specific cortical layer: layer 2/3. neurons in the cortex differ tremendously in their connectivity patterns across different layers [cit] . it would be important in future studies to sample the activity across cortical depth to better understand integration of information across cortex."
"this paper proposes a method that integrates the wpd technique combined with pca and pnn for the fault detection and classification of recessive weakness in a superbuck converter. the character of recessive weakness in a circuit is announced by simulation in both the time and frequency domains. the wpd provides an effective way to extract the character of the signal at different frequency bands. the proposed algorithm decreases the computational burden since the reduction of the input data is through pca. the pnn affords an intelligent method and criterion to the feature comparison. the pnn is then adopted to classify the extracted features and detect the recessive weakness automatically, with a strong ability for generalization and training mechanisms. a related simulation has been performed to identify the capability of this method in fault detection and classification and the consequence shows that prospective accuracy can be achieved to distinguish recessive weakness. the extracted features have excellent robustness to the different recessive weakness, including different sums of the changed component. the proposed fault diagnosis method is simple, accurate, and its effectiveness has also been verified through experimental results. refer to the purpose of this paper is to introduce the recessive weakness among soft faults, which is extremely difficult to detect and influence the performance of the circuit. this work will help to improve the application of the superbuck converter and to put the emphasis on recessive weakness as an important diagnosis index."
"using two-photon microscopy (ultima in vivo multiphoton microscope, bruker) changes in fluorescence of gcamp6s in transfected neurons caused by fluctuations in calcium activity were recorded in awake, head-fixed mice. we recorded from the same cells over many days in layer 2/3 of auditory cortex, using blood vessel architecture, depth from the surface, and the shape of cells to return to the same imaging site. laser power at the brain surface was kept below 30 mw. chronic imaging of the same field of view across days was carried out for the duration of the experiment."
"fault in the circuits refers to the deviation of at least one feature order parameter in a system, which is beyond the acceptable range. the consequence is that the performance of the system is dt s  t 1 : while the switch q is turned on, the diode d is off and the input current passes through the inductor l 1 directly to the load resistor r l . the capacitor c 1 simultaneously depend on the inductor l 2 to spread energy to the load, the inductor l 2, the capacitor c 1, the switch q and the load resistor r l forms a circuit, thus both the current of inductor l 1 and l 2 achieve the linear ascent."
"the supberbuck converter consists of four source components, l 1, l 2, c 1, c 2, which are utilized to verify the effectiveness of the proposed method in this paper. the efficiency and function of c 2 is just a wave filter in the circuit, therefore we design the verification experiment of the proposed algorithm regardless of c 2 . thus, the verification experiment focuses on l 1, l 2 and c 1 ."
"distributed and often redundant coding is a hallmark of neural systems [cit], providing robustness to single-neuron variability [cit] and supporting complexity in the system's potential behavioral repertoire [cit] . a key challenge in understanding this code lies in determining how the nature and strength of correlations between neurons is related to a stimulus [cit] . recent evidence suggests that so-called noise correlations have marked and diverse functions [cit], from impacting information encoding and decoding [cit], to tuning the amount of information present and thus the nature of ensuing cortical representations [cit] . correlations in spike trains have also been noted to contain important information about excitability, latency, and synchronization [cit] . even apart from task-evoked activity, spontaneous activity and correlations of that activity can profoundly impact cortical responses to a sensory input, thereby playing a critical role in information processing [cit] ."
1. malicious vehicles drop or duplicate the data or control that have been received from other surrounding vehicles. these vehicles try to create congestion in the network.
"intuitively, flexibility counts the fraction of times that nodes' community assignments in layers u and u + 1 differ. nodes that differ more frequently have flexibility values closer to 1, while nodes that differ less frequently have flexibility values closer to 0. here, we used the flexibility measure as an index of change in network community structure across recording sessions."
"the ch receives data traffic and control data from cluster members (cms) to validate malicious behaviour and generate alarms. the selection of the ch is based on an election algorithms which are utilised in the clusters [cit] . the semi centralization optimizes communication between vehicles and vehicles with rsus. mac based clustering (tdma clustering) is used in this paper to achieve stability and maximize channel utilization, a cluster technique is beneficial for vanets. a mac algorithm used in the tdma method allows to decrease the number of packet drops as well as collisions and enables vehicles to transmit on the same frequency channel by using clustering of vehicles. fig 3 shows the clustering scheme in vanets."
"(1) fast training, the training time is only slightly more than the interval of reading data. (2) no matter how complex the classification problem is, the optimal solution under bayesian criterion can be guaranteed through enough training data."
"this paper presents the design of an intrusion detection system (ids) to secure the external communication in self-driving vehicles. the proposed system periodically logs, calculates and stores different parameter values from neighboring vehicles to detect both abnormal or malicious behaviour in the network."
"in theory, the higher the decomposition level, the more complete and accurate information is reserved without missing the original features [cit] . however, with the increasing of the decomposition level, the computation time grows also significantly meanwhile. thus, the option of a proper decomposition level is significant, which considers both the number of candidate features and computation time. through the selected mother wavelet, a data-independent selection (dis) method is proposed [cit] to determine the wavelet decomposition level appropriately. the steps of the dis approach is based as follows."
"self-driving vehicle networks have some characteristics which make the design of ids complex. as previously identified above, vanets are much more susceptible to attacks than other networks because of their dynamic topology, open medium communication and absence of centralized security system [cit] ."
"in the frequency domain exist two vital system targets, the amplitude margin (am) and phase margin (pm), which represent the distance to the critical steady state of the closed-loop system and signify the robustness of the whole system against the disturbance. the decrement of both am and pm can forecast the potential fault and deterioration of the circuit system as an important indicator. as a result, it is significant to observe the performance of the recessive weakness in the frequency domain, including the cross frequency, amplitude margin, and phase margin."
"wpd in the proposed algorithm transfers the time domain information to the frequency domain. as demonstrated in figures 2 and 3, the performance of recessive weakness express similarity in time and prominence in frequency. wpd through the wavelet transforms both high and low frequency information to contain possibly more information of the original sampled signal. as discussed, we take the db10 wavelet as the mother wavelet and nine as decomposition level properly. the coefficient of each decomposition level constructs the energy feature vector including both tendency and detail information. the utilizing of wpd is for the construction of a feature vector to the classifier. the results of wpd can clearly distinguish the kind of recessive weakness."
"to create a real communication environment of autonomous and semi-autonomous vehicles on streets, two tools are utilised to generate traffic and mobility scenarios for these cars. this software is: simulation of urban mobility model (sumo) and mobility vehicles (move) and are commonly employed to generate vehicles scenario [cit] . the sumo software is utilised to simulate highway mobility because it is computationally efficient, flexible and utilises a high number of traffic vehicles [cit] . the highway mobility model is employed to evaluate/test the proposed security performance on highways/streets. the proposed ids in this paper has the ability to detect two types of scenarios are normal and abnormal behaviour."
definition: salt value is random number that integrated with messages from source vehicle to destination vehicle. this value plays important role in increase the protection of the communication system of autonomous vehicles.
"to better understand the nature of coherent multi-unit interactions both during intrinsic and stimulus-induced processing, it is necessary to have a language in which to study interunit interaction patterns. in related work in other species and other spatial scales, network science has proven its utility as just such a candidate language [cit] . the notion of a network in its simplest form is akin to the notion of a graph in the field of mathematics known as graph theory [cit] . specifically, an undirected binary graph is composed of nodes, which represent the units of the system, and edges, which link pairs of nodes according to some physical connection, functional relation, or shared feature [cit] . this simplest version of a network can also be expanded to include weights on edges, weights on nodes, dynamics on edges, dynamics on nodes, or multiple types of nodes or edges forming a multilayer or multiplex structure [cit] . by either the simple or expanded encoding, network models of neural systems seek to distill the most salient organizational features of the system, allowing investigations to focus on how the network topology constrains or supports the system's function [cit] . importantly, the network modeling approach is flexible in the sense that its components can be redefined at different spatial scales, and is thus equally applicable to cellular data at the microscale as it is to regional data at the large scale [cit] ."
"future trends of driverless cars are concentrated the safety of the overall traffic environment. the institute of electrical and electronics engineers (ieee) predicts that driverless cars will account for up to 75 [cit] . no driving equals more safety because there'll be no more text or drunk driving. in addition, cars without drivers are smart enough to avoid accidents. there must be enough sensor technology, enough computing power within the automotive and computer algorithms that detect the data output of sensor images, real-life traffic situations and give good feedback to the car. this is the technical challenge, and the manufacturers are working hard to generate new smart environment."
(5) adopt the competition function in the output layer according to the bayesian discriminant and the maximum output of the summation layer is the fault kind x j belongs to.
the paper is organized as follows: section ii provides at literature survey of intrusion prevention security systems in self-driving and semi self-driving vehicles. section iii describes intrusion detection in ad hoc networks and the clustering mechanism. section iv details the design methodology section v provides simulation results and analysis. section vii discusses outcomes of the scheme and while section viii concludes and provides a summary of future works.
"obtaining estimates of network flexibility requires the detection of communities using multi-layer modularity maximization, which depends upon two parameters,  and . these parameters control the resolution (size and number) of modules detected and their stability across layers, respectively. here, we use a recently developed procedure that allows us to obtain a representative sample from the parameter space defined by these two variables [cit] . for each such sample, we calculated a local (node-level) measure of flexibility, ranked the flexibility scores of all nodes, and subsequently averaged these ranked flexibility scores across all samples to generate an average flexibility profile for the population of cells. we note that each layer in this model represents the structure of a network estimated during different recording sessions. the multi-layer model makes it possible to aggregate these different networks into a single mathematical object rather than treating them as independent and disjoint estimates of the network [cit] . this framework has been used widely in network neuroscience for modeling nervous systems whose network structure evolves over time [cit], differs across individuals [cit], and spans multiple association modalities [cit] ."
"the achieved energy feature vector from wpd is of high dimension data, which may influence the computing time and classification accuracy. to perform the more adequate input of classifier, the high dimension vector needs to be reduced. pca can map the high dimension data to the low dimension feature space through linear transform based on the analysis of all features. after the dimension reduction, the most contributing feature vector will be reserved and remained. therefore, the low dimension space has the ability to reflect the most important information from the original data."
"this paper proposes a method that integrates the wpd technique combined with pca and pnn for the fault detection and classification of recessive weakness in a superbuck converter. the character of recessive weakness in a circuit is announced by simulation in both the time and frequency domains. the wpd provides an effective way to extract the character of the signal at different frequency bands. the proposed algorithm decreases the computational burden since the reduction of the input data is through pca. the pnn affords an intelligent method and criterion to the feature comparison. the pnn is then adopted to classify the extracted features and detect the recessive weakness automatically, with a strong ability for generalization and training mechanisms. a related simulation has been performed to identify the capability of this method in fault detection and classification and the consequence shows that prospective accuracy can be achieved to distinguish recessive weakness. the extracted features have excellent robustness to the different recessive weakness, including different sums of the changed component. the proposed fault diagnosis method is simple, accurate, and its effectiveness has also been verified through experimental results. refer to the purpose of this paper is to introduce the recessive weakness among soft faults, which is extremely difficult to detect and influence the performance of the circuit. this work will help to improve the application of the superbuck converter and to put the emphasis on recessive weakness as an important diagnosis index."
"this paper detailed the design of an ids to efficiently detect malicious vehicles and enhance the performance of vanets. an approach to detect sybil and wormhole attacks, which have an adverse effect on the communication and authenticity of self-driving vehicles. the designed ids aims to develop an ids that identifies and isolates malicious vehicles. each ch uses the ids to protect external communication of self-driving from malicious vehicles. they can compare the difference of various parameters obtained from different vehicles at regular intervals. parameters like distance, angle degree, forward value and number of hops play a crucial role in detection sybil and wormhole attacks in vanets. the distance and angle degree are unique parameters, obtained from each mobile vehicle at any time. moreover, if the distance between two vehicles is same, the angle degree of these vehicles differentiates the normal and malicious vehicles. the number of vehicles has an important role in increasing the detection time of abnormal behaviour in the external communication systems of these vehicles."
"vanets have some characteristics which can be the cause of security problems such as the absence of a fixed security system, open medium wireless communication, speed and a highly dynamic topology [cit] . these characteristics expose self-driving and semi self-driving vehicles potentially to many types of attacks, e.g. sybil, black hole, grey hole, wormhole and denial of service (dos) attacks [cit] ."
"pca in a proposed algorithm cannot only reduce the dimension of the feature vector but also lower the calculation amount. as demonstrated in figure 6, the first several decomposition level results of wpd to different kinds of recessive weakness is approximate. through pca, the dimension of the feature vector can be reduced to seven instead of nine finally, which still reserves the most network and also perform the function in de-noising. thus, pca is more conducive to the fault detection and classification of recessive weakness."
"self-driving vehicles are rapidly becoming a key autonomous systems technology. they can make a direct and positive contribution to our society by potentially reducing the number of accidents, cost and environmental impact of cars [cit] . autonomous vehicles attempt to replace humans by automating driving to reduce the number of fatalities and injuries on busy roads caused by human errors. self-driving vehicles rely on ad hoc networks, specifically so-called vehicular ad hoc networks (vanets). these networks enable flexible communication more flexible between vehicles within the radio coverage area."
"applying the wpd feature extraction method with 10db mother wavelet within parseval's theory, the detailed energy distributions in different decomposition level of the output signal, including normal condition and recessive weakness are represented in figure 5, respectively. the x-axis is the decomposition level while the y-axis the energy. the distinction between the normal situation and recessive weakness calculated through (10) is shown in figure 6 . the low dimension space has the ability to reflect the most important information from the original data. in this paper, the original dimension of feature vector is 9, we take target dimension as 7 according to the simulation results."
"we estimated functional connectivity from fluorescence traces. let x i (t) indicate the intensity of fluorescence in cell i at time t. next, we computed the cross-correlation of differenced fluorescence traces for every pair of cells:"
"biological systems generally and neural systems specifically, are frequently required to develop, adapt, and evolve in changing environments [cit] . this pervasive demand for adaptation is thought to be a partial explanation for the striking modular structure observed in biological systems [cit] . each module is thought to have the capacity to change or adapt without adversely impacting the function of other modules. in neural systems, modules are thought to exist in order to segregate specific cognitive function or computations, allowing enhanced specialization of the organism [cit] . such modular structure has also been observed in spontaneous recordings of intact zebrafish larvae, where topographically compact assemblies of functionally similar neurons reflect the tectal retinotopic map despite being independent of retinal drive [cit] . these data suggest that spontaneous activity displays modular structure that is a functional adaptation specifically tuned to support the system's behavior. similar observations have also recently been made in ferret visual cortex, where widespread modular correlation patterns in spontaneous activity accurately predict the local structure of visually evoked orientation columns several millimeters away [cit] ."
"the efficiency of this ids is assessed using ns-2 under two conditions: self-driving vehicles with ids and self-driving vehicles without ids. to evaluate the efficient of vanet with ids, we calculate the performance metrics of vanets, such as packet delivery ratio (pdr), packet delay and throughput [cit], as shown in fig 8: it is easily notice from the simulation results that this ids can play a vital role on enhancing the performance of the external communication in autonomous and semi-autonomous vehicles. the proposed security system can overcome one of the common security problems, which is due to the lack of fixed security infrastructures by using clustering mode in the security system in external communication system for self-driving vehicles external communication systems a virtual gateway of control. it is built on control data and sensitive information that sent/received between vehicles and their rsus on the road side."
6. reaction of chs -the ch will generate alarms and block the malicious vehicle to alert other vehicles in the inter-cluster and sends the same warning message to all chs and rsus in that zone.
"in addition, ids can be divided into three types based on their architectures, i.e. there are hierarchical, cooperative and stand-alone ids architectures. each ids architecture has related strengths and weaknesses. in our paper, we used a hierarchical architecture ids due to it significant capability in detecting malicious activities. the strengths and weaknesses of hierarchical architecture are as shown in table 1: traditional intrusion detection techniques based on auditing data and information can describe the behaviour of vehicles. the highly dynamic topology of vanets has a direct and negative on performance of ids, which consequently creates challenges for ids in collecting and analysing data [cit] . in this case, we design a hierarchical ids which can overcome dynamic environments through installing the ids on cluster head (ch)."
"the superbuck converter only adds one inductance and capacitance to achieve zero ripple input current and continuous output current. the structure of superbuck converter is shown in figure 1 . and the feedback loop consist of compensator, pulse width modulation (pwm), and power drive."
"such vehicles rely heavily on data that is exchanged between the vehicles and dedicated infrastructure, i.e. so-called road side units (rsus). vehicles cannot move or predict the external environment without data and control from beacons. to achieve this beacons periodically broadcast data from one vehicle to others in that zone. in this case, vehicles accept to receive data from beacons based on specific rules that are installed on vehicles or rsus [cit] . these rules are based on vehicle motions states such as speed, heading, position and time."
"where, tr (vr)-transmission range of self-driving vehicle vr. t-packet latency in vehicles. smax-maximum vehicle's speed while smin-minimum speed of vehicles. the ids algorithm relies on the following basic principles:"
"as the vital energy source of the power system in spacecraft, a switching power converter plays a crucial role in the success of a space mission. in recent years, the superbuck converter has gradually become the typical topological structure of the space power supply, because of its low emi (electro-magnetic interference) noise and capacitor current [cit] . more than 90 percent of spacecrafts domestically and overseas adopt such power systems for energy conversion. a fault in the circuit is unexpected and unavoidable, which is caused by many reasons, so it is of great significance to fault detection and diagnosis [cit] . the recessive weakness of component parameters in a circuit is even more complicated to identify and detect, presenting unsteady inner system performance without a dominant output character difference from the normal operating situation."
"to test the security system, we need to determine two types of behaviours: normal and abnormal. the wormhole behaviour leads to forwarding received packets to channel communication or private paths between two vehicles rather than resending these to the destination vehicle or rsu in that zone. the attacks are simulated in ns-2.35rc7 [cit] using the object tool command language (otcl) script to some files in simulator. during abnormal behaviour some files are altered in the aodv routing protocol in order to create sybil attack. the proposed ids is based on features that were extracted from the trace file and routing file generated from ns-2 as well as distances obtained for each vehicle. this is achieved by using x axis, y axis and z axis coordinates from general position system (gps), which is a basic sensor that we assume each self-driving vehicle is equipped with."
"in the previous section we demonstrated that, on average, hierarchical modular structure becomes increasingly dissimilar over time. however, it may be the case that some sets of brain areas maintain their modular structure despite the passage of time, forming a stable temporal core surrounded by a fluctuating and variable periphery [cit] . to test this hypothesis, we (a) analysis pipeline for comparing correlation structure. for any two correlation matrices, w u and w v, whose elements have been z-scored against those obtained under a \"jittered\" null model in which random offsets were added to timeseries (see methods), we vectorize the upper triangular elements and compute their similarity using a pearson correlation coefficient. we compare the observed correlation coefficient against that which we would expect under a null model in which rows and columns of w u are permuted uniformly at random. in panels (b), (c), (d), and (e), we show the scatterplots of standardized similarity scores for pairs of correlation matrices with the number of days separating their respective recording sessions. in panels (f), (g), (h), and (i), each point represents the standardized similarity scores of module co-assignment matrices across pairs of recording sessions."
"where a is the scale factor and  is the time shift. in continuous wavelet transform, a, b and  are all continuous in the time domain."
"there exist already some established fault detection and classification methods for different kinds of power converters. the discrete wavelet transform has been applied in the detection of current waveform in a direct current system [cit] . the shortcoming of this method is that all of the wavelet decomposition coefficients are chosen as the feature vector, which complicate the training process and increase the computing time. a support vector machine (svm) is used to diagnosis the soft fault in an analog circuit. but the final classification accuracy is far from satisfactory [cit] . the fuzzy math and the direction vector of the voltage increment are combined together in the diagnosis of the analog circuit fault. however, the accuracy of this method is easily influenced by the disturbances and noises [cit] . a method for online inverter fault diagnosis of a buck converter is designed to deal with the open circuit and short circuit fault rapidly and effectively. the soft faults are not considered, because they are more difficult and significant to be settled down [cit] . a model-based fault detection and identification method has been proposed for arbitrary faults in components within a broad class of switching power converters. however, the experimental design of the kinds of fault cannot cover all the possible conditions [cit] ."
"without taking the detail and approximation coefficient of the wavelet packet decomposition results immediately as the input of the classifier, the feature extraction is necessary to improve the process of classification. statistical methods, such as mean, standard deviation, rms, skewness and log-energy entropy are used as the feature extractors but show less robustness with noises [cit] . consequently, in this paper, we recommend taking the energy based method to feature extraction in virtue of parseval's theorem."
"the detection of temporal core-periphery structure suggests that there exists a small subset of nodes whose modular organization is preserved across time. to test whether this was the case, we repeated the procedure from the previous section, wherein we calculated the similarity of cell-to-cell correlations across multiple days. here, we calculated the similarity of connections among core nodes (the top 10% ranked by coreness) and connections among non-core nodes. if the core were indeed stable, we would expect that the connections within the core would be more stable across time compared to the connections within the non-core. indeed, we found that this was the case. for mice 1-4, we found that the similarity of connections among core nodes were 0.26, +0.92, 0.16 and +0.06, compared to connections involving the non-core nodes, which were 0.29, 0.52, 0.33, 0.76. on average, the difference in correlation between core and non-core was 0.410.38. this observation suggests that the core nodes are more temporally stable than the periphery nodes, both in terms of their community structure and in terms of their connectivity patterns."
"this difference transform is particularly appropriate when networks are constructed using pearson correlations, where time series are assumed to be stationary and to contain uncorrelated samples."
"in the frequency domain exist two vital system targets, the amplitude margin (am) and phase margin (pm), which represent the distance to the critical steady state of the closed-loop system and signify the robustness of the whole system against the disturbance. the decrement of both am and pm can forecast the potential fault and deterioration of the circuit system as an important indicator. as a result, it is significant to observe the performance of the recessive weakness in the frequency domain, including the cross frequency, amplitude margin, and phase margin."
"in theory, the higher the decomposition level, the more complete and accurate information is reserved without missing the original features [cit] . however, with the increasing of the decomposition level, the computation time grows also significantly meanwhile. thus, the option of a proper decomposition level is significant, which considers both the number of candidate features and computation time. through the selected mother wavelet, a data-independent selection (dis) method is proposed [cit] to determine the wavelet decomposition level appropriately. the steps of the dis approach is based as follows."
"additionally, we also computed spatial statistics for each module. past studies have shown that communities tend to be spatially co-localized, so that other cells located near one another are more likely to belong to the same module compared to cells located far from one another [cit] . to test whether this was also the case in our data, we computed the euclidean distance from each cell to the nearest cell assigned to the same community. we then averaged this measure over all nodes in the same module. if cells were arranged in spatially dense, compact modules, then this measure would be small. here, we calculated this measure for each module at every level of the hierarchy and compared these values against a null distribution generated by randomly and uniformly permuting the cell's spatial locations but preserving their module assignments. for each module, we expressed the mean nearest-neighbor distance as a z-score with respect to this distribution. we found that the observed modules tended to be more spatially compact than expected by chance. for each mouse, the median z-score was less than zero and in all cases the inter-quartile range of z-scores excluded a value of zero (fig 1g, 1h, 1i and 1j), indicating that the observed modules tended to be more spatially compact than expected by chance."
"4. detection phase -in this stage, the ids on cms has ability to detect the wormhole attacks from parameters that have been extracted from the routing table and trace file. the parameters are: number of hops, forward value and time. the ids on cms can identify the sybil vehicles from normal vehicles based on some important features such as distance, angle and vehicle id."
"accompanying the nascent use of tools from network science to understand interaction or connection patterns between neural units, there has been a marked interest in understanding the dynamics of interaction patterns as a function of time, and across a variety of different time scales [cit] . particularly in the human imaging literature, efforts have begun to understand principles of dynamic network reconfiguration on the time scale of minutes or hours [cit], days [cit], weeks [cit], months [cit], and years [cit] . here we exercise that interest in the domain of network models of correlation matrices derived from spontaneous activity in mouse auditory cortex over 2 to 4 weeks of experimentation. our findings suggest that quotidian variation in correlation structure is manifest at multiple scales: (i) at the level of cell-to-cell correlations, but also (ii) at the level of large-scale and module patterns in the network. this latter observation is particularly interesting to consider in light of findings at the macro-scale level of whole-brain networks derived from fmri data. specifically, at this large scale, much of the modular organization of spontaneous correlations in the human brain is conserved across the time scales of days and weeks, with notable flexibility largely present at module boundaries. one could speculate that gross temporal stability in macro-scale networks is underpinned by notable micro-scale variability. it would be interesting in future to more directly address the question of the functional role of this micro-scale network reconfiguration, and specifically to test the hypothesis that the correlation structure of fluorescence traces in mouse primary auditory cortex is reorganized over timescales of days to weeks to support cortical functional reorganization."
"a self-driving vehicle that would like to communicate with rsus or vehicles must be in cluster mode. in a clustering scheme, we need to identify just one vehicle to be the ch for the tdma communication. when another self-driving vehicle joins a cluster area, the group must select one vehicle as ch to manage the group and control transfer of data between multiple vehicles also vehicles and rsus. the success of this scheme depends on the existing cooperation between cms and chs, and this cooperation should be within the coverage area. in other words, the vehicles and chs should be inside the transmission range (tr) that help to report abnormal behaviour from vehicles to ch in that zone. the area of vehicle is calculated based on a formula given below in eq 3 [cit] :"
"in this paper, a new method combined with wavelet packet decomposition and a probabilistic neural network is designed for fault detection and the classification of recessive weakness in a superbuck converter. the characteristics and seriousness of recessive weakness are obviously demonstrated through simulation in both time domain and frequency domain. furthermore, the wavelet packet decomposition provides a more precise analysis method for signals with the continuous decomposition of both low frequency and high frequency data [cit] . moreover, the proposed approach uses the advantage of pca to reduce the dimension of feature vectors to achieve higher efficiency [cit] . in contrast to a bp (back propagation) network, the probabilistic neural network presents a greatly improved training speed and steady convergence to the bayesian optimization solution [cit] ."
"in other natural dynamical systems, it has been noted that density tends to support temporal stability, while sparsity tends to support temporal instability [cit] . in the context of networked systems, the notion can be expanded to describe the phenomenon in which a core of densely interconnected units tends to display weak or slow temporal fluctuations, while a periphery of sparsely interconnected units tends to display strong or fast temporal fluctuations [cit] . in the context of the human brain, this temporal core-periphery structure has been raised as a model for the balanced constraints of task-general processes, implemented by the temporal core, and task-specific processes, implemented by the temporal periphery [cit] . it is interesting to consider whether such a delineation into temporal core and periphery is also characteristic of cellular networks, and whether that separation is functionally meaningful in a similar sense. our findings suggest that, while calcium fluorescence correlation structure changes markedly over time, there remains a relatively small set of cells whose interactions, both as single connections but also as communities, are spared and preserved. there is some evidence in theoretical studies that such core-like structures emerge early in development, and are strengthened through functional activation [cit] . in analyses of macro-scale networks, core stability and peripheral flexibility have been associated with learning [cit], leading us to speculate that the emergence of core-periphery structure in micro-scale networks may serve a similar role in preserving learned (auditory) relationships, while maintaining enough variability to learn and map novel stimuli. thus, future work could be directed to investigate the functional roles of cores and peripheries during task conditions. our observations are not without precedent and are in line with other previous cellularlevel studies. for instance, in a study of tritonia motor programs, [cit] found that in addition to a \"core\" set of neurons that consistently responded to stimulation, the motor network expanded to include a peripheral set of neurons whose membership varied across trials. the authors went on to show that the variable set of neurons included those that were strongly coupled to the network at rest, suggesting that spontaneous coupling patterns predispose cells to be recruited into the motor program. similarly, [cit] distinguished between sets of cells as \"soloists\" and \"choristers,\" which respectively comprise those whose activity was distinct from that of the rest of the population and those whose activity was consistent with that of the bulk of the population. our finding of temporal core-periphery structure echoes both of these studies. in our study, however, we find analogous cores whose community affinity is preserved over days. although the functional relevance of the core-periphery structure reported here remains unclear, large-scale studies of brain networks have linked reorganization of these types of structures to learning [cit] . outside of neuroscience, cores are thought to represent polyfunctional units in a network, positioned where modules overlap with one another [cit], and to occupy positions of influence within the broader topology [cit] . future studies could better disambiguate the functional role of temporal core-periphery structure by linking those types of structures to behavior."
we used modularity maximization to detect network modules based on connectivity data [cit] . this method aims to divide network nodes (cells) into modules whose internal density of connections is maximally greater than what would be expected under a null model. this intuition is formalized by the modularity quality function [cit] :
the minimum number of the wavelet decomposition level that is necessary to obtain an approximation signal compared to the original signal therefore reveals that the upper limit of the associated frequency band remains under the signal frequency:
the fault detection schemes for the recessive weakness of a superbuck converter are still in the early stages of development as a result of the growing requirement of lighter weight and higher reliability. the presented method shows a high classification accuracy to recessive weakness throughout a series of experiments.
"when the power converter operates in the environment of extreme temperature and other complex application conditions, it may accelerate the parameter degradation of the components in the circuit. take the space power superbuck converter as an example, it has two capacitors, two inductors, one diode and one transistor. in total there are six key components to fulfil the circuit principle and meanwhile burden the circuit operation. if the soft fault of these components occurs without timely, accurate and effective methods of detection, it may lead to inestimable loss and serious catastrophe to the entire system and mission."
"the detection and classification method not only reduces the required data for the training of the network but also improves the speed of the calculation and operation. besides, the purpose of the proposed method in this paper is to obtain an available and practical detector and classifier as a fault diagnosis, which can recognize and distinguish the causes of the recessive weakness before the proper mitigating actions can be taken. moreover, the result of this method can be viewed as a forewarning towards the operation situation of the circuit system, accurately forecasting the trend of the converter in the future. there also exists system uncertainty in the proposed algorithm. some kinds of converters operate in dcm (discontinuous current mode) in the condition of heavy load. thus, the output voltage may be unsteady because of the switch in the operating mode [cit] . the discontinuous voltage influences the uncertainty of the algorithm. besides, the process of generating control command signals produce modeling uncertainties and other external disturbances [cit] . there also present uncertainties in measurement and processes which may lead to false-alarm situations [cit] . the parameter identification errors influence the data-driven fault detection design as well [cit] ."
"additionally, we used a multi-layer variant of modularity maximization that makes it possible to track the evolution, formation, and dissolution of communities across recording sessions [cit] . in this procedure, the standard modularity maximization equation is modified to read:"
"the aim of wpd is to develop the representation forms of the analyzed signal ( ) according to the scale and wavelet function [cit] . the wpd deals with not only low frequency but also high frequency and contains possibly more information of the original sampled signal. the procedure of wpd is shown in figure 4, where ( ) is the sampled form of the original signal ( ),  ( ) ( ) represent the high frequency and low frequency decomposition coefficient in each level respectively, is the decomposition level, is the array of the decomposition results."
"our proposed system has the ability to detect sybil and wormhole attacks by monitoring/ analysing the routing table and trace file that have been generated from the network simulator. the trace file describes the behaviour of the network through the send, receive, move, forward and drop packets. a possible further extension of the system is to design ids on rsus as well as designing ids with artificial intelligent (ai) techniques such as neural networks and knearest neighbour."
"spontaneous fluctuations in neural activity at the cellular scale can modulate behavioral responses to incoming sensory stimuli [cit] . yet the nature of that modulation is not well understood, in part due to the fact that such spontaneous activity does not appear to be random in nature, but instead displays heterogeneous dependencies or correlations among units. little is known about the rules constraining the architecture of these correlations, or their variability over time. here we sought to partially address this gap in knowledge by using recently developed techniques in network science to examine the network architecture of correlations in spontaneous activity in mouse auditory cortex as measured by two-photon microscopy and calcium imaging over the course of several weeks. we found that networks exhibited striking modular architecture, with smaller modules being located within larger modules in a multiscale hierarchy. we also found significant temporal rearrangement of modular architecture, as indicated by the fact that the similarity in modules decreased monotonically as a function of the time interval between recording sessions, even when only considering those units that were present in both sessions. finally, we found that the broadly observed temporal rearrangement of modules was complemented by the presence of a small number of cells whose modular allegiance remained stable throughout the 2-4 weeks of experimentation. we confirmed with additional testing that the co-existence of stable and unstable units was consistent with a temporal core-periphery model of system dynamics, where a stable core of units is accompanied by a flexibly periphery."
"fault in the circuits refers to the deviation of at least one feature order parameter in a system, which is beyond the acceptable range. the consequence is that the performance of the system is outside the normal level and is not capable of the expected function. analog circuit faults can be classified into a catastrophic fault and a performance degradation fault [cit] . the catastrophic fault is the open fault and short fault in the circuit, namely the hard fault; the performance degradation fault is the drift out of the tolerance range of the normal value in the component parameter without failure of the whole component function, namely the soft fault."
"in sum, there exist in total 17 situations to test and verify the proposed algorithm, which conclude the single, double and triple changes of the components within l 1, l 2, c 1 . all the variation in both sides of l 1, l 2, c 1 are settled from 5% to 15% according to the normal distribution in the standardized test, which is described by table 3 . table 3 . classification results."
"lastly, we asked whether a module's spatial features varied as a function of where it appeared in the hierarchy. to address this question, we aggregated z-scored nearest neighbor distance for each module along with modules' hierarchical levels. a hierarchical level of 1 indicates large communities comprised of many nodes, while the deeper hierarchical levels refer to divisions of the network into smaller communities. then, separately for each mouse, we computed the correlation of z-scored nearest neighbor distance with hierarchical level. we found negative association in three of the four mice, suggesting that deeper hierarchical levels, i.e. coclassification matrix generated using all statistically significant hierarchical levels. the dendrogram to the right depicts module splits. (c) the number of hierarchical levels aggregating data from all mice and all recording sessions. panels (d), (e), and (f) depict module assignments at different levels of the hierarchy. the network diagrams shown in these panels are identical to one another and represent binarized matrices obtained by thresholding the jitter-adjusted correlation matrix of fluorescence traces between pairs of cells. panels (g), (h), (i), and (j) depict distributions of z-scored mean intra-module euclidean distance for each module and for each mouse. panels (b), (d), (e), and (f) depict representative results from mouse 1. here, the acronyms \"iqr\" and \"med.\" represent interquartile range and median, respectively. note also that nodes in panels b and d-f are ordered according to their hierarchical community assignments."
"t ~dt : while the switch is turned off, the diode is on and the inductor obtains the continuous current through the diode and load resistor . the inductor charges the capacitor which causes the linear decline of inductor current. since the end voltage of is positive, the inductor, the diode and the load resistor constitutes a circuit. t ~t : while the switch is turned on, the diode is off and the input current passes through the inductor directly to the load resistor . the capacitor simultaneously depend on the inductor to spread energy to the load, the inductor, the capacitor, the switch and the load resistor forms a circuit, thus both the current of inductor and achieve the linear ascent."
"recently, many scholars proposed different types of encryption and authentication algorithms and finite state machines (fsms) to obtain secure communication and routing protocols such as saodv and aran [cit], but however, these types of secure routing protocols do not have the ability to resist internal attacks, where the attacker is already based in the vanet, such as sybil attack. this has motivated the design of a hierarchical ids to secure vanets presented here."
"system simulators are common tools used for evaluating intrusion detection in ad hoc networks [cit] . the performance and the reliability of proposed ids are studied by simulators without using real vehicles. to assess the proposed ids, we simulated vanets by using ns-2 to evaluate the performance of the proposed ids [cit] . this was chosen owing to its rich library, free availability and open source. otcl, tcl and awk scripts were produced to analyse the data that have been generated from the trace file and routing table. the performance of the ids is computed in terms of throughput, packet delivery ratio (pdr) and packet delay. table 2 gives the parameters of our simulation in ns-2."
"a related limitation concerns the properties of network connections or edges. here, we used fully weighted and signed representations to calculate network similarity (see fig 2) and sparse, binary networks to detect modules and core-periphery structure (see figs 1 and 3) . we adopted these particular edge definitions because they allowed us to compute similarity using the full distribution of edge weights and to avoid making future, somewhat arbitrary, decisions about the appropriate null model for weighted and signed networks in modularity maximization [cit] . nonetheless, there exists a spectrum of possible ways to define edges and their properties. future work is needed to understand the tradeoffs between different definitions and to ultimately ground edge definition in biophysical properties of nervous systems."
"the accuracy detection and false alarm rate depend on the number and type of parameters that are used while designing the detection scheme [cit] . in our system, we used four types of parameters: the routing table, distance, timestamps and forward value (fv). to obtain these parameters, each vehicle must collect data from its neighbour vehicles in inter-clustering. the following parameters describe normal and abnormal behaviours of self-driving vehicles in vanets:"
"7. performance metrics -in this stage, we evaluate the proposed ids by calculating the performance metrics such as the packet delay rate, (pdr) and throughput."
"here, to evaluate the proposed ids performance, we need to analysing efficiency, effectiveness, and calculate the performance metrics. first, table 4 is generated which describes the different parameter values that have been extracted, calculated and stored by each vehicle. table 4 demonstrates the sample database of vehicles that have been collected and calculated from routing table and trace file. according to table 4, sybil attacks are detected by using distance and angle. to detect the wormhole attacks, the forward value and number of hops are used. the simulation process is applied 20 times to evaluate/test of the proposed security and authentication systems in this research. in addition, the average classification rate is calculated in table 5 of two types of attacks targeting autonomous vehicles in vanets."
"the training and testing samples involve different kinds of recessive weakness (caused by two components simultaneously and three components simultaneously), normal situation, and the single change of the component. 30 samples are used for the training of each scenario with the proposed algorithm. 850 samples are utilized for testing, which cover the range of 17 fault kinds with 50 test samples."
"notice that condition (ii) in theorem 13 implies that we need to solve  1 lmis after constructing, which increase the difficulty of the numerical calculation if the size of the multiagent system is large. we give the following conditions, which can reduce the computational complexity for getting the dof control protocol by solving four lmis."
"after identifying the violation in the system, remedial actions are applied to alleviate the system abnormal conditions [cit] . therefore the main emphasis is on clearing the abnormality of the system due to the special contingency by either removing the failed component or rescheduling the generation unit and resupplying the load if the violation still exists, then load curtailment will be required."
"economic growth is measured in terms of increase in the size of a nation's economy whereas a broad measure of an economy's size is its output. the most widely used measure of economic output is the gross domestic product (gdp). it is generally defined as the market value of goods and services produced by a country. studies have consistently shown a close correlation between a country's gdp and her level of energy consumption per capita as indicated in figure 1. it shows a big gap in power levels (in kw/capita) between the low consumers such as india and africa and the highest consumer, the u.s.a. this correlation made the nigeria condition very pathetic because even among african countries, it has almost the lowest kw/capita value in addition to high unreliability status of its electricity supply. nigeria is beset with many economic challenges such as high unemployment, low industrial capacity utilization due to the cost of providing backup electric energy supply and low productivity. consequently the costs of locally produced goods are high and, hence find it difficult to compete with imported goods of the same type. this led to closures of local industries as is being witnessed in the textile, paper and cement industries. it has also led to flight of internationally owned industries to neighboring african countries where electricity supply is more reliable. nigeria is blessed with many natural resources such as gas, oil, coal, lignite and hydro power potentials and, if effectively harnessed and utilized, can lead to a very adequate and dependable power supply with attendant socio -economic development. nigeria is also situated at the equator with abundant sunlight and long peak sun hours. the effective utilization of solar technologies such as pv, can increase the per capita kw usage and increase industrial production and the well being of citizens. the power crisis in terms of inadequacy, leading to load shedding, and unreliable network resulting in frequent power supply interruptions has been a recurrent senario for the past decade. any effort to ameliorate this situation will greatly contribute to the economic and social well being of nigerians. the distribution system is a major component of the electric power supply network. experience has shown that outages are more frequent in the distribution system than either in the generation or transmission systems."
"enumerating the states in form of a tree graph, truncation of the states and contingency ranking is another technique which can be applied to reduce the number of the state for the system under study. it involves (1) enumeration of contingencies, including circuit outages and unit outages, and (2) enumeration of electric load levels according to a specified load model."
"the total cost of the energy not supplied for the same periods as shown in figure 10, which represents the revenue lost due to outages are n17,439,247,940; n28,718,370,004 and n61,131,521,807 respectively. as the pv systems were injected at various feeders and simulated, the total revenue loss reduced to n6,769,656,987, n12,718,370,004 and n21,359,660,013 respectively representing 61.2%, 55.7% and 65.1% reduction (in revenue loss) for the period under review. figure 11 showed the result of power flow carried out in the network which revealed that most of the injection stations transformers have been loaded above their capacities thus required immediate relief by connecting"
"the unreliability of power supply has adversely affected both economic and social lives of citizens. on the economic side, factories are operated below installed capacity because of the cost of running standby generators, thereby reducing productivity and leading to unemployment and loss of jobs. enterprises such as hotels and small scale industries can hardly make profit because of the cost of running stand-by generators. other consequences include environmental pollution such as noise and air pollution, dangerous emissions from sub-standard generating sets which have led to many deaths. this paper therefore considers the performance of the onitsha business unit distribution network for three years (2009 [cit] ), using analytical technique to quantify the performance of the network over the years by evaluating the power outage data from power holding company of nigeria (phcn) for various feeders that constitute the network under review. it also evaluated the effect of introducing pv/inverter interconnection with the distribution network for improved system reliability."
"it is known that the consensus is asymptotically achieved when there are no communication errors with the designed protocol (see figure 2 (a)). however, communication errors are inevitable. assume that a 1% error appears in all of the communication channels. simulation results show that, under the same protocol, the system diverges in the sense that the position state of each agent is far away from the position state of leader (node 1) as can be seen in figure 2 (b)."
"example 6 implies that, under the influence of communication errors, consensus cannot be achieved for each agent with the given control protocol. this provides motivation to design an appropriate dof control protocol to attenuate the effects of communication errors on the consensus performance. in this paper, we assume that there exist communication errors in the transferred data; that is, the dof control protocol takes the following form:"
"two sets of reliability indices, customer load point indices and system indices have been established to assess the reliability performance of distribution systems. load point indices measure the expected number of outages and their duration for individual customers. system indices such as saidi and saifi measure the overall reliability of the system. the third popular index most utilities have been benchmarking is caidi. these indices can be used to compare the effects of various design and maintenance strategies on system reliability. saifi can be improved upon by reducing the frequency of outages (tree trimming and maintaining equipment), and by reducing the customers' interruptions when outages occur (through provision of reclosers and fuses)."
"one of the main stages in reliability assessment is to analyze the impact of the possible failures that may occur in a practical system on the performance of the overall system. for instance how the overloaded transmission lines influence the overall power system, is an important issue in reliability study of the bulk power systems. network solutions can be applied to perform such system characteristics, abnormal state and the required the remedial action in form of corrective action or load curtailment to clear the abnormality."
"it further investigated the factors affecting the reliability of the distribution network and proffered ways of improving the electricity supply to stimulate industrialization, enhance increased productivity, create job opportunities for the unemployed, attract more investors and consequently lead to improved living standards of citizens."
"the figures 5, 6 figure 8 on the other hand, gave the expected energy not supplied (eens) by various injection substations; whereas the total eens by the entire distribution network for the periods under review are given as 2336368 , 3168856  and 4108399  respectively. figure 9 showed the revenue loss for the injection substations without pv system and when the pv schemes are installed."
"and is the element of the laplacian matrix . in order to characterize the effects of the communication errors on consensus performance, we need to define a controlled output for the multiagent system (12) as follows."
"remark 15. theorem 13 gives the sufficient conditions under which there exists a dof control protocol such that the multiagent system (12) achieve consensus with a given  performance. when the conditions are satisfied, the procedure to construct the dof control protocol is presented as follows."
one of the significant drawbacks of applying the markov techniques to achieve the reliability model is the extremely large number of generated states which assigns a large computational effort for reliability evaluation.
"embarking on electricity distribution facilities automation will no doubt lead to improved system reliability, since the federal government of nigeria has been investing massively in power generation and transmission infrastructures in addition to licensing of some independent power producers for improved generation capacities."
"motivated by the above-mentioned works, we study the consensus problem for linear multiagent systems to attenuate the communication errors by using dynamic output feedback controller. the agent dynamics considered here are general stabilizable and detectable linear systems, and a dynamic consensus protocol is proposed which uses only the relative output information between each agent and its neighbors. the main contributions of this paper can be summarized as two aspects. firstly, in order to describe the effects of communication errors on consensus, a concept called consensus with  performance is introduced which can characterize the effects of communication errors on the difference between the state of each agent and the average of states of all agents. the problem of consensus with  performance is transmitted into an  control problem of another reducedorder system. it is shown that consensus with  performance can be achieved if there exists a common dynamic output feedback controller which can be realized by solving  problem for 1 linear dynamic systems simultaneously, where is the number of agents. secondly, in terms of the  1 linear systems, a sufficient condition based on linear matrix inequalities for the existence of the controller is provided, and the approach to construct the corresponding controller is given."
"mahmud and saeed [cit], presented the results of a preventive maintenance application-based study and modeling of failure rates in breakers of electrical distribution systems and examined the impacts of preventive maintenance on failure rate of selected network connectives. in this paper, distribution system reliability indices which were evaluated using analytical method include:"
the distribution system improvement requires assessing the performance of the system as it exists. this means quantifying the reliability of the distribution network and evaluating the effect of reinforcements and other improvements.
"irrespective of the performance of both the generation and transmission components, the smooth operation of the entire power system may be marred by unr eliable distribution network."
"the research has provided the performance data required by power utility management and prospective investors to make policy and business decisions. the findings will equally avail the utility companies the reliability status of each substation and hence, provide a standard for scheduling maintenance and substations upgrade."
"this paper is devoted to the consensus problem for multiagent systems molded by linear time-invariant systems under fixed directed communication topologies and subject to communication errors in the transferred data. a dynamic output feedback control algorithm is proposed. the theoretical analysis shows that if there exists a common dynamic output feedback controller which can solve  problem for  1 linear time-invariant systems of order, then the consensus with a desired  level can be reached. by using  theory, a sufficient condition in terms of linear matrix inequalities is given to ensure the existence for such a controller. a procedure for the controller design is presented."
the rest of this paper is organized as follows. section 2 introduces basic notations and reviews some useful results on graph theory and robust  control theory. section 3 formulates the problem and conditions for reaching consensus with  performances that are derived. the existence for a dynamic output feedback protocol and a method to construct such controller are proposed in section 4. numerical simulations are provided in section 5. section 6 concludes the paper.
"an important and basic stage is to generate the appropriate reliability model. the system model can be generated by applying the markov process, which is a stochastic and memory less process whereby the present state of the system is independent of all former states except the immediate preceding one [cit] . the process assumes that the transition rates are constant."
"the acu manages the tss admission while maintaining the qos of the already admitted ones. when a new ts demands an admission, the acu first obtains a new si as shown in the previous step and computes number of msdus arrived at the new si using equation (3) . next, it calculates the t xop i for the particular ts using equation (4) . finally, acu admits only the ts if the inequality in equation (5) is satisfied."
"in the case of having more than one ts at qst a i, the size i will represent a total summation of all next frames lengths in bytes. when no data packet is received in the current si due to packet loss, the t xop i of qst a i will be computed as expressed in equation (4). it is worth noting that at the first cap of any ts, the txop is calculated based on equation (4) because no information about the next packet size has been reported yet. instead of sending a single poll"
"in this section, we describe the characteristics of the mimic-ii clinical database [cit] . we also explain how we use these data for our study related to chronic diseases."
"laboratory events and chart events of each patient are summarized into one feature vector. due to the heterogeneity and the different frequencies of the selected medical data, we propose the following approach for the feature extraction according to the type of the measured values:"
"as known, any period of time, t, contains a number of consecutive sis, m, which can be computed for any experiment duration t as t /si. thus, the average aggregated end-to-end delay for the a period of time t can be calculated as follows:"
"ranking loss evaluates the average part of reversely ordered label pairs, for the observation. the score lies between 0 and 1, where 0 corresponds to the best result:"
"assume that the transmission time of a single and multi-polling frame is t poll and t mpoll, respectively. thus amtxop calculates the total txop in any si as follows:"
"based on the discussed deficiency of the hcca accommodating to the fast changing in vbr traffic profile and the solution proposed in atxop in section 2.5. when qst a i, at any si, exploits only portion of its allocated t xop i at the traffic setup time, namely t i ef f, leaving an unspent amount of t i u . thus, the following relation can be held:"
"is the total txop scheduled in any si used in hcca and atxop, respectively. it is worth noting that t d i is calculated from equation (16) which includes the poll overhead."
"the dataset presents a label cardinality of 2.37 and a label density of 0.237, with 1023 possible combinations, of which 522 are present in the dataset."
"the simulation results were validated on low and high quality jurassic park 1 video sequence using the setting in tables 3 and 4 except for the simulation time which set to 50 sec. the traffics start at second 20 from the beginning of the experiment run to leave an ample time for initialization process. for this reason, the number of sis (m ) was 750 which calculated as 30sec * 1000/40ms."
"numerical variables consist of measured values such as blood pressure, creatinine and temperature. when they appear one time, such as the height at the patient admission, they are taken in the feature vector as they are. when they appear several times, the following summary features are computed: mean, median, standard deviation and range (max-min)."
"regarding the hardware environment, we used a workstation equipped with an intel core i7 cpu 870 at 2.93 ghz and 16 gb of memory (ram). concerning the training time, as shown in table 6, although the cpu has 4 cores (8 threads due to hyperthreading), only 1 core is used in practice by the fact that the java implementation of the algorithms is single thread."
"average precision evaluates the average fraction of relevant labels ranked above a particular label y a y i . the score lies between 0 and 1, where 1 corresponds to the best result:"
"the behavior of the examined algorithms in terms of allocating txop in each si is illustrated in figure 6 for the formula 1 video sequence. the allocated txop for one flow is shown against a number of sis for a duration of 30 seconds. the results reveal the fact of assigning fixed txop in hcca for all sis of the flow with accordance to equation (4) . in this case, the hcca computes txop duration based on the maximum msdu size of the flow, namely 7032 bytes and 14431 bytes for low and high-quality video respectively. nevertheless, the both atxop and the proposed scheduler alike adaptively allocate a txop for each si based on the actual frame size obtained from the feedback information which show that in some sis the allocated txop duration in hcca is much higher than the actual need of the flow which considered as over-allocation cases. it is obvious that the txop duration given in 6(b) is higher than that in 6(a) as the mean bit rate of highquality encoded formula 1 is considerably higher than that in low-quality video sequence, refer to table 4 ."
"direction of algorithms exploiting ensemble methods and label power-set methods should be considered. we experienced that, in the particular context of large medical datasets, it is more convenient to use algorithms with few parameters. by the use of sequential data, in order to maintain their interpretability, we decided to use a summary statistics approach for the feature extraction rather than an unsupervised approach such as vector quantization techniques. we think that for this dataset, the conception of a well trained binary relevance method is sufficient to obtain a decent model. more sophisticated multilabel learning methods, such as the promising rakel method, can bring an added value but at the cost of a greater complexity."
"broadcast a multi-polling frame 17 end for each station, the hc compose and broadcast one multi-polling frame which includes aid and computed t xop i ."
"where l i is the nominal msdu length for the i th qsta. then the txop duration of the particular station, t xop i, is calculated as the maximum of the time required to transmit n i msdu or the time to transmit one maximum msdu at the minimum physical rate r i, as stated in equation (4) ."
"to validate the behavior of the examined algorithms, the measurements are done for an increasing number of tss. the system throughput was also investigated to verify that the improvement in the delay is achieved without jeopardizing the channel bandwidth."
the rest of the paper is organized as follows. section 2 illustrates the reference hcca mechanism and its deficiency in supporting vbr video streams and demonstrates some of the hcca related works. section 3 illustrates the proposed dynamic algorithm. the performance evaluation including simulation and analytical results is discussed in section 4. section 5 concludes the work presented in this paper.
where n is the number of admitted tss and m si i is the maximum si of the i th stream. the si is computed so that it satisfies the condition in equation (2) .
"where the denominator, x, is an integer number that divides the beacon interval into the largest number that is equal or less than the m si min ."
"assuming that the transmission of the traffic for all qstas begins as the same time, the end-to-end delay of a qst a i can be computed as the total txop durations of the preceding stas in the polling table including the transmission time of msdu of the qst a i . for the sake of the simplicity, we assume that the stations are scheduled in ascending order according to their index. that is, the preceding delay of qst a i in an si is computed as in the following equation."
"recent years have witnessed a rapid growth of ubiquitous applications in the internet with a vast spread of multimedia streams. this makes providing differentiated quality of service (qos) for such applications in wireless local area networks (wlans) very challenging task. besides, several wireless technologies have been risen from amongst them ieee 802.11 [cit] has assumed as a de facto standard in wlans due to some of its key features like deployment flexibility, infrastructure simplicity and cost effectiveness [cit] . in iee 802.11 wlans, the qos of multimedia communications cannot be efficiently achieved due to frequent collisions and retransmission [cit] . ieee 802.11 introduces two channel access modes, namely distributed coordination function (dcf) and point coordination function (pcf). the former is the mandatory medium access method which is appropriate to serve best effort applications such as http, ftp and smtp. multimedia streams that require a certain qos level are served during the controlled mode (i.e. pcf) since it provides a contention-free polling-based access to the channel to provide the demanded qos. however, due to the fact that pcf only operates on the free-contention period, which may considerably cause an increase in the transmission delay, especially with high bursty traffics it considered not efficient for serving the applications that required high qos constraints. therefore, ieee 802. 11 task group e (tge) has established ieee 802.11e protocol [cit] which then introduced a revised version [cit] with new technical enhancements on mac and physical layer."
"the qos support of ieee 802.11 standard has been extended in ieee 802.11e by means of hybrid coordination function (hcf). enhanced distributed channel access function (edcf) which extends dcf, provides a prioritized qos throughout its distributed access manner to the wireless medium. hcf controlled channel access (hcca) which extends pcf that works based on a centralized polling mechanism to provide differentiated service, according to rigid qos parameters negotiated with the centralized coordination (hc). edca introduces a random access to the wireless medium by means of access categories (acs). the traffics are mapped to acs according to their priority. every ac will be associated with a backoff timer so that the highest priority acs will go through a shorter backoff process. despite edca provides qos support, it is still not efficient for application with rigid qos requirements."
"in order to evaluate the performance of the amtxop scheduler, we have used a network simulation tool. the simulation environment setup, and video traffic used as uplink traffics is described in details in this section. the performance of our scheduler is compared against the hcca. the results of end-to-end delay and throughput are also discussed."
"the worst case scenario when all qstas used their t xop i duration that is, having given different d i si for hcca and atxop, the end-to-end delay of both schemes can be computed as in equation (18)."
"where t preamble, t p lcp, t mac and t l is the transmission time of plcp preamble, plcp header, ieee 802.11e mac header and data payload (bytes) respectively which can be calculated as follows:"
"distance based algorithms, such as ml-knn, may be affected by distortions on distances due to the scale of features can be different. then, we compute the z-score for each element of our feature vectors set, as defined below."
"in this section we describe the feature extraction and the standardization that we apply on the data, then we describe the multi-label learning algorithms considered in this study."
"our recommendation regarding the analysis of these results concerning medical data and chronic diseases is to choose decision tree based algorithms, because they performed well across all evaluation metrics and scale up to large dataset. however, if the scenario of the hamming loss is more important, when we want to achieve the best performance regarding the identification of all diseases a patient may be affected, regardless of their rankings or their significance fig. 1 . the multi-label learning algorithms divided into groups using the categorization presented in the background section. levels, the best algorithms are those based on svm with br, but with the inconvenience to be slow at training time. another consideration to make is that in the case of chronic diseases, like in mimic-ii, pure multi-label algorithms such as ml-knn or adaboostmh do not seem to have an advantage to the br approaches. on the other hand, the bad performance of the nb algorithm gives us a direction: in multilabel medical domains, the correlation between the features is an important characteristic to take into consideration."
"in classic learning approach of multiclass problems, the evaluation is done through common metrics such as accuracy, precision, and recall. in multi-label problems, the evaluation is more complicated and need extended evaluation metrics. the following five evaluation metrics [cit], that are described below, are commonly used with multi-label problems. note that these evaluation metrics consider all the set of labels, this is not a per label evaluation, thus the results are sensitive to the distribution of labels in the dataset."
"where g i is the generation time of packet i at the source qsta, r i is the receiving time of the particular packet at the mac layer of the qap, and n is the total number of packets for all flows in the system. the endto-end delay has been measured for the three video types to study the efficiency of both hcca and our algorithms with different traffic variability. figure 7 (a), 7(b) and 7(c), 7(d) depict the delay experienced by data packets for the low, medium and high-quality video, respectively. one can notice that the end-to-end delay boosts with the increase of the packet size, the highest quality exhibits higher end-to-end delay and vice versa. the increase of the delay in higher quality videos can be justified by the large amount of the allocated to each ts, as in equation (4), which leads to maximize the wasted txops that keep the subsequent tss awaiting in their transmission queue longer time. it is obvious that the proposed scheduler achieved delay enhancement over both hcca and atxop for the examined videos due to the polling overhead reduction in the system. the proposed algorithm achieved delay enhancement up to 12% over the adaptive txop algorithm [cit] and about 59% over hcca scheduler. the reason of achieving better improvement in jurassic park 1 is the higher packet size comparable to that in formula 1 thereby the granted txop obtained in hcca is far from the needed txop which in turn causes higher packet delay. furthermore, the delay improvement in our scheduler is justified by the accurate calculation of the txop. unlike the hcca scheduler that only relies on the mean traffic characteristic which is not reflecting the actual traffic behavior."
"the clinical data we consider are the laboratory tests and the items registered in the chart. by chart, we mean a logbook per patient which records the results of heterogeneous examinations, such as fluid assessment, physiological measure, or severity score which evaluates vital functions. important information such as the age and the gender of the patient is part of the chart. according to the length of the stay, a patient will make several laboratory tests and various examinations. thus, clinical data of patients are time series. in order to attenuate the amount of missing values, we take a subset of items, from the laboratory tests and from the chart, that are present at least for 80% of the patients. we end up with 76 items from the laboratory test and from the chart. a detailed table, with the descriptive statistics, is available in appendix a. note that we do not apply any feature selection algorithms, since these algorithms will select a subset of features that will optimize the accuracy for this particular database. instead, we want to keep this work as general as possible to allow generalization to other clinical databases, in particular icu databases."
"regarding the software environment in use, all the multi-label learning algorithms and evaluation metrics have been implemented with the java programming language. the following java libraries have been used: mulan 5 (version 1.4) and weka 4 (version 3.7.6). the operating system is a ubuntu linux 12.04 lts 64 bits."
"although hcca guarantees a qos for video traffic based on the required tspec parameters, there is a probability to have frames smaller than the mean negotiated msdu size. consequently, a larger txop than needed will be assigned to a qsta causing wasting in wireless channel time and remarkable increase in the end-to-end delay. figure 2 (upper part) illustrates the effect of assigning txops for vbr traffics based on the mean tspec parameters on increasing the packet delay and on the poor wireless channel utilization. suppose there are three stations sending uplink video traffics to qap. hc will accordingly assign t xop 1, t xop 2 and t xop 3 to qst a 1, qst a 2 and qst a 3 respectively. assume that in any si some or all the frames sent is considerably smaller than the negotiated msdu for the ts, in this case the qsta will only utilize a portion of the scheduled txop for sending its data as explained in the si i and si i+1 . the next scheduled txop will initiate according to its scheduled time, regardless the actual exploited time in the previous txop causing increases in the delay and wasting the channel time as well. this issue may be noticeably severe when the number of qsta with vbr traffics increase. moreover, in the transmission of the pre-recorded video, the traffic behavior is known prior to the traffic setup [cit] . these observations motivate us to present an enhancement to hcca scheduler in which hc exploits information sent by qsta about the changes in the traffic profile so as to accurately assign txop to qsta and advance the consecutive txops to minimize the delay and add the residual wireless channel time to edca period. the proposed scheduler is presented in details in the next section."
this section presents a description of ieee 802.11e hcca scheduler and some mpeg-4 video characteristics. the deficiency of hcca in supporting vbr is illustrated and some related works in enhancing its performance are also discussed.
"among the evaluation metrics that we consider, the hamming loss is for us the most important one because it represents the ability of the algorithm to discriminate the symbols associated to the illnesses that the patient has. the second one is the ranking loss as it concerns the ranking of the labels according to the dominant disease of the patient."
"in order to ensure the accuracy of the calculation, the propagation delay, d p, between the source and receiving channel should be considered which is normally a small fixed value [cit] as shown in table 3 . accordingly, analytical study of the end-to-end delay for examined schemes will be discussed in the following subsections."
"one-error evaluates the fraction of top-ranked labels not part of the relevant label set. the score lies between 0 and 1, where 0 corresponds to the best result:"
"note that the data is not missing at random according to the documentation of the database. 3 thus techniques such as interpolation or imputation will not give appropriate results. several approaches exist for handling the problem of missing values in medical datasets [cit] . a trivial approach is to substitute the mean for the missing values [cit], however this is rarely an acceptable solution [cit] . a better approach is to look at medical knowledge to substitute with values within a plausible range [cit] . given these considerations, we consider a plausible range of physiological values in the case of missing data according to information we gathered in the medical literature. we either impute physiological values in ranges that are plausible given the patient disease or we impute physiological values of a healthy person if appropriate. as labels we consider 10 families of chronic diseases where their distributions among the 19,773 extracted patients are presented in table 1 . we use the coding scheme of the international classification of disease revision 9 (icd-9) 2 available in the mimic-ii database for building the 10 families of chronic diseases as described in table 2 . providing a definitive diagnostic is not realistic in our settings. the icd-9 codes system, which is especially used for morbidity statistics or health insurance systems, is not sophisticated enough to define a label for a precise diagnostic. in addition, even when a diagnostic is available, for a given disease the treatment is often specific for each patient, due to different symptoms, results of other examinations, interaction of medications, or allergies. based on these considerations, we decided to form 10 families of chronic diseases by taking into account the medical relevance, the characteristics of the dataset and the hierarchical structure of the icd-9 coding system. providing information on belonging of a patient to one or several disease families will support the physician by suggesting the directions for further investigations. related to the definition of labels, it is important to compute the label cardinality and the label density. label cardinality quantifies the number of alternative labels that characterize the observations in the dataset on average. with respect to label cardinality, label density considers also the number of labels. the two measures are useful because multi-label algorithms may present a different behavior in datasets with similar cardinality, but different density."
"the scheduler calculates the service interval (si) as the minimum of all maximum service intervals (m si) of all admitted traffic streams which is a submultiple of the beacon interval, the time between two subsequent tbtts. the minimum m si for each qsta is obtained from equation (1) ."
"categorical variables consist of observed values such as cardiovascular function assessment score and urine color. for a patient which did several times a particular examination where results are discrete values which can be divided into mutually exclusive classes, we can represent this information as a histogram. then, the relative frequency of each category of the histogram is used as feature. there is also the case where only one observation exists for each patient, such as the gender at the patient admission, in that case, we encode as feature the value in a binary variable."
"more particularly, with highly variable bit rate applications, it is likely one of two cases happen at some certain sis. one case is when the application data rate goes up in which the allocated time is not sufficient to empty the transmission queue of the uplink traffics at the qstas which in turn results in increasing the end-to-end delay. there are two possible solutions to remedy this problem [cit] . one is by maximizing the txop duration more than that allocated with regards to the average txop of the traffic. however, it will much degrade the bandwidth utilization when the data rate drops down. the other one is by applying a bandwidth reclaiming scheme [cit] . the other case is when data rate drops below its average value declared in the tspec of the ts. in this case, the assigned txop is not totally exhausted. this issue is addressed in this paper."
"the results reveal that the proposed mechanism remains stable minimizing the delay even with increasing amount of packet loss. one can notice that, the increase of end-to-end delay of the packets in high-quality videos, 13 and 14, compared to that of the low-quality ones, 11 and 12, for the same reasons discussed in section 4.2.1."
"in ieee 802.11e, the channel divided into occurrences of subsequent superframe which includes free period (cfp) and contention period (cp). cfp is reserved for centralized transmission, which controlled by hcca function. however, controlled access phase (cap) can be initiated during cfp and cp as well when the medium remains idle for at lease a pifs time. this property is one of the key merits of hcca that make hcca overcome the legacy pcf function in that it shorten the time the packet waits in its transmission queue. the scheduling process of qstas is discussed in details in the next section. figure 1 demonstrates an example of hcca transmission during cfp and cp periods."
"the large size of the mimic-ii dataset allows us to divide the dataset randomly in three parts where each contains enough observations for fitting optimally a model. the first part, called training set, is used to build all models across all parameters. the model selection is done using a grid search over a defined parameter space, described in table 3 . the second part, called validation set, is used to select the best parameters for each algorithm. finally the third part, called test set, is used for computing the reported results according to the best parameters. table 3 shows the parameters selected by the grid search for each of the algorithms. this process, which consists of the permutations of the three sets, is repeated 6 times. the reported results, in tables 4 and 5, are the mean of the six iterations under a confidence interval of 95%. the process schema of the experiment is presented in fig. 2 ."
"the amtxop scheduler has been implemented in the well-known network simulator (ns-2 ) [cit] version 2.27. the hcca implementation framework ns2hcca [cit] has been patched to provide the controlled access mode of ieee 802.11e functions, hcca. the ns-2 traffic trace [cit] agent, is used for video stream generation."
"in this appendix, we present the list of variables from the mimic-ii dataset that is considered in our study. the descriptive statistics presented below are computed across all measured values for all patients. in table a .1, the descriptive statistics (mean, median and standard deviation) for the quantitative values are given. in table a .2, the descriptive statistics (frequency) for the categorical values are given."
"performance evaluation throughout simulation experiments and analytical study reveals the efficiency of the proposed scheduler over hcca and atxop. moreover, the schemes were evaluated with mobility scenarios to check the robustness over harsher environments. thanks to multi-polling concept, the total txop duration assigned in amtxop was 32% and 25% less than that in atxop in low and high videos, respectively. this remarkable improvement in the channel utilization has led to significant enhancements in the packet delay. the amtxop achieved delay enhancement up to 12% and 59% over the atxop and hcca, respectively. moreover, the gain in the channel utilization is also considered as bonus merit."
this work shows the significant impact of integrating the multi-polling scheme to the existing piggybacking mechanism in the vbr traffic scheduling. this integrated scheme reduces the packet delay and enhances the channel utilization as well.
"the rest of this document is organized as follows: section 2 presents a background on evaluation metrics and methods for multi-label learning; section 3 describes the mimic-ii database and its properties; section 4 defines the methodology for building models; section 5 presents the results for the multi-label algorithms considered in this study; finally, section 6 concludes this paper and draws the lines for future work."
"mll differs from classical machine learning by tackling the learning problem from a different perspective. in contrast to the classical classification tasks where each observation belongs to only one mutually exclusive class, in mll decision areas of labels (i.e. classes) overlap. this aspect leads to the annotation (i.e. instead of classification) of observations with zero, one or several labels. in addition, instead of expressing the presence or the absence of a label as a binary variable, it is possible to express the confidence of the presence of a label through a score or a probability. this formulation looks natural for many problems in real life, such as the detection of emotions in music [cit], the semantic scene classification [cit] or the classification of text into topics [cit] ."
"quite surprisingly we discovered that the most used multilabel learning algorithms, with the exception of rakel, do not improve the results with the respect of a binary relevance approach that makes the independence assumption of the chronic illnesses. it is difficult to give a complete explanation about these results, however we can provide two opposite reasons for this behavior. the first one could be that the feature extraction process cannot model optimally the correlation between the features in order that multi-label learning approaches can exploit this information. the second one could be that the extracted features are sufficient for discriminating the various illnesses. to confirm this, we think that further studies are required with algorithms and processing methods that can deepen the analysis of dependencies of physiological measurements and chronic diseases. furthermore, in view of the promising results of the rakel algorithm for the ranking of diseases, we think that further developments in the table 4 results for the hamming loss, the ranking loss and the average precision."
the aggregate throughput of the examined algorithms has been investigated against the number of stations. this is to verify that our scheduler is efficient in supporting qos for vbr traffics which maintaining the utilization of the channel bandwidth. the aggregate throughput is calculated using equation (8) .
the end-to-end delay is defined as the time elapsed from the generation of the packet at the source qsta application layer until it has been received at the qap and is expressed in equation (7).
"upon the reception of the first frame from a qst a i at hc, which includes the msdu size of the next frame, the hc will start maintain this information in stations list until the next cap begins. at that time, the txop duration will be computed as expressed in equation (6) ."
"thus, the delay of qst a i in an si is computed as follows, eventually, the delay enhancement in any si when multi-polling scheme is integrated can be expressed as follows:"
"where m is the maximum msdu and o denotes the overhead, including mac and phy headers, inter-frame spaces (ifss), and the acknowledgment and poll frames overheads."
"hc allocates a txop to each admitted qsta so as to enable it to transmit its data with regards to the negotiated qos parameters of the tspec. for the i th qsta, the scheduler computes the number of msdus arrived at the mean data rate  i as in equation (3) ."
"in order to investigate the effect of the mobility on the performance of the examined schemes, we used similar topology to that discussed in section 4.1 except that the stations have been placed to be moved within four diameters each with corresponding phy rate setting as in table 7 . it is worth noting that in scenarios a, b and c, the stations experience higher data rate as they move towards the ap and vice versa. these scenarios have been investigated with different movement speeds, namely 5 and 30 meter per second. the results in figure 16 (c) reveals the robustness of the amtxop compared to atxop and hcca (figures 16(a) and 16(b) to minimizing the access of each ts using multi-poling scheme. the hcca shows high delay due to its deficiency to adapt to the real time changes in which the txop is computed as a fixed duration at the traffic setup time and used during the traffic life regardless to the actual need of the traffic. increasing the mobility speed to 30m/sec as illustrated in figure 17 shows marginally difference compared to that in figure 16 as the destination (ap) is only one hop far from the source. one can notice the effect of increasing the phy on the number of admitted flows which were 4, 8, 13 and 18 tss in 6mbps, 18mbps, 36mbps and 54mbps scenarios, respectively. where, b hcca is the channel utilization of hcca which can be calculated as the proportion of fixed txop to the si, whereas b proposed is for atxop and amtxop. figures 18 and 19 demonstrate the channel utilization improvement as the average of all admitted flows. the atxop achieved utilization improvement over hcca of about 49% and 56% for the low and high quality videos, respectively. whereas, thanks to multipolling scheme, the amtxop achieves up to 56% and 66% over the hcca."
"this section begins with the formal definition of a mll problem and their related evaluation metrics. then, a state-of-the-art of the existing mll techniques is described."
"coverage evaluates how many steps are needed, on the average, to go down the list of labels so as to cover all the relevant labels of the observation. a score as small as possible is better:"
"problem transformation methods transform a multi-label learning problem into several single-label classification problems which allow the use of existing machine learning algorithms. problem transformation methods can be divided into three groups: binary relevance methods, label power-set methods and pair-wise methods."
"simulations have been carried out to exhibit the performance of the examined algorithms, namely hcca, an adaptive txop assignment which we refer to as atxop onward and the proposed algorithm using a different variability level of the same videos. since the main objective is to achieve superior qos support by accurately granting txop to the station to fit its need, packet end-to-end delay of the uplink traffics has been measured which considered as one of the significant metrics to evaluate a qos support of video streams."
"in the future, we intend to design an admission control unit (hcca) to utilize the surplus bandwidth gain using amtxop mechanism and manage these bandwidth resources among the hcf functions, edca and hcca for maximizing the number of admitted flows in the system and study the affect of increasing the admitted flows on the tcp flows of the edca. moreover, an enhancement will be introduced to support live video transmission by exploiting the perfecting concept and govern the resulted additional delay budget. the other possible future direction is to enhance the proposed scheduler to cope with more sophisticated wireless conditions and with the mac level fragmentation and multirate adaptation support."
"), beacon interval and t cp is the duration reserved for edca. the hc sends an acceptance message (addtsresponse) to the requested qsta if the condition in equation (5) is true or send a rejection message otherwise. the accepted ts will be added to the polling list of the hc."
"algorithm adaptation methods are modifications of existing machine learning algorithms for solving multi-label learning problems. the modification allows the new algorithm to handle directly multi-label data. adaptations for multi-label learning have been proposed for adaboost [cit], k-nearest neighbors [cit], decision trees [cit], neural networks [cit] and support vector machines [cit] ."
"hamming loss is defined as the fraction of the proper labels to the total number of labels. the score lies between 0 and 1, where 0 corresponds to the best result:"
"a star topology has been used for constructing the simulation scenario to form an infrastructure network with one qap surrounded by varying number of the qstas ranging from 1 to 12. all qstas were distributed uniformly around the qap with a radius of 30 feet as shown in figure 5 . the stations were placed within the qap coverage area, in the same basic service set bss, and the wireless channel is assumed to be ideal. since we focus on hcca performance measurement, all the stations operate only on the contentionfree mode by setting t cp in equation (5) to zero. the qstas communicate directly without a hidden node problem. rts/cts mechanism, mac level fragmentation, and multirate support were disabled. qap is the sink receiver, while each qsta is a video source because only one flow per station is supported by ns2hcca patch. therefore, for simulating concurrent video streams multiple stations are added each with one flow. in order to leave an ample time for initialization, stations start their transmission after 20 (sec) from the start of the simulation time and last until the simulation end. wireless channel assumed to be an error-free, and no admission control used for the sake of investigating the maxi- mum scheduling capability of each examined algorithm under heavy traffic conditions. simulation parameters are summarized in table 3 . for evaluating the performance of our scheduler against the reference scheduler of hcca, jurassic park 1 video sequence trace encoded using mpeg-4 was chosen from a publicly available library for video traces [cit] . we tested the amtxop scheduler on jurassic park 1 and formula 1 trace files which can be classified into movie and sport, respectively, which show different variability level. table 4 demonstrates some statistics of the examined traces. the selected video is encoded using different compression ratio, which results in varying quality. in this paper, we have tested the algorithms with low and highquality video trace. it is worth noting that the variability of the selected videos is measured by the coefficient of variation (cov), which is the standard deviation of the frame size divided by the average frame size. tspec parameters used for each video traffic are shown in table 5 with regards to video qos requirements."
"for transformation methods which need a base classifier, we decided to use support vector machines (svm) [cit], naive bayes (nb) [cit] and decision trees (j48) [cit] . the choice of svm with an rbf kernel is motivated by its ability to model non-linear problems where classes are not linearly separable. svm needs two parameters, the first one is the coefficient of the rbf kernel (), the second one is the penalty of the error term (c). the choice of nb is justified by the fact the algorithm works well in many scenarios due to its simple structure and its surprising classification performance, even with the conditional independence assumption [cit] . similar to nb, decision trees (using j48 implementation) are simple algorithms that scale optimally to large dataset, and furthermore they provide an easy explanation of the rules used for the classification. j48 needs two parameters, the first one is the confidence threshold for pruning (p), and the second one is the minimum number of instances per leaf (l)."
"by looking at the results, in tables 4 and 5, we can say that decision trees perform optimally considering all the evaluation metrics, and they have the following advantages: they scale well to large datasets and they are easy to interpret. svm-based approaches give the best accuracy for the hamming loss but they do not rank well the illnesses, and in addition, they take a very long time to train when the gramian matrix is big, as in the case of the mimic-ii dataset, as shown in table 6 ."
"where size i is the received packet size at the qap, time is the simulation time and n is the total number of the received packets at qap during the simulation time. figure 8 (a) depicts the aggregate throughput with increasing the network load for the high-quality jurassic park 1 videos. the result shows that the throughput is the same as that achieved by the hcca scheduling algorithm. this implies that the proposed approach enhanced the end-to-delay without jeopardizing the channel bandwidth."
"chronic diseases, also called noncommunicable diseases (ncds) [cit], are characterized by a long duration and generally a slow progression. widespread chronic diseases include cardiovascular diseases, chronic respiratory diseases and diabetes. chronic conditions are a major concern for public health programs of governments, particularly due to their negative effect in the continuous growth of medical care costs [cit] . chronic obstructive pulmonary disease (copd) is an incurable illness, mainly due to tobacco smoking, where the treatment merely slows the progress of the condition. the world health organization (who) estimates that 64 [cit] . concerning another major chronic disease, diabetes affects 347 [cit] . who projects that diabetes will be the 7th [cit] . type 2 diabetes consists of 90% of people with diabetes, and is mostly the consequence of excess body weight and physical inactivity [cit] ."
"the throughput achieved by the proposed mechanism by the presence of packet loss is depicted in figure 15 . the proposed mechanism and hcca perform similarly attaining the same throughput. indeed, the achievable throughput is degrading as the per increases due to the increase of packet loss on the system."
"the binary relevance (br) method [cit] splits the multi-label learning problem into several binary classification problems using the one-against-all strategy. since the br method has a major drawback to not consider correlation between labels, the classifier chain (cc) approach [cit] was introduced as an extension. in a similar way to the br method, the cc approach involves a binary transformation for each label, and the extension concerns the addition in the feature space of binary variables for label relevances of all previous classifiers, thus resulting to a classifiers chain which considers a form of correlation between labels."
"in this paper, a novel scheduler called amtxop has been proposed to leverage the performance of the controlled access mode of ieee 802.11e. the enhancements of amtxop is two fold. first, the hc polls qstas with regard to fast changing in the traffic profile so as to prevent qstas from receiving unnecessary large txop. second, the surplus time of the wireless channel is conserved by reducing the number of poll frames using the multi-polling approach."
"the rakel algorithm, being an ensemble method, suffers from scalability problems as we have a large dataset. rakel extends the lp method by breaking the original set of labels in several random subsets where lp is used internally. although rakel improves substantially over lp, the drawbacks inherent of lp remain. moreover rakel needs two additional parameters which have been defined as fixed values according to the recommendation of rakel's authors [cit], since in the case of our study we have a large dataset. in particular, according to table 6, a grid search over 4 parameters (instead of 2) with an svm as the base classifier would have been impracticable under our available computing power. however, despite the scalability issue, rakel with an svm as the base classifier obtains competitive results to the br methods, in particular it improves the results for the average precision and the ranking loss, which is relevant for the ranking of the labels according to the dominant disease of the patient. in addition, rakel with an svm as the base classifier achieves the best performance, over all other methods considered in our study, for the one-error evaluation metric which evaluates the reliability of the top-ranked label."
"after considering several alternatives we settled for the six gestures shown in figure 1 . these gestures require a small movement of the hand to tap once, or twice, the thumb with the index, middle or ring finger. we originally considered an additional two gestures that involved the little finger, but our initial efforts showed that it was difficult to discriminate these gestures with those using the ring finger. these gestures fulfill the criteria defined above, but their subtlety makes it challenging to recognize them. dementyev and paradiso report on their efforts to recognize the gesture made by taping the thumb with the index finger [cit] . we next describe our proposed approach to recognize these gestures."
"the user should be able to perform the gesture in different locations, while performing a variety of activates, while in different body postures (laying down, sitting, running, etc.) and in different social circumstances (in a meeting, at home, walking in the street, etc.). thus, the gestures should be discrete and subtle; the user should be able to make the gesture without others noticing it."
"the study design is within-subjects, in which participants are asked to perform the 6 gestures using a smartwatch that records the signals from the accelerometer and gyroscope. two android smartwatches were used by each participant: an lg g100 and an asus zenwatch 2."
"the precision obtained for 6 gestures (81%) seems is low for the application, it would generate a significant number of false positives. however, considering only 3 gestures the precision improved to 91% which seems practical for several circumstances, particularly considering that participatory sensing is often done with high frequency and a few data points incorrectly labeled might have little impact. in addition, the application could decide to query the user a second time if it estimates that the answer provided is inconsistent with the readings from the sensor."
"the acceleration signal is first processed to eliminate the acceleration due to gravity, which depending on the position of the hand could affect all axes. a low-pass filter is used to reduce the effect of this component."
"labeling mobile sensing data is a task that requires considerable effort and is error-prone, particularly if the data is captured in naturalistic conditions. one approach is to capture data in controlled conditions, such as in a laboratory environment. in such a setting labeling can be done by the research team with the help of specialized equipment. for instance, the individual can be made to walk at a certain speed on a treadmill; asked to prepare tea in an instrumented kitchen; or be monitored while sleeping using polysomnography with specialized equipment."
"this article proposes an approach to data labeling based on self-report from the individual conducting the activity using the recognition of simple gestures. the approach follows the experience sample method [cit], in which the subject under study wears a smartwatch, and receives a notification, either in the form of vibration or sound, requesting him for a label about his current activity or mood. the subject responds with a subtle gesture that demands little cognitive and physical effort and it is also discreet and thus can be used in various social settings without others being aware. for instance, the system could at random times query the subject about his current social setting (i.e., alone; with work peers; with family and/or friends; with strangers), or when the device infers from accelerometer data that the subject is moving it can ask him to confirm this and indicate the mode of transportation (i.e., not moving; walking; running; in a car/bus). the rest of the paper describes the subtle gestures proposed for labeling, the methodology used to classify the gesture, the classification results, and a discussion of the results and application of the proposed approach."
"wearable computers are increasingly being used to infer activities. some of these devices, notably smartwatches, already detect gestures to activate diverse functions, for instance turning the screen on when the wearer raises and turns his arm in the direction of his face, signaling that he wants to look at the watch."
"once the signal is classified as having a gesture using dwt we proceed to determine where the gesture starts and ends. the signal (magnitude of angular velocity) is divided in windows of approximately 240 ms each. the average angular velocity of each window is calculated and a new, compressed signal is produced with each average value per segment. a low and a high threshold were empirically determined to estimate the presence of a gesture in the signal. when the magnitude exceeds the high threshold we establish the possible presence of the gesture and a low threshold is used to establish where the signal starts and ends. figure 2 illustrates this process with the signal of a double-tap with the index. a value above the high threshold is initially detected in data point 15, indicating the presence of a gesture. the window for the gesture is established between data point 9 and 45, the first and last values below the low threshold. if the length of the potential gesture segmented is less than 200 ms the signal is considered too short to include a gesture and the user is queried again to repeat the gesture. this minimum length of time was estimated empirically. to eliminate potential peaks in the signal produced by sporadic movements we confirm that the start and end of the signal is no less than 220 ms away from a threshold above 0.4 in angular velocity. if this is not the case, the length of the signal is adjusted as shown in figure 3 . similarly, if the signal lasts more than 1300 ms it will be rejected. once the signal with a potential gesture is detected we proceed to extract relevant features to be used for gesture classification. the signals from each axis obtained from each sensor are combined to produce 14 signals (7 per"
a total of 15 participants were recruited to gather data to train the classifiers. inclusion criteria for participants included: age between 10 and 60; no previous experience using a smartwatch; and being right-handed. as criteria for exclusion we considered having known motor problems that could cause excessive movement in their arms/hands.
"requiring subjects to attend a lab to capture and label data can be expensive. furthermore, ecological validity can be compromised by the fact that the subjects are being observed or the instrumentation can interfere with the manner in which they perform the activity. for instance, sleeping in a lab with sensors attached to the body vs. sleeping in your own bed and subject to normal environmental conditions (noise, light, the presence of others, etc.)."
"we performed an additional evaluation using just the 3 gestures that require only a single tap. the average accuracy improved to 91.46% using svm. wristflex is an array of force sensitive resistors worn around the wrist, which was developed to infer gestures similar to the ones proposed here [cit] . a test was conducted to recognize 6 gestures, including the 3 we propose, plus taping also the little finger, a relaxed hand, and an open hand with fingers spread, with 10 individuals. while the approaches could not be directly compared given the slight differences in gestures used the accuracy obtained was similar. yet, our approach does not require the use of specialized hardware, but rather uses standard commercial smartwatches."
"we describe the criteria proposed to define the gestures to be recognized, those that were selected and the approach proposed for their recognition as well as to assess its accuracy."
"another common approach is to ask the subject under study to perform her own labeling. this could be done with the help of mobile systems programed to capture labeling data. one such example would be a smartphone app that requires the subject to press a button just before going to sleep and right after waking up. this approach has some drawbacks, including the fact that the subject could forget to indicate that the activity is being performed, or he can do so at a later time. the labeling could also interfere with the activity, for instance in the case of a subject who stops cycling to take his phone out and indicate that he is riding a bicycle."
"the next step is to segment the signal that includes the gesture. the recording of the gestures initiates right after the query is given (through vibration of the device or sound), but the subject might wait a short amount of time before it performs the gesture. from the analysis of signals of the gestures we estimated the maximum amount of time required to perform the more complex gesture (double-tap with the ring finger) and add 1.5 s to consider the time it takes the user to react after perceiving the query. this time window guarantees that the gesture will be contained in the signal, except in cases were there is considerable delay by the user in performing the gesture. the dynamic time wrapping (dtw) algorithm is used [cit] to make an initial assessment of the presence of a gesture by comparing the signal with that of a sample gesture. we found the signal from the gyroscope to be more stable and thus, the comparison is made with the signal registering angular velocity. the comparison is made with total angular velocity, which adds the contribution from the 3 axes, to account for differences in orientation when performing the gesture [cit] ."
"activity, and behavior recognition has become one of the most active areas of research in ubiquitous computing. the field makes use of data gathered using mobile, wearable and/or environmental sensors to create models capable of inferring the activity being performed by an individual [cit] . these models are often obtained by training supervised classification algorithms. thus, the data needs to be labeled in order to have \"ground-truth\" for training and testing the model or classifier [cit] ."
"each participant is asked to perform each gesture twice in each posture, for a total of 36 gestures per individual. sound was used to indicate to the user which gesture to perform next. a researcher used a smartphone connected via bluetooth to the smartwatch to initiate and control the intervention."
the individuals performed the gestures while in three different postures: standing with the arms facing down; standing with the arm bent and the watch facing the user; and siting down with the arm resting on a pile of books on top of a table (to provide support).
"the 208 features obtained from each signal are normalized in the range of [-1, 1] to avoid for features of greater magnitude to dominate the classification."
"two sensors in the smartwatch will be used to characterize the movement in the wrist associated to each gesture: accelerometer and gyroscope. the accelerometer measures the acceleration at which the sensor moves in three axes, while the gyroscope measures orientation and angular velocity in the same axes. one advantage of using a smartwatch is that it is normally worn in the wrist of the left hand of the individual and the x-axis coincides with the direction of the arm with positive values towards the fingers, while the z-axis is perpendicular to the screen of the device."
"we described an approach to data labeling for activity recognition thru gesture recognition with a smartwatch. data labeling remains an open problem in activity and behavior recognition, which often use supervised classifiers. we proposed 6 subtle gestures that take little time and effort to perform and that could be enacted in a wide variety of postures and social settings. with 6 different labels participatory data labeling can be done for a variety of mobile sensing applications which might require the user to confirm her current activity/behavior, or respond to binary, tertiary or likert-type queries."
a limitation of this work is that the gestures were performed in control conditions; we expect accuracy to decrease when the gestures are performed under naturalistic conditions.
"this con!rms that the three last !ngers cannot be used to control something independently of the index !nger, even if they are used together as a whole. such dependencies induce difficulties for users to efficiently control the hand 10 dof, and decrease this upper bound around 4 or 6 dof (two or three independent fingers)."
starting finger positions for deformation tasks we already observe that starting positions of fingers is relevant in order to disambiguate navigation from object manipulation tasks. further investigations about fingers starting positions have been performed for object deformation tasks.
"this experiment was conducted on a 22'' multi-touch display by 3m (473 \" 296 mm, 1680 \" 1050 pixels, 60 hz, 90 dpi). the software was based on the qt and ogre3d libraries. 31 participants, composed of 8 women and 23 men, were tested. average age was 30 (min. 22, max 49). all participants had normal or corrected to normal vision. for left-handed participants, the experiment was mirrored. participant's background was variable, and not only computer scientist background. participants' experience with 3d applications, and tactile devices was variable, but this was not an issue, as the goal of the experiment was to get some understanding of fundamental physical behavior."
"when participants use their non-dominant hand, their fingers typically remain far away from part of the object that is deformed (even sometimes at the opposite side). by performing such gesture, participants keep in place the object, while she/he works on a region of interest of the object -such as designers keep in place their paper while drawing [cit] ."
"noticeable gestural design pattern as we already observed, a majority of users performed ambiguous gestures, and therefore the interface need some disambiguation between modes. on the opposite, we note that some manipulations can be linked together, enabling us to identify typical gestures and a gestural pattern for each mode. once phases are analyzed, hand gestures on a surface can be easily classified into 5 main classes (fig. 10) . due to scaling interferences, no gesture is identified when all three phases are detected. on the contrary, the detected gesture is rotation 2. in table 1 (last column), we associate each task of the user study to the corresponding typical gestures for the first hand."
"however, the dominant hand gestures are typically performed around the deformed object. for instance, on bending tasks, the thumb position corresponds to the center of rotation, and remains static, while a rotation gesture is detected by phase analysis (table 1) . local scaling (such as stretching or compression tasks) is typically performed by a shrink gesture, where the gesture barycenter is located nearby the center of the part of the object that is being deformated."
"a second clue for disambiguation is the number of fingers used. the average number of fingers involved to navigate is about 3 while this number decreased to 2 for object positioning. though, the non-dominant hand gives the most relevant number of fingers: 1 finger used for navigation, no finger for object positioning and 1 or more for object deformation. in a large proportion, the non-dominant hand fingers reached the border of the screen for navigation tasks when it has a support function."
"group selection another issue investigated is how a transformation could be applied to a couple of objects. the same gesture was usually performed for both objects (# 75% of users), each object involving one hand. but this does not scale to more than two objects, and cannot be applied to gestures requiring both hands."
a !rst look at the traces produced by participants' !ngers con!rms an intuitive hypothesis: hand gestures on a table can be decomposed into global motion phases ( fig. 3 ) and some local motion phases.
"the !rst manipulation tool humans ever use is their hand, which enables them to touch, grab, pinch, move, or rotate many objects. thanks to multi-touch devices, these abilities are nearly extended to the digital world. before creating a 3d user interface for multi-touch device, understanding the hand gesture is mandatory. two aspects need to be studied: the hand gestures themselves, and the mapping between these gestures and tasks."
"since the simulation results are based on three routing protocols, specifically, dsdv, dsr and aodv, a brief detail of these protocols is given below along with their performances in ieee 802.11 ad hoc networks."
"requests for accurate dynamic response analysis using a simulation tool have been increasing in many engineering fields, including ship building industry. especially in the ship building industry, it is very important to predict the delivery day of the ship to its owner. therefore, for process planning designers in ship yards, accurate dynamic response analysis is becoming more important during the shipbuilding process. for this reason, the ship yards designers use commercial programs when they receive requests for dynamic response analysis. however, these methods have some limitations. the commercial programs for dynamic analysis are usually developed for general purposes, so that they may not be suitable for various requirements of process planning in ship yards."
coh and hachet recent research lead them to another approach of understanding gestures for manipulating 3d contents [cit] . their paper focused on object positioning tasks.
"in dsdv, the entries in the table are indicated by numbers assigned by the destination node. these numbers act as status indicators of the nodes which therefore minimizes routing loops. routing update packets are transmitted throughout the network to maintain table consistency. these packets indicate which nodes are accessible from each node and the number of hops required reaching the destination nodes using distance-vector algorithms. these update packets can result in large amount of traffic. two types of update packets are present in dsdv based networks. the first one which is infrequently transmitted is called the full dump. this type of packet carries all available routing information. the second type called incremental packet is used to forward only that information which has changed since the last full dump. both update packets have fixed size network protocol data unit (npdu)."
"the analysis process performed for the first experiment was reproduced with little differences. however, we had to adapt the hand parameterization to the number of fingers in contact with the table. contrary to the first experiment, where each finger could be identified by the starting position, all interactions did not always involve the five fingers (e.g., the thumb was not always used)."
"the first experiment demonstrated that the thumb is usually the most stable finger (this was our reason for using it as origin of the local frame). therefore, we assumed the thumb to be the one that was moving the less (this assumption can be wrong when the gesture is a translation, but this is non-issue, since all the fingers are being moved the same way in this case). the other fingers do not need to be distinguished. we also had to perform two distinct phase analyses, one for each hand, to interpret the gestures."
"recurdyn is the three-dimensional simulation software that combines dynamic response analysis and finite element analysis tools for multibody systems. it is from 2 to 20 times faster than other dynamic solutions because of its advanced fully recursive formulation. various joints and external forces can also be applied to the multibody systems, but recurdyn cannot handle discontinuous state variables, event triggered conditions, and state triggered conditions."
"highly dynamic destination-sequenced distance-vector routing (dsdv) is another protocol which is a table-driven protocol used in wireless networks [cit] . various performance parameters for these protocols have been explored including packet fractional delay (pdf), average delay, throughput, goodput, normalized routing load (nrl) and routing overhead (ro). considering the challenging and demanding aspects by the modern wireless systems, the broadband wireless access industry, which provides high-rate network connections to stationary locations and end stations, has advanced to a point at which it now has a robust standard for second generation wireless metropolitan area networks. ieee 802.16 standard which is well known as worldwide interoperability for microwave access (wimax) is the solution for such wireless networks [cit] . when this technology is considered for mobile networks, it is expected to provide around 15 mbps of channel capacity within a particular cell. in this paper, we have evaluated the performance of dsr, aodv and dsdv in wimax networks and studied various performance parameters for such networks. the remainder of this paper is organized as follows: section 2 gives a brief introduction to wimax networks and the standards being used at the medium access control (mac) layer. in section 3, details of dsr, aodv and dsr routing protocols are presented. the performance analysis of the three routing protocols in wimax networks has been carried out in section 4. finally, the conclusion of our work is summarized in section 5. the standard defines the specifications related to the service-specific convergence sublayer (cs), the mac common part sublayer (cps), the security sublayer, and the physical layer. the mac management messages are implemented to operate the wimax networks. all operations between the base station (bs) and subscriber station (ss) over a super frame interval follow the procedures of the 802.16 standard."
"based on the previous results, fig. 3 shows the performance of the best protocol dsdv (i.e. pdf performance wise) for different number of connections and varying data rates. the pdf for dsdv as the number of connections and data rate vary has no large impact. but for the average delay, variation of delay is produced when the data rate is 2 packets/s. but as the data rate increases, the system becomes more stable in terms of end to end delay. on the other hand, as the number of connections increases, the nrl decreases for a fixed data rate. this is because the total number of data packets increases as the number of connections increases (i.e. larger number of nodes sending packets) and becomes much larger than the routing packets. also, from fig. 3 (d), the number of routing packets is almost the same for all the connections."
"a !rst goal of this paper is to evaluate the upper bound of the number of dof that can be simultaneously controlled by a hand on a multi-touch device. this is done through an experiment that con!rms and re!nes what our common sense, as well as what the corpus of current multi-touch interaction techniques tell us: the number of dof of the hand on a surface is between 4 and 6."
"therefore, the different modes could be automatically distinguished during user interaction by mixing these two criteria: a finger-count method [cit] would give the selected interface mode, while finger locations could tell to which object the interaction is to be applied, if not to the whole scene."
"using routing protocols in this section, we have investigated the performance of the three protocols in wimax networks. the results shown in fig.1 are obtained using the following parameters: cbr traffic type with 50 nodes having 20 maximum connections. the nodes maximum speed is 20 m/s and generating packets at the rate of 4 packets/s. the simulation is carried out for 900 simulation time and using a topology size of 1200 by 300. in fig.1 (a), the pdf for dsdv outperforms that of both dsr and aodv. for highest mobility, almost 75% of the packets are delivered in case of dsdv. further, almost 100% pdf is achieved at 500 pause time. the pdf performance for both on-demand routing protocols is very poor. but as shown in fig.1 (b), the average delay is high in dsdv compared to both dsr and aodv for the first 30 seconds of pause time after which the delay is almost the same for all protocols. similarly, fig. 1 (c) shows the goodput for all the protocols. goodput is the number of data packets successfully sent and received by the entire network within a certain period of time which is proportional to pdf. as expected, dsdv has the best goodput and outperforms the other two protocols."
"scaling interferences during most tasks, the participants performed scaling phases while performing their interaction. in many cases, the dof involved by scaling was meaningless. for instance, fig. 9 illustrates the gesture of a participant during a navigation task: a translation in the (x, y) plane. in this illustration, more than 90% of the motion was analyzed as translation phase, while short scaling phases occurred in parallel. as stressed when analyzing the first experiment, the stable and useful part of scaling motions usually takes place when the translation and rotation phases of a motion have ended. therefore scaling phases should not be taken account when they occur concurrently to other phases, and the gesture in fig. 9 should be interpreted as a bare translation."
"to analyze gestures and dof, using a new hand parameterization, we successfully decomposed gestures into elementary motion phases, such as translation, rotation and scaling phases. such phase analysis method permits us to investigate fundamental behaviors of hands and gestures."
"basically, the ieee 802.16 mac protocol [cit] was designed for point-to-multipoint broadband wireless applications. high data rates at both uplink and downlink is considered to be the main objective of this standard. resource allocation algorithms are used for channel sharing among users. the 802.16 mac layer supports continuous and burst nature traffics. the first sublayer in the mac layer is the convergence sublayer which is used to map different traffics to a form compatible with wimax mac traffic type. the second layer is the common part sublayer which provides the core mac functionality of system access, bandwidth allocation, scheduling, contention mechanism, connection establishment, and maintenance. it receives data from various sources through the mac service access points (saps), which is classified to particular mac connections."
"the position of each !nger in the local frame is parameterized by a couple (r i, s i ) -for rotation and scale-where r i is the angle de!ned by the !nger of the local frame (i.e., the angle between the thumb/fore!nger direction at the starting position and the thumb/!nger direction at the current position); and s i is the ratio between the current distance to the thumb of the !nger and its distance to the thumb at the starting position ( fig. 2 ). with these de!nitions, a simple translation of the hand keeps the couple (r i, s i ) constant (only the origin of the local frame changes); a rotation of the hand changes all the r i by the same amount but does not impact the s i . in contrast, a pinch gesture will only impact the s i, making them decrease from 1 (!ngers at the same distance from the thumb than while resting in the starting position) to a value smaller than 1 (!ngers closer to the thumb)."
"to further validate this order of manipulations, we can look at the number of phases needed to validate the trials. fig. 4 summarizes those results: for more than 93% of the cases, users need a single translation phase to correctly position their hand; while a correct rotation is achieved within a single phase for 68% of the trials and a correct scale for 35% of the trials. thus we think that hand gestures can be decomposed into sub-parts that have different degrees of stability: from the most stable motion (global translation) to the less stable motion (one finger motion). for instance (fig. 5), the global translation is the easiest to get right (1 phase only), without any interference afterwards. on the contrary, global translation could induce interferences on rotation (first rotation phase), before that the major rotation motion is performed (second rotation phase). as translation and rotation are simultaneously performed, sometimes rotation motion has to be corrected (third phase)."
"another kind of 3d manipulation involves a widget that acts as a proxy to the real object. 3d transformation widgets are commonly used in 3d applications. a recent example of 3d transformation widgets for multi-touch devices is the tbox [cit] . to easily manipulate 3d objects, they are enclosed in their bounding box that is made interactive. this is an extension of the standard manipulation widget (represented by 3 arrows). the existing manipulations on objects are translation, rotation and scaling. all user gestures have to involve the cube widget -specifically the vertices, edges or faces of the cube. for instance, pushing a single edge performs rotations, while translating along edge performs translations. a shrink gesture on both sides of the tbox widget represents a single axis scaling."
"for the uplink traffic, each ss should range to the bs before entering the domain. in the first ranging duration, a burst record is established between ss and bs which includes interval usage code (diuc) to the bs. similarly, after that, uplink interval usage code (uiuc) is going to be agreed upon between bs and ss. the downlink-map and uplink-map which contain the channel id and the map information elements (ies) describes the physical layer specification. the burst profile includes the diuc, uiuc, and the type-length-value (tlv) encoded information. the burst profile also includes type-length-value (tlv) encoded information. the tlv encoded information will notify the physical layer of the modulation type, (forward error control) fec code type, and encoding parameters (e.g. coding rates). the final mac data payload is packed by these encoding types. the ieee 802.16 uses the frame-based transmission architecture where the frame length is variable. these frames called superframes are divided into two subframes: downlink subframe and the uplink subframe."
"a. dsdv this is one of the table-driven routing protocols based on the bellman-ford routing mechanism. in table-driven routing protocols [cit], the main objective is to maintain consistent and up-to-date routing information from each source node to other destination nodes in the network. each node maintains one or more tables to store the required routing information. these tables are updated according to changes in network topology by propagating update information throughout the network. two key elements are important in such protocols, the number of routing tables and the update method being used."
"instead of simultaneously/sequentially manipulating the different 3d contents, fewer participants (# 20%) preferred to first select the object by clicking (or double clicking) before manipulation. only two users performed a \"lasso\" gesture to select object before performing the transformation. after the object selection, the gesture was performed either on one of the object, or near the barycenter of the group. this leads us to conclude that a specific widget should be created to represent the selected group."
"wire rope therefore, dynamics kernel was developed, which can automatically generate the equations of motion of multibody systems for accurate analysis of dynamic systems. to deal with multibody system in a discontinuous environment, the multibody dynamics kernel is integrated into the discrete event simulation framework, which is developed based on the discrete event system specification (devs) formalism. devs formalism is a modular and hierarchical formalism for modeling and analyzing systems under event triggered conditions, which are described by discontinuous state variables."
"this value of 10 dof is clearly an upper bound of the actual number of dof that a user can simultaneously manipulate with a single hand. several evidences show that the actual number is lower: so far, no multi-touch interaction uses the positions of the !ve !ngers of a hand to control 10 parameters of the object being manipulated. our common sense tells us that our !ngers are not totally independent, since they are linked by the hand, and moreover that even for movements that would be physically doable, we can hardly control each !nger independently."
"we ran a second experiment to understand the most natural mapping between 2d gestures and 3d tasks. recent researches have focused either on navigation tasks (e.g., [cit] ) or object positioning tasks (e.g., [cit] ). mixing both kinds of task increases the number of possible mapping. therefore, one of our goals was to discover if the implicit information included in an interaction could be used to automatically switch between interaction modes, rather than having to provide explicit widgets for mode selection the participant observes an animation of the desired task on the !rst part of the screen (fig. 8a, b) (left)), and then he/she performs a gesture of their choice to perform this task (right). the experiment was composed of thirty-six trials, divided into three classes: eleven navigation tasks, nine object positioning tasks and sixteen object deformations tasks. for navigation or object positioning tasks, the scene was composed of two cubes, a grid, and a background picture (fig. 8a) . for object deformations, only the grid and 3d object were shown (fig. 8b )."
"the first goal of this paper was to understand hand gestures on a surface. the phase analysis technique we proposed provides a simple, yet consistent way to analyze and classify gestures, especially regarding global hand motion. therefore, an interesting direction for future research would be to develop new interaction methods directly relying on such phase analysis to drive task control."
"using such a gestural design pattern for all 3d multi-touch interfaces would be a real advantage, since users would need to learn the pattern only once, and would immediately be efficient with new tools."
"their approach was to classify gestures using three parameters: form, initial point locations, and trajectory. they identified gestures by the number of moving/unmoving fingers (the form), their starting locations (initial point), and the kind of motion (trajectory), while exploring object translation, rotation or one axis scaling tasks."
"though, the last finger suffers from the same issue on two and three touch techniques. indeed, as the roll and pitch rotation are mapped in the cartesian frame, rotation and scaling local phase are mixed during the last finger gestures. therefore, performing pure roll or a pitch rotation are interfering each other."
"modes disambiguation the vast majority of users (87%) performed ambiguous gestures, i.e., used similar gestures for two different tasks. this leads us to look for ways to disambiguate those gestures."
"the gestural pattern is summarized in table 3 . on the first hand, manipulations that transform the scene/object on the 2d screen plane mainly used one-handed gestures (e.g., translation/extrusion along x, y axis tasks are performed by one hand translation gestures). scaling manipulations can be gathered into two possible gestures, which both represent a shrink gesture, either performed by one or two hands. table. 3. table grouping a set of action -either usable on navigation tasks, or object positioning/deformation tasks -and the users associated gestures."
"on the other hand, manipulations that required depth axis motions need more attention. for instance, rotation tasks are usually performed with two hands: one hand is keeping the object in place, while the second hand is \"pushing\" the object, like in the trackball technique [cit] . though, manipulations that correspond to a translation along depth axis are outsiders: no gesture was consistently used to perform these tasks."
"a second goal of the paper is to study how those dof can be mapped to actual 3d manipulations, i.e., which interactions are the most efficient to exploit those dof. despite interaction with 3d content on tabletops is not \"natural\", in the sense that there is no consensus among participants on how nontrivial 3d manipulations should be performed through 2d gestures, through another experiment, we discover some principles for designing 3d interactions on tabletop, which enable us to disambiguate 3d content manipulations. possible manipulations correspond to navigation tasks (when the point of view is manipulated), object positioning tasks (i.e., object translation, rotation or scaling) and object deformation tasks (i.e., stretching, compressing or bending some part of an object)."
"to the tbox authors mind, one goal of their interactions was to discriminate between rotation and translation. therefore, users cannot efficiently switch between these two manipulations: they have to stop their first gesture and reach again the required edge. on the other hand, phase analysis based interface would permit to easily switch between these manipulations, maybe at the cost of stability."
"for those tasks, while we used a different methodology, our results are largely consistent with their findings: in our case, form and trajectory parameters are considered by the phase analysis. nearly all their classifications are coherent with the gestural pattern that we defined above. for instance, their rotation gestures (except for r3 and r8) are identical to our rotation phase. moreover, both papers observe that a majority of users prefer to start on or nearby the object. the main difference is the parameters used to define the starting locations. while we only defined the neighborhood of object to distinguish between modes, they divided this parameter according to cube elements (faces, edges, corners and external). both classifications bring their own advantages. using cube elements to directly manipulate complex 3d content such as large triangular meshes would be meaningless. on the opposite, manipulating 3d content with 3d transformation widgets could always make use of cube like widgets, and therefore use the proposed decomposition."
"for instance, a dead weight 300,000-ton vlcc (very large crude carrier), which can carry 300,000 tons of crude * namkug ku available online at www.sciencedirect.com oil, can be delivered to a ship owner after a total design and production period of about 14 months. to build the vlcc, the ship is divided into about 200 blocks, as shown in fig 1-(a), and the blocks are erected in a dock. for the erection of the blocks, block-lifting and transport design is required, and is a part of the process planning. blocklifting and transport are performed using various types of cranes, and the cranes are defined as multibody systems, which are collections of interconnected rigid bodies, consistent with various types of joints that limit the relative motion of pairs of bodies. fig 1-(b) shows a goliath crane, which can also be regarded as a multibody system. therefore, the process planning designers need to analyze the dynamic response of multibody systems. the block-lifting and transport procedure is composed, however, of several discontinuous stages, such as hoist-up, hoistdown, and turn-over. meanwhile most of commercial programs for multibody dynamic analysis cannot deal with discontinuous state variables, event triggered conditions, and state triggered conditions."
"adams (automatic dynamic analysis of mechanical systems) is a software system that consists of a number of integrated programs that help an engineer in performing three-dimensional kinematic and dynamic analysis of mechanical systems [cit] . adams generates equations of motion for multibody systems using augmented formulation. the user can define any multibody system composed of several bodies that are interconnected by joints. adams supplies various types of joints, such as fixed, revolute, and spherical joints. various external forces can also be applied to multibody systems, but adams cannot handle discontinuous state variables, event triggered conditions, and state triggered conditions. ode (open dynamics engine) is an open-source, library for simulating multibody dynamics [cit] . similar to adams, ode derives equations of motion for multibody systems using augmented formulation. ode can treat only rigid bodies, though, not flexible bodies. moreover, it cannot handle discontinuous state variables, event triggered conditions, and state triggered conditions."
"we will further focus on the differences between these methods, compared with our phase analysis method. even though the one touch technique is the most stable gesture (as it can only provide translation phases), the technique suffers of dof distinctions: all interactions are mapped to the same gesture. on the other hand, the three-touch technique easily decomposes translation and z-axis rotation to translation and rotation phases into the two first finger motions. translation and rotation phases can be mainly performed at the same time, with little interference between them, so users are more efficient while performing such techniques."
"a first kind of 3d manipulation is interactions that are directly performed onto the objects. hancock and cockburn researches identify 3 techniques, based on the number of fingers used (which are extended by martinet's works for depth axis translation) [cit] . their paper is focused on the comparison between three techniques that enable users to perform translations and rotations. the first technique, involving only onetouch interactions, corresponds to an extension of the rnt algorithm [cit] . by doing so, the interface can manipulate 5 dof with a single finger. the second technique, involving two touch interactions, the first finger correspond to the rnt algorithm for translations and yaw motions, while the second finger is used to specify the remaining rotations. the last technique maps each group of motion to a specific finger -translation to the thumb, yaw rotation to the second finger and the remaining motions to the last finger."
"to get a better understanding of possible hand gestures when the !ngertips are constrained to remain on a table, we ran a !rst experiment that does not involve any 3d task. since our goal was to estimate the number of dof a user is able to simultaneously control with a single hand, we asked participants to use their dominant hand to perform a number of speci!c gestures. the gesture is speci!ed by a starting position and an ending position. those positions consist of !ve circles, each circle (resp. labeled with 1, 2, etc.) representing the position of a !nger (resp. the thumb, the fore!nger, etc.), as depicted on fig. 1 .a. once a !nger is correctly positioned, the corresponding circle turns green. once all !ngers are correctly positioned, the circles vanish, and the ending position appears. then, the participant has to move his/her !ngers to match the ending position, while keeping all !ngers in contact with the surface. she/he can take as much time as she wants to perform each gesture. the experiment was composed of thirty-seven trials. those thirty-seven gestures are designed to be of various complexities: the simpler ones only involve movements of the whole hand, while the more complex ones involve the combinations of both hand movements and individual uncorrelated !nger movements. our set of gestures set was designed by testing in a preliminary study a comprehensive combination of elementary movements, and by discarding those that were too difficult to perform."
"navigation tasks: zoom vs. depth axis translation two consecutive trials were depth axis translation and zooming tasks. to distinguish the different kinds of trials, a background image was added to the 3d scene. though, every user but two asked for the differences. once answered, they mainly succeed to understand the shown transformation."
"this protocol is based on source-initiated on-demand routing. this type of routing creates routes only when desired by the source node. route discovery process starts on demand by the source. this process is completed once a route is found or all possible routes have been explored. it provides unicast, broadcast, and multicast communication in ad hoc mobile networks [cit] . routes are maintained as long as they are needed by the source node. aodv nodes maintain a route table in which next hop routing information for destination nodes is stored."
"moreover, we can also notice that, although they did know the difference (as they asked for it), half of the participants still performed the same gestures for both tasks."
"it is noticeable that they stop their comparison up to three-finger techniques that corresponds to our effective upper bound number of fingers. they compared the three techniques in two experiments. for both tasks, they concluded that the three-touch technique was the fastest to use, while the one touch techniques was the less efficient one."
different routing protocols behave differently in 802.16 networks according to their internal working mechanism. it has been seen that the table-driven dsdv protocol has the best performance in terms of the packet delivery fraction parameter which outperforms both dsr and aodv but the delay experienced by dsdv packets are greater than the delay experienced by the on-demand routing protocols.
"for the !rst ten trials, an animation between the starting and the ending position was shown to the user prior the trial, whereas for the other trials, no path was suggested. the participants were not asked to follow the suggestion, and its presence had no noticeable effect on the results we report below."
"the first observation about tbox, once analyzed into phases, is that all object manipulations are translation phases only (scaling corresponding to one hand translation and a symmetric second hand role). in terms of stability, such gestures are the most efficient, as no interferences can occur. moreover, such widget leaves a lot of possible interactions for other manipulations (such as deformations)."
"hands/fingers uses to deeper investigate the efficient dof a hand can control, we first observe that only three participants used more than 3 fingers by hand. those cases mostly involved navigation tasks. in more details, when participants involved more than 3 fingers to manipulate the 3d contents, the principal phase of their interaction corresponds to translation phase (i.e., the most global motion). on average, fewer fingers by hand are used to handle objects than to navigate ( however, many users interacted using both hands. from our observations, the nondominant hand had two main functions: a support function (sup.) (e.g., frequently indicating the parts of the scene that should not move by keeping a still hand on them); or a symmetric function (sym.) (e.g., doing symmetric gestures with both hands for scaling). the support function is most frequently used, specifically on ob-ject manipulation tasks where it is used to maintain some objects or some part of the object of interest in place."
"the performance of the protocols in terms of normalized routing load, which is the number of routing packets transmitted per data packet delivered at the destination, and the routing overhead, which is the total number of routing packets transmitted during the simulation are shown in fig.1 (d) and (e) respectively. both these parameters measure the scalability of a protocol, the degree to which it will function in congested or low bandwidth environments, and its efficiency in terms of consuming node battery power. these two parameters are related to pdf in the sense that lower pdf means that the delay metric is evaluated with lesser number of samples. longer the path lengths, the higher the probability of a packet drop. hence, sending large numbers of routing packets can also increase the probability of packet collisions and may delay data packets in network interface transmission queues. thus, with a lower pdf, samples are usually biased in favor of smaller path lengths and thus have less delay [cit] . as indicated in the figure, dsdv has highest routing overhead whereas both on-demand protocols have very low routing overhead. for this reason, in dsdv, packets exhibit more delay because of the increase in probability of packet collisions. for this reason, the key motivation behind the design of on-demand protocols is the reduction of the routing load. figure 2 shows the performance for all the protocols using different nodes. for this simulation, the pause time is fixed to zero, the data rate is 2 packets/s and the maximum number of connections is 10. pdf for all the protocols have the same pattern. for a fixed number of nodes, the pdf dsdv is better than those for the other two on-demand protocols. for average delay, it is interesting to see that the number of nodes in dsr has no impact on the delay. further, for higher number of nodes, the delay converges to the same value. for nrl, the performance is almost the same for all nodes with dsr and aodv. but this performance shoots up for dsdv. although at 50 nodes, the average delay for all the protocols are same, but the routing overhead difference between dsdv and dsr or aodv is considerable."
"the quality of service (qos) types supported by wimax are unsolicited grant service (ugs) for the constant bit rate (cbr) service, real-time polling service (rtps) for the variable bit rate (vbr) service, non-real-time polling service (nrtps) for non-real-time vbr, and best effort service (be) for service with no rate or delay requirements. traffic classes are associated with certain qos parameters and according to these parameters, mac scheduler makes appropriate handling. the upper-layer data are queued with an assigned connection id."
"combining different manipulations some tasks of the experiment consisted in combining elementary motions -for instance, object translation and rotation. in order not to influence the participants, and leave them free to invent their own interaction mode, only a before/after animation was shown in this case. analyzing data by phase analysis techniques enables us to easily distinguish whether users prefer to perform each \"elementary\" motion sequentially, or simultaneously. the results are gathered on table 2 ."
"during each trial, the trajectories of the tip of the !ngers were recorded. to analyze gestures, we de!ne the following parameterization of the hand: we use the position of the thumb as the origin of a local frame, in order to simplify the decomposition into phases. the !rst axis of the frame is given by the thumb/fore!nger direction of the starting position. therefore, the hand position is given by the local frame (2 dof for the position of the origin), and by the position of each !nger in this frame (0 dof for the thumb as it is always at the origin, 2 dof -distance and angle-for the other !ngers)."
"to further investigate the interdependencies among the last three fingers, we split the trials into three groups, depending on the number of !ngers the users have to move among the middle, ring and little !ngers. fig. 7 shows for each group (vertically: 1f, 2f, 3f), the relative time spent moving 1, 2 or 3 of those !ngers. it is interesting to note that even when asked to move a single !nger (1f), the participants spend more than 30% of their time moving two or more !ngers. on the other hand, when participants have to perform the same motion for the last three !ngers (3f), only one third of the time is used to move the !ngers together, while # 40% of the time the !ngers are moved individually."
"theoretically, multi-touch devices offer the possibility of manipulating 3d scenes while simultaneously controlling many dof: up to 20 actually, if the two hands were used. however, this upper bound is never reached. because of the interferences between !ngers and to their restricted motion when moved in contact with a plane, complex gestures involving all !ngers are often unstable, and the time it takes to perform them would be prohibitive for an interactive use. as shown by the second experiment, users easily invent gestures to interact with 3d content. quite interestingly, they tend to use all !ngers for global hand gestures such as translation, rotation, and scaling, although two or three !ngers would be sufficient (in this case, using all !ngers is easy, since there is no local hand motion to control). for more complex interaction gestures, users naturally limit themselves to one to three !ngers per hand. this leads us to the following methodological rules when designing 3d interaction on a multi-touch table:"
"to have a more obvious result of the network capacity analyses, we stimulate the achievable data rates of the random topology with the same node amount which can represent the topological characteristics of the practical power grid. the topology is displayed in the section 6 using gpsr protocol based on tdma (slot number is 10). figure 5 displays the simulation results under the earthquake impacts with magnitude ranging from 4 to 7.5. the blue line is the result of the achievable data rates of a simulated practical network. the green line is the simulation result of the system which has the same transmission substation amount, but with the regular hexagonal topology for the arbitrary network, that is, network capacity of arbitrary network."
"international journal of distributed sensor networks in (24) that is inversely proportional to the channel capacity ( ) given in (6), that is, the larger the channel capacity, the smaller the value of will be. the current earthquake early warning system can provide tens of seconds of warning time before an earthquake strikes [cit] . although the warning time is short, it is sufficient to inform the substations of the epicenter and magnitude information to implement earthquake prevention measures and enable r-gpsr when an automatic mechanism is founded in future. after the earthquake information obtained, since the r-gpsr is geographic routing protocol, the fragility probabilities of the substations in power grid can be estimated based on their location information using the models in this study. and the channel capacity of their interlinks can also be computed. combining the distance to the destination, the substation with the minimum routing cost is chosen as the next hop."
"in this section, the modeling of seismic impacts on power grids is discussed, including the seismic ground motion model and fragility probability model of substations. we use the ground motion model, also called attenuation relation model, to simulate an earthquake to obtain the peak ground acceleration (pga) value of an exact location. based on the pga values, the fragility probabilities of substations can be computed."
"where mag is the magnitude term; dis is the distance term; flt is the style-of-faulting (fault mechanism) term; hng is the hanging-wall term; site is the shallow site response term; sed is the basin response term. those parameters can be approximately estimated based on the earthquake magnitude and the distance from the epicenter for a certain location. figure 1 shows the estimated pga values of the locations with the distances to the epicenter ranging from 0 to 100 km given the earthquake magnitudes of 6, 7, and 8. it shows that the seismic intensities (i.e., pga) are highly lasting and slowly attenuating in the distance range from 0 to 15 km to the epicenter. it indicates that the substations within such range receive the most significant impacts of the earthquake."
"since the distribution substations are uniformly located along the backbone transmission power line, the number of the branches of each transmission power line is equal. hence"
"we study the achievable data rates of the network, because the theoretical lower bond on network capacity represents the harshest situation that the numerical results can be near zero. meanwhile, unlike wireless network, the practical topology of power grids can be analyzed neither totally randomly nor arbitrarily. however it should be analyzed according to practical topology characteristics. hence the theoretical lower bond on network capacity can be less meaningful than the results of a lower bond which can represent the practical network characteristics."
"extensive experimentations have been carried out in order to evaluate the performance of the proposed 2d dghmbased face recognition method as compared to the existing methods. this section describes the databases, the experimental conditions, and results of the comparisons."
where  pq is the kronecker delta function. an orthonormal relation may be obtained by using a normalized version of the hermite polynomials given b
"the network capacity depends on many factors including network topologies, power and bandwidth constraints, routing protocol, and radio interference, and so forth. the network capacity analyses, especially the upper and lower bonds of network capacity, are valuable to the network designers. in this section, the upper bond of network capacity of smart grids communication networks are analyzed. to obtain the upper bond of network capacity, we apply an arbitrary topology to make the communication network to function to the utmost. the postearthquake bond of network capacity is analyzed combing analyses of seismic impacts above. besides the upper bond analyses, the achievable data rates of the network is obtained through simulations based on a simulated random topology with the same node amount which can represent the topological characteristics of the practical power grid."
"the paper has been organized as follows. in section 2, image moments are defined, and then, the process of obtaining the 2d ghms and reconstruction of face images from these moments are described. section 3 presents the statistical approach for identification of a person using the stored orthogonal moments. in section 4, the face databases used in the experiments are described, and results of the proposed and existing face recognition methods are presented. finally, conclusions are drawn in section 5."
"as figure 5 shows, there exists a large numerical gap between the network capacity and the achievable data rates which is more than 1000 times. many reasons contributes to this gap, and the main of which is we use the mean surviving probability to represent the nodes surviving situation of the whole grid. the practical seismic impacts cause the node to be either damaged or survived due to probabilities, which can be more severer than the average probability. in the simulation of magnitude 6, the nodes can be damaged to near 67% to the utmost (transmission substation survivors: 9/30, distribution substation survivors: 123/362) in the random topology, however in the arbitrary topology the average fragility probability is just near 41% due to the topology changes. hence the 26% more nodes among the amount 392 survive in the arbitrary network. different numbers of power lines due to the difference between arbitrary network and the simulated network can contribute to this gap. because in the random topology, the transmission lines amount is 60 while in the arbitrary topology is 90 which means 30 more channel links. since the upper bond should be the situation of the utmost probability, every link has 26% more nodes survived and the whole network has 126% capacity with the exponent of 30 which results in 1026 times. moreover, the achievable data rates are simulated with full contentions and interferences on the limited surviving links which can also contribute to this gap. meanwhile, reasonable omissions of some power line parameters in the formula deduction can also be a factor in this gap."
"is supposed to be transmitting to ( ) over the power line at power level at some time, and  represents the set of all simultaneous transmitters over power line to ( ) at that time. to include the signal power of in the denominator,"
"where is the probability of survival which can be computed with the fragility probability. the surviving probability of a macrocomponent (e.g., substation) equals the cumulative products of its microstructures. since the power line can endure a much larger acceleration than the substation [cit], the fragility of power grid can be measured as the fragility of substations for the worst case. figure 2 shows the relation of the fragility probability of a transmission substation given a pga value ranging from 0.01 g to 0.5 g. it can be found that the substation is totally damaged with the pga larger than 0.3 g. according to ieee std 693 [cit], the substations with medium perform level of earthquake-resistance should be capable of enduring the pga value within 0.25 g. hence the numerical result of the estimation is considered to be credible and representative. the results of figures 1 and 2 infer that the substations in the region of 15 km to epicenter are most likely affected by earthquakes. the substations within 5 km to epicenter are almost collapsed. an earthquake can cause a damaged hole in the topology of the power grid, which is a great threat to the communication subsystem of the smart grid."
"the proposed dghm features are compared with the existing features using the restricted video pairs; the results of which are reported in the website maintained by the organizers of this database. figure 10 shows the roc curves obtained from the minimum distance-based similarity and the matched background similarity (mbgs) of lbp features [cit], the adaptive probabilistic elastic matching (apem) [cit], and the large margin multimetric learning (lm 3 l) [cit] methods both of which use a fusion of sift and lbp features. it can be seen from this figure that the proposed dghm feature performs better than the lbp feature independent of the similarity measures chosen. at the same time, the proposed feature shows a competitive rate of verifying true-positives for a given false-positive as compared to the fused features obtained from the sift and lbp such as those in the apem and lm 3 l methods. figure 11 shows selected frames of videos of typical 16 pairs of identities that are used in the verification experiments of the ytf database. among these pairs, half represent the intra-personal pairs and the rest the inter-personal pairs. the correct and incorrect verifications of these identities are shown in separate groups. the sample images of correct verification reveal that the proposed dghm features are capable of identifying subjects; the videos of which have significant variations in terms of resolutions, pose, and color. however, the proposed method fails when the frames of the video suffer from severe motion blurs or occlusions due to embedded texts and shadows. a few examples of close appearance of inter-personal facial frames that are verified correctly by the proposed dghm features are shown in fig. 11 ."
"hence, we combine the advantages of the radial and ring topologies with the euler characteristic of planar graphs, to apply an arbitrary regular hexagonal topology. we consider the scenario where transmission substations, power line links, and distribution substations are located in an infinite plane. the transmission substations are connected regular triangularly and hexagonally through power lines which compose ring topologies. the distribution substations are uniformly connected to the backbone transmission power lines as tree-shaped topologies which can balance the multipath effects of the plc channel according to the plc channel characteristics. figure 4 displays the concept of the regular hexagonal topology."
"the proposed method was evaluated on a number of face databases; however, the results presented in this paper are those obtained using the popular at&t face database [cit], a generic face database obtained from the comprehensive face recognition grand challenge (frgc) v2.0 database [cit], and the standard protocols of the face recognition technology (feret) database [cit], the labeled faces in the wild (lfw) database [cit], and the youtube faces (ytf) database [cit] . the details of the databases considered in the experiments are discussed under separate subheadings as a convenience."
"as (23) shows, the routing metric of the neighbor substation is based on three portions, which are the normalized fragility probability given in (4), the normalized channel capacity of the link given in (24), and the normalized distance to the destination of neighboring substation given in (25), with the tunable weights and . max and min are the maximum and minimum values of the neighboring channel capacities. max and min are the maximum and minimum distances of the neighboring substations to destination. note the normalized channel capacity given 8"
"currently few researches have covered the reliability against earthquakes for the communication subsystem of smart grids. this paper models the seismic impacts on the communication networks of smart grid using a seismic ground motion model as an example. the fragility probability of the power grid is obtained by using the structural fragility probability model. based on these, the seismic impacts on communication channels are analyzed. besides these, the network capacity is analyzed and the postearthquake network capacity upper bond is obtained through formula derivations based international journal of distributed sensor networks on an arbitrary network model. meanwhile, based on a simulated random topology which can represent the topological characteristics of the practical power grid, the achievable data rates are obtained through simulations. finally, to reduce the seismic impacts and enhance the robustness, a robust routing protocol (r-gpsr) is proposed based on the classic gpsr protocol, which shows obviously better performances on the expectation channel capacity and the packets loss rate of the route, especially within the earthquake magnitude ranging from 5 to 6. in future work, small-scale scenarios of the smart grid can be studied, including home area networks (han) and neighborhood area networks (nan) which cover both lv and mv power networks. more robust protocols working across the communication, information and energy subsystems of the smart grid can be proposed. a seismic emergency response and recovery system for the smart grid can be systematically studied combining the earthquake early warning system with this research."
"the seismic impacts on the communication channel in power grids are determined by the surviving number of the distribution substations along a transmission line that is, the number of the signal paths. figure 3 shows the changes in the channel capacity of a typical 1320 m mv power line given the psd ranging from 90 to 30 dbm/hz and different surviving number of the distribution substations on plc cenelec a band. this is a typical example sampled from a topology of the mv power grid which can represent the characteristics of other power lines of a grid. it shows that the surviving number of distribution substations in the network can have significant influence on the channel capacity of any single link. as it shows the ordering of the line does not follow the increasing number of survivors, that is, because the results reflect the combination of the multipath effects, random weighting factors, and branch lengths which make the results nonlinear."
"since the moments that constitute f are obtained from the orthogonal polynomial functions, the features may be treated as independent given the class. thus, the pdf of features given the class may be written as"
"to analyze the upper bond of the network capacity, we need to make the network function to its utmost based on the arbitrary network model. in an arbitrary network, nodes are arbitrarily located in the plane. each node has an arbitrarily chosen destination with an arbitrary traffic rate and traffic pattern. each node can also choose an arbitrary range or power level for each transmission. in this paper, we apply a hexagonal topology for the arbitrary network which needs to increase its the number of communication channels that is, the number of the power line links. however, the practical power lines can not be intercrossed and overlapping randomly. according to euler"
"to have an obvious numerical result of the performance improvement of the proposed robust routing protocol, we use the expectation channel capacity and packets loss rate of the route as the measures of the performance of these protocols. based on the power grid we adopt above, we run simulations to test the two measures."
"the expectation channel capacity of the route is a concept in graph theory that is widely applied in expectation-maximum capacity route searching, network capacity computing and data flow analyses, among others. it represents the maximum data transport ability of the route that combines the reliability probability and the channel capacity of the route. it is the product of the minimum channel capacity and reliability probability of the route:"
"in designing the classifier, p(f) may be ignored since it does not dependent on m. hence, an unknown face image having feature vector f should be assigned to the class m that maximizes the following decision function [cit] :"
"representation of images and formation of feature sets from such a representation play key roles in the success of a face recognition algorithm. compact representation of images is desirable to reduce the storage requirement, a critical issue for face databases. the feature sets should be capable of capturing the higher-order nonlinear structures hidden in a face image, and at the same time, the features must be sparse enough to discriminate the identities. in this paper, the 2d ghms have been used to represent face images so that a face database may be compactly represented. use of these orthogonal moments for face recognition has also been motivated from the fact that certain linear combinations of these moments form geometric moments that are invariants to shift, scale, and rotation of a pattern. the key contribution of the paper is that features of facial images have been obtained by selecting only those moments having greater discriminatory power, as measured by their large values for the icc, instead of choosing a heuristic set of ghms. this is an effective means of filtering out those moments that contribute little to distinguishing among different classes of the face images. classification of test images has been performed using the naive bayes classifier, which is simple, but fast and known to perform remarkably well in many practical applications with huge size database. in order to compare the recognition performance of the proposed method, extensive experiments have been carried out on a number of commonly used image and videobased face databases, such as the at&t, frgc, color feret, lfw, and ytf that have facial images or frames with variations in terms of appearance, occlusion, expression, pose, color, resolution, and illumination both in the constrained and unconstrained environments. the results have shown that the proposed 2d-dghm-based method provides higher accuracy in face recognition than those provided by the popular 2d-pca, 2d-lda, and 2d-cca methods, as well as the 2d kcm method even when the training data set has small number of samples per subject in the constrained environments. the face recognition and verification results on the standard protocols of the databases of unconstrained environments also reveal that the proposed dghm features can perform better than the commonly used descriptors such as the lbp or sift."
"we also compare the proposed dghm-based features with some of the state-of-the-art descriptors that are used in the unconstrained face recognition problems. in this scenario, the standard protocols of the color feret, the lfw-a, and the ytf databases are used for the evaluation of the performance of face recognition or verification. the fa dataset in feret is used as the gallery images, while the fb, dup-i, and dup-ii are used as the probe sets. the rank 1 recognition accuracies of the known classes are estimated from the minimum of proposed decision function and by using the features of the gallery and probe sets. in the experiments, \"view-2\" set of the lfw that consists of 10 folds of randomly chosen 300 positive and 300 negative image pairs is used for evaluation of face verification. the images of the feret and lfw databases are represented in terms of the ghms by choosing the values of  to be 0.25 and 0.15, respectively. in order to train the ghm-based features of the feret datasets, the entire set of frontal images are considered. on the other hand, the features of the lfw-a are trained using the subjects that have sample sizes in between 10 and 50 by avoiding the subjects having too small or large sample sizes. the ghm features of the videos of ytf are obtained by taking the mean of each moment obtained from the frames of the clips and by choosing  to be 0.15. in order to train the ghm features of the ytf database for the experiment of verifications, the intra-personal and inter-personal pairs are chosen as two different classes while estimating the iccs."
"in order to improve readability, the superscript of m gh pq will be removed from the remainder of this paper and m pq will be referred to as the 2d ghm of order ( p, q). figure 1 shows a few number of 2d gh basis functions obtained from the tensor product of two independent 1d gh polynomials. the ghms of the 2d image signal may be considered as the projections of the signal onto these 2d basis functions. thus, these moments characterize the image signal at different spatial modes that are defined by certain combinations of the derivatives of the gaussian functions. ideally, from all possible moments, the image i(x, y) may be reconstructed without any error as"
"with the fragility probabilities of basic structural type obtained and the structure of substations confirmed, the surviving and fragility probabilities of substation can be computed:"
"in this paper, the moments that will be used for face recognition are obtained from the orthogonal gaussianhermite polynomials. hence, a brief review of hermite polynomials and their orthogonality relations with the gaussian weighting function are given first. next, the method of obtaining the orthogonal ghms of the face images from these polynomials and the reconstruction of images from these moments are presented."
"international journal of distributed sensor networks 5 characteristic of planar graphs and euler's formula [cit], in a planar graph with crossing number is 0 and vertices number is more than 3 like this scenario, to obtain the most number of links and also perfectly cover the plane, the nodes can only be connected regular triangularly and regular hexagonally, that is, the nodal degree is 6."
"to compare the dghm features with other descriptors previously reported on the lfw database, the verification results acquired from the restricted image set are provided. figure 8 shows the receiver operating characteristic (roc) curves that compare verification performance of the features obtained from the eigenfaces [cit], scaleinvariant feature transform (sift) [cit], lbp [cit], multiple kernel learning (mkl) [cit], and the dghms. it is seen from this figure that the proposed dghm features provide a true positive rate better than the sift or pcabased features and a competitive true rate when comparing with the lbp or mkl-based features. figure 9 shows the face verification results obtained from the proposed dghm method using 16 pairs of face images considered in the view 2 experiments of the lfw database. in this figure, the results are presented using four types of verifications, namely, the correct or incorrect verification when the subjects are identical and the same when the identities are different. the results of the positive pairs show that the proposed method is capable of verifying the subject when there remains scaling, rotation of faces both inand out-of-planes, or even illumination variation. however, if pose variation, occlusion, or makeup is significant, then the positive pairs may not be verified accurately. it is also found from this figure that if the appearances of faces are very similar, then the negative pairs can be wrongly identified."
"we introduce the average surviving probabilities of transmission and distribution substations, ts and ds, into the physical transmission model we analyzed above. the surviving probability of a substation is defined to denote its probability of its existence in the topology. hence the number of branches along the backbone transmission power line in (13) need to be rewrite as ( / ) ds . the nodal degree in (17) has to be rewrite as 6  ts . the power line could survive only if its both ends have survived, so the number of the surviving power line channel can be obtained as 3  2 ts . hence (18) can be rewritten as"
"in this section, to reduce the seismic impacts and enhance the robustness of the communication subsystem of smart grids against earthquakes, also to avoid the potential packets loss and the topological routing voids caused by the damage of substations, a robust routing protocol based on greedy perimeter stateless routing (gpsr) is proposed for a plcbased communication network in the smart grid scada system. the r-gpsr routing metric is developed based on the fragility probability, the channel capacity, and the distance to the destination. gpsr is a routing protocol originally proposed for wireless networks. gpsr makes greedy forwarding decisions using location information about a routers immediate neighbors in the network topology [cit] . the reason why gpsr is selected among the existing protocols is for the extension to consider the impact of earthquakes. it is a welldeveloped technology on plc network based on geographic positions knowledge of nodes which can easily combined with earthquake warning system and the impact models we analyze above. in this paper, we keep the original forwarding rules of gpsr. based on the location information of neighbors known as a priori, the next hop with the minimum routing cost is determined until the destination is reached. however, we design a new robust gpsr routing metric based on the impact analysis of natural disasters in this paper to replace the original routing metric that is the minimum distance to destination. the new routing metric is defined as follows:"
"over the past two decades, biometric security systems that allow identification of individuals using their physiological or behaviorial traits, have become an integral part of many security-aware applications due to the increasing demand for \"identity verification\". by overcoming the spoofing vulnerability of the traditional methods of authenticity verification such as passwords, pin, or id cards, biometric security systems have become indispensable tools in financial transactions, access control, and *correspondence: mahbubur@eee.buet.ac.bd 1 department of electrical and electronic engineering, bangladesh university of engineering and technology, 1205 dhaka, bangladesh full list of author information is available at the end of the article surveillance. among various existing biometric methods, automated face recognition is often preferred due to its nonintrusive nature, high level of machine compatibility, and positive attitude of the public [cit] . in most of the existing face recognition algorithms, the identification of a person is performed using the pixels of two-dimensional (2d) face images captured by ubiquitous ccd-based visible light sensors. the video-based face recognition focuses on the identification of a person in the noncooperative environment by using the chronological variations in the appearances of 2d face images over the frames [cit] . instances may also be found in the literature with regard to the 3d face recognition algorithms, wherein identification is performed considering the information of light intensities captured by pixels of 2d face images along with the depth information of each pixel. notably, the 3d face recognition techniques have two major difficulties with regard to the acquisition and processing of images [cit] . in the acquisition process, the depth information of an image is captured using multiple sensors such as the ccd and laser sensors in which case the latter provide much lower resolution than the former, and thus, registration to obtain 3d image becomes complex. a number of ccd sensors may provide depth information, but the focus adjustment of camera lenses and maintaining spatial distances among cameras requires extreme precision. in addition to complexity of the acquisition process, 3d face recognition involves significant memory requirements, and computational load while performing a one-to-many searching task as compared to the 2d counterparts. it is for these very reasons that commercial face recognition systems, which deal with large numbers of people in busy places, still use the popular 2d face recognition techniques and research centers on ways to improve the identification performance of such methods. in the research studies, statistical methods are being increasingly used to analyze the random variations of 2d spatial distribution of pixel intensities among the face images of individuals or subjects and to develop efficient face recognition algorithms."
"meanwhile, three kinds of mv topologies exist in the practical power girds which are radial, ring, and networked topology, respectively [cit] . the radial topology has many advantages in fault current protection, control of power flows and lower cost, and so forth. in a ring topology, the substations of the network are interconnected composing a loop. a more complex radial topology is the form of a tree-shaped topology. the ring topology can be seen as an improved radial topology that creates redundancies and overcomes the weakness of radial topology."
") . (20) when an earthquake happens, the substations are damaged to varied extent. according to previous analyses on seismic impacts on substations and communication channels, the different surviving number of the distribution substations can result in the change of channel capacity and network topology. hence the network capacity would obvious change after an earthquake happens. according to the previous analyses, when an earthquake happens, the location of epicenter and the magnitude of the earthquake can be measured by the earthquake warning system. the fragility probability of the substations can be computed by the seismic motion model and fragility probability model which are mentioned above. hence the surviving probability of substation can be obtained through (3) ."
"the values of the moments m pq are computed using (13) for each of the images in the face database, whereas the iccs for each of the moments are estimated using (19) (20) (21) only for the images of the training set. thus, a total of (n + 1) 2 number of iccs are computed corresponding to the (n + 1) 2 number of ghms representing each face image. it is to be noted that not all these moments are useful for face classification. further, the estimated iccs quantify the discrimination capability of the moments towards identification of classes. hence, we select as features only those ghms that correspond to the t(t n) largest iccs, t being the number of moments used for classification. in such a case, the features referred to as the dghms for the k-th image of class may be denoted as"
"where is the minimum signal-to-interference ratio (sir), 0 is the ambient noise power level, is the number of branches on the power line between node and node . is a constant which is determined by the power line cable parameters, carrier frequency, and signal velocity. differs according to varied applied scenarios and can be measured and computed through (8) . consider"
"use the same deduction process from (18) to (20), the postearthquake upper bond on network capacity of the communication network in smart grids can be obtained as"
"computer aided software engineering (case) tool support refers to the use of integrated programming tools [cit] . effective software reuse may make use of tools where the repository is integrated in the programming tools, reducing reuse barriers as much as possible. by further integrating case tool support the idea of an application generator may become feasible [cit] ."
software reuse in agile development settings and the open source community indicate that there is a wide range of practices that have to be addressed to utilize software reuse [cit] . without a holistic approach to software reuse organizations are not able to address related issues and even more important they are not able to identify potential improvement areas for achieving even greater benefits.
"the assessment method component consists of the measurement of two variables. the first is the scoring of reuse factors, which is done according to the three dimensions defined by daskalantonakis: 'approach', 'deployment' and 'results' [cit] . the combination of these three dimensions leads to a fixed score on a scale of 0-10. by dividing the score by 2 the related reuse level can be found. an odd score can be used to indicate that the current reuse level shows characteristics of two adjacent reuse levels, but has not met all the characteristics of the upper level."
the use of reuse measurement and costs models is another reuse factor indicating a systematic reuse process. the measurement of reuse activities may be enforced by top management for performing cost-benefit analysis [cit] . investments in reuse measurement and costs models may be substantial and are not always desired [cit] . the factor is nevertheless taken as a reuse factor as it contributes to a systematic reuse process.
"top management support and instrumental mechanisms are taken as a factor for indicating the commitment of top management. the support provided by top management can be both passive and active. they can be reinforced through several instrumental mechanisms. instrumental mechanisms mentioned in literature include the use of reuse champions, sample solutions, rewards and incentives, reuse education and training [cit] ."
"requirements management over multiple projects can provide a solid basis for identifying commonalities, which is an indicator of a systematic software reuse process [cit] . based on the commonalities potential reusable assets can be identified and produced."
"quality management is arguably one of the most important reuse factors. rosenbaum describes quality management as mandatory for a successful reuse program [cit] . quality management can be installed through the use of quality models or ranking systems [cit] . the way how quality management is addressed and its importance differs over the various reuse levels. supplier management addresses issues related to the acquisition of externally acquired assets (such as from black-box component markets and open source projects), asset certification and legal aspects [cit] ."
"at level 4 software reuse is defined as quantitatively managed architected reuse. architected indicates that an architecture is used to define and fit the code components, which is in line with the software product line approach. quantitatively managed reuse means that the reuse processes are controlled using statistical and other quantitative analysis techniques. also, the reuse processes are managed throughout the entire software lifecycle."
"the first set of components is stored within a central revision management system. the other component set is usually stored within a project specific repository. these components have to be extracted from a project first before they can be reused, they are however set up in such a way that this can be done relatively quickly. documentation such as functional designs and test cases is stored separated from the assets if available at all."
"the two-fold definition explicitly indicates that software reuse it not purely a technical issue, but also an organizational one. systematic software reuse has been regarded as the only appropriate solution for the notion of the software crisis [cit] . systematic software reuse on its turn is defined as reuse which is repeatable and excludes ad-hoc reuse events [cit] ."
"at level 3 the processes are standardized and thus followed by the organisation. the code components are managed and controlled by a group, guiding reuse in the right direction. additional elements of a systematic reuse process are implemented."
"this research would not have been possible without the support of the anonymous experts and the resources provided by the case organization. furthermore we want to thank jasper laagland and johan te winkel for their support and contributions made to the content of this paper. the work presented in this paper is also partly supported by the agile service development project, which is part of the service innovation & ict program (www.si-i.nl), sponsored by the dutch ministry of economic affairs."
"the process factors can be directly related to a systematic reuse process. the literature review resulted into six process factors: 'reuse planning', 'reuse measurement and cost justification', 'requirements management', 'quality management', 'supplier management' and 'configuration and change management'."
the second element of the assessment method is the relevance variable. the relevance variable is added to the assessment method to overcome the limitation of not being able to tailor the management tool to the demands of the individual organization. furthermore the relevance variable is used as an indicator of the need to scale a certain reuse factor. the use of a relevance variable favors the continuous approach of the cmmi-dev model [cit] . by using the relevance variable in multiple case studies in similar research settings it is expected that reuse patterns can be discovered. the use of reuse patterns favors the staged approach of the cmmi-dev model. investigating reuse patterns was out of scope for this research.
the technology factors were also considered relevant factors. the experts did note that the technology factors are rather supportive. basic technology issues have to be addressed and can be developed along the demands of the organization.
"configuration and change management seems to be difficult at the case organization. individual projects wish to reuse a component in a slightly different that the way they are offered, leading to the possible existence of multiple versions. these versions have to be merged back at a certain point in time, requiring substantial efforts. configuration and change management was installed through a revision management system and through frequent meetings regarding the most important assets. this point is expected to require additional attention in the future."
reuse planning describes whether reuse events are planned in the beginning of a software development project or they pop-in during the development process. reuse planning is an indicator for a systematic reuse process [cit] . domain analysis may be used for systematically scanning the domain for potential reuse opportunities [cit] .
repository supports refers to the use of a repository for storing and retrieving reusable assets. this reuse factor includes the use of search and retrieval techniques [cit] and configuration management tools. a simple configuration management tool may serve as the basis for providing and managing reusable assets [cit] .
"permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. to copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. sac '11, march 21-25, 2011, taichung, taiwan. [cit] acm 978-1-4503-0113-8/11/03$10.00."
"the combination of a reuse factor and a maturity level is operationalized through the use of reuse practices [cit] . a reuse practice describes a set of practices which a reuse factor has to meet before it reaches a certain maturity stage. the reuse practices form the basis for the assessment method component discussed in the following two sections. due to space limitations in this paper, the practices are not further elaborated on. the original set of reuse practices and related information can be found in the master thesis of spoelstra [cit] ."
a consumer is a person who is using existing reusable assets to create new solutions. consumer skills and experience relate to all the knowledge and practices required at the consumer side including knowledge about the reusable assets itself and best practices for integrating reusable assets.
software reuse processes are at best a secondary concern as the focus of a normal project is on project specific goals and outcomes [cit] . both organizational and technical issues have to be addressed to facilitate software reuse within and outside the normal project scope in order to reach higher levels of reuse.
"requirements are analyzed at customer level and are only to a limited extent useable over multiple projects. it is mainly the individual who recognizes and exploits reuse opportunities. the idea of requirements management across multiple projects has been proposed by the business, but so far its applicability appears to be limited."
"software reuse activities are stimulated and encouraged within all business units. as one of the experts noted: 'software reuse is considered an integrated part of the selling strategy'. top management assigned dedicated resources for the production of reusable assets. due to resource constraints the roles dedicated to software reuse are merely part time roles and only the most important reusable assets are maintained actively. the other reusable assets are often created within the normal project scope and have to be extracted from there in order to be reused. the communication channels are installed and information is shared across multiple business units. the extent of sharing is however informal and relies greatly on individual efforts. some experts noted that they would like to see more information from other business units, but at the same time such information is likely to be irrelevant as the most important aspects are shared. furthermore the experts emphasized that sharing information is a desired situation and it should not be formalized. simply asking around is considered as being a good practice. additional documentation in general could further improve the information flow though. the three reuse factors we have identified were considered by the expert panel as relevant factors for software reuse."
"the reuse of existing software assets offers intuitively great benefits, but despite its promises it has never reached its full potential. the first wave of researchers focused on the technical aspects to optimize software reuse. the second wave of researchers shifted towards the organizational aspects of software reuse [cit] ."
"a producer is a person actively working and the production of reusable assets. producer skills and experience include both technical aspects such as programming languages skills, but also organization aspects such as domain specific knowledge. the latter one is required to recognize reuse patterns and to separate project specific functionality from functionality reusable over multiple projects [cit] ."
"at level 2 the reuse process is characterised as managed. the basic infrastructure is installed to let reuse events take place. its processes are more structured and can be controlled and monitored. pieces of leveraged code, or rather code components, are used at this level."
"in the final maturity stage, level 5, the reuse events are optimized for a specific domain. not only do the reusable assets have to fit within the architecture, but the architecture is also guiding the development of reusable assets. new components complement or extend the existing architecture. each product is composed of or created by reusable assets exploiting software reuse to its limits."
the organizational structure and related reuse roles are often approached as a formal separation between producers of reusable assets and consumers of reusable assets [cit] the producers of reusable assets are those who create the assets and the consumers are those who use the assets in building new solutions. the way how these reuse roles and others are installed within the organizational are related to the reuse levels.
the identification of communications channels and how they are supported is identified as a potential reuse factor. this reuse factor is embodied in a systematic software reuse process and includes the communication of change requests [cit] . the factor is emphasized by agile development methodologies as they tend to rely on informal processes and face-to-face communication [cit] .
"a formal role distinction between producers and consumers of reusable assets was less applicable at the case organization, as roles are interchangeable. this is also a characteristic of agile development environments [cit] . the experts noted that producers of reusable assets are usually experienced developers in line with the findings of fafchamps [cit] . best practices have been defined and the producers sometimes act as a coach towards the consumers of reusable assets, helping them learn the required knowledge and skills. when a consumer wants to change a component the change request must be placed at the producer of this asset. if the change request is accepted the change requester has to populate the change into the software asset. when a consumer decides the skip this step a separate component version is created leading to possible merge conflicts in the future. the assessment of the people factors appeared to be difficult as the people capability maturity model seems to be more applicable for larger organizations. smaller organization do address the development of skills and experience, but arguable do not do this structurally. additional research is required to improve the assessment of the people factors."
"at level 1 ad-hoc reuse events are found. ad-hoc reuse events are reuse events caused by individuals. the reuse events are not coordinated and not monitored. no formal reuse processes are present. the individual is driven by previous experience, where code is often scavenged. scavenged code is code that is copypasted from previous projects. every organization is expected to have a form of ad-hoc reuse."
communication tool support is used to describe the active support of communication among producers and consumers through the use of tools. the reuse factor is included into the management tool as it is also identified as a possible way to scale agile methodologies for more complex environments.
"the literature review resulted in the identification of the business factor 'domain focus'. the domain focus is an indicator for the level of commonalities among products. the development of applications for a narrow and focused domain will likely result in high levels of commonalities among created solutions, while the development of solutions for a broad and unfocussed domain will likely result in low levels of commonalities. the software reuse levels are therefore influenced by the strategic choice of an organization to focus on a certain domain. griss also recognizes the domain focus as a form of a need to commit to software reuse [cit] . he notes that when there is no need to commit to reuse employees may behave according to the not invented here (nih) syndrome [cit] . this syndrome assumes that developers prefer to build their own assets instead of reusing the assets created by someone else."
"this section presents the expert opinions based on the evaluation during and after the application process. the proposed reuse levels are considered useful and relevant. during the discussion presented in previous sections of this chapter, several points regarding the reuse factors already became clear. supplier management and case tool support are considered less relevant, but may be more applicable in other organizations. furthermore, the experts emphasized the importance of the selling strategy related to the domain focus. it is reasonable to assume that the selling strategy is the basis for the domain focus, it was however not taken explicitly in the reuse factor itself. due to interchangeable roles the experts had problems with making a distinction between the people factors, but at the same time it proved to be valuable for discussing relevant aspects from different viewpoints. based on the expert evaluation no direct changes were proposed to the proposed set of reuse factors, they are considered complete without superfluous elements. the reuse practices were not evaluated in detail, but the application process did provided valuable insights. a high standard deviation among the results can possibly serve as an indicator for assessment difficulties. the relatively high standard deviation for the people factors confirmed such difficulties. the used practices for the people factors are derived from the p-cmm [cit], which are likely to be more applicable in larger organizations. the experts did note that these practices contained relevant elements, but the exact matching of the elements was a combination spread out over multiple reuse levels. improving the practices through the use of additional case studies likely leads to a higher applicability of the management tool in general. the experts also suggested improving the assessment method through the use of an additional variable. this variable should measure the desired scalability of a reuse factor explicitly. the relevance variable indirectly includes the scalability as well, but the way it includes the scalability did not completely satisfy the experts."
"case tool support is integrated as far as desired, namely, for interface components. the experts do not see potential in further investing in case tool support for software reuse. in a narrower domain it may however offer additional benefits."
communication tool support is installed through the use of mailing lists and a bug tracking system. both tools are not specific for software reuse and are used in a broader sense. the experts emphasized that communication channels should remain informal. wikipedia pages may be used for complementary documentation.
"the purpose of this research is to develop a conceptual software reuse management tool for addressing both technical and organizational reuse issues. in order to set up such a management tool a solid theoretical basis is required. a literature survey has been conducted to evaluate the current state of software reuse literature, existing reuse models and frameworks. furthermore, the reuse issues are analyzed in greater detail through a systematic literature review of the top 25 is journals. the proposed management tool is validated through an expert panel at a medium sized software development organization, providing valuable insights in the software reuse practices and the validity of the management tool. the results are presented in the following chapters. in section 2 we explain the core components of our approach. section 3 is devoted to an account on the results of the validation we performed for our tool by applying it in an agile software development organization. we conclude the paper with a discussion of some conclusions and of several pointers to future work."
the experts noted that reuse events and costs can be roughly estimated based on the experience of a developer and relevant domain knowledge. formal reuse measurement and costs models are not present at the case organization.
"this paper proposes a conceptual management tool for addressing software reuse issues. the management tool extends existing literature and provides valuable insights in the adoption of software reuse in agile development organizations. although the tool was intended to be specific for agile development organizations it was set up in a more generic way, because agile development methodologies have proven to be applicable in more complex environments. the combination of the cmmi-dev model with agile methodologies seems to be contradictive, but recent literature investigates the combination of both resulting in several success stories by balancing both aspects [cit] . in all cases it remains up to the individual organization to choose which levels of the management tool are desirable for the business environment. furthermore, the case study resulted in insights regarding the validity of the management tool. case tool support and supplier management appeared to be less applicable for the case, but are expected to be more important for other organizations. the assessment of the people factors should be further improved through the use of additional case studies. the use of a single case study is considered a major limitation of this research. another limitation is that the performed case study is based on a great amount of quantitative data. despite the use of a voice recorder, transcripts and multiple data sources the results are coloured by individual perception. interestingly, the results discovered during the case study are in line with the expectations of agile development organizations, which maintain less formalized processes and focus on people aspects. the case study confirms that such organizations are indeed well capable of identifying and seizing reuse opportunities as stated in the introduction. future research can use the assessment results as a basis for identifying reuse patterns. a reuse pattern defines a combination of reuse scores and relevance variables linked to certain types of organizations. when several reuse patterns have been identified they can be used for new organizations as implementation guideline. the purpose of the management tool in such a case is then no longer focussed on evaluating software reuse, but also on providing prescriptive guidelines for implementing software reuse into organizations."
"our observation is that process factors scored consistently lower during the evaluation process compared to the other reuse factors. the experts also added lower relevance variables indicating that major improvement are not desired. the discussion with the experts confirmed this statement. the essence seems to be that reuse events should be planned in the beginning of a project as much as possible, outside the project scope the efforts are difficult to justify."
the experts were in general not positive about externally acquired assets from black-box component markets. the functionality required was often slightly different from the functionality provided by such components. open source projects such as .spring and nhibernate are successfully integrated and have proven their value. formal supplier management appeared to be not applicable for open source projects.
"when analyzing the problem regarding software reuse in greater detail, it becomes evident that there is no holistic approach available to address software reuse issues. literature assumes that systematic and formalized processes are key for achieving higher levels of reuse, but agile development organizations and the open source community are also successfully practicing software reuse without having these, often extensively documented and formalized procedures or task definitions."
"the case organization is a medium sized dutch software development organization. this organization utilizes agile development methodologies [cit] to be able to quickly respond to fast changing dynamic markets. within the organization each business units focuses on separate market segments (e.g., financial sector, healthcare, etc). because each business unit is operating in a separate market segment, its domain is defined to a certain extend and natural levels of reuse exist. the expert panel consist of 9 members and covers four business units operating on the same knowledge base. the roles covered by the expert panel are: developer, analyst and project manager. each role is covered multiple times."
"quality models appeared seem to be less relevant as the organization relies on the expertise of developers. an expert noted that: 'when a component appears to be of insufficient quality it will be improved the next time it is used'. the disadvantage of this strategy is, however, that the other projects using this component may have to be updated as well. this line of thought indicates a possible relation between the amount of times a component is reused and its quality. formal quality models may be more applicable in larger organizations."
"the processes regarding software reuse can grouped around two sets of components. the first set of components consists of those that are centrally and more formally managed. the assessment results for these components were remarkably higher than for the other set of components. the other set of components is managed more informally often within a business unit or a specific project. the components have to be extracted from these projects first before they can be reused. for both component sets the customer demands are directive. the customer is effecting a pull mechanism on the development of components. the components are not developed for being pushed into the market. in some rare cases a separate project is set up to create reusable assets, this can be due to converging versions of reusable assets which have to be merged again or because the expected paybacks of a reusable asset are so obvious that the costs can be easily justified. apart from these extraordinary situations reuse events are usually planned during the design phase of a project or emerge during the development phase. in the latter case experienced developers recognize and exploit reuse opportunities. the chance of successfully creating reusable assets is highest at the beginning of a project, because priorities shift near the end of the project and negatively influence the resources available for creating reusable assets."
"the last process factor is configuration and change management required for the managed of reusable assets. without configuration and change management, reusable assets are expected to start 'having a life of their own'. in literature configuration and change management is addressed by assigning dedicated roles [cit] or assuming that components are carefully tested before they are populated into a repository [cit] . in practice additional mechanisms are likely to be present varying over different reuse levels."
"for a specific grasp force level and posture, we gathered the data from the 100-millisecond perturbation window (refer to the shaded regions in fig. 1b ) from all eight directions and all three measurements per direction. the perturbation window was approximately 200 milliseconds after the end of the position perturbation, where voluntary muscle activity does not contribute to the restoring force 13 ."
"where  j is duration of pulse symbols defined in figure 2, t j is repetition period of jtids slot equaling to 1/128 s,  m  i is dme interrogation pulse pair duration defined as  m shown in figure 2 ."
"to fasten a screw into a wall, a screwdriver is guided into the screw's slotted hole to rotate it. this task is unstable as the screwdriver can slip out of the hole during rotation. humans cannot react fast enough to correct a slipping screwdriver as visual feedback is delayed by approximately 200 milliseconds. instead, the central nervous system (cns) adopts a strategy of co-contracting muscle pairs in a joint to increase the endpoint stiffness of the arm. the endpoint stiffness as defined in this study is the stiffness of the upper arm, including the hand, which generates a restoring force that bring the hand to its equilibrium position. the endpoint stiffness is a mechanical property of the arm, such that the restoring force is generated without sensory feedback delays 1 ."
"estimation of endpoint stiffness. for small perturbations, we assumed that the inertia m, viscosity d and stiffness k of the arm were linear 7 such that the dynamics could be described by 0 and f 0 is the force measured prior to the perturbation. once the position of the arm reaches its perturbed steady-state value with null velocity and null acceleration, this equation simplifies to"
"where c is velocity of light. dme signal is a pair of pulses illustrated in figure 1, the waveform is depicted to be rectangular for simplicity. dme pulse pair characteristics are shown in table 1 . pulse duration w m is 3.5 s to all signals, while pulse interval s m is determined by dme mode (x or y) and signal type (interrogation or reply), which equals to 12 s or 30 s or 36 s. signal duration  d equals to w m plus s m . the average prf of interrogations is (10-30) hz or (40-150) hz when interrogator is operating on tracking or searching condition, respectively. the transponder operates at a transmission rate, including randomly distributed pulse pairs and identification signal and distance reply pulse pairs, of no less than 700 pulse pairs per second (ppps) and no more than 2700 ppps."
"event m and event j are assumed to be statistically independent, that is to say, inter-system and intra-system interference are independent of each other. substituting equations (4) and (8) into (3), we obtain"
"what caused the endpoint stiffness of the arm to increase with the grasp force? we recruited a new set of 10 subjects and measured their endpoint stiffness as a function of the grasp force. in this experiment, six wireless surface electromyography (emg) sensors were placed on the subject's elbow monoarticular, biarticular and shoulder www.nature.com/scientificreports www.nature.com/scientificreports/ muscle pairs (see methods for selected muscles). the mean muscle activity was measured in the 100-millisecond window where the restoring force from the arm was measured. for this experiment, we only measured the endpoint stiffness in the center position, and each grasp force level was measured five times. subjects also wore a wrist brace to prevent wrist movements. the activity of each muscle was normalized by dividing its mean activity measured in all grasp force levels (see methods for details). table 2 shows the group mean and standard error endpoint stiffness values for all grasp force levels. as with the data shown in table 1 from the previous experiment, the endpoint stiffness values increased with the grasp force."
"we perturbed the position of the arm to estimate the arm's endpoint stiffness in three different positions of the workspace with different levels of power grasp force. the magnitude of the endpoint stiffness increased with the grasp force. this relationship was observed in the three different postures that we tested, implying that the linear correlation between the grasp force and the endpoint stiffness magnitude was robust under different postures."
"jtids refers to the communications component of link 16, which is the designation of a tactical data link that is being fully integrated into operations of the joint services, the forces of the north atlantic treaty organization (nato), and other allies. jtids has the integrated capability of communication, navigation and identification. jtids uses the principle of time division multiple access (tdma), all jtids units are preassigned sets of time slots in which to transmit their data and in which to receive data from other units. each time slot is 1/128 s, or 7.8125 milliseconds (ms), in duration. the time distribution of a jtids slot is shown in figure 2, there is a random jittering time t j at the beginning of a slot time, then pulse signals time  j follows, safeguard time t g is in the last."
"where i denotes the event that valid interrogations are received by transponder, d is the event that valid interrogations are not interfered by identification pulses, and r is the event that reply transmitted from transponder which is not interfered by jtids. the probabilities of these events are p(i), p(d) and p(r), respectively."
"the single pulse symbol packet consists of one 6.4 s pulse of modulated carrier followed a 6.6 s dead time for a total pulse symbol packet duration of 13 s. the period of pulse signals is composed of 258 pulses when the data pulse format of the time slot is packed in standard double pulse or packed-2 single pulse, thus, the pulse signals lasted 3.354 ms. if the data pulse format of the time slot is packed in packed-2 double pulse or packed-4 single pulse, the period of pulse signals is composed of 444 pulses, jittering time is 0, safeguard time is 2.0405 ms, and then the pulse signal is 5.772 ms [cit] ."
the portion of the frequency spectrum between 950 mhz and 1150 mhz is called the lx band. dme and jtids are all operating within the lx band; the distribution of frequency is shown in the frequency is not held constant during the time slot but is changed rapidly (every 13 microsecond) according to a predetermined pseudorandom pattern; this technique is called frequency hopping.
"although the grasp force methodology may signify changes in the magnitude of the arm's endpoint stiffness, the spatial information of the endpoint stiffness is lost. the grasp force is unsuitable for measuring a change in the orientation of the stiffness ellipse, which might be useful when examining how humans optimize their endpoint stiffness to counter unstable or unpredictable environments 4 . the loss of the stiffness ellipse's orientation is traded off by the superior temporal information provided by the grasp force. as was observed in a previous study of ours 11, the grasp force changed not only between trials, but within each reaching movement. the change in table 2 . the group mean and standard error of the endpoint stiffness matrix parameters k xx, k xy, k yx and k yy in the center position from the second set of 10 subjects where the emg from the six muscles in the arm were also measured."
"measurement of muscle activity. in the second experiment, 10 additional participants were recruited to measure the muscular activity in the elbow and shoulder while grasping the robotic interface. in this experiment, we only measured the stiffness of the arm in the center posture, and tested each grasp force level 5 times. the electromyography (emg) was measured using the delsys trigno wireless emg sensor system at 1000 hz from the elbow monoarticular pair (brachioradialis and triceps lateral head); the biarticular pair (biceps brachii and the triceps long head); and the shoulder muscle pairs were measured (pectoralis major and the posterior deltoid). in this experiment, subjects also wore a wrist brace to prevent wrist movements.the mean emg activity in the same 100-millisecond window, where the restoring forces were measured, was calculated for each perturbation."
"the remainder of the paper is organized as follows. in section 2, we revisited the principles of dme and jtids briefly, and focused on the technical characteristics of the two systems that are pertinent to the analysis of interference such as signal structure, pulse duration and prf. in section 3, we constructed the flow chart of dme signal firstly, and then defined the evaluation criteria of dme capability. finally, we built different models to analyze the impact of intra-system and inter-system interference on dme using periodic pulse collision method (ppcm) and theory, respectively. in section 4, we compared the result from ppcm and theory with that from monte carlo simulation, then analyzed re with respect to capacity, studied capacity of dme when re and reply rate are limited. in section 5, we provide conclusions."
"the subjects were seated facing the kinarm planar robotic manipulandum from bkin technologies (see fig. 1a ). a subject held onto the kinarm interface via a handle that was affixed with a three-axis force sensor (tec gihan) to measure the grasp force between the palm of the hand and the handle. the kinarm's handle itself contains a six-axis force sensor, which was used to measure the arm's restoring force in the two dimensional plane of the task. an edero armon arm support was used under the elbow to support the arm's weight when using the robotic interface. visual feedback was provided on a monitor that was placed upside-down such that subjects viewed a reflection of the monitor on a thin film mirror placed above the hand, which obscured it from view. the data from the kinarm and the two force sensors was recorded at 1000 hz."
"next, we looked at the restoring force in the left, center and right positions from a sample subject (see fig. 1c ). the lighter colors denote a stronger grasp force. the lines connect the three repeated measurements for each position perturbation direction. the restoring force was symmetrically distributed around the origin. the restoring force from the stronger grasp force trials was larger than during a weaker grasp. this restoring force data was linearly regressed with the position perturbation data to estimate the endpoint stiffness at each grasp force level, separately for each posture."
"since the fundamental reason of interference is that interfering signals overlap with the desired signal, overlap method is applied widely to analyze interference in the time domain; overlap is judged by the relative position of rising edge between desired and interfering signal. analysis of overlap can be achieved by monte carlo simulation or mathematical derivation. using monte carlo method, the initial time difference between desired and interfering signal varies randomly in every simulation. then the average overlap probability is achieved after many simulations. [cit] derived the formula of overlap probability between two or more signals, the statistical analytic model of interference analysis is derived from pulse duration and pulse repetition frequency (prf). houdzoumis [cit] derived the formula of overlap probability based on poisson distribution of pulse stream composed of desired signal and interfering signals. the error of the models mentioned above is little if the number of signals is not too many and overlap probability is not too big, otherwise, the error is large. the reason is that dead time of receiver is thought to be a part of pulse duration for the entire desired signals. in fact, dead time can be regarded as a part of pulse duration only when the desired signal does not overlap with others. it is assumed that desired signal can be received only when it does not overlap with its adjacent pulses and its arrival time is out of dead time. however, the formulas mentioned above cannot calculate the overlap between arrival and dead time rightly, and consequently the error occurs. dead time is generated by the receiving of front desired signals, but it affects the receiving of subsequent signals since they are thought to be interfered if their arrival times overlap with dead time; however, it is too difficult to judge easily which one of the subsequent signals is in the dead time. owing to the similarity between the receiving of signal and queueing process, queueing theory is often applied to analyze the characteristics of communication [cit] . this paper developed a mathematical model for calculating collision probability between desired signal and intra-system interference based on m/m/1/0 queueing theory model, thus, the collision probability between dead time and subsequent pulse signals is calculated accurately even if the pulse-stream density is high, and consequently intra-system interference can be analyzed accurately. combining the mathematical model of collision probability for periodic pulse, we built a mathematical model to calculate re for dme when interfered by intra-system and inter-system signals."
"we examined if the subjects were successful in maintaining the target grasp force during the experiment. the grasp force was averaged in a 100-millisecond window of the perturbation near the end of the position perturbation. this average grasp force was calculated per perturbation direction, and the mean of all directions and postures was taken to find the group mean grasp force for each target level of grasp force. the group mean grasp force, along with the standard error, were found to be 4.79  0.06 n, 9.55  0.09 n, 14.30  0.10 n and 19.14  0.10 n. the grasp force was comfortably within a tolerance of 4% of the target level, implying that the subjects were successful in maintaining the grasp force during the perturbations."
"the aim of this study was to assess the relationship between the hand's power grasp force and the magnitude of the endpoint stiffness of the arm. we hypothesized that this relationship may depend on the arm's posture, and so we measured the arm's endpoint stiffness as a function of grasp force in three different postures. when measuring the endpoint stiffness of the arm, we opted for a position perturbation paradigm commonly used in the literature where a hand-held robotic interface perturbs the arm's position by several millimeters and measures the restoring force to estimate the arm's endpoint stiffness 8, 9, 13, 14 . we hypothesized that the magnitude of the endpoint stiffness of the arm was positively correlated with the grasp force, and that the posture may interact with this relationship."
"after dme transponder receives a valid interrogation, delays it a dead time, then transmit back to interrogator; meanwhile, prf of dme interrogations is jittered to ensure that interrogator can recognize their own reply pulses only, thus, there is no intra-system interference in dme reply pulses. they can be interfered by inter-system interference sources (e.g., jtids) only just shown in figure 4, according to the probability formula for inter-system interference, p(r) can be calculated as follows:"
"since the calculation for reply efficiency is based on pulse collision, and dme signal duration in mode y is larger than that in mode x from table 1, the collision probability in mode y is larger than that in mode x according to equations (13)- (15); as a result, reply efficiency in mode y is smaller than that in mode x just as shown in figure 6 . moreover, we can see that the difference of reply efficiency between mode x and y increases with the number of aircraft, and yet the differences is not always increases in fact. according to equation (17), the difference r e can be calculated as follows:"
"combining an m/m/1/0 queueing model with ppcm, we construct an analytical model of calculating overlap probability between two or more pulses. no matter how many aircraft there are, the analytical model can be used to analyze intra-system and inter-system interference on dme. according to the results mentioned in section 4, we may reasonably conclude that the m/m/1/0 queueing model can calculate the collision probability between dead time of transponder and duration of subsequent desired signals accurately. hence, the analytical model is more consistent with simulation than single ppcm."
"with the rise of the number of aircraft, intra-system interference increases, on the contrary, re decreases. in addition, re is affected by inter-system interference; for example, jtids interference causes re to decrease about 5% when there are about 100 aircraft. dme capacity is related to reply rate and re simultaneously, they are interrelated and interact on each other. in accordance with the limits of re and reply rate recommended by icao, the overall performance of dme cannot be improved unless multi parameters are optimized systematically."
"where the subscript s, b and e denote the shoulder, the biarticular and the elbow, and the positive and negative subscripts indicate the flexor and extensor muscle, respectively. under the kelvin-voigt muscle model, the stiffness of a muscle is proportional to its muscle activation [cit], e.g. to compare the activation of the muscles between subjects, the activity of each muscle was normalized by dividing it with its average activity in all trials,"
"our study cannot determine whether the primary contributor to the increase in the endpoint stiffness was the grasp stiffness or the stiffness of the arm. however, the emg data shows that the increase in the endpoint stiffness was not solely due to greater grasp stiffness, which is known to increase with the grasp force 20 . by measuring the muscle activity of the elbow monoarticular pair, the biarticular pair and the shoulder pair in the second experiment, we found that muscular co-contraction of the elbow, shoulder and cross-joints increased with the grasp force. this synergistic activation has been previously observed between the power grasp and either the elbow or the shoulder muscles in other studies 21, 22 . the increase in muscle activity was somewhat more pronounced in the elbow monoarticular pair in comparison to the biarticular and the shoulder muscle pair. this suggests that muscles closer to the hand activate more strongly during grasping in comparison to muscles further away from the hand."
"the restoring force was symmetrically distributed around the origin in the left, center and right positions, implying that for small position perturbations and a minimum grasp force of 5 n, there is little risk of the robotic handle slipping out of the subject's hand, which may adversely influence the measurement of the endpoint stiffness. furthermore, the high proportion of the variance explained by the linear regression of the restoring force against the position perturbation shows that a consistent estimate of endpoint stiffness was obtained by our perturbation methodology where visual feedback of the grasp force was provided."
"www.nature.com/scientificreports www.nature.com/scientificreports/ the grasp force resembled the temporal profile of the movement imprecision when reaching in an unstable force field. emg also provides a high temporal resolution estimate of the endpoint stiffness of the arm, but it requires calibration at different postures to be effective in estimating the endpoint stiffness during reaching 19 . the grasp force, on the other hand, was constant during normal reaching and was not influenced by the load force nor by external forces imposed by a robotic interface 11 . thus, analyzing the changes in the grasp force may further our understanding of the adaptation in endpoint stiffness during both postural and reaching tasks."
"2.1. first principles and baseband signal of dme dme is an aeronautical radio navigation system and consists of transponder installed on the ground and interrogator fitted in the aircraft. dme can provide the slant range d between aircraft and ground beacon continuously. interrogator transmits interrogations to transponder. after receiving interrogation, transponder delays it for quite a period named as dead time t d, transponder refuses to receive other interrogations in the dead time. after the delay of a dead time, interrogations are renamed as reply and transmitted back to interrogator, it takes a period  r for interrogator to transmit interrogations and receive reply [cit] . the slant range d can be calculated as follows:"
the quantities of dx and df were measured by the kinarm's position sensing and its six-axis force sensor. this data was linearly regressed to find the endpoint stiffness k. the data from all perturbations in all trials was used in the analysis to estimate the endpoint stiffness for a given posture and grasp force level per subject.
"the endpoint stiffness matrix k can be visualized as an ellipse of restoring forces that is produced when a unit position perturbation vector is multiplied by the endpoint stiffness matrix 13 . the unit perturbation vector is slowly rotated in increments between the angles of zero and 360 degrees to calculate the restoring force generated by the endpoint stiffness. by plotting these restoring forces, an ellipse is generated whose size and orientation reveal in which direction the restoring force is greatest. the larger the ellipse, the larger the restoring force. the major axis of the ellipse is the direction in which the endpoint stiffness is greatest. the size of the endpoint stiffness was computed by the product of the length of the major and minor axes of the endpoint stiffness ellipse with ."
"where  m  r is the reply pulse pair duration defined as  m shown in figure 1, its value can be seen from table 1 ."
"experimental setup. 20 subjects, who all gave their informed consent, participated in the study (10 subjects in the first experiment and 10 subjects in the second experiment). the experimental protocol was approved by the ethical review committee for epidemiological studies at the tokyo institute of technology (reference number a17086), [cit] helsinki declaration and its later amendments or comparable ethical standards."
"studies have shown that humans are able to adapt the stiffness of their arm to suppress self-generated motor variability 2,3 and reject perturbations from the environment 4, 5 . existing methods to estimate the arm's endpoint stiffness employ a robotic interface to perturb the arm's position and measure the restoring force generated by the arm [cit] . force perturbations can also be employed to estimate the arm's endpoint stiffness 10 . these methods require a robotic interface, which are expensive and are not portable, making them unsuitable for measurements outside of the laboratory environment. in a recent study, we reported that when subjects had to reach with higher movement precision, they increased their hand's power grasp force 11, which we define as the force measured between the palm of the hand and the handle of the robotic interface. the adaptation of the grasp force was similar to the manner in which the arm's endpoint stiffness has been reported to change in an unstable environment 12 . if the grasp force and the arm's endpoint stiffness are related, measuring changes in the power grasp force may yield an affordable and portable method of estimating changes in the magnitude of the arm's endpoint stiffness."
"in this study, we deliberately chose to measure the endpoint stiffness by exerting position perturbations through the subject's hand. some studies use a mold of the hand that is attached to the handle of the robotic interface to perturb the subject's arm at the wrist, effectively bypassing the hand and the need to grasp the robotic interface. however, the endpoint stiffness of the arm in real-world tasks does not bypass the hand. when using a hammer or a chisel to carve wood, the endpoint stiffness at the tip of the tool depends on both the stiffness of the grasp and the stiffness of the arm. we define the grasp stiffness to be the stiffness in the connection between the handle of a held object and the hand. the stiffness of the arm arises from the muscular co-contraction of muscles in the elbow and the shoulder. the stiffness of the arm alone may not reflect how humans increase endpoint stiffness in real-world tasks. as such, it is important to ascertain the endpoint stiffness at the hand, i.e. the combined stiffness of the grasp and the arm."
"once seated, subjects were instructed to keep their head and torso in the same posture throughout the experiment. this was accomplished by having the subjects rest their forehead on a fixed headrest, with the chest pressing against a table placed below the robotic interface. the position of the shoulder, and the lengths of the forearm and the upper-arm were measured prior to the experiment to determine the posture of the arm."
"dme is interfered by inter-system and intra-system interference at the same time, as has been discussed above, the probability of inter-system interference are p(d) and p(r), and p(r) is the probability of intra-system interference. some simulation parameters are the same as mentioned above, others are shown as follows:"
"in another study, we found that the grasp force increased when subjects desired higher movement precision in two reaching tasks 11 . the reaching tasks tested in this study were inspired by earlier studies that had reported an increase in endpoint stiffness during their tasks [cit] . in one of these tasks, subjects reached in an unstable force field that pushed the hand left or right when it deviated from the midline. this task required higher endpoint stiffness to precisely move along the midline to reduce the effects of the unstable force field 4 . the grasp force measured during the learning of this task 11 resembled the change in the co-contraction of the arm reported in another study 23 . namely, the grasp force increased rapidly over the course of five trials, and then exponentially declined in magnitude and plateaued at a level higher than in training trials without the force field."
